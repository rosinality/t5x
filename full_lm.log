Using ssh batch size of 8. Attempting to SSH into 1 nodes with a total of 8 workers.
SSH: Attempting to connect to worker 0...
SSH: Attempting to connect to worker 1...
SSH: Attempting to connect to worker 2...
SSH: Attempting to connect to worker 3...
SSH: Attempting to connect to worker 4...
SSH: Attempting to connect to worker 5...
SSH: Attempting to connect to worker 6...
SSH: Attempting to connect to worker 7...
2024-05-12 23:41:52.002490: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2024-05-12 23:41:52.005351: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2024-05-12 23:41:52.052171: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2024-05-12 23:41:52.054483: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2024-05-12 23:41:52.058219: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2024-05-12 23:41:52.086088: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2024-05-12 23:41:52.280307: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2024-05-12 23:41:52.298560: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
WARNING:absl:Type handler registry overriding type "<class 'float'>" collision on scalar
WARNING:absl:Type handler registry overriding type "<class 'bytes'>" collision on scalar
WARNING:absl:Type handler registry overriding type "<class 'numpy.number'>" collision on scalar
WARNING:absl:Type handler registry overriding type "<class 'float'>" collision on scalar
WARNING:absl:Type handler registry overriding type "<class 'bytes'>" collision on scalar
WARNING:absl:Type handler registry overriding type "<class 'numpy.number'>" collision on scalar
WARNING:absl:Type handler registry overriding type "<class 'float'>" collision on scalar
WARNING:absl:Type handler registry overriding type "<class 'bytes'>" collision on scalar
WARNING:absl:Type handler registry overriding type "<class 'numpy.number'>" collision on scalar
WARNING:absl:Type handler registry overriding type "<class 'float'>" collision on scalar
WARNING:absl:Type handler registry overriding type "<class 'bytes'>" collision on scalar
WARNING:absl:Type handler registry overriding type "<class 'numpy.number'>" collision on scalar
WARNING:absl:Type handler registry overriding type "<class 'float'>" collision on scalar
WARNING:absl:Type handler registry overriding type "<class 'bytes'>" collision on scalar
WARNING:absl:Type handler registry overriding type "<class 'numpy.number'>" collision on scalar
WARNING:absl:Type handler registry overriding type "<class 'float'>" collision on scalar
WARNING:absl:Type handler registry overriding type "<class 'bytes'>" collision on scalar
WARNING:absl:Type handler registry overriding type "<class 'numpy.number'>" collision on scalar
WARNING:absl:Type handler registry overriding type "<class 'float'>" collision on scalar
WARNING:absl:Type handler registry overriding type "<class 'bytes'>" collision on scalar
WARNING:absl:Type handler registry overriding type "<class 'numpy.number'>" collision on scalar
WARNING:absl:Type handler registry overriding type "<class 'float'>" collision on scalar
WARNING:absl:Type handler registry overriding type "<class 'bytes'>" collision on scalar
WARNING:absl:Type handler registry overriding type "<class 'numpy.number'>" collision on scalar
I0512 23:41:54.145467 140031916288000 resource_reader.py:50] system_path_file_exists:t5x/examples/t5/t5_1_1/examples/openmoe_large_full_lm.gin
I0512 23:41:54.146329 140031916288000 resource_reader.py:37] gin-config opened resource file:/home/rosinality/t5x/t5x/examples/t5/t5_1_1/examples/openmoe_large_full_lm.gin
I0512 23:41:54.163763 139674595104768 resource_reader.py:50] system_path_file_exists:t5x/examples/t5/t5_1_1/examples/openmoe_large_full_lm.gin
I0512 23:41:54.164586 139674595104768 resource_reader.py:37] gin-config opened resource file:/home/rosinality/t5x/t5x/examples/t5/t5_1_1/examples/openmoe_large_full_lm.gin
I0512 23:41:54.181263 140339465779200 resource_reader.py:50] system_path_file_exists:t5x/examples/t5/t5_1_1/examples/openmoe_large_full_lm.gin
I0512 23:41:54.182108 140339465779200 resource_reader.py:37] gin-config opened resource file:/home/rosinality/t5x/t5x/examples/t5/t5_1_1/examples/openmoe_large_full_lm.gin
I0512 23:41:54.219188 140440593786880 resource_reader.py:50] system_path_file_exists:t5x/examples/t5/t5_1_1/examples/openmoe_large_full_lm.gin
I0512 23:41:54.220022 140440593786880 resource_reader.py:37] gin-config opened resource file:/home/rosinality/t5x/t5x/examples/t5/t5_1_1/examples/openmoe_large_full_lm.gin
I0512 23:41:54.228310 140095893518336 resource_reader.py:50] system_path_file_exists:t5x/examples/t5/t5_1_1/examples/openmoe_large_full_lm.gin
I0512 23:41:54.229175 140095893518336 resource_reader.py:37] gin-config opened resource file:/home/rosinality/t5x/t5x/examples/t5/t5_1_1/examples/openmoe_large_full_lm.gin
I0512 23:41:54.391991 140031916288000 resource_reader.py:50] system_path_file_exists:flaxformer/flaxformer/t5x/configs/moe/models/st_moe_decoder_only_large.gin
E0512 23:41:54.392409 140031916288000 resource_reader.py:55] Path not found: flaxformer/flaxformer/t5x/configs/moe/models/st_moe_decoder_only_large.gin
I0512 23:41:54.392470 140031916288000 resource_reader.py:50] system_path_file_exists:./flaxformer/flaxformer/t5x/configs/moe/models/st_moe_decoder_only_large.gin
E0512 23:41:54.392515 140031916288000 resource_reader.py:55] Path not found: ./flaxformer/flaxformer/t5x/configs/moe/models/st_moe_decoder_only_large.gin
I0512 23:41:54.410406 139674595104768 resource_reader.py:50] system_path_file_exists:flaxformer/flaxformer/t5x/configs/moe/models/st_moe_decoder_only_large.gin
E0512 23:41:54.410857 139674595104768 resource_reader.py:55] Path not found: flaxformer/flaxformer/t5x/configs/moe/models/st_moe_decoder_only_large.gin
I0512 23:41:54.410921 139674595104768 resource_reader.py:50] system_path_file_exists:./flaxformer/flaxformer/t5x/configs/moe/models/st_moe_decoder_only_large.gin
E0512 23:41:54.410969 139674595104768 resource_reader.py:55] Path not found: ./flaxformer/flaxformer/t5x/configs/moe/models/st_moe_decoder_only_large.gin
I0512 23:41:54.433071 140339465779200 resource_reader.py:50] system_path_file_exists:flaxformer/flaxformer/t5x/configs/moe/models/st_moe_decoder_only_large.gin
E0512 23:41:54.433523 140339465779200 resource_reader.py:55] Path not found: flaxformer/flaxformer/t5x/configs/moe/models/st_moe_decoder_only_large.gin
I0512 23:41:54.433587 140339465779200 resource_reader.py:50] system_path_file_exists:./flaxformer/flaxformer/t5x/configs/moe/models/st_moe_decoder_only_large.gin
E0512 23:41:54.433637 140339465779200 resource_reader.py:55] Path not found: ./flaxformer/flaxformer/t5x/configs/moe/models/st_moe_decoder_only_large.gin
I0512 23:41:54.448296 140031916288000 resource_reader.py:50] system_path_file_exists:flaxformer/flaxformer/t5x/configs/moe/models/tokens_choose_decoder_only_large.gin
E0512 23:41:54.448478 140031916288000 resource_reader.py:55] Path not found: flaxformer/flaxformer/t5x/configs/moe/models/tokens_choose_decoder_only_large.gin
I0512 23:41:54.448535 140031916288000 resource_reader.py:50] system_path_file_exists:./flaxformer/flaxformer/t5x/configs/moe/models/tokens_choose_decoder_only_large.gin
E0512 23:41:54.448578 140031916288000 resource_reader.py:55] Path not found: ./flaxformer/flaxformer/t5x/configs/moe/models/tokens_choose_decoder_only_large.gin
I0512 23:41:54.449431 140031916288000 resource_reader.py:50] system_path_file_exists:flaxformer/flaxformer/t5x/configs/moe/architectures/moe_decoder_only.gin
E0512 23:41:54.449562 140031916288000 resource_reader.py:55] Path not found: flaxformer/flaxformer/t5x/configs/moe/architectures/moe_decoder_only.gin
I0512 23:41:54.449612 140031916288000 resource_reader.py:50] system_path_file_exists:./flaxformer/flaxformer/t5x/configs/moe/architectures/moe_decoder_only.gin
E0512 23:41:54.449653 140031916288000 resource_reader.py:55] Path not found: ./flaxformer/flaxformer/t5x/configs/moe/architectures/moe_decoder_only.gin
I0512 23:41:54.444047 140311617681408 resource_reader.py:50] system_path_file_exists:t5x/examples/t5/t5_1_1/examples/openmoe_large_full_lm.gin
I0512 23:41:54.444901 140311617681408 resource_reader.py:37] gin-config opened resource file:/home/rosinality/t5x/t5x/examples/t5/t5_1_1/examples/openmoe_large_full_lm.gin
I0512 23:41:54.466686 139674595104768 resource_reader.py:50] system_path_file_exists:flaxformer/flaxformer/t5x/configs/moe/models/tokens_choose_decoder_only_large.gin
I0512 23:41:54.465158 140595318249472 resource_reader.py:50] system_path_file_exists:t5x/examples/t5/t5_1_1/examples/openmoe_large_full_lm.gin
E0512 23:41:54.466871 139674595104768 resource_reader.py:55] Path not found: flaxformer/flaxformer/t5x/configs/moe/models/tokens_choose_decoder_only_large.gin
I0512 23:41:54.466930 139674595104768 resource_reader.py:50] system_path_file_exists:./flaxformer/flaxformer/t5x/configs/moe/models/tokens_choose_decoder_only_large.gin
I0512 23:41:54.466069 140595318249472 resource_reader.py:37] gin-config opened resource file:/home/rosinality/t5x/t5x/examples/t5/t5_1_1/examples/openmoe_large_full_lm.gin
E0512 23:41:54.466976 139674595104768 resource_reader.py:55] Path not found: ./flaxformer/flaxformer/t5x/configs/moe/models/tokens_choose_decoder_only_large.gin
I0512 23:41:54.467876 139674595104768 resource_reader.py:50] system_path_file_exists:flaxformer/flaxformer/t5x/configs/moe/architectures/moe_decoder_only.gin
E0512 23:41:54.468014 139674595104768 resource_reader.py:55] Path not found: flaxformer/flaxformer/t5x/configs/moe/architectures/moe_decoder_only.gin
I0512 23:41:54.468067 139674595104768 resource_reader.py:50] system_path_file_exists:./flaxformer/flaxformer/t5x/configs/moe/architectures/moe_decoder_only.gin
E0512 23:41:54.468108 139674595104768 resource_reader.py:55] Path not found: ./flaxformer/flaxformer/t5x/configs/moe/architectures/moe_decoder_only.gin
I0512 23:41:54.466987 140440593786880 resource_reader.py:50] system_path_file_exists:flaxformer/flaxformer/t5x/configs/moe/models/st_moe_decoder_only_large.gin
E0512 23:41:54.467410 140440593786880 resource_reader.py:55] Path not found: flaxformer/flaxformer/t5x/configs/moe/models/st_moe_decoder_only_large.gin
I0512 23:41:54.467473 140440593786880 resource_reader.py:50] system_path_file_exists:./flaxformer/flaxformer/t5x/configs/moe/models/st_moe_decoder_only_large.gin
E0512 23:41:54.467519 140440593786880 resource_reader.py:55] Path not found: ./flaxformer/flaxformer/t5x/configs/moe/models/st_moe_decoder_only_large.gin
I0512 23:41:54.483327 140095893518336 resource_reader.py:50] system_path_file_exists:flaxformer/flaxformer/t5x/configs/moe/models/st_moe_decoder_only_large.gin
E0512 23:41:54.483764 140095893518336 resource_reader.py:55] Path not found: flaxformer/flaxformer/t5x/configs/moe/models/st_moe_decoder_only_large.gin
I0512 23:41:54.483826 140095893518336 resource_reader.py:50] system_path_file_exists:./flaxformer/flaxformer/t5x/configs/moe/models/st_moe_decoder_only_large.gin
E0512 23:41:54.483873 140095893518336 resource_reader.py:55] Path not found: ./flaxformer/flaxformer/t5x/configs/moe/models/st_moe_decoder_only_large.gin
I0512 23:41:54.470960 140529434355712 resource_reader.py:50] system_path_file_exists:t5x/examples/t5/t5_1_1/examples/openmoe_large_full_lm.gin
I0512 23:41:54.471805 140529434355712 resource_reader.py:37] gin-config opened resource file:/home/rosinality/t5x/t5x/examples/t5/t5_1_1/examples/openmoe_large_full_lm.gin
I0512 23:41:54.489854 140339465779200 resource_reader.py:50] system_path_file_exists:flaxformer/flaxformer/t5x/configs/moe/models/tokens_choose_decoder_only_large.gin
E0512 23:41:54.490041 140339465779200 resource_reader.py:55] Path not found: flaxformer/flaxformer/t5x/configs/moe/models/tokens_choose_decoder_only_large.gin
I0512 23:41:54.490099 140339465779200 resource_reader.py:50] system_path_file_exists:./flaxformer/flaxformer/t5x/configs/moe/models/tokens_choose_decoder_only_large.gin
E0512 23:41:54.490143 140339465779200 resource_reader.py:55] Path not found: ./flaxformer/flaxformer/t5x/configs/moe/models/tokens_choose_decoder_only_large.gin
I0512 23:41:54.491012 140339465779200 resource_reader.py:50] system_path_file_exists:flaxformer/flaxformer/t5x/configs/moe/architectures/moe_decoder_only.gin
E0512 23:41:54.491147 140339465779200 resource_reader.py:55] Path not found: flaxformer/flaxformer/t5x/configs/moe/architectures/moe_decoder_only.gin
I0512 23:41:54.491200 140339465779200 resource_reader.py:50] system_path_file_exists:./flaxformer/flaxformer/t5x/configs/moe/architectures/moe_decoder_only.gin
E0512 23:41:54.491243 140339465779200 resource_reader.py:55] Path not found: ./flaxformer/flaxformer/t5x/configs/moe/architectures/moe_decoder_only.gin
I0512 23:41:54.494436 140031916288000 resource_reader.py:50] system_path_file_exists:t5x/contrib/moe/configs/runs/pretrain.gin
I0512 23:41:54.512451 139674595104768 resource_reader.py:50] system_path_file_exists:t5x/contrib/moe/configs/runs/pretrain.gin
I0512 23:41:54.523777 140440593786880 resource_reader.py:50] system_path_file_exists:flaxformer/flaxformer/t5x/configs/moe/models/tokens_choose_decoder_only_large.gin
E0512 23:41:54.523957 140440593786880 resource_reader.py:55] Path not found: flaxformer/flaxformer/t5x/configs/moe/models/tokens_choose_decoder_only_large.gin
I0512 23:41:54.524019 140440593786880 resource_reader.py:50] system_path_file_exists:./flaxformer/flaxformer/t5x/configs/moe/models/tokens_choose_decoder_only_large.gin
E0512 23:41:54.524069 140440593786880 resource_reader.py:55] Path not found: ./flaxformer/flaxformer/t5x/configs/moe/models/tokens_choose_decoder_only_large.gin
I0512 23:41:54.525000 140440593786880 resource_reader.py:50] system_path_file_exists:flaxformer/flaxformer/t5x/configs/moe/architectures/moe_decoder_only.gin
E0512 23:41:54.525140 140440593786880 resource_reader.py:55] Path not found: flaxformer/flaxformer/t5x/configs/moe/architectures/moe_decoder_only.gin
I0512 23:41:54.525202 140440593786880 resource_reader.py:50] system_path_file_exists:./flaxformer/flaxformer/t5x/configs/moe/architectures/moe_decoder_only.gin
E0512 23:41:54.525244 140440593786880 resource_reader.py:55] Path not found: ./flaxformer/flaxformer/t5x/configs/moe/architectures/moe_decoder_only.gin
I0512 23:41:54.535410 140339465779200 resource_reader.py:50] system_path_file_exists:t5x/contrib/moe/configs/runs/pretrain.gin
I0512 23:41:54.539868 140095893518336 resource_reader.py:50] system_path_file_exists:flaxformer/flaxformer/t5x/configs/moe/models/tokens_choose_decoder_only_large.gin
E0512 23:41:54.540040 140095893518336 resource_reader.py:55] Path not found: flaxformer/flaxformer/t5x/configs/moe/models/tokens_choose_decoder_only_large.gin
I0512 23:41:54.540096 140095893518336 resource_reader.py:50] system_path_file_exists:./flaxformer/flaxformer/t5x/configs/moe/models/tokens_choose_decoder_only_large.gin
E0512 23:41:54.540145 140095893518336 resource_reader.py:55] Path not found: ./flaxformer/flaxformer/t5x/configs/moe/models/tokens_choose_decoder_only_large.gin
I0512 23:41:54.541007 140095893518336 resource_reader.py:50] system_path_file_exists:flaxformer/flaxformer/t5x/configs/moe/architectures/moe_decoder_only.gin
E0512 23:41:54.541148 140095893518336 resource_reader.py:55] Path not found: flaxformer/flaxformer/t5x/configs/moe/architectures/moe_decoder_only.gin
I0512 23:41:54.541201 140095893518336 resource_reader.py:50] system_path_file_exists:./flaxformer/flaxformer/t5x/configs/moe/architectures/moe_decoder_only.gin
E0512 23:41:54.541241 140095893518336 resource_reader.py:55] Path not found: ./flaxformer/flaxformer/t5x/configs/moe/architectures/moe_decoder_only.gin
I0512 23:41:54.494808 140031916288000 resource_reader.py:37] gin-config opened resource file:/home/rosinality/t5x/t5x/contrib/moe/configs/runs/pretrain.gin
I0512 23:41:54.495307 140031916288000 resource_reader.py:50] system_path_file_exists:t5x/configs/runs/pretrain.gin
I0512 23:41:54.495607 140031916288000 resource_reader.py:37] gin-config opened resource file:/home/rosinality/t5x/t5x/configs/runs/pretrain.gin
I0512 23:41:54.509283 140031916288000 gin_utils.py:83] Gin Configuration:
I0512 23:41:54.525017 140031916288000 gin_utils.py:85] from __gin__ import dynamic_registration
I0512 23:41:54.525085 140031916288000 gin_utils.py:85] import __main__ as train_script
I0512 23:41:54.525125 140031916288000 gin_utils.py:85] from flax import linen
I0512 23:41:54.525159 140031916288000 gin_utils.py:85] import flaxformer
I0512 23:41:54.525191 140031916288000 gin_utils.py:85] from flaxformer.architectures.moe import moe_architecture
I0512 23:41:54.525223 140031916288000 gin_utils.py:85] from flaxformer.architectures.moe import moe_enums
I0512 23:41:54.525255 140031916288000 gin_utils.py:85] from flaxformer.architectures.moe import moe_layers
I0512 23:41:54.525287 140031916288000 gin_utils.py:85] from flaxformer.architectures.moe import routing
I0512 23:41:54.525319 140031916288000 gin_utils.py:85] from flaxformer.architectures.t5 import t5_architecture
I0512 23:41:54.525350 140031916288000 gin_utils.py:85] from flaxformer.components.attention import dense_attention
I0512 23:41:54.525381 140031916288000 gin_utils.py:85] from flaxformer.components.attention import memory_efficient_attention
I0512 23:41:54.525412 140031916288000 gin_utils.py:85] from flaxformer.components import dense
I0512 23:41:54.525443 140031916288000 gin_utils.py:85] from flaxformer.components import embedding
I0512 23:41:54.525474 140031916288000 gin_utils.py:85] from flaxformer.components import layer_norm
I0512 23:41:54.525505 140031916288000 gin_utils.py:85] from gin import config
I0512 23:41:54.525535 140031916288000 gin_utils.py:85] import seqio
I0512 23:41:54.525566 140031916288000 gin_utils.py:85] import t5.data.mixtures
I0512 23:41:54.525597 140031916288000 gin_utils.py:85] from t5x import adafactor
I0512 23:41:54.525628 140031916288000 gin_utils.py:85] from t5x.contrib.moe import adafactor_utils
I0512 23:41:54.525659 140031916288000 gin_utils.py:85] from t5x.contrib.moe import models
I0512 23:41:54.525698 140031916288000 gin_utils.py:85] from t5x.contrib.moe import partitioning as moe_partitioning
I0512 23:41:54.525730 140031916288000 gin_utils.py:85] from t5x.contrib.moe import trainer as moe_trainer
I0512 23:41:54.525761 140031916288000 gin_utils.py:85] from t5x import gin_utils
I0512 23:41:54.525791 140031916288000 gin_utils.py:85] from t5x import partitioning
I0512 23:41:54.525822 140031916288000 gin_utils.py:85] from t5x import trainer
I0512 23:41:54.525853 140031916288000 gin_utils.py:85] from t5x import utils
I0512 23:41:54.525884 140031916288000 gin_utils.py:85] 
I0512 23:41:54.525916 140031916288000 gin_utils.py:85] # Macros:
I0512 23:41:54.525950 140031916288000 gin_utils.py:85] # ==============================================================================
I0512 23:41:54.525981 140031916288000 gin_utils.py:85] ACTIVATION_DTYPE = 'bfloat16'
I0512 23:41:54.526013 140031916288000 gin_utils.py:85] ACTIVATION_PARTITIONING_DIMS = 1
I0512 23:41:54.526043 140031916288000 gin_utils.py:85] ARCHITECTURE = @t5_architecture.DecoderOnly()
I0512 23:41:54.526074 140031916288000 gin_utils.py:85] AUX_LOSS_FACTOR = 0.01
I0512 23:41:54.526132 140031916288000 gin_utils.py:85] BATCH_SIZE = 384
I0512 23:41:54.526165 140031916288000 gin_utils.py:85] BIAS_INIT = @bias_init/linen.initializers.normal()
I0512 23:41:54.526196 140031916288000 gin_utils.py:85] DECODER_SPARSE_LAYOUT = %flaxformer.architectures.moe.moe_enums.LayerLayout.MIXED
I0512 23:41:54.526226 140031916288000 gin_utils.py:85] DROPOUT_FACTORY = @dropout_factory/linen.Dropout
I0512 23:41:54.526257 140031916288000 gin_utils.py:85] DROPOUT_RATE = 0.0
I0512 23:41:54.526288 140031916288000 gin_utils.py:85] EMBED_DIM = 2048
I0512 23:41:54.526319 140031916288000 gin_utils.py:85] EVAL_EXPERT_CAPACITY_FACTOR = 2.0
I0512 23:41:54.526350 140031916288000 gin_utils.py:85] EXPERT_DROPOUT_RATE = %DROPOUT_RATE
I0512 23:41:54.526380 140031916288000 gin_utils.py:85] EXPERT_MLP_DIM = %MLP_DIM
I0512 23:41:54.526411 140031916288000 gin_utils.py:85] GROUP_SIZE = 4096
I0512 23:41:54.526442 140031916288000 gin_utils.py:85] HEAD_DIM = 128
I0512 23:41:54.526473 140031916288000 gin_utils.py:85] JITTER_NOISE = 0.0
I0512 23:41:54.526503 140031916288000 gin_utils.py:85] LABEL_SMOOTHING = 0.0
I0512 23:41:54.526534 140031916288000 gin_utils.py:85] LOSS_NORMALIZING_FACTOR = 'NUM_REAL_TARGET_TOKENS'
I0512 23:41:54.526564 140031916288000 gin_utils.py:85] MIXTURE_OR_TASK_MODULE = None
I0512 23:41:54.526596 140031916288000 gin_utils.py:85] MIXTURE_OR_TASK_NAME = 'mix_full_lm_test'
I0512 23:41:54.526626 140031916288000 gin_utils.py:85] MLP_DIM = 8192
I0512 23:41:54.526657 140031916288000 gin_utils.py:85] MODEL = @models.MoeDecoderOnlyModel()
I0512 23:41:54.526695 140031916288000 gin_utils.py:85] MODEL_DIR = 'gs://rosinality-tpu-bucket/openmoe_8b/training'
I0512 23:41:54.526727 140031916288000 gin_utils.py:85] MODEL_PARALLEL_SUBMESH = None
I0512 23:41:54.526758 140031916288000 gin_utils.py:85] MOE_TRUNCATED_DTYPE = 'bfloat16'
I0512 23:41:54.526789 140031916288000 gin_utils.py:85] NUM_DECODER_LAYERS = 24
I0512 23:41:54.526820 140031916288000 gin_utils.py:85] NUM_DECODER_SPARSE_LAYERS = 4
I0512 23:41:54.526851 140031916288000 gin_utils.py:85] NUM_EMBEDDINGS = 256384
I0512 23:41:54.526882 140031916288000 gin_utils.py:85] NUM_EXPERT_PARTITIONS = 8
I0512 23:41:54.526913 140031916288000 gin_utils.py:85] NUM_EXPERTS = 8
I0512 23:41:54.526946 140031916288000 gin_utils.py:85] NUM_HEADS = 24
I0512 23:41:54.526977 140031916288000 gin_utils.py:85] NUM_MODEL_PARTITIONS = 4
I0512 23:41:54.527008 140031916288000 gin_utils.py:85] NUM_SELECTED_EXPERTS = 2
I0512 23:41:54.527039 140031916288000 gin_utils.py:85] OPTIMIZER = @adafactor.Adafactor()
I0512 23:41:54.527070 140031916288000 gin_utils.py:85] RANDOM_SEED = None
I0512 23:41:54.527101 140031916288000 gin_utils.py:85] ROUTER_Z_LOSS_FACTOR = 0.0001
I0512 23:41:54.527131 140031916288000 gin_utils.py:85] SCALE = 0.1
I0512 23:41:54.527162 140031916288000 gin_utils.py:85] SHUFFLE_TRAIN_EXAMPLES = True
I0512 23:41:54.527193 140031916288000 gin_utils.py:85] TASK_FEATURE_LENGTHS = {'targets': 2048}
I0512 23:41:54.527224 140031916288000 gin_utils.py:85] TRAIN_EXPERT_CAPACITY_FACTOR = 1.25
I0512 23:41:54.527254 140031916288000 gin_utils.py:85] TRAIN_STEPS = 500000
I0512 23:41:54.527285 140031916288000 gin_utils.py:85] USE_CACHED_TASKS = False
I0512 23:41:54.527316 140031916288000 gin_utils.py:85] USE_HARDWARE_RNG = False
I0512 23:41:54.527346 140031916288000 gin_utils.py:85] VOCABULARY = @seqio.SentencePieceVocabulary()
I0512 23:41:54.527377 140031916288000 gin_utils.py:85] Z_LOSS = 0.0001
I0512 23:41:54.527408 140031916288000 gin_utils.py:85] 
I0512 23:41:54.527439 140031916288000 gin_utils.py:85] # Parameters for adafactor.Adafactor:
I0512 23:41:54.527469 140031916288000 gin_utils.py:85] # ==============================================================================
I0512 23:41:54.527499 140031916288000 gin_utils.py:85] adafactor.Adafactor.decay_rate = 0.8
I0512 23:41:54.527530 140031916288000 gin_utils.py:85] adafactor.Adafactor.logical_factor_rules = @adafactor_utils.logical_factor_rules()
I0512 23:41:54.527561 140031916288000 gin_utils.py:85] adafactor.Adafactor.step_offset = 0
I0512 23:41:54.527592 140031916288000 gin_utils.py:85] 
I0512 23:41:54.570560 140440593786880 resource_reader.py:50] system_path_file_exists:t5x/contrib/moe/configs/runs/pretrain.gin
I0512 23:41:54.512820 139674595104768 resource_reader.py:37] gin-config opened resource file:/home/rosinality/t5x/t5x/contrib/moe/configs/runs/pretrain.gin
I0512 23:41:54.513325 139674595104768 resource_reader.py:50] system_path_file_exists:t5x/configs/runs/pretrain.gin
I0512 23:41:54.513635 139674595104768 resource_reader.py:37] gin-config opened resource file:/home/rosinality/t5x/t5x/configs/runs/pretrain.gin
I0512 23:41:54.527453 139674595104768 gin_utils.py:83] Gin Configuration:
I0512 23:41:54.543234 139674595104768 gin_utils.py:85] from __gin__ import dynamic_registration
I0512 23:41:54.543323 139674595104768 gin_utils.py:85] import __main__ as train_script
I0512 23:41:54.543368 139674595104768 gin_utils.py:85] from flax import linen
I0512 23:41:54.543404 139674595104768 gin_utils.py:85] import flaxformer
I0512 23:41:54.543439 139674595104768 gin_utils.py:85] from flaxformer.architectures.moe import moe_architecture
I0512 23:41:54.543472 139674595104768 gin_utils.py:85] from flaxformer.architectures.moe import moe_enums
I0512 23:41:54.543505 139674595104768 gin_utils.py:85] from flaxformer.architectures.moe import moe_layers
I0512 23:41:54.543538 139674595104768 gin_utils.py:85] from flaxformer.architectures.moe import routing
I0512 23:41:54.543570 139674595104768 gin_utils.py:85] from flaxformer.architectures.t5 import t5_architecture
I0512 23:41:54.543603 139674595104768 gin_utils.py:85] from flaxformer.components.attention import dense_attention
I0512 23:41:54.543636 139674595104768 gin_utils.py:85] from flaxformer.components.attention import memory_efficient_attention
I0512 23:41:54.543675 139674595104768 gin_utils.py:85] from flaxformer.components import dense
I0512 23:41:54.543709 139674595104768 gin_utils.py:85] from flaxformer.components import embedding
I0512 23:41:54.543742 139674595104768 gin_utils.py:85] from flaxformer.components import layer_norm
I0512 23:41:54.543775 139674595104768 gin_utils.py:85] from gin import config
I0512 23:41:54.543808 139674595104768 gin_utils.py:85] import seqio
I0512 23:41:54.543840 139674595104768 gin_utils.py:85] import t5.data.mixtures
I0512 23:41:54.543873 139674595104768 gin_utils.py:85] from t5x import adafactor
I0512 23:41:54.543905 139674595104768 gin_utils.py:85] from t5x.contrib.moe import adafactor_utils
I0512 23:41:54.543941 139674595104768 gin_utils.py:85] from t5x.contrib.moe import models
I0512 23:41:54.543974 139674595104768 gin_utils.py:85] from t5x.contrib.moe import partitioning as moe_partitioning
I0512 23:41:54.544007 139674595104768 gin_utils.py:85] from t5x.contrib.moe import trainer as moe_trainer
I0512 23:41:54.544039 139674595104768 gin_utils.py:85] from t5x import gin_utils
I0512 23:41:54.544071 139674595104768 gin_utils.py:85] from t5x import partitioning
I0512 23:41:54.544103 139674595104768 gin_utils.py:85] from t5x import trainer
I0512 23:41:54.544136 139674595104768 gin_utils.py:85] from t5x import utils
I0512 23:41:54.544168 139674595104768 gin_utils.py:85] 
I0512 23:41:54.544201 139674595104768 gin_utils.py:85] # Macros:
I0512 23:41:54.544234 139674595104768 gin_utils.py:85] # ==============================================================================
I0512 23:41:54.544266 139674595104768 gin_utils.py:85] ACTIVATION_DTYPE = 'bfloat16'
I0512 23:41:54.544299 139674595104768 gin_utils.py:85] ACTIVATION_PARTITIONING_DIMS = 1
I0512 23:41:54.544331 139674595104768 gin_utils.py:85] ARCHITECTURE = @t5_architecture.DecoderOnly()
I0512 23:41:54.544364 139674595104768 gin_utils.py:85] AUX_LOSS_FACTOR = 0.01
I0512 23:41:54.544396 139674595104768 gin_utils.py:85] BATCH_SIZE = 384
I0512 23:41:54.544429 139674595104768 gin_utils.py:85] BIAS_INIT = @bias_init/linen.initializers.normal()
I0512 23:41:54.544461 139674595104768 gin_utils.py:85] DECODER_SPARSE_LAYOUT = %flaxformer.architectures.moe.moe_enums.LayerLayout.MIXED
I0512 23:41:54.544494 139674595104768 gin_utils.py:85] DROPOUT_FACTORY = @dropout_factory/linen.Dropout
I0512 23:41:54.544526 139674595104768 gin_utils.py:85] DROPOUT_RATE = 0.0
I0512 23:41:54.544559 139674595104768 gin_utils.py:85] EMBED_DIM = 2048
I0512 23:41:54.544591 139674595104768 gin_utils.py:85] EVAL_EXPERT_CAPACITY_FACTOR = 2.0
I0512 23:41:54.544623 139674595104768 gin_utils.py:85] EXPERT_DROPOUT_RATE = %DROPOUT_RATE
I0512 23:41:54.544661 139674595104768 gin_utils.py:85] EXPERT_MLP_DIM = %MLP_DIM
I0512 23:41:54.544695 139674595104768 gin_utils.py:85] GROUP_SIZE = 4096
I0512 23:41:54.544728 139674595104768 gin_utils.py:85] HEAD_DIM = 128
I0512 23:41:54.544760 139674595104768 gin_utils.py:85] JITTER_NOISE = 0.0
I0512 23:41:54.544793 139674595104768 gin_utils.py:85] LABEL_SMOOTHING = 0.0
I0512 23:41:54.544825 139674595104768 gin_utils.py:85] LOSS_NORMALIZING_FACTOR = 'NUM_REAL_TARGET_TOKENS'
I0512 23:41:54.544858 139674595104768 gin_utils.py:85] MIXTURE_OR_TASK_MODULE = None
I0512 23:41:54.544891 139674595104768 gin_utils.py:85] MIXTURE_OR_TASK_NAME = 'mix_full_lm_test'
I0512 23:41:54.544925 139674595104768 gin_utils.py:85] MLP_DIM = 8192
I0512 23:41:54.544959 139674595104768 gin_utils.py:85] MODEL = @models.MoeDecoderOnlyModel()
I0512 23:41:54.544992 139674595104768 gin_utils.py:85] MODEL_DIR = 'gs://rosinality-tpu-bucket/openmoe_8b/training'
I0512 23:41:54.545025 139674595104768 gin_utils.py:85] MODEL_PARALLEL_SUBMESH = None
I0512 23:41:54.545057 139674595104768 gin_utils.py:85] MOE_TRUNCATED_DTYPE = 'bfloat16'
I0512 23:41:54.545089 139674595104768 gin_utils.py:85] NUM_DECODER_LAYERS = 24
I0512 23:41:54.545122 139674595104768 gin_utils.py:85] NUM_DECODER_SPARSE_LAYERS = 4
I0512 23:41:54.545154 139674595104768 gin_utils.py:85] NUM_EMBEDDINGS = 256384
I0512 23:41:54.545186 139674595104768 gin_utils.py:85] NUM_EXPERT_PARTITIONS = 8
I0512 23:41:54.545219 139674595104768 gin_utils.py:85] NUM_EXPERTS = 8
I0512 23:41:54.545251 139674595104768 gin_utils.py:85] NUM_HEADS = 24
I0512 23:41:54.545284 139674595104768 gin_utils.py:85] NUM_MODEL_PARTITIONS = 4
I0512 23:41:54.545316 139674595104768 gin_utils.py:85] NUM_SELECTED_EXPERTS = 2
I0512 23:41:54.545348 139674595104768 gin_utils.py:85] OPTIMIZER = @adafactor.Adafactor()
I0512 23:41:54.545380 139674595104768 gin_utils.py:85] RANDOM_SEED = None
I0512 23:41:54.545413 139674595104768 gin_utils.py:85] ROUTER_Z_LOSS_FACTOR = 0.0001
I0512 23:41:54.545445 139674595104768 gin_utils.py:85] SCALE = 0.1
I0512 23:41:54.545477 139674595104768 gin_utils.py:85] SHUFFLE_TRAIN_EXAMPLES = True
I0512 23:41:54.545509 139674595104768 gin_utils.py:85] TASK_FEATURE_LENGTHS = {'targets': 2048}
I0512 23:41:54.545542 139674595104768 gin_utils.py:85] TRAIN_EXPERT_CAPACITY_FACTOR = 1.25
I0512 23:41:54.545574 139674595104768 gin_utils.py:85] TRAIN_STEPS = 500000
I0512 23:41:54.545606 139674595104768 gin_utils.py:85] USE_CACHED_TASKS = False
I0512 23:41:54.545638 139674595104768 gin_utils.py:85] USE_HARDWARE_RNG = False
I0512 23:41:54.545676 139674595104768 gin_utils.py:85] VOCABULARY = @seqio.SentencePieceVocabulary()
I0512 23:41:54.545709 139674595104768 gin_utils.py:85] Z_LOSS = 0.0001
I0512 23:41:54.545742 139674595104768 gin_utils.py:85] 
I0512 23:41:54.545774 139674595104768 gin_utils.py:85] # Parameters for adafactor.Adafactor:
I0512 23:41:54.545806 139674595104768 gin_utils.py:85] # ==============================================================================
I0512 23:41:54.545838 139674595104768 gin_utils.py:85] adafactor.Adafactor.decay_rate = 0.8
I0512 23:41:54.545870 139674595104768 gin_utils.py:85] adafactor.Adafactor.logical_factor_rules = @adafactor_utils.logical_factor_rules()
I0512 23:41:54.545901 139674595104768 gin_utils.py:85] adafactor.Adafactor.step_offset = 0
I0512 23:41:54.545936 139674595104768 gin_utils.py:85] 
I0512 23:41:54.585335 140095893518336 resource_reader.py:50] system_path_file_exists:t5x/contrib/moe/configs/runs/pretrain.gin
I0512 23:41:54.535775 140339465779200 resource_reader.py:37] gin-config opened resource file:/home/rosinality/t5x/t5x/contrib/moe/configs/runs/pretrain.gin
I0512 23:41:54.536597 140339465779200 resource_reader.py:50] system_path_file_exists:t5x/configs/runs/pretrain.gin
I0512 23:41:54.536918 140339465779200 resource_reader.py:37] gin-config opened resource file:/home/rosinality/t5x/t5x/configs/runs/pretrain.gin
I0512 23:41:54.550560 140339465779200 gin_utils.py:83] Gin Configuration:
I0512 23:41:54.566463 140339465779200 gin_utils.py:85] from __gin__ import dynamic_registration
I0512 23:41:54.566532 140339465779200 gin_utils.py:85] import __main__ as train_script
I0512 23:41:54.566574 140339465779200 gin_utils.py:85] from flax import linen
I0512 23:41:54.566609 140339465779200 gin_utils.py:85] import flaxformer
I0512 23:41:54.566646 140339465779200 gin_utils.py:85] from flaxformer.architectures.moe import moe_architecture
I0512 23:41:54.566680 140339465779200 gin_utils.py:85] from flaxformer.architectures.moe import moe_enums
I0512 23:41:54.566713 140339465779200 gin_utils.py:85] from flaxformer.architectures.moe import moe_layers
I0512 23:41:54.566746 140339465779200 gin_utils.py:85] from flaxformer.architectures.moe import routing
I0512 23:41:54.566779 140339465779200 gin_utils.py:85] from flaxformer.architectures.t5 import t5_architecture
I0512 23:41:54.566811 140339465779200 gin_utils.py:85] from flaxformer.components.attention import dense_attention
I0512 23:41:54.566844 140339465779200 gin_utils.py:85] from flaxformer.components.attention import memory_efficient_attention
I0512 23:41:54.566876 140339465779200 gin_utils.py:85] from flaxformer.components import dense
I0512 23:41:54.566909 140339465779200 gin_utils.py:85] from flaxformer.components import embedding
I0512 23:41:54.566942 140339465779200 gin_utils.py:85] from flaxformer.components import layer_norm
I0512 23:41:54.566974 140339465779200 gin_utils.py:85] from gin import config
I0512 23:41:54.567007 140339465779200 gin_utils.py:85] import seqio
I0512 23:41:54.567039 140339465779200 gin_utils.py:85] import t5.data.mixtures
I0512 23:41:54.567071 140339465779200 gin_utils.py:85] from t5x import adafactor
I0512 23:41:54.567103 140339465779200 gin_utils.py:85] from t5x.contrib.moe import adafactor_utils
I0512 23:41:54.567136 140339465779200 gin_utils.py:85] from t5x.contrib.moe import models
I0512 23:41:54.567168 140339465779200 gin_utils.py:85] from t5x.contrib.moe import partitioning as moe_partitioning
I0512 23:41:54.567201 140339465779200 gin_utils.py:85] from t5x.contrib.moe import trainer as moe_trainer
I0512 23:41:54.567234 140339465779200 gin_utils.py:85] from t5x import gin_utils
I0512 23:41:54.567266 140339465779200 gin_utils.py:85] from t5x import partitioning
I0512 23:41:54.567298 140339465779200 gin_utils.py:85] from t5x import trainer
I0512 23:41:54.567331 140339465779200 gin_utils.py:85] from t5x import utils
I0512 23:41:54.567369 140339465779200 gin_utils.py:85] 
I0512 23:41:54.567404 140339465779200 gin_utils.py:85] # Macros:
I0512 23:41:54.567437 140339465779200 gin_utils.py:85] # ==============================================================================
I0512 23:41:54.567470 140339465779200 gin_utils.py:85] ACTIVATION_DTYPE = 'bfloat16'
I0512 23:41:54.567502 140339465779200 gin_utils.py:85] ACTIVATION_PARTITIONING_DIMS = 1
I0512 23:41:54.567534 140339465779200 gin_utils.py:85] ARCHITECTURE = @t5_architecture.DecoderOnly()
I0512 23:41:54.567567 140339465779200 gin_utils.py:85] AUX_LOSS_FACTOR = 0.01
I0512 23:41:54.567600 140339465779200 gin_utils.py:85] BATCH_SIZE = 384
I0512 23:41:54.567635 140339465779200 gin_utils.py:85] BIAS_INIT = @bias_init/linen.initializers.normal()
I0512 23:41:54.567668 140339465779200 gin_utils.py:85] DECODER_SPARSE_LAYOUT = %flaxformer.architectures.moe.moe_enums.LayerLayout.MIXED
I0512 23:41:54.567700 140339465779200 gin_utils.py:85] DROPOUT_FACTORY = @dropout_factory/linen.Dropout
I0512 23:41:54.567733 140339465779200 gin_utils.py:85] DROPOUT_RATE = 0.0
I0512 23:41:54.567765 140339465779200 gin_utils.py:85] EMBED_DIM = 2048
I0512 23:41:54.567797 140339465779200 gin_utils.py:85] EVAL_EXPERT_CAPACITY_FACTOR = 2.0
I0512 23:41:54.567830 140339465779200 gin_utils.py:85] EXPERT_DROPOUT_RATE = %DROPOUT_RATE
I0512 23:41:54.567862 140339465779200 gin_utils.py:85] EXPERT_MLP_DIM = %MLP_DIM
I0512 23:41:54.567894 140339465779200 gin_utils.py:85] GROUP_SIZE = 4096
I0512 23:41:54.567927 140339465779200 gin_utils.py:85] HEAD_DIM = 128
I0512 23:41:54.567959 140339465779200 gin_utils.py:85] JITTER_NOISE = 0.0
I0512 23:41:54.567991 140339465779200 gin_utils.py:85] LABEL_SMOOTHING = 0.0
I0512 23:41:54.568024 140339465779200 gin_utils.py:85] LOSS_NORMALIZING_FACTOR = 'NUM_REAL_TARGET_TOKENS'
I0512 23:41:54.568056 140339465779200 gin_utils.py:85] MIXTURE_OR_TASK_MODULE = None
I0512 23:41:54.568088 140339465779200 gin_utils.py:85] MIXTURE_OR_TASK_NAME = 'mix_full_lm_test'
I0512 23:41:54.568121 140339465779200 gin_utils.py:85] MLP_DIM = 8192
I0512 23:41:54.568153 140339465779200 gin_utils.py:85] MODEL = @models.MoeDecoderOnlyModel()
I0512 23:41:54.568210 140339465779200 gin_utils.py:85] MODEL_DIR = 'gs://rosinality-tpu-bucket/openmoe_8b/training'
I0512 23:41:54.568246 140339465779200 gin_utils.py:85] MODEL_PARALLEL_SUBMESH = None
I0512 23:41:54.568279 140339465779200 gin_utils.py:85] MOE_TRUNCATED_DTYPE = 'bfloat16'
I0512 23:41:54.568311 140339465779200 gin_utils.py:85] NUM_DECODER_LAYERS = 24
I0512 23:41:54.568343 140339465779200 gin_utils.py:85] NUM_DECODER_SPARSE_LAYERS = 4
I0512 23:41:54.568382 140339465779200 gin_utils.py:85] NUM_EMBEDDINGS = 256384
I0512 23:41:54.568415 140339465779200 gin_utils.py:85] NUM_EXPERT_PARTITIONS = 8
I0512 23:41:54.568448 140339465779200 gin_utils.py:85] NUM_EXPERTS = 8
I0512 23:41:54.568480 140339465779200 gin_utils.py:85] NUM_HEADS = 24
I0512 23:41:54.568512 140339465779200 gin_utils.py:85] NUM_MODEL_PARTITIONS = 4
I0512 23:41:54.568545 140339465779200 gin_utils.py:85] NUM_SELECTED_EXPERTS = 2
I0512 23:41:54.568577 140339465779200 gin_utils.py:85] OPTIMIZER = @adafactor.Adafactor()
I0512 23:41:54.568609 140339465779200 gin_utils.py:85] RANDOM_SEED = None
I0512 23:41:54.568645 140339465779200 gin_utils.py:85] ROUTER_Z_LOSS_FACTOR = 0.0001
I0512 23:41:54.568678 140339465779200 gin_utils.py:85] SCALE = 0.1
I0512 23:41:54.568711 140339465779200 gin_utils.py:85] SHUFFLE_TRAIN_EXAMPLES = True
I0512 23:41:54.568743 140339465779200 gin_utils.py:85] TASK_FEATURE_LENGTHS = {'targets': 2048}
I0512 23:41:54.568775 140339465779200 gin_utils.py:85] TRAIN_EXPERT_CAPACITY_FACTOR = 1.25
I0512 23:41:54.568808 140339465779200 gin_utils.py:85] TRAIN_STEPS = 500000
I0512 23:41:54.568840 140339465779200 gin_utils.py:85] USE_CACHED_TASKS = False
I0512 23:41:54.568873 140339465779200 gin_utils.py:85] USE_HARDWARE_RNG = False
I0512 23:41:54.568905 140339465779200 gin_utils.py:85] VOCABULARY = @seqio.SentencePieceVocabulary()
I0512 23:41:54.568937 140339465779200 gin_utils.py:85] Z_LOSS = 0.0001
I0512 23:41:54.568969 140339465779200 gin_utils.py:85] 
I0512 23:41:54.569002 140339465779200 gin_utils.py:85] # Parameters for adafactor.Adafactor:
I0512 23:41:54.569034 140339465779200 gin_utils.py:85] # ==============================================================================
I0512 23:41:54.569066 140339465779200 gin_utils.py:85] adafactor.Adafactor.decay_rate = 0.8
I0512 23:41:54.569099 140339465779200 gin_utils.py:85] adafactor.Adafactor.logical_factor_rules = @adafactor_utils.logical_factor_rules()
I0512 23:41:54.569131 140339465779200 gin_utils.py:85] adafactor.Adafactor.step_offset = 0
I0512 23:41:54.569163 140339465779200 gin_utils.py:85] 
I0512 23:41:54.527623 140031916288000 gin_utils.py:85] # Parameters for utils.CheckpointConfig:
I0512 23:41:54.527654 140031916288000 gin_utils.py:85] # ==============================================================================
I0512 23:41:54.527691 140031916288000 gin_utils.py:85] utils.CheckpointConfig.restore = @utils.RestoreCheckpointConfig()
I0512 23:41:54.527722 140031916288000 gin_utils.py:85] utils.CheckpointConfig.save = @utils.SaveCheckpointConfig()
I0512 23:41:54.527753 140031916288000 gin_utils.py:85] 
I0512 23:41:54.527784 140031916288000 gin_utils.py:85] # Parameters for moe_partitioning.compute_num_model_partitions:
I0512 23:41:54.527815 140031916288000 gin_utils.py:85] # ==============================================================================
I0512 23:41:54.527845 140031916288000 gin_utils.py:85] moe_partitioning.compute_num_model_partitions.model_parallel_submesh = \
I0512 23:41:54.527876 140031916288000 gin_utils.py:85]     %MODEL_PARALLEL_SUBMESH
I0512 23:41:54.527907 140031916288000 gin_utils.py:85] moe_partitioning.compute_num_model_partitions.num_model_partitions = \
I0512 23:41:54.527940 140031916288000 gin_utils.py:85]     %NUM_MODEL_PARTITIONS
I0512 23:41:54.527972 140031916288000 gin_utils.py:85] 
I0512 23:41:54.528003 140031916288000 gin_utils.py:85] # Parameters for utils.create_learning_rate_scheduler:
I0512 23:41:54.528034 140031916288000 gin_utils.py:85] # ==============================================================================
I0512 23:41:54.528064 140031916288000 gin_utils.py:85] utils.create_learning_rate_scheduler.base_learning_rate = 1.0
I0512 23:41:54.528095 140031916288000 gin_utils.py:85] utils.create_learning_rate_scheduler.factors = 'constant * rsqrt_decay'
I0512 23:41:54.528126 140031916288000 gin_utils.py:85] utils.create_learning_rate_scheduler.warmup_steps = 10000
I0512 23:41:54.528157 140031916288000 gin_utils.py:85] 
I0512 23:41:54.528188 140031916288000 gin_utils.py:85] # Parameters for train/utils.DatasetConfig:
I0512 23:41:54.528219 140031916288000 gin_utils.py:85] # ==============================================================================
I0512 23:41:54.528250 140031916288000 gin_utils.py:85] train/utils.DatasetConfig.batch_size = %BATCH_SIZE
I0512 23:41:54.528280 140031916288000 gin_utils.py:85] train/utils.DatasetConfig.mixture_or_task_name = %MIXTURE_OR_TASK_NAME
I0512 23:41:54.528311 140031916288000 gin_utils.py:85] train/utils.DatasetConfig.module = %MIXTURE_OR_TASK_MODULE
I0512 23:41:54.528342 140031916288000 gin_utils.py:85] train/utils.DatasetConfig.pack = True
I0512 23:41:54.528372 140031916288000 gin_utils.py:85] train/utils.DatasetConfig.seed = None
I0512 23:41:54.528403 140031916288000 gin_utils.py:85] train/utils.DatasetConfig.shuffle = %SHUFFLE_TRAIN_EXAMPLES
I0512 23:41:54.528434 140031916288000 gin_utils.py:85] train/utils.DatasetConfig.split = 'train'
I0512 23:41:54.528464 140031916288000 gin_utils.py:85] train/utils.DatasetConfig.task_feature_lengths = %TASK_FEATURE_LENGTHS
I0512 23:41:54.528495 140031916288000 gin_utils.py:85] train/utils.DatasetConfig.use_cached = %USE_CACHED_TASKS
I0512 23:41:54.528526 140031916288000 gin_utils.py:85] 
I0512 23:41:54.528556 140031916288000 gin_utils.py:85] # Parameters for train_eval/utils.DatasetConfig:
I0512 23:41:54.528587 140031916288000 gin_utils.py:85] # ==============================================================================
I0512 23:41:54.528618 140031916288000 gin_utils.py:85] train_eval/utils.DatasetConfig.batch_size = 128
I0512 23:41:54.528648 140031916288000 gin_utils.py:85] train_eval/utils.DatasetConfig.mixture_or_task_name = %MIXTURE_OR_TASK_NAME
I0512 23:41:54.528685 140031916288000 gin_utils.py:85] train_eval/utils.DatasetConfig.module = %MIXTURE_OR_TASK_MODULE
I0512 23:41:54.528716 140031916288000 gin_utils.py:85] train_eval/utils.DatasetConfig.pack = True
I0512 23:41:54.528747 140031916288000 gin_utils.py:85] train_eval/utils.DatasetConfig.seed = 42
I0512 23:41:54.528778 140031916288000 gin_utils.py:85] train_eval/utils.DatasetConfig.shuffle = False
I0512 23:41:54.528809 140031916288000 gin_utils.py:85] train_eval/utils.DatasetConfig.split = 'validation'
I0512 23:41:54.528839 140031916288000 gin_utils.py:85] train_eval/utils.DatasetConfig.task_feature_lengths = %TASK_FEATURE_LENGTHS
I0512 23:41:54.528870 140031916288000 gin_utils.py:85] train_eval/utils.DatasetConfig.use_cached = %USE_CACHED_TASKS
I0512 23:41:54.528900 140031916288000 gin_utils.py:85] 
I0512 23:41:54.528934 140031916288000 gin_utils.py:85] # Parameters for t5_architecture.DecoderLayer:
I0512 23:41:54.528966 140031916288000 gin_utils.py:85] # ==============================================================================
I0512 23:41:54.528997 140031916288000 gin_utils.py:85] t5_architecture.DecoderLayer.activation_partitioning_dims = \
I0512 23:41:54.529028 140031916288000 gin_utils.py:85]     %ACTIVATION_PARTITIONING_DIMS
I0512 23:41:54.529059 140031916288000 gin_utils.py:85] t5_architecture.DecoderLayer.dropout_factory = %DROPOUT_FACTORY
I0512 23:41:54.529090 140031916288000 gin_utils.py:85] t5_architecture.DecoderLayer.encoder_decoder_attention = None
I0512 23:41:54.529120 140031916288000 gin_utils.py:85] t5_architecture.DecoderLayer.layer_norm_factory = @layer_norm.T5LayerNorm
I0512 23:41:54.529151 140031916288000 gin_utils.py:85] t5_architecture.DecoderLayer.mlp = @dense.MlpBlock()
I0512 23:41:54.529182 140031916288000 gin_utils.py:85] t5_architecture.DecoderLayer.scanned = False
I0512 23:41:54.529213 140031916288000 gin_utils.py:85] t5_architecture.DecoderLayer.self_attention = \
I0512 23:41:54.529244 140031916288000 gin_utils.py:85]     @dense_attention.MultiHeadDotProductAttention()
I0512 23:41:54.529274 140031916288000 gin_utils.py:85] 
I0512 23:41:54.529305 140031916288000 gin_utils.py:85] # Parameters for t5_architecture.DecoderOnly:
I0512 23:41:54.529336 140031916288000 gin_utils.py:85] # ==============================================================================
I0512 23:41:54.529367 140031916288000 gin_utils.py:85] t5_architecture.DecoderOnly.decoder_factory = @moe_architecture.SparseDecoder
I0512 23:41:54.529397 140031916288000 gin_utils.py:85] t5_architecture.DecoderOnly.dtype = %ACTIVATION_DTYPE
I0512 23:41:54.529428 140031916288000 gin_utils.py:85] t5_architecture.DecoderOnly.shared_token_embedder_factory = @embedding.Embed
I0512 23:41:54.529459 140031916288000 gin_utils.py:85] 
I0512 23:41:54.529490 140031916288000 gin_utils.py:85] # Parameters for output_logits/dense.DenseGeneral:
I0512 23:41:54.529521 140031916288000 gin_utils.py:85] # ==============================================================================
I0512 23:41:54.529552 140031916288000 gin_utils.py:85] output_logits/dense.DenseGeneral.bias_init = %BIAS_INIT
I0512 23:41:54.529583 140031916288000 gin_utils.py:85] output_logits/dense.DenseGeneral.dtype = 'float32'
I0512 23:41:54.529613 140031916288000 gin_utils.py:85] output_logits/dense.DenseGeneral.features = %NUM_EMBEDDINGS
I0512 23:41:54.529644 140031916288000 gin_utils.py:85] output_logits/dense.DenseGeneral.kernel_axis_names = ['embed', 'vocab']
I0512 23:41:54.529681 140031916288000 gin_utils.py:85] output_logits/dense.DenseGeneral.kernel_init = \
I0512 23:41:54.529713 140031916288000 gin_utils.py:85]     @output_logits_kernel_init/linen.initializers.variance_scaling()
I0512 23:41:54.529744 140031916288000 gin_utils.py:85] output_logits/dense.DenseGeneral.use_bias = False
I0512 23:41:54.529778 140031916288000 gin_utils.py:85] 
I0512 23:41:54.529811 140031916288000 gin_utils.py:85] # Parameters for dropout_factory/linen.Dropout:
I0512 23:41:54.529842 140031916288000 gin_utils.py:85] # ==============================================================================
I0512 23:41:54.529873 140031916288000 gin_utils.py:85] dropout_factory/linen.Dropout.broadcast_dims = (-2,)
I0512 23:41:54.529903 140031916288000 gin_utils.py:85] dropout_factory/linen.Dropout.rate = %DROPOUT_RATE
I0512 23:41:54.529936 140031916288000 gin_utils.py:85] 
I0512 23:41:54.529967 140031916288000 gin_utils.py:85] # Parameters for embedding.Embed:
I0512 23:41:54.529998 140031916288000 gin_utils.py:85] # ==============================================================================
I0512 23:41:54.530029 140031916288000 gin_utils.py:85] embedding.Embed.attend_dtype = 'float32'
I0512 23:41:54.530060 140031916288000 gin_utils.py:85] embedding.Embed.cast_input_dtype = 'int32'
I0512 23:41:54.530111 140031916288000 gin_utils.py:85] embedding.Embed.dtype = %ACTIVATION_DTYPE
I0512 23:41:54.530146 140031916288000 gin_utils.py:85] embedding.Embed.embedding_init = @token_embedder_init/linen.initializers.normal()
I0512 23:41:54.530177 140031916288000 gin_utils.py:85] embedding.Embed.features = %EMBED_DIM
I0512 23:41:54.530208 140031916288000 gin_utils.py:85] embedding.Embed.name = 'token_embedder'
I0512 23:41:54.530239 140031916288000 gin_utils.py:85] embedding.Embed.num_embeddings = %NUM_EMBEDDINGS
I0512 23:41:54.530269 140031916288000 gin_utils.py:85] embedding.Embed.one_hot = True
I0512 23:41:54.530300 140031916288000 gin_utils.py:85] 
I0512 23:41:54.530331 140031916288000 gin_utils.py:85] # Parameters for dense.MlpBlock:
I0512 23:41:54.530362 140031916288000 gin_utils.py:85] # ==============================================================================
I0512 23:41:54.530393 140031916288000 gin_utils.py:85] dense.MlpBlock.activations = ('swiglu', 'linear')
I0512 23:41:54.530424 140031916288000 gin_utils.py:85] dense.MlpBlock.bias_init = %BIAS_INIT
I0512 23:41:54.530455 140031916288000 gin_utils.py:85] dense.MlpBlock.dtype = %ACTIVATION_DTYPE
I0512 23:41:54.530486 140031916288000 gin_utils.py:85] dense.MlpBlock.final_dropout_rate = 0
I0512 23:41:54.530516 140031916288000 gin_utils.py:85] dense.MlpBlock.input_axis_name = 'mlp_embed'
I0512 23:41:54.530547 140031916288000 gin_utils.py:85] dense.MlpBlock.intermediate_dim = %MLP_DIM
I0512 23:41:54.530577 140031916288000 gin_utils.py:85] dense.MlpBlock.intermediate_dropout_rate = %DROPOUT_RATE
I0512 23:41:54.545968 139674595104768 gin_utils.py:85] # Parameters for utils.CheckpointConfig:
I0512 23:41:54.546000 139674595104768 gin_utils.py:85] # ==============================================================================
I0512 23:41:54.546032 139674595104768 gin_utils.py:85] utils.CheckpointConfig.restore = @utils.RestoreCheckpointConfig()
I0512 23:41:54.546064 139674595104768 gin_utils.py:85] utils.CheckpointConfig.save = @utils.SaveCheckpointConfig()
I0512 23:41:54.546096 139674595104768 gin_utils.py:85] 
I0512 23:41:54.546128 139674595104768 gin_utils.py:85] # Parameters for moe_partitioning.compute_num_model_partitions:
I0512 23:41:54.546159 139674595104768 gin_utils.py:85] # ==============================================================================
I0512 23:41:54.546191 139674595104768 gin_utils.py:85] moe_partitioning.compute_num_model_partitions.model_parallel_submesh = \
I0512 23:41:54.546228 139674595104768 gin_utils.py:85]     %MODEL_PARALLEL_SUBMESH
I0512 23:41:54.546261 139674595104768 gin_utils.py:85] moe_partitioning.compute_num_model_partitions.num_model_partitions = \
I0512 23:41:54.546293 139674595104768 gin_utils.py:85]     %NUM_MODEL_PARTITIONS
I0512 23:41:54.546325 139674595104768 gin_utils.py:85] 
I0512 23:41:54.546357 139674595104768 gin_utils.py:85] # Parameters for utils.create_learning_rate_scheduler:
I0512 23:41:54.546389 139674595104768 gin_utils.py:85] # ==============================================================================
I0512 23:41:54.546421 139674595104768 gin_utils.py:85] utils.create_learning_rate_scheduler.base_learning_rate = 1.0
I0512 23:41:54.546453 139674595104768 gin_utils.py:85] utils.create_learning_rate_scheduler.factors = 'constant * rsqrt_decay'
I0512 23:41:54.546484 139674595104768 gin_utils.py:85] utils.create_learning_rate_scheduler.warmup_steps = 10000
I0512 23:41:54.546516 139674595104768 gin_utils.py:85] 
I0512 23:41:54.546547 139674595104768 gin_utils.py:85] # Parameters for train/utils.DatasetConfig:
I0512 23:41:54.546579 139674595104768 gin_utils.py:85] # ==============================================================================
I0512 23:41:54.546611 139674595104768 gin_utils.py:85] train/utils.DatasetConfig.batch_size = %BATCH_SIZE
I0512 23:41:54.546643 139674595104768 gin_utils.py:85] train/utils.DatasetConfig.mixture_or_task_name = %MIXTURE_OR_TASK_NAME
I0512 23:41:54.546681 139674595104768 gin_utils.py:85] train/utils.DatasetConfig.module = %MIXTURE_OR_TASK_MODULE
I0512 23:41:54.546713 139674595104768 gin_utils.py:85] train/utils.DatasetConfig.pack = True
I0512 23:41:54.546745 139674595104768 gin_utils.py:85] train/utils.DatasetConfig.seed = None
I0512 23:41:54.546777 139674595104768 gin_utils.py:85] train/utils.DatasetConfig.shuffle = %SHUFFLE_TRAIN_EXAMPLES
I0512 23:41:54.546808 139674595104768 gin_utils.py:85] train/utils.DatasetConfig.split = 'train'
I0512 23:41:54.546840 139674595104768 gin_utils.py:85] train/utils.DatasetConfig.task_feature_lengths = %TASK_FEATURE_LENGTHS
I0512 23:41:54.546872 139674595104768 gin_utils.py:85] train/utils.DatasetConfig.use_cached = %USE_CACHED_TASKS
I0512 23:41:54.546904 139674595104768 gin_utils.py:85] 
I0512 23:41:54.546938 139674595104768 gin_utils.py:85] # Parameters for train_eval/utils.DatasetConfig:
I0512 23:41:54.546971 139674595104768 gin_utils.py:85] # ==============================================================================
I0512 23:41:54.547003 139674595104768 gin_utils.py:85] train_eval/utils.DatasetConfig.batch_size = 128
I0512 23:41:54.547035 139674595104768 gin_utils.py:85] train_eval/utils.DatasetConfig.mixture_or_task_name = %MIXTURE_OR_TASK_NAME
I0512 23:41:54.547067 139674595104768 gin_utils.py:85] train_eval/utils.DatasetConfig.module = %MIXTURE_OR_TASK_MODULE
I0512 23:41:54.547099 139674595104768 gin_utils.py:85] train_eval/utils.DatasetConfig.pack = True
I0512 23:41:54.547131 139674595104768 gin_utils.py:85] train_eval/utils.DatasetConfig.seed = 42
I0512 23:41:54.547162 139674595104768 gin_utils.py:85] train_eval/utils.DatasetConfig.shuffle = False
I0512 23:41:54.547194 139674595104768 gin_utils.py:85] train_eval/utils.DatasetConfig.split = 'validation'
I0512 23:41:54.547226 139674595104768 gin_utils.py:85] train_eval/utils.DatasetConfig.task_feature_lengths = %TASK_FEATURE_LENGTHS
I0512 23:41:54.547258 139674595104768 gin_utils.py:85] train_eval/utils.DatasetConfig.use_cached = %USE_CACHED_TASKS
I0512 23:41:54.547316 139674595104768 gin_utils.py:85] 
I0512 23:41:54.547355 139674595104768 gin_utils.py:85] # Parameters for t5_architecture.DecoderLayer:
I0512 23:41:54.547388 139674595104768 gin_utils.py:85] # ==============================================================================
I0512 23:41:54.547420 139674595104768 gin_utils.py:85] t5_architecture.DecoderLayer.activation_partitioning_dims = \
I0512 23:41:54.547453 139674595104768 gin_utils.py:85]     %ACTIVATION_PARTITIONING_DIMS
I0512 23:41:54.547484 139674595104768 gin_utils.py:85] t5_architecture.DecoderLayer.dropout_factory = %DROPOUT_FACTORY
I0512 23:41:54.547516 139674595104768 gin_utils.py:85] t5_architecture.DecoderLayer.encoder_decoder_attention = None
I0512 23:41:54.547548 139674595104768 gin_utils.py:85] t5_architecture.DecoderLayer.layer_norm_factory = @layer_norm.T5LayerNorm
I0512 23:41:54.547580 139674595104768 gin_utils.py:85] t5_architecture.DecoderLayer.mlp = @dense.MlpBlock()
I0512 23:41:54.547612 139674595104768 gin_utils.py:85] t5_architecture.DecoderLayer.scanned = False
I0512 23:41:54.547644 139674595104768 gin_utils.py:85] t5_architecture.DecoderLayer.self_attention = \
I0512 23:41:54.547682 139674595104768 gin_utils.py:85]     @dense_attention.MultiHeadDotProductAttention()
I0512 23:41:54.547714 139674595104768 gin_utils.py:85] 
I0512 23:41:54.547746 139674595104768 gin_utils.py:85] # Parameters for t5_architecture.DecoderOnly:
I0512 23:41:54.547778 139674595104768 gin_utils.py:85] # ==============================================================================
I0512 23:41:54.547810 139674595104768 gin_utils.py:85] t5_architecture.DecoderOnly.decoder_factory = @moe_architecture.SparseDecoder
I0512 23:41:54.547842 139674595104768 gin_utils.py:85] t5_architecture.DecoderOnly.dtype = %ACTIVATION_DTYPE
I0512 23:41:54.547874 139674595104768 gin_utils.py:85] t5_architecture.DecoderOnly.shared_token_embedder_factory = @embedding.Embed
I0512 23:41:54.547906 139674595104768 gin_utils.py:85] 
I0512 23:41:54.547940 139674595104768 gin_utils.py:85] # Parameters for output_logits/dense.DenseGeneral:
I0512 23:41:54.547972 139674595104768 gin_utils.py:85] # ==============================================================================
I0512 23:41:54.548004 139674595104768 gin_utils.py:85] output_logits/dense.DenseGeneral.bias_init = %BIAS_INIT
I0512 23:41:54.548036 139674595104768 gin_utils.py:85] output_logits/dense.DenseGeneral.dtype = 'float32'
I0512 23:41:54.548068 139674595104768 gin_utils.py:85] output_logits/dense.DenseGeneral.features = %NUM_EMBEDDINGS
I0512 23:41:54.548099 139674595104768 gin_utils.py:85] output_logits/dense.DenseGeneral.kernel_axis_names = ['embed', 'vocab']
I0512 23:41:54.548131 139674595104768 gin_utils.py:85] output_logits/dense.DenseGeneral.kernel_init = \
I0512 23:41:54.548163 139674595104768 gin_utils.py:85]     @output_logits_kernel_init/linen.initializers.variance_scaling()
I0512 23:41:54.548195 139674595104768 gin_utils.py:85] output_logits/dense.DenseGeneral.use_bias = False
I0512 23:41:54.548227 139674595104768 gin_utils.py:85] 
I0512 23:41:54.548259 139674595104768 gin_utils.py:85] # Parameters for dropout_factory/linen.Dropout:
I0512 23:41:54.548290 139674595104768 gin_utils.py:85] # ==============================================================================
I0512 23:41:54.548322 139674595104768 gin_utils.py:85] dropout_factory/linen.Dropout.broadcast_dims = (-2,)
I0512 23:41:54.548354 139674595104768 gin_utils.py:85] dropout_factory/linen.Dropout.rate = %DROPOUT_RATE
I0512 23:41:54.548386 139674595104768 gin_utils.py:85] 
I0512 23:41:54.548418 139674595104768 gin_utils.py:85] # Parameters for embedding.Embed:
I0512 23:41:54.548449 139674595104768 gin_utils.py:85] # ==============================================================================
I0512 23:41:54.548481 139674595104768 gin_utils.py:85] embedding.Embed.attend_dtype = 'float32'
I0512 23:41:54.548513 139674595104768 gin_utils.py:85] embedding.Embed.cast_input_dtype = 'int32'
I0512 23:41:54.548545 139674595104768 gin_utils.py:85] embedding.Embed.dtype = %ACTIVATION_DTYPE
I0512 23:41:54.548576 139674595104768 gin_utils.py:85] embedding.Embed.embedding_init = @token_embedder_init/linen.initializers.normal()
I0512 23:41:54.548608 139674595104768 gin_utils.py:85] embedding.Embed.features = %EMBED_DIM
I0512 23:41:54.548640 139674595104768 gin_utils.py:85] embedding.Embed.name = 'token_embedder'
I0512 23:41:54.548678 139674595104768 gin_utils.py:85] embedding.Embed.num_embeddings = %NUM_EMBEDDINGS
I0512 23:41:54.548711 139674595104768 gin_utils.py:85] embedding.Embed.one_hot = True
I0512 23:41:54.548743 139674595104768 gin_utils.py:85] 
I0512 23:41:54.548775 139674595104768 gin_utils.py:85] # Parameters for dense.MlpBlock:
I0512 23:41:54.548806 139674595104768 gin_utils.py:85] # ==============================================================================
I0512 23:41:54.548839 139674595104768 gin_utils.py:85] dense.MlpBlock.activations = ('swiglu', 'linear')
I0512 23:41:54.548870 139674595104768 gin_utils.py:85] dense.MlpBlock.bias_init = %BIAS_INIT
I0512 23:41:54.548903 139674595104768 gin_utils.py:85] dense.MlpBlock.dtype = %ACTIVATION_DTYPE
I0512 23:41:54.548937 139674595104768 gin_utils.py:85] dense.MlpBlock.final_dropout_rate = 0
I0512 23:41:54.548970 139674595104768 gin_utils.py:85] dense.MlpBlock.input_axis_name = 'mlp_embed'
I0512 23:41:54.549002 139674595104768 gin_utils.py:85] dense.MlpBlock.intermediate_dim = %MLP_DIM
I0512 23:41:54.549034 139674595104768 gin_utils.py:85] dense.MlpBlock.intermediate_dropout_rate = %DROPOUT_RATE
I0512 23:41:54.570922 140440593786880 resource_reader.py:37] gin-config opened resource file:/home/rosinality/t5x/t5x/contrib/moe/configs/runs/pretrain.gin
I0512 23:41:54.571441 140440593786880 resource_reader.py:50] system_path_file_exists:t5x/configs/runs/pretrain.gin
I0512 23:41:54.571745 140440593786880 resource_reader.py:37] gin-config opened resource file:/home/rosinality/t5x/t5x/configs/runs/pretrain.gin
I0512 23:41:54.585614 140440593786880 gin_utils.py:83] Gin Configuration:
I0512 23:41:54.601499 140440593786880 gin_utils.py:85] from __gin__ import dynamic_registration
I0512 23:41:54.601570 140440593786880 gin_utils.py:85] import __main__ as train_script
I0512 23:41:54.601612 140440593786880 gin_utils.py:85] from flax import linen
I0512 23:41:54.601646 140440593786880 gin_utils.py:85] import flaxformer
I0512 23:41:54.601678 140440593786880 gin_utils.py:85] from flaxformer.architectures.moe import moe_architecture
I0512 23:41:54.601711 140440593786880 gin_utils.py:85] from flaxformer.architectures.moe import moe_enums
I0512 23:41:54.601743 140440593786880 gin_utils.py:85] from flaxformer.architectures.moe import moe_layers
I0512 23:41:54.601775 140440593786880 gin_utils.py:85] from flaxformer.architectures.moe import routing
I0512 23:41:54.601807 140440593786880 gin_utils.py:85] from flaxformer.architectures.t5 import t5_architecture
I0512 23:41:54.601838 140440593786880 gin_utils.py:85] from flaxformer.components.attention import dense_attention
I0512 23:41:54.601870 140440593786880 gin_utils.py:85] from flaxformer.components.attention import memory_efficient_attention
I0512 23:41:54.601901 140440593786880 gin_utils.py:85] from flaxformer.components import dense
I0512 23:41:54.601933 140440593786880 gin_utils.py:85] from flaxformer.components import embedding
I0512 23:41:54.601967 140440593786880 gin_utils.py:85] from flaxformer.components import layer_norm
I0512 23:41:54.602001 140440593786880 gin_utils.py:85] from gin import config
I0512 23:41:54.602032 140440593786880 gin_utils.py:85] import seqio
I0512 23:41:54.602064 140440593786880 gin_utils.py:85] import t5.data.mixtures
I0512 23:41:54.602096 140440593786880 gin_utils.py:85] from t5x import adafactor
I0512 23:41:54.602127 140440593786880 gin_utils.py:85] from t5x.contrib.moe import adafactor_utils
I0512 23:41:54.602159 140440593786880 gin_utils.py:85] from t5x.contrib.moe import models
I0512 23:41:54.602196 140440593786880 gin_utils.py:85] from t5x.contrib.moe import partitioning as moe_partitioning
I0512 23:41:54.602230 140440593786880 gin_utils.py:85] from t5x.contrib.moe import trainer as moe_trainer
I0512 23:41:54.602262 140440593786880 gin_utils.py:85] from t5x import gin_utils
I0512 23:41:54.602293 140440593786880 gin_utils.py:85] from t5x import partitioning
I0512 23:41:54.602325 140440593786880 gin_utils.py:85] from t5x import trainer
I0512 23:41:54.602356 140440593786880 gin_utils.py:85] from t5x import utils
I0512 23:41:54.602388 140440593786880 gin_utils.py:85] 
I0512 23:41:54.602419 140440593786880 gin_utils.py:85] # Macros:
I0512 23:41:54.602451 140440593786880 gin_utils.py:85] # ==============================================================================
I0512 23:41:54.602483 140440593786880 gin_utils.py:85] ACTIVATION_DTYPE = 'bfloat16'
I0512 23:41:54.602514 140440593786880 gin_utils.py:85] ACTIVATION_PARTITIONING_DIMS = 1
I0512 23:41:54.602545 140440593786880 gin_utils.py:85] ARCHITECTURE = @t5_architecture.DecoderOnly()
I0512 23:41:54.602576 140440593786880 gin_utils.py:85] AUX_LOSS_FACTOR = 0.01
I0512 23:41:54.602608 140440593786880 gin_utils.py:85] BATCH_SIZE = 384
I0512 23:41:54.602639 140440593786880 gin_utils.py:85] BIAS_INIT = @bias_init/linen.initializers.normal()
I0512 23:41:54.602671 140440593786880 gin_utils.py:85] DECODER_SPARSE_LAYOUT = %flaxformer.architectures.moe.moe_enums.LayerLayout.MIXED
I0512 23:41:54.602702 140440593786880 gin_utils.py:85] DROPOUT_FACTORY = @dropout_factory/linen.Dropout
I0512 23:41:54.602733 140440593786880 gin_utils.py:85] DROPOUT_RATE = 0.0
I0512 23:41:54.602765 140440593786880 gin_utils.py:85] EMBED_DIM = 2048
I0512 23:41:54.602796 140440593786880 gin_utils.py:85] EVAL_EXPERT_CAPACITY_FACTOR = 2.0
I0512 23:41:54.602828 140440593786880 gin_utils.py:85] EXPERT_DROPOUT_RATE = %DROPOUT_RATE
I0512 23:41:54.602859 140440593786880 gin_utils.py:85] EXPERT_MLP_DIM = %MLP_DIM
I0512 23:41:54.602890 140440593786880 gin_utils.py:85] GROUP_SIZE = 4096
I0512 23:41:54.602921 140440593786880 gin_utils.py:85] HEAD_DIM = 128
I0512 23:41:54.602954 140440593786880 gin_utils.py:85] JITTER_NOISE = 0.0
I0512 23:41:54.602986 140440593786880 gin_utils.py:85] LABEL_SMOOTHING = 0.0
I0512 23:41:54.603018 140440593786880 gin_utils.py:85] LOSS_NORMALIZING_FACTOR = 'NUM_REAL_TARGET_TOKENS'
I0512 23:41:54.603049 140440593786880 gin_utils.py:85] MIXTURE_OR_TASK_MODULE = None
I0512 23:41:54.603080 140440593786880 gin_utils.py:85] MIXTURE_OR_TASK_NAME = 'mix_full_lm_test'
I0512 23:41:54.603111 140440593786880 gin_utils.py:85] MLP_DIM = 8192
I0512 23:41:54.603143 140440593786880 gin_utils.py:85] MODEL = @models.MoeDecoderOnlyModel()
I0512 23:41:54.603174 140440593786880 gin_utils.py:85] MODEL_DIR = 'gs://rosinality-tpu-bucket/openmoe_8b/training'
I0512 23:41:54.603211 140440593786880 gin_utils.py:85] MODEL_PARALLEL_SUBMESH = None
I0512 23:41:54.603244 140440593786880 gin_utils.py:85] MOE_TRUNCATED_DTYPE = 'bfloat16'
I0512 23:41:54.603275 140440593786880 gin_utils.py:85] NUM_DECODER_LAYERS = 24
I0512 23:41:54.603306 140440593786880 gin_utils.py:85] NUM_DECODER_SPARSE_LAYERS = 4
I0512 23:41:54.603337 140440593786880 gin_utils.py:85] NUM_EMBEDDINGS = 256384
I0512 23:41:54.603368 140440593786880 gin_utils.py:85] NUM_EXPERT_PARTITIONS = 8
I0512 23:41:54.603399 140440593786880 gin_utils.py:85] NUM_EXPERTS = 8
I0512 23:41:54.603431 140440593786880 gin_utils.py:85] NUM_HEADS = 24
I0512 23:41:54.603461 140440593786880 gin_utils.py:85] NUM_MODEL_PARTITIONS = 4
I0512 23:41:54.603493 140440593786880 gin_utils.py:85] NUM_SELECTED_EXPERTS = 2
I0512 23:41:54.603524 140440593786880 gin_utils.py:85] OPTIMIZER = @adafactor.Adafactor()
I0512 23:41:54.603555 140440593786880 gin_utils.py:85] RANDOM_SEED = None
I0512 23:41:54.603586 140440593786880 gin_utils.py:85] ROUTER_Z_LOSS_FACTOR = 0.0001
I0512 23:41:54.603617 140440593786880 gin_utils.py:85] SCALE = 0.1
I0512 23:41:54.603648 140440593786880 gin_utils.py:85] SHUFFLE_TRAIN_EXAMPLES = True
I0512 23:41:54.603679 140440593786880 gin_utils.py:85] TASK_FEATURE_LENGTHS = {'targets': 2048}
I0512 23:41:54.603710 140440593786880 gin_utils.py:85] TRAIN_EXPERT_CAPACITY_FACTOR = 1.25
I0512 23:41:54.603741 140440593786880 gin_utils.py:85] TRAIN_STEPS = 500000
I0512 23:41:54.603772 140440593786880 gin_utils.py:85] USE_CACHED_TASKS = False
I0512 23:41:54.603803 140440593786880 gin_utils.py:85] USE_HARDWARE_RNG = False
I0512 23:41:54.603834 140440593786880 gin_utils.py:85] VOCABULARY = @seqio.SentencePieceVocabulary()
I0512 23:41:54.603865 140440593786880 gin_utils.py:85] Z_LOSS = 0.0001
I0512 23:41:54.603897 140440593786880 gin_utils.py:85] 
I0512 23:41:54.603928 140440593786880 gin_utils.py:85] # Parameters for adafactor.Adafactor:
I0512 23:41:54.603961 140440593786880 gin_utils.py:85] # ==============================================================================
I0512 23:41:54.603993 140440593786880 gin_utils.py:85] adafactor.Adafactor.decay_rate = 0.8
I0512 23:41:54.604025 140440593786880 gin_utils.py:85] adafactor.Adafactor.logical_factor_rules = @adafactor_utils.logical_factor_rules()
I0512 23:41:54.604056 140440593786880 gin_utils.py:85] adafactor.Adafactor.step_offset = 0
I0512 23:41:54.604088 140440593786880 gin_utils.py:85] 
I0512 23:41:54.585695 140095893518336 resource_reader.py:37] gin-config opened resource file:/home/rosinality/t5x/t5x/contrib/moe/configs/runs/pretrain.gin
I0512 23:41:54.586205 140095893518336 resource_reader.py:50] system_path_file_exists:t5x/configs/runs/pretrain.gin
I0512 23:41:54.586543 140095893518336 resource_reader.py:37] gin-config opened resource file:/home/rosinality/t5x/t5x/configs/runs/pretrain.gin
I0512 23:41:54.600228 140095893518336 gin_utils.py:83] Gin Configuration:
I0512 23:41:54.615979 140095893518336 gin_utils.py:85] from __gin__ import dynamic_registration
I0512 23:41:54.616048 140095893518336 gin_utils.py:85] import __main__ as train_script
I0512 23:41:54.616088 140095893518336 gin_utils.py:85] from flax import linen
I0512 23:41:54.616121 140095893518336 gin_utils.py:85] import flaxformer
I0512 23:41:54.616162 140095893518336 gin_utils.py:85] from flaxformer.architectures.moe import moe_architecture
I0512 23:41:54.616195 140095893518336 gin_utils.py:85] from flaxformer.architectures.moe import moe_enums
I0512 23:41:54.616227 140095893518336 gin_utils.py:85] from flaxformer.architectures.moe import moe_layers
I0512 23:41:54.616259 140095893518336 gin_utils.py:85] from flaxformer.architectures.moe import routing
I0512 23:41:54.616291 140095893518336 gin_utils.py:85] from flaxformer.architectures.t5 import t5_architecture
I0512 23:41:54.616322 140095893518336 gin_utils.py:85] from flaxformer.components.attention import dense_attention
I0512 23:41:54.616353 140095893518336 gin_utils.py:85] from flaxformer.components.attention import memory_efficient_attention
I0512 23:41:54.616386 140095893518336 gin_utils.py:85] from flaxformer.components import dense
I0512 23:41:54.616419 140095893518336 gin_utils.py:85] from flaxformer.components import embedding
I0512 23:41:54.616450 140095893518336 gin_utils.py:85] from flaxformer.components import layer_norm
I0512 23:41:54.616481 140095893518336 gin_utils.py:85] from gin import config
I0512 23:41:54.616513 140095893518336 gin_utils.py:85] import seqio
I0512 23:41:54.616544 140095893518336 gin_utils.py:85] import t5.data.mixtures
I0512 23:41:54.616575 140095893518336 gin_utils.py:85] from t5x import adafactor
I0512 23:41:54.616605 140095893518336 gin_utils.py:85] from t5x.contrib.moe import adafactor_utils
I0512 23:41:54.616636 140095893518336 gin_utils.py:85] from t5x.contrib.moe import models
I0512 23:41:54.616667 140095893518336 gin_utils.py:85] from t5x.contrib.moe import partitioning as moe_partitioning
I0512 23:41:54.616699 140095893518336 gin_utils.py:85] from t5x.contrib.moe import trainer as moe_trainer
I0512 23:41:54.616730 140095893518336 gin_utils.py:85] from t5x import gin_utils
I0512 23:41:54.616760 140095893518336 gin_utils.py:85] from t5x import partitioning
I0512 23:41:54.616791 140095893518336 gin_utils.py:85] from t5x import trainer
I0512 23:41:54.616822 140095893518336 gin_utils.py:85] from t5x import utils
I0512 23:41:54.616853 140095893518336 gin_utils.py:85] 
I0512 23:41:54.616885 140095893518336 gin_utils.py:85] # Macros:
I0512 23:41:54.616916 140095893518336 gin_utils.py:85] # ==============================================================================
I0512 23:41:54.616947 140095893518336 gin_utils.py:85] ACTIVATION_DTYPE = 'bfloat16'
I0512 23:41:54.616979 140095893518336 gin_utils.py:85] ACTIVATION_PARTITIONING_DIMS = 1
I0512 23:41:54.617009 140095893518336 gin_utils.py:85] ARCHITECTURE = @t5_architecture.DecoderOnly()
I0512 23:41:54.617040 140095893518336 gin_utils.py:85] AUX_LOSS_FACTOR = 0.01
I0512 23:41:54.617071 140095893518336 gin_utils.py:85] BATCH_SIZE = 384
I0512 23:41:54.617102 140095893518336 gin_utils.py:85] BIAS_INIT = @bias_init/linen.initializers.normal()
I0512 23:41:54.617139 140095893518336 gin_utils.py:85] DECODER_SPARSE_LAYOUT = %flaxformer.architectures.moe.moe_enums.LayerLayout.MIXED
I0512 23:41:54.617171 140095893518336 gin_utils.py:85] DROPOUT_FACTORY = @dropout_factory/linen.Dropout
I0512 23:41:54.617202 140095893518336 gin_utils.py:85] DROPOUT_RATE = 0.0
I0512 23:41:54.617233 140095893518336 gin_utils.py:85] EMBED_DIM = 2048
I0512 23:41:54.617264 140095893518336 gin_utils.py:85] EVAL_EXPERT_CAPACITY_FACTOR = 2.0
I0512 23:41:54.617295 140095893518336 gin_utils.py:85] EXPERT_DROPOUT_RATE = %DROPOUT_RATE
I0512 23:41:54.617325 140095893518336 gin_utils.py:85] EXPERT_MLP_DIM = %MLP_DIM
I0512 23:41:54.617356 140095893518336 gin_utils.py:85] GROUP_SIZE = 4096
I0512 23:41:54.617388 140095893518336 gin_utils.py:85] HEAD_DIM = 128
I0512 23:41:54.617420 140095893518336 gin_utils.py:85] JITTER_NOISE = 0.0
I0512 23:41:54.617451 140095893518336 gin_utils.py:85] LABEL_SMOOTHING = 0.0
I0512 23:41:54.617482 140095893518336 gin_utils.py:85] LOSS_NORMALIZING_FACTOR = 'NUM_REAL_TARGET_TOKENS'
I0512 23:41:54.617513 140095893518336 gin_utils.py:85] MIXTURE_OR_TASK_MODULE = None
I0512 23:41:54.617544 140095893518336 gin_utils.py:85] MIXTURE_OR_TASK_NAME = 'mix_full_lm_test'
I0512 23:41:54.617574 140095893518336 gin_utils.py:85] MLP_DIM = 8192
I0512 23:41:54.617605 140095893518336 gin_utils.py:85] MODEL = @models.MoeDecoderOnlyModel()
I0512 23:41:54.617636 140095893518336 gin_utils.py:85] MODEL_DIR = 'gs://rosinality-tpu-bucket/openmoe_8b/training'
I0512 23:41:54.617667 140095893518336 gin_utils.py:85] MODEL_PARALLEL_SUBMESH = None
I0512 23:41:54.617698 140095893518336 gin_utils.py:85] MOE_TRUNCATED_DTYPE = 'bfloat16'
I0512 23:41:54.617729 140095893518336 gin_utils.py:85] NUM_DECODER_LAYERS = 24
I0512 23:41:54.617759 140095893518336 gin_utils.py:85] NUM_DECODER_SPARSE_LAYERS = 4
I0512 23:41:54.617790 140095893518336 gin_utils.py:85] NUM_EMBEDDINGS = 256384
I0512 23:41:54.617821 140095893518336 gin_utils.py:85] NUM_EXPERT_PARTITIONS = 8
I0512 23:41:54.617852 140095893518336 gin_utils.py:85] NUM_EXPERTS = 8
I0512 23:41:54.617882 140095893518336 gin_utils.py:85] NUM_HEADS = 24
I0512 23:41:54.617913 140095893518336 gin_utils.py:85] NUM_MODEL_PARTITIONS = 4
I0512 23:41:54.617944 140095893518336 gin_utils.py:85] NUM_SELECTED_EXPERTS = 2
I0512 23:41:54.617975 140095893518336 gin_utils.py:85] OPTIMIZER = @adafactor.Adafactor()
I0512 23:41:54.618005 140095893518336 gin_utils.py:85] RANDOM_SEED = None
I0512 23:41:54.618036 140095893518336 gin_utils.py:85] ROUTER_Z_LOSS_FACTOR = 0.0001
I0512 23:41:54.618067 140095893518336 gin_utils.py:85] SCALE = 0.1
I0512 23:41:54.618098 140095893518336 gin_utils.py:85] SHUFFLE_TRAIN_EXAMPLES = True
I0512 23:41:54.618134 140095893518336 gin_utils.py:85] TASK_FEATURE_LENGTHS = {'targets': 2048}
I0512 23:41:54.618166 140095893518336 gin_utils.py:85] TRAIN_EXPERT_CAPACITY_FACTOR = 1.25
I0512 23:41:54.618197 140095893518336 gin_utils.py:85] TRAIN_STEPS = 500000
I0512 23:41:54.618228 140095893518336 gin_utils.py:85] USE_CACHED_TASKS = False
I0512 23:41:54.618277 140095893518336 gin_utils.py:85] USE_HARDWARE_RNG = False
I0512 23:41:54.618314 140095893518336 gin_utils.py:85] VOCABULARY = @seqio.SentencePieceVocabulary()
I0512 23:41:54.618347 140095893518336 gin_utils.py:85] Z_LOSS = 0.0001
I0512 23:41:54.618379 140095893518336 gin_utils.py:85] 
I0512 23:41:54.618412 140095893518336 gin_utils.py:85] # Parameters for adafactor.Adafactor:
I0512 23:41:54.618443 140095893518336 gin_utils.py:85] # ==============================================================================
I0512 23:41:54.618474 140095893518336 gin_utils.py:85] adafactor.Adafactor.decay_rate = 0.8
I0512 23:41:54.618505 140095893518336 gin_utils.py:85] adafactor.Adafactor.logical_factor_rules = @adafactor_utils.logical_factor_rules()
I0512 23:41:54.618536 140095893518336 gin_utils.py:85] adafactor.Adafactor.step_offset = 0
I0512 23:41:54.618568 140095893518336 gin_utils.py:85] 
I0512 23:41:54.569195 140339465779200 gin_utils.py:85] # Parameters for utils.CheckpointConfig:
I0512 23:41:54.569227 140339465779200 gin_utils.py:85] # ==============================================================================
I0512 23:41:54.569260 140339465779200 gin_utils.py:85] utils.CheckpointConfig.restore = @utils.RestoreCheckpointConfig()
I0512 23:41:54.569292 140339465779200 gin_utils.py:85] utils.CheckpointConfig.save = @utils.SaveCheckpointConfig()
I0512 23:41:54.569324 140339465779200 gin_utils.py:85] 
I0512 23:41:54.569362 140339465779200 gin_utils.py:85] # Parameters for moe_partitioning.compute_num_model_partitions:
I0512 23:41:54.569397 140339465779200 gin_utils.py:85] # ==============================================================================
I0512 23:41:54.569429 140339465779200 gin_utils.py:85] moe_partitioning.compute_num_model_partitions.model_parallel_submesh = \
I0512 23:41:54.569462 140339465779200 gin_utils.py:85]     %MODEL_PARALLEL_SUBMESH
I0512 23:41:54.569494 140339465779200 gin_utils.py:85] moe_partitioning.compute_num_model_partitions.num_model_partitions = \
I0512 23:41:54.569528 140339465779200 gin_utils.py:85]     %NUM_MODEL_PARTITIONS
I0512 23:41:54.569560 140339465779200 gin_utils.py:85] 
I0512 23:41:54.569593 140339465779200 gin_utils.py:85] # Parameters for utils.create_learning_rate_scheduler:
I0512 23:41:54.569632 140339465779200 gin_utils.py:85] # ==============================================================================
I0512 23:41:54.569666 140339465779200 gin_utils.py:85] utils.create_learning_rate_scheduler.base_learning_rate = 1.0
I0512 23:41:54.569698 140339465779200 gin_utils.py:85] utils.create_learning_rate_scheduler.factors = 'constant * rsqrt_decay'
I0512 23:41:54.569730 140339465779200 gin_utils.py:85] utils.create_learning_rate_scheduler.warmup_steps = 10000
I0512 23:41:54.569763 140339465779200 gin_utils.py:85] 
I0512 23:41:54.569795 140339465779200 gin_utils.py:85] # Parameters for train/utils.DatasetConfig:
I0512 23:41:54.569828 140339465779200 gin_utils.py:85] # ==============================================================================
I0512 23:41:54.569860 140339465779200 gin_utils.py:85] train/utils.DatasetConfig.batch_size = %BATCH_SIZE
I0512 23:41:54.569893 140339465779200 gin_utils.py:85] train/utils.DatasetConfig.mixture_or_task_name = %MIXTURE_OR_TASK_NAME
I0512 23:41:54.569926 140339465779200 gin_utils.py:85] train/utils.DatasetConfig.module = %MIXTURE_OR_TASK_MODULE
I0512 23:41:54.569958 140339465779200 gin_utils.py:85] train/utils.DatasetConfig.pack = True
I0512 23:41:54.569991 140339465779200 gin_utils.py:85] train/utils.DatasetConfig.seed = None
I0512 23:41:54.570023 140339465779200 gin_utils.py:85] train/utils.DatasetConfig.shuffle = %SHUFFLE_TRAIN_EXAMPLES
I0512 23:41:54.570056 140339465779200 gin_utils.py:85] train/utils.DatasetConfig.split = 'train'
I0512 23:41:54.570088 140339465779200 gin_utils.py:85] train/utils.DatasetConfig.task_feature_lengths = %TASK_FEATURE_LENGTHS
I0512 23:41:54.570121 140339465779200 gin_utils.py:85] train/utils.DatasetConfig.use_cached = %USE_CACHED_TASKS
I0512 23:41:54.570154 140339465779200 gin_utils.py:85] 
I0512 23:41:54.570185 140339465779200 gin_utils.py:85] # Parameters for train_eval/utils.DatasetConfig:
I0512 23:41:54.570218 140339465779200 gin_utils.py:85] # ==============================================================================
I0512 23:41:54.570250 140339465779200 gin_utils.py:85] train_eval/utils.DatasetConfig.batch_size = 128
I0512 23:41:54.570283 140339465779200 gin_utils.py:85] train_eval/utils.DatasetConfig.mixture_or_task_name = %MIXTURE_OR_TASK_NAME
I0512 23:41:54.570315 140339465779200 gin_utils.py:85] train_eval/utils.DatasetConfig.module = %MIXTURE_OR_TASK_MODULE
I0512 23:41:54.570348 140339465779200 gin_utils.py:85] train_eval/utils.DatasetConfig.pack = True
I0512 23:41:54.570387 140339465779200 gin_utils.py:85] train_eval/utils.DatasetConfig.seed = 42
I0512 23:41:54.570420 140339465779200 gin_utils.py:85] train_eval/utils.DatasetConfig.shuffle = False
I0512 23:41:54.570453 140339465779200 gin_utils.py:85] train_eval/utils.DatasetConfig.split = 'validation'
I0512 23:41:54.570485 140339465779200 gin_utils.py:85] train_eval/utils.DatasetConfig.task_feature_lengths = %TASK_FEATURE_LENGTHS
I0512 23:41:54.570518 140339465779200 gin_utils.py:85] train_eval/utils.DatasetConfig.use_cached = %USE_CACHED_TASKS
I0512 23:41:54.570551 140339465779200 gin_utils.py:85] 
I0512 23:41:54.570583 140339465779200 gin_utils.py:85] # Parameters for t5_architecture.DecoderLayer:
I0512 23:41:54.570616 140339465779200 gin_utils.py:85] # ==============================================================================
I0512 23:41:54.570651 140339465779200 gin_utils.py:85] t5_architecture.DecoderLayer.activation_partitioning_dims = \
I0512 23:41:54.570684 140339465779200 gin_utils.py:85]     %ACTIVATION_PARTITIONING_DIMS
I0512 23:41:54.570716 140339465779200 gin_utils.py:85] t5_architecture.DecoderLayer.dropout_factory = %DROPOUT_FACTORY
I0512 23:41:54.570749 140339465779200 gin_utils.py:85] t5_architecture.DecoderLayer.encoder_decoder_attention = None
I0512 23:41:54.570781 140339465779200 gin_utils.py:85] t5_architecture.DecoderLayer.layer_norm_factory = @layer_norm.T5LayerNorm
I0512 23:41:54.570813 140339465779200 gin_utils.py:85] t5_architecture.DecoderLayer.mlp = @dense.MlpBlock()
I0512 23:41:54.570845 140339465779200 gin_utils.py:85] t5_architecture.DecoderLayer.scanned = False
I0512 23:41:54.570878 140339465779200 gin_utils.py:85] t5_architecture.DecoderLayer.self_attention = \
I0512 23:41:54.570910 140339465779200 gin_utils.py:85]     @dense_attention.MultiHeadDotProductAttention()
I0512 23:41:54.570942 140339465779200 gin_utils.py:85] 
I0512 23:41:54.570975 140339465779200 gin_utils.py:85] # Parameters for t5_architecture.DecoderOnly:
I0512 23:41:54.571007 140339465779200 gin_utils.py:85] # ==============================================================================
I0512 23:41:54.571040 140339465779200 gin_utils.py:85] t5_architecture.DecoderOnly.decoder_factory = @moe_architecture.SparseDecoder
I0512 23:41:54.571072 140339465779200 gin_utils.py:85] t5_architecture.DecoderOnly.dtype = %ACTIVATION_DTYPE
I0512 23:41:54.571104 140339465779200 gin_utils.py:85] t5_architecture.DecoderOnly.shared_token_embedder_factory = @embedding.Embed
I0512 23:41:54.571137 140339465779200 gin_utils.py:85] 
I0512 23:41:54.571169 140339465779200 gin_utils.py:85] # Parameters for output_logits/dense.DenseGeneral:
I0512 23:41:54.571201 140339465779200 gin_utils.py:85] # ==============================================================================
I0512 23:41:54.571233 140339465779200 gin_utils.py:85] output_logits/dense.DenseGeneral.bias_init = %BIAS_INIT
I0512 23:41:54.571266 140339465779200 gin_utils.py:85] output_logits/dense.DenseGeneral.dtype = 'float32'
I0512 23:41:54.571298 140339465779200 gin_utils.py:85] output_logits/dense.DenseGeneral.features = %NUM_EMBEDDINGS
I0512 23:41:54.571331 140339465779200 gin_utils.py:85] output_logits/dense.DenseGeneral.kernel_axis_names = ['embed', 'vocab']
I0512 23:41:54.571368 140339465779200 gin_utils.py:85] output_logits/dense.DenseGeneral.kernel_init = \
I0512 23:41:54.571402 140339465779200 gin_utils.py:85]     @output_logits_kernel_init/linen.initializers.variance_scaling()
I0512 23:41:54.571434 140339465779200 gin_utils.py:85] output_logits/dense.DenseGeneral.use_bias = False
I0512 23:41:54.571467 140339465779200 gin_utils.py:85] 
I0512 23:41:54.571499 140339465779200 gin_utils.py:85] # Parameters for dropout_factory/linen.Dropout:
I0512 23:41:54.571532 140339465779200 gin_utils.py:85] # ==============================================================================
I0512 23:41:54.571564 140339465779200 gin_utils.py:85] dropout_factory/linen.Dropout.broadcast_dims = (-2,)
I0512 23:41:54.571597 140339465779200 gin_utils.py:85] dropout_factory/linen.Dropout.rate = %DROPOUT_RATE
I0512 23:41:54.571631 140339465779200 gin_utils.py:85] 
I0512 23:41:54.571665 140339465779200 gin_utils.py:85] # Parameters for embedding.Embed:
I0512 23:41:54.571697 140339465779200 gin_utils.py:85] # ==============================================================================
I0512 23:41:54.571730 140339465779200 gin_utils.py:85] embedding.Embed.attend_dtype = 'float32'
I0512 23:41:54.571763 140339465779200 gin_utils.py:85] embedding.Embed.cast_input_dtype = 'int32'
I0512 23:41:54.571795 140339465779200 gin_utils.py:85] embedding.Embed.dtype = %ACTIVATION_DTYPE
I0512 23:41:54.571828 140339465779200 gin_utils.py:85] embedding.Embed.embedding_init = @token_embedder_init/linen.initializers.normal()
I0512 23:41:54.571860 140339465779200 gin_utils.py:85] embedding.Embed.features = %EMBED_DIM
I0512 23:41:54.571893 140339465779200 gin_utils.py:85] embedding.Embed.name = 'token_embedder'
I0512 23:41:54.571925 140339465779200 gin_utils.py:85] embedding.Embed.num_embeddings = %NUM_EMBEDDINGS
I0512 23:41:54.571958 140339465779200 gin_utils.py:85] embedding.Embed.one_hot = True
I0512 23:41:54.571990 140339465779200 gin_utils.py:85] 
I0512 23:41:54.572022 140339465779200 gin_utils.py:85] # Parameters for dense.MlpBlock:
I0512 23:41:54.572055 140339465779200 gin_utils.py:85] # ==============================================================================
I0512 23:41:54.572088 140339465779200 gin_utils.py:85] dense.MlpBlock.activations = ('swiglu', 'linear')
I0512 23:41:54.572120 140339465779200 gin_utils.py:85] dense.MlpBlock.bias_init = %BIAS_INIT
I0512 23:41:54.572153 140339465779200 gin_utils.py:85] dense.MlpBlock.dtype = %ACTIVATION_DTYPE
I0512 23:41:54.572211 140339465779200 gin_utils.py:85] dense.MlpBlock.final_dropout_rate = 0
I0512 23:41:54.572247 140339465779200 gin_utils.py:85] dense.MlpBlock.input_axis_name = 'mlp_embed'
I0512 23:41:54.572372 140339465779200 gin_utils.py:85] dense.MlpBlock.intermediate_dim = %MLP_DIM
I0512 23:41:54.572423 140339465779200 gin_utils.py:85] dense.MlpBlock.intermediate_dropout_rate = %DROPOUT_RATE
I0512 23:41:54.530608 140031916288000 gin_utils.py:85] dense.MlpBlock.kernel_init = @mlp_kernel_init/linen.initializers.variance_scaling()
I0512 23:41:54.530639 140031916288000 gin_utils.py:85] dense.MlpBlock.output_axis_name = 'mlp_embed'
I0512 23:41:54.530675 140031916288000 gin_utils.py:85] dense.MlpBlock.use_bias = False
I0512 23:41:54.530708 140031916288000 gin_utils.py:85] 
I0512 23:41:54.530738 140031916288000 gin_utils.py:85] # Parameters for expert/dense.MlpBlock:
I0512 23:41:54.530769 140031916288000 gin_utils.py:85] # ==============================================================================
I0512 23:41:54.530800 140031916288000 gin_utils.py:85] expert/dense.MlpBlock.activation_partitioning_dims = 1
I0512 23:41:54.530831 140031916288000 gin_utils.py:85] expert/dense.MlpBlock.activations = ('swiglu', 'linear')
I0512 23:41:54.530861 140031916288000 gin_utils.py:85] expert/dense.MlpBlock.bias_init = %BIAS_INIT
I0512 23:41:54.530892 140031916288000 gin_utils.py:85] expert/dense.MlpBlock.data_sharding_constraints = ('expert_replicas', 'mlp')
I0512 23:41:54.530924 140031916288000 gin_utils.py:85] expert/dense.MlpBlock.dtype = %MOE_TRUNCATED_DTYPE
I0512 23:41:54.530956 140031916288000 gin_utils.py:85] expert/dense.MlpBlock.final_dropout_rate = 0.0
I0512 23:41:54.530987 140031916288000 gin_utils.py:85] expert/dense.MlpBlock.input_axis_name = 'mlp_embed'
I0512 23:41:54.531018 140031916288000 gin_utils.py:85] expert/dense.MlpBlock.intermediate_axis_name = 'expert_mlp'
I0512 23:41:54.531048 140031916288000 gin_utils.py:85] expert/dense.MlpBlock.intermediate_dim = %EXPERT_MLP_DIM
I0512 23:41:54.531079 140031916288000 gin_utils.py:85] expert/dense.MlpBlock.intermediate_dropout_rate = %EXPERT_DROPOUT_RATE
I0512 23:41:54.531110 140031916288000 gin_utils.py:85] expert/dense.MlpBlock.kernel_init = \
I0512 23:41:54.531140 140031916288000 gin_utils.py:85]     @expert_kernel_init/linen.initializers.variance_scaling()
I0512 23:41:54.531171 140031916288000 gin_utils.py:85] expert/dense.MlpBlock.output_axis_name = 'mlp_embed'
I0512 23:41:54.549067 139674595104768 gin_utils.py:85] dense.MlpBlock.kernel_init = @mlp_kernel_init/linen.initializers.variance_scaling()
I0512 23:41:54.549099 139674595104768 gin_utils.py:85] dense.MlpBlock.output_axis_name = 'mlp_embed'
I0512 23:41:54.549131 139674595104768 gin_utils.py:85] dense.MlpBlock.use_bias = False
I0512 23:41:54.549163 139674595104768 gin_utils.py:85] 
I0512 23:41:54.549196 139674595104768 gin_utils.py:85] # Parameters for expert/dense.MlpBlock:
I0512 23:41:54.549232 139674595104768 gin_utils.py:85] # ==============================================================================
I0512 23:41:54.549264 139674595104768 gin_utils.py:85] expert/dense.MlpBlock.activation_partitioning_dims = 1
I0512 23:41:54.549296 139674595104768 gin_utils.py:85] expert/dense.MlpBlock.activations = ('swiglu', 'linear')
I0512 23:41:54.549329 139674595104768 gin_utils.py:85] expert/dense.MlpBlock.bias_init = %BIAS_INIT
I0512 23:41:54.549361 139674595104768 gin_utils.py:85] expert/dense.MlpBlock.data_sharding_constraints = ('expert_replicas', 'mlp')
I0512 23:41:54.549393 139674595104768 gin_utils.py:85] expert/dense.MlpBlock.dtype = %MOE_TRUNCATED_DTYPE
I0512 23:41:54.549425 139674595104768 gin_utils.py:85] expert/dense.MlpBlock.final_dropout_rate = 0.0
I0512 23:41:54.549458 139674595104768 gin_utils.py:85] expert/dense.MlpBlock.input_axis_name = 'mlp_embed'
I0512 23:41:54.549489 139674595104768 gin_utils.py:85] expert/dense.MlpBlock.intermediate_axis_name = 'expert_mlp'
I0512 23:41:54.549521 139674595104768 gin_utils.py:85] expert/dense.MlpBlock.intermediate_dim = %EXPERT_MLP_DIM
I0512 23:41:54.549553 139674595104768 gin_utils.py:85] expert/dense.MlpBlock.intermediate_dropout_rate = %EXPERT_DROPOUT_RATE
I0512 23:41:54.549585 139674595104768 gin_utils.py:85] expert/dense.MlpBlock.kernel_init = \
I0512 23:41:54.549617 139674595104768 gin_utils.py:85]     @expert_kernel_init/linen.initializers.variance_scaling()
I0512 23:41:54.549649 139674595104768 gin_utils.py:85] expert/dense.MlpBlock.output_axis_name = 'mlp_embed'
I0512 23:41:54.604119 140440593786880 gin_utils.py:85] # Parameters for utils.CheckpointConfig:
I0512 23:41:54.604151 140440593786880 gin_utils.py:85] # ==============================================================================
I0512 23:41:54.604182 140440593786880 gin_utils.py:85] utils.CheckpointConfig.restore = @utils.RestoreCheckpointConfig()
I0512 23:41:54.604220 140440593786880 gin_utils.py:85] utils.CheckpointConfig.save = @utils.SaveCheckpointConfig()
I0512 23:41:54.604252 140440593786880 gin_utils.py:85] 
I0512 23:41:54.604284 140440593786880 gin_utils.py:85] # Parameters for moe_partitioning.compute_num_model_partitions:
I0512 23:41:54.604315 140440593786880 gin_utils.py:85] # ==============================================================================
I0512 23:41:54.604346 140440593786880 gin_utils.py:85] moe_partitioning.compute_num_model_partitions.model_parallel_submesh = \
I0512 23:41:54.604378 140440593786880 gin_utils.py:85]     %MODEL_PARALLEL_SUBMESH
I0512 23:41:54.604429 140440593786880 gin_utils.py:85] moe_partitioning.compute_num_model_partitions.num_model_partitions = \
I0512 23:41:54.604467 140440593786880 gin_utils.py:85]     %NUM_MODEL_PARTITIONS
I0512 23:41:54.604500 140440593786880 gin_utils.py:85] 
I0512 23:41:54.604532 140440593786880 gin_utils.py:85] # Parameters for utils.create_learning_rate_scheduler:
I0512 23:41:54.604563 140440593786880 gin_utils.py:85] # ==============================================================================
I0512 23:41:54.604595 140440593786880 gin_utils.py:85] utils.create_learning_rate_scheduler.base_learning_rate = 1.0
I0512 23:41:54.604626 140440593786880 gin_utils.py:85] utils.create_learning_rate_scheduler.factors = 'constant * rsqrt_decay'
I0512 23:41:54.604657 140440593786880 gin_utils.py:85] utils.create_learning_rate_scheduler.warmup_steps = 10000
I0512 23:41:54.604689 140440593786880 gin_utils.py:85] 
I0512 23:41:54.604720 140440593786880 gin_utils.py:85] # Parameters for train/utils.DatasetConfig:
I0512 23:41:54.604751 140440593786880 gin_utils.py:85] # ==============================================================================
I0512 23:41:54.604783 140440593786880 gin_utils.py:85] train/utils.DatasetConfig.batch_size = %BATCH_SIZE
I0512 23:41:54.604814 140440593786880 gin_utils.py:85] train/utils.DatasetConfig.mixture_or_task_name = %MIXTURE_OR_TASK_NAME
I0512 23:41:54.604846 140440593786880 gin_utils.py:85] train/utils.DatasetConfig.module = %MIXTURE_OR_TASK_MODULE
I0512 23:41:54.604877 140440593786880 gin_utils.py:85] train/utils.DatasetConfig.pack = True
I0512 23:41:54.604908 140440593786880 gin_utils.py:85] train/utils.DatasetConfig.seed = None
I0512 23:41:54.604940 140440593786880 gin_utils.py:85] train/utils.DatasetConfig.shuffle = %SHUFFLE_TRAIN_EXAMPLES
I0512 23:41:54.604974 140440593786880 gin_utils.py:85] train/utils.DatasetConfig.split = 'train'
I0512 23:41:54.605006 140440593786880 gin_utils.py:85] train/utils.DatasetConfig.task_feature_lengths = %TASK_FEATURE_LENGTHS
I0512 23:41:54.605038 140440593786880 gin_utils.py:85] train/utils.DatasetConfig.use_cached = %USE_CACHED_TASKS
I0512 23:41:54.605070 140440593786880 gin_utils.py:85] 
I0512 23:41:54.605102 140440593786880 gin_utils.py:85] # Parameters for train_eval/utils.DatasetConfig:
I0512 23:41:54.605133 140440593786880 gin_utils.py:85] # ==============================================================================
I0512 23:41:54.605165 140440593786880 gin_utils.py:85] train_eval/utils.DatasetConfig.batch_size = 128
I0512 23:41:54.605201 140440593786880 gin_utils.py:85] train_eval/utils.DatasetConfig.mixture_or_task_name = %MIXTURE_OR_TASK_NAME
I0512 23:41:54.605234 140440593786880 gin_utils.py:85] train_eval/utils.DatasetConfig.module = %MIXTURE_OR_TASK_MODULE
I0512 23:41:54.605266 140440593786880 gin_utils.py:85] train_eval/utils.DatasetConfig.pack = True
I0512 23:41:54.605297 140440593786880 gin_utils.py:85] train_eval/utils.DatasetConfig.seed = 42
I0512 23:41:54.605329 140440593786880 gin_utils.py:85] train_eval/utils.DatasetConfig.shuffle = False
I0512 23:41:54.605360 140440593786880 gin_utils.py:85] train_eval/utils.DatasetConfig.split = 'validation'
I0512 23:41:54.605391 140440593786880 gin_utils.py:85] train_eval/utils.DatasetConfig.task_feature_lengths = %TASK_FEATURE_LENGTHS
I0512 23:41:54.605423 140440593786880 gin_utils.py:85] train_eval/utils.DatasetConfig.use_cached = %USE_CACHED_TASKS
I0512 23:41:54.605454 140440593786880 gin_utils.py:85] 
I0512 23:41:54.605486 140440593786880 gin_utils.py:85] # Parameters for t5_architecture.DecoderLayer:
I0512 23:41:54.605517 140440593786880 gin_utils.py:85] # ==============================================================================
I0512 23:41:54.605549 140440593786880 gin_utils.py:85] t5_architecture.DecoderLayer.activation_partitioning_dims = \
I0512 23:41:54.605580 140440593786880 gin_utils.py:85]     %ACTIVATION_PARTITIONING_DIMS
I0512 23:41:54.605612 140440593786880 gin_utils.py:85] t5_architecture.DecoderLayer.dropout_factory = %DROPOUT_FACTORY
I0512 23:41:54.605643 140440593786880 gin_utils.py:85] t5_architecture.DecoderLayer.encoder_decoder_attention = None
I0512 23:41:54.605674 140440593786880 gin_utils.py:85] t5_architecture.DecoderLayer.layer_norm_factory = @layer_norm.T5LayerNorm
I0512 23:41:54.605705 140440593786880 gin_utils.py:85] t5_architecture.DecoderLayer.mlp = @dense.MlpBlock()
I0512 23:41:54.605737 140440593786880 gin_utils.py:85] t5_architecture.DecoderLayer.scanned = False
I0512 23:41:54.605769 140440593786880 gin_utils.py:85] t5_architecture.DecoderLayer.self_attention = \
I0512 23:41:54.605800 140440593786880 gin_utils.py:85]     @dense_attention.MultiHeadDotProductAttention()
I0512 23:41:54.605831 140440593786880 gin_utils.py:85] 
I0512 23:41:54.605863 140440593786880 gin_utils.py:85] # Parameters for t5_architecture.DecoderOnly:
I0512 23:41:54.605894 140440593786880 gin_utils.py:85] # ==============================================================================
I0512 23:41:54.605925 140440593786880 gin_utils.py:85] t5_architecture.DecoderOnly.decoder_factory = @moe_architecture.SparseDecoder
I0512 23:41:54.605958 140440593786880 gin_utils.py:85] t5_architecture.DecoderOnly.dtype = %ACTIVATION_DTYPE
I0512 23:41:54.605991 140440593786880 gin_utils.py:85] t5_architecture.DecoderOnly.shared_token_embedder_factory = @embedding.Embed
I0512 23:41:54.606022 140440593786880 gin_utils.py:85] 
I0512 23:41:54.606054 140440593786880 gin_utils.py:85] # Parameters for output_logits/dense.DenseGeneral:
I0512 23:41:54.606086 140440593786880 gin_utils.py:85] # ==============================================================================
I0512 23:41:54.606117 140440593786880 gin_utils.py:85] output_logits/dense.DenseGeneral.bias_init = %BIAS_INIT
I0512 23:41:54.606148 140440593786880 gin_utils.py:85] output_logits/dense.DenseGeneral.dtype = 'float32'
I0512 23:41:54.606180 140440593786880 gin_utils.py:85] output_logits/dense.DenseGeneral.features = %NUM_EMBEDDINGS
I0512 23:41:54.606217 140440593786880 gin_utils.py:85] output_logits/dense.DenseGeneral.kernel_axis_names = ['embed', 'vocab']
I0512 23:41:54.606249 140440593786880 gin_utils.py:85] output_logits/dense.DenseGeneral.kernel_init = \
I0512 23:41:54.606281 140440593786880 gin_utils.py:85]     @output_logits_kernel_init/linen.initializers.variance_scaling()
I0512 23:41:54.606312 140440593786880 gin_utils.py:85] output_logits/dense.DenseGeneral.use_bias = False
I0512 23:41:54.606344 140440593786880 gin_utils.py:85] 
I0512 23:41:54.606375 140440593786880 gin_utils.py:85] # Parameters for dropout_factory/linen.Dropout:
I0512 23:41:54.606407 140440593786880 gin_utils.py:85] # ==============================================================================
I0512 23:41:54.606438 140440593786880 gin_utils.py:85] dropout_factory/linen.Dropout.broadcast_dims = (-2,)
I0512 23:41:54.606469 140440593786880 gin_utils.py:85] dropout_factory/linen.Dropout.rate = %DROPOUT_RATE
I0512 23:41:54.606500 140440593786880 gin_utils.py:85] 
I0512 23:41:54.606531 140440593786880 gin_utils.py:85] # Parameters for embedding.Embed:
I0512 23:41:54.606562 140440593786880 gin_utils.py:85] # ==============================================================================
I0512 23:41:54.606594 140440593786880 gin_utils.py:85] embedding.Embed.attend_dtype = 'float32'
I0512 23:41:54.606625 140440593786880 gin_utils.py:85] embedding.Embed.cast_input_dtype = 'int32'
I0512 23:41:54.606657 140440593786880 gin_utils.py:85] embedding.Embed.dtype = %ACTIVATION_DTYPE
I0512 23:41:54.606688 140440593786880 gin_utils.py:85] embedding.Embed.embedding_init = @token_embedder_init/linen.initializers.normal()
I0512 23:41:54.606719 140440593786880 gin_utils.py:85] embedding.Embed.features = %EMBED_DIM
I0512 23:41:54.606750 140440593786880 gin_utils.py:85] embedding.Embed.name = 'token_embedder'
I0512 23:41:54.606782 140440593786880 gin_utils.py:85] embedding.Embed.num_embeddings = %NUM_EMBEDDINGS
I0512 23:41:54.606813 140440593786880 gin_utils.py:85] embedding.Embed.one_hot = True
I0512 23:41:54.606844 140440593786880 gin_utils.py:85] 
I0512 23:41:54.606875 140440593786880 gin_utils.py:85] # Parameters for dense.MlpBlock:
I0512 23:41:54.606906 140440593786880 gin_utils.py:85] # ==============================================================================
I0512 23:41:54.606938 140440593786880 gin_utils.py:85] dense.MlpBlock.activations = ('swiglu', 'linear')
I0512 23:41:54.606972 140440593786880 gin_utils.py:85] dense.MlpBlock.bias_init = %BIAS_INIT
I0512 23:41:54.607004 140440593786880 gin_utils.py:85] dense.MlpBlock.dtype = %ACTIVATION_DTYPE
I0512 23:41:54.607035 140440593786880 gin_utils.py:85] dense.MlpBlock.final_dropout_rate = 0
I0512 23:41:54.607067 140440593786880 gin_utils.py:85] dense.MlpBlock.input_axis_name = 'mlp_embed'
I0512 23:41:54.607098 140440593786880 gin_utils.py:85] dense.MlpBlock.intermediate_dim = %MLP_DIM
I0512 23:41:54.607130 140440593786880 gin_utils.py:85] dense.MlpBlock.intermediate_dropout_rate = %DROPOUT_RATE
I0512 23:41:54.618599 140095893518336 gin_utils.py:85] # Parameters for utils.CheckpointConfig:
I0512 23:41:54.618630 140095893518336 gin_utils.py:85] # ==============================================================================
I0512 23:41:54.618661 140095893518336 gin_utils.py:85] utils.CheckpointConfig.restore = @utils.RestoreCheckpointConfig()
I0512 23:41:54.618692 140095893518336 gin_utils.py:85] utils.CheckpointConfig.save = @utils.SaveCheckpointConfig()
I0512 23:41:54.618723 140095893518336 gin_utils.py:85] 
I0512 23:41:54.618754 140095893518336 gin_utils.py:85] # Parameters for moe_partitioning.compute_num_model_partitions:
I0512 23:41:54.618785 140095893518336 gin_utils.py:85] # ==============================================================================
I0512 23:41:54.618816 140095893518336 gin_utils.py:85] moe_partitioning.compute_num_model_partitions.model_parallel_submesh = \
I0512 23:41:54.618847 140095893518336 gin_utils.py:85]     %MODEL_PARALLEL_SUBMESH
I0512 23:41:54.618878 140095893518336 gin_utils.py:85] moe_partitioning.compute_num_model_partitions.num_model_partitions = \
I0512 23:41:54.618909 140095893518336 gin_utils.py:85]     %NUM_MODEL_PARTITIONS
I0512 23:41:54.618940 140095893518336 gin_utils.py:85] 
I0512 23:41:54.618971 140095893518336 gin_utils.py:85] # Parameters for utils.create_learning_rate_scheduler:
I0512 23:41:54.619002 140095893518336 gin_utils.py:85] # ==============================================================================
I0512 23:41:54.619033 140095893518336 gin_utils.py:85] utils.create_learning_rate_scheduler.base_learning_rate = 1.0
I0512 23:41:54.619064 140095893518336 gin_utils.py:85] utils.create_learning_rate_scheduler.factors = 'constant * rsqrt_decay'
I0512 23:41:54.619095 140095893518336 gin_utils.py:85] utils.create_learning_rate_scheduler.warmup_steps = 10000
I0512 23:41:54.619131 140095893518336 gin_utils.py:85] 
I0512 23:41:54.619164 140095893518336 gin_utils.py:85] # Parameters for train/utils.DatasetConfig:
I0512 23:41:54.619196 140095893518336 gin_utils.py:85] # ==============================================================================
I0512 23:41:54.619227 140095893518336 gin_utils.py:85] train/utils.DatasetConfig.batch_size = %BATCH_SIZE
I0512 23:41:54.619258 140095893518336 gin_utils.py:85] train/utils.DatasetConfig.mixture_or_task_name = %MIXTURE_OR_TASK_NAME
I0512 23:41:54.619289 140095893518336 gin_utils.py:85] train/utils.DatasetConfig.module = %MIXTURE_OR_TASK_MODULE
I0512 23:41:54.619320 140095893518336 gin_utils.py:85] train/utils.DatasetConfig.pack = True
I0512 23:41:54.619351 140095893518336 gin_utils.py:85] train/utils.DatasetConfig.seed = None
I0512 23:41:54.619383 140095893518336 gin_utils.py:85] train/utils.DatasetConfig.shuffle = %SHUFFLE_TRAIN_EXAMPLES
I0512 23:41:54.619415 140095893518336 gin_utils.py:85] train/utils.DatasetConfig.split = 'train'
I0512 23:41:54.619447 140095893518336 gin_utils.py:85] train/utils.DatasetConfig.task_feature_lengths = %TASK_FEATURE_LENGTHS
I0512 23:41:54.619478 140095893518336 gin_utils.py:85] train/utils.DatasetConfig.use_cached = %USE_CACHED_TASKS
I0512 23:41:54.619509 140095893518336 gin_utils.py:85] 
I0512 23:41:54.619540 140095893518336 gin_utils.py:85] # Parameters for train_eval/utils.DatasetConfig:
I0512 23:41:54.619571 140095893518336 gin_utils.py:85] # ==============================================================================
I0512 23:41:54.619602 140095893518336 gin_utils.py:85] train_eval/utils.DatasetConfig.batch_size = 128
I0512 23:41:54.619633 140095893518336 gin_utils.py:85] train_eval/utils.DatasetConfig.mixture_or_task_name = %MIXTURE_OR_TASK_NAME
I0512 23:41:54.619663 140095893518336 gin_utils.py:85] train_eval/utils.DatasetConfig.module = %MIXTURE_OR_TASK_MODULE
I0512 23:41:54.619695 140095893518336 gin_utils.py:85] train_eval/utils.DatasetConfig.pack = True
I0512 23:41:54.619726 140095893518336 gin_utils.py:85] train_eval/utils.DatasetConfig.seed = 42
I0512 23:41:54.619756 140095893518336 gin_utils.py:85] train_eval/utils.DatasetConfig.shuffle = False
I0512 23:41:54.619787 140095893518336 gin_utils.py:85] train_eval/utils.DatasetConfig.split = 'validation'
I0512 23:41:54.619818 140095893518336 gin_utils.py:85] train_eval/utils.DatasetConfig.task_feature_lengths = %TASK_FEATURE_LENGTHS
I0512 23:41:54.619849 140095893518336 gin_utils.py:85] train_eval/utils.DatasetConfig.use_cached = %USE_CACHED_TASKS
I0512 23:41:54.619880 140095893518336 gin_utils.py:85] 
I0512 23:41:54.619922 140095893518336 gin_utils.py:85] # Parameters for t5_architecture.DecoderLayer:
I0512 23:41:54.619954 140095893518336 gin_utils.py:85] # ==============================================================================
I0512 23:41:54.619985 140095893518336 gin_utils.py:85] t5_architecture.DecoderLayer.activation_partitioning_dims = \
I0512 23:41:54.620016 140095893518336 gin_utils.py:85]     %ACTIVATION_PARTITIONING_DIMS
I0512 23:41:54.620047 140095893518336 gin_utils.py:85] t5_architecture.DecoderLayer.dropout_factory = %DROPOUT_FACTORY
I0512 23:41:54.620079 140095893518336 gin_utils.py:85] t5_architecture.DecoderLayer.encoder_decoder_attention = None
I0512 23:41:54.620110 140095893518336 gin_utils.py:85] t5_architecture.DecoderLayer.layer_norm_factory = @layer_norm.T5LayerNorm
I0512 23:41:54.620146 140095893518336 gin_utils.py:85] t5_architecture.DecoderLayer.mlp = @dense.MlpBlock()
I0512 23:41:54.620178 140095893518336 gin_utils.py:85] t5_architecture.DecoderLayer.scanned = False
I0512 23:41:54.620209 140095893518336 gin_utils.py:85] t5_architecture.DecoderLayer.self_attention = \
I0512 23:41:54.620240 140095893518336 gin_utils.py:85]     @dense_attention.MultiHeadDotProductAttention()
I0512 23:41:54.620271 140095893518336 gin_utils.py:85] 
I0512 23:41:54.620301 140095893518336 gin_utils.py:85] # Parameters for t5_architecture.DecoderOnly:
I0512 23:41:54.620332 140095893518336 gin_utils.py:85] # ==============================================================================
I0512 23:41:54.620363 140095893518336 gin_utils.py:85] t5_architecture.DecoderOnly.decoder_factory = @moe_architecture.SparseDecoder
I0512 23:41:54.620396 140095893518336 gin_utils.py:85] t5_architecture.DecoderOnly.dtype = %ACTIVATION_DTYPE
I0512 23:41:54.620428 140095893518336 gin_utils.py:85] t5_architecture.DecoderOnly.shared_token_embedder_factory = @embedding.Embed
I0512 23:41:54.620459 140095893518336 gin_utils.py:85] 
I0512 23:41:54.620490 140095893518336 gin_utils.py:85] # Parameters for output_logits/dense.DenseGeneral:
I0512 23:41:54.620521 140095893518336 gin_utils.py:85] # ==============================================================================
I0512 23:41:54.620551 140095893518336 gin_utils.py:85] output_logits/dense.DenseGeneral.bias_init = %BIAS_INIT
I0512 23:41:54.620582 140095893518336 gin_utils.py:85] output_logits/dense.DenseGeneral.dtype = 'float32'
I0512 23:41:54.620613 140095893518336 gin_utils.py:85] output_logits/dense.DenseGeneral.features = %NUM_EMBEDDINGS
I0512 23:41:54.620644 140095893518336 gin_utils.py:85] output_logits/dense.DenseGeneral.kernel_axis_names = ['embed', 'vocab']
I0512 23:41:54.620675 140095893518336 gin_utils.py:85] output_logits/dense.DenseGeneral.kernel_init = \
I0512 23:41:54.620706 140095893518336 gin_utils.py:85]     @output_logits_kernel_init/linen.initializers.variance_scaling()
I0512 23:41:54.620737 140095893518336 gin_utils.py:85] output_logits/dense.DenseGeneral.use_bias = False
I0512 23:41:54.620768 140095893518336 gin_utils.py:85] 
I0512 23:41:54.620798 140095893518336 gin_utils.py:85] # Parameters for dropout_factory/linen.Dropout:
I0512 23:41:54.620829 140095893518336 gin_utils.py:85] # ==============================================================================
I0512 23:41:54.620860 140095893518336 gin_utils.py:85] dropout_factory/linen.Dropout.broadcast_dims = (-2,)
I0512 23:41:54.620890 140095893518336 gin_utils.py:85] dropout_factory/linen.Dropout.rate = %DROPOUT_RATE
I0512 23:41:54.620921 140095893518336 gin_utils.py:85] 
I0512 23:41:54.620952 140095893518336 gin_utils.py:85] # Parameters for embedding.Embed:
I0512 23:41:54.620983 140095893518336 gin_utils.py:85] # ==============================================================================
I0512 23:41:54.621014 140095893518336 gin_utils.py:85] embedding.Embed.attend_dtype = 'float32'
I0512 23:41:54.621044 140095893518336 gin_utils.py:85] embedding.Embed.cast_input_dtype = 'int32'
I0512 23:41:54.621075 140095893518336 gin_utils.py:85] embedding.Embed.dtype = %ACTIVATION_DTYPE
I0512 23:41:54.621105 140095893518336 gin_utils.py:85] embedding.Embed.embedding_init = @token_embedder_init/linen.initializers.normal()
I0512 23:41:54.621142 140095893518336 gin_utils.py:85] embedding.Embed.features = %EMBED_DIM
I0512 23:41:54.621174 140095893518336 gin_utils.py:85] embedding.Embed.name = 'token_embedder'
I0512 23:41:54.621205 140095893518336 gin_utils.py:85] embedding.Embed.num_embeddings = %NUM_EMBEDDINGS
I0512 23:41:54.621236 140095893518336 gin_utils.py:85] embedding.Embed.one_hot = True
I0512 23:41:54.621266 140095893518336 gin_utils.py:85] 
I0512 23:41:54.621297 140095893518336 gin_utils.py:85] # Parameters for dense.MlpBlock:
I0512 23:41:54.621328 140095893518336 gin_utils.py:85] # ==============================================================================
I0512 23:41:54.621359 140095893518336 gin_utils.py:85] dense.MlpBlock.activations = ('swiglu', 'linear')
I0512 23:41:54.621392 140095893518336 gin_utils.py:85] dense.MlpBlock.bias_init = %BIAS_INIT
I0512 23:41:54.621424 140095893518336 gin_utils.py:85] dense.MlpBlock.dtype = %ACTIVATION_DTYPE
I0512 23:41:54.621455 140095893518336 gin_utils.py:85] dense.MlpBlock.final_dropout_rate = 0
I0512 23:41:54.621485 140095893518336 gin_utils.py:85] dense.MlpBlock.input_axis_name = 'mlp_embed'
I0512 23:41:54.621516 140095893518336 gin_utils.py:85] dense.MlpBlock.intermediate_dim = %MLP_DIM
I0512 23:41:54.621546 140095893518336 gin_utils.py:85] dense.MlpBlock.intermediate_dropout_rate = %DROPOUT_RATE
I0512 23:41:54.572459 140339465779200 gin_utils.py:85] dense.MlpBlock.kernel_init = @mlp_kernel_init/linen.initializers.variance_scaling()
I0512 23:41:54.572492 140339465779200 gin_utils.py:85] dense.MlpBlock.output_axis_name = 'mlp_embed'
I0512 23:41:54.572576 140339465779200 gin_utils.py:85] dense.MlpBlock.use_bias = False
I0512 23:41:54.572618 140339465779200 gin_utils.py:85] 
I0512 23:41:54.572657 140339465779200 gin_utils.py:85] # Parameters for expert/dense.MlpBlock:
I0512 23:41:54.572691 140339465779200 gin_utils.py:85] # ==============================================================================
I0512 23:41:54.572724 140339465779200 gin_utils.py:85] expert/dense.MlpBlock.activation_partitioning_dims = 1
I0512 23:41:54.572757 140339465779200 gin_utils.py:85] expert/dense.MlpBlock.activations = ('swiglu', 'linear')
I0512 23:41:54.572825 140339465779200 gin_utils.py:85] expert/dense.MlpBlock.bias_init = %BIAS_INIT
I0512 23:41:54.572862 140339465779200 gin_utils.py:85] expert/dense.MlpBlock.data_sharding_constraints = ('expert_replicas', 'mlp')
I0512 23:41:54.572896 140339465779200 gin_utils.py:85] expert/dense.MlpBlock.dtype = %MOE_TRUNCATED_DTYPE
I0512 23:41:54.572929 140339465779200 gin_utils.py:85] expert/dense.MlpBlock.final_dropout_rate = 0.0
I0512 23:41:54.572962 140339465779200 gin_utils.py:85] expert/dense.MlpBlock.input_axis_name = 'mlp_embed'
I0512 23:41:54.573018 140339465779200 gin_utils.py:85] expert/dense.MlpBlock.intermediate_axis_name = 'expert_mlp'
I0512 23:41:54.573053 140339465779200 gin_utils.py:85] expert/dense.MlpBlock.intermediate_dim = %EXPERT_MLP_DIM
I0512 23:41:54.573086 140339465779200 gin_utils.py:85] expert/dense.MlpBlock.intermediate_dropout_rate = %EXPERT_DROPOUT_RATE
I0512 23:41:54.573118 140339465779200 gin_utils.py:85] expert/dense.MlpBlock.kernel_init = \
I0512 23:41:54.573151 140339465779200 gin_utils.py:85]     @expert_kernel_init/linen.initializers.variance_scaling()
I0512 23:41:54.573184 140339465779200 gin_utils.py:85] expert/dense.MlpBlock.output_axis_name = 'mlp_embed'
I0512 23:41:54.691740 140311617681408 resource_reader.py:50] system_path_file_exists:flaxformer/flaxformer/t5x/configs/moe/models/st_moe_decoder_only_large.gin
E0512 23:41:54.692168 140311617681408 resource_reader.py:55] Path not found: flaxformer/flaxformer/t5x/configs/moe/models/st_moe_decoder_only_large.gin
I0512 23:41:54.692229 140311617681408 resource_reader.py:50] system_path_file_exists:./flaxformer/flaxformer/t5x/configs/moe/models/st_moe_decoder_only_large.gin
E0512 23:41:54.692276 140311617681408 resource_reader.py:55] Path not found: ./flaxformer/flaxformer/t5x/configs/moe/models/st_moe_decoder_only_large.gin
I0512 23:41:54.717545 140595318249472 resource_reader.py:50] system_path_file_exists:flaxformer/flaxformer/t5x/configs/moe/models/st_moe_decoder_only_large.gin
E0512 23:41:54.718019 140595318249472 resource_reader.py:55] Path not found: flaxformer/flaxformer/t5x/configs/moe/models/st_moe_decoder_only_large.gin
I0512 23:41:54.718079 140595318249472 resource_reader.py:50] system_path_file_exists:./flaxformer/flaxformer/t5x/configs/moe/models/st_moe_decoder_only_large.gin
E0512 23:41:54.718125 140595318249472 resource_reader.py:55] Path not found: ./flaxformer/flaxformer/t5x/configs/moe/models/st_moe_decoder_only_large.gin
I0512 23:41:54.531202 140031916288000 gin_utils.py:85] expert/dense.MlpBlock.use_bias = False
I0512 23:41:54.531232 140031916288000 gin_utils.py:85] 
I0512 23:41:54.531263 140031916288000 gin_utils.py:85] # Parameters for models.MoeDecoderOnlyModel:
I0512 23:41:54.531293 140031916288000 gin_utils.py:85] # ==============================================================================
I0512 23:41:54.531324 140031916288000 gin_utils.py:85] models.MoeDecoderOnlyModel.aux_loss_factor = %AUX_LOSS_FACTOR
I0512 23:41:54.531355 140031916288000 gin_utils.py:85] models.MoeDecoderOnlyModel.label_smoothing = %LABEL_SMOOTHING
I0512 23:41:54.531386 140031916288000 gin_utils.py:85] models.MoeDecoderOnlyModel.loss_normalizing_factor = %LOSS_NORMALIZING_FACTOR
I0512 23:41:54.531417 140031916288000 gin_utils.py:85] models.MoeDecoderOnlyModel.module = %ARCHITECTURE
I0512 23:41:54.531447 140031916288000 gin_utils.py:85] models.MoeDecoderOnlyModel.optimizer_def = %OPTIMIZER
I0512 23:41:54.531479 140031916288000 gin_utils.py:85] models.MoeDecoderOnlyModel.router_z_loss_factor = %ROUTER_Z_LOSS_FACTOR
I0512 23:41:54.531509 140031916288000 gin_utils.py:85] models.MoeDecoderOnlyModel.vocabulary = %VOCABULARY
I0512 23:41:54.531540 140031916288000 gin_utils.py:85] models.MoeDecoderOnlyModel.z_loss = %Z_LOSS
I0512 23:41:54.531571 140031916288000 gin_utils.py:85] 
I0512 23:41:54.531602 140031916288000 gin_utils.py:85] # Parameters for moe_layers.MoeLayer:
I0512 23:41:54.531632 140031916288000 gin_utils.py:85] # ==============================================================================
I0512 23:41:54.531667 140031916288000 gin_utils.py:85] moe_layers.MoeLayer.dtype = %MOE_TRUNCATED_DTYPE
I0512 23:41:54.531699 140031916288000 gin_utils.py:85] moe_layers.MoeLayer.eval_capacity_factor = %EVAL_EXPERT_CAPACITY_FACTOR
I0512 23:41:54.531730 140031916288000 gin_utils.py:85] moe_layers.MoeLayer.expert = @expert/dense.MlpBlock()
I0512 23:41:54.531761 140031916288000 gin_utils.py:85] moe_layers.MoeLayer.max_group_size = %GROUP_SIZE
I0512 23:41:54.531792 140031916288000 gin_utils.py:85] moe_layers.MoeLayer.num_expert_partitions = %NUM_EXPERT_PARTITIONS
I0512 23:41:54.531823 140031916288000 gin_utils.py:85] moe_layers.MoeLayer.num_experts = %NUM_EXPERTS
I0512 23:41:54.531853 140031916288000 gin_utils.py:85] moe_layers.MoeLayer.num_model_partitions = \
I0512 23:41:54.531884 140031916288000 gin_utils.py:85]     @moe_partitioning.compute_num_model_partitions()
I0512 23:41:54.531915 140031916288000 gin_utils.py:85] moe_layers.MoeLayer.train_capacity_factor = %TRAIN_EXPERT_CAPACITY_FACTOR
I0512 23:41:54.531948 140031916288000 gin_utils.py:85] 
I0512 23:41:54.531979 140031916288000 gin_utils.py:85] # Parameters for sparse_decoder/moe_layers.MoeLayer:
I0512 23:41:54.532010 140031916288000 gin_utils.py:85] # ==============================================================================
I0512 23:41:54.532041 140031916288000 gin_utils.py:85] sparse_decoder/moe_layers.MoeLayer.router = \
I0512 23:41:54.532071 140031916288000 gin_utils.py:85]     @sparse_decoder/routing.TokensChooseMaskedRouter()
I0512 23:41:54.532101 140031916288000 gin_utils.py:85] 
I0512 23:41:54.532132 140031916288000 gin_utils.py:85] # Parameters for moe_partitioning.MoePjitPartitioner:
I0512 23:41:54.532163 140031916288000 gin_utils.py:85] # ==============================================================================
I0512 23:41:54.532193 140031916288000 gin_utils.py:85] moe_partitioning.MoePjitPartitioner.model_parallel_submesh = \
I0512 23:41:54.532224 140031916288000 gin_utils.py:85]     %MODEL_PARALLEL_SUBMESH
I0512 23:41:54.532255 140031916288000 gin_utils.py:85] moe_partitioning.MoePjitPartitioner.num_expert_partitions = %NUM_EXPERT_PARTITIONS
I0512 23:41:54.532286 140031916288000 gin_utils.py:85] moe_partitioning.MoePjitPartitioner.num_partitions = %NUM_MODEL_PARTITIONS
I0512 23:41:54.532317 140031916288000 gin_utils.py:85] 
I0512 23:41:54.532347 140031916288000 gin_utils.py:85] # Parameters for moe_trainer.MoeTrainer:
I0512 23:41:54.532378 140031916288000 gin_utils.py:85] # ==============================================================================
I0512 23:41:54.532409 140031916288000 gin_utils.py:85] moe_trainer.MoeTrainer.learning_rate_fn = @utils.create_learning_rate_scheduler()
I0512 23:41:54.532439 140031916288000 gin_utils.py:85] moe_trainer.MoeTrainer.num_expert_partitions = %NUM_EXPERT_PARTITIONS
I0512 23:41:54.532470 140031916288000 gin_utils.py:85] moe_trainer.MoeTrainer.num_microbatches = 8
I0512 23:41:54.532501 140031916288000 gin_utils.py:85] 
I0512 23:41:54.532532 140031916288000 gin_utils.py:85] # Parameters for dense_attention.MultiHeadDotProductAttention:
I0512 23:41:54.532563 140031916288000 gin_utils.py:85] # ==============================================================================
I0512 23:41:54.532594 140031916288000 gin_utils.py:85] dense_attention.MultiHeadDotProductAttention.bias_init = %BIAS_INIT
I0512 23:41:54.532624 140031916288000 gin_utils.py:85] dense_attention.MultiHeadDotProductAttention.broadcast_dropout = True
I0512 23:41:54.532655 140031916288000 gin_utils.py:85] dense_attention.MultiHeadDotProductAttention.dropout_rate = %DROPOUT_RATE
I0512 23:41:54.532691 140031916288000 gin_utils.py:85] dense_attention.MultiHeadDotProductAttention.dtype = %ACTIVATION_DTYPE
I0512 23:41:54.532723 140031916288000 gin_utils.py:85] dense_attention.MultiHeadDotProductAttention.head_dim = %HEAD_DIM
I0512 23:41:54.532754 140031916288000 gin_utils.py:85] dense_attention.MultiHeadDotProductAttention.kernel_init = \
I0512 23:41:54.532785 140031916288000 gin_utils.py:85]     @attention_kernel_init/linen.initializers.variance_scaling()
I0512 23:41:54.532815 140031916288000 gin_utils.py:85] dense_attention.MultiHeadDotProductAttention.num_heads = %NUM_HEADS
I0512 23:41:54.532846 140031916288000 gin_utils.py:85] dense_attention.MultiHeadDotProductAttention.use_bias = False
I0512 23:41:54.532877 140031916288000 gin_utils.py:85] dense_attention.MultiHeadDotProductAttention.use_rotary_embedding = True
I0512 23:41:54.532908 140031916288000 gin_utils.py:85] 
I0512 23:41:54.532941 140031916288000 gin_utils.py:85] # Parameters for bias_init/linen.initializers.normal:
I0512 23:41:54.532972 140031916288000 gin_utils.py:85] # ==============================================================================
I0512 23:41:54.533003 140031916288000 gin_utils.py:85] bias_init/linen.initializers.normal.stddev = 1e-06
I0512 23:41:54.533034 140031916288000 gin_utils.py:85] 
I0512 23:41:54.533065 140031916288000 gin_utils.py:85] # Parameters for router_init/linen.initializers.normal:
I0512 23:41:54.533097 140031916288000 gin_utils.py:85] # ==============================================================================
I0512 23:41:54.533128 140031916288000 gin_utils.py:85] router_init/linen.initializers.normal.stddev = 0.02
I0512 23:41:54.533159 140031916288000 gin_utils.py:85] 
I0512 23:41:54.533189 140031916288000 gin_utils.py:85] # Parameters for token_embedder_init/linen.initializers.normal:
I0512 23:41:54.533220 140031916288000 gin_utils.py:85] # ==============================================================================
I0512 23:41:54.533250 140031916288000 gin_utils.py:85] token_embedder_init/linen.initializers.normal.stddev = 1.0
I0512 23:41:54.533281 140031916288000 gin_utils.py:85] 
I0512 23:41:54.533312 140031916288000 gin_utils.py:85] # Parameters for partitioning.PjitPartitioner:
I0512 23:41:54.533342 140031916288000 gin_utils.py:85] # ==============================================================================
I0512 23:41:54.533373 140031916288000 gin_utils.py:85] partitioning.PjitPartitioner.logical_axis_rules = \
I0512 23:41:54.533404 140031916288000 gin_utils.py:85]     @partitioning.standard_logical_axis_rules()
I0512 23:41:54.533434 140031916288000 gin_utils.py:85] partitioning.PjitPartitioner.model_parallel_submesh = None
I0512 23:41:54.533464 140031916288000 gin_utils.py:85] partitioning.PjitPartitioner.num_partitions = 1
I0512 23:41:54.533494 140031916288000 gin_utils.py:85] 
I0512 23:41:54.533525 140031916288000 gin_utils.py:85] # Parameters for utils.RestoreCheckpointConfig:
I0512 23:41:54.533556 140031916288000 gin_utils.py:85] # ==============================================================================
I0512 23:41:54.533586 140031916288000 gin_utils.py:85] utils.RestoreCheckpointConfig.path = []
I0512 23:41:54.533617 140031916288000 gin_utils.py:85] 
I0512 23:41:54.533647 140031916288000 gin_utils.py:85] # Parameters for routing.RouterWeights:
I0512 23:41:54.533684 140031916288000 gin_utils.py:85] # ==============================================================================
I0512 23:41:54.533715 140031916288000 gin_utils.py:85] routing.RouterWeights.bias_init = %BIAS_INIT
I0512 23:41:54.533746 140031916288000 gin_utils.py:85] routing.RouterWeights.dtype = 'float32'
I0512 23:41:54.533776 140031916288000 gin_utils.py:85] routing.RouterWeights.kernel_init = @router_init/linen.initializers.normal()
I0512 23:41:54.533807 140031916288000 gin_utils.py:85] routing.RouterWeights.use_bias = False
I0512 23:41:54.533837 140031916288000 gin_utils.py:85] 
I0512 23:41:54.533868 140031916288000 gin_utils.py:85] # Parameters for utils.SaveCheckpointConfig:
I0512 23:41:54.533898 140031916288000 gin_utils.py:85] # ==============================================================================
I0512 23:41:54.533931 140031916288000 gin_utils.py:85] utils.SaveCheckpointConfig.dtype = 'float32'
I0512 23:41:54.533962 140031916288000 gin_utils.py:85] utils.SaveCheckpointConfig.keep = 20
I0512 23:41:54.533993 140031916288000 gin_utils.py:85] utils.SaveCheckpointConfig.period = 2500
I0512 23:41:54.534024 140031916288000 gin_utils.py:85] utils.SaveCheckpointConfig.save_dataset = False
I0512 23:41:54.534054 140031916288000 gin_utils.py:85] 
I0512 23:41:54.534102 140031916288000 gin_utils.py:85] # Parameters for seqio.SentencePieceVocabulary:
I0512 23:41:54.534138 140031916288000 gin_utils.py:85] # ==============================================================================
I0512 23:41:54.534169 140031916288000 gin_utils.py:85] seqio.SentencePieceVocabulary.extra_ids = 300
I0512 23:41:54.534200 140031916288000 gin_utils.py:85] seqio.SentencePieceVocabulary.sentencepiece_model_file = \
I0512 23:41:54.534230 140031916288000 gin_utils.py:85]     'gs://rosinality-tpu-bucket/sentencepiece.model'
I0512 23:41:54.534261 140031916288000 gin_utils.py:85] 
I0512 23:41:54.534292 140031916288000 gin_utils.py:85] # Parameters for moe_architecture.SparseDecoder:
I0512 23:41:54.534323 140031916288000 gin_utils.py:85] # ==============================================================================
I0512 23:41:54.534353 140031916288000 gin_utils.py:85] moe_architecture.SparseDecoder.dropout_factory = %DROPOUT_FACTORY
I0512 23:41:54.534384 140031916288000 gin_utils.py:85] moe_architecture.SparseDecoder.dtype = %ACTIVATION_DTYPE
I0512 23:41:54.534415 140031916288000 gin_utils.py:85] moe_architecture.SparseDecoder.layer_factory = @t5_architecture.DecoderLayer
I0512 23:41:54.534445 140031916288000 gin_utils.py:85] moe_architecture.SparseDecoder.layer_norm_factory = @layer_norm.T5LayerNorm
I0512 23:41:54.534476 140031916288000 gin_utils.py:85] moe_architecture.SparseDecoder.num_layers = %NUM_DECODER_LAYERS
I0512 23:41:54.534507 140031916288000 gin_utils.py:85] moe_architecture.SparseDecoder.num_sparse_layers = %NUM_DECODER_SPARSE_LAYERS
I0512 23:41:54.534537 140031916288000 gin_utils.py:85] moe_architecture.SparseDecoder.output_logits_factory = \
I0512 23:41:54.534568 140031916288000 gin_utils.py:85]     @output_logits/dense.DenseGeneral
I0512 23:41:54.534598 140031916288000 gin_utils.py:85] moe_architecture.SparseDecoder.sparse_layer_factory = \
I0512 23:41:54.534629 140031916288000 gin_utils.py:85]     @moe_architecture.SparseDecoderLayer
I0512 23:41:54.534659 140031916288000 gin_utils.py:85] moe_architecture.SparseDecoder.sparse_layout = %DECODER_SPARSE_LAYOUT
I0512 23:41:54.534696 140031916288000 gin_utils.py:85] 
I0512 23:41:54.534727 140031916288000 gin_utils.py:85] # Parameters for moe_architecture.SparseDecoderLayer:
I0512 23:41:54.534758 140031916288000 gin_utils.py:85] # ==============================================================================
I0512 23:41:54.534789 140031916288000 gin_utils.py:85] moe_architecture.SparseDecoderLayer.activation_partitioning_dims = \
I0512 23:41:54.534820 140031916288000 gin_utils.py:85]     %ACTIVATION_PARTITIONING_DIMS
I0512 23:41:54.534850 140031916288000 gin_utils.py:85] moe_architecture.SparseDecoderLayer.dropout_factory = %DROPOUT_FACTORY
I0512 23:41:54.534881 140031916288000 gin_utils.py:85] moe_architecture.SparseDecoderLayer.encoder_decoder_attention = None
I0512 23:41:54.534913 140031916288000 gin_utils.py:85] moe_architecture.SparseDecoderLayer.extra_mlp = @dense.MlpBlock()
I0512 23:41:54.534946 140031916288000 gin_utils.py:85] moe_architecture.SparseDecoderLayer.layer_norm_factory = @layer_norm.T5LayerNorm
I0512 23:41:54.534978 140031916288000 gin_utils.py:85] moe_architecture.SparseDecoderLayer.mlp = @sparse_decoder/moe_layers.MoeLayer()
I0512 23:41:54.535009 140031916288000 gin_utils.py:85] moe_architecture.SparseDecoderLayer.scanned = False
I0512 23:41:54.535040 140031916288000 gin_utils.py:85] moe_architecture.SparseDecoderLayer.self_attention = \
I0512 23:41:54.535070 140031916288000 gin_utils.py:85]     @dense_attention.MultiHeadDotProductAttention()
I0512 23:41:54.535101 140031916288000 gin_utils.py:85] 
I0512 23:41:54.535132 140031916288000 gin_utils.py:85] # Parameters for layer_norm.T5LayerNorm:
I0512 23:41:54.535163 140031916288000 gin_utils.py:85] # ==============================================================================
I0512 23:41:54.535194 140031916288000 gin_utils.py:85] layer_norm.T5LayerNorm.dtype = %ACTIVATION_DTYPE
I0512 23:41:54.535225 140031916288000 gin_utils.py:85] 
I0512 23:41:54.535256 140031916288000 gin_utils.py:85] # Parameters for routing.TokensChooseMaskedRouter:
I0512 23:41:54.535286 140031916288000 gin_utils.py:85] # ==============================================================================
I0512 23:41:54.535317 140031916288000 gin_utils.py:85] routing.TokensChooseMaskedRouter.dtype = 'float32'
I0512 23:41:54.535347 140031916288000 gin_utils.py:85] routing.TokensChooseMaskedRouter.ignore_padding_tokens = False
I0512 23:41:54.535378 140031916288000 gin_utils.py:85] routing.TokensChooseMaskedRouter.jitter_noise = %JITTER_NOISE
I0512 23:41:54.535408 140031916288000 gin_utils.py:85] routing.TokensChooseMaskedRouter.num_selected_experts = %NUM_SELECTED_EXPERTS
I0512 23:41:54.535439 140031916288000 gin_utils.py:85] routing.TokensChooseMaskedRouter.router_weights = @routing.RouterWeights()
I0512 23:41:54.535469 140031916288000 gin_utils.py:85] 
I0512 23:41:54.535499 140031916288000 gin_utils.py:85] # Parameters for sparse_decoder/routing.TokensChooseMaskedRouter:
I0512 23:41:54.535530 140031916288000 gin_utils.py:85] # ==============================================================================
I0512 23:41:54.535560 140031916288000 gin_utils.py:85] sparse_decoder/routing.TokensChooseMaskedRouter.batch_prioritized_routing = False
I0512 23:41:54.535590 140031916288000 gin_utils.py:85] 
I0512 23:41:54.535621 140031916288000 gin_utils.py:85] # Parameters for sparse_encoder/routing.TokensChooseMaskedRouter:
I0512 23:41:54.535651 140031916288000 gin_utils.py:85] # ==============================================================================
I0512 23:41:54.535688 140031916288000 gin_utils.py:85] sparse_encoder/routing.TokensChooseMaskedRouter.batch_prioritized_routing = False
I0512 23:41:54.535719 140031916288000 gin_utils.py:85] 
I0512 23:41:54.535751 140031916288000 gin_utils.py:85] # Parameters for train_script.train:
I0512 23:41:54.535782 140031916288000 gin_utils.py:85] # ==============================================================================
I0512 23:41:54.535813 140031916288000 gin_utils.py:85] train_script.train.checkpoint_cfg = @utils.CheckpointConfig()
I0512 23:41:54.535845 140031916288000 gin_utils.py:85] train_script.train.eval_period = 2500
I0512 23:41:54.607161 140440593786880 gin_utils.py:85] dense.MlpBlock.kernel_init = @mlp_kernel_init/linen.initializers.variance_scaling()
I0512 23:41:54.607198 140440593786880 gin_utils.py:85] dense.MlpBlock.output_axis_name = 'mlp_embed'
I0512 23:41:54.607231 140440593786880 gin_utils.py:85] dense.MlpBlock.use_bias = False
I0512 23:41:54.607263 140440593786880 gin_utils.py:85] 
I0512 23:41:54.607295 140440593786880 gin_utils.py:85] # Parameters for expert/dense.MlpBlock:
I0512 23:41:54.607327 140440593786880 gin_utils.py:85] # ==============================================================================
I0512 23:41:54.607358 140440593786880 gin_utils.py:85] expert/dense.MlpBlock.activation_partitioning_dims = 1
I0512 23:41:54.607390 140440593786880 gin_utils.py:85] expert/dense.MlpBlock.activations = ('swiglu', 'linear')
I0512 23:41:54.607421 140440593786880 gin_utils.py:85] expert/dense.MlpBlock.bias_init = %BIAS_INIT
I0512 23:41:54.607453 140440593786880 gin_utils.py:85] expert/dense.MlpBlock.data_sharding_constraints = ('expert_replicas', 'mlp')
I0512 23:41:54.607484 140440593786880 gin_utils.py:85] expert/dense.MlpBlock.dtype = %MOE_TRUNCATED_DTYPE
I0512 23:41:54.607515 140440593786880 gin_utils.py:85] expert/dense.MlpBlock.final_dropout_rate = 0.0
I0512 23:41:54.607547 140440593786880 gin_utils.py:85] expert/dense.MlpBlock.input_axis_name = 'mlp_embed'
I0512 23:41:54.607578 140440593786880 gin_utils.py:85] expert/dense.MlpBlock.intermediate_axis_name = 'expert_mlp'
I0512 23:41:54.607610 140440593786880 gin_utils.py:85] expert/dense.MlpBlock.intermediate_dim = %EXPERT_MLP_DIM
I0512 23:41:54.607641 140440593786880 gin_utils.py:85] expert/dense.MlpBlock.intermediate_dropout_rate = %EXPERT_DROPOUT_RATE
I0512 23:41:54.607673 140440593786880 gin_utils.py:85] expert/dense.MlpBlock.kernel_init = \
I0512 23:41:54.607704 140440593786880 gin_utils.py:85]     @expert_kernel_init/linen.initializers.variance_scaling()
I0512 23:41:54.607736 140440593786880 gin_utils.py:85] expert/dense.MlpBlock.output_axis_name = 'mlp_embed'
I0512 23:41:54.720432 140529434355712 resource_reader.py:50] system_path_file_exists:flaxformer/flaxformer/t5x/configs/moe/models/st_moe_decoder_only_large.gin
E0512 23:41:54.720846 140529434355712 resource_reader.py:55] Path not found: flaxformer/flaxformer/t5x/configs/moe/models/st_moe_decoder_only_large.gin
I0512 23:41:54.720910 140529434355712 resource_reader.py:50] system_path_file_exists:./flaxformer/flaxformer/t5x/configs/moe/models/st_moe_decoder_only_large.gin
E0512 23:41:54.720958 140529434355712 resource_reader.py:55] Path not found: ./flaxformer/flaxformer/t5x/configs/moe/models/st_moe_decoder_only_large.gin
I0512 23:41:54.549687 139674595104768 gin_utils.py:85] expert/dense.MlpBlock.use_bias = False
I0512 23:41:54.549720 139674595104768 gin_utils.py:85] 
I0512 23:41:54.549752 139674595104768 gin_utils.py:85] # Parameters for models.MoeDecoderOnlyModel:
I0512 23:41:54.549784 139674595104768 gin_utils.py:85] # ==============================================================================
I0512 23:41:54.549816 139674595104768 gin_utils.py:85] models.MoeDecoderOnlyModel.aux_loss_factor = %AUX_LOSS_FACTOR
I0512 23:41:54.549848 139674595104768 gin_utils.py:85] models.MoeDecoderOnlyModel.label_smoothing = %LABEL_SMOOTHING
I0512 23:41:54.549880 139674595104768 gin_utils.py:85] models.MoeDecoderOnlyModel.loss_normalizing_factor = %LOSS_NORMALIZING_FACTOR
I0512 23:41:54.549913 139674595104768 gin_utils.py:85] models.MoeDecoderOnlyModel.module = %ARCHITECTURE
I0512 23:41:54.549947 139674595104768 gin_utils.py:85] models.MoeDecoderOnlyModel.optimizer_def = %OPTIMIZER
I0512 23:41:54.549979 139674595104768 gin_utils.py:85] models.MoeDecoderOnlyModel.router_z_loss_factor = %ROUTER_Z_LOSS_FACTOR
I0512 23:41:54.550012 139674595104768 gin_utils.py:85] models.MoeDecoderOnlyModel.vocabulary = %VOCABULARY
I0512 23:41:54.550044 139674595104768 gin_utils.py:85] models.MoeDecoderOnlyModel.z_loss = %Z_LOSS
I0512 23:41:54.550076 139674595104768 gin_utils.py:85] 
I0512 23:41:54.550108 139674595104768 gin_utils.py:85] # Parameters for moe_layers.MoeLayer:
I0512 23:41:54.550140 139674595104768 gin_utils.py:85] # ==============================================================================
I0512 23:41:54.550172 139674595104768 gin_utils.py:85] moe_layers.MoeLayer.dtype = %MOE_TRUNCATED_DTYPE
I0512 23:41:54.550204 139674595104768 gin_utils.py:85] moe_layers.MoeLayer.eval_capacity_factor = %EVAL_EXPERT_CAPACITY_FACTOR
I0512 23:41:54.550237 139674595104768 gin_utils.py:85] moe_layers.MoeLayer.expert = @expert/dense.MlpBlock()
I0512 23:41:54.550269 139674595104768 gin_utils.py:85] moe_layers.MoeLayer.max_group_size = %GROUP_SIZE
I0512 23:41:54.550301 139674595104768 gin_utils.py:85] moe_layers.MoeLayer.num_expert_partitions = %NUM_EXPERT_PARTITIONS
I0512 23:41:54.550333 139674595104768 gin_utils.py:85] moe_layers.MoeLayer.num_experts = %NUM_EXPERTS
I0512 23:41:54.550366 139674595104768 gin_utils.py:85] moe_layers.MoeLayer.num_model_partitions = \
I0512 23:41:54.550398 139674595104768 gin_utils.py:85]     @moe_partitioning.compute_num_model_partitions()
I0512 23:41:54.550430 139674595104768 gin_utils.py:85] moe_layers.MoeLayer.train_capacity_factor = %TRAIN_EXPERT_CAPACITY_FACTOR
I0512 23:41:54.550462 139674595104768 gin_utils.py:85] 
I0512 23:41:54.550494 139674595104768 gin_utils.py:85] # Parameters for sparse_decoder/moe_layers.MoeLayer:
I0512 23:41:54.550526 139674595104768 gin_utils.py:85] # ==============================================================================
I0512 23:41:54.550558 139674595104768 gin_utils.py:85] sparse_decoder/moe_layers.MoeLayer.router = \
I0512 23:41:54.550590 139674595104768 gin_utils.py:85]     @sparse_decoder/routing.TokensChooseMaskedRouter()
I0512 23:41:54.550622 139674595104768 gin_utils.py:85] 
I0512 23:41:54.550654 139674595104768 gin_utils.py:85] # Parameters for moe_partitioning.MoePjitPartitioner:
I0512 23:41:54.550692 139674595104768 gin_utils.py:85] # ==============================================================================
I0512 23:41:54.550725 139674595104768 gin_utils.py:85] moe_partitioning.MoePjitPartitioner.model_parallel_submesh = \
I0512 23:41:54.550757 139674595104768 gin_utils.py:85]     %MODEL_PARALLEL_SUBMESH
I0512 23:41:54.550789 139674595104768 gin_utils.py:85] moe_partitioning.MoePjitPartitioner.num_expert_partitions = %NUM_EXPERT_PARTITIONS
I0512 23:41:54.550821 139674595104768 gin_utils.py:85] moe_partitioning.MoePjitPartitioner.num_partitions = %NUM_MODEL_PARTITIONS
I0512 23:41:54.550853 139674595104768 gin_utils.py:85] 
I0512 23:41:54.550886 139674595104768 gin_utils.py:85] # Parameters for moe_trainer.MoeTrainer:
I0512 23:41:54.550919 139674595104768 gin_utils.py:85] # ==============================================================================
I0512 23:41:54.550952 139674595104768 gin_utils.py:85] moe_trainer.MoeTrainer.learning_rate_fn = @utils.create_learning_rate_scheduler()
I0512 23:41:54.550985 139674595104768 gin_utils.py:85] moe_trainer.MoeTrainer.num_expert_partitions = %NUM_EXPERT_PARTITIONS
I0512 23:41:54.551017 139674595104768 gin_utils.py:85] moe_trainer.MoeTrainer.num_microbatches = 8
I0512 23:41:54.551050 139674595104768 gin_utils.py:85] 
I0512 23:41:54.551082 139674595104768 gin_utils.py:85] # Parameters for dense_attention.MultiHeadDotProductAttention:
I0512 23:41:54.551114 139674595104768 gin_utils.py:85] # ==============================================================================
I0512 23:41:54.551146 139674595104768 gin_utils.py:85] dense_attention.MultiHeadDotProductAttention.bias_init = %BIAS_INIT
I0512 23:41:54.551179 139674595104768 gin_utils.py:85] dense_attention.MultiHeadDotProductAttention.broadcast_dropout = True
I0512 23:41:54.551211 139674595104768 gin_utils.py:85] dense_attention.MultiHeadDotProductAttention.dropout_rate = %DROPOUT_RATE
I0512 23:41:54.551244 139674595104768 gin_utils.py:85] dense_attention.MultiHeadDotProductAttention.dtype = %ACTIVATION_DTYPE
I0512 23:41:54.551276 139674595104768 gin_utils.py:85] dense_attention.MultiHeadDotProductAttention.head_dim = %HEAD_DIM
I0512 23:41:54.551334 139674595104768 gin_utils.py:85] dense_attention.MultiHeadDotProductAttention.kernel_init = \
I0512 23:41:54.551369 139674595104768 gin_utils.py:85]     @attention_kernel_init/linen.initializers.variance_scaling()
I0512 23:41:54.551402 139674595104768 gin_utils.py:85] dense_attention.MultiHeadDotProductAttention.num_heads = %NUM_HEADS
I0512 23:41:54.551434 139674595104768 gin_utils.py:85] dense_attention.MultiHeadDotProductAttention.use_bias = False
I0512 23:41:54.551466 139674595104768 gin_utils.py:85] dense_attention.MultiHeadDotProductAttention.use_rotary_embedding = True
I0512 23:41:54.551498 139674595104768 gin_utils.py:85] 
I0512 23:41:54.551530 139674595104768 gin_utils.py:85] # Parameters for bias_init/linen.initializers.normal:
I0512 23:41:54.551562 139674595104768 gin_utils.py:85] # ==============================================================================
I0512 23:41:54.551594 139674595104768 gin_utils.py:85] bias_init/linen.initializers.normal.stddev = 1e-06
I0512 23:41:54.551627 139674595104768 gin_utils.py:85] 
I0512 23:41:54.551665 139674595104768 gin_utils.py:85] # Parameters for router_init/linen.initializers.normal:
I0512 23:41:54.551699 139674595104768 gin_utils.py:85] # ==============================================================================
I0512 23:41:54.551732 139674595104768 gin_utils.py:85] router_init/linen.initializers.normal.stddev = 0.02
I0512 23:41:54.551764 139674595104768 gin_utils.py:85] 
I0512 23:41:54.551796 139674595104768 gin_utils.py:85] # Parameters for token_embedder_init/linen.initializers.normal:
I0512 23:41:54.551828 139674595104768 gin_utils.py:85] # ==============================================================================
I0512 23:41:54.551860 139674595104768 gin_utils.py:85] token_embedder_init/linen.initializers.normal.stddev = 1.0
I0512 23:41:54.551892 139674595104768 gin_utils.py:85] 
I0512 23:41:54.551927 139674595104768 gin_utils.py:85] # Parameters for partitioning.PjitPartitioner:
I0512 23:41:54.551959 139674595104768 gin_utils.py:85] # ==============================================================================
I0512 23:41:54.551992 139674595104768 gin_utils.py:85] partitioning.PjitPartitioner.logical_axis_rules = \
I0512 23:41:54.552024 139674595104768 gin_utils.py:85]     @partitioning.standard_logical_axis_rules()
I0512 23:41:54.552057 139674595104768 gin_utils.py:85] partitioning.PjitPartitioner.model_parallel_submesh = None
I0512 23:41:54.552089 139674595104768 gin_utils.py:85] partitioning.PjitPartitioner.num_partitions = 1
I0512 23:41:54.552121 139674595104768 gin_utils.py:85] 
I0512 23:41:54.552153 139674595104768 gin_utils.py:85] # Parameters for utils.RestoreCheckpointConfig:
I0512 23:41:54.552185 139674595104768 gin_utils.py:85] # ==============================================================================
I0512 23:41:54.552220 139674595104768 gin_utils.py:85] utils.RestoreCheckpointConfig.path = []
I0512 23:41:54.552254 139674595104768 gin_utils.py:85] 
I0512 23:41:54.552286 139674595104768 gin_utils.py:85] # Parameters for routing.RouterWeights:
I0512 23:41:54.552317 139674595104768 gin_utils.py:85] # ==============================================================================
I0512 23:41:54.552350 139674595104768 gin_utils.py:85] routing.RouterWeights.bias_init = %BIAS_INIT
I0512 23:41:54.552381 139674595104768 gin_utils.py:85] routing.RouterWeights.dtype = 'float32'
I0512 23:41:54.552413 139674595104768 gin_utils.py:85] routing.RouterWeights.kernel_init = @router_init/linen.initializers.normal()
I0512 23:41:54.552445 139674595104768 gin_utils.py:85] routing.RouterWeights.use_bias = False
I0512 23:41:54.552477 139674595104768 gin_utils.py:85] 
I0512 23:41:54.552510 139674595104768 gin_utils.py:85] # Parameters for utils.SaveCheckpointConfig:
I0512 23:41:54.552542 139674595104768 gin_utils.py:85] # ==============================================================================
I0512 23:41:54.552574 139674595104768 gin_utils.py:85] utils.SaveCheckpointConfig.dtype = 'float32'
I0512 23:41:54.552606 139674595104768 gin_utils.py:85] utils.SaveCheckpointConfig.keep = 20
I0512 23:41:54.552638 139674595104768 gin_utils.py:85] utils.SaveCheckpointConfig.period = 2500
I0512 23:41:54.552676 139674595104768 gin_utils.py:85] utils.SaveCheckpointConfig.save_dataset = False
I0512 23:41:54.552709 139674595104768 gin_utils.py:85] 
I0512 23:41:54.552741 139674595104768 gin_utils.py:85] # Parameters for seqio.SentencePieceVocabulary:
I0512 23:41:54.552773 139674595104768 gin_utils.py:85] # ==============================================================================
I0512 23:41:54.552804 139674595104768 gin_utils.py:85] seqio.SentencePieceVocabulary.extra_ids = 300
I0512 23:41:54.552837 139674595104768 gin_utils.py:85] seqio.SentencePieceVocabulary.sentencepiece_model_file = \
I0512 23:41:54.552869 139674595104768 gin_utils.py:85]     'gs://rosinality-tpu-bucket/sentencepiece.model'
I0512 23:41:54.552901 139674595104768 gin_utils.py:85] 
I0512 23:41:54.552935 139674595104768 gin_utils.py:85] # Parameters for moe_architecture.SparseDecoder:
I0512 23:41:54.552968 139674595104768 gin_utils.py:85] # ==============================================================================
I0512 23:41:54.553000 139674595104768 gin_utils.py:85] moe_architecture.SparseDecoder.dropout_factory = %DROPOUT_FACTORY
I0512 23:41:54.553032 139674595104768 gin_utils.py:85] moe_architecture.SparseDecoder.dtype = %ACTIVATION_DTYPE
I0512 23:41:54.553064 139674595104768 gin_utils.py:85] moe_architecture.SparseDecoder.layer_factory = @t5_architecture.DecoderLayer
I0512 23:41:54.553096 139674595104768 gin_utils.py:85] moe_architecture.SparseDecoder.layer_norm_factory = @layer_norm.T5LayerNorm
I0512 23:41:54.553128 139674595104768 gin_utils.py:85] moe_architecture.SparseDecoder.num_layers = %NUM_DECODER_LAYERS
I0512 23:41:54.553160 139674595104768 gin_utils.py:85] moe_architecture.SparseDecoder.num_sparse_layers = %NUM_DECODER_SPARSE_LAYERS
I0512 23:41:54.553193 139674595104768 gin_utils.py:85] moe_architecture.SparseDecoder.output_logits_factory = \
I0512 23:41:54.553225 139674595104768 gin_utils.py:85]     @output_logits/dense.DenseGeneral
I0512 23:41:54.553257 139674595104768 gin_utils.py:85] moe_architecture.SparseDecoder.sparse_layer_factory = \
I0512 23:41:54.553289 139674595104768 gin_utils.py:85]     @moe_architecture.SparseDecoderLayer
I0512 23:41:54.553321 139674595104768 gin_utils.py:85] moe_architecture.SparseDecoder.sparse_layout = %DECODER_SPARSE_LAYOUT
I0512 23:41:54.553353 139674595104768 gin_utils.py:85] 
I0512 23:41:54.553385 139674595104768 gin_utils.py:85] # Parameters for moe_architecture.SparseDecoderLayer:
I0512 23:41:54.553417 139674595104768 gin_utils.py:85] # ==============================================================================
I0512 23:41:54.553449 139674595104768 gin_utils.py:85] moe_architecture.SparseDecoderLayer.activation_partitioning_dims = \
I0512 23:41:54.553481 139674595104768 gin_utils.py:85]     %ACTIVATION_PARTITIONING_DIMS
I0512 23:41:54.553514 139674595104768 gin_utils.py:85] moe_architecture.SparseDecoderLayer.dropout_factory = %DROPOUT_FACTORY
I0512 23:41:54.553545 139674595104768 gin_utils.py:85] moe_architecture.SparseDecoderLayer.encoder_decoder_attention = None
I0512 23:41:54.553579 139674595104768 gin_utils.py:85] moe_architecture.SparseDecoderLayer.extra_mlp = @dense.MlpBlock()
I0512 23:41:54.553611 139674595104768 gin_utils.py:85] moe_architecture.SparseDecoderLayer.layer_norm_factory = @layer_norm.T5LayerNorm
I0512 23:41:54.553644 139674595104768 gin_utils.py:85] moe_architecture.SparseDecoderLayer.mlp = @sparse_decoder/moe_layers.MoeLayer()
I0512 23:41:54.553683 139674595104768 gin_utils.py:85] moe_architecture.SparseDecoderLayer.scanned = False
I0512 23:41:54.553716 139674595104768 gin_utils.py:85] moe_architecture.SparseDecoderLayer.self_attention = \
I0512 23:41:54.553749 139674595104768 gin_utils.py:85]     @dense_attention.MultiHeadDotProductAttention()
I0512 23:41:54.553781 139674595104768 gin_utils.py:85] 
I0512 23:41:54.553813 139674595104768 gin_utils.py:85] # Parameters for layer_norm.T5LayerNorm:
I0512 23:41:54.553846 139674595104768 gin_utils.py:85] # ==============================================================================
I0512 23:41:54.553878 139674595104768 gin_utils.py:85] layer_norm.T5LayerNorm.dtype = %ACTIVATION_DTYPE
I0512 23:41:54.553911 139674595104768 gin_utils.py:85] 
I0512 23:41:54.553945 139674595104768 gin_utils.py:85] # Parameters for routing.TokensChooseMaskedRouter:
I0512 23:41:54.553977 139674595104768 gin_utils.py:85] # ==============================================================================
I0512 23:41:54.554009 139674595104768 gin_utils.py:85] routing.TokensChooseMaskedRouter.dtype = 'float32'
I0512 23:41:54.554042 139674595104768 gin_utils.py:85] routing.TokensChooseMaskedRouter.ignore_padding_tokens = False
I0512 23:41:54.554074 139674595104768 gin_utils.py:85] routing.TokensChooseMaskedRouter.jitter_noise = %JITTER_NOISE
I0512 23:41:54.554105 139674595104768 gin_utils.py:85] routing.TokensChooseMaskedRouter.num_selected_experts = %NUM_SELECTED_EXPERTS
I0512 23:41:54.554138 139674595104768 gin_utils.py:85] routing.TokensChooseMaskedRouter.router_weights = @routing.RouterWeights()
I0512 23:41:54.554170 139674595104768 gin_utils.py:85] 
I0512 23:41:54.554203 139674595104768 gin_utils.py:85] # Parameters for sparse_decoder/routing.TokensChooseMaskedRouter:
I0512 23:41:54.554234 139674595104768 gin_utils.py:85] # ==============================================================================
I0512 23:41:54.554267 139674595104768 gin_utils.py:85] sparse_decoder/routing.TokensChooseMaskedRouter.batch_prioritized_routing = False
I0512 23:41:54.554299 139674595104768 gin_utils.py:85] 
I0512 23:41:54.554331 139674595104768 gin_utils.py:85] # Parameters for sparse_encoder/routing.TokensChooseMaskedRouter:
I0512 23:41:54.554363 139674595104768 gin_utils.py:85] # ==============================================================================
I0512 23:41:54.554395 139674595104768 gin_utils.py:85] sparse_encoder/routing.TokensChooseMaskedRouter.batch_prioritized_routing = False
I0512 23:41:54.554428 139674595104768 gin_utils.py:85] 
I0512 23:41:54.554461 139674595104768 gin_utils.py:85] # Parameters for train_script.train:
I0512 23:41:54.554493 139674595104768 gin_utils.py:85] # ==============================================================================
I0512 23:41:54.554525 139674595104768 gin_utils.py:85] train_script.train.checkpoint_cfg = @utils.CheckpointConfig()
I0512 23:41:54.554558 139674595104768 gin_utils.py:85] train_script.train.eval_period = 2500
I0512 23:41:54.621577 140095893518336 gin_utils.py:85] dense.MlpBlock.kernel_init = @mlp_kernel_init/linen.initializers.variance_scaling()
I0512 23:41:54.621608 140095893518336 gin_utils.py:85] dense.MlpBlock.output_axis_name = 'mlp_embed'
I0512 23:41:54.621639 140095893518336 gin_utils.py:85] dense.MlpBlock.use_bias = False
I0512 23:41:54.621670 140095893518336 gin_utils.py:85] 
I0512 23:41:54.621701 140095893518336 gin_utils.py:85] # Parameters for expert/dense.MlpBlock:
I0512 23:41:54.621731 140095893518336 gin_utils.py:85] # ==============================================================================
I0512 23:41:54.621762 140095893518336 gin_utils.py:85] expert/dense.MlpBlock.activation_partitioning_dims = 1
I0512 23:41:54.621793 140095893518336 gin_utils.py:85] expert/dense.MlpBlock.activations = ('swiglu', 'linear')
I0512 23:41:54.621824 140095893518336 gin_utils.py:85] expert/dense.MlpBlock.bias_init = %BIAS_INIT
I0512 23:41:54.621854 140095893518336 gin_utils.py:85] expert/dense.MlpBlock.data_sharding_constraints = ('expert_replicas', 'mlp')
I0512 23:41:54.621885 140095893518336 gin_utils.py:85] expert/dense.MlpBlock.dtype = %MOE_TRUNCATED_DTYPE
I0512 23:41:54.621916 140095893518336 gin_utils.py:85] expert/dense.MlpBlock.final_dropout_rate = 0.0
I0512 23:41:54.621946 140095893518336 gin_utils.py:85] expert/dense.MlpBlock.input_axis_name = 'mlp_embed'
I0512 23:41:54.621977 140095893518336 gin_utils.py:85] expert/dense.MlpBlock.intermediate_axis_name = 'expert_mlp'
I0512 23:41:54.622007 140095893518336 gin_utils.py:85] expert/dense.MlpBlock.intermediate_dim = %EXPERT_MLP_DIM
I0512 23:41:54.622038 140095893518336 gin_utils.py:85] expert/dense.MlpBlock.intermediate_dropout_rate = %EXPERT_DROPOUT_RATE
I0512 23:41:54.622069 140095893518336 gin_utils.py:85] expert/dense.MlpBlock.kernel_init = \
I0512 23:41:54.622100 140095893518336 gin_utils.py:85]     @expert_kernel_init/linen.initializers.variance_scaling()
I0512 23:41:54.622136 140095893518336 gin_utils.py:85] expert/dense.MlpBlock.output_axis_name = 'mlp_embed'
I0512 23:41:54.748684 140311617681408 resource_reader.py:50] system_path_file_exists:flaxformer/flaxformer/t5x/configs/moe/models/tokens_choose_decoder_only_large.gin
E0512 23:41:54.748867 140311617681408 resource_reader.py:55] Path not found: flaxformer/flaxformer/t5x/configs/moe/models/tokens_choose_decoder_only_large.gin
I0512 23:41:54.748924 140311617681408 resource_reader.py:50] system_path_file_exists:./flaxformer/flaxformer/t5x/configs/moe/models/tokens_choose_decoder_only_large.gin
E0512 23:41:54.748968 140311617681408 resource_reader.py:55] Path not found: ./flaxformer/flaxformer/t5x/configs/moe/models/tokens_choose_decoder_only_large.gin
I0512 23:41:54.749864 140311617681408 resource_reader.py:50] system_path_file_exists:flaxformer/flaxformer/t5x/configs/moe/architectures/moe_decoder_only.gin
E0512 23:41:54.750000 140311617681408 resource_reader.py:55] Path not found: flaxformer/flaxformer/t5x/configs/moe/architectures/moe_decoder_only.gin
I0512 23:41:54.750053 140311617681408 resource_reader.py:50] system_path_file_exists:./flaxformer/flaxformer/t5x/configs/moe/architectures/moe_decoder_only.gin
E0512 23:41:54.750093 140311617681408 resource_reader.py:55] Path not found: ./flaxformer/flaxformer/t5x/configs/moe/architectures/moe_decoder_only.gin
I0512 23:41:54.774501 140595318249472 resource_reader.py:50] system_path_file_exists:flaxformer/flaxformer/t5x/configs/moe/models/tokens_choose_decoder_only_large.gin
E0512 23:41:54.774705 140595318249472 resource_reader.py:55] Path not found: flaxformer/flaxformer/t5x/configs/moe/models/tokens_choose_decoder_only_large.gin
I0512 23:41:54.774762 140595318249472 resource_reader.py:50] system_path_file_exists:./flaxformer/flaxformer/t5x/configs/moe/models/tokens_choose_decoder_only_large.gin
E0512 23:41:54.774806 140595318249472 resource_reader.py:55] Path not found: ./flaxformer/flaxformer/t5x/configs/moe/models/tokens_choose_decoder_only_large.gin
I0512 23:41:54.775697 140595318249472 resource_reader.py:50] system_path_file_exists:flaxformer/flaxformer/t5x/configs/moe/architectures/moe_decoder_only.gin
E0512 23:41:54.775830 140595318249472 resource_reader.py:55] Path not found: flaxformer/flaxformer/t5x/configs/moe/architectures/moe_decoder_only.gin
I0512 23:41:54.775881 140595318249472 resource_reader.py:50] system_path_file_exists:./flaxformer/flaxformer/t5x/configs/moe/architectures/moe_decoder_only.gin
E0512 23:41:54.775920 140595318249472 resource_reader.py:55] Path not found: ./flaxformer/flaxformer/t5x/configs/moe/architectures/moe_decoder_only.gin
I0512 23:41:54.573216 140339465779200 gin_utils.py:85] expert/dense.MlpBlock.use_bias = False
I0512 23:41:54.573249 140339465779200 gin_utils.py:85] 
I0512 23:41:54.573282 140339465779200 gin_utils.py:85] # Parameters for models.MoeDecoderOnlyModel:
I0512 23:41:54.573314 140339465779200 gin_utils.py:85] # ==============================================================================
I0512 23:41:54.573347 140339465779200 gin_utils.py:85] models.MoeDecoderOnlyModel.aux_loss_factor = %AUX_LOSS_FACTOR
I0512 23:41:54.573386 140339465779200 gin_utils.py:85] models.MoeDecoderOnlyModel.label_smoothing = %LABEL_SMOOTHING
I0512 23:41:54.573420 140339465779200 gin_utils.py:85] models.MoeDecoderOnlyModel.loss_normalizing_factor = %LOSS_NORMALIZING_FACTOR
I0512 23:41:54.573453 140339465779200 gin_utils.py:85] models.MoeDecoderOnlyModel.module = %ARCHITECTURE
I0512 23:41:54.573486 140339465779200 gin_utils.py:85] models.MoeDecoderOnlyModel.optimizer_def = %OPTIMIZER
I0512 23:41:54.573519 140339465779200 gin_utils.py:85] models.MoeDecoderOnlyModel.router_z_loss_factor = %ROUTER_Z_LOSS_FACTOR
I0512 23:41:54.573552 140339465779200 gin_utils.py:85] models.MoeDecoderOnlyModel.vocabulary = %VOCABULARY
I0512 23:41:54.573584 140339465779200 gin_utils.py:85] models.MoeDecoderOnlyModel.z_loss = %Z_LOSS
I0512 23:41:54.573618 140339465779200 gin_utils.py:85] 
I0512 23:41:54.573653 140339465779200 gin_utils.py:85] # Parameters for moe_layers.MoeLayer:
I0512 23:41:54.573686 140339465779200 gin_utils.py:85] # ==============================================================================
I0512 23:41:54.573718 140339465779200 gin_utils.py:85] moe_layers.MoeLayer.dtype = %MOE_TRUNCATED_DTYPE
I0512 23:41:54.573759 140339465779200 gin_utils.py:85] moe_layers.MoeLayer.eval_capacity_factor = %EVAL_EXPERT_CAPACITY_FACTOR
I0512 23:41:54.573792 140339465779200 gin_utils.py:85] moe_layers.MoeLayer.expert = @expert/dense.MlpBlock()
I0512 23:41:54.573825 140339465779200 gin_utils.py:85] moe_layers.MoeLayer.max_group_size = %GROUP_SIZE
I0512 23:41:54.573858 140339465779200 gin_utils.py:85] moe_layers.MoeLayer.num_expert_partitions = %NUM_EXPERT_PARTITIONS
I0512 23:41:54.573890 140339465779200 gin_utils.py:85] moe_layers.MoeLayer.num_experts = %NUM_EXPERTS
I0512 23:41:54.573923 140339465779200 gin_utils.py:85] moe_layers.MoeLayer.num_model_partitions = \
I0512 23:41:54.573955 140339465779200 gin_utils.py:85]     @moe_partitioning.compute_num_model_partitions()
I0512 23:41:54.573987 140339465779200 gin_utils.py:85] moe_layers.MoeLayer.train_capacity_factor = %TRAIN_EXPERT_CAPACITY_FACTOR
I0512 23:41:54.574020 140339465779200 gin_utils.py:85] 
I0512 23:41:54.574052 140339465779200 gin_utils.py:85] # Parameters for sparse_decoder/moe_layers.MoeLayer:
I0512 23:41:54.574084 140339465779200 gin_utils.py:85] # ==============================================================================
I0512 23:41:54.574117 140339465779200 gin_utils.py:85] sparse_decoder/moe_layers.MoeLayer.router = \
I0512 23:41:54.574149 140339465779200 gin_utils.py:85]     @sparse_decoder/routing.TokensChooseMaskedRouter()
I0512 23:41:54.574181 140339465779200 gin_utils.py:85] 
I0512 23:41:54.574214 140339465779200 gin_utils.py:85] # Parameters for moe_partitioning.MoePjitPartitioner:
I0512 23:41:54.574246 140339465779200 gin_utils.py:85] # ==============================================================================
I0512 23:41:54.574279 140339465779200 gin_utils.py:85] moe_partitioning.MoePjitPartitioner.model_parallel_submesh = \
I0512 23:41:54.574311 140339465779200 gin_utils.py:85]     %MODEL_PARALLEL_SUBMESH
I0512 23:41:54.574344 140339465779200 gin_utils.py:85] moe_partitioning.MoePjitPartitioner.num_expert_partitions = %NUM_EXPERT_PARTITIONS
I0512 23:41:54.574383 140339465779200 gin_utils.py:85] moe_partitioning.MoePjitPartitioner.num_partitions = %NUM_MODEL_PARTITIONS
I0512 23:41:54.574416 140339465779200 gin_utils.py:85] 
I0512 23:41:54.574449 140339465779200 gin_utils.py:85] # Parameters for moe_trainer.MoeTrainer:
I0512 23:41:54.574481 140339465779200 gin_utils.py:85] # ==============================================================================
I0512 23:41:54.574514 140339465779200 gin_utils.py:85] moe_trainer.MoeTrainer.learning_rate_fn = @utils.create_learning_rate_scheduler()
I0512 23:41:54.574547 140339465779200 gin_utils.py:85] moe_trainer.MoeTrainer.num_expert_partitions = %NUM_EXPERT_PARTITIONS
I0512 23:41:54.574579 140339465779200 gin_utils.py:85] moe_trainer.MoeTrainer.num_microbatches = 8
I0512 23:41:54.574613 140339465779200 gin_utils.py:85] 
I0512 23:41:54.574647 140339465779200 gin_utils.py:85] # Parameters for dense_attention.MultiHeadDotProductAttention:
I0512 23:41:54.574680 140339465779200 gin_utils.py:85] # ==============================================================================
I0512 23:41:54.574712 140339465779200 gin_utils.py:85] dense_attention.MultiHeadDotProductAttention.bias_init = %BIAS_INIT
I0512 23:41:54.574745 140339465779200 gin_utils.py:85] dense_attention.MultiHeadDotProductAttention.broadcast_dropout = True
I0512 23:41:54.574777 140339465779200 gin_utils.py:85] dense_attention.MultiHeadDotProductAttention.dropout_rate = %DROPOUT_RATE
I0512 23:41:54.574809 140339465779200 gin_utils.py:85] dense_attention.MultiHeadDotProductAttention.dtype = %ACTIVATION_DTYPE
I0512 23:41:54.574842 140339465779200 gin_utils.py:85] dense_attention.MultiHeadDotProductAttention.head_dim = %HEAD_DIM
I0512 23:41:54.574874 140339465779200 gin_utils.py:85] dense_attention.MultiHeadDotProductAttention.kernel_init = \
I0512 23:41:54.574906 140339465779200 gin_utils.py:85]     @attention_kernel_init/linen.initializers.variance_scaling()
I0512 23:41:54.574938 140339465779200 gin_utils.py:85] dense_attention.MultiHeadDotProductAttention.num_heads = %NUM_HEADS
I0512 23:41:54.574970 140339465779200 gin_utils.py:85] dense_attention.MultiHeadDotProductAttention.use_bias = False
I0512 23:41:54.575002 140339465779200 gin_utils.py:85] dense_attention.MultiHeadDotProductAttention.use_rotary_embedding = True
I0512 23:41:54.575035 140339465779200 gin_utils.py:85] 
I0512 23:41:54.575067 140339465779200 gin_utils.py:85] # Parameters for bias_init/linen.initializers.normal:
I0512 23:41:54.575099 140339465779200 gin_utils.py:85] # ==============================================================================
I0512 23:41:54.575131 140339465779200 gin_utils.py:85] bias_init/linen.initializers.normal.stddev = 1e-06
I0512 23:41:54.575164 140339465779200 gin_utils.py:85] 
I0512 23:41:54.575197 140339465779200 gin_utils.py:85] # Parameters for router_init/linen.initializers.normal:
I0512 23:41:54.575231 140339465779200 gin_utils.py:85] # ==============================================================================
I0512 23:41:54.575263 140339465779200 gin_utils.py:85] router_init/linen.initializers.normal.stddev = 0.02
I0512 23:41:54.575296 140339465779200 gin_utils.py:85] 
I0512 23:41:54.575329 140339465779200 gin_utils.py:85] # Parameters for token_embedder_init/linen.initializers.normal:
I0512 23:41:54.575367 140339465779200 gin_utils.py:85] # ==============================================================================
I0512 23:41:54.575401 140339465779200 gin_utils.py:85] token_embedder_init/linen.initializers.normal.stddev = 1.0
I0512 23:41:54.575433 140339465779200 gin_utils.py:85] 
I0512 23:41:54.575465 140339465779200 gin_utils.py:85] # Parameters for partitioning.PjitPartitioner:
I0512 23:41:54.575497 140339465779200 gin_utils.py:85] # ==============================================================================
I0512 23:41:54.575530 140339465779200 gin_utils.py:85] partitioning.PjitPartitioner.logical_axis_rules = \
I0512 23:41:54.575563 140339465779200 gin_utils.py:85]     @partitioning.standard_logical_axis_rules()
I0512 23:41:54.575595 140339465779200 gin_utils.py:85] partitioning.PjitPartitioner.model_parallel_submesh = None
I0512 23:41:54.575632 140339465779200 gin_utils.py:85] partitioning.PjitPartitioner.num_partitions = 1
I0512 23:41:54.575668 140339465779200 gin_utils.py:85] 
I0512 23:41:54.575700 140339465779200 gin_utils.py:85] # Parameters for utils.RestoreCheckpointConfig:
I0512 23:41:54.575732 140339465779200 gin_utils.py:85] # ==============================================================================
I0512 23:41:54.575765 140339465779200 gin_utils.py:85] utils.RestoreCheckpointConfig.path = []
I0512 23:41:54.575797 140339465779200 gin_utils.py:85] 
I0512 23:41:54.575829 140339465779200 gin_utils.py:85] # Parameters for routing.RouterWeights:
I0512 23:41:54.575861 140339465779200 gin_utils.py:85] # ==============================================================================
I0512 23:41:54.575894 140339465779200 gin_utils.py:85] routing.RouterWeights.bias_init = %BIAS_INIT
I0512 23:41:54.575926 140339465779200 gin_utils.py:85] routing.RouterWeights.dtype = 'float32'
I0512 23:41:54.575959 140339465779200 gin_utils.py:85] routing.RouterWeights.kernel_init = @router_init/linen.initializers.normal()
I0512 23:41:54.575991 140339465779200 gin_utils.py:85] routing.RouterWeights.use_bias = False
I0512 23:41:54.576024 140339465779200 gin_utils.py:85] 
I0512 23:41:54.576056 140339465779200 gin_utils.py:85] # Parameters for utils.SaveCheckpointConfig:
I0512 23:41:54.576088 140339465779200 gin_utils.py:85] # ==============================================================================
I0512 23:41:54.576121 140339465779200 gin_utils.py:85] utils.SaveCheckpointConfig.dtype = 'float32'
I0512 23:41:54.576153 140339465779200 gin_utils.py:85] utils.SaveCheckpointConfig.keep = 20
I0512 23:41:54.576212 140339465779200 gin_utils.py:85] utils.SaveCheckpointConfig.period = 2500
I0512 23:41:54.576247 140339465779200 gin_utils.py:85] utils.SaveCheckpointConfig.save_dataset = False
I0512 23:41:54.576279 140339465779200 gin_utils.py:85] 
I0512 23:41:54.576312 140339465779200 gin_utils.py:85] # Parameters for seqio.SentencePieceVocabulary:
I0512 23:41:54.576344 140339465779200 gin_utils.py:85] # ==============================================================================
I0512 23:41:54.576383 140339465779200 gin_utils.py:85] seqio.SentencePieceVocabulary.extra_ids = 300
I0512 23:41:54.576416 140339465779200 gin_utils.py:85] seqio.SentencePieceVocabulary.sentencepiece_model_file = \
I0512 23:41:54.576449 140339465779200 gin_utils.py:85]     'gs://rosinality-tpu-bucket/sentencepiece.model'
I0512 23:41:54.576482 140339465779200 gin_utils.py:85] 
I0512 23:41:54.576514 140339465779200 gin_utils.py:85] # Parameters for moe_architecture.SparseDecoder:
I0512 23:41:54.576546 140339465779200 gin_utils.py:85] # ==============================================================================
I0512 23:41:54.576579 140339465779200 gin_utils.py:85] moe_architecture.SparseDecoder.dropout_factory = %DROPOUT_FACTORY
I0512 23:41:54.576612 140339465779200 gin_utils.py:85] moe_architecture.SparseDecoder.dtype = %ACTIVATION_DTYPE
I0512 23:41:54.576647 140339465779200 gin_utils.py:85] moe_architecture.SparseDecoder.layer_factory = @t5_architecture.DecoderLayer
I0512 23:41:54.576680 140339465779200 gin_utils.py:85] moe_architecture.SparseDecoder.layer_norm_factory = @layer_norm.T5LayerNorm
I0512 23:41:54.576712 140339465779200 gin_utils.py:85] moe_architecture.SparseDecoder.num_layers = %NUM_DECODER_LAYERS
I0512 23:41:54.576745 140339465779200 gin_utils.py:85] moe_architecture.SparseDecoder.num_sparse_layers = %NUM_DECODER_SPARSE_LAYERS
I0512 23:41:54.576777 140339465779200 gin_utils.py:85] moe_architecture.SparseDecoder.output_logits_factory = \
I0512 23:41:54.576810 140339465779200 gin_utils.py:85]     @output_logits/dense.DenseGeneral
I0512 23:41:54.576843 140339465779200 gin_utils.py:85] moe_architecture.SparseDecoder.sparse_layer_factory = \
I0512 23:41:54.576875 140339465779200 gin_utils.py:85]     @moe_architecture.SparseDecoderLayer
I0512 23:41:54.576908 140339465779200 gin_utils.py:85] moe_architecture.SparseDecoder.sparse_layout = %DECODER_SPARSE_LAYOUT
I0512 23:41:54.576940 140339465779200 gin_utils.py:85] 
I0512 23:41:54.576972 140339465779200 gin_utils.py:85] # Parameters for moe_architecture.SparseDecoderLayer:
I0512 23:41:54.577005 140339465779200 gin_utils.py:85] # ==============================================================================
I0512 23:41:54.577038 140339465779200 gin_utils.py:85] moe_architecture.SparseDecoderLayer.activation_partitioning_dims = \
I0512 23:41:54.577071 140339465779200 gin_utils.py:85]     %ACTIVATION_PARTITIONING_DIMS
I0512 23:41:54.577103 140339465779200 gin_utils.py:85] moe_architecture.SparseDecoderLayer.dropout_factory = %DROPOUT_FACTORY
I0512 23:41:54.577136 140339465779200 gin_utils.py:85] moe_architecture.SparseDecoderLayer.encoder_decoder_attention = None
I0512 23:41:54.577169 140339465779200 gin_utils.py:85] moe_architecture.SparseDecoderLayer.extra_mlp = @dense.MlpBlock()
I0512 23:41:54.577202 140339465779200 gin_utils.py:85] moe_architecture.SparseDecoderLayer.layer_norm_factory = @layer_norm.T5LayerNorm
I0512 23:41:54.577234 140339465779200 gin_utils.py:85] moe_architecture.SparseDecoderLayer.mlp = @sparse_decoder/moe_layers.MoeLayer()
I0512 23:41:54.577267 140339465779200 gin_utils.py:85] moe_architecture.SparseDecoderLayer.scanned = False
I0512 23:41:54.577299 140339465779200 gin_utils.py:85] moe_architecture.SparseDecoderLayer.self_attention = \
I0512 23:41:54.577332 140339465779200 gin_utils.py:85]     @dense_attention.MultiHeadDotProductAttention()
I0512 23:41:54.577370 140339465779200 gin_utils.py:85] 
I0512 23:41:54.577405 140339465779200 gin_utils.py:85] # Parameters for layer_norm.T5LayerNorm:
I0512 23:41:54.577439 140339465779200 gin_utils.py:85] # ==============================================================================
I0512 23:41:54.577471 140339465779200 gin_utils.py:85] layer_norm.T5LayerNorm.dtype = %ACTIVATION_DTYPE
I0512 23:41:54.577503 140339465779200 gin_utils.py:85] 
I0512 23:41:54.577536 140339465779200 gin_utils.py:85] # Parameters for routing.TokensChooseMaskedRouter:
I0512 23:41:54.577568 140339465779200 gin_utils.py:85] # ==============================================================================
I0512 23:41:54.577600 140339465779200 gin_utils.py:85] routing.TokensChooseMaskedRouter.dtype = 'float32'
I0512 23:41:54.577636 140339465779200 gin_utils.py:85] routing.TokensChooseMaskedRouter.ignore_padding_tokens = False
I0512 23:41:54.577670 140339465779200 gin_utils.py:85] routing.TokensChooseMaskedRouter.jitter_noise = %JITTER_NOISE
I0512 23:41:54.577703 140339465779200 gin_utils.py:85] routing.TokensChooseMaskedRouter.num_selected_experts = %NUM_SELECTED_EXPERTS
I0512 23:41:54.577735 140339465779200 gin_utils.py:85] routing.TokensChooseMaskedRouter.router_weights = @routing.RouterWeights()
I0512 23:41:54.577768 140339465779200 gin_utils.py:85] 
I0512 23:41:54.577800 140339465779200 gin_utils.py:85] # Parameters for sparse_decoder/routing.TokensChooseMaskedRouter:
I0512 23:41:54.577833 140339465779200 gin_utils.py:85] # ==============================================================================
I0512 23:41:54.577866 140339465779200 gin_utils.py:85] sparse_decoder/routing.TokensChooseMaskedRouter.batch_prioritized_routing = False
I0512 23:41:54.577898 140339465779200 gin_utils.py:85] 
I0512 23:41:54.577931 140339465779200 gin_utils.py:85] # Parameters for sparse_encoder/routing.TokensChooseMaskedRouter:
I0512 23:41:54.577964 140339465779200 gin_utils.py:85] # ==============================================================================
I0512 23:41:54.577996 140339465779200 gin_utils.py:85] sparse_encoder/routing.TokensChooseMaskedRouter.batch_prioritized_routing = False
I0512 23:41:54.578029 140339465779200 gin_utils.py:85] 
I0512 23:41:54.578062 140339465779200 gin_utils.py:85] # Parameters for train_script.train:
I0512 23:41:54.578096 140339465779200 gin_utils.py:85] # ==============================================================================
I0512 23:41:54.578129 140339465779200 gin_utils.py:85] train_script.train.checkpoint_cfg = @utils.CheckpointConfig()
I0512 23:41:54.578162 140339465779200 gin_utils.py:85] train_script.train.eval_period = 2500
I0512 23:41:54.535876 140031916288000 gin_utils.py:85] train_script.train.eval_steps = 20
I0512 23:41:54.535906 140031916288000 gin_utils.py:85] train_script.train.infer_eval_dataset_cfg = None
I0512 23:41:54.535939 140031916288000 gin_utils.py:85] train_script.train.model = %MODEL
I0512 23:41:54.535971 140031916288000 gin_utils.py:85] train_script.train.model_dir = %MODEL_DIR
I0512 23:41:54.536002 140031916288000 gin_utils.py:85] train_script.train.partitioner = @moe_partitioning.MoePjitPartitioner()
I0512 23:41:54.536032 140031916288000 gin_utils.py:85] train_script.train.random_seed = %RANDOM_SEED
I0512 23:41:54.536064 140031916288000 gin_utils.py:85] train_script.train.stats_period = 10
I0512 23:41:54.536094 140031916288000 gin_utils.py:85] train_script.train.summarize_config_fn = @gin_utils.summarize_gin_config
I0512 23:41:54.536125 140031916288000 gin_utils.py:85] train_script.train.total_steps = %TRAIN_STEPS
I0512 23:41:54.536156 140031916288000 gin_utils.py:85] train_script.train.train_dataset_cfg = @train/utils.DatasetConfig()
I0512 23:41:54.536187 140031916288000 gin_utils.py:85] train_script.train.train_eval_dataset_cfg = None
I0512 23:41:54.536218 140031916288000 gin_utils.py:85] train_script.train.trainer_cls = @moe_trainer.MoeTrainer
I0512 23:41:54.536249 140031916288000 gin_utils.py:85] train_script.train.use_hardware_rng = %USE_HARDWARE_RNG
I0512 23:41:54.536279 140031916288000 gin_utils.py:85] 
I0512 23:41:54.536310 140031916288000 gin_utils.py:85] # Parameters for trainer.Trainer:
I0512 23:41:54.536340 140031916288000 gin_utils.py:85] # ==============================================================================
I0512 23:41:54.536371 140031916288000 gin_utils.py:85] trainer.Trainer.learning_rate_fn = @utils.create_learning_rate_scheduler()
I0512 23:41:54.536402 140031916288000 gin_utils.py:85] trainer.Trainer.num_microbatches = None
I0512 23:41:54.536433 140031916288000 gin_utils.py:85] 
I0512 23:41:54.536463 140031916288000 gin_utils.py:85] # Parameters for attention_kernel_init/linen.initializers.variance_scaling:
I0512 23:41:54.536494 140031916288000 gin_utils.py:85] # ==============================================================================
I0512 23:41:54.536524 140031916288000 gin_utils.py:85] attention_kernel_init/linen.initializers.variance_scaling.distribution = 'normal'
I0512 23:41:54.536555 140031916288000 gin_utils.py:85] attention_kernel_init/linen.initializers.variance_scaling.mode = 'fan_in'
I0512 23:41:54.536586 140031916288000 gin_utils.py:85] attention_kernel_init/linen.initializers.variance_scaling.scale = %SCALE
I0512 23:41:54.536617 140031916288000 gin_utils.py:85] 
I0512 23:41:54.536648 140031916288000 gin_utils.py:85] # Parameters for expert_kernel_init/linen.initializers.variance_scaling:
I0512 23:41:54.536684 140031916288000 gin_utils.py:85] # ==============================================================================
I0512 23:41:54.536715 140031916288000 gin_utils.py:85] expert_kernel_init/linen.initializers.variance_scaling.distribution = 'normal'
I0512 23:41:54.536746 140031916288000 gin_utils.py:85] expert_kernel_init/linen.initializers.variance_scaling.mode = 'fan_in'
I0512 23:41:54.536777 140031916288000 gin_utils.py:85] expert_kernel_init/linen.initializers.variance_scaling.scale = %SCALE
I0512 23:41:54.536808 140031916288000 gin_utils.py:85] 
I0512 23:41:54.536839 140031916288000 gin_utils.py:85] # Parameters for mlp_kernel_init/linen.initializers.variance_scaling:
I0512 23:41:54.536870 140031916288000 gin_utils.py:85] # ==============================================================================
I0512 23:41:54.536900 140031916288000 gin_utils.py:85] mlp_kernel_init/linen.initializers.variance_scaling.distribution = \
I0512 23:41:54.536933 140031916288000 gin_utils.py:85]     'truncated_normal'
I0512 23:41:54.536965 140031916288000 gin_utils.py:85] mlp_kernel_init/linen.initializers.variance_scaling.mode = 'fan_in'
I0512 23:41:54.536996 140031916288000 gin_utils.py:85] mlp_kernel_init/linen.initializers.variance_scaling.scale = %SCALE
I0512 23:41:54.537027 140031916288000 gin_utils.py:85] 
I0512 23:41:54.537058 140031916288000 gin_utils.py:85] # Parameters for output_logits_kernel_init/linen.initializers.variance_scaling:
I0512 23:41:54.537089 140031916288000 gin_utils.py:85] # ==============================================================================
I0512 23:41:54.537120 140031916288000 gin_utils.py:85] output_logits_kernel_init/linen.initializers.variance_scaling.distribution = \
I0512 23:41:54.537150 140031916288000 gin_utils.py:85]     'truncated_normal'
I0512 23:41:54.537181 140031916288000 gin_utils.py:85] output_logits_kernel_init/linen.initializers.variance_scaling.mode = 'fan_in'
I0512 23:41:54.537212 140031916288000 gin_utils.py:85] output_logits_kernel_init/linen.initializers.variance_scaling.scale = %SCALE
I0512 23:41:54.538131 140031916288000 partitioning.py:559] `activation_partitioning_dims` = 2, `parameter_partitioning_dims` = 1
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
I0000 00:00:1715557314.628568  453630 pjrt_api.cc:98] GetPjrtApi was found for tpu at /home/rosinality/openmoe_venv/lib/python3.10/site-packages/libtpu/libtpu.so
I0000 00:00:1715557314.628618  453630 pjrt_api.cc:67] PJRT_Api is set for device type tpu
I0000 00:00:1715557314.628621  453630 pjrt_api.cc:72] PJRT plugin for tpu has PJRT API version 0.30. The framework PJRT API version is 0.30.
I0512 23:41:54.777678 140529434355712 resource_reader.py:50] system_path_file_exists:flaxformer/flaxformer/t5x/configs/moe/models/tokens_choose_decoder_only_large.gin
E0512 23:41:54.777865 140529434355712 resource_reader.py:55] Path not found: flaxformer/flaxformer/t5x/configs/moe/models/tokens_choose_decoder_only_large.gin
I0512 23:41:54.777925 140529434355712 resource_reader.py:50] system_path_file_exists:./flaxformer/flaxformer/t5x/configs/moe/models/tokens_choose_decoder_only_large.gin
E0512 23:41:54.777969 140529434355712 resource_reader.py:55] Path not found: ./flaxformer/flaxformer/t5x/configs/moe/models/tokens_choose_decoder_only_large.gin
I0512 23:41:54.778826 140529434355712 resource_reader.py:50] system_path_file_exists:flaxformer/flaxformer/t5x/configs/moe/architectures/moe_decoder_only.gin
E0512 23:41:54.778957 140529434355712 resource_reader.py:55] Path not found: flaxformer/flaxformer/t5x/configs/moe/architectures/moe_decoder_only.gin
I0512 23:41:54.779010 140529434355712 resource_reader.py:50] system_path_file_exists:./flaxformer/flaxformer/t5x/configs/moe/architectures/moe_decoder_only.gin
E0512 23:41:54.779050 140529434355712 resource_reader.py:55] Path not found: ./flaxformer/flaxformer/t5x/configs/moe/architectures/moe_decoder_only.gin
I0512 23:41:54.607768 140440593786880 gin_utils.py:85] expert/dense.MlpBlock.use_bias = False
I0512 23:41:54.607799 140440593786880 gin_utils.py:85] 
I0512 23:41:54.607831 140440593786880 gin_utils.py:85] # Parameters for models.MoeDecoderOnlyModel:
I0512 23:41:54.607862 140440593786880 gin_utils.py:85] # ==============================================================================
I0512 23:41:54.607894 140440593786880 gin_utils.py:85] models.MoeDecoderOnlyModel.aux_loss_factor = %AUX_LOSS_FACTOR
I0512 23:41:54.607925 140440593786880 gin_utils.py:85] models.MoeDecoderOnlyModel.label_smoothing = %LABEL_SMOOTHING
I0512 23:41:54.607959 140440593786880 gin_utils.py:85] models.MoeDecoderOnlyModel.loss_normalizing_factor = %LOSS_NORMALIZING_FACTOR
I0512 23:41:54.607992 140440593786880 gin_utils.py:85] models.MoeDecoderOnlyModel.module = %ARCHITECTURE
I0512 23:41:54.608024 140440593786880 gin_utils.py:85] models.MoeDecoderOnlyModel.optimizer_def = %OPTIMIZER
I0512 23:41:54.608055 140440593786880 gin_utils.py:85] models.MoeDecoderOnlyModel.router_z_loss_factor = %ROUTER_Z_LOSS_FACTOR
I0512 23:41:54.608087 140440593786880 gin_utils.py:85] models.MoeDecoderOnlyModel.vocabulary = %VOCABULARY
I0512 23:41:54.608119 140440593786880 gin_utils.py:85] models.MoeDecoderOnlyModel.z_loss = %Z_LOSS
I0512 23:41:54.608150 140440593786880 gin_utils.py:85] 
I0512 23:41:54.608181 140440593786880 gin_utils.py:85] # Parameters for moe_layers.MoeLayer:
I0512 23:41:54.608220 140440593786880 gin_utils.py:85] # ==============================================================================
I0512 23:41:54.608252 140440593786880 gin_utils.py:85] moe_layers.MoeLayer.dtype = %MOE_TRUNCATED_DTYPE
I0512 23:41:54.608284 140440593786880 gin_utils.py:85] moe_layers.MoeLayer.eval_capacity_factor = %EVAL_EXPERT_CAPACITY_FACTOR
I0512 23:41:54.608316 140440593786880 gin_utils.py:85] moe_layers.MoeLayer.expert = @expert/dense.MlpBlock()
I0512 23:41:54.608347 140440593786880 gin_utils.py:85] moe_layers.MoeLayer.max_group_size = %GROUP_SIZE
I0512 23:41:54.608378 140440593786880 gin_utils.py:85] moe_layers.MoeLayer.num_expert_partitions = %NUM_EXPERT_PARTITIONS
I0512 23:41:54.608431 140440593786880 gin_utils.py:85] moe_layers.MoeLayer.num_experts = %NUM_EXPERTS
I0512 23:41:54.608468 140440593786880 gin_utils.py:85] moe_layers.MoeLayer.num_model_partitions = \
I0512 23:41:54.608501 140440593786880 gin_utils.py:85]     @moe_partitioning.compute_num_model_partitions()
I0512 23:41:54.608533 140440593786880 gin_utils.py:85] moe_layers.MoeLayer.train_capacity_factor = %TRAIN_EXPERT_CAPACITY_FACTOR
I0512 23:41:54.608565 140440593786880 gin_utils.py:85] 
I0512 23:41:54.608596 140440593786880 gin_utils.py:85] # Parameters for sparse_decoder/moe_layers.MoeLayer:
I0512 23:41:54.608628 140440593786880 gin_utils.py:85] # ==============================================================================
I0512 23:41:54.608659 140440593786880 gin_utils.py:85] sparse_decoder/moe_layers.MoeLayer.router = \
I0512 23:41:54.608691 140440593786880 gin_utils.py:85]     @sparse_decoder/routing.TokensChooseMaskedRouter()
I0512 23:41:54.608722 140440593786880 gin_utils.py:85] 
I0512 23:41:54.608754 140440593786880 gin_utils.py:85] # Parameters for moe_partitioning.MoePjitPartitioner:
I0512 23:41:54.608785 140440593786880 gin_utils.py:85] # ==============================================================================
I0512 23:41:54.608817 140440593786880 gin_utils.py:85] moe_partitioning.MoePjitPartitioner.model_parallel_submesh = \
I0512 23:41:54.608848 140440593786880 gin_utils.py:85]     %MODEL_PARALLEL_SUBMESH
I0512 23:41:54.608880 140440593786880 gin_utils.py:85] moe_partitioning.MoePjitPartitioner.num_expert_partitions = %NUM_EXPERT_PARTITIONS
I0512 23:41:54.608911 140440593786880 gin_utils.py:85] moe_partitioning.MoePjitPartitioner.num_partitions = %NUM_MODEL_PARTITIONS
I0512 23:41:54.608943 140440593786880 gin_utils.py:85] 
I0512 23:41:54.608977 140440593786880 gin_utils.py:85] # Parameters for moe_trainer.MoeTrainer:
I0512 23:41:54.609009 140440593786880 gin_utils.py:85] # ==============================================================================
I0512 23:41:54.609040 140440593786880 gin_utils.py:85] moe_trainer.MoeTrainer.learning_rate_fn = @utils.create_learning_rate_scheduler()
I0512 23:41:54.609071 140440593786880 gin_utils.py:85] moe_trainer.MoeTrainer.num_expert_partitions = %NUM_EXPERT_PARTITIONS
I0512 23:41:54.609103 140440593786880 gin_utils.py:85] moe_trainer.MoeTrainer.num_microbatches = 8
I0512 23:41:54.609135 140440593786880 gin_utils.py:85] 
I0512 23:41:54.609166 140440593786880 gin_utils.py:85] # Parameters for dense_attention.MultiHeadDotProductAttention:
I0512 23:41:54.609204 140440593786880 gin_utils.py:85] # ==============================================================================
I0512 23:41:54.609237 140440593786880 gin_utils.py:85] dense_attention.MultiHeadDotProductAttention.bias_init = %BIAS_INIT
I0512 23:41:54.609270 140440593786880 gin_utils.py:85] dense_attention.MultiHeadDotProductAttention.broadcast_dropout = True
I0512 23:41:54.609302 140440593786880 gin_utils.py:85] dense_attention.MultiHeadDotProductAttention.dropout_rate = %DROPOUT_RATE
I0512 23:41:54.609333 140440593786880 gin_utils.py:85] dense_attention.MultiHeadDotProductAttention.dtype = %ACTIVATION_DTYPE
I0512 23:41:54.609365 140440593786880 gin_utils.py:85] dense_attention.MultiHeadDotProductAttention.head_dim = %HEAD_DIM
I0512 23:41:54.609396 140440593786880 gin_utils.py:85] dense_attention.MultiHeadDotProductAttention.kernel_init = \
I0512 23:41:54.609428 140440593786880 gin_utils.py:85]     @attention_kernel_init/linen.initializers.variance_scaling()
I0512 23:41:54.609460 140440593786880 gin_utils.py:85] dense_attention.MultiHeadDotProductAttention.num_heads = %NUM_HEADS
I0512 23:41:54.609492 140440593786880 gin_utils.py:85] dense_attention.MultiHeadDotProductAttention.use_bias = False
I0512 23:41:54.609523 140440593786880 gin_utils.py:85] dense_attention.MultiHeadDotProductAttention.use_rotary_embedding = True
I0512 23:41:54.609555 140440593786880 gin_utils.py:85] 
I0512 23:41:54.609587 140440593786880 gin_utils.py:85] # Parameters for bias_init/linen.initializers.normal:
I0512 23:41:54.609618 140440593786880 gin_utils.py:85] # ==============================================================================
I0512 23:41:54.609649 140440593786880 gin_utils.py:85] bias_init/linen.initializers.normal.stddev = 1e-06
I0512 23:41:54.609681 140440593786880 gin_utils.py:85] 
I0512 23:41:54.609713 140440593786880 gin_utils.py:85] # Parameters for router_init/linen.initializers.normal:
I0512 23:41:54.609745 140440593786880 gin_utils.py:85] # ==============================================================================
I0512 23:41:54.609777 140440593786880 gin_utils.py:85] router_init/linen.initializers.normal.stddev = 0.02
I0512 23:41:54.609808 140440593786880 gin_utils.py:85] 
I0512 23:41:54.609840 140440593786880 gin_utils.py:85] # Parameters for token_embedder_init/linen.initializers.normal:
I0512 23:41:54.609871 140440593786880 gin_utils.py:85] # ==============================================================================
I0512 23:41:54.609903 140440593786880 gin_utils.py:85] token_embedder_init/linen.initializers.normal.stddev = 1.0
I0512 23:41:54.609934 140440593786880 gin_utils.py:85] 
I0512 23:41:54.609968 140440593786880 gin_utils.py:85] # Parameters for partitioning.PjitPartitioner:
I0512 23:41:54.610001 140440593786880 gin_utils.py:85] # ==============================================================================
I0512 23:41:54.610033 140440593786880 gin_utils.py:85] partitioning.PjitPartitioner.logical_axis_rules = \
I0512 23:41:54.610064 140440593786880 gin_utils.py:85]     @partitioning.standard_logical_axis_rules()
I0512 23:41:54.610095 140440593786880 gin_utils.py:85] partitioning.PjitPartitioner.model_parallel_submesh = None
I0512 23:41:54.610127 140440593786880 gin_utils.py:85] partitioning.PjitPartitioner.num_partitions = 1
I0512 23:41:54.610159 140440593786880 gin_utils.py:85] 
I0512 23:41:54.610196 140440593786880 gin_utils.py:85] # Parameters for utils.RestoreCheckpointConfig:
I0512 23:41:54.610229 140440593786880 gin_utils.py:85] # ==============================================================================
I0512 23:41:54.610261 140440593786880 gin_utils.py:85] utils.RestoreCheckpointConfig.path = []
I0512 23:41:54.610293 140440593786880 gin_utils.py:85] 
I0512 23:41:54.610325 140440593786880 gin_utils.py:85] # Parameters for routing.RouterWeights:
I0512 23:41:54.610356 140440593786880 gin_utils.py:85] # ==============================================================================
I0512 23:41:54.610388 140440593786880 gin_utils.py:85] routing.RouterWeights.bias_init = %BIAS_INIT
I0512 23:41:54.610419 140440593786880 gin_utils.py:85] routing.RouterWeights.dtype = 'float32'
I0512 23:41:54.610451 140440593786880 gin_utils.py:85] routing.RouterWeights.kernel_init = @router_init/linen.initializers.normal()
I0512 23:41:54.610482 140440593786880 gin_utils.py:85] routing.RouterWeights.use_bias = False
I0512 23:41:54.610514 140440593786880 gin_utils.py:85] 
I0512 23:41:54.610545 140440593786880 gin_utils.py:85] # Parameters for utils.SaveCheckpointConfig:
I0512 23:41:54.610577 140440593786880 gin_utils.py:85] # ==============================================================================
I0512 23:41:54.610609 140440593786880 gin_utils.py:85] utils.SaveCheckpointConfig.dtype = 'float32'
I0512 23:41:54.610640 140440593786880 gin_utils.py:85] utils.SaveCheckpointConfig.keep = 20
I0512 23:41:54.610671 140440593786880 gin_utils.py:85] utils.SaveCheckpointConfig.period = 2500
I0512 23:41:54.610702 140440593786880 gin_utils.py:85] utils.SaveCheckpointConfig.save_dataset = False
I0512 23:41:54.610734 140440593786880 gin_utils.py:85] 
I0512 23:41:54.610766 140440593786880 gin_utils.py:85] # Parameters for seqio.SentencePieceVocabulary:
I0512 23:41:54.610797 140440593786880 gin_utils.py:85] # ==============================================================================
I0512 23:41:54.610828 140440593786880 gin_utils.py:85] seqio.SentencePieceVocabulary.extra_ids = 300
I0512 23:41:54.610860 140440593786880 gin_utils.py:85] seqio.SentencePieceVocabulary.sentencepiece_model_file = \
I0512 23:41:54.610891 140440593786880 gin_utils.py:85]     'gs://rosinality-tpu-bucket/sentencepiece.model'
I0512 23:41:54.610923 140440593786880 gin_utils.py:85] 
I0512 23:41:54.610956 140440593786880 gin_utils.py:85] # Parameters for moe_architecture.SparseDecoder:
I0512 23:41:54.610989 140440593786880 gin_utils.py:85] # ==============================================================================
I0512 23:41:54.611021 140440593786880 gin_utils.py:85] moe_architecture.SparseDecoder.dropout_factory = %DROPOUT_FACTORY
I0512 23:41:54.611052 140440593786880 gin_utils.py:85] moe_architecture.SparseDecoder.dtype = %ACTIVATION_DTYPE
I0512 23:41:54.611084 140440593786880 gin_utils.py:85] moe_architecture.SparseDecoder.layer_factory = @t5_architecture.DecoderLayer
I0512 23:41:54.611115 140440593786880 gin_utils.py:85] moe_architecture.SparseDecoder.layer_norm_factory = @layer_norm.T5LayerNorm
I0512 23:41:54.611147 140440593786880 gin_utils.py:85] moe_architecture.SparseDecoder.num_layers = %NUM_DECODER_LAYERS
I0512 23:41:54.611179 140440593786880 gin_utils.py:85] moe_architecture.SparseDecoder.num_sparse_layers = %NUM_DECODER_SPARSE_LAYERS
I0512 23:41:54.611217 140440593786880 gin_utils.py:85] moe_architecture.SparseDecoder.output_logits_factory = \
I0512 23:41:54.611249 140440593786880 gin_utils.py:85]     @output_logits/dense.DenseGeneral
I0512 23:41:54.611281 140440593786880 gin_utils.py:85] moe_architecture.SparseDecoder.sparse_layer_factory = \
I0512 23:41:54.611312 140440593786880 gin_utils.py:85]     @moe_architecture.SparseDecoderLayer
I0512 23:41:54.611344 140440593786880 gin_utils.py:85] moe_architecture.SparseDecoder.sparse_layout = %DECODER_SPARSE_LAYOUT
I0512 23:41:54.611375 140440593786880 gin_utils.py:85] 
I0512 23:41:54.611407 140440593786880 gin_utils.py:85] # Parameters for moe_architecture.SparseDecoderLayer:
I0512 23:41:54.611439 140440593786880 gin_utils.py:85] # ==============================================================================
I0512 23:41:54.611471 140440593786880 gin_utils.py:85] moe_architecture.SparseDecoderLayer.activation_partitioning_dims = \
I0512 23:41:54.611502 140440593786880 gin_utils.py:85]     %ACTIVATION_PARTITIONING_DIMS
I0512 23:41:54.611534 140440593786880 gin_utils.py:85] moe_architecture.SparseDecoderLayer.dropout_factory = %DROPOUT_FACTORY
I0512 23:41:54.611566 140440593786880 gin_utils.py:85] moe_architecture.SparseDecoderLayer.encoder_decoder_attention = None
I0512 23:41:54.611598 140440593786880 gin_utils.py:85] moe_architecture.SparseDecoderLayer.extra_mlp = @dense.MlpBlock()
I0512 23:41:54.611630 140440593786880 gin_utils.py:85] moe_architecture.SparseDecoderLayer.layer_norm_factory = @layer_norm.T5LayerNorm
I0512 23:41:54.611662 140440593786880 gin_utils.py:85] moe_architecture.SparseDecoderLayer.mlp = @sparse_decoder/moe_layers.MoeLayer()
I0512 23:41:54.611694 140440593786880 gin_utils.py:85] moe_architecture.SparseDecoderLayer.scanned = False
I0512 23:41:54.611726 140440593786880 gin_utils.py:85] moe_architecture.SparseDecoderLayer.self_attention = \
I0512 23:41:54.611758 140440593786880 gin_utils.py:85]     @dense_attention.MultiHeadDotProductAttention()
I0512 23:41:54.611790 140440593786880 gin_utils.py:85] 
I0512 23:41:54.611822 140440593786880 gin_utils.py:85] # Parameters for layer_norm.T5LayerNorm:
I0512 23:41:54.611854 140440593786880 gin_utils.py:85] # ==============================================================================
I0512 23:41:54.611886 140440593786880 gin_utils.py:85] layer_norm.T5LayerNorm.dtype = %ACTIVATION_DTYPE
I0512 23:41:54.611917 140440593786880 gin_utils.py:85] 
I0512 23:41:54.611949 140440593786880 gin_utils.py:85] # Parameters for routing.TokensChooseMaskedRouter:
I0512 23:41:54.611982 140440593786880 gin_utils.py:85] # ==============================================================================
I0512 23:41:54.612014 140440593786880 gin_utils.py:85] routing.TokensChooseMaskedRouter.dtype = 'float32'
I0512 23:41:54.612045 140440593786880 gin_utils.py:85] routing.TokensChooseMaskedRouter.ignore_padding_tokens = False
I0512 23:41:54.612076 140440593786880 gin_utils.py:85] routing.TokensChooseMaskedRouter.jitter_noise = %JITTER_NOISE
I0512 23:41:54.612107 140440593786880 gin_utils.py:85] routing.TokensChooseMaskedRouter.num_selected_experts = %NUM_SELECTED_EXPERTS
I0512 23:41:54.612139 140440593786880 gin_utils.py:85] routing.TokensChooseMaskedRouter.router_weights = @routing.RouterWeights()
I0512 23:41:54.612170 140440593786880 gin_utils.py:85] 
I0512 23:41:54.612208 140440593786880 gin_utils.py:85] # Parameters for sparse_decoder/routing.TokensChooseMaskedRouter:
I0512 23:41:54.612240 140440593786880 gin_utils.py:85] # ==============================================================================
I0512 23:41:54.612272 140440593786880 gin_utils.py:85] sparse_decoder/routing.TokensChooseMaskedRouter.batch_prioritized_routing = False
I0512 23:41:54.612303 140440593786880 gin_utils.py:85] 
I0512 23:41:54.612334 140440593786880 gin_utils.py:85] # Parameters for sparse_encoder/routing.TokensChooseMaskedRouter:
I0512 23:41:54.612366 140440593786880 gin_utils.py:85] # ==============================================================================
I0512 23:41:54.612397 140440593786880 gin_utils.py:85] sparse_encoder/routing.TokensChooseMaskedRouter.batch_prioritized_routing = False
I0512 23:41:54.612454 140440593786880 gin_utils.py:85] 
I0512 23:41:54.612488 140440593786880 gin_utils.py:85] # Parameters for train_script.train:
I0512 23:41:54.612521 140440593786880 gin_utils.py:85] # ==============================================================================
I0512 23:41:54.612552 140440593786880 gin_utils.py:85] train_script.train.checkpoint_cfg = @utils.CheckpointConfig()
I0512 23:41:54.612584 140440593786880 gin_utils.py:85] train_script.train.eval_period = 2500
I0512 23:41:54.794332 140311617681408 resource_reader.py:50] system_path_file_exists:t5x/contrib/moe/configs/runs/pretrain.gin
I0512 23:41:54.794707 140311617681408 resource_reader.py:37] gin-config opened resource file:/home/rosinality/t5x/t5x/contrib/moe/configs/runs/pretrain.gin
I0512 23:41:54.622168 140095893518336 gin_utils.py:85] expert/dense.MlpBlock.use_bias = False
I0512 23:41:54.622199 140095893518336 gin_utils.py:85] 
I0512 23:41:54.622230 140095893518336 gin_utils.py:85] # Parameters for models.MoeDecoderOnlyModel:
I0512 23:41:54.622280 140095893518336 gin_utils.py:85] # ==============================================================================
I0512 23:41:54.622317 140095893518336 gin_utils.py:85] models.MoeDecoderOnlyModel.aux_loss_factor = %AUX_LOSS_FACTOR
I0512 23:41:54.622349 140095893518336 gin_utils.py:85] models.MoeDecoderOnlyModel.label_smoothing = %LABEL_SMOOTHING
I0512 23:41:54.622381 140095893518336 gin_utils.py:85] models.MoeDecoderOnlyModel.loss_normalizing_factor = %LOSS_NORMALIZING_FACTOR
I0512 23:41:54.622413 140095893518336 gin_utils.py:85] models.MoeDecoderOnlyModel.module = %ARCHITECTURE
I0512 23:41:54.622445 140095893518336 gin_utils.py:85] models.MoeDecoderOnlyModel.optimizer_def = %OPTIMIZER
I0512 23:41:54.622476 140095893518336 gin_utils.py:85] models.MoeDecoderOnlyModel.router_z_loss_factor = %ROUTER_Z_LOSS_FACTOR
I0512 23:41:54.622507 140095893518336 gin_utils.py:85] models.MoeDecoderOnlyModel.vocabulary = %VOCABULARY
I0512 23:41:54.622539 140095893518336 gin_utils.py:85] models.MoeDecoderOnlyModel.z_loss = %Z_LOSS
I0512 23:41:54.622569 140095893518336 gin_utils.py:85] 
I0512 23:41:54.622600 140095893518336 gin_utils.py:85] # Parameters for moe_layers.MoeLayer:
I0512 23:41:54.622631 140095893518336 gin_utils.py:85] # ==============================================================================
I0512 23:41:54.622662 140095893518336 gin_utils.py:85] moe_layers.MoeLayer.dtype = %MOE_TRUNCATED_DTYPE
I0512 23:41:54.622692 140095893518336 gin_utils.py:85] moe_layers.MoeLayer.eval_capacity_factor = %EVAL_EXPERT_CAPACITY_FACTOR
I0512 23:41:54.622724 140095893518336 gin_utils.py:85] moe_layers.MoeLayer.expert = @expert/dense.MlpBlock()
I0512 23:41:54.622754 140095893518336 gin_utils.py:85] moe_layers.MoeLayer.max_group_size = %GROUP_SIZE
I0512 23:41:54.622785 140095893518336 gin_utils.py:85] moe_layers.MoeLayer.num_expert_partitions = %NUM_EXPERT_PARTITIONS
I0512 23:41:54.622816 140095893518336 gin_utils.py:85] moe_layers.MoeLayer.num_experts = %NUM_EXPERTS
I0512 23:41:54.622847 140095893518336 gin_utils.py:85] moe_layers.MoeLayer.num_model_partitions = \
I0512 23:41:54.622878 140095893518336 gin_utils.py:85]     @moe_partitioning.compute_num_model_partitions()
I0512 23:41:54.622908 140095893518336 gin_utils.py:85] moe_layers.MoeLayer.train_capacity_factor = %TRAIN_EXPERT_CAPACITY_FACTOR
I0512 23:41:54.622939 140095893518336 gin_utils.py:85] 
I0512 23:41:54.622970 140095893518336 gin_utils.py:85] # Parameters for sparse_decoder/moe_layers.MoeLayer:
I0512 23:41:54.623001 140095893518336 gin_utils.py:85] # ==============================================================================
I0512 23:41:54.623032 140095893518336 gin_utils.py:85] sparse_decoder/moe_layers.MoeLayer.router = \
I0512 23:41:54.623062 140095893518336 gin_utils.py:85]     @sparse_decoder/routing.TokensChooseMaskedRouter()
I0512 23:41:54.623093 140095893518336 gin_utils.py:85] 
I0512 23:41:54.623129 140095893518336 gin_utils.py:85] # Parameters for moe_partitioning.MoePjitPartitioner:
I0512 23:41:54.623161 140095893518336 gin_utils.py:85] # ==============================================================================
I0512 23:41:54.623193 140095893518336 gin_utils.py:85] moe_partitioning.MoePjitPartitioner.model_parallel_submesh = \
I0512 23:41:54.623224 140095893518336 gin_utils.py:85]     %MODEL_PARALLEL_SUBMESH
I0512 23:41:54.623255 140095893518336 gin_utils.py:85] moe_partitioning.MoePjitPartitioner.num_expert_partitions = %NUM_EXPERT_PARTITIONS
I0512 23:41:54.623286 140095893518336 gin_utils.py:85] moe_partitioning.MoePjitPartitioner.num_partitions = %NUM_MODEL_PARTITIONS
I0512 23:41:54.623317 140095893518336 gin_utils.py:85] 
I0512 23:41:54.623347 140095893518336 gin_utils.py:85] # Parameters for moe_trainer.MoeTrainer:
I0512 23:41:54.623379 140095893518336 gin_utils.py:85] # ==============================================================================
I0512 23:41:54.623412 140095893518336 gin_utils.py:85] moe_trainer.MoeTrainer.learning_rate_fn = @utils.create_learning_rate_scheduler()
I0512 23:41:54.623443 140095893518336 gin_utils.py:85] moe_trainer.MoeTrainer.num_expert_partitions = %NUM_EXPERT_PARTITIONS
I0512 23:41:54.623474 140095893518336 gin_utils.py:85] moe_trainer.MoeTrainer.num_microbatches = 8
I0512 23:41:54.623505 140095893518336 gin_utils.py:85] 
I0512 23:41:54.623536 140095893518336 gin_utils.py:85] # Parameters for dense_attention.MultiHeadDotProductAttention:
I0512 23:41:54.623567 140095893518336 gin_utils.py:85] # ==============================================================================
I0512 23:41:54.623598 140095893518336 gin_utils.py:85] dense_attention.MultiHeadDotProductAttention.bias_init = %BIAS_INIT
I0512 23:41:54.623629 140095893518336 gin_utils.py:85] dense_attention.MultiHeadDotProductAttention.broadcast_dropout = True
I0512 23:41:54.623660 140095893518336 gin_utils.py:85] dense_attention.MultiHeadDotProductAttention.dropout_rate = %DROPOUT_RATE
I0512 23:41:54.623691 140095893518336 gin_utils.py:85] dense_attention.MultiHeadDotProductAttention.dtype = %ACTIVATION_DTYPE
I0512 23:41:54.623722 140095893518336 gin_utils.py:85] dense_attention.MultiHeadDotProductAttention.head_dim = %HEAD_DIM
I0512 23:41:54.623753 140095893518336 gin_utils.py:85] dense_attention.MultiHeadDotProductAttention.kernel_init = \
I0512 23:41:54.623784 140095893518336 gin_utils.py:85]     @attention_kernel_init/linen.initializers.variance_scaling()
I0512 23:41:54.623815 140095893518336 gin_utils.py:85] dense_attention.MultiHeadDotProductAttention.num_heads = %NUM_HEADS
I0512 23:41:54.623846 140095893518336 gin_utils.py:85] dense_attention.MultiHeadDotProductAttention.use_bias = False
I0512 23:41:54.623877 140095893518336 gin_utils.py:85] dense_attention.MultiHeadDotProductAttention.use_rotary_embedding = True
I0512 23:41:54.623908 140095893518336 gin_utils.py:85] 
I0512 23:41:54.623939 140095893518336 gin_utils.py:85] # Parameters for bias_init/linen.initializers.normal:
I0512 23:41:54.623970 140095893518336 gin_utils.py:85] # ==============================================================================
I0512 23:41:54.624001 140095893518336 gin_utils.py:85] bias_init/linen.initializers.normal.stddev = 1e-06
I0512 23:41:54.624032 140095893518336 gin_utils.py:85] 
I0512 23:41:54.624063 140095893518336 gin_utils.py:85] # Parameters for router_init/linen.initializers.normal:
I0512 23:41:54.624094 140095893518336 gin_utils.py:85] # ==============================================================================
I0512 23:41:54.624130 140095893518336 gin_utils.py:85] router_init/linen.initializers.normal.stddev = 0.02
I0512 23:41:54.624164 140095893518336 gin_utils.py:85] 
I0512 23:41:54.624195 140095893518336 gin_utils.py:85] # Parameters for token_embedder_init/linen.initializers.normal:
I0512 23:41:54.624226 140095893518336 gin_utils.py:85] # ==============================================================================
I0512 23:41:54.624258 140095893518336 gin_utils.py:85] token_embedder_init/linen.initializers.normal.stddev = 1.0
I0512 23:41:54.624288 140095893518336 gin_utils.py:85] 
I0512 23:41:54.624319 140095893518336 gin_utils.py:85] # Parameters for partitioning.PjitPartitioner:
I0512 23:41:54.624350 140095893518336 gin_utils.py:85] # ==============================================================================
I0512 23:41:54.624382 140095893518336 gin_utils.py:85] partitioning.PjitPartitioner.logical_axis_rules = \
I0512 23:41:54.624414 140095893518336 gin_utils.py:85]     @partitioning.standard_logical_axis_rules()
I0512 23:41:54.624445 140095893518336 gin_utils.py:85] partitioning.PjitPartitioner.model_parallel_submesh = None
I0512 23:41:54.624476 140095893518336 gin_utils.py:85] partitioning.PjitPartitioner.num_partitions = 1
I0512 23:41:54.624507 140095893518336 gin_utils.py:85] 
I0512 23:41:54.624537 140095893518336 gin_utils.py:85] # Parameters for utils.RestoreCheckpointConfig:
I0512 23:41:54.624568 140095893518336 gin_utils.py:85] # ==============================================================================
I0512 23:41:54.624599 140095893518336 gin_utils.py:85] utils.RestoreCheckpointConfig.path = []
I0512 23:41:54.624629 140095893518336 gin_utils.py:85] 
I0512 23:41:54.624660 140095893518336 gin_utils.py:85] # Parameters for routing.RouterWeights:
I0512 23:41:54.624691 140095893518336 gin_utils.py:85] # ==============================================================================
I0512 23:41:54.624721 140095893518336 gin_utils.py:85] routing.RouterWeights.bias_init = %BIAS_INIT
I0512 23:41:54.624752 140095893518336 gin_utils.py:85] routing.RouterWeights.dtype = 'float32'
I0512 23:41:54.624782 140095893518336 gin_utils.py:85] routing.RouterWeights.kernel_init = @router_init/linen.initializers.normal()
I0512 23:41:54.624813 140095893518336 gin_utils.py:85] routing.RouterWeights.use_bias = False
I0512 23:41:54.624844 140095893518336 gin_utils.py:85] 
I0512 23:41:54.624874 140095893518336 gin_utils.py:85] # Parameters for utils.SaveCheckpointConfig:
I0512 23:41:54.624905 140095893518336 gin_utils.py:85] # ==============================================================================
I0512 23:41:54.624936 140095893518336 gin_utils.py:85] utils.SaveCheckpointConfig.dtype = 'float32'
I0512 23:41:54.624966 140095893518336 gin_utils.py:85] utils.SaveCheckpointConfig.keep = 20
I0512 23:41:54.624997 140095893518336 gin_utils.py:85] utils.SaveCheckpointConfig.period = 2500
I0512 23:41:54.625028 140095893518336 gin_utils.py:85] utils.SaveCheckpointConfig.save_dataset = False
I0512 23:41:54.625058 140095893518336 gin_utils.py:85] 
I0512 23:41:54.625089 140095893518336 gin_utils.py:85] # Parameters for seqio.SentencePieceVocabulary:
I0512 23:41:54.625120 140095893518336 gin_utils.py:85] # ==============================================================================
I0512 23:41:54.625158 140095893518336 gin_utils.py:85] seqio.SentencePieceVocabulary.extra_ids = 300
I0512 23:41:54.625190 140095893518336 gin_utils.py:85] seqio.SentencePieceVocabulary.sentencepiece_model_file = \
I0512 23:41:54.625221 140095893518336 gin_utils.py:85]     'gs://rosinality-tpu-bucket/sentencepiece.model'
I0512 23:41:54.625252 140095893518336 gin_utils.py:85] 
I0512 23:41:54.625282 140095893518336 gin_utils.py:85] # Parameters for moe_architecture.SparseDecoder:
I0512 23:41:54.625313 140095893518336 gin_utils.py:85] # ==============================================================================
I0512 23:41:54.625344 140095893518336 gin_utils.py:85] moe_architecture.SparseDecoder.dropout_factory = %DROPOUT_FACTORY
I0512 23:41:54.625375 140095893518336 gin_utils.py:85] moe_architecture.SparseDecoder.dtype = %ACTIVATION_DTYPE
I0512 23:41:54.625408 140095893518336 gin_utils.py:85] moe_architecture.SparseDecoder.layer_factory = @t5_architecture.DecoderLayer
I0512 23:41:54.625439 140095893518336 gin_utils.py:85] moe_architecture.SparseDecoder.layer_norm_factory = @layer_norm.T5LayerNorm
I0512 23:41:54.625470 140095893518336 gin_utils.py:85] moe_architecture.SparseDecoder.num_layers = %NUM_DECODER_LAYERS
I0512 23:41:54.625500 140095893518336 gin_utils.py:85] moe_architecture.SparseDecoder.num_sparse_layers = %NUM_DECODER_SPARSE_LAYERS
I0512 23:41:54.625531 140095893518336 gin_utils.py:85] moe_architecture.SparseDecoder.output_logits_factory = \
I0512 23:41:54.625562 140095893518336 gin_utils.py:85]     @output_logits/dense.DenseGeneral
I0512 23:41:54.625593 140095893518336 gin_utils.py:85] moe_architecture.SparseDecoder.sparse_layer_factory = \
I0512 23:41:54.625623 140095893518336 gin_utils.py:85]     @moe_architecture.SparseDecoderLayer
I0512 23:41:54.625653 140095893518336 gin_utils.py:85] moe_architecture.SparseDecoder.sparse_layout = %DECODER_SPARSE_LAYOUT
I0512 23:41:54.625684 140095893518336 gin_utils.py:85] 
I0512 23:41:54.625715 140095893518336 gin_utils.py:85] # Parameters for moe_architecture.SparseDecoderLayer:
I0512 23:41:54.625746 140095893518336 gin_utils.py:85] # ==============================================================================
I0512 23:41:54.625776 140095893518336 gin_utils.py:85] moe_architecture.SparseDecoderLayer.activation_partitioning_dims = \
I0512 23:41:54.625807 140095893518336 gin_utils.py:85]     %ACTIVATION_PARTITIONING_DIMS
I0512 23:41:54.625837 140095893518336 gin_utils.py:85] moe_architecture.SparseDecoderLayer.dropout_factory = %DROPOUT_FACTORY
I0512 23:41:54.625868 140095893518336 gin_utils.py:85] moe_architecture.SparseDecoderLayer.encoder_decoder_attention = None
I0512 23:41:54.625900 140095893518336 gin_utils.py:85] moe_architecture.SparseDecoderLayer.extra_mlp = @dense.MlpBlock()
I0512 23:41:54.625930 140095893518336 gin_utils.py:85] moe_architecture.SparseDecoderLayer.layer_norm_factory = @layer_norm.T5LayerNorm
I0512 23:41:54.625961 140095893518336 gin_utils.py:85] moe_architecture.SparseDecoderLayer.mlp = @sparse_decoder/moe_layers.MoeLayer()
I0512 23:41:54.625993 140095893518336 gin_utils.py:85] moe_architecture.SparseDecoderLayer.scanned = False
I0512 23:41:54.626024 140095893518336 gin_utils.py:85] moe_architecture.SparseDecoderLayer.self_attention = \
I0512 23:41:54.626054 140095893518336 gin_utils.py:85]     @dense_attention.MultiHeadDotProductAttention()
I0512 23:41:54.626084 140095893518336 gin_utils.py:85] 
I0512 23:41:54.626115 140095893518336 gin_utils.py:85] # Parameters for layer_norm.T5LayerNorm:
I0512 23:41:54.626152 140095893518336 gin_utils.py:85] # ==============================================================================
I0512 23:41:54.626183 140095893518336 gin_utils.py:85] layer_norm.T5LayerNorm.dtype = %ACTIVATION_DTYPE
I0512 23:41:54.626214 140095893518336 gin_utils.py:85] 
I0512 23:41:54.626245 140095893518336 gin_utils.py:85] # Parameters for routing.TokensChooseMaskedRouter:
I0512 23:41:54.626296 140095893518336 gin_utils.py:85] # ==============================================================================
I0512 23:41:54.626330 140095893518336 gin_utils.py:85] routing.TokensChooseMaskedRouter.dtype = 'float32'
I0512 23:41:54.626361 140095893518336 gin_utils.py:85] routing.TokensChooseMaskedRouter.ignore_padding_tokens = False
I0512 23:41:54.626394 140095893518336 gin_utils.py:85] routing.TokensChooseMaskedRouter.jitter_noise = %JITTER_NOISE
I0512 23:41:54.626425 140095893518336 gin_utils.py:85] routing.TokensChooseMaskedRouter.num_selected_experts = %NUM_SELECTED_EXPERTS
I0512 23:41:54.626456 140095893518336 gin_utils.py:85] routing.TokensChooseMaskedRouter.router_weights = @routing.RouterWeights()
I0512 23:41:54.626487 140095893518336 gin_utils.py:85] 
I0512 23:41:54.626517 140095893518336 gin_utils.py:85] # Parameters for sparse_decoder/routing.TokensChooseMaskedRouter:
I0512 23:41:54.626548 140095893518336 gin_utils.py:85] # ==============================================================================
I0512 23:41:54.626579 140095893518336 gin_utils.py:85] sparse_decoder/routing.TokensChooseMaskedRouter.batch_prioritized_routing = False
I0512 23:41:54.626610 140095893518336 gin_utils.py:85] 
I0512 23:41:54.626641 140095893518336 gin_utils.py:85] # Parameters for sparse_encoder/routing.TokensChooseMaskedRouter:
I0512 23:41:54.626672 140095893518336 gin_utils.py:85] # ==============================================================================
I0512 23:41:54.626703 140095893518336 gin_utils.py:85] sparse_encoder/routing.TokensChooseMaskedRouter.batch_prioritized_routing = False
I0512 23:41:54.626734 140095893518336 gin_utils.py:85] 
I0512 23:41:54.626765 140095893518336 gin_utils.py:85] # Parameters for train_script.train:
I0512 23:41:54.626797 140095893518336 gin_utils.py:85] # ==============================================================================
I0512 23:41:54.626829 140095893518336 gin_utils.py:85] train_script.train.checkpoint_cfg = @utils.CheckpointConfig()
I0512 23:41:54.626861 140095893518336 gin_utils.py:85] train_script.train.eval_period = 2500
I0512 23:41:54.820194 140595318249472 resource_reader.py:50] system_path_file_exists:t5x/contrib/moe/configs/runs/pretrain.gin
I0512 23:41:54.823091 140529434355712 resource_reader.py:50] system_path_file_exists:t5x/contrib/moe/configs/runs/pretrain.gin
I0512 23:41:54.823467 140529434355712 resource_reader.py:37] gin-config opened resource file:/home/rosinality/t5x/t5x/contrib/moe/configs/runs/pretrain.gin
I0512 23:41:54.578195 140339465779200 gin_utils.py:85] train_script.train.eval_steps = 20
I0512 23:41:54.578228 140339465779200 gin_utils.py:85] train_script.train.infer_eval_dataset_cfg = None
I0512 23:41:54.578261 140339465779200 gin_utils.py:85] train_script.train.model = %MODEL
I0512 23:41:54.578294 140339465779200 gin_utils.py:85] train_script.train.model_dir = %MODEL_DIR
I0512 23:41:54.578326 140339465779200 gin_utils.py:85] train_script.train.partitioner = @moe_partitioning.MoePjitPartitioner()
I0512 23:41:54.578364 140339465779200 gin_utils.py:85] train_script.train.random_seed = %RANDOM_SEED
I0512 23:41:54.578399 140339465779200 gin_utils.py:85] train_script.train.stats_period = 10
I0512 23:41:54.578432 140339465779200 gin_utils.py:85] train_script.train.summarize_config_fn = @gin_utils.summarize_gin_config
I0512 23:41:54.578465 140339465779200 gin_utils.py:85] train_script.train.total_steps = %TRAIN_STEPS
I0512 23:41:54.578498 140339465779200 gin_utils.py:85] train_script.train.train_dataset_cfg = @train/utils.DatasetConfig()
I0512 23:41:54.578531 140339465779200 gin_utils.py:85] train_script.train.train_eval_dataset_cfg = None
I0512 23:41:54.578563 140339465779200 gin_utils.py:85] train_script.train.trainer_cls = @moe_trainer.MoeTrainer
I0512 23:41:54.578596 140339465779200 gin_utils.py:85] train_script.train.use_hardware_rng = %USE_HARDWARE_RNG
I0512 23:41:54.578631 140339465779200 gin_utils.py:85] 
I0512 23:41:54.578665 140339465779200 gin_utils.py:85] # Parameters for trainer.Trainer:
I0512 23:41:54.578697 140339465779200 gin_utils.py:85] # ==============================================================================
I0512 23:41:54.578731 140339465779200 gin_utils.py:85] trainer.Trainer.learning_rate_fn = @utils.create_learning_rate_scheduler()
I0512 23:41:54.578763 140339465779200 gin_utils.py:85] trainer.Trainer.num_microbatches = None
I0512 23:41:54.578797 140339465779200 gin_utils.py:85] 
I0512 23:41:54.578829 140339465779200 gin_utils.py:85] # Parameters for attention_kernel_init/linen.initializers.variance_scaling:
I0512 23:41:54.578862 140339465779200 gin_utils.py:85] # ==============================================================================
I0512 23:41:54.578895 140339465779200 gin_utils.py:85] attention_kernel_init/linen.initializers.variance_scaling.distribution = 'normal'
I0512 23:41:54.578928 140339465779200 gin_utils.py:85] attention_kernel_init/linen.initializers.variance_scaling.mode = 'fan_in'
I0512 23:41:54.578961 140339465779200 gin_utils.py:85] attention_kernel_init/linen.initializers.variance_scaling.scale = %SCALE
I0512 23:41:54.578994 140339465779200 gin_utils.py:85] 
I0512 23:41:54.579027 140339465779200 gin_utils.py:85] # Parameters for expert_kernel_init/linen.initializers.variance_scaling:
I0512 23:41:54.579060 140339465779200 gin_utils.py:85] # ==============================================================================
I0512 23:41:54.579093 140339465779200 gin_utils.py:85] expert_kernel_init/linen.initializers.variance_scaling.distribution = 'normal'
I0512 23:41:54.579126 140339465779200 gin_utils.py:85] expert_kernel_init/linen.initializers.variance_scaling.mode = 'fan_in'
I0512 23:41:54.579159 140339465779200 gin_utils.py:85] expert_kernel_init/linen.initializers.variance_scaling.scale = %SCALE
I0512 23:41:54.579191 140339465779200 gin_utils.py:85] 
I0512 23:41:54.579224 140339465779200 gin_utils.py:85] # Parameters for mlp_kernel_init/linen.initializers.variance_scaling:
I0512 23:41:54.579257 140339465779200 gin_utils.py:85] # ==============================================================================
I0512 23:41:54.579289 140339465779200 gin_utils.py:85] mlp_kernel_init/linen.initializers.variance_scaling.distribution = \
I0512 23:41:54.579322 140339465779200 gin_utils.py:85]     'truncated_normal'
I0512 23:41:54.579354 140339465779200 gin_utils.py:85] mlp_kernel_init/linen.initializers.variance_scaling.mode = 'fan_in'
I0512 23:41:54.579395 140339465779200 gin_utils.py:85] mlp_kernel_init/linen.initializers.variance_scaling.scale = %SCALE
I0512 23:41:54.579427 140339465779200 gin_utils.py:85] 
I0512 23:41:54.579460 140339465779200 gin_utils.py:85] # Parameters for output_logits_kernel_init/linen.initializers.variance_scaling:
I0512 23:41:54.579493 140339465779200 gin_utils.py:85] # ==============================================================================
I0512 23:41:54.579525 140339465779200 gin_utils.py:85] output_logits_kernel_init/linen.initializers.variance_scaling.distribution = \
I0512 23:41:54.579558 140339465779200 gin_utils.py:85]     'truncated_normal'
I0512 23:41:54.579591 140339465779200 gin_utils.py:85] output_logits_kernel_init/linen.initializers.variance_scaling.mode = 'fan_in'
I0512 23:41:54.579626 140339465779200 gin_utils.py:85] output_logits_kernel_init/linen.initializers.variance_scaling.scale = %SCALE
I0512 23:41:54.580575 140339465779200 partitioning.py:559] `activation_partitioning_dims` = 2, `parameter_partitioning_dims` = 1
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
I0000 00:00:1715557314.646663  449796 pjrt_api.cc:98] GetPjrtApi was found for tpu at /home/rosinality/openmoe_venv/lib/python3.10/site-packages/libtpu/libtpu.so
I0000 00:00:1715557314.646729  449796 pjrt_api.cc:67] PJRT_Api is set for device type tpu
I0000 00:00:1715557314.646731  449796 pjrt_api.cc:72] PJRT plugin for tpu has PJRT API version 0.30. The framework PJRT API version is 0.30.
I0512 23:41:54.612616 140440593786880 gin_utils.py:85] train_script.train.eval_steps = 20
I0512 23:41:54.612647 140440593786880 gin_utils.py:85] train_script.train.infer_eval_dataset_cfg = None
I0512 23:41:54.612679 140440593786880 gin_utils.py:85] train_script.train.model = %MODEL
I0512 23:41:54.612710 140440593786880 gin_utils.py:85] train_script.train.model_dir = %MODEL_DIR
I0512 23:41:54.612741 140440593786880 gin_utils.py:85] train_script.train.partitioner = @moe_partitioning.MoePjitPartitioner()
I0512 23:41:54.612773 140440593786880 gin_utils.py:85] train_script.train.random_seed = %RANDOM_SEED
I0512 23:41:54.612804 140440593786880 gin_utils.py:85] train_script.train.stats_period = 10
I0512 23:41:54.612836 140440593786880 gin_utils.py:85] train_script.train.summarize_config_fn = @gin_utils.summarize_gin_config
I0512 23:41:54.612867 140440593786880 gin_utils.py:85] train_script.train.total_steps = %TRAIN_STEPS
I0512 23:41:54.612898 140440593786880 gin_utils.py:85] train_script.train.train_dataset_cfg = @train/utils.DatasetConfig()
I0512 23:41:54.612930 140440593786880 gin_utils.py:85] train_script.train.train_eval_dataset_cfg = None
I0512 23:41:54.612963 140440593786880 gin_utils.py:85] train_script.train.trainer_cls = @moe_trainer.MoeTrainer
I0512 23:41:54.612996 140440593786880 gin_utils.py:85] train_script.train.use_hardware_rng = %USE_HARDWARE_RNG
I0512 23:41:54.613028 140440593786880 gin_utils.py:85] 
I0512 23:41:54.613059 140440593786880 gin_utils.py:85] # Parameters for trainer.Trainer:
I0512 23:41:54.613090 140440593786880 gin_utils.py:85] # ==============================================================================
I0512 23:41:54.613121 140440593786880 gin_utils.py:85] trainer.Trainer.learning_rate_fn = @utils.create_learning_rate_scheduler()
I0512 23:41:54.613153 140440593786880 gin_utils.py:85] trainer.Trainer.num_microbatches = None
I0512 23:41:54.613184 140440593786880 gin_utils.py:85] 
I0512 23:41:54.613222 140440593786880 gin_utils.py:85] # Parameters for attention_kernel_init/linen.initializers.variance_scaling:
I0512 23:41:54.613254 140440593786880 gin_utils.py:85] # ==============================================================================
I0512 23:41:54.613286 140440593786880 gin_utils.py:85] attention_kernel_init/linen.initializers.variance_scaling.distribution = 'normal'
I0512 23:41:54.613317 140440593786880 gin_utils.py:85] attention_kernel_init/linen.initializers.variance_scaling.mode = 'fan_in'
I0512 23:41:54.613348 140440593786880 gin_utils.py:85] attention_kernel_init/linen.initializers.variance_scaling.scale = %SCALE
I0512 23:41:54.613379 140440593786880 gin_utils.py:85] 
I0512 23:41:54.613411 140440593786880 gin_utils.py:85] # Parameters for expert_kernel_init/linen.initializers.variance_scaling:
I0512 23:41:54.613442 140440593786880 gin_utils.py:85] # ==============================================================================
I0512 23:41:54.613474 140440593786880 gin_utils.py:85] expert_kernel_init/linen.initializers.variance_scaling.distribution = 'normal'
I0512 23:41:54.613505 140440593786880 gin_utils.py:85] expert_kernel_init/linen.initializers.variance_scaling.mode = 'fan_in'
I0512 23:41:54.613536 140440593786880 gin_utils.py:85] expert_kernel_init/linen.initializers.variance_scaling.scale = %SCALE
I0512 23:41:54.613568 140440593786880 gin_utils.py:85] 
I0512 23:41:54.613599 140440593786880 gin_utils.py:85] # Parameters for mlp_kernel_init/linen.initializers.variance_scaling:
I0512 23:41:54.613630 140440593786880 gin_utils.py:85] # ==============================================================================
I0512 23:41:54.613662 140440593786880 gin_utils.py:85] mlp_kernel_init/linen.initializers.variance_scaling.distribution = \
I0512 23:41:54.613692 140440593786880 gin_utils.py:85]     'truncated_normal'
I0512 23:41:54.613724 140440593786880 gin_utils.py:85] mlp_kernel_init/linen.initializers.variance_scaling.mode = 'fan_in'
I0512 23:41:54.613755 140440593786880 gin_utils.py:85] mlp_kernel_init/linen.initializers.variance_scaling.scale = %SCALE
I0512 23:41:54.613786 140440593786880 gin_utils.py:85] 
I0512 23:41:54.613817 140440593786880 gin_utils.py:85] # Parameters for output_logits_kernel_init/linen.initializers.variance_scaling:
I0512 23:41:54.613849 140440593786880 gin_utils.py:85] # ==============================================================================
I0512 23:41:54.613880 140440593786880 gin_utils.py:85] output_logits_kernel_init/linen.initializers.variance_scaling.distribution = \
I0512 23:41:54.613912 140440593786880 gin_utils.py:85]     'truncated_normal'
I0512 23:41:54.613945 140440593786880 gin_utils.py:85] output_logits_kernel_init/linen.initializers.variance_scaling.mode = 'fan_in'
I0512 23:41:54.613978 140440593786880 gin_utils.py:85] output_logits_kernel_init/linen.initializers.variance_scaling.scale = %SCALE
I0512 23:41:54.614900 140440593786880 partitioning.py:559] `activation_partitioning_dims` = 2, `parameter_partitioning_dims` = 1
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
I0000 00:00:1715557314.690681  475641 pjrt_api.cc:98] GetPjrtApi was found for tpu at /home/rosinality/openmoe_venv/lib/python3.10/site-packages/libtpu/libtpu.so
I0000 00:00:1715557314.690728  475641 pjrt_api.cc:67] PJRT_Api is set for device type tpu
I0000 00:00:1715557314.690731  475641 pjrt_api.cc:72] PJRT plugin for tpu has PJRT API version 0.30. The framework PJRT API version is 0.30.
I0512 23:41:54.820564 140595318249472 resource_reader.py:37] gin-config opened resource file:/home/rosinality/t5x/t5x/contrib/moe/configs/runs/pretrain.gin
I0512 23:41:54.821095 140595318249472 resource_reader.py:50] system_path_file_exists:t5x/configs/runs/pretrain.gin
I0512 23:41:54.821414 140595318249472 resource_reader.py:37] gin-config opened resource file:/home/rosinality/t5x/t5x/configs/runs/pretrain.gin
I0512 23:41:54.835235 140595318249472 gin_utils.py:83] Gin Configuration:
I0512 23:41:54.851242 140595318249472 gin_utils.py:85] from __gin__ import dynamic_registration
I0512 23:41:54.851312 140595318249472 gin_utils.py:85] import __main__ as train_script
I0512 23:41:54.851353 140595318249472 gin_utils.py:85] from flax import linen
I0512 23:41:54.851387 140595318249472 gin_utils.py:85] import flaxformer
I0512 23:41:54.851420 140595318249472 gin_utils.py:85] from flaxformer.architectures.moe import moe_architecture
I0512 23:41:54.851454 140595318249472 gin_utils.py:85] from flaxformer.architectures.moe import moe_enums
I0512 23:41:54.851486 140595318249472 gin_utils.py:85] from flaxformer.architectures.moe import moe_layers
I0512 23:41:54.851517 140595318249472 gin_utils.py:85] from flaxformer.architectures.moe import routing
I0512 23:41:54.851548 140595318249472 gin_utils.py:85] from flaxformer.architectures.t5 import t5_architecture
I0512 23:41:54.851578 140595318249472 gin_utils.py:85] from flaxformer.components.attention import dense_attention
I0512 23:41:54.851609 140595318249472 gin_utils.py:85] from flaxformer.components.attention import memory_efficient_attention
I0512 23:41:54.851640 140595318249472 gin_utils.py:85] from flaxformer.components import dense
I0512 23:41:54.851671 140595318249472 gin_utils.py:85] from flaxformer.components import embedding
I0512 23:41:54.851702 140595318249472 gin_utils.py:85] from flaxformer.components import layer_norm
I0512 23:41:54.851733 140595318249472 gin_utils.py:85] from gin import config
I0512 23:41:54.851763 140595318249472 gin_utils.py:85] import seqio
I0512 23:41:54.851794 140595318249472 gin_utils.py:85] import t5.data.mixtures
I0512 23:41:54.851825 140595318249472 gin_utils.py:85] from t5x import adafactor
I0512 23:41:54.851856 140595318249472 gin_utils.py:85] from t5x.contrib.moe import adafactor_utils
I0512 23:41:54.851886 140595318249472 gin_utils.py:85] from t5x.contrib.moe import models
I0512 23:41:54.851917 140595318249472 gin_utils.py:85] from t5x.contrib.moe import partitioning as moe_partitioning
I0512 23:41:54.851948 140595318249472 gin_utils.py:85] from t5x.contrib.moe import trainer as moe_trainer
I0512 23:41:54.851979 140595318249472 gin_utils.py:85] from t5x import gin_utils
I0512 23:41:54.852009 140595318249472 gin_utils.py:85] from t5x import partitioning
I0512 23:41:54.852040 140595318249472 gin_utils.py:85] from t5x import trainer
I0512 23:41:54.852070 140595318249472 gin_utils.py:85] from t5x import utils
I0512 23:41:54.852101 140595318249472 gin_utils.py:85] 
I0512 23:41:54.852132 140595318249472 gin_utils.py:85] # Macros:
I0512 23:41:54.852163 140595318249472 gin_utils.py:85] # ==============================================================================
I0512 23:41:54.852201 140595318249472 gin_utils.py:85] ACTIVATION_DTYPE = 'bfloat16'
I0512 23:41:54.852233 140595318249472 gin_utils.py:85] ACTIVATION_PARTITIONING_DIMS = 1
I0512 23:41:54.852264 140595318249472 gin_utils.py:85] ARCHITECTURE = @t5_architecture.DecoderOnly()
I0512 23:41:54.852295 140595318249472 gin_utils.py:85] AUX_LOSS_FACTOR = 0.01
I0512 23:41:54.852326 140595318249472 gin_utils.py:85] BATCH_SIZE = 384
I0512 23:41:54.852357 140595318249472 gin_utils.py:85] BIAS_INIT = @bias_init/linen.initializers.normal()
I0512 23:41:54.852387 140595318249472 gin_utils.py:85] DECODER_SPARSE_LAYOUT = %flaxformer.architectures.moe.moe_enums.LayerLayout.MIXED
I0512 23:41:54.852419 140595318249472 gin_utils.py:85] DROPOUT_FACTORY = @dropout_factory/linen.Dropout
I0512 23:41:54.852452 140595318249472 gin_utils.py:85] DROPOUT_RATE = 0.0
I0512 23:41:54.852483 140595318249472 gin_utils.py:85] EMBED_DIM = 2048
I0512 23:41:54.852514 140595318249472 gin_utils.py:85] EVAL_EXPERT_CAPACITY_FACTOR = 2.0
I0512 23:41:54.852545 140595318249472 gin_utils.py:85] EXPERT_DROPOUT_RATE = %DROPOUT_RATE
I0512 23:41:54.852576 140595318249472 gin_utils.py:85] EXPERT_MLP_DIM = %MLP_DIM
I0512 23:41:54.852607 140595318249472 gin_utils.py:85] GROUP_SIZE = 4096
I0512 23:41:54.852637 140595318249472 gin_utils.py:85] HEAD_DIM = 128
I0512 23:41:54.852668 140595318249472 gin_utils.py:85] JITTER_NOISE = 0.0
I0512 23:41:54.852699 140595318249472 gin_utils.py:85] LABEL_SMOOTHING = 0.0
I0512 23:41:54.852729 140595318249472 gin_utils.py:85] LOSS_NORMALIZING_FACTOR = 'NUM_REAL_TARGET_TOKENS'
I0512 23:41:54.852760 140595318249472 gin_utils.py:85] MIXTURE_OR_TASK_MODULE = None
I0512 23:41:54.852791 140595318249472 gin_utils.py:85] MIXTURE_OR_TASK_NAME = 'mix_full_lm_test'
I0512 23:41:54.852821 140595318249472 gin_utils.py:85] MLP_DIM = 8192
I0512 23:41:54.852852 140595318249472 gin_utils.py:85] MODEL = @models.MoeDecoderOnlyModel()
I0512 23:41:54.852883 140595318249472 gin_utils.py:85] MODEL_DIR = 'gs://rosinality-tpu-bucket/openmoe_8b/training'
I0512 23:41:54.852913 140595318249472 gin_utils.py:85] MODEL_PARALLEL_SUBMESH = None
I0512 23:41:54.852944 140595318249472 gin_utils.py:85] MOE_TRUNCATED_DTYPE = 'bfloat16'
I0512 23:41:54.853001 140595318249472 gin_utils.py:85] NUM_DECODER_LAYERS = 24
I0512 23:41:54.853034 140595318249472 gin_utils.py:85] NUM_DECODER_SPARSE_LAYERS = 4
I0512 23:41:54.853065 140595318249472 gin_utils.py:85] NUM_EMBEDDINGS = 256384
I0512 23:41:54.853096 140595318249472 gin_utils.py:85] NUM_EXPERT_PARTITIONS = 8
I0512 23:41:54.853127 140595318249472 gin_utils.py:85] NUM_EXPERTS = 8
I0512 23:41:54.853158 140595318249472 gin_utils.py:85] NUM_HEADS = 24
I0512 23:41:54.853194 140595318249472 gin_utils.py:85] NUM_MODEL_PARTITIONS = 4
I0512 23:41:54.853226 140595318249472 gin_utils.py:85] NUM_SELECTED_EXPERTS = 2
I0512 23:41:54.853256 140595318249472 gin_utils.py:85] OPTIMIZER = @adafactor.Adafactor()
I0512 23:41:54.853287 140595318249472 gin_utils.py:85] RANDOM_SEED = None
I0512 23:41:54.853318 140595318249472 gin_utils.py:85] ROUTER_Z_LOSS_FACTOR = 0.0001
I0512 23:41:54.853349 140595318249472 gin_utils.py:85] SCALE = 0.1
I0512 23:41:54.853380 140595318249472 gin_utils.py:85] SHUFFLE_TRAIN_EXAMPLES = True
I0512 23:41:54.853410 140595318249472 gin_utils.py:85] TASK_FEATURE_LENGTHS = {'targets': 2048}
I0512 23:41:54.853443 140595318249472 gin_utils.py:85] TRAIN_EXPERT_CAPACITY_FACTOR = 1.25
I0512 23:41:54.853474 140595318249472 gin_utils.py:85] TRAIN_STEPS = 500000
I0512 23:41:54.853505 140595318249472 gin_utils.py:85] USE_CACHED_TASKS = False
I0512 23:41:54.853536 140595318249472 gin_utils.py:85] USE_HARDWARE_RNG = False
I0512 23:41:54.853567 140595318249472 gin_utils.py:85] VOCABULARY = @seqio.SentencePieceVocabulary()
I0512 23:41:54.853597 140595318249472 gin_utils.py:85] Z_LOSS = 0.0001
I0512 23:41:54.853628 140595318249472 gin_utils.py:85] 
I0512 23:41:54.853658 140595318249472 gin_utils.py:85] # Parameters for adafactor.Adafactor:
I0512 23:41:54.853689 140595318249472 gin_utils.py:85] # ==============================================================================
I0512 23:41:54.853720 140595318249472 gin_utils.py:85] adafactor.Adafactor.decay_rate = 0.8
I0512 23:41:54.853751 140595318249472 gin_utils.py:85] adafactor.Adafactor.logical_factor_rules = @adafactor_utils.logical_factor_rules()
I0512 23:41:54.853782 140595318249472 gin_utils.py:85] adafactor.Adafactor.step_offset = 0
I0512 23:41:54.853812 140595318249472 gin_utils.py:85] 
I0512 23:41:54.795212 140311617681408 resource_reader.py:50] system_path_file_exists:t5x/configs/runs/pretrain.gin
I0512 23:41:54.795527 140311617681408 resource_reader.py:37] gin-config opened resource file:/home/rosinality/t5x/t5x/configs/runs/pretrain.gin
I0512 23:41:54.809266 140311617681408 gin_utils.py:83] Gin Configuration:
I0512 23:41:54.825095 140311617681408 gin_utils.py:85] from __gin__ import dynamic_registration
I0512 23:41:54.825164 140311617681408 gin_utils.py:85] import __main__ as train_script
I0512 23:41:54.825206 140311617681408 gin_utils.py:85] from flax import linen
I0512 23:41:54.825242 140311617681408 gin_utils.py:85] import flaxformer
I0512 23:41:54.825276 140311617681408 gin_utils.py:85] from flaxformer.architectures.moe import moe_architecture
I0512 23:41:54.825310 140311617681408 gin_utils.py:85] from flaxformer.architectures.moe import moe_enums
I0512 23:41:54.825343 140311617681408 gin_utils.py:85] from flaxformer.architectures.moe import moe_layers
I0512 23:41:54.825376 140311617681408 gin_utils.py:85] from flaxformer.architectures.moe import routing
I0512 23:41:54.825410 140311617681408 gin_utils.py:85] from flaxformer.architectures.t5 import t5_architecture
I0512 23:41:54.825445 140311617681408 gin_utils.py:85] from flaxformer.components.attention import dense_attention
I0512 23:41:54.825479 140311617681408 gin_utils.py:85] from flaxformer.components.attention import memory_efficient_attention
I0512 23:41:54.825512 140311617681408 gin_utils.py:85] from flaxformer.components import dense
I0512 23:41:54.825545 140311617681408 gin_utils.py:85] from flaxformer.components import embedding
I0512 23:41:54.825577 140311617681408 gin_utils.py:85] from flaxformer.components import layer_norm
I0512 23:41:54.825609 140311617681408 gin_utils.py:85] from gin import config
I0512 23:41:54.825642 140311617681408 gin_utils.py:85] import seqio
I0512 23:41:54.825683 140311617681408 gin_utils.py:85] import t5.data.mixtures
I0512 23:41:54.825717 140311617681408 gin_utils.py:85] from t5x import adafactor
I0512 23:41:54.825750 140311617681408 gin_utils.py:85] from t5x.contrib.moe import adafactor_utils
I0512 23:41:54.825803 140311617681408 gin_utils.py:85] from t5x.contrib.moe import models
I0512 23:41:54.825840 140311617681408 gin_utils.py:85] from t5x.contrib.moe import partitioning as moe_partitioning
I0512 23:41:54.825874 140311617681408 gin_utils.py:85] from t5x.contrib.moe import trainer as moe_trainer
I0512 23:41:54.825907 140311617681408 gin_utils.py:85] from t5x import gin_utils
I0512 23:41:54.825940 140311617681408 gin_utils.py:85] from t5x import partitioning
I0512 23:41:54.825973 140311617681408 gin_utils.py:85] from t5x import trainer
I0512 23:41:54.826005 140311617681408 gin_utils.py:85] from t5x import utils
I0512 23:41:54.826038 140311617681408 gin_utils.py:85] 
I0512 23:41:54.826071 140311617681408 gin_utils.py:85] # Macros:
I0512 23:41:54.826104 140311617681408 gin_utils.py:85] # ==============================================================================
I0512 23:41:54.826137 140311617681408 gin_utils.py:85] ACTIVATION_DTYPE = 'bfloat16'
I0512 23:41:54.826170 140311617681408 gin_utils.py:85] ACTIVATION_PARTITIONING_DIMS = 1
I0512 23:41:54.826203 140311617681408 gin_utils.py:85] ARCHITECTURE = @t5_architecture.DecoderOnly()
I0512 23:41:54.826236 140311617681408 gin_utils.py:85] AUX_LOSS_FACTOR = 0.01
I0512 23:41:54.826268 140311617681408 gin_utils.py:85] BATCH_SIZE = 384
I0512 23:41:54.826301 140311617681408 gin_utils.py:85] BIAS_INIT = @bias_init/linen.initializers.normal()
I0512 23:41:54.826333 140311617681408 gin_utils.py:85] DECODER_SPARSE_LAYOUT = %flaxformer.architectures.moe.moe_enums.LayerLayout.MIXED
I0512 23:41:54.826366 140311617681408 gin_utils.py:85] DROPOUT_FACTORY = @dropout_factory/linen.Dropout
I0512 23:41:54.826399 140311617681408 gin_utils.py:85] DROPOUT_RATE = 0.0
I0512 23:41:54.826434 140311617681408 gin_utils.py:85] EMBED_DIM = 2048
I0512 23:41:54.826467 140311617681408 gin_utils.py:85] EVAL_EXPERT_CAPACITY_FACTOR = 2.0
I0512 23:41:54.826500 140311617681408 gin_utils.py:85] EXPERT_DROPOUT_RATE = %DROPOUT_RATE
I0512 23:41:54.826533 140311617681408 gin_utils.py:85] EXPERT_MLP_DIM = %MLP_DIM
I0512 23:41:54.826565 140311617681408 gin_utils.py:85] GROUP_SIZE = 4096
I0512 23:41:54.826597 140311617681408 gin_utils.py:85] HEAD_DIM = 128
I0512 23:41:54.826630 140311617681408 gin_utils.py:85] JITTER_NOISE = 0.0
I0512 23:41:54.826668 140311617681408 gin_utils.py:85] LABEL_SMOOTHING = 0.0
I0512 23:41:54.826702 140311617681408 gin_utils.py:85] LOSS_NORMALIZING_FACTOR = 'NUM_REAL_TARGET_TOKENS'
I0512 23:41:54.826735 140311617681408 gin_utils.py:85] MIXTURE_OR_TASK_MODULE = None
I0512 23:41:54.826767 140311617681408 gin_utils.py:85] MIXTURE_OR_TASK_NAME = 'mix_full_lm_test'
I0512 23:41:54.826800 140311617681408 gin_utils.py:85] MLP_DIM = 8192
I0512 23:41:54.826833 140311617681408 gin_utils.py:85] MODEL = @models.MoeDecoderOnlyModel()
I0512 23:41:54.826865 140311617681408 gin_utils.py:85] MODEL_DIR = 'gs://rosinality-tpu-bucket/openmoe_8b/training'
I0512 23:41:54.826898 140311617681408 gin_utils.py:85] MODEL_PARALLEL_SUBMESH = None
I0512 23:41:54.826930 140311617681408 gin_utils.py:85] MOE_TRUNCATED_DTYPE = 'bfloat16'
I0512 23:41:54.826962 140311617681408 gin_utils.py:85] NUM_DECODER_LAYERS = 24
I0512 23:41:54.826995 140311617681408 gin_utils.py:85] NUM_DECODER_SPARSE_LAYERS = 4
I0512 23:41:54.827028 140311617681408 gin_utils.py:85] NUM_EMBEDDINGS = 256384
I0512 23:41:54.827060 140311617681408 gin_utils.py:85] NUM_EXPERT_PARTITIONS = 8
I0512 23:41:54.827092 140311617681408 gin_utils.py:85] NUM_EXPERTS = 8
I0512 23:41:54.827125 140311617681408 gin_utils.py:85] NUM_HEADS = 24
I0512 23:41:54.823981 140529434355712 resource_reader.py:50] system_path_file_exists:t5x/configs/runs/pretrain.gin
I0512 23:41:54.824304 140529434355712 resource_reader.py:37] gin-config opened resource file:/home/rosinality/t5x/t5x/configs/runs/pretrain.gin
I0512 23:41:54.837943 140529434355712 gin_utils.py:83] Gin Configuration:
I0512 23:41:54.853780 140529434355712 gin_utils.py:85] from __gin__ import dynamic_registration
I0512 23:41:54.853849 140529434355712 gin_utils.py:85] import __main__ as train_script
I0512 23:41:54.853893 140529434355712 gin_utils.py:85] from flax import linen
I0512 23:41:54.853929 140529434355712 gin_utils.py:85] import flaxformer
I0512 23:41:54.853962 140529434355712 gin_utils.py:85] from flaxformer.architectures.moe import moe_architecture
I0512 23:41:54.853996 140529434355712 gin_utils.py:85] from flaxformer.architectures.moe import moe_enums
I0512 23:41:54.854030 140529434355712 gin_utils.py:85] from flaxformer.architectures.moe import moe_layers
I0512 23:41:54.854063 140529434355712 gin_utils.py:85] from flaxformer.architectures.moe import routing
I0512 23:41:54.854096 140529434355712 gin_utils.py:85] from flaxformer.architectures.t5 import t5_architecture
I0512 23:41:54.854134 140529434355712 gin_utils.py:85] from flaxformer.components.attention import dense_attention
I0512 23:41:54.854170 140529434355712 gin_utils.py:85] from flaxformer.components.attention import memory_efficient_attention
I0512 23:41:54.854204 140529434355712 gin_utils.py:85] from flaxformer.components import dense
I0512 23:41:54.854236 140529434355712 gin_utils.py:85] from flaxformer.components import embedding
I0512 23:41:54.854269 140529434355712 gin_utils.py:85] from flaxformer.components import layer_norm
I0512 23:41:54.854302 140529434355712 gin_utils.py:85] from gin import config
I0512 23:41:54.854334 140529434355712 gin_utils.py:85] import seqio
I0512 23:41:54.854367 140529434355712 gin_utils.py:85] import t5.data.mixtures
I0512 23:41:54.854399 140529434355712 gin_utils.py:85] from t5x import adafactor
I0512 23:41:54.854432 140529434355712 gin_utils.py:85] from t5x.contrib.moe import adafactor_utils
I0512 23:41:54.854464 140529434355712 gin_utils.py:85] from t5x.contrib.moe import models
I0512 23:41:54.854497 140529434355712 gin_utils.py:85] from t5x.contrib.moe import partitioning
I0512 23:41:54.854529 140529434355712 gin_utils.py:85] from t5x.contrib.moe import trainer as moe_trainer
I0512 23:41:54.854562 140529434355712 gin_utils.py:85] from t5x import gin_utils
I0512 23:41:54.854594 140529434355712 gin_utils.py:85] from t5x import partitioning as partitioning2
I0512 23:41:54.854627 140529434355712 gin_utils.py:85] from t5x import trainer
I0512 23:41:54.854659 140529434355712 gin_utils.py:85] from t5x import utils
I0512 23:41:54.854691 140529434355712 gin_utils.py:85] 
I0512 23:41:54.854724 140529434355712 gin_utils.py:85] # Macros:
I0512 23:41:54.854757 140529434355712 gin_utils.py:85] # ==============================================================================
I0512 23:41:54.854790 140529434355712 gin_utils.py:85] ACTIVATION_DTYPE = 'bfloat16'
I0512 23:41:54.854822 140529434355712 gin_utils.py:85] ACTIVATION_PARTITIONING_DIMS = 1
I0512 23:41:54.854855 140529434355712 gin_utils.py:85] ARCHITECTURE = @t5_architecture.DecoderOnly()
I0512 23:41:54.854888 140529434355712 gin_utils.py:85] AUX_LOSS_FACTOR = 0.01
I0512 23:41:54.854923 140529434355712 gin_utils.py:85] BATCH_SIZE = 384
I0512 23:41:54.854955 140529434355712 gin_utils.py:85] BIAS_INIT = @bias_init/linen.initializers.normal()
I0512 23:41:54.854988 140529434355712 gin_utils.py:85] DECODER_SPARSE_LAYOUT = %flaxformer.architectures.moe.moe_enums.LayerLayout.MIXED
I0512 23:41:54.855020 140529434355712 gin_utils.py:85] DROPOUT_FACTORY = @dropout_factory/linen.Dropout
I0512 23:41:54.855053 140529434355712 gin_utils.py:85] DROPOUT_RATE = 0.0
I0512 23:41:54.855086 140529434355712 gin_utils.py:85] EMBED_DIM = 2048
I0512 23:41:54.855118 140529434355712 gin_utils.py:85] EVAL_EXPERT_CAPACITY_FACTOR = 2.0
I0512 23:41:54.855158 140529434355712 gin_utils.py:85] EXPERT_DROPOUT_RATE = %DROPOUT_RATE
I0512 23:41:54.855191 140529434355712 gin_utils.py:85] EXPERT_MLP_DIM = %MLP_DIM
I0512 23:41:54.855224 140529434355712 gin_utils.py:85] GROUP_SIZE = 4096
I0512 23:41:54.855256 140529434355712 gin_utils.py:85] HEAD_DIM = 128
I0512 23:41:54.855288 140529434355712 gin_utils.py:85] JITTER_NOISE = 0.0
I0512 23:41:54.855321 140529434355712 gin_utils.py:85] LABEL_SMOOTHING = 0.0
I0512 23:41:54.855353 140529434355712 gin_utils.py:85] LOSS_NORMALIZING_FACTOR = 'NUM_REAL_TARGET_TOKENS'
I0512 23:41:54.855386 140529434355712 gin_utils.py:85] MIXTURE_OR_TASK_MODULE = None
I0512 23:41:54.855418 140529434355712 gin_utils.py:85] MIXTURE_OR_TASK_NAME = 'mix_full_lm_test'
I0512 23:41:54.855451 140529434355712 gin_utils.py:85] MLP_DIM = 8192
I0512 23:41:54.855483 140529434355712 gin_utils.py:85] MODEL = @models.MoeDecoderOnlyModel()
I0512 23:41:54.855516 140529434355712 gin_utils.py:85] MODEL_DIR = 'gs://rosinality-tpu-bucket/openmoe_8b/training'
I0512 23:41:54.855548 140529434355712 gin_utils.py:85] MODEL_PARALLEL_SUBMESH = None
I0512 23:41:54.855581 140529434355712 gin_utils.py:85] MOE_TRUNCATED_DTYPE = 'bfloat16'
I0512 23:41:54.855613 140529434355712 gin_utils.py:85] NUM_DECODER_LAYERS = 24
I0512 23:41:54.855645 140529434355712 gin_utils.py:85] NUM_DECODER_SPARSE_LAYERS = 4
I0512 23:41:54.855678 140529434355712 gin_utils.py:85] NUM_EMBEDDINGS = 256384
I0512 23:41:54.855710 140529434355712 gin_utils.py:85] NUM_EXPERT_PARTITIONS = 8
I0512 23:41:54.855743 140529434355712 gin_utils.py:85] NUM_EXPERTS = 8
I0512 23:41:54.855775 140529434355712 gin_utils.py:85] NUM_HEADS = 24
I0512 23:41:54.855808 140529434355712 gin_utils.py:85] NUM_MODEL_PARTITIONS = 4
I0512 23:41:54.855840 140529434355712 gin_utils.py:85] NUM_SELECTED_EXPERTS = 2
I0512 23:41:54.855872 140529434355712 gin_utils.py:85] OPTIMIZER = @adafactor.Adafactor()
I0512 23:41:54.855907 140529434355712 gin_utils.py:85] RANDOM_SEED = None
I0512 23:41:54.855940 140529434355712 gin_utils.py:85] ROUTER_Z_LOSS_FACTOR = 0.0001
I0512 23:41:54.855973 140529434355712 gin_utils.py:85] SCALE = 0.1
I0512 23:41:54.856030 140529434355712 gin_utils.py:85] SHUFFLE_TRAIN_EXAMPLES = True
I0512 23:41:54.856064 140529434355712 gin_utils.py:85] TASK_FEATURE_LENGTHS = {'targets': 2048}
I0512 23:41:54.856096 140529434355712 gin_utils.py:85] TRAIN_EXPERT_CAPACITY_FACTOR = 1.25
I0512 23:41:54.856133 140529434355712 gin_utils.py:85] TRAIN_STEPS = 500000
I0512 23:41:54.856168 140529434355712 gin_utils.py:85] USE_CACHED_TASKS = False
I0512 23:41:54.856200 140529434355712 gin_utils.py:85] USE_HARDWARE_RNG = False
I0512 23:41:54.856233 140529434355712 gin_utils.py:85] VOCABULARY = @seqio.SentencePieceVocabulary()
I0512 23:41:54.856266 140529434355712 gin_utils.py:85] Z_LOSS = 0.0001
I0512 23:41:54.856298 140529434355712 gin_utils.py:85] 
I0512 23:41:54.856330 140529434355712 gin_utils.py:85] # Parameters for adafactor.Adafactor:
I0512 23:41:54.856362 140529434355712 gin_utils.py:85] # ==============================================================================
I0512 23:41:54.856395 140529434355712 gin_utils.py:85] adafactor.Adafactor.decay_rate = 0.8
I0512 23:41:54.856427 140529434355712 gin_utils.py:85] adafactor.Adafactor.logical_factor_rules = @adafactor_utils.logical_factor_rules()
I0512 23:41:54.856460 140529434355712 gin_utils.py:85] adafactor.Adafactor.step_offset = 0
I0512 23:41:54.856492 140529434355712 gin_utils.py:85] 
I0512 23:41:54.856524 140529434355712 gin_utils.py:85] # Parameters for utils.CheckpointConfig:
I0512 23:41:54.626891 140095893518336 gin_utils.py:85] train_script.train.eval_steps = 20
I0512 23:41:54.626922 140095893518336 gin_utils.py:85] train_script.train.infer_eval_dataset_cfg = None
I0512 23:41:54.626954 140095893518336 gin_utils.py:85] train_script.train.model = %MODEL
I0512 23:41:54.626984 140095893518336 gin_utils.py:85] train_script.train.model_dir = %MODEL_DIR
I0512 23:41:54.627015 140095893518336 gin_utils.py:85] train_script.train.partitioner = @moe_partitioning.MoePjitPartitioner()
I0512 23:41:54.627047 140095893518336 gin_utils.py:85] train_script.train.random_seed = %RANDOM_SEED
I0512 23:41:54.627079 140095893518336 gin_utils.py:85] train_script.train.stats_period = 10
I0512 23:41:54.627110 140095893518336 gin_utils.py:85] train_script.train.summarize_config_fn = @gin_utils.summarize_gin_config
I0512 23:41:54.627147 140095893518336 gin_utils.py:85] train_script.train.total_steps = %TRAIN_STEPS
I0512 23:41:54.627179 140095893518336 gin_utils.py:85] train_script.train.train_dataset_cfg = @train/utils.DatasetConfig()
I0512 23:41:54.627211 140095893518336 gin_utils.py:85] train_script.train.train_eval_dataset_cfg = None
I0512 23:41:54.627241 140095893518336 gin_utils.py:85] train_script.train.trainer_cls = @moe_trainer.MoeTrainer
I0512 23:41:54.627272 140095893518336 gin_utils.py:85] train_script.train.use_hardware_rng = %USE_HARDWARE_RNG
I0512 23:41:54.627303 140095893518336 gin_utils.py:85] 
I0512 23:41:54.627334 140095893518336 gin_utils.py:85] # Parameters for trainer.Trainer:
I0512 23:41:54.627365 140095893518336 gin_utils.py:85] # ==============================================================================
I0512 23:41:54.627398 140095893518336 gin_utils.py:85] trainer.Trainer.learning_rate_fn = @utils.create_learning_rate_scheduler()
I0512 23:41:54.627430 140095893518336 gin_utils.py:85] trainer.Trainer.num_microbatches = None
I0512 23:41:54.627461 140095893518336 gin_utils.py:85] 
I0512 23:41:54.627492 140095893518336 gin_utils.py:85] # Parameters for attention_kernel_init/linen.initializers.variance_scaling:
I0512 23:41:54.627523 140095893518336 gin_utils.py:85] # ==============================================================================
I0512 23:41:54.627553 140095893518336 gin_utils.py:85] attention_kernel_init/linen.initializers.variance_scaling.distribution = 'normal'
I0512 23:41:54.627584 140095893518336 gin_utils.py:85] attention_kernel_init/linen.initializers.variance_scaling.mode = 'fan_in'
I0512 23:41:54.627615 140095893518336 gin_utils.py:85] attention_kernel_init/linen.initializers.variance_scaling.scale = %SCALE
I0512 23:41:54.627646 140095893518336 gin_utils.py:85] 
I0512 23:41:54.627677 140095893518336 gin_utils.py:85] # Parameters for expert_kernel_init/linen.initializers.variance_scaling:
I0512 23:41:54.627708 140095893518336 gin_utils.py:85] # ==============================================================================
I0512 23:41:54.627739 140095893518336 gin_utils.py:85] expert_kernel_init/linen.initializers.variance_scaling.distribution = 'normal'
I0512 23:41:54.627770 140095893518336 gin_utils.py:85] expert_kernel_init/linen.initializers.variance_scaling.mode = 'fan_in'
I0512 23:41:54.627801 140095893518336 gin_utils.py:85] expert_kernel_init/linen.initializers.variance_scaling.scale = %SCALE
I0512 23:41:54.627832 140095893518336 gin_utils.py:85] 
I0512 23:41:54.627863 140095893518336 gin_utils.py:85] # Parameters for mlp_kernel_init/linen.initializers.variance_scaling:
I0512 23:41:54.627894 140095893518336 gin_utils.py:85] # ==============================================================================
I0512 23:41:54.627925 140095893518336 gin_utils.py:85] mlp_kernel_init/linen.initializers.variance_scaling.distribution = \
I0512 23:41:54.627956 140095893518336 gin_utils.py:85]     'truncated_normal'
I0512 23:41:54.627987 140095893518336 gin_utils.py:85] mlp_kernel_init/linen.initializers.variance_scaling.mode = 'fan_in'
I0512 23:41:54.628018 140095893518336 gin_utils.py:85] mlp_kernel_init/linen.initializers.variance_scaling.scale = %SCALE
I0512 23:41:54.628050 140095893518336 gin_utils.py:85] 
I0512 23:41:54.628081 140095893518336 gin_utils.py:85] # Parameters for output_logits_kernel_init/linen.initializers.variance_scaling:
I0512 23:41:54.628112 140095893518336 gin_utils.py:85] # ==============================================================================
I0512 23:41:54.628152 140095893518336 gin_utils.py:85] output_logits_kernel_init/linen.initializers.variance_scaling.distribution = \
I0512 23:41:54.628184 140095893518336 gin_utils.py:85]     'truncated_normal'
I0512 23:41:54.628214 140095893518336 gin_utils.py:85] output_logits_kernel_init/linen.initializers.variance_scaling.mode = 'fan_in'
I0512 23:41:54.628245 140095893518336 gin_utils.py:85] output_logits_kernel_init/linen.initializers.variance_scaling.scale = %SCALE
I0512 23:41:54.629150 140095893518336 partitioning.py:559] `activation_partitioning_dims` = 2, `parameter_partitioning_dims` = 1
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
I0000 00:00:1715557314.676777  450615 pjrt_api.cc:98] GetPjrtApi was found for tpu at /home/rosinality/openmoe_venv/lib/python3.10/site-packages/libtpu/libtpu.so
I0000 00:00:1715557314.676833  450615 pjrt_api.cc:67] PJRT_Api is set for device type tpu
I0000 00:00:1715557314.676835  450615 pjrt_api.cc:72] PJRT plugin for tpu has PJRT API version 0.30. The framework PJRT API version is 0.30.
I0512 23:41:54.853843 140595318249472 gin_utils.py:85] # Parameters for utils.CheckpointConfig:
I0512 23:41:54.853874 140595318249472 gin_utils.py:85] # ==============================================================================
I0512 23:41:54.853905 140595318249472 gin_utils.py:85] utils.CheckpointConfig.restore = @utils.RestoreCheckpointConfig()
I0512 23:41:54.853936 140595318249472 gin_utils.py:85] utils.CheckpointConfig.save = @utils.SaveCheckpointConfig()
I0512 23:41:54.853967 140595318249472 gin_utils.py:85] 
I0512 23:41:54.853998 140595318249472 gin_utils.py:85] # Parameters for moe_partitioning.compute_num_model_partitions:
I0512 23:41:54.854028 140595318249472 gin_utils.py:85] # ==============================================================================
I0512 23:41:54.854059 140595318249472 gin_utils.py:85] moe_partitioning.compute_num_model_partitions.model_parallel_submesh = \
I0512 23:41:54.854090 140595318249472 gin_utils.py:85]     %MODEL_PARALLEL_SUBMESH
I0512 23:41:54.854121 140595318249472 gin_utils.py:85] moe_partitioning.compute_num_model_partitions.num_model_partitions = \
I0512 23:41:54.854152 140595318249472 gin_utils.py:85]     %NUM_MODEL_PARTITIONS
I0512 23:41:54.854187 140595318249472 gin_utils.py:85] 
I0512 23:41:54.854220 140595318249472 gin_utils.py:85] # Parameters for utils.create_learning_rate_scheduler:
I0512 23:41:54.854251 140595318249472 gin_utils.py:85] # ==============================================================================
I0512 23:41:54.854282 140595318249472 gin_utils.py:85] utils.create_learning_rate_scheduler.base_learning_rate = 1.0
I0512 23:41:54.854313 140595318249472 gin_utils.py:85] utils.create_learning_rate_scheduler.factors = 'constant * rsqrt_decay'
I0512 23:41:54.854343 140595318249472 gin_utils.py:85] utils.create_learning_rate_scheduler.warmup_steps = 10000
I0512 23:41:54.854374 140595318249472 gin_utils.py:85] 
I0512 23:41:54.854405 140595318249472 gin_utils.py:85] # Parameters for train/utils.DatasetConfig:
I0512 23:41:54.854437 140595318249472 gin_utils.py:85] # ==============================================================================
I0512 23:41:54.854470 140595318249472 gin_utils.py:85] train/utils.DatasetConfig.batch_size = %BATCH_SIZE
I0512 23:41:54.854501 140595318249472 gin_utils.py:85] train/utils.DatasetConfig.mixture_or_task_name = %MIXTURE_OR_TASK_NAME
I0512 23:41:54.854532 140595318249472 gin_utils.py:85] train/utils.DatasetConfig.module = %MIXTURE_OR_TASK_MODULE
I0512 23:41:54.854562 140595318249472 gin_utils.py:85] train/utils.DatasetConfig.pack = True
I0512 23:41:54.854593 140595318249472 gin_utils.py:85] train/utils.DatasetConfig.seed = None
I0512 23:41:54.854624 140595318249472 gin_utils.py:85] train/utils.DatasetConfig.shuffle = %SHUFFLE_TRAIN_EXAMPLES
I0512 23:41:54.854655 140595318249472 gin_utils.py:85] train/utils.DatasetConfig.split = 'train'
I0512 23:41:54.854686 140595318249472 gin_utils.py:85] train/utils.DatasetConfig.task_feature_lengths = %TASK_FEATURE_LENGTHS
I0512 23:41:54.854716 140595318249472 gin_utils.py:85] train/utils.DatasetConfig.use_cached = %USE_CACHED_TASKS
I0512 23:41:54.854747 140595318249472 gin_utils.py:85] 
I0512 23:41:54.854777 140595318249472 gin_utils.py:85] # Parameters for train_eval/utils.DatasetConfig:
I0512 23:41:54.854808 140595318249472 gin_utils.py:85] # ==============================================================================
I0512 23:41:54.854839 140595318249472 gin_utils.py:85] train_eval/utils.DatasetConfig.batch_size = 128
I0512 23:41:54.854869 140595318249472 gin_utils.py:85] train_eval/utils.DatasetConfig.mixture_or_task_name = %MIXTURE_OR_TASK_NAME
I0512 23:41:54.854901 140595318249472 gin_utils.py:85] train_eval/utils.DatasetConfig.module = %MIXTURE_OR_TASK_MODULE
I0512 23:41:54.854932 140595318249472 gin_utils.py:85] train_eval/utils.DatasetConfig.pack = True
I0512 23:41:54.854962 140595318249472 gin_utils.py:85] train_eval/utils.DatasetConfig.seed = 42
I0512 23:41:54.854993 140595318249472 gin_utils.py:85] train_eval/utils.DatasetConfig.shuffle = False
I0512 23:41:54.855024 140595318249472 gin_utils.py:85] train_eval/utils.DatasetConfig.split = 'validation'
I0512 23:41:54.855055 140595318249472 gin_utils.py:85] train_eval/utils.DatasetConfig.task_feature_lengths = %TASK_FEATURE_LENGTHS
I0512 23:41:54.855085 140595318249472 gin_utils.py:85] train_eval/utils.DatasetConfig.use_cached = %USE_CACHED_TASKS
I0512 23:41:54.855116 140595318249472 gin_utils.py:85] 
I0512 23:41:54.855147 140595318249472 gin_utils.py:85] # Parameters for t5_architecture.DecoderLayer:
I0512 23:41:54.855182 140595318249472 gin_utils.py:85] # ==============================================================================
I0512 23:41:54.855215 140595318249472 gin_utils.py:85] t5_architecture.DecoderLayer.activation_partitioning_dims = \
I0512 23:41:54.855246 140595318249472 gin_utils.py:85]     %ACTIVATION_PARTITIONING_DIMS
I0512 23:41:54.855276 140595318249472 gin_utils.py:85] t5_architecture.DecoderLayer.dropout_factory = %DROPOUT_FACTORY
I0512 23:41:54.855307 140595318249472 gin_utils.py:85] t5_architecture.DecoderLayer.encoder_decoder_attention = None
I0512 23:41:54.855338 140595318249472 gin_utils.py:85] t5_architecture.DecoderLayer.layer_norm_factory = @layer_norm.T5LayerNorm
I0512 23:41:54.855368 140595318249472 gin_utils.py:85] t5_architecture.DecoderLayer.mlp = @dense.MlpBlock()
I0512 23:41:54.855399 140595318249472 gin_utils.py:85] t5_architecture.DecoderLayer.scanned = False
I0512 23:41:54.855431 140595318249472 gin_utils.py:85] t5_architecture.DecoderLayer.self_attention = \
I0512 23:41:54.855463 140595318249472 gin_utils.py:85]     @dense_attention.MultiHeadDotProductAttention()
I0512 23:41:54.855494 140595318249472 gin_utils.py:85] 
I0512 23:41:54.855525 140595318249472 gin_utils.py:85] # Parameters for t5_architecture.DecoderOnly:
I0512 23:41:54.855556 140595318249472 gin_utils.py:85] # ==============================================================================
I0512 23:41:54.855586 140595318249472 gin_utils.py:85] t5_architecture.DecoderOnly.decoder_factory = @moe_architecture.SparseDecoder
I0512 23:41:54.855617 140595318249472 gin_utils.py:85] t5_architecture.DecoderOnly.dtype = %ACTIVATION_DTYPE
I0512 23:41:54.855648 140595318249472 gin_utils.py:85] t5_architecture.DecoderOnly.shared_token_embedder_factory = @embedding.Embed
I0512 23:41:54.855679 140595318249472 gin_utils.py:85] 
I0512 23:41:54.855709 140595318249472 gin_utils.py:85] # Parameters for output_logits/dense.DenseGeneral:
I0512 23:41:54.855740 140595318249472 gin_utils.py:85] # ==============================================================================
I0512 23:41:54.855770 140595318249472 gin_utils.py:85] output_logits/dense.DenseGeneral.bias_init = %BIAS_INIT
I0512 23:41:54.855801 140595318249472 gin_utils.py:85] output_logits/dense.DenseGeneral.dtype = 'float32'
I0512 23:41:54.855832 140595318249472 gin_utils.py:85] output_logits/dense.DenseGeneral.features = %NUM_EMBEDDINGS
I0512 23:41:54.855863 140595318249472 gin_utils.py:85] output_logits/dense.DenseGeneral.kernel_axis_names = ['embed', 'vocab']
I0512 23:41:54.855893 140595318249472 gin_utils.py:85] output_logits/dense.DenseGeneral.kernel_init = \
I0512 23:41:54.855924 140595318249472 gin_utils.py:85]     @output_logits_kernel_init/linen.initializers.variance_scaling()
I0512 23:41:54.855955 140595318249472 gin_utils.py:85] output_logits/dense.DenseGeneral.use_bias = False
I0512 23:41:54.855988 140595318249472 gin_utils.py:85] 
I0512 23:41:54.856020 140595318249472 gin_utils.py:85] # Parameters for dropout_factory/linen.Dropout:
I0512 23:41:54.856051 140595318249472 gin_utils.py:85] # ==============================================================================
I0512 23:41:54.856082 140595318249472 gin_utils.py:85] dropout_factory/linen.Dropout.broadcast_dims = (-2,)
I0512 23:41:54.856113 140595318249472 gin_utils.py:85] dropout_factory/linen.Dropout.rate = %DROPOUT_RATE
I0512 23:41:54.856143 140595318249472 gin_utils.py:85] 
I0512 23:41:54.856179 140595318249472 gin_utils.py:85] # Parameters for embedding.Embed:
I0512 23:41:54.856211 140595318249472 gin_utils.py:85] # ==============================================================================
I0512 23:41:54.856242 140595318249472 gin_utils.py:85] embedding.Embed.attend_dtype = 'float32'
I0512 23:41:54.856273 140595318249472 gin_utils.py:85] embedding.Embed.cast_input_dtype = 'int32'
I0512 23:41:54.856303 140595318249472 gin_utils.py:85] embedding.Embed.dtype = %ACTIVATION_DTYPE
I0512 23:41:54.856334 140595318249472 gin_utils.py:85] embedding.Embed.embedding_init = @token_embedder_init/linen.initializers.normal()
I0512 23:41:54.856364 140595318249472 gin_utils.py:85] embedding.Embed.features = %EMBED_DIM
I0512 23:41:54.856395 140595318249472 gin_utils.py:85] embedding.Embed.name = 'token_embedder'
I0512 23:41:54.856427 140595318249472 gin_utils.py:85] embedding.Embed.num_embeddings = %NUM_EMBEDDINGS
I0512 23:41:54.856459 140595318249472 gin_utils.py:85] embedding.Embed.one_hot = True
I0512 23:41:54.856490 140595318249472 gin_utils.py:85] 
I0512 23:41:54.856521 140595318249472 gin_utils.py:85] # Parameters for dense.MlpBlock:
I0512 23:41:54.856552 140595318249472 gin_utils.py:85] # ==============================================================================
I0512 23:41:54.856583 140595318249472 gin_utils.py:85] dense.MlpBlock.activations = ('swiglu', 'linear')
I0512 23:41:54.856614 140595318249472 gin_utils.py:85] dense.MlpBlock.bias_init = %BIAS_INIT
I0512 23:41:54.856645 140595318249472 gin_utils.py:85] dense.MlpBlock.dtype = %ACTIVATION_DTYPE
I0512 23:41:54.856676 140595318249472 gin_utils.py:85] dense.MlpBlock.final_dropout_rate = 0
I0512 23:41:54.856706 140595318249472 gin_utils.py:85] dense.MlpBlock.input_axis_name = 'mlp_embed'
I0512 23:41:54.856737 140595318249472 gin_utils.py:85] dense.MlpBlock.intermediate_dim = %MLP_DIM
I0512 23:41:54.856768 140595318249472 gin_utils.py:85] dense.MlpBlock.intermediate_dropout_rate = %DROPOUT_RATE
I0512 23:41:54.827157 140311617681408 gin_utils.py:85] NUM_MODEL_PARTITIONS = 4
I0512 23:41:54.827189 140311617681408 gin_utils.py:85] NUM_SELECTED_EXPERTS = 2
I0512 23:41:54.827222 140311617681408 gin_utils.py:85] OPTIMIZER = @adafactor.Adafactor()
I0512 23:41:54.827254 140311617681408 gin_utils.py:85] RANDOM_SEED = None
I0512 23:41:54.827286 140311617681408 gin_utils.py:85] ROUTER_Z_LOSS_FACTOR = 0.0001
I0512 23:41:54.827319 140311617681408 gin_utils.py:85] SCALE = 0.1
I0512 23:41:54.827351 140311617681408 gin_utils.py:85] SHUFFLE_TRAIN_EXAMPLES = True
I0512 23:41:54.827383 140311617681408 gin_utils.py:85] TASK_FEATURE_LENGTHS = {'targets': 2048}
I0512 23:41:54.827417 140311617681408 gin_utils.py:85] TRAIN_EXPERT_CAPACITY_FACTOR = 1.25
I0512 23:41:54.827451 140311617681408 gin_utils.py:85] TRAIN_STEPS = 500000
I0512 23:41:54.827484 140311617681408 gin_utils.py:85] USE_CACHED_TASKS = False
I0512 23:41:54.827517 140311617681408 gin_utils.py:85] USE_HARDWARE_RNG = False
I0512 23:41:54.827549 140311617681408 gin_utils.py:85] VOCABULARY = @seqio.SentencePieceVocabulary()
I0512 23:41:54.827581 140311617681408 gin_utils.py:85] Z_LOSS = 0.0001
I0512 23:41:54.827614 140311617681408 gin_utils.py:85] 
I0512 23:41:54.827646 140311617681408 gin_utils.py:85] # Parameters for adafactor.Adafactor:
I0512 23:41:54.827685 140311617681408 gin_utils.py:85] # ==============================================================================
I0512 23:41:54.827723 140311617681408 gin_utils.py:85] adafactor.Adafactor.decay_rate = 0.8
I0512 23:41:54.827755 140311617681408 gin_utils.py:85] adafactor.Adafactor.logical_factor_rules = @adafactor_utils.logical_factor_rules()
I0512 23:41:54.827788 140311617681408 gin_utils.py:85] adafactor.Adafactor.step_offset = 0
I0512 23:41:54.827820 140311617681408 gin_utils.py:85] 
I0512 23:41:54.827852 140311617681408 gin_utils.py:85] # Parameters for utils.CheckpointConfig:
I0512 23:41:54.827885 140311617681408 gin_utils.py:85] # ==============================================================================
I0512 23:41:54.827918 140311617681408 gin_utils.py:85] utils.CheckpointConfig.restore = @utils.RestoreCheckpointConfig()
I0512 23:41:54.827950 140311617681408 gin_utils.py:85] utils.CheckpointConfig.save = @utils.SaveCheckpointConfig()
I0512 23:41:54.827983 140311617681408 gin_utils.py:85] 
I0512 23:41:54.828016 140311617681408 gin_utils.py:85] # Parameters for moe_partitioning.compute_num_model_partitions:
I0512 23:41:54.828048 140311617681408 gin_utils.py:85] # ==============================================================================
I0512 23:41:54.828081 140311617681408 gin_utils.py:85] moe_partitioning.compute_num_model_partitions.model_parallel_submesh = \
I0512 23:41:54.828114 140311617681408 gin_utils.py:85]     %MODEL_PARALLEL_SUBMESH
I0512 23:41:54.828146 140311617681408 gin_utils.py:85] moe_partitioning.compute_num_model_partitions.num_model_partitions = \
I0512 23:41:54.828179 140311617681408 gin_utils.py:85]     %NUM_MODEL_PARTITIONS
I0512 23:41:54.828211 140311617681408 gin_utils.py:85] 
I0512 23:41:54.828243 140311617681408 gin_utils.py:85] # Parameters for utils.create_learning_rate_scheduler:
I0512 23:41:54.828276 140311617681408 gin_utils.py:85] # ==============================================================================
I0512 23:41:54.828309 140311617681408 gin_utils.py:85] utils.create_learning_rate_scheduler.base_learning_rate = 1.0
I0512 23:41:54.828341 140311617681408 gin_utils.py:85] utils.create_learning_rate_scheduler.factors = 'constant * rsqrt_decay'
I0512 23:41:54.828374 140311617681408 gin_utils.py:85] utils.create_learning_rate_scheduler.warmup_steps = 10000
I0512 23:41:54.828407 140311617681408 gin_utils.py:85] 
I0512 23:41:54.828442 140311617681408 gin_utils.py:85] # Parameters for train/utils.DatasetConfig:
I0512 23:41:54.828475 140311617681408 gin_utils.py:85] # ==============================================================================
I0512 23:41:54.828507 140311617681408 gin_utils.py:85] train/utils.DatasetConfig.batch_size = %BATCH_SIZE
I0512 23:41:54.828540 140311617681408 gin_utils.py:85] train/utils.DatasetConfig.mixture_or_task_name = %MIXTURE_OR_TASK_NAME
I0512 23:41:54.828572 140311617681408 gin_utils.py:85] train/utils.DatasetConfig.module = %MIXTURE_OR_TASK_MODULE
I0512 23:41:54.828605 140311617681408 gin_utils.py:85] train/utils.DatasetConfig.pack = True
I0512 23:41:54.828638 140311617681408 gin_utils.py:85] train/utils.DatasetConfig.seed = None
I0512 23:41:54.828676 140311617681408 gin_utils.py:85] train/utils.DatasetConfig.shuffle = %SHUFFLE_TRAIN_EXAMPLES
I0512 23:41:54.828710 140311617681408 gin_utils.py:85] train/utils.DatasetConfig.split = 'train'
I0512 23:41:54.828743 140311617681408 gin_utils.py:85] train/utils.DatasetConfig.task_feature_lengths = %TASK_FEATURE_LENGTHS
I0512 23:41:54.828776 140311617681408 gin_utils.py:85] train/utils.DatasetConfig.use_cached = %USE_CACHED_TASKS
I0512 23:41:54.828809 140311617681408 gin_utils.py:85] 
I0512 23:41:54.828841 140311617681408 gin_utils.py:85] # Parameters for train_eval/utils.DatasetConfig:
I0512 23:41:54.828873 140311617681408 gin_utils.py:85] # ==============================================================================
I0512 23:41:54.828906 140311617681408 gin_utils.py:85] train_eval/utils.DatasetConfig.batch_size = 128
I0512 23:41:54.828938 140311617681408 gin_utils.py:85] train_eval/utils.DatasetConfig.mixture_or_task_name = %MIXTURE_OR_TASK_NAME
I0512 23:41:54.828970 140311617681408 gin_utils.py:85] train_eval/utils.DatasetConfig.module = %MIXTURE_OR_TASK_MODULE
I0512 23:41:54.829003 140311617681408 gin_utils.py:85] train_eval/utils.DatasetConfig.pack = True
I0512 23:41:54.829035 140311617681408 gin_utils.py:85] train_eval/utils.DatasetConfig.seed = 42
I0512 23:41:54.829068 140311617681408 gin_utils.py:85] train_eval/utils.DatasetConfig.shuffle = False
I0512 23:41:54.829100 140311617681408 gin_utils.py:85] train_eval/utils.DatasetConfig.split = 'validation'
I0512 23:41:54.829132 140311617681408 gin_utils.py:85] train_eval/utils.DatasetConfig.task_feature_lengths = %TASK_FEATURE_LENGTHS
I0512 23:41:54.829164 140311617681408 gin_utils.py:85] train_eval/utils.DatasetConfig.use_cached = %USE_CACHED_TASKS
I0512 23:41:54.829197 140311617681408 gin_utils.py:85] 
I0512 23:41:54.829229 140311617681408 gin_utils.py:85] # Parameters for t5_architecture.DecoderLayer:
I0512 23:41:54.829262 140311617681408 gin_utils.py:85] # ==============================================================================
I0512 23:41:54.829294 140311617681408 gin_utils.py:85] t5_architecture.DecoderLayer.activation_partitioning_dims = \
I0512 23:41:54.829327 140311617681408 gin_utils.py:85]     %ACTIVATION_PARTITIONING_DIMS
I0512 23:41:54.829359 140311617681408 gin_utils.py:85] t5_architecture.DecoderLayer.dropout_factory = %DROPOUT_FACTORY
I0512 23:41:54.829392 140311617681408 gin_utils.py:85] t5_architecture.DecoderLayer.encoder_decoder_attention = None
I0512 23:41:54.829427 140311617681408 gin_utils.py:85] t5_architecture.DecoderLayer.layer_norm_factory = @layer_norm.T5LayerNorm
I0512 23:41:54.829460 140311617681408 gin_utils.py:85] t5_architecture.DecoderLayer.mlp = @dense.MlpBlock()
I0512 23:41:54.829493 140311617681408 gin_utils.py:85] t5_architecture.DecoderLayer.scanned = False
I0512 23:41:54.829526 140311617681408 gin_utils.py:85] t5_architecture.DecoderLayer.self_attention = \
I0512 23:41:54.829559 140311617681408 gin_utils.py:85]     @dense_attention.MultiHeadDotProductAttention()
I0512 23:41:54.829591 140311617681408 gin_utils.py:85] 
I0512 23:41:54.829623 140311617681408 gin_utils.py:85] # Parameters for t5_architecture.DecoderOnly:
I0512 23:41:54.829661 140311617681408 gin_utils.py:85] # ==============================================================================
I0512 23:41:54.829695 140311617681408 gin_utils.py:85] t5_architecture.DecoderOnly.decoder_factory = @moe_architecture.SparseDecoder
I0512 23:41:54.829728 140311617681408 gin_utils.py:85] t5_architecture.DecoderOnly.dtype = %ACTIVATION_DTYPE
I0512 23:41:54.829761 140311617681408 gin_utils.py:85] t5_architecture.DecoderOnly.shared_token_embedder_factory = @embedding.Embed
I0512 23:41:54.829821 140311617681408 gin_utils.py:85] 
I0512 23:41:54.829856 140311617681408 gin_utils.py:85] # Parameters for output_logits/dense.DenseGeneral:
I0512 23:41:54.829889 140311617681408 gin_utils.py:85] # ==============================================================================
I0512 23:41:54.829921 140311617681408 gin_utils.py:85] output_logits/dense.DenseGeneral.bias_init = %BIAS_INIT
I0512 23:41:54.829954 140311617681408 gin_utils.py:85] output_logits/dense.DenseGeneral.dtype = 'float32'
I0512 23:41:54.829987 140311617681408 gin_utils.py:85] output_logits/dense.DenseGeneral.features = %NUM_EMBEDDINGS
I0512 23:41:54.830020 140311617681408 gin_utils.py:85] output_logits/dense.DenseGeneral.kernel_axis_names = ['embed', 'vocab']
I0512 23:41:54.830053 140311617681408 gin_utils.py:85] output_logits/dense.DenseGeneral.kernel_init = \
I0512 23:41:54.830086 140311617681408 gin_utils.py:85]     @output_logits_kernel_init/linen.initializers.variance_scaling()
I0512 23:41:54.830118 140311617681408 gin_utils.py:85] output_logits/dense.DenseGeneral.use_bias = False
I0512 23:41:54.830151 140311617681408 gin_utils.py:85] 
I0512 23:41:54.830183 140311617681408 gin_utils.py:85] # Parameters for dropout_factory/linen.Dropout:
I0512 23:41:54.830216 140311617681408 gin_utils.py:85] # ==============================================================================
I0512 23:41:54.830249 140311617681408 gin_utils.py:85] dropout_factory/linen.Dropout.broadcast_dims = (-2,)
I0512 23:41:54.830281 140311617681408 gin_utils.py:85] dropout_factory/linen.Dropout.rate = %DROPOUT_RATE
I0512 23:41:54.830314 140311617681408 gin_utils.py:85] 
I0512 23:41:54.830346 140311617681408 gin_utils.py:85] # Parameters for embedding.Embed:
I0512 23:41:54.856557 140529434355712 gin_utils.py:85] # ==============================================================================
I0512 23:41:54.856589 140529434355712 gin_utils.py:85] utils.CheckpointConfig.restore = @utils.RestoreCheckpointConfig()
I0512 23:41:54.856622 140529434355712 gin_utils.py:85] utils.CheckpointConfig.save = @utils.SaveCheckpointConfig()
I0512 23:41:54.856655 140529434355712 gin_utils.py:85] 
I0512 23:41:54.856687 140529434355712 gin_utils.py:85] # Parameters for partitioning.compute_num_model_partitions:
I0512 23:41:54.856719 140529434355712 gin_utils.py:85] # ==============================================================================
I0512 23:41:54.856752 140529434355712 gin_utils.py:85] partitioning.compute_num_model_partitions.model_parallel_submesh = \
I0512 23:41:54.856784 140529434355712 gin_utils.py:85]     %MODEL_PARALLEL_SUBMESH
I0512 23:41:54.856817 140529434355712 gin_utils.py:85] partitioning.compute_num_model_partitions.num_model_partitions = \
I0512 23:41:54.856849 140529434355712 gin_utils.py:85]     %NUM_MODEL_PARTITIONS
I0512 23:41:54.856883 140529434355712 gin_utils.py:85] 
I0512 23:41:54.856917 140529434355712 gin_utils.py:85] # Parameters for utils.create_learning_rate_scheduler:
I0512 23:41:54.856950 140529434355712 gin_utils.py:85] # ==============================================================================
I0512 23:41:54.856983 140529434355712 gin_utils.py:85] utils.create_learning_rate_scheduler.base_learning_rate = 1.0
I0512 23:41:54.857016 140529434355712 gin_utils.py:85] utils.create_learning_rate_scheduler.factors = 'constant * rsqrt_decay'
I0512 23:41:54.857048 140529434355712 gin_utils.py:85] utils.create_learning_rate_scheduler.warmup_steps = 10000
I0512 23:41:54.857086 140529434355712 gin_utils.py:85] 
I0512 23:41:54.857122 140529434355712 gin_utils.py:85] # Parameters for train/utils.DatasetConfig:
I0512 23:41:54.857164 140529434355712 gin_utils.py:85] # ==============================================================================
I0512 23:41:54.857198 140529434355712 gin_utils.py:85] train/utils.DatasetConfig.batch_size = %BATCH_SIZE
I0512 23:41:54.857234 140529434355712 gin_utils.py:85] train/utils.DatasetConfig.mixture_or_task_name = %MIXTURE_OR_TASK_NAME
I0512 23:41:54.857270 140529434355712 gin_utils.py:85] train/utils.DatasetConfig.module = %MIXTURE_OR_TASK_MODULE
I0512 23:41:54.857304 140529434355712 gin_utils.py:85] train/utils.DatasetConfig.pack = True
I0512 23:41:54.857340 140529434355712 gin_utils.py:85] train/utils.DatasetConfig.seed = None
I0512 23:41:54.857376 140529434355712 gin_utils.py:85] train/utils.DatasetConfig.shuffle = %SHUFFLE_TRAIN_EXAMPLES
I0512 23:41:54.857409 140529434355712 gin_utils.py:85] train/utils.DatasetConfig.split = 'train'
I0512 23:41:54.857444 140529434355712 gin_utils.py:85] train/utils.DatasetConfig.task_feature_lengths = %TASK_FEATURE_LENGTHS
I0512 23:41:54.857478 140529434355712 gin_utils.py:85] train/utils.DatasetConfig.use_cached = %USE_CACHED_TASKS
I0512 23:41:54.857514 140529434355712 gin_utils.py:85] 
I0512 23:41:54.857568 140529434355712 gin_utils.py:85] # Parameters for train_eval/utils.DatasetConfig:
I0512 23:41:54.857609 140529434355712 gin_utils.py:85] # ==============================================================================
I0512 23:41:54.857645 140529434355712 gin_utils.py:85] train_eval/utils.DatasetConfig.batch_size = 128
I0512 23:41:54.857680 140529434355712 gin_utils.py:85] train_eval/utils.DatasetConfig.mixture_or_task_name = %MIXTURE_OR_TASK_NAME
I0512 23:41:54.857713 140529434355712 gin_utils.py:85] train_eval/utils.DatasetConfig.module = %MIXTURE_OR_TASK_MODULE
I0512 23:41:54.857749 140529434355712 gin_utils.py:85] train_eval/utils.DatasetConfig.pack = True
I0512 23:41:54.857785 140529434355712 gin_utils.py:85] train_eval/utils.DatasetConfig.seed = 42
I0512 23:41:54.857818 140529434355712 gin_utils.py:85] train_eval/utils.DatasetConfig.shuffle = False
I0512 23:41:54.857854 140529434355712 gin_utils.py:85] train_eval/utils.DatasetConfig.split = 'validation'
I0512 23:41:54.857889 140529434355712 gin_utils.py:85] train_eval/utils.DatasetConfig.task_feature_lengths = %TASK_FEATURE_LENGTHS
I0512 23:41:54.857928 140529434355712 gin_utils.py:85] train_eval/utils.DatasetConfig.use_cached = %USE_CACHED_TASKS
I0512 23:41:54.857964 140529434355712 gin_utils.py:85] 
I0512 23:41:54.857998 140529434355712 gin_utils.py:85] # Parameters for t5_architecture.DecoderLayer:
I0512 23:41:54.858033 140529434355712 gin_utils.py:85] # ==============================================================================
I0512 23:41:54.858067 140529434355712 gin_utils.py:85] t5_architecture.DecoderLayer.activation_partitioning_dims = \
I0512 23:41:54.858103 140529434355712 gin_utils.py:85]     %ACTIVATION_PARTITIONING_DIMS
I0512 23:41:54.858143 140529434355712 gin_utils.py:85] t5_architecture.DecoderLayer.dropout_factory = %DROPOUT_FACTORY
I0512 23:41:54.858180 140529434355712 gin_utils.py:85] t5_architecture.DecoderLayer.encoder_decoder_attention = None
I0512 23:41:54.858214 140529434355712 gin_utils.py:85] t5_architecture.DecoderLayer.layer_norm_factory = @layer_norm.T5LayerNorm
I0512 23:41:54.858250 140529434355712 gin_utils.py:85] t5_architecture.DecoderLayer.mlp = @dense.MlpBlock()
I0512 23:41:54.858285 140529434355712 gin_utils.py:85] t5_architecture.DecoderLayer.scanned = False
I0512 23:41:54.858319 140529434355712 gin_utils.py:85] t5_architecture.DecoderLayer.self_attention = \
I0512 23:41:54.858355 140529434355712 gin_utils.py:85]     @dense_attention.MultiHeadDotProductAttention()
I0512 23:41:54.858391 140529434355712 gin_utils.py:85] 
I0512 23:41:54.858423 140529434355712 gin_utils.py:85] # Parameters for t5_architecture.DecoderOnly:
I0512 23:41:54.858459 140529434355712 gin_utils.py:85] # ==============================================================================
I0512 23:41:54.858492 140529434355712 gin_utils.py:85] t5_architecture.DecoderOnly.decoder_factory = @moe_architecture.SparseDecoder
I0512 23:41:54.858528 140529434355712 gin_utils.py:85] t5_architecture.DecoderOnly.dtype = %ACTIVATION_DTYPE
I0512 23:41:54.858564 140529434355712 gin_utils.py:85] t5_architecture.DecoderOnly.shared_token_embedder_factory = @embedding.Embed
I0512 23:41:54.858597 140529434355712 gin_utils.py:85] 
I0512 23:41:54.858633 140529434355712 gin_utils.py:85] # Parameters for output_logits/dense.DenseGeneral:
I0512 23:41:54.858666 140529434355712 gin_utils.py:85] # ==============================================================================
I0512 23:41:54.858702 140529434355712 gin_utils.py:85] output_logits/dense.DenseGeneral.bias_init = %BIAS_INIT
I0512 23:41:54.858735 140529434355712 gin_utils.py:85] output_logits/dense.DenseGeneral.dtype = 'float32'
I0512 23:41:54.858771 140529434355712 gin_utils.py:85] output_logits/dense.DenseGeneral.features = %NUM_EMBEDDINGS
I0512 23:41:54.858804 140529434355712 gin_utils.py:85] output_logits/dense.DenseGeneral.kernel_axis_names = ['embed', 'vocab']
I0512 23:41:54.858840 140529434355712 gin_utils.py:85] output_logits/dense.DenseGeneral.kernel_init = \
I0512 23:41:54.858876 140529434355712 gin_utils.py:85]     @output_logits_kernel_init/linen.initializers.variance_scaling()
I0512 23:41:54.858912 140529434355712 gin_utils.py:85] output_logits/dense.DenseGeneral.use_bias = False
I0512 23:41:54.858949 140529434355712 gin_utils.py:85] 
I0512 23:41:54.858982 140529434355712 gin_utils.py:85] # Parameters for dropout_factory/linen.Dropout:
I0512 23:41:54.859018 140529434355712 gin_utils.py:85] # ==============================================================================
I0512 23:41:54.859051 140529434355712 gin_utils.py:85] dropout_factory/linen.Dropout.broadcast_dims = (-2,)
I0512 23:41:54.859087 140529434355712 gin_utils.py:85] dropout_factory/linen.Dropout.rate = %DROPOUT_RATE
I0512 23:41:54.859123 140529434355712 gin_utils.py:85] 
I0512 23:41:54.859161 140529434355712 gin_utils.py:85] # Parameters for embedding.Embed:
I0512 23:41:54.856799 140595318249472 gin_utils.py:85] dense.MlpBlock.kernel_init = @mlp_kernel_init/linen.initializers.variance_scaling()
I0512 23:41:54.856830 140595318249472 gin_utils.py:85] dense.MlpBlock.output_axis_name = 'mlp_embed'
I0512 23:41:54.856861 140595318249472 gin_utils.py:85] dense.MlpBlock.use_bias = False
I0512 23:41:54.856892 140595318249472 gin_utils.py:85] 
I0512 23:41:54.856923 140595318249472 gin_utils.py:85] # Parameters for expert/dense.MlpBlock:
I0512 23:41:54.856973 140595318249472 gin_utils.py:85] # ==============================================================================
I0512 23:41:54.857012 140595318249472 gin_utils.py:85] expert/dense.MlpBlock.activation_partitioning_dims = 1
I0512 23:41:54.857044 140595318249472 gin_utils.py:85] expert/dense.MlpBlock.activations = ('swiglu', 'linear')
I0512 23:41:54.857076 140595318249472 gin_utils.py:85] expert/dense.MlpBlock.bias_init = %BIAS_INIT
I0512 23:41:54.857107 140595318249472 gin_utils.py:85] expert/dense.MlpBlock.data_sharding_constraints = ('expert_replicas', 'mlp')
I0512 23:41:54.857138 140595318249472 gin_utils.py:85] expert/dense.MlpBlock.dtype = %MOE_TRUNCATED_DTYPE
I0512 23:41:54.857168 140595318249472 gin_utils.py:85] expert/dense.MlpBlock.final_dropout_rate = 0.0
I0512 23:41:54.857206 140595318249472 gin_utils.py:85] expert/dense.MlpBlock.input_axis_name = 'mlp_embed'
I0512 23:41:54.857238 140595318249472 gin_utils.py:85] expert/dense.MlpBlock.intermediate_axis_name = 'expert_mlp'
I0512 23:41:54.857269 140595318249472 gin_utils.py:85] expert/dense.MlpBlock.intermediate_dim = %EXPERT_MLP_DIM
I0512 23:41:54.857299 140595318249472 gin_utils.py:85] expert/dense.MlpBlock.intermediate_dropout_rate = %EXPERT_DROPOUT_RATE
I0512 23:41:54.857330 140595318249472 gin_utils.py:85] expert/dense.MlpBlock.kernel_init = \
I0512 23:41:54.857361 140595318249472 gin_utils.py:85]     @expert_kernel_init/linen.initializers.variance_scaling()
I0512 23:41:54.857391 140595318249472 gin_utils.py:85] expert/dense.MlpBlock.output_axis_name = 'mlp_embed'
I0512 23:41:54.830379 140311617681408 gin_utils.py:85] # ==============================================================================
I0512 23:41:54.830412 140311617681408 gin_utils.py:85] embedding.Embed.attend_dtype = 'float32'
I0512 23:41:54.830447 140311617681408 gin_utils.py:85] embedding.Embed.cast_input_dtype = 'int32'
I0512 23:41:54.830480 140311617681408 gin_utils.py:85] embedding.Embed.dtype = %ACTIVATION_DTYPE
I0512 23:41:54.830513 140311617681408 gin_utils.py:85] embedding.Embed.embedding_init = @token_embedder_init/linen.initializers.normal()
I0512 23:41:54.830546 140311617681408 gin_utils.py:85] embedding.Embed.features = %EMBED_DIM
I0512 23:41:54.830579 140311617681408 gin_utils.py:85] embedding.Embed.name = 'token_embedder'
I0512 23:41:54.830611 140311617681408 gin_utils.py:85] embedding.Embed.num_embeddings = %NUM_EMBEDDINGS
I0512 23:41:54.830643 140311617681408 gin_utils.py:85] embedding.Embed.one_hot = True
I0512 23:41:54.830682 140311617681408 gin_utils.py:85] 
I0512 23:41:54.830720 140311617681408 gin_utils.py:85] # Parameters for dense.MlpBlock:
I0512 23:41:54.830753 140311617681408 gin_utils.py:85] # ==============================================================================
I0512 23:41:54.830786 140311617681408 gin_utils.py:85] dense.MlpBlock.activations = ('swiglu', 'linear')
I0512 23:41:54.830819 140311617681408 gin_utils.py:85] dense.MlpBlock.bias_init = %BIAS_INIT
I0512 23:41:54.830852 140311617681408 gin_utils.py:85] dense.MlpBlock.dtype = %ACTIVATION_DTYPE
I0512 23:41:54.830885 140311617681408 gin_utils.py:85] dense.MlpBlock.final_dropout_rate = 0
I0512 23:41:54.830917 140311617681408 gin_utils.py:85] dense.MlpBlock.input_axis_name = 'mlp_embed'
I0512 23:41:54.830949 140311617681408 gin_utils.py:85] dense.MlpBlock.intermediate_dim = %MLP_DIM
I0512 23:41:54.830982 140311617681408 gin_utils.py:85] dense.MlpBlock.intermediate_dropout_rate = %DROPOUT_RATE
I0512 23:41:54.831014 140311617681408 gin_utils.py:85] dense.MlpBlock.kernel_init = @mlp_kernel_init/linen.initializers.variance_scaling()
I0512 23:41:54.831047 140311617681408 gin_utils.py:85] dense.MlpBlock.output_axis_name = 'mlp_embed'
I0512 23:41:54.831079 140311617681408 gin_utils.py:85] dense.MlpBlock.use_bias = False
I0512 23:41:54.831111 140311617681408 gin_utils.py:85] 
I0512 23:41:54.831143 140311617681408 gin_utils.py:85] # Parameters for expert/dense.MlpBlock:
I0512 23:41:54.831176 140311617681408 gin_utils.py:85] # ==============================================================================
I0512 23:41:54.831208 140311617681408 gin_utils.py:85] expert/dense.MlpBlock.activation_partitioning_dims = 1
I0512 23:41:54.831241 140311617681408 gin_utils.py:85] expert/dense.MlpBlock.activations = ('swiglu', 'linear')
I0512 23:41:54.831274 140311617681408 gin_utils.py:85] expert/dense.MlpBlock.bias_init = %BIAS_INIT
I0512 23:41:54.831306 140311617681408 gin_utils.py:85] expert/dense.MlpBlock.data_sharding_constraints = ('expert_replicas', 'mlp')
I0512 23:41:54.831339 140311617681408 gin_utils.py:85] expert/dense.MlpBlock.dtype = %MOE_TRUNCATED_DTYPE
I0512 23:41:54.831371 140311617681408 gin_utils.py:85] expert/dense.MlpBlock.final_dropout_rate = 0.0
I0512 23:41:54.831404 140311617681408 gin_utils.py:85] expert/dense.MlpBlock.input_axis_name = 'mlp_embed'
I0512 23:41:54.831439 140311617681408 gin_utils.py:85] expert/dense.MlpBlock.intermediate_axis_name = 'expert_mlp'
I0512 23:41:54.831472 140311617681408 gin_utils.py:85] expert/dense.MlpBlock.intermediate_dim = %EXPERT_MLP_DIM
I0512 23:41:54.831504 140311617681408 gin_utils.py:85] expert/dense.MlpBlock.intermediate_dropout_rate = %EXPERT_DROPOUT_RATE
I0512 23:41:54.831537 140311617681408 gin_utils.py:85] expert/dense.MlpBlock.kernel_init = \
I0512 23:41:54.831569 140311617681408 gin_utils.py:85]     @expert_kernel_init/linen.initializers.variance_scaling()
I0512 23:41:54.831601 140311617681408 gin_utils.py:85] expert/dense.MlpBlock.output_axis_name = 'mlp_embed'
I0512 23:41:54.831634 140311617681408 gin_utils.py:85] expert/dense.MlpBlock.use_bias = False
I0512 23:41:54.554589 139674595104768 gin_utils.py:85] train_script.train.eval_steps = 20
I0512 23:41:54.554621 139674595104768 gin_utils.py:85] train_script.train.infer_eval_dataset_cfg = None
I0512 23:41:54.554653 139674595104768 gin_utils.py:85] train_script.train.model = %MODEL
I0512 23:41:54.554692 139674595104768 gin_utils.py:85] train_script.train.model_dir = %MODEL_DIR
I0512 23:41:54.554725 139674595104768 gin_utils.py:85] train_script.train.partitioner = @moe_partitioning.MoePjitPartitioner()
I0512 23:41:54.554757 139674595104768 gin_utils.py:85] train_script.train.random_seed = %RANDOM_SEED
I0512 23:41:54.554789 139674595104768 gin_utils.py:85] train_script.train.stats_period = 10
I0512 23:41:54.554820 139674595104768 gin_utils.py:85] train_script.train.summarize_config_fn = @gin_utils.summarize_gin_config
I0512 23:41:54.554852 139674595104768 gin_utils.py:85] train_script.train.total_steps = %TRAIN_STEPS
I0512 23:41:54.554884 139674595104768 gin_utils.py:85] train_script.train.train_dataset_cfg = @train/utils.DatasetConfig()
I0512 23:41:54.859198 140529434355712 gin_utils.py:85] # ==============================================================================
I0512 23:41:54.859231 140529434355712 gin_utils.py:85] embedding.Embed.attend_dtype = 'float32'
I0512 23:41:54.859268 140529434355712 gin_utils.py:85] embedding.Embed.cast_input_dtype = 'int32'
I0512 23:41:54.859301 140529434355712 gin_utils.py:85] embedding.Embed.dtype = %ACTIVATION_DTYPE
I0512 23:41:54.859338 140529434355712 gin_utils.py:85] embedding.Embed.embedding_init = @token_embedder_init/linen.initializers.normal()
I0512 23:41:54.859373 140529434355712 gin_utils.py:85] embedding.Embed.features = %EMBED_DIM
I0512 23:41:54.859406 140529434355712 gin_utils.py:85] embedding.Embed.name = 'token_embedder'
I0512 23:41:54.859442 140529434355712 gin_utils.py:85] embedding.Embed.num_embeddings = %NUM_EMBEDDINGS
I0512 23:41:54.859475 140529434355712 gin_utils.py:85] embedding.Embed.one_hot = True
I0512 23:41:54.859511 140529434355712 gin_utils.py:85] 
I0512 23:41:54.859544 140529434355712 gin_utils.py:85] # Parameters for dense.MlpBlock:
I0512 23:41:54.859580 140529434355712 gin_utils.py:85] # ==============================================================================
I0512 23:41:54.859616 140529434355712 gin_utils.py:85] dense.MlpBlock.activations = ('swiglu', 'linear')
I0512 23:41:54.859650 140529434355712 gin_utils.py:85] dense.MlpBlock.bias_init = %BIAS_INIT
I0512 23:41:54.859685 140529434355712 gin_utils.py:85] dense.MlpBlock.dtype = %ACTIVATION_DTYPE
I0512 23:41:54.859719 140529434355712 gin_utils.py:85] dense.MlpBlock.final_dropout_rate = 0
I0512 23:41:54.859755 140529434355712 gin_utils.py:85] dense.MlpBlock.input_axis_name = 'mlp_embed'
I0512 23:41:54.859788 140529434355712 gin_utils.py:85] dense.MlpBlock.intermediate_dim = %MLP_DIM
I0512 23:41:54.859824 140529434355712 gin_utils.py:85] dense.MlpBlock.intermediate_dropout_rate = %DROPOUT_RATE
I0512 23:41:54.859858 140529434355712 gin_utils.py:85] dense.MlpBlock.kernel_init = @mlp_kernel_init/linen.initializers.variance_scaling()
I0512 23:41:54.859895 140529434355712 gin_utils.py:85] dense.MlpBlock.output_axis_name = 'mlp_embed'
I0512 23:41:54.859932 140529434355712 gin_utils.py:85] dense.MlpBlock.use_bias = False
I0512 23:41:54.859966 140529434355712 gin_utils.py:85] 
I0512 23:41:54.860001 140529434355712 gin_utils.py:85] # Parameters for expert/dense.MlpBlock:
I0512 23:41:54.860034 140529434355712 gin_utils.py:85] # ==============================================================================
I0512 23:41:54.860067 140529434355712 gin_utils.py:85] expert/dense.MlpBlock.activation_partitioning_dims = 1
I0512 23:41:54.860103 140529434355712 gin_utils.py:85] expert/dense.MlpBlock.activations = ('swiglu', 'linear')
I0512 23:41:54.860142 140529434355712 gin_utils.py:85] expert/dense.MlpBlock.bias_init = %BIAS_INIT
I0512 23:41:54.860176 140529434355712 gin_utils.py:85] expert/dense.MlpBlock.data_sharding_constraints = ('expert_replicas', 'mlp')
I0512 23:41:54.860209 140529434355712 gin_utils.py:85] expert/dense.MlpBlock.dtype = %MOE_TRUNCATED_DTYPE
I0512 23:41:54.860242 140529434355712 gin_utils.py:85] expert/dense.MlpBlock.final_dropout_rate = 0.0
I0512 23:41:54.860274 140529434355712 gin_utils.py:85] expert/dense.MlpBlock.input_axis_name = 'mlp_embed'
I0512 23:41:54.860307 140529434355712 gin_utils.py:85] expert/dense.MlpBlock.intermediate_axis_name = 'expert_mlp'
I0512 23:41:54.860343 140529434355712 gin_utils.py:85] expert/dense.MlpBlock.intermediate_dim = %EXPERT_MLP_DIM
I0512 23:41:54.860379 140529434355712 gin_utils.py:85] expert/dense.MlpBlock.intermediate_dropout_rate = %EXPERT_DROPOUT_RATE
I0512 23:41:54.860412 140529434355712 gin_utils.py:85] expert/dense.MlpBlock.kernel_init = \
I0512 23:41:54.860445 140529434355712 gin_utils.py:85]     @expert_kernel_init/linen.initializers.variance_scaling()
I0512 23:41:54.860481 140529434355712 gin_utils.py:85] expert/dense.MlpBlock.output_axis_name = 'mlp_embed'
I0512 23:41:54.860514 140529434355712 gin_utils.py:85] expert/dense.MlpBlock.use_bias = False
I0512 23:41:54.860550 140529434355712 gin_utils.py:85] 
I0512 23:41:54.857422 140595318249472 gin_utils.py:85] expert/dense.MlpBlock.use_bias = False
I0512 23:41:54.857456 140595318249472 gin_utils.py:85] 
I0512 23:41:54.857487 140595318249472 gin_utils.py:85] # Parameters for models.MoeDecoderOnlyModel:
I0512 23:41:54.857518 140595318249472 gin_utils.py:85] # ==============================================================================
I0512 23:41:54.857549 140595318249472 gin_utils.py:85] models.MoeDecoderOnlyModel.aux_loss_factor = %AUX_LOSS_FACTOR
I0512 23:41:54.857580 140595318249472 gin_utils.py:85] models.MoeDecoderOnlyModel.label_smoothing = %LABEL_SMOOTHING
I0512 23:41:54.857611 140595318249472 gin_utils.py:85] models.MoeDecoderOnlyModel.loss_normalizing_factor = %LOSS_NORMALIZING_FACTOR
I0512 23:41:54.857641 140595318249472 gin_utils.py:85] models.MoeDecoderOnlyModel.module = %ARCHITECTURE
I0512 23:41:54.857672 140595318249472 gin_utils.py:85] models.MoeDecoderOnlyModel.optimizer_def = %OPTIMIZER
I0512 23:41:54.857703 140595318249472 gin_utils.py:85] models.MoeDecoderOnlyModel.router_z_loss_factor = %ROUTER_Z_LOSS_FACTOR
I0512 23:41:54.857735 140595318249472 gin_utils.py:85] models.MoeDecoderOnlyModel.vocabulary = %VOCABULARY
I0512 23:41:54.857765 140595318249472 gin_utils.py:85] models.MoeDecoderOnlyModel.z_loss = %Z_LOSS
I0512 23:41:54.857796 140595318249472 gin_utils.py:85] 
I0512 23:41:54.857827 140595318249472 gin_utils.py:85] # Parameters for moe_layers.MoeLayer:
I0512 23:41:54.857858 140595318249472 gin_utils.py:85] # ==============================================================================
I0512 23:41:54.857889 140595318249472 gin_utils.py:85] moe_layers.MoeLayer.dtype = %MOE_TRUNCATED_DTYPE
I0512 23:41:54.857920 140595318249472 gin_utils.py:85] moe_layers.MoeLayer.eval_capacity_factor = %EVAL_EXPERT_CAPACITY_FACTOR
I0512 23:41:54.857951 140595318249472 gin_utils.py:85] moe_layers.MoeLayer.expert = @expert/dense.MlpBlock()
I0512 23:41:54.857982 140595318249472 gin_utils.py:85] moe_layers.MoeLayer.max_group_size = %GROUP_SIZE
I0512 23:41:54.858012 140595318249472 gin_utils.py:85] moe_layers.MoeLayer.num_expert_partitions = %NUM_EXPERT_PARTITIONS
I0512 23:41:54.858043 140595318249472 gin_utils.py:85] moe_layers.MoeLayer.num_experts = %NUM_EXPERTS
I0512 23:41:54.858074 140595318249472 gin_utils.py:85] moe_layers.MoeLayer.num_model_partitions = \
I0512 23:41:54.858105 140595318249472 gin_utils.py:85]     @moe_partitioning.compute_num_model_partitions()
I0512 23:41:54.858136 140595318249472 gin_utils.py:85] moe_layers.MoeLayer.train_capacity_factor = %TRAIN_EXPERT_CAPACITY_FACTOR
I0512 23:41:54.858166 140595318249472 gin_utils.py:85] 
I0512 23:41:54.858203 140595318249472 gin_utils.py:85] # Parameters for sparse_decoder/moe_layers.MoeLayer:
I0512 23:41:54.858234 140595318249472 gin_utils.py:85] # ==============================================================================
I0512 23:41:54.858265 140595318249472 gin_utils.py:85] sparse_decoder/moe_layers.MoeLayer.router = \
I0512 23:41:54.858296 140595318249472 gin_utils.py:85]     @sparse_decoder/routing.TokensChooseMaskedRouter()
I0512 23:41:54.858327 140595318249472 gin_utils.py:85] 
I0512 23:41:54.858357 140595318249472 gin_utils.py:85] # Parameters for moe_partitioning.MoePjitPartitioner:
I0512 23:41:54.858388 140595318249472 gin_utils.py:85] # ==============================================================================
I0512 23:41:54.858419 140595318249472 gin_utils.py:85] moe_partitioning.MoePjitPartitioner.model_parallel_submesh = \
I0512 23:41:54.858452 140595318249472 gin_utils.py:85]     %MODEL_PARALLEL_SUBMESH
I0512 23:41:54.858484 140595318249472 gin_utils.py:85] moe_partitioning.MoePjitPartitioner.num_expert_partitions = %NUM_EXPERT_PARTITIONS
I0512 23:41:54.858515 140595318249472 gin_utils.py:85] moe_partitioning.MoePjitPartitioner.num_partitions = %NUM_MODEL_PARTITIONS
I0512 23:41:54.858546 140595318249472 gin_utils.py:85] 
I0512 23:41:54.858576 140595318249472 gin_utils.py:85] # Parameters for moe_trainer.MoeTrainer:
I0512 23:41:54.858607 140595318249472 gin_utils.py:85] # ==============================================================================
I0512 23:41:54.858637 140595318249472 gin_utils.py:85] moe_trainer.MoeTrainer.learning_rate_fn = @utils.create_learning_rate_scheduler()
I0512 23:41:54.858668 140595318249472 gin_utils.py:85] moe_trainer.MoeTrainer.num_expert_partitions = %NUM_EXPERT_PARTITIONS
I0512 23:41:54.858699 140595318249472 gin_utils.py:85] moe_trainer.MoeTrainer.num_microbatches = 8
I0512 23:41:54.858730 140595318249472 gin_utils.py:85] 
I0512 23:41:54.858760 140595318249472 gin_utils.py:85] # Parameters for dense_attention.MultiHeadDotProductAttention:
I0512 23:41:54.858791 140595318249472 gin_utils.py:85] # ==============================================================================
I0512 23:41:54.858822 140595318249472 gin_utils.py:85] dense_attention.MultiHeadDotProductAttention.bias_init = %BIAS_INIT
I0512 23:41:54.858853 140595318249472 gin_utils.py:85] dense_attention.MultiHeadDotProductAttention.broadcast_dropout = True
I0512 23:41:54.858884 140595318249472 gin_utils.py:85] dense_attention.MultiHeadDotProductAttention.dropout_rate = %DROPOUT_RATE
I0512 23:41:54.858915 140595318249472 gin_utils.py:85] dense_attention.MultiHeadDotProductAttention.dtype = %ACTIVATION_DTYPE
I0512 23:41:54.858946 140595318249472 gin_utils.py:85] dense_attention.MultiHeadDotProductAttention.head_dim = %HEAD_DIM
I0512 23:41:54.858977 140595318249472 gin_utils.py:85] dense_attention.MultiHeadDotProductAttention.kernel_init = \
I0512 23:41:54.859007 140595318249472 gin_utils.py:85]     @attention_kernel_init/linen.initializers.variance_scaling()
I0512 23:41:54.859038 140595318249472 gin_utils.py:85] dense_attention.MultiHeadDotProductAttention.num_heads = %NUM_HEADS
I0512 23:41:54.859069 140595318249472 gin_utils.py:85] dense_attention.MultiHeadDotProductAttention.use_bias = False
I0512 23:41:54.859099 140595318249472 gin_utils.py:85] dense_attention.MultiHeadDotProductAttention.use_rotary_embedding = True
I0512 23:41:54.859130 140595318249472 gin_utils.py:85] 
I0512 23:41:54.859160 140595318249472 gin_utils.py:85] # Parameters for bias_init/linen.initializers.normal:
I0512 23:41:54.859197 140595318249472 gin_utils.py:85] # ==============================================================================
I0512 23:41:54.859228 140595318249472 gin_utils.py:85] bias_init/linen.initializers.normal.stddev = 1e-06
I0512 23:41:54.859260 140595318249472 gin_utils.py:85] 
I0512 23:41:54.859292 140595318249472 gin_utils.py:85] # Parameters for router_init/linen.initializers.normal:
I0512 23:41:54.859323 140595318249472 gin_utils.py:85] # ==============================================================================
I0512 23:41:54.859355 140595318249472 gin_utils.py:85] router_init/linen.initializers.normal.stddev = 0.02
I0512 23:41:54.859385 140595318249472 gin_utils.py:85] 
I0512 23:41:54.859416 140595318249472 gin_utils.py:85] # Parameters for token_embedder_init/linen.initializers.normal:
I0512 23:41:54.859449 140595318249472 gin_utils.py:85] # ==============================================================================
I0512 23:41:54.859481 140595318249472 gin_utils.py:85] token_embedder_init/linen.initializers.normal.stddev = 1.0
I0512 23:41:54.859512 140595318249472 gin_utils.py:85] 
I0512 23:41:54.859543 140595318249472 gin_utils.py:85] # Parameters for partitioning.PjitPartitioner:
I0512 23:41:54.859574 140595318249472 gin_utils.py:85] # ==============================================================================
I0512 23:41:54.859604 140595318249472 gin_utils.py:85] partitioning.PjitPartitioner.logical_axis_rules = \
I0512 23:41:54.859635 140595318249472 gin_utils.py:85]     @partitioning.standard_logical_axis_rules()
I0512 23:41:54.859665 140595318249472 gin_utils.py:85] partitioning.PjitPartitioner.model_parallel_submesh = None
I0512 23:41:54.859696 140595318249472 gin_utils.py:85] partitioning.PjitPartitioner.num_partitions = 1
I0512 23:41:54.859726 140595318249472 gin_utils.py:85] 
I0512 23:41:54.859757 140595318249472 gin_utils.py:85] # Parameters for utils.RestoreCheckpointConfig:
I0512 23:41:54.859787 140595318249472 gin_utils.py:85] # ==============================================================================
I0512 23:41:54.859818 140595318249472 gin_utils.py:85] utils.RestoreCheckpointConfig.path = []
I0512 23:41:54.859848 140595318249472 gin_utils.py:85] 
I0512 23:41:54.859879 140595318249472 gin_utils.py:85] # Parameters for routing.RouterWeights:
I0512 23:41:54.859910 140595318249472 gin_utils.py:85] # ==============================================================================
I0512 23:41:54.859941 140595318249472 gin_utils.py:85] routing.RouterWeights.bias_init = %BIAS_INIT
I0512 23:41:54.859972 140595318249472 gin_utils.py:85] routing.RouterWeights.dtype = 'float32'
I0512 23:41:54.860002 140595318249472 gin_utils.py:85] routing.RouterWeights.kernel_init = @router_init/linen.initializers.normal()
I0512 23:41:54.860033 140595318249472 gin_utils.py:85] routing.RouterWeights.use_bias = False
I0512 23:41:54.860064 140595318249472 gin_utils.py:85] 
I0512 23:41:54.860095 140595318249472 gin_utils.py:85] # Parameters for utils.SaveCheckpointConfig:
I0512 23:41:54.860125 140595318249472 gin_utils.py:85] # ==============================================================================
I0512 23:41:54.860156 140595318249472 gin_utils.py:85] utils.SaveCheckpointConfig.dtype = 'float32'
I0512 23:41:54.860192 140595318249472 gin_utils.py:85] utils.SaveCheckpointConfig.keep = 20
I0512 23:41:54.860224 140595318249472 gin_utils.py:85] utils.SaveCheckpointConfig.period = 2500
I0512 23:41:54.860254 140595318249472 gin_utils.py:85] utils.SaveCheckpointConfig.save_dataset = False
I0512 23:41:54.860285 140595318249472 gin_utils.py:85] 
I0512 23:41:54.860315 140595318249472 gin_utils.py:85] # Parameters for seqio.SentencePieceVocabulary:
I0512 23:41:54.860346 140595318249472 gin_utils.py:85] # ==============================================================================
I0512 23:41:54.860376 140595318249472 gin_utils.py:85] seqio.SentencePieceVocabulary.extra_ids = 300
I0512 23:41:54.860407 140595318249472 gin_utils.py:85] seqio.SentencePieceVocabulary.sentencepiece_model_file = \
I0512 23:41:54.860439 140595318249472 gin_utils.py:85]     'gs://rosinality-tpu-bucket/sentencepiece.model'
I0512 23:41:54.860470 140595318249472 gin_utils.py:85] 
I0512 23:41:54.860501 140595318249472 gin_utils.py:85] # Parameters for moe_architecture.SparseDecoder:
I0512 23:41:54.860532 140595318249472 gin_utils.py:85] # ==============================================================================
I0512 23:41:54.860563 140595318249472 gin_utils.py:85] moe_architecture.SparseDecoder.dropout_factory = %DROPOUT_FACTORY
I0512 23:41:54.860594 140595318249472 gin_utils.py:85] moe_architecture.SparseDecoder.dtype = %ACTIVATION_DTYPE
I0512 23:41:54.860624 140595318249472 gin_utils.py:85] moe_architecture.SparseDecoder.layer_factory = @t5_architecture.DecoderLayer
I0512 23:41:54.860655 140595318249472 gin_utils.py:85] moe_architecture.SparseDecoder.layer_norm_factory = @layer_norm.T5LayerNorm
I0512 23:41:54.860686 140595318249472 gin_utils.py:85] moe_architecture.SparseDecoder.num_layers = %NUM_DECODER_LAYERS
I0512 23:41:54.860717 140595318249472 gin_utils.py:85] moe_architecture.SparseDecoder.num_sparse_layers = %NUM_DECODER_SPARSE_LAYERS
I0512 23:41:54.860747 140595318249472 gin_utils.py:85] moe_architecture.SparseDecoder.output_logits_factory = \
I0512 23:41:54.860778 140595318249472 gin_utils.py:85]     @output_logits/dense.DenseGeneral
I0512 23:41:54.860809 140595318249472 gin_utils.py:85] moe_architecture.SparseDecoder.sparse_layer_factory = \
I0512 23:41:54.860839 140595318249472 gin_utils.py:85]     @moe_architecture.SparseDecoderLayer
I0512 23:41:54.860870 140595318249472 gin_utils.py:85] moe_architecture.SparseDecoder.sparse_layout = %DECODER_SPARSE_LAYOUT
I0512 23:41:54.860901 140595318249472 gin_utils.py:85] 
I0512 23:41:54.860932 140595318249472 gin_utils.py:85] # Parameters for moe_architecture.SparseDecoderLayer:
I0512 23:41:54.860984 140595318249472 gin_utils.py:85] # ==============================================================================
I0512 23:41:54.861021 140595318249472 gin_utils.py:85] moe_architecture.SparseDecoderLayer.activation_partitioning_dims = \
I0512 23:41:54.861052 140595318249472 gin_utils.py:85]     %ACTIVATION_PARTITIONING_DIMS
I0512 23:41:54.861083 140595318249472 gin_utils.py:85] moe_architecture.SparseDecoderLayer.dropout_factory = %DROPOUT_FACTORY
I0512 23:41:54.861114 140595318249472 gin_utils.py:85] moe_architecture.SparseDecoderLayer.encoder_decoder_attention = None
I0512 23:41:54.861146 140595318249472 gin_utils.py:85] moe_architecture.SparseDecoderLayer.extra_mlp = @dense.MlpBlock()
I0512 23:41:54.861182 140595318249472 gin_utils.py:85] moe_architecture.SparseDecoderLayer.layer_norm_factory = @layer_norm.T5LayerNorm
I0512 23:41:54.861214 140595318249472 gin_utils.py:85] moe_architecture.SparseDecoderLayer.mlp = @sparse_decoder/moe_layers.MoeLayer()
I0512 23:41:54.861246 140595318249472 gin_utils.py:85] moe_architecture.SparseDecoderLayer.scanned = False
I0512 23:41:54.861277 140595318249472 gin_utils.py:85] moe_architecture.SparseDecoderLayer.self_attention = \
I0512 23:41:54.861308 140595318249472 gin_utils.py:85]     @dense_attention.MultiHeadDotProductAttention()
I0512 23:41:54.861339 140595318249472 gin_utils.py:85] 
I0512 23:41:54.861370 140595318249472 gin_utils.py:85] # Parameters for layer_norm.T5LayerNorm:
I0512 23:41:54.861401 140595318249472 gin_utils.py:85] # ==============================================================================
I0512 23:41:54.861433 140595318249472 gin_utils.py:85] layer_norm.T5LayerNorm.dtype = %ACTIVATION_DTYPE
I0512 23:41:54.861465 140595318249472 gin_utils.py:85] 
I0512 23:41:54.861497 140595318249472 gin_utils.py:85] # Parameters for routing.TokensChooseMaskedRouter:
I0512 23:41:54.861528 140595318249472 gin_utils.py:85] # ==============================================================================
I0512 23:41:54.861558 140595318249472 gin_utils.py:85] routing.TokensChooseMaskedRouter.dtype = 'float32'
I0512 23:41:54.861589 140595318249472 gin_utils.py:85] routing.TokensChooseMaskedRouter.ignore_padding_tokens = False
I0512 23:41:54.861620 140595318249472 gin_utils.py:85] routing.TokensChooseMaskedRouter.jitter_noise = %JITTER_NOISE
I0512 23:41:54.861650 140595318249472 gin_utils.py:85] routing.TokensChooseMaskedRouter.num_selected_experts = %NUM_SELECTED_EXPERTS
I0512 23:41:54.861681 140595318249472 gin_utils.py:85] routing.TokensChooseMaskedRouter.router_weights = @routing.RouterWeights()
I0512 23:41:54.861711 140595318249472 gin_utils.py:85] 
I0512 23:41:54.861742 140595318249472 gin_utils.py:85] # Parameters for sparse_decoder/routing.TokensChooseMaskedRouter:
I0512 23:41:54.861773 140595318249472 gin_utils.py:85] # ==============================================================================
I0512 23:41:54.861803 140595318249472 gin_utils.py:85] sparse_decoder/routing.TokensChooseMaskedRouter.batch_prioritized_routing = False
I0512 23:41:54.861834 140595318249472 gin_utils.py:85] 
I0512 23:41:54.861865 140595318249472 gin_utils.py:85] # Parameters for sparse_encoder/routing.TokensChooseMaskedRouter:
I0512 23:41:54.861895 140595318249472 gin_utils.py:85] # ==============================================================================
I0512 23:41:54.861925 140595318249472 gin_utils.py:85] sparse_encoder/routing.TokensChooseMaskedRouter.batch_prioritized_routing = False
I0512 23:41:54.861956 140595318249472 gin_utils.py:85] 
I0512 23:41:54.861987 140595318249472 gin_utils.py:85] # Parameters for train_script.train:
I0512 23:41:54.862019 140595318249472 gin_utils.py:85] # ==============================================================================
I0512 23:41:54.862050 140595318249472 gin_utils.py:85] train_script.train.checkpoint_cfg = @utils.CheckpointConfig()
I0512 23:41:54.862082 140595318249472 gin_utils.py:85] train_script.train.eval_period = 2500
I0512 23:41:54.831671 140311617681408 gin_utils.py:85] 
I0512 23:41:54.831705 140311617681408 gin_utils.py:85] # Parameters for models.MoeDecoderOnlyModel:
I0512 23:41:54.831737 140311617681408 gin_utils.py:85] # ==============================================================================
I0512 23:41:54.831770 140311617681408 gin_utils.py:85] models.MoeDecoderOnlyModel.aux_loss_factor = %AUX_LOSS_FACTOR
I0512 23:41:54.831803 140311617681408 gin_utils.py:85] models.MoeDecoderOnlyModel.label_smoothing = %LABEL_SMOOTHING
I0512 23:41:54.831836 140311617681408 gin_utils.py:85] models.MoeDecoderOnlyModel.loss_normalizing_factor = %LOSS_NORMALIZING_FACTOR
I0512 23:41:54.831868 140311617681408 gin_utils.py:85] models.MoeDecoderOnlyModel.module = %ARCHITECTURE
I0512 23:41:54.831901 140311617681408 gin_utils.py:85] models.MoeDecoderOnlyModel.optimizer_def = %OPTIMIZER
I0512 23:41:54.831934 140311617681408 gin_utils.py:85] models.MoeDecoderOnlyModel.router_z_loss_factor = %ROUTER_Z_LOSS_FACTOR
I0512 23:41:54.831967 140311617681408 gin_utils.py:85] models.MoeDecoderOnlyModel.vocabulary = %VOCABULARY
I0512 23:41:54.831999 140311617681408 gin_utils.py:85] models.MoeDecoderOnlyModel.z_loss = %Z_LOSS
I0512 23:41:54.832031 140311617681408 gin_utils.py:85] 
I0512 23:41:54.832064 140311617681408 gin_utils.py:85] # Parameters for moe_layers.MoeLayer:
I0512 23:41:54.832096 140311617681408 gin_utils.py:85] # ==============================================================================
I0512 23:41:54.832129 140311617681408 gin_utils.py:85] moe_layers.MoeLayer.dtype = %MOE_TRUNCATED_DTYPE
I0512 23:41:54.832161 140311617681408 gin_utils.py:85] moe_layers.MoeLayer.eval_capacity_factor = %EVAL_EXPERT_CAPACITY_FACTOR
I0512 23:41:54.832194 140311617681408 gin_utils.py:85] moe_layers.MoeLayer.expert = @expert/dense.MlpBlock()
I0512 23:41:54.832226 140311617681408 gin_utils.py:85] moe_layers.MoeLayer.max_group_size = %GROUP_SIZE
I0512 23:41:54.832259 140311617681408 gin_utils.py:85] moe_layers.MoeLayer.num_expert_partitions = %NUM_EXPERT_PARTITIONS
I0512 23:41:54.832292 140311617681408 gin_utils.py:85] moe_layers.MoeLayer.num_experts = %NUM_EXPERTS
I0512 23:41:54.832324 140311617681408 gin_utils.py:85] moe_layers.MoeLayer.num_model_partitions = \
I0512 23:41:54.832357 140311617681408 gin_utils.py:85]     @moe_partitioning.compute_num_model_partitions()
I0512 23:41:54.832389 140311617681408 gin_utils.py:85] moe_layers.MoeLayer.train_capacity_factor = %TRAIN_EXPERT_CAPACITY_FACTOR
I0512 23:41:54.832423 140311617681408 gin_utils.py:85] 
I0512 23:41:54.832457 140311617681408 gin_utils.py:85] # Parameters for sparse_decoder/moe_layers.MoeLayer:
I0512 23:41:54.832489 140311617681408 gin_utils.py:85] # ==============================================================================
I0512 23:41:54.832522 140311617681408 gin_utils.py:85] sparse_decoder/moe_layers.MoeLayer.router = \
I0512 23:41:54.832554 140311617681408 gin_utils.py:85]     @sparse_decoder/routing.TokensChooseMaskedRouter()
I0512 23:41:54.832587 140311617681408 gin_utils.py:85] 
I0512 23:41:54.832620 140311617681408 gin_utils.py:85] # Parameters for moe_partitioning.MoePjitPartitioner:
I0512 23:41:54.832652 140311617681408 gin_utils.py:85] # ==============================================================================
I0512 23:41:54.832691 140311617681408 gin_utils.py:85] moe_partitioning.MoePjitPartitioner.model_parallel_submesh = \
I0512 23:41:54.832725 140311617681408 gin_utils.py:85]     %MODEL_PARALLEL_SUBMESH
I0512 23:41:54.832757 140311617681408 gin_utils.py:85] moe_partitioning.MoePjitPartitioner.num_expert_partitions = %NUM_EXPERT_PARTITIONS
I0512 23:41:54.832790 140311617681408 gin_utils.py:85] moe_partitioning.MoePjitPartitioner.num_partitions = %NUM_MODEL_PARTITIONS
I0512 23:41:54.832822 140311617681408 gin_utils.py:85] 
I0512 23:41:54.832854 140311617681408 gin_utils.py:85] # Parameters for moe_trainer.MoeTrainer:
I0512 23:41:54.832887 140311617681408 gin_utils.py:85] # ==============================================================================
I0512 23:41:54.832920 140311617681408 gin_utils.py:85] moe_trainer.MoeTrainer.learning_rate_fn = @utils.create_learning_rate_scheduler()
I0512 23:41:54.832952 140311617681408 gin_utils.py:85] moe_trainer.MoeTrainer.num_expert_partitions = %NUM_EXPERT_PARTITIONS
I0512 23:41:54.832985 140311617681408 gin_utils.py:85] moe_trainer.MoeTrainer.num_microbatches = 8
I0512 23:41:54.833018 140311617681408 gin_utils.py:85] 
I0512 23:41:54.833050 140311617681408 gin_utils.py:85] # Parameters for dense_attention.MultiHeadDotProductAttention:
I0512 23:41:54.833083 140311617681408 gin_utils.py:85] # ==============================================================================
I0512 23:41:54.833116 140311617681408 gin_utils.py:85] dense_attention.MultiHeadDotProductAttention.bias_init = %BIAS_INIT
I0512 23:41:54.833148 140311617681408 gin_utils.py:85] dense_attention.MultiHeadDotProductAttention.broadcast_dropout = True
I0512 23:41:54.833181 140311617681408 gin_utils.py:85] dense_attention.MultiHeadDotProductAttention.dropout_rate = %DROPOUT_RATE
I0512 23:41:54.833213 140311617681408 gin_utils.py:85] dense_attention.MultiHeadDotProductAttention.dtype = %ACTIVATION_DTYPE
I0512 23:41:54.833245 140311617681408 gin_utils.py:85] dense_attention.MultiHeadDotProductAttention.head_dim = %HEAD_DIM
I0512 23:41:54.833278 140311617681408 gin_utils.py:85] dense_attention.MultiHeadDotProductAttention.kernel_init = \
I0512 23:41:54.833311 140311617681408 gin_utils.py:85]     @attention_kernel_init/linen.initializers.variance_scaling()
I0512 23:41:54.833343 140311617681408 gin_utils.py:85] dense_attention.MultiHeadDotProductAttention.num_heads = %NUM_HEADS
I0512 23:41:54.833375 140311617681408 gin_utils.py:85] dense_attention.MultiHeadDotProductAttention.use_bias = False
I0512 23:41:54.833408 140311617681408 gin_utils.py:85] dense_attention.MultiHeadDotProductAttention.use_rotary_embedding = True
I0512 23:41:54.833442 140311617681408 gin_utils.py:85] 
I0512 23:41:54.833475 140311617681408 gin_utils.py:85] # Parameters for bias_init/linen.initializers.normal:
I0512 23:41:54.833507 140311617681408 gin_utils.py:85] # ==============================================================================
I0512 23:41:54.833540 140311617681408 gin_utils.py:85] bias_init/linen.initializers.normal.stddev = 1e-06
I0512 23:41:54.833573 140311617681408 gin_utils.py:85] 
I0512 23:41:54.833606 140311617681408 gin_utils.py:85] # Parameters for router_init/linen.initializers.normal:
I0512 23:41:54.833639 140311617681408 gin_utils.py:85] # ==============================================================================
I0512 23:41:54.833681 140311617681408 gin_utils.py:85] router_init/linen.initializers.normal.stddev = 0.02
I0512 23:41:54.833719 140311617681408 gin_utils.py:85] 
I0512 23:41:54.833797 140311617681408 gin_utils.py:85] # Parameters for token_embedder_init/linen.initializers.normal:
I0512 23:41:54.833835 140311617681408 gin_utils.py:85] # ==============================================================================
I0512 23:41:54.833868 140311617681408 gin_utils.py:85] token_embedder_init/linen.initializers.normal.stddev = 1.0
I0512 23:41:54.833902 140311617681408 gin_utils.py:85] 
I0512 23:41:54.833935 140311617681408 gin_utils.py:85] # Parameters for partitioning.PjitPartitioner:
I0512 23:41:54.833968 140311617681408 gin_utils.py:85] # ==============================================================================
I0512 23:41:54.834000 140311617681408 gin_utils.py:85] partitioning.PjitPartitioner.logical_axis_rules = \
I0512 23:41:54.834033 140311617681408 gin_utils.py:85]     @partitioning.standard_logical_axis_rules()
I0512 23:41:54.834066 140311617681408 gin_utils.py:85] partitioning.PjitPartitioner.model_parallel_submesh = None
I0512 23:41:54.834099 140311617681408 gin_utils.py:85] partitioning.PjitPartitioner.num_partitions = 1
I0512 23:41:54.834132 140311617681408 gin_utils.py:85] 
I0512 23:41:54.834165 140311617681408 gin_utils.py:85] # Parameters for utils.RestoreCheckpointConfig:
I0512 23:41:54.834197 140311617681408 gin_utils.py:85] # ==============================================================================
I0512 23:41:54.834230 140311617681408 gin_utils.py:85] utils.RestoreCheckpointConfig.path = []
I0512 23:41:54.834263 140311617681408 gin_utils.py:85] 
I0512 23:41:54.834295 140311617681408 gin_utils.py:85] # Parameters for routing.RouterWeights:
I0512 23:41:54.834328 140311617681408 gin_utils.py:85] # ==============================================================================
I0512 23:41:54.834360 140311617681408 gin_utils.py:85] routing.RouterWeights.bias_init = %BIAS_INIT
I0512 23:41:54.834393 140311617681408 gin_utils.py:85] routing.RouterWeights.dtype = 'float32'
I0512 23:41:54.834429 140311617681408 gin_utils.py:85] routing.RouterWeights.kernel_init = @router_init/linen.initializers.normal()
I0512 23:41:54.834463 140311617681408 gin_utils.py:85] routing.RouterWeights.use_bias = False
I0512 23:41:54.834496 140311617681408 gin_utils.py:85] 
I0512 23:41:54.834528 140311617681408 gin_utils.py:85] # Parameters for utils.SaveCheckpointConfig:
I0512 23:41:54.834561 140311617681408 gin_utils.py:85] # ==============================================================================
I0512 23:41:54.834594 140311617681408 gin_utils.py:85] utils.SaveCheckpointConfig.dtype = 'float32'
I0512 23:41:54.834626 140311617681408 gin_utils.py:85] utils.SaveCheckpointConfig.keep = 20
I0512 23:41:54.834666 140311617681408 gin_utils.py:85] utils.SaveCheckpointConfig.period = 2500
I0512 23:41:54.834701 140311617681408 gin_utils.py:85] utils.SaveCheckpointConfig.save_dataset = False
I0512 23:41:54.834733 140311617681408 gin_utils.py:85] 
I0512 23:41:54.834766 140311617681408 gin_utils.py:85] # Parameters for seqio.SentencePieceVocabulary:
I0512 23:41:54.834800 140311617681408 gin_utils.py:85] # ==============================================================================
I0512 23:41:54.834832 140311617681408 gin_utils.py:85] seqio.SentencePieceVocabulary.extra_ids = 300
I0512 23:41:54.834865 140311617681408 gin_utils.py:85] seqio.SentencePieceVocabulary.sentencepiece_model_file = \
I0512 23:41:54.834897 140311617681408 gin_utils.py:85]     'gs://rosinality-tpu-bucket/sentencepiece.model'
I0512 23:41:54.834930 140311617681408 gin_utils.py:85] 
I0512 23:41:54.834963 140311617681408 gin_utils.py:85] # Parameters for moe_architecture.SparseDecoder:
I0512 23:41:54.834995 140311617681408 gin_utils.py:85] # ==============================================================================
I0512 23:41:54.835028 140311617681408 gin_utils.py:85] moe_architecture.SparseDecoder.dropout_factory = %DROPOUT_FACTORY
I0512 23:41:54.835061 140311617681408 gin_utils.py:85] moe_architecture.SparseDecoder.dtype = %ACTIVATION_DTYPE
I0512 23:41:54.835093 140311617681408 gin_utils.py:85] moe_architecture.SparseDecoder.layer_factory = @t5_architecture.DecoderLayer
I0512 23:41:54.835126 140311617681408 gin_utils.py:85] moe_architecture.SparseDecoder.layer_norm_factory = @layer_norm.T5LayerNorm
I0512 23:41:54.835159 140311617681408 gin_utils.py:85] moe_architecture.SparseDecoder.num_layers = %NUM_DECODER_LAYERS
I0512 23:41:54.835191 140311617681408 gin_utils.py:85] moe_architecture.SparseDecoder.num_sparse_layers = %NUM_DECODER_SPARSE_LAYERS
I0512 23:41:54.835224 140311617681408 gin_utils.py:85] moe_architecture.SparseDecoder.output_logits_factory = \
I0512 23:41:54.835257 140311617681408 gin_utils.py:85]     @output_logits/dense.DenseGeneral
I0512 23:41:54.835289 140311617681408 gin_utils.py:85] moe_architecture.SparseDecoder.sparse_layer_factory = \
I0512 23:41:54.835322 140311617681408 gin_utils.py:85]     @moe_architecture.SparseDecoderLayer
I0512 23:41:54.835355 140311617681408 gin_utils.py:85] moe_architecture.SparseDecoder.sparse_layout = %DECODER_SPARSE_LAYOUT
I0512 23:41:54.835388 140311617681408 gin_utils.py:85] 
I0512 23:41:54.835422 140311617681408 gin_utils.py:85] # Parameters for moe_architecture.SparseDecoderLayer:
I0512 23:41:54.862112 140595318249472 gin_utils.py:85] train_script.train.eval_steps = 20
I0512 23:41:54.862143 140595318249472 gin_utils.py:85] train_script.train.infer_eval_dataset_cfg = None
I0512 23:41:54.862181 140595318249472 gin_utils.py:85] train_script.train.model = %MODEL
I0512 23:41:54.862214 140595318249472 gin_utils.py:85] train_script.train.model_dir = %MODEL_DIR
I0512 23:41:54.862245 140595318249472 gin_utils.py:85] train_script.train.partitioner = @moe_partitioning.MoePjitPartitioner()
I0512 23:41:54.862276 140595318249472 gin_utils.py:85] train_script.train.random_seed = %RANDOM_SEED
I0512 23:41:54.862307 140595318249472 gin_utils.py:85] train_script.train.stats_period = 10
I0512 23:41:54.862338 140595318249472 gin_utils.py:85] train_script.train.summarize_config_fn = @gin_utils.summarize_gin_config
I0512 23:41:54.862369 140595318249472 gin_utils.py:85] train_script.train.total_steps = %TRAIN_STEPS
I0512 23:41:54.862400 140595318249472 gin_utils.py:85] train_script.train.train_dataset_cfg = @train/utils.DatasetConfig()
I0512 23:41:54.862431 140595318249472 gin_utils.py:85] train_script.train.train_eval_dataset_cfg = None
I0512 23:41:54.862463 140595318249472 gin_utils.py:85] train_script.train.trainer_cls = @moe_trainer.MoeTrainer
I0512 23:41:54.862494 140595318249472 gin_utils.py:85] train_script.train.use_hardware_rng = %USE_HARDWARE_RNG
I0512 23:41:54.862525 140595318249472 gin_utils.py:85] 
I0512 23:41:54.862556 140595318249472 gin_utils.py:85] # Parameters for trainer.Trainer:
I0512 23:41:54.862587 140595318249472 gin_utils.py:85] # ==============================================================================
I0512 23:41:54.862618 140595318249472 gin_utils.py:85] trainer.Trainer.learning_rate_fn = @utils.create_learning_rate_scheduler()
I0512 23:41:54.862649 140595318249472 gin_utils.py:85] trainer.Trainer.num_microbatches = None
I0512 23:41:54.862680 140595318249472 gin_utils.py:85] 
I0512 23:41:54.862710 140595318249472 gin_utils.py:85] # Parameters for attention_kernel_init/linen.initializers.variance_scaling:
I0512 23:41:54.862741 140595318249472 gin_utils.py:85] # ==============================================================================
I0512 23:41:54.862772 140595318249472 gin_utils.py:85] attention_kernel_init/linen.initializers.variance_scaling.distribution = 'normal'
I0512 23:41:54.862803 140595318249472 gin_utils.py:85] attention_kernel_init/linen.initializers.variance_scaling.mode = 'fan_in'
I0512 23:41:54.862834 140595318249472 gin_utils.py:85] attention_kernel_init/linen.initializers.variance_scaling.scale = %SCALE
I0512 23:41:54.862864 140595318249472 gin_utils.py:85] 
I0512 23:41:54.862895 140595318249472 gin_utils.py:85] # Parameters for expert_kernel_init/linen.initializers.variance_scaling:
I0512 23:41:54.862926 140595318249472 gin_utils.py:85] # ==============================================================================
I0512 23:41:54.862957 140595318249472 gin_utils.py:85] expert_kernel_init/linen.initializers.variance_scaling.distribution = 'normal'
I0512 23:41:54.862987 140595318249472 gin_utils.py:85] expert_kernel_init/linen.initializers.variance_scaling.mode = 'fan_in'
I0512 23:41:54.863018 140595318249472 gin_utils.py:85] expert_kernel_init/linen.initializers.variance_scaling.scale = %SCALE
I0512 23:41:54.863049 140595318249472 gin_utils.py:85] 
I0512 23:41:54.863080 140595318249472 gin_utils.py:85] # Parameters for mlp_kernel_init/linen.initializers.variance_scaling:
I0512 23:41:54.863111 140595318249472 gin_utils.py:85] # ==============================================================================
I0512 23:41:54.863142 140595318249472 gin_utils.py:85] mlp_kernel_init/linen.initializers.variance_scaling.distribution = \
I0512 23:41:54.863178 140595318249472 gin_utils.py:85]     'truncated_normal'
I0512 23:41:54.863210 140595318249472 gin_utils.py:85] mlp_kernel_init/linen.initializers.variance_scaling.mode = 'fan_in'
I0512 23:41:54.863242 140595318249472 gin_utils.py:85] mlp_kernel_init/linen.initializers.variance_scaling.scale = %SCALE
I0512 23:41:54.863272 140595318249472 gin_utils.py:85] 
I0512 23:41:54.863303 140595318249472 gin_utils.py:85] # Parameters for output_logits_kernel_init/linen.initializers.variance_scaling:
I0512 23:41:54.863334 140595318249472 gin_utils.py:85] # ==============================================================================
I0512 23:41:54.863365 140595318249472 gin_utils.py:85] output_logits_kernel_init/linen.initializers.variance_scaling.distribution = \
I0512 23:41:54.863396 140595318249472 gin_utils.py:85]     'truncated_normal'
I0512 23:41:54.863428 140595318249472 gin_utils.py:85] output_logits_kernel_init/linen.initializers.variance_scaling.mode = 'fan_in'
I0512 23:41:54.863460 140595318249472 gin_utils.py:85] output_logits_kernel_init/linen.initializers.variance_scaling.scale = %SCALE
I0512 23:41:54.864405 140595318249472 partitioning.py:559] `activation_partitioning_dims` = 2, `parameter_partitioning_dims` = 1
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
I0000 00:00:1715557314.903462  450135 pjrt_api.cc:98] GetPjrtApi was found for tpu at /home/rosinality/openmoe_venv/lib/python3.10/site-packages/libtpu/libtpu.so
I0000 00:00:1715557314.903523  450135 pjrt_api.cc:67] PJRT_Api is set for device type tpu
I0000 00:00:1715557314.903525  450135 pjrt_api.cc:72] PJRT plugin for tpu has PJRT API version 0.30. The framework PJRT API version is 0.30.
I0512 23:41:54.860586 140529434355712 gin_utils.py:85] # Parameters for models.MoeDecoderOnlyModel:
I0512 23:41:54.860622 140529434355712 gin_utils.py:85] # ==============================================================================
I0512 23:41:54.860656 140529434355712 gin_utils.py:85] models.MoeDecoderOnlyModel.aux_loss_factor = %AUX_LOSS_FACTOR
I0512 23:41:54.860692 140529434355712 gin_utils.py:85] models.MoeDecoderOnlyModel.label_smoothing = %LABEL_SMOOTHING
I0512 23:41:54.860728 140529434355712 gin_utils.py:85] models.MoeDecoderOnlyModel.loss_normalizing_factor = %LOSS_NORMALIZING_FACTOR
I0512 23:41:54.860763 140529434355712 gin_utils.py:85] models.MoeDecoderOnlyModel.module = %ARCHITECTURE
I0512 23:41:54.860797 140529434355712 gin_utils.py:85] models.MoeDecoderOnlyModel.optimizer_def = %OPTIMIZER
I0512 23:41:54.860833 140529434355712 gin_utils.py:85] models.MoeDecoderOnlyModel.router_z_loss_factor = %ROUTER_Z_LOSS_FACTOR
I0512 23:41:54.860869 140529434355712 gin_utils.py:85] models.MoeDecoderOnlyModel.vocabulary = %VOCABULARY
I0512 23:41:54.860904 140529434355712 gin_utils.py:85] models.MoeDecoderOnlyModel.z_loss = %Z_LOSS
I0512 23:41:54.860941 140529434355712 gin_utils.py:85] 
I0512 23:41:54.860977 140529434355712 gin_utils.py:85] # Parameters for moe_layers.MoeLayer:
I0512 23:41:54.861010 140529434355712 gin_utils.py:85] # ==============================================================================
I0512 23:41:54.861046 140529434355712 gin_utils.py:85] moe_layers.MoeLayer.dtype = %MOE_TRUNCATED_DTYPE
I0512 23:41:54.861082 140529434355712 gin_utils.py:85] moe_layers.MoeLayer.eval_capacity_factor = %EVAL_EXPERT_CAPACITY_FACTOR
I0512 23:41:54.861115 140529434355712 gin_utils.py:85] moe_layers.MoeLayer.expert = @expert/dense.MlpBlock()
I0512 23:41:54.861157 140529434355712 gin_utils.py:85] moe_layers.MoeLayer.max_group_size = %GROUP_SIZE
I0512 23:41:54.861193 140529434355712 gin_utils.py:85] moe_layers.MoeLayer.num_expert_partitions = %NUM_EXPERT_PARTITIONS
I0512 23:41:54.861229 140529434355712 gin_utils.py:85] moe_layers.MoeLayer.num_experts = %NUM_EXPERTS
I0512 23:41:54.861265 140529434355712 gin_utils.py:85] moe_layers.MoeLayer.num_model_partitions = \
I0512 23:41:54.861299 140529434355712 gin_utils.py:85]     @partitioning.compute_num_model_partitions()
I0512 23:41:54.861335 140529434355712 gin_utils.py:85] moe_layers.MoeLayer.train_capacity_factor = %TRAIN_EXPERT_CAPACITY_FACTOR
I0512 23:41:54.861371 140529434355712 gin_utils.py:85] 
I0512 23:41:54.861404 140529434355712 gin_utils.py:85] # Parameters for sparse_decoder/moe_layers.MoeLayer:
I0512 23:41:54.861440 140529434355712 gin_utils.py:85] # ==============================================================================
I0512 23:41:54.861476 140529434355712 gin_utils.py:85] sparse_decoder/moe_layers.MoeLayer.router = \
I0512 23:41:54.861509 140529434355712 gin_utils.py:85]     @sparse_decoder/routing.TokensChooseMaskedRouter()
I0512 23:41:54.861567 140529434355712 gin_utils.py:85] 
I0512 23:41:54.861607 140529434355712 gin_utils.py:85] # Parameters for partitioning.MoePjitPartitioner:
I0512 23:41:54.861644 140529434355712 gin_utils.py:85] # ==============================================================================
I0512 23:41:54.861680 140529434355712 gin_utils.py:85] partitioning.MoePjitPartitioner.model_parallel_submesh = %MODEL_PARALLEL_SUBMESH
I0512 23:41:54.861716 140529434355712 gin_utils.py:85] partitioning.MoePjitPartitioner.num_expert_partitions = %NUM_EXPERT_PARTITIONS
I0512 23:41:54.861750 140529434355712 gin_utils.py:85] partitioning.MoePjitPartitioner.num_partitions = %NUM_MODEL_PARTITIONS
I0512 23:41:54.861786 140529434355712 gin_utils.py:85] 
I0512 23:41:54.861822 140529434355712 gin_utils.py:85] # Parameters for moe_trainer.MoeTrainer:
I0512 23:41:54.861855 140529434355712 gin_utils.py:85] # ==============================================================================
I0512 23:41:54.861892 140529434355712 gin_utils.py:85] moe_trainer.MoeTrainer.learning_rate_fn = @utils.create_learning_rate_scheduler()
I0512 23:41:54.861929 140529434355712 gin_utils.py:85] moe_trainer.MoeTrainer.num_expert_partitions = %NUM_EXPERT_PARTITIONS
I0512 23:41:54.861966 140529434355712 gin_utils.py:85] moe_trainer.MoeTrainer.num_microbatches = 8
I0512 23:41:54.861999 140529434355712 gin_utils.py:85] 
I0512 23:41:54.862035 140529434355712 gin_utils.py:85] # Parameters for dense_attention.MultiHeadDotProductAttention:
I0512 23:41:54.862071 140529434355712 gin_utils.py:85] # ==============================================================================
I0512 23:41:54.862107 140529434355712 gin_utils.py:85] dense_attention.MultiHeadDotProductAttention.bias_init = %BIAS_INIT
I0512 23:41:54.862147 140529434355712 gin_utils.py:85] dense_attention.MultiHeadDotProductAttention.broadcast_dropout = True
I0512 23:41:54.862183 140529434355712 gin_utils.py:85] dense_attention.MultiHeadDotProductAttention.dropout_rate = %DROPOUT_RATE
I0512 23:41:54.862219 140529434355712 gin_utils.py:85] dense_attention.MultiHeadDotProductAttention.dtype = %ACTIVATION_DTYPE
I0512 23:41:54.862255 140529434355712 gin_utils.py:85] dense_attention.MultiHeadDotProductAttention.head_dim = %HEAD_DIM
I0512 23:41:54.862289 140529434355712 gin_utils.py:85] dense_attention.MultiHeadDotProductAttention.kernel_init = \
I0512 23:41:54.862325 140529434355712 gin_utils.py:85]     @attention_kernel_init/linen.initializers.variance_scaling()
I0512 23:41:54.862361 140529434355712 gin_utils.py:85] dense_attention.MultiHeadDotProductAttention.num_heads = %NUM_HEADS
I0512 23:41:54.862394 140529434355712 gin_utils.py:85] dense_attention.MultiHeadDotProductAttention.use_bias = False
I0512 23:41:54.862431 140529434355712 gin_utils.py:85] dense_attention.MultiHeadDotProductAttention.use_rotary_embedding = True
I0512 23:41:54.862466 140529434355712 gin_utils.py:85] 
I0512 23:41:54.862500 140529434355712 gin_utils.py:85] # Parameters for bias_init/linen.initializers.normal:
I0512 23:41:54.862535 140529434355712 gin_utils.py:85] # ==============================================================================
I0512 23:41:54.862571 140529434355712 gin_utils.py:85] bias_init/linen.initializers.normal.stddev = 1e-06
I0512 23:41:54.862605 140529434355712 gin_utils.py:85] 
I0512 23:41:54.862641 140529434355712 gin_utils.py:85] # Parameters for router_init/linen.initializers.normal:
I0512 23:41:54.862678 140529434355712 gin_utils.py:85] # ==============================================================================
I0512 23:41:54.862712 140529434355712 gin_utils.py:85] router_init/linen.initializers.normal.stddev = 0.02
I0512 23:41:54.862749 140529434355712 gin_utils.py:85] 
I0512 23:41:54.862785 140529434355712 gin_utils.py:85] # Parameters for token_embedder_init/linen.initializers.normal:
I0512 23:41:54.862818 140529434355712 gin_utils.py:85] # ==============================================================================
I0512 23:41:54.862855 140529434355712 gin_utils.py:85] token_embedder_init/linen.initializers.normal.stddev = 1.0
I0512 23:41:54.862892 140529434355712 gin_utils.py:85] 
I0512 23:41:54.862929 140529434355712 gin_utils.py:85] # Parameters for partitioning2.PjitPartitioner:
I0512 23:41:54.862962 140529434355712 gin_utils.py:85] # ==============================================================================
I0512 23:41:54.862998 140529434355712 gin_utils.py:85] partitioning2.PjitPartitioner.logical_axis_rules = \
I0512 23:41:54.863034 140529434355712 gin_utils.py:85]     @partitioning2.standard_logical_axis_rules()
I0512 23:41:54.863067 140529434355712 gin_utils.py:85] partitioning2.PjitPartitioner.model_parallel_submesh = None
I0512 23:41:54.863104 140529434355712 gin_utils.py:85] partitioning2.PjitPartitioner.num_partitions = 1
I0512 23:41:54.863144 140529434355712 gin_utils.py:85] 
I0512 23:41:54.863181 140529434355712 gin_utils.py:85] # Parameters for utils.RestoreCheckpointConfig:
I0512 23:41:54.863217 140529434355712 gin_utils.py:85] # ==============================================================================
I0512 23:41:54.863250 140529434355712 gin_utils.py:85] utils.RestoreCheckpointConfig.path = []
I0512 23:41:54.863286 140529434355712 gin_utils.py:85] 
I0512 23:41:54.863319 140529434355712 gin_utils.py:85] # Parameters for routing.RouterWeights:
I0512 23:41:54.863354 140529434355712 gin_utils.py:85] # ==============================================================================
I0512 23:41:54.863390 140529434355712 gin_utils.py:85] routing.RouterWeights.bias_init = %BIAS_INIT
I0512 23:41:54.863424 140529434355712 gin_utils.py:85] routing.RouterWeights.dtype = 'float32'
I0512 23:41:54.863460 140529434355712 gin_utils.py:85] routing.RouterWeights.kernel_init = @router_init/linen.initializers.normal()
I0512 23:41:54.863497 140529434355712 gin_utils.py:85] routing.RouterWeights.use_bias = False
I0512 23:41:54.863532 140529434355712 gin_utils.py:85] 
I0512 23:41:54.863566 140529434355712 gin_utils.py:85] # Parameters for utils.SaveCheckpointConfig:
I0512 23:41:54.863602 140529434355712 gin_utils.py:85] # ==============================================================================
I0512 23:41:54.863639 140529434355712 gin_utils.py:85] utils.SaveCheckpointConfig.dtype = 'float32'
I0512 23:41:54.863672 140529434355712 gin_utils.py:85] utils.SaveCheckpointConfig.keep = 20
I0512 23:41:54.863708 140529434355712 gin_utils.py:85] utils.SaveCheckpointConfig.period = 2500
I0512 23:41:54.863743 140529434355712 gin_utils.py:85] utils.SaveCheckpointConfig.save_dataset = False
I0512 23:41:54.863776 140529434355712 gin_utils.py:85] 
I0512 23:41:54.863813 140529434355712 gin_utils.py:85] # Parameters for seqio.SentencePieceVocabulary:
I0512 23:41:54.863848 140529434355712 gin_utils.py:85] # ==============================================================================
I0512 23:41:54.863883 140529434355712 gin_utils.py:85] seqio.SentencePieceVocabulary.extra_ids = 300
I0512 23:41:54.863921 140529434355712 gin_utils.py:85] seqio.SentencePieceVocabulary.sentencepiece_model_file = \
I0512 23:41:54.863957 140529434355712 gin_utils.py:85]     'gs://rosinality-tpu-bucket/sentencepiece.model'
I0512 23:41:54.863990 140529434355712 gin_utils.py:85] 
I0512 23:41:54.864026 140529434355712 gin_utils.py:85] # Parameters for moe_architecture.SparseDecoder:
I0512 23:41:54.864063 140529434355712 gin_utils.py:85] # ==============================================================================
I0512 23:41:54.864098 140529434355712 gin_utils.py:85] moe_architecture.SparseDecoder.dropout_factory = %DROPOUT_FACTORY
I0512 23:41:54.864137 140529434355712 gin_utils.py:85] moe_architecture.SparseDecoder.dtype = %ACTIVATION_DTYPE
I0512 23:41:54.864171 140529434355712 gin_utils.py:85] moe_architecture.SparseDecoder.layer_factory = @t5_architecture.DecoderLayer
I0512 23:41:54.864204 140529434355712 gin_utils.py:85] moe_architecture.SparseDecoder.layer_norm_factory = @layer_norm.T5LayerNorm
I0512 23:41:54.864237 140529434355712 gin_utils.py:85] moe_architecture.SparseDecoder.num_layers = %NUM_DECODER_LAYERS
I0512 23:41:54.864269 140529434355712 gin_utils.py:85] moe_architecture.SparseDecoder.num_sparse_layers = %NUM_DECODER_SPARSE_LAYERS
I0512 23:41:54.864302 140529434355712 gin_utils.py:85] moe_architecture.SparseDecoder.output_logits_factory = \
I0512 23:41:54.864340 140529434355712 gin_utils.py:85]     @output_logits/dense.DenseGeneral
I0512 23:41:54.864374 140529434355712 gin_utils.py:85] moe_architecture.SparseDecoder.sparse_layer_factory = \
I0512 23:41:54.864411 140529434355712 gin_utils.py:85]     @moe_architecture.SparseDecoderLayer
I0512 23:41:54.864448 140529434355712 gin_utils.py:85] moe_architecture.SparseDecoder.sparse_layout = %DECODER_SPARSE_LAYOUT
I0512 23:41:54.864482 140529434355712 gin_utils.py:85] 
I0512 23:41:54.864519 140529434355712 gin_utils.py:85] # Parameters for moe_architecture.SparseDecoderLayer:
I0512 23:41:54.864556 140529434355712 gin_utils.py:85] # ==============================================================================
I0512 23:41:54.864590 140529434355712 gin_utils.py:85] moe_architecture.SparseDecoderLayer.activation_partitioning_dims = \
I0512 23:41:54.864628 140529434355712 gin_utils.py:85]     %ACTIVATION_PARTITIONING_DIMS
I0512 23:41:54.864664 140529434355712 gin_utils.py:85] moe_architecture.SparseDecoderLayer.dropout_factory = %DROPOUT_FACTORY
I0512 23:41:54.864698 140529434355712 gin_utils.py:85] moe_architecture.SparseDecoderLayer.encoder_decoder_attention = None
I0512 23:41:54.864733 140529434355712 gin_utils.py:85] moe_architecture.SparseDecoderLayer.extra_mlp = @dense.MlpBlock()
I0512 23:41:54.864768 140529434355712 gin_utils.py:85] moe_architecture.SparseDecoderLayer.layer_norm_factory = @layer_norm.T5LayerNorm
I0512 23:41:54.864804 140529434355712 gin_utils.py:85] moe_architecture.SparseDecoderLayer.mlp = @sparse_decoder/moe_layers.MoeLayer()
I0512 23:41:54.864837 140529434355712 gin_utils.py:85] moe_architecture.SparseDecoderLayer.scanned = False
I0512 23:41:54.864874 140529434355712 gin_utils.py:85] moe_architecture.SparseDecoderLayer.self_attention = \
I0512 23:41:54.864912 140529434355712 gin_utils.py:85]     @dense_attention.MultiHeadDotProductAttention()
I0512 23:41:54.864946 140529434355712 gin_utils.py:85] 
I0512 23:41:54.864983 140529434355712 gin_utils.py:85] # Parameters for layer_norm.T5LayerNorm:
I0512 23:41:54.865020 140529434355712 gin_utils.py:85] # ==============================================================================
I0512 23:41:54.865057 140529434355712 gin_utils.py:85] layer_norm.T5LayerNorm.dtype = %ACTIVATION_DTYPE
I0512 23:41:54.865091 140529434355712 gin_utils.py:85] 
I0512 23:41:54.865132 140529434355712 gin_utils.py:85] # Parameters for routing.TokensChooseMaskedRouter:
I0512 23:41:54.865170 140529434355712 gin_utils.py:85] # ==============================================================================
I0512 23:41:54.865203 140529434355712 gin_utils.py:85] routing.TokensChooseMaskedRouter.dtype = 'float32'
I0512 23:41:54.865240 140529434355712 gin_utils.py:85] routing.TokensChooseMaskedRouter.ignore_padding_tokens = False
I0512 23:41:54.865273 140529434355712 gin_utils.py:85] routing.TokensChooseMaskedRouter.jitter_noise = %JITTER_NOISE
I0512 23:41:54.865309 140529434355712 gin_utils.py:85] routing.TokensChooseMaskedRouter.num_selected_experts = %NUM_SELECTED_EXPERTS
I0512 23:41:54.865346 140529434355712 gin_utils.py:85] routing.TokensChooseMaskedRouter.router_weights = @routing.RouterWeights()
I0512 23:41:54.865380 140529434355712 gin_utils.py:85] 
I0512 23:41:54.865416 140529434355712 gin_utils.py:85] # Parameters for sparse_decoder/routing.TokensChooseMaskedRouter:
I0512 23:41:54.865453 140529434355712 gin_utils.py:85] # ==============================================================================
I0512 23:41:54.865489 140529434355712 gin_utils.py:85] sparse_decoder/routing.TokensChooseMaskedRouter.batch_prioritized_routing = False
I0512 23:41:54.865523 140529434355712 gin_utils.py:85] 
I0512 23:41:54.865587 140529434355712 gin_utils.py:85] # Parameters for sparse_encoder/routing.TokensChooseMaskedRouter:
I0512 23:41:54.865626 140529434355712 gin_utils.py:85] # ==============================================================================
I0512 23:41:54.865660 140529434355712 gin_utils.py:85] sparse_encoder/routing.TokensChooseMaskedRouter.batch_prioritized_routing = False
I0512 23:41:54.865697 140529434355712 gin_utils.py:85] 
I0512 23:41:54.865733 140529434355712 gin_utils.py:85] # Parameters for train_script.train:
I0512 23:41:54.865770 140529434355712 gin_utils.py:85] # ==============================================================================
I0512 23:41:54.865807 140529434355712 gin_utils.py:85] train_script.train.checkpoint_cfg = @utils.CheckpointConfig()
I0512 23:41:54.865841 140529434355712 gin_utils.py:85] train_script.train.eval_period = 2500
I0512 23:41:54.865878 140529434355712 gin_utils.py:85] train_script.train.eval_steps = 20
I0512 23:41:54.865912 140529434355712 gin_utils.py:85] train_script.train.infer_eval_dataset_cfg = None
I0512 23:41:54.835457 140311617681408 gin_utils.py:85] # ==============================================================================
I0512 23:41:54.835489 140311617681408 gin_utils.py:85] moe_architecture.SparseDecoderLayer.activation_partitioning_dims = \
I0512 23:41:54.835523 140311617681408 gin_utils.py:85]     %ACTIVATION_PARTITIONING_DIMS
I0512 23:41:54.835555 140311617681408 gin_utils.py:85] moe_architecture.SparseDecoderLayer.dropout_factory = %DROPOUT_FACTORY
I0512 23:41:54.835588 140311617681408 gin_utils.py:85] moe_architecture.SparseDecoderLayer.encoder_decoder_attention = None
I0512 23:41:54.835621 140311617681408 gin_utils.py:85] moe_architecture.SparseDecoderLayer.extra_mlp = @dense.MlpBlock()
I0512 23:41:54.835660 140311617681408 gin_utils.py:85] moe_architecture.SparseDecoderLayer.layer_norm_factory = @layer_norm.T5LayerNorm
I0512 23:41:54.835695 140311617681408 gin_utils.py:85] moe_architecture.SparseDecoderLayer.mlp = @sparse_decoder/moe_layers.MoeLayer()
I0512 23:41:54.835728 140311617681408 gin_utils.py:85] moe_architecture.SparseDecoderLayer.scanned = False
I0512 23:41:54.835761 140311617681408 gin_utils.py:85] moe_architecture.SparseDecoderLayer.self_attention = \
I0512 23:41:54.835793 140311617681408 gin_utils.py:85]     @dense_attention.MultiHeadDotProductAttention()
I0512 23:41:54.835826 140311617681408 gin_utils.py:85] 
I0512 23:41:54.835859 140311617681408 gin_utils.py:85] # Parameters for layer_norm.T5LayerNorm:
I0512 23:41:54.835891 140311617681408 gin_utils.py:85] # ==============================================================================
I0512 23:41:54.835923 140311617681408 gin_utils.py:85] layer_norm.T5LayerNorm.dtype = %ACTIVATION_DTYPE
I0512 23:41:54.835956 140311617681408 gin_utils.py:85] 
I0512 23:41:54.835988 140311617681408 gin_utils.py:85] # Parameters for routing.TokensChooseMaskedRouter:
I0512 23:41:54.836020 140311617681408 gin_utils.py:85] # ==============================================================================
I0512 23:41:54.836053 140311617681408 gin_utils.py:85] routing.TokensChooseMaskedRouter.dtype = 'float32'
I0512 23:41:54.836085 140311617681408 gin_utils.py:85] routing.TokensChooseMaskedRouter.ignore_padding_tokens = False
I0512 23:41:54.836117 140311617681408 gin_utils.py:85] routing.TokensChooseMaskedRouter.jitter_noise = %JITTER_NOISE
I0512 23:41:54.836149 140311617681408 gin_utils.py:85] routing.TokensChooseMaskedRouter.num_selected_experts = %NUM_SELECTED_EXPERTS
I0512 23:41:54.836182 140311617681408 gin_utils.py:85] routing.TokensChooseMaskedRouter.router_weights = @routing.RouterWeights()
I0512 23:41:54.836214 140311617681408 gin_utils.py:85] 
I0512 23:41:54.836246 140311617681408 gin_utils.py:85] # Parameters for sparse_decoder/routing.TokensChooseMaskedRouter:
I0512 23:41:54.836278 140311617681408 gin_utils.py:85] # ==============================================================================
I0512 23:41:54.836310 140311617681408 gin_utils.py:85] sparse_decoder/routing.TokensChooseMaskedRouter.batch_prioritized_routing = False
I0512 23:41:54.836343 140311617681408 gin_utils.py:85] 
I0512 23:41:54.836374 140311617681408 gin_utils.py:85] # Parameters for sparse_encoder/routing.TokensChooseMaskedRouter:
I0512 23:41:54.836408 140311617681408 gin_utils.py:85] # ==============================================================================
I0512 23:41:54.836442 140311617681408 gin_utils.py:85] sparse_encoder/routing.TokensChooseMaskedRouter.batch_prioritized_routing = False
I0512 23:41:54.836475 140311617681408 gin_utils.py:85] 
I0512 23:41:54.836507 140311617681408 gin_utils.py:85] # Parameters for train_script.train:
I0512 23:41:54.836540 140311617681408 gin_utils.py:85] # ==============================================================================
I0512 23:41:54.836573 140311617681408 gin_utils.py:85] train_script.train.checkpoint_cfg = @utils.CheckpointConfig()
I0512 23:41:54.836606 140311617681408 gin_utils.py:85] train_script.train.eval_period = 2500
I0512 23:41:54.836638 140311617681408 gin_utils.py:85] train_script.train.eval_steps = 20
I0512 23:41:54.554917 139674595104768 gin_utils.py:85] train_script.train.train_eval_dataset_cfg = None
I0512 23:41:54.554951 139674595104768 gin_utils.py:85] train_script.train.trainer_cls = @moe_trainer.MoeTrainer
I0512 23:41:54.554983 139674595104768 gin_utils.py:85] train_script.train.use_hardware_rng = %USE_HARDWARE_RNG
I0512 23:41:54.555014 139674595104768 gin_utils.py:85] 
I0512 23:41:54.555046 139674595104768 gin_utils.py:85] # Parameters for trainer.Trainer:
I0512 23:41:54.555078 139674595104768 gin_utils.py:85] # ==============================================================================
I0512 23:41:54.555112 139674595104768 gin_utils.py:85] trainer.Trainer.learning_rate_fn = @utils.create_learning_rate_scheduler()
I0512 23:41:54.555145 139674595104768 gin_utils.py:85] trainer.Trainer.num_microbatches = None
I0512 23:41:54.555178 139674595104768 gin_utils.py:85] 
I0512 23:41:54.555212 139674595104768 gin_utils.py:85] # Parameters for attention_kernel_init/linen.initializers.variance_scaling:
I0512 23:41:54.555247 139674595104768 gin_utils.py:85] # ==============================================================================
I0512 23:41:54.555279 139674595104768 gin_utils.py:85] attention_kernel_init/linen.initializers.variance_scaling.distribution = 'normal'
I0512 23:41:54.555336 139674595104768 gin_utils.py:85] attention_kernel_init/linen.initializers.variance_scaling.mode = 'fan_in'
I0512 23:41:54.555371 139674595104768 gin_utils.py:85] attention_kernel_init/linen.initializers.variance_scaling.scale = %SCALE
I0512 23:41:54.555403 139674595104768 gin_utils.py:85] 
I0512 23:41:54.555436 139674595104768 gin_utils.py:85] # Parameters for expert_kernel_init/linen.initializers.variance_scaling:
I0512 23:41:54.555468 139674595104768 gin_utils.py:85] # ==============================================================================
I0512 23:41:54.555500 139674595104768 gin_utils.py:85] expert_kernel_init/linen.initializers.variance_scaling.distribution = 'normal'
I0512 23:41:54.555532 139674595104768 gin_utils.py:85] expert_kernel_init/linen.initializers.variance_scaling.mode = 'fan_in'
I0512 23:41:54.555564 139674595104768 gin_utils.py:85] expert_kernel_init/linen.initializers.variance_scaling.scale = %SCALE
I0512 23:41:54.555596 139674595104768 gin_utils.py:85] 
I0512 23:41:54.555628 139674595104768 gin_utils.py:85] # Parameters for mlp_kernel_init/linen.initializers.variance_scaling:
I0512 23:41:54.555665 139674595104768 gin_utils.py:85] # ==============================================================================
I0512 23:41:54.555698 139674595104768 gin_utils.py:85] mlp_kernel_init/linen.initializers.variance_scaling.distribution = \
I0512 23:41:54.555731 139674595104768 gin_utils.py:85]     'truncated_normal'
I0512 23:41:54.555763 139674595104768 gin_utils.py:85] mlp_kernel_init/linen.initializers.variance_scaling.mode = 'fan_in'
I0512 23:41:54.555795 139674595104768 gin_utils.py:85] mlp_kernel_init/linen.initializers.variance_scaling.scale = %SCALE
I0512 23:41:54.555827 139674595104768 gin_utils.py:85] 
I0512 23:41:54.555859 139674595104768 gin_utils.py:85] # Parameters for output_logits_kernel_init/linen.initializers.variance_scaling:
I0512 23:41:54.555891 139674595104768 gin_utils.py:85] # ==============================================================================
I0512 23:41:54.555925 139674595104768 gin_utils.py:85] output_logits_kernel_init/linen.initializers.variance_scaling.distribution = \
I0512 23:41:54.555958 139674595104768 gin_utils.py:85]     'truncated_normal'
I0512 23:41:54.555990 139674595104768 gin_utils.py:85] output_logits_kernel_init/linen.initializers.variance_scaling.mode = 'fan_in'
I0512 23:41:54.556022 139674595104768 gin_utils.py:85] output_logits_kernel_init/linen.initializers.variance_scaling.scale = %SCALE
I0512 23:41:54.556928 139674595104768 partitioning.py:559] `activation_partitioning_dims` = 2, `parameter_partitioning_dims` = 1
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
I0000 00:00:1715557314.601564  455454 pjrt_api.cc:98] GetPjrtApi was found for tpu at /home/rosinality/openmoe_venv/lib/python3.10/site-packages/libtpu/libtpu.so
I0000 00:00:1715557314.601616  455454 pjrt_api.cc:67] PJRT_Api is set for device type tpu
I0000 00:00:1715557314.601619  455454 pjrt_api.cc:72] PJRT plugin for tpu has PJRT API version 0.30. The framework PJRT API version is 0.30.
I0512 23:41:54.865945 140529434355712 gin_utils.py:85] train_script.train.model = %MODEL
I0512 23:41:54.865977 140529434355712 gin_utils.py:85] train_script.train.model_dir = %MODEL_DIR
I0512 23:41:54.866010 140529434355712 gin_utils.py:85] train_script.train.partitioner = @partitioning.MoePjitPartitioner()
I0512 23:41:54.866042 140529434355712 gin_utils.py:85] train_script.train.random_seed = %RANDOM_SEED
I0512 23:41:54.866108 140529434355712 gin_utils.py:85] train_script.train.stats_period = 10
I0512 23:41:54.866147 140529434355712 gin_utils.py:85] train_script.train.summarize_config_fn = @gin_utils.summarize_gin_config
I0512 23:41:54.866181 140529434355712 gin_utils.py:85] train_script.train.total_steps = %TRAIN_STEPS
I0512 23:41:54.866214 140529434355712 gin_utils.py:85] train_script.train.train_dataset_cfg = @train/utils.DatasetConfig()
I0512 23:41:54.866247 140529434355712 gin_utils.py:85] train_script.train.train_eval_dataset_cfg = None
I0512 23:41:54.866280 140529434355712 gin_utils.py:85] train_script.train.trainer_cls = @moe_trainer.MoeTrainer
I0512 23:41:54.866312 140529434355712 gin_utils.py:85] train_script.train.use_hardware_rng = %USE_HARDWARE_RNG
I0512 23:41:54.866345 140529434355712 gin_utils.py:85] 
I0512 23:41:54.866377 140529434355712 gin_utils.py:85] # Parameters for trainer.Trainer:
I0512 23:41:54.866409 140529434355712 gin_utils.py:85] # ==============================================================================
I0512 23:41:54.866441 140529434355712 gin_utils.py:85] trainer.Trainer.learning_rate_fn = @utils.create_learning_rate_scheduler()
I0512 23:41:54.866474 140529434355712 gin_utils.py:85] trainer.Trainer.num_microbatches = None
I0512 23:41:54.866507 140529434355712 gin_utils.py:85] 
I0512 23:41:54.866539 140529434355712 gin_utils.py:85] # Parameters for attention_kernel_init/linen.initializers.variance_scaling:
I0512 23:41:54.866571 140529434355712 gin_utils.py:85] # ==============================================================================
I0512 23:41:54.866603 140529434355712 gin_utils.py:85] attention_kernel_init/linen.initializers.variance_scaling.distribution = 'normal'
I0512 23:41:54.866635 140529434355712 gin_utils.py:85] attention_kernel_init/linen.initializers.variance_scaling.mode = 'fan_in'
I0512 23:41:54.866668 140529434355712 gin_utils.py:85] attention_kernel_init/linen.initializers.variance_scaling.scale = %SCALE
I0512 23:41:54.866700 140529434355712 gin_utils.py:85] 
I0512 23:41:54.866733 140529434355712 gin_utils.py:85] # Parameters for expert_kernel_init/linen.initializers.variance_scaling:
I0512 23:41:54.866765 140529434355712 gin_utils.py:85] # ==============================================================================
I0512 23:41:54.866798 140529434355712 gin_utils.py:85] expert_kernel_init/linen.initializers.variance_scaling.distribution = 'normal'
I0512 23:41:54.866830 140529434355712 gin_utils.py:85] expert_kernel_init/linen.initializers.variance_scaling.mode = 'fan_in'
I0512 23:41:54.866862 140529434355712 gin_utils.py:85] expert_kernel_init/linen.initializers.variance_scaling.scale = %SCALE
I0512 23:41:54.866897 140529434355712 gin_utils.py:85] 
I0512 23:41:54.866930 140529434355712 gin_utils.py:85] # Parameters for mlp_kernel_init/linen.initializers.variance_scaling:
I0512 23:41:54.866963 140529434355712 gin_utils.py:85] # ==============================================================================
I0512 23:41:54.866996 140529434355712 gin_utils.py:85] mlp_kernel_init/linen.initializers.variance_scaling.distribution = \
I0512 23:41:54.867028 140529434355712 gin_utils.py:85]     'truncated_normal'
I0512 23:41:54.867060 140529434355712 gin_utils.py:85] mlp_kernel_init/linen.initializers.variance_scaling.mode = 'fan_in'
I0512 23:41:54.867093 140529434355712 gin_utils.py:85] mlp_kernel_init/linen.initializers.variance_scaling.scale = %SCALE
I0512 23:41:54.867130 140529434355712 gin_utils.py:85] 
I0512 23:41:54.867164 140529434355712 gin_utils.py:85] # Parameters for output_logits_kernel_init/linen.initializers.variance_scaling:
I0512 23:41:54.867197 140529434355712 gin_utils.py:85] # ==============================================================================
I0512 23:41:54.867229 140529434355712 gin_utils.py:85] output_logits_kernel_init/linen.initializers.variance_scaling.distribution = \
I0512 23:41:54.867262 140529434355712 gin_utils.py:85]     'truncated_normal'
I0512 23:41:54.867294 140529434355712 gin_utils.py:85] output_logits_kernel_init/linen.initializers.variance_scaling.mode = 'fan_in'
I0512 23:41:54.867326 140529434355712 gin_utils.py:85] output_logits_kernel_init/linen.initializers.variance_scaling.scale = %SCALE
I0512 23:41:54.868240 140529434355712 partitioning.py:559] `activation_partitioning_dims` = 2, `parameter_partitioning_dims` = 1
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
I0000 00:00:1715557314.924727  489306 pjrt_api.cc:98] GetPjrtApi was found for tpu at /home/rosinality/openmoe_venv/lib/python3.10/site-packages/libtpu/libtpu.so
I0000 00:00:1715557314.924786  489306 pjrt_api.cc:67] PJRT_Api is set for device type tpu
I0000 00:00:1715557314.924788  489306 pjrt_api.cc:72] PJRT plugin for tpu has PJRT API version 0.30. The framework PJRT API version is 0.30.
I0512 23:41:54.836677 140311617681408 gin_utils.py:85] train_script.train.infer_eval_dataset_cfg = None
I0512 23:41:54.836715 140311617681408 gin_utils.py:85] train_script.train.model = %MODEL
I0512 23:41:54.836748 140311617681408 gin_utils.py:85] train_script.train.model_dir = %MODEL_DIR
I0512 23:41:54.836780 140311617681408 gin_utils.py:85] train_script.train.partitioner = @moe_partitioning.MoePjitPartitioner()
I0512 23:41:54.836813 140311617681408 gin_utils.py:85] train_script.train.random_seed = %RANDOM_SEED
I0512 23:41:54.836846 140311617681408 gin_utils.py:85] train_script.train.stats_period = 10
I0512 23:41:54.836878 140311617681408 gin_utils.py:85] train_script.train.summarize_config_fn = @gin_utils.summarize_gin_config
I0512 23:41:54.836911 140311617681408 gin_utils.py:85] train_script.train.total_steps = %TRAIN_STEPS
I0512 23:41:54.836943 140311617681408 gin_utils.py:85] train_script.train.train_dataset_cfg = @train/utils.DatasetConfig()
I0512 23:41:54.836976 140311617681408 gin_utils.py:85] train_script.train.train_eval_dataset_cfg = None
I0512 23:41:54.837008 140311617681408 gin_utils.py:85] train_script.train.trainer_cls = @moe_trainer.MoeTrainer
I0512 23:41:54.837041 140311617681408 gin_utils.py:85] train_script.train.use_hardware_rng = %USE_HARDWARE_RNG
I0512 23:41:54.837073 140311617681408 gin_utils.py:85] 
I0512 23:41:54.837105 140311617681408 gin_utils.py:85] # Parameters for trainer.Trainer:
I0512 23:41:54.837137 140311617681408 gin_utils.py:85] # ==============================================================================
I0512 23:41:54.837170 140311617681408 gin_utils.py:85] trainer.Trainer.learning_rate_fn = @utils.create_learning_rate_scheduler()
I0512 23:41:54.837202 140311617681408 gin_utils.py:85] trainer.Trainer.num_microbatches = None
I0512 23:41:54.837234 140311617681408 gin_utils.py:85] 
I0512 23:41:54.837266 140311617681408 gin_utils.py:85] # Parameters for attention_kernel_init/linen.initializers.variance_scaling:
I0512 23:41:54.837298 140311617681408 gin_utils.py:85] # ==============================================================================
I0512 23:41:54.837331 140311617681408 gin_utils.py:85] attention_kernel_init/linen.initializers.variance_scaling.distribution = 'normal'
I0512 23:41:54.837363 140311617681408 gin_utils.py:85] attention_kernel_init/linen.initializers.variance_scaling.mode = 'fan_in'
I0512 23:41:54.837395 140311617681408 gin_utils.py:85] attention_kernel_init/linen.initializers.variance_scaling.scale = %SCALE
I0512 23:41:54.837429 140311617681408 gin_utils.py:85] 
I0512 23:41:54.837462 140311617681408 gin_utils.py:85] # Parameters for expert_kernel_init/linen.initializers.variance_scaling:
I0512 23:41:54.837495 140311617681408 gin_utils.py:85] # ==============================================================================
I0512 23:41:54.837527 140311617681408 gin_utils.py:85] expert_kernel_init/linen.initializers.variance_scaling.distribution = 'normal'
I0512 23:41:54.837560 140311617681408 gin_utils.py:85] expert_kernel_init/linen.initializers.variance_scaling.mode = 'fan_in'
I0512 23:41:54.837592 140311617681408 gin_utils.py:85] expert_kernel_init/linen.initializers.variance_scaling.scale = %SCALE
I0512 23:41:54.837624 140311617681408 gin_utils.py:85] 
I0512 23:41:54.837662 140311617681408 gin_utils.py:85] # Parameters for mlp_kernel_init/linen.initializers.variance_scaling:
I0512 23:41:54.837696 140311617681408 gin_utils.py:85] # ==============================================================================
I0512 23:41:54.837728 140311617681408 gin_utils.py:85] mlp_kernel_init/linen.initializers.variance_scaling.distribution = \
I0512 23:41:54.837761 140311617681408 gin_utils.py:85]     'truncated_normal'
I0512 23:41:54.837817 140311617681408 gin_utils.py:85] mlp_kernel_init/linen.initializers.variance_scaling.mode = 'fan_in'
I0512 23:41:54.837851 140311617681408 gin_utils.py:85] mlp_kernel_init/linen.initializers.variance_scaling.scale = %SCALE
I0512 23:41:54.837884 140311617681408 gin_utils.py:85] 
I0512 23:41:54.837916 140311617681408 gin_utils.py:85] # Parameters for output_logits_kernel_init/linen.initializers.variance_scaling:
I0512 23:41:54.837949 140311617681408 gin_utils.py:85] # ==============================================================================
I0512 23:41:54.837981 140311617681408 gin_utils.py:85] output_logits_kernel_init/linen.initializers.variance_scaling.distribution = \
I0512 23:41:54.838014 140311617681408 gin_utils.py:85]     'truncated_normal'
I0512 23:41:54.838047 140311617681408 gin_utils.py:85] output_logits_kernel_init/linen.initializers.variance_scaling.mode = 'fan_in'
I0512 23:41:54.838079 140311617681408 gin_utils.py:85] output_logits_kernel_init/linen.initializers.variance_scaling.scale = %SCALE
I0512 23:41:54.838984 140311617681408 partitioning.py:559] `activation_partitioning_dims` = 2, `parameter_partitioning_dims` = 1
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
I0000 00:00:1715557314.895997  487766 pjrt_api.cc:98] GetPjrtApi was found for tpu at /home/rosinality/openmoe_venv/lib/python3.10/site-packages/libtpu/libtpu.so
I0000 00:00:1715557314.896042  487766 pjrt_api.cc:67] PJRT_Api is set for device type tpu
I0000 00:00:1715557314.896044  487766 pjrt_api.cc:72] PJRT plugin for tpu has PJRT API version 0.30. The framework PJRT API version is 0.30.
I0000 00:00:1715557317.783447  453630 tfrt_cpu_pjrt_client.cc:349] TfrtCpuClient created.
I0512 23:41:57.783655 140031916288000 train.py:196] Process ID: 6
I0000 00:00:1715557317.795996  450615 tfrt_cpu_pjrt_client.cc:349] TfrtCpuClient created.
I0512 23:41:57.796272 140095893518336 train.py:196] Process ID: 3
I0000 00:00:1715557317.801279  455454 tfrt_cpu_pjrt_client.cc:349] TfrtCpuClient created.
I0512 23:41:57.801494 139674595104768 train.py:196] Process ID: 2
I0000 00:00:1715557317.807521  449796 tfrt_cpu_pjrt_client.cc:349] TfrtCpuClient created.
I0512 23:41:57.807761 140339465779200 train.py:196] Process ID: 5
I0000 00:00:1715557317.814435  475641 tfrt_cpu_pjrt_client.cc:349] TfrtCpuClient created.
I0512 23:41:57.814629 140440593786880 train.py:196] Process ID: 7
I0000 00:00:1715557317.816437  489306 tfrt_cpu_pjrt_client.cc:349] TfrtCpuClient created.
I0512 23:41:57.816663 140529434355712 train.py:196] Process ID: 0
I0000 00:00:1715557317.834863  450135 tfrt_cpu_pjrt_client.cc:349] TfrtCpuClient created.
I0512 23:41:57.835103 140595318249472 train.py:196] Process ID: 1
I0512 23:41:57.849027 140031916288000 train.py:250] Using fast RngBitGenerator PRNG for initialization and dropout.
I0512 23:41:57.858748 140095893518336 train.py:250] Using fast RngBitGenerator PRNG for initialization and dropout.
I0512 23:41:57.859668 140339465779200 train.py:250] Using fast RngBitGenerator PRNG for initialization and dropout.
I0512 23:41:57.873686 139674595104768 train.py:250] Using fast RngBitGenerator PRNG for initialization and dropout.
I0512 23:41:57.873242 140440593786880 train.py:250] Using fast RngBitGenerator PRNG for initialization and dropout.
I0000 00:00:1715557317.868979  487766 tfrt_cpu_pjrt_client.cc:349] TfrtCpuClient created.
I0512 23:41:57.869178 140311617681408 train.py:196] Process ID: 4
I0512 23:41:57.878135 140529434355712 train.py:250] Using fast RngBitGenerator PRNG for initialization and dropout.
I0512 23:41:57.900266 140595318249472 train.py:250] Using fast RngBitGenerator PRNG for initialization and dropout.
I0512 23:41:57.939344 140311617681408 train.py:250] Using fast RngBitGenerator PRNG for initialization and dropout.
I0512 23:41:58.018483 140095893518336 train.py:256] Random seed not provided, using RNG seed 1715557317
I0512 23:41:58.018527 139674595104768 train.py:256] Random seed not provided, using RNG seed 1715557317
I0512 23:41:58.018454 140440593786880 train.py:256] Random seed not provided, using RNG seed 1715557317
I0512 23:41:58.018421 140595318249472 train.py:256] Random seed not provided, using RNG seed 1715557317
I0512 23:41:58.018386 140031916288000 train.py:256] Random seed not provided, using RNG seed 1715557317
I0512 23:41:58.018472 140339465779200 train.py:256] Random seed not provided, using RNG seed 1715557317
I0512 23:41:58.018531 140529434355712 train.py:256] Random seed not provided, using RNG seed 1715557317
I0512 23:41:58.018428 140311617681408 train.py:256] Random seed not provided, using RNG seed 1715557317
I0512 23:41:58.210504 140440593786880 partitioning.py:94] For MoE, first construct vanilla T5X (data, model) mesh.
I0512 23:41:58.210857 140440593786880 partitioning.py:151] last device coords : (1, 3, 3, 0)
last local device coords: (1, 1, 0, 0)
W0512 23:41:58.211058 140440593786880 partitioning.py:197] Tiling device assignment mesh by hosts, which may lead to reduced XLA collective performance. To avoid this, modify the model parallel submesh or run with more tasks per host.
I0512 23:41:58.211215 140440593786880 partitioning.py:255] global_mesh axis_names: ('data', 'model')
I0512 23:41:58.211270 140440593786880 partitioning.py:256] global_mesh devices: [[TpuDevice(id=0, process_index=0, coords=(0,0,0), core_on_chip=0)
  TpuDevice(id=2, process_index=0, coords=(0,1,0), core_on_chip=0)
  TpuDevice(id=1, process_index=0, coords=(1,0,0), core_on_chip=0)
  TpuDevice(id=3, process_index=0, coords=(1,1,0), core_on_chip=0)]
 [TpuDevice(id=8, process_index=2, coords=(0,0,1), core_on_chip=0)
  TpuDevice(id=10, process_index=2, coords=(0,1,1), core_on_chip=0)
  TpuDevice(id=9, process_index=2, coords=(1,0,1), core_on_chip=0)
  TpuDevice(id=11, process_index=2, coords=(1,1,1), core_on_chip=0)]
 [TpuDevice(id=16, process_index=4, coords=(0,0,2), core_on_chip=0)
  TpuDevice(id=18, process_index=4, coords=(0,1,2), core_on_chip=0)
  TpuDevice(id=17, process_index=4, coords=(1,0,2), core_on_chip=0)
  TpuDevice(id=19, process_index=4, coords=(1,1,2), core_on_chip=0)]
 [TpuDevice(id=24, process_index=6, coords=(0,0,3), core_on_chip=0)
  TpuDevice(id=26, process_index=6, coords=(0,1,3), core_on_chip=0)
  TpuDevice(id=25, process_index=6, coords=(1,0,3), core_on_chip=0)
  TpuDevice(id=27, process_index=6, coords=(1,1,3), core_on_chip=0)]
 [TpuDevice(id=4, process_index=1, coords=(0,2,0), core_on_chip=0)
  TpuDevice(id=6, process_index=1, coords=(0,3,0), core_on_chip=0)
  TpuDevice(id=5, process_index=1, coords=(1,2,0), core_on_chip=0)
  TpuDevice(id=7, process_index=1, coords=(1,3,0), core_on_chip=0)]
 [TpuDevice(id=12, process_index=3, coords=(0,2,1), core_on_chip=0)
  TpuDevice(id=14, process_index=3, coords=(0,3,1), core_on_chip=0)
  TpuDevice(id=13, process_index=3, coords=(1,2,1), core_on_chip=0)
  TpuDevice(id=15, process_index=3, coords=(1,3,1), core_on_chip=0)]
 [TpuDevice(id=20, process_index=5, coords=(0,2,2), core_on_chip=0)
  TpuDevice(id=22, process_index=5, coords=(0,3,2), core_on_chip=0)
  TpuDevice(id=21, process_index=5, coords=(1,2,2), core_on_chip=0)
  TpuDevice(id=23, process_index=5, coords=(1,3,2), core_on_chip=0)]
 [TpuDevice(id=28, process_index=7, coords=(0,2,3), core_on_chip=0)
  TpuDevice(id=30, process_index=7, coords=(0,3,3), core_on_chip=0)
  TpuDevice(id=29, process_index=7, coords=(1,2,3), core_on_chip=0)
  TpuDevice(id=31, process_index=7, coords=(1,3,3), core_on_chip=0)]]
I0512 23:41:58.211506 140440593786880 partitioning.py:257] global_mesh devices shape: (8, 4)
I0512 23:41:58.211571 140440593786880 partitioning.py:111] Overridden MoE global_mesh axes_names: ('data', 'expert', 'model')
I0512 23:41:58.211615 140440593786880 partitioning.py:114] Overridden MoE global_mesh devices: [[[TpuDevice(id=0, process_index=0, coords=(0,0,0), core_on_chip=0)
   TpuDevice(id=2, process_index=0, coords=(0,1,0), core_on_chip=0)
   TpuDevice(id=1, process_index=0, coords=(1,0,0), core_on_chip=0)
   TpuDevice(id=3, process_index=0, coords=(1,1,0), core_on_chip=0)]
  [TpuDevice(id=8, process_index=2, coords=(0,0,1), core_on_chip=0)
   TpuDevice(id=10, process_index=2, coords=(0,1,1), core_on_chip=0)
   TpuDevice(id=9, process_index=2, coords=(1,0,1), core_on_chip=0)
   TpuDevice(id=11, process_index=2, coords=(1,1,1), core_on_chip=0)]
  [TpuDevice(id=16, process_index=4, coords=(0,0,2), core_on_chip=0)
   TpuDevice(id=18, process_index=4, coords=(0,1,2), core_on_chip=0)
   TpuDevice(id=17, process_index=4, coords=(1,0,2), core_on_chip=0)
   TpuDevice(id=19, process_index=4, coords=(1,1,2), core_on_chip=0)]
  [TpuDevice(id=24, process_index=6, coords=(0,0,3), core_on_chip=0)
   TpuDevice(id=26, process_index=6, coords=(0,1,3), core_on_chip=0)
   TpuDevice(id=25, process_index=6, coords=(1,0,3), core_on_chip=0)
   TpuDevice(id=27, process_index=6, coords=(1,1,3), core_on_chip=0)]
  [TpuDevice(id=4, process_index=1, coords=(0,2,0), core_on_chip=0)
   TpuDevice(id=6, process_index=1, coords=(0,3,0), core_on_chip=0)
   TpuDevice(id=5, process_index=1, coords=(1,2,0), core_on_chip=0)
   TpuDevice(id=7, process_index=1, coords=(1,3,0), core_on_chip=0)]
  [TpuDevice(id=12, process_index=3, coords=(0,2,1), core_on_chip=0)
   TpuDevice(id=14, process_index=3, coords=(0,3,1), core_on_chip=0)
   TpuDevice(id=13, process_index=3, coords=(1,2,1), core_on_chip=0)
   TpuDevice(id=15, process_index=3, coords=(1,3,1), core_on_chip=0)]
  [TpuDevice(id=20, process_index=5, coords=(0,2,2), core_on_chip=0)
   TpuDevice(id=22, process_index=5, coords=(0,3,2), core_on_chip=0)
   TpuDevice(id=21, process_index=5, coords=(1,2,2), core_on_chip=0)
   TpuDevice(id=23, process_index=5, coords=(1,3,2), core_on_chip=0)]
  [TpuDevice(id=28, process_index=7, coords=(0,2,3), core_on_chip=0)
   TpuDevice(id=30, process_index=7, coords=(0,3,3), core_on_chip=0)
   TpuDevice(id=29, process_index=7, coords=(1,2,3), core_on_chip=0)
   TpuDevice(id=31, process_index=7, coords=(1,3,3), core_on_chip=0)]]]
I0512 23:41:58.211800 140440593786880 partitioning.py:115] Overridden MoE global_mesh shape: (1, 8, 4)
I0512 23:41:58.216810 139674595104768 partitioning.py:94] For MoE, first construct vanilla T5X (data, model) mesh.
I0512 23:41:58.215041 140031916288000 partitioning.py:94] For MoE, first construct vanilla T5X (data, model) mesh.
I0512 23:41:58.217202 139674595104768 partitioning.py:151] last device coords : (1, 3, 3, 0)
last local device coords: (1, 1, 0, 0)
I0512 23:41:58.215408 140031916288000 partitioning.py:151] last device coords : (1, 3, 3, 0)
last local device coords: (1, 1, 0, 0)
W0512 23:41:58.217412 139674595104768 partitioning.py:197] Tiling device assignment mesh by hosts, which may lead to reduced XLA collective performance. To avoid this, modify the model parallel submesh or run with more tasks per host.
W0512 23:41:58.215610 140031916288000 partitioning.py:197] Tiling device assignment mesh by hosts, which may lead to reduced XLA collective performance. To avoid this, modify the model parallel submesh or run with more tasks per host.
I0512 23:41:58.217578 139674595104768 partitioning.py:255] global_mesh axis_names: ('data', 'model')
I0512 23:41:58.217635 139674595104768 partitioning.py:256] global_mesh devices: [[TpuDevice(id=0, process_index=0, coords=(0,0,0), core_on_chip=0)
  TpuDevice(id=2, process_index=0, coords=(0,1,0), core_on_chip=0)
  TpuDevice(id=1, process_index=0, coords=(1,0,0), core_on_chip=0)
  TpuDevice(id=3, process_index=0, coords=(1,1,0), core_on_chip=0)]
 [TpuDevice(id=8, process_index=2, coords=(0,0,1), core_on_chip=0)
  TpuDevice(id=10, process_index=2, coords=(0,1,1), core_on_chip=0)
  TpuDevice(id=9, process_index=2, coords=(1,0,1), core_on_chip=0)
  TpuDevice(id=11, process_index=2, coords=(1,1,1), core_on_chip=0)]
 [TpuDevice(id=16, process_index=4, coords=(0,0,2), core_on_chip=0)
  TpuDevice(id=18, process_index=4, coords=(0,1,2), core_on_chip=0)
  TpuDevice(id=17, process_index=4, coords=(1,0,2), core_on_chip=0)
  TpuDevice(id=19, process_index=4, coords=(1,1,2), core_on_chip=0)]
 [TpuDevice(id=24, process_index=6, coords=(0,0,3), core_on_chip=0)
  TpuDevice(id=26, process_index=6, coords=(0,1,3), core_on_chip=0)
  TpuDevice(id=25, process_index=6, coords=(1,0,3), core_on_chip=0)
  TpuDevice(id=27, process_index=6, coords=(1,1,3), core_on_chip=0)]
 [TpuDevice(id=4, process_index=1, coords=(0,2,0), core_on_chip=0)
  TpuDevice(id=6, process_index=1, coords=(0,3,0), core_on_chip=0)
  TpuDevice(id=5, process_index=1, coords=(1,2,0), core_on_chip=0)
  TpuDevice(id=7, process_index=1, coords=(1,3,0), core_on_chip=0)]
 [TpuDevice(id=12, process_index=3, coords=(0,2,1), core_on_chip=0)
  TpuDevice(id=14, process_index=3, coords=(0,3,1), core_on_chip=0)
  TpuDevice(id=13, process_index=3, coords=(1,2,1), core_on_chip=0)
  TpuDevice(id=15, process_index=3, coords=(1,3,1), core_on_chip=0)]
 [TpuDevice(id=20, process_index=5, coords=(0,2,2), core_on_chip=0)
  TpuDevice(id=22, process_index=5, coords=(0,3,2), core_on_chip=0)
  TpuDevice(id=21, process_index=5, coords=(1,2,2), core_on_chip=0)
  TpuDevice(id=23, process_index=5, coords=(1,3,2), core_on_chip=0)]
 [TpuDevice(id=28, process_index=7, coords=(0,2,3), core_on_chip=0)
  TpuDevice(id=30, process_index=7, coords=(0,3,3), core_on_chip=0)
  TpuDevice(id=29, process_index=7, coords=(1,2,3), core_on_chip=0)
  TpuDevice(id=31, process_index=7, coords=(1,3,3), core_on_chip=0)]]
I0512 23:41:58.215772 140031916288000 partitioning.py:255] global_mesh axis_names: ('data', 'model')
I0512 23:41:58.215828 140031916288000 partitioning.py:256] global_mesh devices: [[TpuDevice(id=0, process_index=0, coords=(0,0,0), core_on_chip=0)
  TpuDevice(id=2, process_index=0, coords=(0,1,0), core_on_chip=0)
  TpuDevice(id=1, process_index=0, coords=(1,0,0), core_on_chip=0)
  TpuDevice(id=3, process_index=0, coords=(1,1,0), core_on_chip=0)]
 [TpuDevice(id=8, process_index=2, coords=(0,0,1), core_on_chip=0)
  TpuDevice(id=10, process_index=2, coords=(0,1,1), core_on_chip=0)
  TpuDevice(id=9, process_index=2, coords=(1,0,1), core_on_chip=0)
  TpuDevice(id=11, process_index=2, coords=(1,1,1), core_on_chip=0)]
 [TpuDevice(id=16, process_index=4, coords=(0,0,2), core_on_chip=0)
  TpuDevice(id=18, process_index=4, coords=(0,1,2), core_on_chip=0)
  TpuDevice(id=17, process_index=4, coords=(1,0,2), core_on_chip=0)
  TpuDevice(id=19, process_index=4, coords=(1,1,2), core_on_chip=0)]
 [TpuDevice(id=24, process_index=6, coords=(0,0,3), core_on_chip=0)
  TpuDevice(id=26, process_index=6, coords=(0,1,3), core_on_chip=0)
  TpuDevice(id=25, process_index=6, coords=(1,0,3), core_on_chip=0)
  TpuDevice(id=27, process_index=6, coords=(1,1,3), core_on_chip=0)]
 [TpuDevice(id=4, process_index=1, coords=(0,2,0), core_on_chip=0)
  TpuDevice(id=6, process_index=1, coords=(0,3,0), core_on_chip=0)
  TpuDevice(id=5, process_index=1, coords=(1,2,0), core_on_chip=0)
  TpuDevice(id=7, process_index=1, coords=(1,3,0), core_on_chip=0)]
 [TpuDevice(id=12, process_index=3, coords=(0,2,1), core_on_chip=0)
  TpuDevice(id=14, process_index=3, coords=(0,3,1), core_on_chip=0)
  TpuDevice(id=13, process_index=3, coords=(1,2,1), core_on_chip=0)
  TpuDevice(id=15, process_index=3, coords=(1,3,1), core_on_chip=0)]
 [TpuDevice(id=20, process_index=5, coords=(0,2,2), core_on_chip=0)
  TpuDevice(id=22, process_index=5, coords=(0,3,2), core_on_chip=0)
  TpuDevice(id=21, process_index=5, coords=(1,2,2), core_on_chip=0)
  TpuDevice(id=23, process_index=5, coords=(1,3,2), core_on_chip=0)]
 [TpuDevice(id=28, process_index=7, coords=(0,2,3), core_on_chip=0)
  TpuDevice(id=30, process_index=7, coords=(0,3,3), core_on_chip=0)
  TpuDevice(id=29, process_index=7, coords=(1,2,3), core_on_chip=0)
  TpuDevice(id=31, process_index=7, coords=(1,3,3), core_on_chip=0)]]
I0512 23:41:58.217879 139674595104768 partitioning.py:257] global_mesh devices shape: (8, 4)
I0512 23:41:58.217943 139674595104768 partitioning.py:111] Overridden MoE global_mesh axes_names: ('data', 'expert', 'model')
I0512 23:41:58.217987 139674595104768 partitioning.py:114] Overridden MoE global_mesh devices: [[[TpuDevice(id=0, process_index=0, coords=(0,0,0), core_on_chip=0)
   TpuDevice(id=2, process_index=0, coords=(0,1,0), core_on_chip=0)
   TpuDevice(id=1, process_index=0, coords=(1,0,0), core_on_chip=0)
   TpuDevice(id=3, process_index=0, coords=(1,1,0), core_on_chip=0)]
  [TpuDevice(id=8, process_index=2, coords=(0,0,1), core_on_chip=0)
   TpuDevice(id=10, process_index=2, coords=(0,1,1), core_on_chip=0)
   TpuDevice(id=9, process_index=2, coords=(1,0,1), core_on_chip=0)
   TpuDevice(id=11, process_index=2, coords=(1,1,1), core_on_chip=0)]
  [TpuDevice(id=16, process_index=4, coords=(0,0,2), core_on_chip=0)
   TpuDevice(id=18, process_index=4, coords=(0,1,2), core_on_chip=0)
   TpuDevice(id=17, process_index=4, coords=(1,0,2), core_on_chip=0)
   TpuDevice(id=19, process_index=4, coords=(1,1,2), core_on_chip=0)]
  [TpuDevice(id=24, process_index=6, coords=(0,0,3), core_on_chip=0)
   TpuDevice(id=26, process_index=6, coords=(0,1,3), core_on_chip=0)
   TpuDevice(id=25, process_index=6, coords=(1,0,3), core_on_chip=0)
   TpuDevice(id=27, process_index=6, coords=(1,1,3), core_on_chip=0)]
  [TpuDevice(id=4, process_index=1, coords=(0,2,0), core_on_chip=0)
   TpuDevice(id=6, process_index=1, coords=(0,3,0), core_on_chip=0)
   TpuDevice(id=5, process_index=1, coords=(1,2,0), core_on_chip=0)
   TpuDevice(id=7, process_index=1, coords=(1,3,0), core_on_chip=0)]
  [TpuDevice(id=12, process_index=3, coords=(0,2,1), core_on_chip=0)
   TpuDevice(id=14, process_index=3, coords=(0,3,1), core_on_chip=0)
   TpuDevice(id=13, process_index=3, coords=(1,2,1), core_on_chip=0)
   TpuDevice(id=15, process_index=3, coords=(1,3,1), core_on_chip=0)]
  [TpuDevice(id=20, process_index=5, coords=(0,2,2), core_on_chip=0)
   TpuDevice(id=22, process_index=5, coords=(0,3,2), core_on_chip=0)
   TpuDevice(id=21, process_index=5, coords=(1,2,2), core_on_chip=0)
   TpuDevice(id=23, process_index=5, coords=(1,3,2), core_on_chip=0)]
  [TpuDevice(id=28, process_index=7, coords=(0,2,3), core_on_chip=0)
   TpuDevice(id=30, process_index=7, coords=(0,3,3), core_on_chip=0)
   TpuDevice(id=29, process_index=7, coords=(1,2,3), core_on_chip=0)
   TpuDevice(id=31, process_index=7, coords=(1,3,3), core_on_chip=0)]]]
I0512 23:41:58.216071 140031916288000 partitioning.py:257] global_mesh devices shape: (8, 4)
I0512 23:41:58.216135 140031916288000 partitioning.py:111] Overridden MoE global_mesh axes_names: ('data', 'expert', 'model')
I0512 23:41:58.216180 140031916288000 partitioning.py:114] Overridden MoE global_mesh devices: [[[TpuDevice(id=0, process_index=0, coords=(0,0,0), core_on_chip=0)
   TpuDevice(id=2, process_index=0, coords=(0,1,0), core_on_chip=0)
   TpuDevice(id=1, process_index=0, coords=(1,0,0), core_on_chip=0)
   TpuDevice(id=3, process_index=0, coords=(1,1,0), core_on_chip=0)]
  [TpuDevice(id=8, process_index=2, coords=(0,0,1), core_on_chip=0)
   TpuDevice(id=10, process_index=2, coords=(0,1,1), core_on_chip=0)
   TpuDevice(id=9, process_index=2, coords=(1,0,1), core_on_chip=0)
   TpuDevice(id=11, process_index=2, coords=(1,1,1), core_on_chip=0)]
  [TpuDevice(id=16, process_index=4, coords=(0,0,2), core_on_chip=0)
   TpuDevice(id=18, process_index=4, coords=(0,1,2), core_on_chip=0)
   TpuDevice(id=17, process_index=4, coords=(1,0,2), core_on_chip=0)
   TpuDevice(id=19, process_index=4, coords=(1,1,2), core_on_chip=0)]
  [TpuDevice(id=24, process_index=6, coords=(0,0,3), core_on_chip=0)
   TpuDevice(id=26, process_index=6, coords=(0,1,3), core_on_chip=0)
   TpuDevice(id=25, process_index=6, coords=(1,0,3), core_on_chip=0)
   TpuDevice(id=27, process_index=6, coords=(1,1,3), core_on_chip=0)]
  [TpuDevice(id=4, process_index=1, coords=(0,2,0), core_on_chip=0)
   TpuDevice(id=6, process_index=1, coords=(0,3,0), core_on_chip=0)
   TpuDevice(id=5, process_index=1, coords=(1,2,0), core_on_chip=0)
   TpuDevice(id=7, process_index=1, coords=(1,3,0), core_on_chip=0)]
  [TpuDevice(id=12, process_index=3, coords=(0,2,1), core_on_chip=0)
   TpuDevice(id=14, process_index=3, coords=(0,3,1), core_on_chip=0)
   TpuDevice(id=13, process_index=3, coords=(1,2,1), core_on_chip=0)
   TpuDevice(id=15, process_index=3, coords=(1,3,1), core_on_chip=0)]
  [TpuDevice(id=20, process_index=5, coords=(0,2,2), core_on_chip=0)
   TpuDevice(id=22, process_index=5, coords=(0,3,2), core_on_chip=0)
   TpuDevice(id=21, process_index=5, coords=(1,2,2), core_on_chip=0)
   TpuDevice(id=23, process_index=5, coords=(1,3,2), core_on_chip=0)]
  [TpuDevice(id=28, process_index=7, coords=(0,2,3), core_on_chip=0)
   TpuDevice(id=30, process_index=7, coords=(0,3,3), core_on_chip=0)
   TpuDevice(id=29, process_index=7, coords=(1,2,3), core_on_chip=0)
   TpuDevice(id=31, process_index=7, coords=(1,3,3), core_on_chip=0)]]]
I0512 23:41:58.218171 139674595104768 partitioning.py:115] Overridden MoE global_mesh shape: (1, 8, 4)
I0512 23:41:58.216367 140031916288000 partitioning.py:115] Overridden MoE global_mesh shape: (1, 8, 4)
I0512 23:41:58.217940 140339465779200 partitioning.py:94] For MoE, first construct vanilla T5X (data, model) mesh.
I0512 23:41:58.218312 140339465779200 partitioning.py:151] last device coords : (1, 3, 3, 0)
last local device coords: (1, 1, 0, 0)
W0512 23:41:58.218510 140339465779200 partitioning.py:197] Tiling device assignment mesh by hosts, which may lead to reduced XLA collective performance. To avoid this, modify the model parallel submesh or run with more tasks per host.
I0512 23:41:58.218694 140339465779200 partitioning.py:255] global_mesh axis_names: ('data', 'model')
I0512 23:41:58.218750 140339465779200 partitioning.py:256] global_mesh devices: [[TpuDevice(id=0, process_index=0, coords=(0,0,0), core_on_chip=0)
  TpuDevice(id=2, process_index=0, coords=(0,1,0), core_on_chip=0)
  TpuDevice(id=1, process_index=0, coords=(1,0,0), core_on_chip=0)
  TpuDevice(id=3, process_index=0, coords=(1,1,0), core_on_chip=0)]
 [TpuDevice(id=8, process_index=2, coords=(0,0,1), core_on_chip=0)
  TpuDevice(id=10, process_index=2, coords=(0,1,1), core_on_chip=0)
  TpuDevice(id=9, process_index=2, coords=(1,0,1), core_on_chip=0)
  TpuDevice(id=11, process_index=2, coords=(1,1,1), core_on_chip=0)]
 [TpuDevice(id=16, process_index=4, coords=(0,0,2), core_on_chip=0)
  TpuDevice(id=18, process_index=4, coords=(0,1,2), core_on_chip=0)
  TpuDevice(id=17, process_index=4, coords=(1,0,2), core_on_chip=0)
  TpuDevice(id=19, process_index=4, coords=(1,1,2), core_on_chip=0)]
 [TpuDevice(id=24, process_index=6, coords=(0,0,3), core_on_chip=0)
  TpuDevice(id=26, process_index=6, coords=(0,1,3), core_on_chip=0)
  TpuDevice(id=25, process_index=6, coords=(1,0,3), core_on_chip=0)
  TpuDevice(id=27, process_index=6, coords=(1,1,3), core_on_chip=0)]
 [TpuDevice(id=4, process_index=1, coords=(0,2,0), core_on_chip=0)
  TpuDevice(id=6, process_index=1, coords=(0,3,0), core_on_chip=0)
  TpuDevice(id=5, process_index=1, coords=(1,2,0), core_on_chip=0)
  TpuDevice(id=7, process_index=1, coords=(1,3,0), core_on_chip=0)]
 [TpuDevice(id=12, process_index=3, coords=(0,2,1), core_on_chip=0)
  TpuDevice(id=14, process_index=3, coords=(0,3,1), core_on_chip=0)
  TpuDevice(id=13, process_index=3, coords=(1,2,1), core_on_chip=0)
  TpuDevice(id=15, process_index=3, coords=(1,3,1), core_on_chip=0)]
 [TpuDevice(id=20, process_index=5, coords=(0,2,2), core_on_chip=0)
  TpuDevice(id=22, process_index=5, coords=(0,3,2), core_on_chip=0)
  TpuDevice(id=21, process_index=5, coords=(1,2,2), core_on_chip=0)
  TpuDevice(id=23, process_index=5, coords=(1,3,2), core_on_chip=0)]
 [TpuDevice(id=28, process_index=7, coords=(0,2,3), core_on_chip=0)
  TpuDevice(id=30, process_index=7, coords=(0,3,3), core_on_chip=0)
  TpuDevice(id=29, process_index=7, coords=(1,2,3), core_on_chip=0)
  TpuDevice(id=31, process_index=7, coords=(1,3,3), core_on_chip=0)]]
I0512 23:41:58.218991 140339465779200 partitioning.py:257] global_mesh devices shape: (8, 4)
I0512 23:41:58.219059 140339465779200 partitioning.py:111] Overridden MoE global_mesh axes_names: ('data', 'expert', 'model')
I0512 23:41:58.219104 140339465779200 partitioning.py:114] Overridden MoE global_mesh devices: [[[TpuDevice(id=0, process_index=0, coords=(0,0,0), core_on_chip=0)
   TpuDevice(id=2, process_index=0, coords=(0,1,0), core_on_chip=0)
   TpuDevice(id=1, process_index=0, coords=(1,0,0), core_on_chip=0)
   TpuDevice(id=3, process_index=0, coords=(1,1,0), core_on_chip=0)]
  [TpuDevice(id=8, process_index=2, coords=(0,0,1), core_on_chip=0)
   TpuDevice(id=10, process_index=2, coords=(0,1,1), core_on_chip=0)
   TpuDevice(id=9, process_index=2, coords=(1,0,1), core_on_chip=0)
   TpuDevice(id=11, process_index=2, coords=(1,1,1), core_on_chip=0)]
  [TpuDevice(id=16, process_index=4, coords=(0,0,2), core_on_chip=0)
   TpuDevice(id=18, process_index=4, coords=(0,1,2), core_on_chip=0)
   TpuDevice(id=17, process_index=4, coords=(1,0,2), core_on_chip=0)
   TpuDevice(id=19, process_index=4, coords=(1,1,2), core_on_chip=0)]
  [TpuDevice(id=24, process_index=6, coords=(0,0,3), core_on_chip=0)
   TpuDevice(id=26, process_index=6, coords=(0,1,3), core_on_chip=0)
   TpuDevice(id=25, process_index=6, coords=(1,0,3), core_on_chip=0)
   TpuDevice(id=27, process_index=6, coords=(1,1,3), core_on_chip=0)]
  [TpuDevice(id=4, process_index=1, coords=(0,2,0), core_on_chip=0)
   TpuDevice(id=6, process_index=1, coords=(0,3,0), core_on_chip=0)
   TpuDevice(id=5, process_index=1, coords=(1,2,0), core_on_chip=0)
   TpuDevice(id=7, process_index=1, coords=(1,3,0), core_on_chip=0)]
  [TpuDevice(id=12, process_index=3, coords=(0,2,1), core_on_chip=0)
   TpuDevice(id=14, process_index=3, coords=(0,3,1), core_on_chip=0)
   TpuDevice(id=13, process_index=3, coords=(1,2,1), core_on_chip=0)
   TpuDevice(id=15, process_index=3, coords=(1,3,1), core_on_chip=0)]
  [TpuDevice(id=20, process_index=5, coords=(0,2,2), core_on_chip=0)
   TpuDevice(id=22, process_index=5, coords=(0,3,2), core_on_chip=0)
   TpuDevice(id=21, process_index=5, coords=(1,2,2), core_on_chip=0)
   TpuDevice(id=23, process_index=5, coords=(1,3,2), core_on_chip=0)]
  [TpuDevice(id=28, process_index=7, coords=(0,2,3), core_on_chip=0)
   TpuDevice(id=30, process_index=7, coords=(0,3,3), core_on_chip=0)
   TpuDevice(id=29, process_index=7, coords=(1,2,3), core_on_chip=0)
   TpuDevice(id=31, process_index=7, coords=(1,3,3), core_on_chip=0)]]]
I0512 23:41:58.219297 140339465779200 partitioning.py:115] Overridden MoE global_mesh shape: (1, 8, 4)
I0512 23:41:58.222263 140595318249472 partitioning.py:94] For MoE, first construct vanilla T5X (data, model) mesh.
I0512 23:41:58.222661 140595318249472 partitioning.py:151] last device coords : (1, 3, 3, 0)
last local device coords: (1, 1, 0, 0)
W0512 23:41:58.222864 140595318249472 partitioning.py:197] Tiling device assignment mesh by hosts, which may lead to reduced XLA collective performance. To avoid this, modify the model parallel submesh or run with more tasks per host.
I0512 23:41:58.223038 140595318249472 partitioning.py:255] global_mesh axis_names: ('data', 'model')
I0512 23:41:58.223092 140595318249472 partitioning.py:256] global_mesh devices: [[TpuDevice(id=0, process_index=0, coords=(0,0,0), core_on_chip=0)
  TpuDevice(id=2, process_index=0, coords=(0,1,0), core_on_chip=0)
  TpuDevice(id=1, process_index=0, coords=(1,0,0), core_on_chip=0)
  TpuDevice(id=3, process_index=0, coords=(1,1,0), core_on_chip=0)]
 [TpuDevice(id=8, process_index=2, coords=(0,0,1), core_on_chip=0)
  TpuDevice(id=10, process_index=2, coords=(0,1,1), core_on_chip=0)
  TpuDevice(id=9, process_index=2, coords=(1,0,1), core_on_chip=0)
  TpuDevice(id=11, process_index=2, coords=(1,1,1), core_on_chip=0)]
 [TpuDevice(id=16, process_index=4, coords=(0,0,2), core_on_chip=0)
  TpuDevice(id=18, process_index=4, coords=(0,1,2), core_on_chip=0)
  TpuDevice(id=17, process_index=4, coords=(1,0,2), core_on_chip=0)
  TpuDevice(id=19, process_index=4, coords=(1,1,2), core_on_chip=0)]
 [TpuDevice(id=24, process_index=6, coords=(0,0,3), core_on_chip=0)
  TpuDevice(id=26, process_index=6, coords=(0,1,3), core_on_chip=0)
  TpuDevice(id=25, process_index=6, coords=(1,0,3), core_on_chip=0)
  TpuDevice(id=27, process_index=6, coords=(1,1,3), core_on_chip=0)]
 [TpuDevice(id=4, process_index=1, coords=(0,2,0), core_on_chip=0)
  TpuDevice(id=6, process_index=1, coords=(0,3,0), core_on_chip=0)
  TpuDevice(id=5, process_index=1, coords=(1,2,0), core_on_chip=0)
  TpuDevice(id=7, process_index=1, coords=(1,3,0), core_on_chip=0)]
 [TpuDevice(id=12, process_index=3, coords=(0,2,1), core_on_chip=0)
  TpuDevice(id=14, process_index=3, coords=(0,3,1), core_on_chip=0)
  TpuDevice(id=13, process_index=3, coords=(1,2,1), core_on_chip=0)
  TpuDevice(id=15, process_index=3, coords=(1,3,1), core_on_chip=0)]
 [TpuDevice(id=20, process_index=5, coords=(0,2,2), core_on_chip=0)
  TpuDevice(id=22, process_index=5, coords=(0,3,2), core_on_chip=0)
  TpuDevice(id=21, process_index=5, coords=(1,2,2), core_on_chip=0)
  TpuDevice(id=23, process_index=5, coords=(1,3,2), core_on_chip=0)]
 [TpuDevice(id=28, process_index=7, coords=(0,2,3), core_on_chip=0)
  TpuDevice(id=30, process_index=7, coords=(0,3,3), core_on_chip=0)
  TpuDevice(id=29, process_index=7, coords=(1,2,3), core_on_chip=0)
  TpuDevice(id=31, process_index=7, coords=(1,3,3), core_on_chip=0)]]
I0512 23:41:58.223330 140595318249472 partitioning.py:257] global_mesh devices shape: (8, 4)
I0512 23:41:58.223393 140595318249472 partitioning.py:111] Overridden MoE global_mesh axes_names: ('data', 'expert', 'model')
I0512 23:41:58.223442 140595318249472 partitioning.py:114] Overridden MoE global_mesh devices: [[[TpuDevice(id=0, process_index=0, coords=(0,0,0), core_on_chip=0)
   TpuDevice(id=2, process_index=0, coords=(0,1,0), core_on_chip=0)
   TpuDevice(id=1, process_index=0, coords=(1,0,0), core_on_chip=0)
   TpuDevice(id=3, process_index=0, coords=(1,1,0), core_on_chip=0)]
  [TpuDevice(id=8, process_index=2, coords=(0,0,1), core_on_chip=0)
   TpuDevice(id=10, process_index=2, coords=(0,1,1), core_on_chip=0)
   TpuDevice(id=9, process_index=2, coords=(1,0,1), core_on_chip=0)
   TpuDevice(id=11, process_index=2, coords=(1,1,1), core_on_chip=0)]
  [TpuDevice(id=16, process_index=4, coords=(0,0,2), core_on_chip=0)
   TpuDevice(id=18, process_index=4, coords=(0,1,2), core_on_chip=0)
   TpuDevice(id=17, process_index=4, coords=(1,0,2), core_on_chip=0)
   TpuDevice(id=19, process_index=4, coords=(1,1,2), core_on_chip=0)]
  [TpuDevice(id=24, process_index=6, coords=(0,0,3), core_on_chip=0)
   TpuDevice(id=26, process_index=6, coords=(0,1,3), core_on_chip=0)
   TpuDevice(id=25, process_index=6, coords=(1,0,3), core_on_chip=0)
   TpuDevice(id=27, process_index=6, coords=(1,1,3), core_on_chip=0)]
  [TpuDevice(id=4, process_index=1, coords=(0,2,0), core_on_chip=0)
   TpuDevice(id=6, process_index=1, coords=(0,3,0), core_on_chip=0)
   TpuDevice(id=5, process_index=1, coords=(1,2,0), core_on_chip=0)
   TpuDevice(id=7, process_index=1, coords=(1,3,0), core_on_chip=0)]
  [TpuDevice(id=12, process_index=3, coords=(0,2,1), core_on_chip=0)
   TpuDevice(id=14, process_index=3, coords=(0,3,1), core_on_chip=0)
   TpuDevice(id=13, process_index=3, coords=(1,2,1), core_on_chip=0)
   TpuDevice(id=15, process_index=3, coords=(1,3,1), core_on_chip=0)]
  [TpuDevice(id=20, process_index=5, coords=(0,2,2), core_on_chip=0)
   TpuDevice(id=22, process_index=5, coords=(0,3,2), core_on_chip=0)
   TpuDevice(id=21, process_index=5, coords=(1,2,2), core_on_chip=0)
   TpuDevice(id=23, process_index=5, coords=(1,3,2), core_on_chip=0)]
  [TpuDevice(id=28, process_index=7, coords=(0,2,3), core_on_chip=0)
   TpuDevice(id=30, process_index=7, coords=(0,3,3), core_on_chip=0)
   TpuDevice(id=29, process_index=7, coords=(1,2,3), core_on_chip=0)
   TpuDevice(id=31, process_index=7, coords=(1,3,3), core_on_chip=0)]]]
I0512 23:41:58.223657 140595318249472 partitioning.py:115] Overridden MoE global_mesh shape: (1, 8, 4)
I0512 23:41:58.230697 140095893518336 partitioning.py:94] For MoE, first construct vanilla T5X (data, model) mesh.
I0512 23:41:58.231174 140095893518336 partitioning.py:151] last device coords : (1, 3, 3, 0)
last local device coords: (1, 1, 0, 0)
W0512 23:41:58.231406 140095893518336 partitioning.py:197] Tiling device assignment mesh by hosts, which may lead to reduced XLA collective performance. To avoid this, modify the model parallel submesh or run with more tasks per host.
I0512 23:41:58.231630 140095893518336 partitioning.py:255] global_mesh axis_names: ('data', 'model')
I0512 23:41:58.231690 140095893518336 partitioning.py:256] global_mesh devices: [[TpuDevice(id=0, process_index=0, coords=(0,0,0), core_on_chip=0)
  TpuDevice(id=2, process_index=0, coords=(0,1,0), core_on_chip=0)
  TpuDevice(id=1, process_index=0, coords=(1,0,0), core_on_chip=0)
  TpuDevice(id=3, process_index=0, coords=(1,1,0), core_on_chip=0)]
 [TpuDevice(id=8, process_index=2, coords=(0,0,1), core_on_chip=0)
  TpuDevice(id=10, process_index=2, coords=(0,1,1), core_on_chip=0)
  TpuDevice(id=9, process_index=2, coords=(1,0,1), core_on_chip=0)
  TpuDevice(id=11, process_index=2, coords=(1,1,1), core_on_chip=0)]
 [TpuDevice(id=16, process_index=4, coords=(0,0,2), core_on_chip=0)
  TpuDevice(id=18, process_index=4, coords=(0,1,2), core_on_chip=0)
  TpuDevice(id=17, process_index=4, coords=(1,0,2), core_on_chip=0)
  TpuDevice(id=19, process_index=4, coords=(1,1,2), core_on_chip=0)]
 [TpuDevice(id=24, process_index=6, coords=(0,0,3), core_on_chip=0)
  TpuDevice(id=26, process_index=6, coords=(0,1,3), core_on_chip=0)
  TpuDevice(id=25, process_index=6, coords=(1,0,3), core_on_chip=0)
  TpuDevice(id=27, process_index=6, coords=(1,1,3), core_on_chip=0)]
 [TpuDevice(id=4, process_index=1, coords=(0,2,0), core_on_chip=0)
  TpuDevice(id=6, process_index=1, coords=(0,3,0), core_on_chip=0)
  TpuDevice(id=5, process_index=1, coords=(1,2,0), core_on_chip=0)
  TpuDevice(id=7, process_index=1, coords=(1,3,0), core_on_chip=0)]
 [TpuDevice(id=12, process_index=3, coords=(0,2,1), core_on_chip=0)
  TpuDevice(id=14, process_index=3, coords=(0,3,1), core_on_chip=0)
  TpuDevice(id=13, process_index=3, coords=(1,2,1), core_on_chip=0)
  TpuDevice(id=15, process_index=3, coords=(1,3,1), core_on_chip=0)]
 [TpuDevice(id=20, process_index=5, coords=(0,2,2), core_on_chip=0)
  TpuDevice(id=22, process_index=5, coords=(0,3,2), core_on_chip=0)
  TpuDevice(id=21, process_index=5, coords=(1,2,2), core_on_chip=0)
  TpuDevice(id=23, process_index=5, coords=(1,3,2), core_on_chip=0)]
 [TpuDevice(id=28, process_index=7, coords=(0,2,3), core_on_chip=0)
  TpuDevice(id=30, process_index=7, coords=(0,3,3), core_on_chip=0)
  TpuDevice(id=29, process_index=7, coords=(1,2,3), core_on_chip=0)
  TpuDevice(id=31, process_index=7, coords=(1,3,3), core_on_chip=0)]]
I0512 23:41:58.231966 140095893518336 partitioning.py:257] global_mesh devices shape: (8, 4)
I0512 23:41:58.232039 140095893518336 partitioning.py:111] Overridden MoE global_mesh axes_names: ('data', 'expert', 'model')
I0512 23:41:58.232084 140095893518336 partitioning.py:114] Overridden MoE global_mesh devices: [[[TpuDevice(id=0, process_index=0, coords=(0,0,0), core_on_chip=0)
   TpuDevice(id=2, process_index=0, coords=(0,1,0), core_on_chip=0)
   TpuDevice(id=1, process_index=0, coords=(1,0,0), core_on_chip=0)
   TpuDevice(id=3, process_index=0, coords=(1,1,0), core_on_chip=0)]
  [TpuDevice(id=8, process_index=2, coords=(0,0,1), core_on_chip=0)
   TpuDevice(id=10, process_index=2, coords=(0,1,1), core_on_chip=0)
   TpuDevice(id=9, process_index=2, coords=(1,0,1), core_on_chip=0)
   TpuDevice(id=11, process_index=2, coords=(1,1,1), core_on_chip=0)]
  [TpuDevice(id=16, process_index=4, coords=(0,0,2), core_on_chip=0)
   TpuDevice(id=18, process_index=4, coords=(0,1,2), core_on_chip=0)
   TpuDevice(id=17, process_index=4, coords=(1,0,2), core_on_chip=0)
   TpuDevice(id=19, process_index=4, coords=(1,1,2), core_on_chip=0)]
  [TpuDevice(id=24, process_index=6, coords=(0,0,3), core_on_chip=0)
   TpuDevice(id=26, process_index=6, coords=(0,1,3), core_on_chip=0)
   TpuDevice(id=25, process_index=6, coords=(1,0,3), core_on_chip=0)
   TpuDevice(id=27, process_index=6, coords=(1,1,3), core_on_chip=0)]
  [TpuDevice(id=4, process_index=1, coords=(0,2,0), core_on_chip=0)
   TpuDevice(id=6, process_index=1, coords=(0,3,0), core_on_chip=0)
   TpuDevice(id=5, process_index=1, coords=(1,2,0), core_on_chip=0)
   TpuDevice(id=7, process_index=1, coords=(1,3,0), core_on_chip=0)]
  [TpuDevice(id=12, process_index=3, coords=(0,2,1), core_on_chip=0)
   TpuDevice(id=14, process_index=3, coords=(0,3,1), core_on_chip=0)
   TpuDevice(id=13, process_index=3, coords=(1,2,1), core_on_chip=0)
   TpuDevice(id=15, process_index=3, coords=(1,3,1), core_on_chip=0)]
  [TpuDevice(id=20, process_index=5, coords=(0,2,2), core_on_chip=0)
   TpuDevice(id=22, process_index=5, coords=(0,3,2), core_on_chip=0)
   TpuDevice(id=21, process_index=5, coords=(1,2,2), core_on_chip=0)
   TpuDevice(id=23, process_index=5, coords=(1,3,2), core_on_chip=0)]
  [TpuDevice(id=28, process_index=7, coords=(0,2,3), core_on_chip=0)
   TpuDevice(id=30, process_index=7, coords=(0,3,3), core_on_chip=0)
   TpuDevice(id=29, process_index=7, coords=(1,2,3), core_on_chip=0)
   TpuDevice(id=31, process_index=7, coords=(1,3,3), core_on_chip=0)]]]
I0512 23:41:58.232268 140095893518336 partitioning.py:115] Overridden MoE global_mesh shape: (1, 8, 4)
I0512 23:41:58.212565 140311617681408 partitioning.py:94] For MoE, first construct vanilla T5X (data, model) mesh.
I0512 23:41:58.212928 140311617681408 partitioning.py:151] last device coords : (1, 3, 3, 0)
last local device coords: (1, 1, 0, 0)
W0512 23:41:58.213124 140311617681408 partitioning.py:197] Tiling device assignment mesh by hosts, which may lead to reduced XLA collective performance. To avoid this, modify the model parallel submesh or run with more tasks per host.
I0512 23:41:58.213296 140311617681408 partitioning.py:255] global_mesh axis_names: ('data', 'model')
I0512 23:41:58.213352 140311617681408 partitioning.py:256] global_mesh devices: [[TpuDevice(id=0, process_index=0, coords=(0,0,0), core_on_chip=0)
  TpuDevice(id=2, process_index=0, coords=(0,1,0), core_on_chip=0)
  TpuDevice(id=1, process_index=0, coords=(1,0,0), core_on_chip=0)
  TpuDevice(id=3, process_index=0, coords=(1,1,0), core_on_chip=0)]
 [TpuDevice(id=8, process_index=2, coords=(0,0,1), core_on_chip=0)
  TpuDevice(id=10, process_index=2, coords=(0,1,1), core_on_chip=0)
  TpuDevice(id=9, process_index=2, coords=(1,0,1), core_on_chip=0)
  TpuDevice(id=11, process_index=2, coords=(1,1,1), core_on_chip=0)]
 [TpuDevice(id=16, process_index=4, coords=(0,0,2), core_on_chip=0)
  TpuDevice(id=18, process_index=4, coords=(0,1,2), core_on_chip=0)
  TpuDevice(id=17, process_index=4, coords=(1,0,2), core_on_chip=0)
  TpuDevice(id=19, process_index=4, coords=(1,1,2), core_on_chip=0)]
 [TpuDevice(id=24, process_index=6, coords=(0,0,3), core_on_chip=0)
  TpuDevice(id=26, process_index=6, coords=(0,1,3), core_on_chip=0)
  TpuDevice(id=25, process_index=6, coords=(1,0,3), core_on_chip=0)
  TpuDevice(id=27, process_index=6, coords=(1,1,3), core_on_chip=0)]
 [TpuDevice(id=4, process_index=1, coords=(0,2,0), core_on_chip=0)
  TpuDevice(id=6, process_index=1, coords=(0,3,0), core_on_chip=0)
  TpuDevice(id=5, process_index=1, coords=(1,2,0), core_on_chip=0)
  TpuDevice(id=7, process_index=1, coords=(1,3,0), core_on_chip=0)]
 [TpuDevice(id=12, process_index=3, coords=(0,2,1), core_on_chip=0)
  TpuDevice(id=14, process_index=3, coords=(0,3,1), core_on_chip=0)
  TpuDevice(id=13, process_index=3, coords=(1,2,1), core_on_chip=0)
  TpuDevice(id=15, process_index=3, coords=(1,3,1), core_on_chip=0)]
 [TpuDevice(id=20, process_index=5, coords=(0,2,2), core_on_chip=0)
  TpuDevice(id=22, process_index=5, coords=(0,3,2), core_on_chip=0)
  TpuDevice(id=21, process_index=5, coords=(1,2,2), core_on_chip=0)
  TpuDevice(id=23, process_index=5, coords=(1,3,2), core_on_chip=0)]
 [TpuDevice(id=28, process_index=7, coords=(0,2,3), core_on_chip=0)
  TpuDevice(id=30, process_index=7, coords=(0,3,3), core_on_chip=0)
  TpuDevice(id=29, process_index=7, coords=(1,2,3), core_on_chip=0)
  TpuDevice(id=31, process_index=7, coords=(1,3,3), core_on_chip=0)]]
I0512 23:41:58.213679 140311617681408 partitioning.py:257] global_mesh devices shape: (8, 4)
I0512 23:41:58.213800 140311617681408 partitioning.py:111] Overridden MoE global_mesh axes_names: ('data', 'expert', 'model')
I0512 23:41:58.213850 140311617681408 partitioning.py:114] Overridden MoE global_mesh devices: [[[TpuDevice(id=0, process_index=0, coords=(0,0,0), core_on_chip=0)
   TpuDevice(id=2, process_index=0, coords=(0,1,0), core_on_chip=0)
   TpuDevice(id=1, process_index=0, coords=(1,0,0), core_on_chip=0)
   TpuDevice(id=3, process_index=0, coords=(1,1,0), core_on_chip=0)]
  [TpuDevice(id=8, process_index=2, coords=(0,0,1), core_on_chip=0)
   TpuDevice(id=10, process_index=2, coords=(0,1,1), core_on_chip=0)
   TpuDevice(id=9, process_index=2, coords=(1,0,1), core_on_chip=0)
   TpuDevice(id=11, process_index=2, coords=(1,1,1), core_on_chip=0)]
  [TpuDevice(id=16, process_index=4, coords=(0,0,2), core_on_chip=0)
   TpuDevice(id=18, process_index=4, coords=(0,1,2), core_on_chip=0)
   TpuDevice(id=17, process_index=4, coords=(1,0,2), core_on_chip=0)
   TpuDevice(id=19, process_index=4, coords=(1,1,2), core_on_chip=0)]
  [TpuDevice(id=24, process_index=6, coords=(0,0,3), core_on_chip=0)
   TpuDevice(id=26, process_index=6, coords=(0,1,3), core_on_chip=0)
   TpuDevice(id=25, process_index=6, coords=(1,0,3), core_on_chip=0)
   TpuDevice(id=27, process_index=6, coords=(1,1,3), core_on_chip=0)]
  [TpuDevice(id=4, process_index=1, coords=(0,2,0), core_on_chip=0)
   TpuDevice(id=6, process_index=1, coords=(0,3,0), core_on_chip=0)
   TpuDevice(id=5, process_index=1, coords=(1,2,0), core_on_chip=0)
   TpuDevice(id=7, process_index=1, coords=(1,3,0), core_on_chip=0)]
  [TpuDevice(id=12, process_index=3, coords=(0,2,1), core_on_chip=0)
   TpuDevice(id=14, process_index=3, coords=(0,3,1), core_on_chip=0)
   TpuDevice(id=13, process_index=3, coords=(1,2,1), core_on_chip=0)
   TpuDevice(id=15, process_index=3, coords=(1,3,1), core_on_chip=0)]
  [TpuDevice(id=20, process_index=5, coords=(0,2,2), core_on_chip=0)
   TpuDevice(id=22, process_index=5, coords=(0,3,2), core_on_chip=0)
   TpuDevice(id=21, process_index=5, coords=(1,2,2), core_on_chip=0)
   TpuDevice(id=23, process_index=5, coords=(1,3,2), core_on_chip=0)]
  [TpuDevice(id=28, process_index=7, coords=(0,2,3), core_on_chip=0)
   TpuDevice(id=30, process_index=7, coords=(0,3,3), core_on_chip=0)
   TpuDevice(id=29, process_index=7, coords=(1,2,3), core_on_chip=0)
   TpuDevice(id=31, process_index=7, coords=(1,3,3), core_on_chip=0)]]]
I0512 23:41:58.214042 140311617681408 partitioning.py:115] Overridden MoE global_mesh shape: (1, 8, 4)
I0512 23:41:58.225413 140529434355712 partitioning.py:94] For MoE, first construct vanilla T5X (data, model) mesh.
I0512 23:41:58.225857 140529434355712 partitioning.py:151] last device coords : (1, 3, 3, 0)
last local device coords: (1, 1, 0, 0)
W0512 23:41:58.226071 140529434355712 partitioning.py:197] Tiling device assignment mesh by hosts, which may lead to reduced XLA collective performance. To avoid this, modify the model parallel submesh or run with more tasks per host.
I0512 23:41:58.226265 140529434355712 partitioning.py:255] global_mesh axis_names: ('data', 'model')
I0512 23:41:58.226319 140529434355712 partitioning.py:256] global_mesh devices: [[TpuDevice(id=0, process_index=0, coords=(0,0,0), core_on_chip=0)
  TpuDevice(id=2, process_index=0, coords=(0,1,0), core_on_chip=0)
  TpuDevice(id=1, process_index=0, coords=(1,0,0), core_on_chip=0)
  TpuDevice(id=3, process_index=0, coords=(1,1,0), core_on_chip=0)]
 [TpuDevice(id=8, process_index=2, coords=(0,0,1), core_on_chip=0)
  TpuDevice(id=10, process_index=2, coords=(0,1,1), core_on_chip=0)
  TpuDevice(id=9, process_index=2, coords=(1,0,1), core_on_chip=0)
  TpuDevice(id=11, process_index=2, coords=(1,1,1), core_on_chip=0)]
 [TpuDevice(id=16, process_index=4, coords=(0,0,2), core_on_chip=0)
  TpuDevice(id=18, process_index=4, coords=(0,1,2), core_on_chip=0)
  TpuDevice(id=17, process_index=4, coords=(1,0,2), core_on_chip=0)
  TpuDevice(id=19, process_index=4, coords=(1,1,2), core_on_chip=0)]
 [TpuDevice(id=24, process_index=6, coords=(0,0,3), core_on_chip=0)
  TpuDevice(id=26, process_index=6, coords=(0,1,3), core_on_chip=0)
  TpuDevice(id=25, process_index=6, coords=(1,0,3), core_on_chip=0)
  TpuDevice(id=27, process_index=6, coords=(1,1,3), core_on_chip=0)]
 [TpuDevice(id=4, process_index=1, coords=(0,2,0), core_on_chip=0)
  TpuDevice(id=6, process_index=1, coords=(0,3,0), core_on_chip=0)
  TpuDevice(id=5, process_index=1, coords=(1,2,0), core_on_chip=0)
  TpuDevice(id=7, process_index=1, coords=(1,3,0), core_on_chip=0)]
 [TpuDevice(id=12, process_index=3, coords=(0,2,1), core_on_chip=0)
  TpuDevice(id=14, process_index=3, coords=(0,3,1), core_on_chip=0)
  TpuDevice(id=13, process_index=3, coords=(1,2,1), core_on_chip=0)
  TpuDevice(id=15, process_index=3, coords=(1,3,1), core_on_chip=0)]
 [TpuDevice(id=20, process_index=5, coords=(0,2,2), core_on_chip=0)
  TpuDevice(id=22, process_index=5, coords=(0,3,2), core_on_chip=0)
  TpuDevice(id=21, process_index=5, coords=(1,2,2), core_on_chip=0)
  TpuDevice(id=23, process_index=5, coords=(1,3,2), core_on_chip=0)]
 [TpuDevice(id=28, process_index=7, coords=(0,2,3), core_on_chip=0)
  TpuDevice(id=30, process_index=7, coords=(0,3,3), core_on_chip=0)
  TpuDevice(id=29, process_index=7, coords=(1,2,3), core_on_chip=0)
  TpuDevice(id=31, process_index=7, coords=(1,3,3), core_on_chip=0)]]
I0512 23:41:58.226557 140529434355712 partitioning.py:257] global_mesh devices shape: (8, 4)
I0512 23:41:58.226618 140529434355712 partitioning.py:111] Overridden MoE global_mesh axes_names: ('data', 'expert', 'model')
I0512 23:41:58.226661 140529434355712 partitioning.py:114] Overridden MoE global_mesh devices: [[[TpuDevice(id=0, process_index=0, coords=(0,0,0), core_on_chip=0)
   TpuDevice(id=2, process_index=0, coords=(0,1,0), core_on_chip=0)
   TpuDevice(id=1, process_index=0, coords=(1,0,0), core_on_chip=0)
   TpuDevice(id=3, process_index=0, coords=(1,1,0), core_on_chip=0)]
  [TpuDevice(id=8, process_index=2, coords=(0,0,1), core_on_chip=0)
   TpuDevice(id=10, process_index=2, coords=(0,1,1), core_on_chip=0)
   TpuDevice(id=9, process_index=2, coords=(1,0,1), core_on_chip=0)
   TpuDevice(id=11, process_index=2, coords=(1,1,1), core_on_chip=0)]
  [TpuDevice(id=16, process_index=4, coords=(0,0,2), core_on_chip=0)
   TpuDevice(id=18, process_index=4, coords=(0,1,2), core_on_chip=0)
   TpuDevice(id=17, process_index=4, coords=(1,0,2), core_on_chip=0)
   TpuDevice(id=19, process_index=4, coords=(1,1,2), core_on_chip=0)]
  [TpuDevice(id=24, process_index=6, coords=(0,0,3), core_on_chip=0)
   TpuDevice(id=26, process_index=6, coords=(0,1,3), core_on_chip=0)
   TpuDevice(id=25, process_index=6, coords=(1,0,3), core_on_chip=0)
   TpuDevice(id=27, process_index=6, coords=(1,1,3), core_on_chip=0)]
  [TpuDevice(id=4, process_index=1, coords=(0,2,0), core_on_chip=0)
   TpuDevice(id=6, process_index=1, coords=(0,3,0), core_on_chip=0)
   TpuDevice(id=5, process_index=1, coords=(1,2,0), core_on_chip=0)
   TpuDevice(id=7, process_index=1, coords=(1,3,0), core_on_chip=0)]
  [TpuDevice(id=12, process_index=3, coords=(0,2,1), core_on_chip=0)
   TpuDevice(id=14, process_index=3, coords=(0,3,1), core_on_chip=0)
   TpuDevice(id=13, process_index=3, coords=(1,2,1), core_on_chip=0)
   TpuDevice(id=15, process_index=3, coords=(1,3,1), core_on_chip=0)]
  [TpuDevice(id=20, process_index=5, coords=(0,2,2), core_on_chip=0)
   TpuDevice(id=22, process_index=5, coords=(0,3,2), core_on_chip=0)
   TpuDevice(id=21, process_index=5, coords=(1,2,2), core_on_chip=0)
   TpuDevice(id=23, process_index=5, coords=(1,3,2), core_on_chip=0)]
  [TpuDevice(id=28, process_index=7, coords=(0,2,3), core_on_chip=0)
   TpuDevice(id=30, process_index=7, coords=(0,3,3), core_on_chip=0)
   TpuDevice(id=29, process_index=7, coords=(1,2,3), core_on_chip=0)
   TpuDevice(id=31, process_index=7, coords=(1,3,3), core_on_chip=0)]]]
I0512 23:41:58.226847 140529434355712 partitioning.py:115] Overridden MoE global_mesh shape: (1, 8, 4)
I0512 23:41:59.087656 140095893518336 utils.py:1979] Initializing dataset for task 'mix_full_lm_test' with a replica batch size of 48 and a seed of 1715557319
I0512 23:41:59.087710 139674595104768 utils.py:1979] Initializing dataset for task 'mix_full_lm_test' with a replica batch size of 48 and a seed of 1715557319
I0512 23:41:59.087626 140440593786880 utils.py:1979] Initializing dataset for task 'mix_full_lm_test' with a replica batch size of 48 and a seed of 1715557319
I0512 23:41:59.087709 140595318249472 utils.py:1979] Initializing dataset for task 'mix_full_lm_test' with a replica batch size of 48 and a seed of 1715557319
I0512 23:41:59.087660 140031916288000 utils.py:1979] Initializing dataset for task 'mix_full_lm_test' with a replica batch size of 48 and a seed of 1715557319
I0512 23:41:59.087658 140339465779200 utils.py:1979] Initializing dataset for task 'mix_full_lm_test' with a replica batch size of 48 and a seed of 1715557319
I0512 23:41:59.087783 140529434355712 utils.py:1979] Initializing dataset for task 'mix_full_lm_test' with a replica batch size of 48 and a seed of 1715557319
I0512 23:41:59.087602 140311617681408 utils.py:1979] Initializing dataset for task 'mix_full_lm_test' with a replica batch size of 48 and a seed of 1715557319
I0512 23:41:59.406347 140031916288000 dataset_info.py:540] Load dataset info from gs://rosinality-tpu-bucket/redpajama_arxiv/1.0.0
I0512 23:41:59.415103 140095893518336 dataset_info.py:540] Load dataset info from gs://rosinality-tpu-bucket/redpajama_arxiv/1.0.0
I0512 23:41:59.414055 140440593786880 dataset_info.py:540] Load dataset info from gs://rosinality-tpu-bucket/redpajama_arxiv/1.0.0
I0512 23:41:59.415264 140595318249472 dataset_info.py:540] Load dataset info from gs://rosinality-tpu-bucket/redpajama_arxiv/1.0.0
I0512 23:41:59.417365 140339465779200 dataset_info.py:540] Load dataset info from gs://rosinality-tpu-bucket/redpajama_arxiv/1.0.0
I0512 23:41:59.431734 139674595104768 dataset_info.py:540] Load dataset info from gs://rosinality-tpu-bucket/redpajama_arxiv/1.0.0
I0512 23:41:59.415701 140529434355712 dataset_info.py:540] Load dataset info from gs://rosinality-tpu-bucket/redpajama_arxiv/1.0.0
I0512 23:41:59.427833 140311617681408 dataset_info.py:540] Load dataset info from gs://rosinality-tpu-bucket/redpajama_arxiv/1.0.0
I0512 23:41:59.635547 140440593786880 dataset_providers.py:1597] Sharding at the data source: 8 of 8
I0512 23:41:59.647505 140595318249472 dataset_providers.py:1597] Sharding at the data source: 5 of 8
I0512 23:41:59.660053 140095893518336 dataset_providers.py:1597] Sharding at the data source: 6 of 8
I0512 23:41:59.639866 140529434355712 dataset_providers.py:1597] Sharding at the data source: 1 of 8
I0512 23:41:59.657869 140339465779200 dataset_providers.py:1597] Sharding at the data source: 7 of 8
I0512 23:41:59.663728 139674595104768 dataset_providers.py:1597] Sharding at the data source: 2 of 8
I0512 23:41:59.662344 140031916288000 dataset_providers.py:1597] Sharding at the data source: 4 of 8
I0512 23:41:59.644226 140311617681408 dataset_providers.py:1597] Sharding at the data source: 3 of 8
I0512 23:41:59.698413 140440593786880 dataset_info.py:540] Load dataset info from gs://rosinality-tpu-bucket/redpajama_arxiv/1.0.0
I0512 23:41:59.709914 140595318249472 dataset_info.py:540] Load dataset info from gs://rosinality-tpu-bucket/redpajama_arxiv/1.0.0
I0512 23:41:59.697040 140529434355712 dataset_info.py:540] Load dataset info from gs://rosinality-tpu-bucket/redpajama_arxiv/1.0.0
I0512 23:41:59.720021 140095893518336 dataset_info.py:540] Load dataset info from gs://rosinality-tpu-bucket/redpajama_arxiv/1.0.0
I0512 23:41:59.725401 139674595104768 dataset_info.py:540] Load dataset info from gs://rosinality-tpu-bucket/redpajama_arxiv/1.0.0
I0512 23:41:59.705183 140311617681408 dataset_info.py:540] Load dataset info from gs://rosinality-tpu-bucket/redpajama_arxiv/1.0.0
I0512 23:41:59.725496 140339465779200 dataset_info.py:540] Load dataset info from gs://rosinality-tpu-bucket/redpajama_arxiv/1.0.0
I0512 23:41:59.741484 140031916288000 dataset_info.py:540] Load dataset info from gs://rosinality-tpu-bucket/redpajama_arxiv/1.0.0
I0512 23:41:59.924680 140595318249472 dataset_builder.py:421] Reusing dataset redpajama_arxiv (gs://rosinality-tpu-bucket/redpajama_arxiv/1.0.0)
I0512 23:41:59.914878 140529434355712 dataset_builder.py:421] Reusing dataset redpajama_arxiv (gs://rosinality-tpu-bucket/redpajama_arxiv/1.0.0)
I0512 23:41:59.940086 140339465779200 dataset_builder.py:421] Reusing dataset redpajama_arxiv (gs://rosinality-tpu-bucket/redpajama_arxiv/1.0.0)
I0512 23:41:59.925031 140311617681408 dataset_builder.py:421] Reusing dataset redpajama_arxiv (gs://rosinality-tpu-bucket/redpajama_arxiv/1.0.0)
I0512 23:41:59.949620 140095893518336 dataset_builder.py:421] Reusing dataset redpajama_arxiv (gs://rosinality-tpu-bucket/redpajama_arxiv/1.0.0)
I0512 23:41:59.950305 140440593786880 dataset_builder.py:421] Reusing dataset redpajama_arxiv (gs://rosinality-tpu-bucket/redpajama_arxiv/1.0.0)
I0512 23:41:59.985987 139674595104768 dataset_builder.py:421] Reusing dataset redpajama_arxiv (gs://rosinality-tpu-bucket/redpajama_arxiv/1.0.0)
I0512 23:42:00.010991 140031916288000 dataset_builder.py:421] Reusing dataset redpajama_arxiv (gs://rosinality-tpu-bucket/redpajama_arxiv/1.0.0)
I0512 23:42:00.093005 140595318249472 logging_logger.py:49] Constructing tf.data.Dataset redpajama_arxiv for split train, from gs://rosinality-tpu-bucket/redpajama_arxiv/1.0.0
I0512 23:42:00.080692 140529434355712 logging_logger.py:49] Constructing tf.data.Dataset redpajama_arxiv for split train, from gs://rosinality-tpu-bucket/redpajama_arxiv/1.0.0
I0512 23:42:00.098129 140339465779200 logging_logger.py:49] Constructing tf.data.Dataset redpajama_arxiv for split train, from gs://rosinality-tpu-bucket/redpajama_arxiv/1.0.0
I0512 23:42:00.081922 140311617681408 logging_logger.py:49] Constructing tf.data.Dataset redpajama_arxiv for split train, from gs://rosinality-tpu-bucket/redpajama_arxiv/1.0.0
I0512 23:42:00.107064 140095893518336 logging_logger.py:49] Constructing tf.data.Dataset redpajama_arxiv for split train, from gs://rosinality-tpu-bucket/redpajama_arxiv/1.0.0
I0512 23:42:00.109706 140440593786880 logging_logger.py:49] Constructing tf.data.Dataset redpajama_arxiv for split train, from gs://rosinality-tpu-bucket/redpajama_arxiv/1.0.0
I0512 23:42:00.159785 139674595104768 logging_logger.py:49] Constructing tf.data.Dataset redpajama_arxiv for split train, from gs://rosinality-tpu-bucket/redpajama_arxiv/1.0.0
I0512 23:42:00.177047 140031916288000 logging_logger.py:49] Constructing tf.data.Dataset redpajama_arxiv for split train, from gs://rosinality-tpu-bucket/redpajama_arxiv/1.0.0
WARNING:tensorflow:From /home/rosinality/openmoe_venv/lib/python3.10/site-packages/tensorflow/python/util/nest_util.py:1105: _RandomDataset.__init__ (from tensorflow.python.data.experimental.ops.random_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.random(...)`.
W0512 23:42:01.069559 140339465779200 deprecation.py:50] From /home/rosinality/openmoe_venv/lib/python3.10/site-packages/tensorflow/python/util/nest_util.py:1105: _RandomDataset.__init__ (from tensorflow.python.data.experimental.ops.random_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.random(...)`.
WARNING:tensorflow:From /home/rosinality/openmoe_venv/lib/python3.10/site-packages/tensorflow/python/util/nest_util.py:1105: _RandomDataset.__init__ (from tensorflow.python.data.experimental.ops.random_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.random(...)`.
W0512 23:42:01.059219 140311617681408 deprecation.py:50] From /home/rosinality/openmoe_venv/lib/python3.10/site-packages/tensorflow/python/util/nest_util.py:1105: _RandomDataset.__init__ (from tensorflow.python.data.experimental.ops.random_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.random(...)`.
WARNING:tensorflow:From /home/rosinality/openmoe_venv/lib/python3.10/site-packages/tensorflow/python/util/nest_util.py:1105: _RandomDataset.__init__ (from tensorflow.python.data.experimental.ops.random_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.random(...)`.
W0512 23:42:01.083044 140440593786880 deprecation.py:50] From /home/rosinality/openmoe_venv/lib/python3.10/site-packages/tensorflow/python/util/nest_util.py:1105: _RandomDataset.__init__ (from tensorflow.python.data.experimental.ops.random_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.random(...)`.
WARNING:tensorflow:From /home/rosinality/openmoe_venv/lib/python3.10/site-packages/tensorflow/python/util/nest_util.py:1105: _RandomDataset.__init__ (from tensorflow.python.data.experimental.ops.random_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.random(...)`.
W0512 23:42:01.087924 140595318249472 deprecation.py:50] From /home/rosinality/openmoe_venv/lib/python3.10/site-packages/tensorflow/python/util/nest_util.py:1105: _RandomDataset.__init__ (from tensorflow.python.data.experimental.ops.random_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.random(...)`.
WARNING:tensorflow:From /home/rosinality/openmoe_venv/lib/python3.10/site-packages/tensorflow/python/util/nest_util.py:1105: _RandomDataset.__init__ (from tensorflow.python.data.experimental.ops.random_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.random(...)`.
W0512 23:42:01.101081 140095893518336 deprecation.py:50] From /home/rosinality/openmoe_venv/lib/python3.10/site-packages/tensorflow/python/util/nest_util.py:1105: _RandomDataset.__init__ (from tensorflow.python.data.experimental.ops.random_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.random(...)`.
WARNING:tensorflow:From /home/rosinality/openmoe_venv/lib/python3.10/site-packages/tensorflow/python/util/nest_util.py:1105: _RandomDataset.__init__ (from tensorflow.python.data.experimental.ops.random_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.random(...)`.
W0512 23:42:01.082549 140529434355712 deprecation.py:50] From /home/rosinality/openmoe_venv/lib/python3.10/site-packages/tensorflow/python/util/nest_util.py:1105: _RandomDataset.__init__ (from tensorflow.python.data.experimental.ops.random_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.random(...)`.
WARNING:tensorflow:From /home/rosinality/openmoe_venv/lib/python3.10/site-packages/tensorflow/python/util/nest_util.py:1105: _RandomDataset.__init__ (from tensorflow.python.data.experimental.ops.random_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.random(...)`.
W0512 23:42:01.143121 139674595104768 deprecation.py:50] From /home/rosinality/openmoe_venv/lib/python3.10/site-packages/tensorflow/python/util/nest_util.py:1105: _RandomDataset.__init__ (from tensorflow.python.data.experimental.ops.random_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.random(...)`.
WARNING:tensorflow:From /home/rosinality/openmoe_venv/lib/python3.10/site-packages/tensorflow/python/util/nest_util.py:1105: _RandomDataset.__init__ (from tensorflow.python.data.experimental.ops.random_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.random(...)`.
W0512 23:42:01.169483 140031916288000 deprecation.py:50] From /home/rosinality/openmoe_venv/lib/python3.10/site-packages/tensorflow/python/util/nest_util.py:1105: _RandomDataset.__init__ (from tensorflow.python.data.experimental.ops.random_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.random(...)`.
I0512 23:42:03.202967 140311617681408 dataset_info.py:540] Load dataset info from gs://rosinality-tpu-bucket/redpajama_c4/1.0.0
I0512 23:42:03.225159 140339465779200 dataset_info.py:540] Load dataset info from gs://rosinality-tpu-bucket/redpajama_c4/1.0.0
I0512 23:42:03.250324 140440593786880 dataset_info.py:540] Load dataset info from gs://rosinality-tpu-bucket/redpajama_c4/1.0.0
I0512 23:42:03.262928 140529434355712 dataset_info.py:540] Load dataset info from gs://rosinality-tpu-bucket/redpajama_c4/1.0.0
I0512 23:42:03.295565 140595318249472 dataset_info.py:540] Load dataset info from gs://rosinality-tpu-bucket/redpajama_c4/1.0.0
I0512 23:42:03.300206 140095893518336 dataset_info.py:540] Load dataset info from gs://rosinality-tpu-bucket/redpajama_c4/1.0.0
I0512 23:42:03.299543 139674595104768 dataset_info.py:540] Load dataset info from gs://rosinality-tpu-bucket/redpajama_c4/1.0.0
I0512 23:42:03.331013 140031916288000 dataset_info.py:540] Load dataset info from gs://rosinality-tpu-bucket/redpajama_c4/1.0.0
I0512 23:42:03.434786 140339465779200 dataset_providers.py:1597] Sharding at the data source: 7 of 8
I0512 23:42:03.418397 140311617681408 dataset_providers.py:1597] Sharding at the data source: 3 of 8
I0512 23:42:03.478011 140440593786880 dataset_providers.py:1597] Sharding at the data source: 8 of 8
I0512 23:42:03.491857 140339465779200 dataset_info.py:540] Load dataset info from gs://rosinality-tpu-bucket/redpajama_c4/1.0.0
I0512 23:42:03.501663 139674595104768 dataset_providers.py:1597] Sharding at the data source: 2 of 8
I0512 23:42:03.507971 140095893518336 dataset_providers.py:1597] Sharding at the data source: 6 of 8
I0512 23:42:03.500200 140311617681408 dataset_info.py:540] Load dataset info from gs://rosinality-tpu-bucket/redpajama_c4/1.0.0
I0512 23:42:03.513759 140529434355712 dataset_providers.py:1597] Sharding at the data source: 1 of 8
I0512 23:42:03.536763 140440593786880 dataset_info.py:540] Load dataset info from gs://rosinality-tpu-bucket/redpajama_c4/1.0.0
I0512 23:42:03.547254 140031916288000 dataset_providers.py:1597] Sharding at the data source: 4 of 8
I0512 23:42:03.559795 140595318249472 dataset_providers.py:1597] Sharding at the data source: 5 of 8
I0512 23:42:03.571539 139674595104768 dataset_info.py:540] Load dataset info from gs://rosinality-tpu-bucket/redpajama_c4/1.0.0
I0512 23:42:03.571623 140529434355712 dataset_info.py:540] Load dataset info from gs://rosinality-tpu-bucket/redpajama_c4/1.0.0
I0512 23:42:03.595372 140095893518336 dataset_info.py:540] Load dataset info from gs://rosinality-tpu-bucket/redpajama_c4/1.0.0
I0512 23:42:03.603627 140031916288000 dataset_info.py:540] Load dataset info from gs://rosinality-tpu-bucket/redpajama_c4/1.0.0
I0512 23:42:03.613656 140595318249472 dataset_info.py:540] Load dataset info from gs://rosinality-tpu-bucket/redpajama_c4/1.0.0
I0512 23:42:03.717308 140311617681408 dataset_builder.py:421] Reusing dataset redpajama_c4 (gs://rosinality-tpu-bucket/redpajama_c4/1.0.0)
I0512 23:42:03.740500 140339465779200 dataset_builder.py:421] Reusing dataset redpajama_c4 (gs://rosinality-tpu-bucket/redpajama_c4/1.0.0)
I0512 23:42:03.803226 139674595104768 dataset_builder.py:421] Reusing dataset redpajama_c4 (gs://rosinality-tpu-bucket/redpajama_c4/1.0.0)
I0512 23:42:03.810293 140095893518336 dataset_builder.py:421] Reusing dataset redpajama_c4 (gs://rosinality-tpu-bucket/redpajama_c4/1.0.0)
I0512 23:42:03.791078 140311617681408 logging_logger.py:49] Constructing tf.data.Dataset redpajama_c4 for split train[:-2000], from gs://rosinality-tpu-bucket/redpajama_c4/1.0.0
I0512 23:42:03.818575 140440593786880 dataset_builder.py:421] Reusing dataset redpajama_c4 (gs://rosinality-tpu-bucket/redpajama_c4/1.0.0)
I0512 23:42:03.819864 140339465779200 logging_logger.py:49] Constructing tf.data.Dataset redpajama_c4 for split train[:-2000], from gs://rosinality-tpu-bucket/redpajama_c4/1.0.0
I0512 23:42:03.838642 140031916288000 dataset_builder.py:421] Reusing dataset redpajama_c4 (gs://rosinality-tpu-bucket/redpajama_c4/1.0.0)
I0512 23:42:03.848997 140595318249472 dataset_builder.py:421] Reusing dataset redpajama_c4 (gs://rosinality-tpu-bucket/redpajama_c4/1.0.0)
I0512 23:42:03.849633 140529434355712 dataset_builder.py:421] Reusing dataset redpajama_c4 (gs://rosinality-tpu-bucket/redpajama_c4/1.0.0)
I0512 23:42:03.876890 139674595104768 logging_logger.py:49] Constructing tf.data.Dataset redpajama_c4 for split train[:-2000], from gs://rosinality-tpu-bucket/redpajama_c4/1.0.0
I0512 23:42:03.884132 140095893518336 logging_logger.py:49] Constructing tf.data.Dataset redpajama_c4 for split train[:-2000], from gs://rosinality-tpu-bucket/redpajama_c4/1.0.0
I0512 23:42:03.890395 140440593786880 logging_logger.py:49] Constructing tf.data.Dataset redpajama_c4 for split train[:-2000], from gs://rosinality-tpu-bucket/redpajama_c4/1.0.0
I0512 23:42:03.913462 140031916288000 logging_logger.py:49] Constructing tf.data.Dataset redpajama_c4 for split train[:-2000], from gs://rosinality-tpu-bucket/redpajama_c4/1.0.0
I0512 23:42:03.921828 140595318249472 logging_logger.py:49] Constructing tf.data.Dataset redpajama_c4 for split train[:-2000], from gs://rosinality-tpu-bucket/redpajama_c4/1.0.0
I0512 23:42:03.924210 140529434355712 logging_logger.py:49] Constructing tf.data.Dataset redpajama_c4 for split train[:-2000], from gs://rosinality-tpu-bucket/redpajama_c4/1.0.0
I0512 23:42:04.238569 140339465779200 dataset_info.py:540] Load dataset info from gs://rosinality-tpu-bucket/redpajama_common_crawl/1.0.0
I0512 23:42:04.250906 140311617681408 dataset_info.py:540] Load dataset info from gs://rosinality-tpu-bucket/redpajama_common_crawl/1.0.0
I0512 23:42:04.323515 139674595104768 dataset_info.py:540] Load dataset info from gs://rosinality-tpu-bucket/redpajama_common_crawl/1.0.0
I0512 23:42:04.331112 140095893518336 dataset_info.py:540] Load dataset info from gs://rosinality-tpu-bucket/redpajama_common_crawl/1.0.0
I0512 23:42:04.337456 140595318249472 dataset_info.py:540] Load dataset info from gs://rosinality-tpu-bucket/redpajama_common_crawl/1.0.0
I0512 23:42:04.362143 140440593786880 dataset_info.py:540] Load dataset info from gs://rosinality-tpu-bucket/redpajama_common_crawl/1.0.0
I0512 23:42:04.356756 140529434355712 dataset_info.py:540] Load dataset info from gs://rosinality-tpu-bucket/redpajama_common_crawl/1.0.0
I0512 23:42:04.409212 140031916288000 dataset_info.py:540] Load dataset info from gs://rosinality-tpu-bucket/redpajama_common_crawl/1.0.0
I0512 23:42:04.531114 140339465779200 dataset_providers.py:1597] Sharding at the data source: 7 of 8
I0512 23:42:04.587798 139674595104768 dataset_providers.py:1597] Sharding at the data source: 2 of 8
I0512 23:42:04.572641 140311617681408 dataset_providers.py:1597] Sharding at the data source: 3 of 8
I0512 23:42:04.593982 140339465779200 dataset_info.py:540] Load dataset info from gs://rosinality-tpu-bucket/redpajama_common_crawl/1.0.0
I0512 23:42:04.612024 140595318249472 dataset_providers.py:1597] Sharding at the data source: 5 of 8
I0512 23:42:04.628394 140095893518336 dataset_providers.py:1597] Sharding at the data source: 6 of 8
I0512 23:42:04.634420 140440593786880 dataset_providers.py:1597] Sharding at the data source: 8 of 8
I0512 23:42:04.648195 139674595104768 dataset_info.py:540] Load dataset info from gs://rosinality-tpu-bucket/redpajama_common_crawl/1.0.0
I0512 23:42:04.633578 140529434355712 dataset_providers.py:1597] Sharding at the data source: 1 of 8
I0512 23:42:04.643468 140311617681408 dataset_info.py:540] Load dataset info from gs://rosinality-tpu-bucket/redpajama_common_crawl/1.0.0
I0512 23:42:04.672826 140031916288000 dataset_providers.py:1597] Sharding at the data source: 4 of 8
I0512 23:42:04.685240 140095893518336 dataset_info.py:540] Load dataset info from gs://rosinality-tpu-bucket/redpajama_common_crawl/1.0.0
I0512 23:42:04.684912 140595318249472 dataset_info.py:540] Load dataset info from gs://rosinality-tpu-bucket/redpajama_common_crawl/1.0.0
I0512 23:42:04.694015 140440593786880 dataset_info.py:540] Load dataset info from gs://rosinality-tpu-bucket/redpajama_common_crawl/1.0.0
I0512 23:42:04.704093 140529434355712 dataset_info.py:540] Load dataset info from gs://rosinality-tpu-bucket/redpajama_common_crawl/1.0.0
I0512 23:42:04.776497 140031916288000 dataset_info.py:540] Load dataset info from gs://rosinality-tpu-bucket/redpajama_common_crawl/1.0.0
I0512 23:42:04.881770 140339465779200 dataset_builder.py:421] Reusing dataset redpajama_common_crawl (gs://rosinality-tpu-bucket/redpajama_common_crawl/1.0.0)
I0512 23:42:04.905004 139674595104768 dataset_builder.py:421] Reusing dataset redpajama_common_crawl (gs://rosinality-tpu-bucket/redpajama_common_crawl/1.0.0)
I0512 23:42:04.935044 140311617681408 dataset_builder.py:421] Reusing dataset redpajama_common_crawl (gs://rosinality-tpu-bucket/redpajama_common_crawl/1.0.0)
I0512 23:42:04.974090 140095893518336 dataset_builder.py:421] Reusing dataset redpajama_common_crawl (gs://rosinality-tpu-bucket/redpajama_common_crawl/1.0.0)
I0512 23:42:04.973465 140339465779200 logging_logger.py:49] Constructing tf.data.Dataset redpajama_common_crawl for split train, from gs://rosinality-tpu-bucket/redpajama_common_crawl/1.0.0
I0512 23:42:04.984586 140440593786880 dataset_builder.py:421] Reusing dataset redpajama_common_crawl (gs://rosinality-tpu-bucket/redpajama_common_crawl/1.0.0)
I0512 23:42:04.998592 139674595104768 logging_logger.py:49] Constructing tf.data.Dataset redpajama_common_crawl for split train, from gs://rosinality-tpu-bucket/redpajama_common_crawl/1.0.0
I0512 23:42:05.013451 140595318249472 dataset_builder.py:421] Reusing dataset redpajama_common_crawl (gs://rosinality-tpu-bucket/redpajama_common_crawl/1.0.0)
I0512 23:42:05.003073 140529434355712 dataset_builder.py:421] Reusing dataset redpajama_common_crawl (gs://rosinality-tpu-bucket/redpajama_common_crawl/1.0.0)
I0512 23:42:05.028706 140311617681408 logging_logger.py:49] Constructing tf.data.Dataset redpajama_common_crawl for split train, from gs://rosinality-tpu-bucket/redpajama_common_crawl/1.0.0
I0512 23:42:05.070542 140095893518336 logging_logger.py:49] Constructing tf.data.Dataset redpajama_common_crawl for split train, from gs://rosinality-tpu-bucket/redpajama_common_crawl/1.0.0
I0512 23:42:05.086510 140440593786880 logging_logger.py:49] Constructing tf.data.Dataset redpajama_common_crawl for split train, from gs://rosinality-tpu-bucket/redpajama_common_crawl/1.0.0
I0512 23:42:05.104848 140595318249472 logging_logger.py:49] Constructing tf.data.Dataset redpajama_common_crawl for split train, from gs://rosinality-tpu-bucket/redpajama_common_crawl/1.0.0
I0512 23:42:05.110189 140031916288000 dataset_builder.py:421] Reusing dataset redpajama_common_crawl (gs://rosinality-tpu-bucket/redpajama_common_crawl/1.0.0)
I0512 23:42:05.104763 140529434355712 logging_logger.py:49] Constructing tf.data.Dataset redpajama_common_crawl for split train, from gs://rosinality-tpu-bucket/redpajama_common_crawl/1.0.0
I0512 23:42:05.208544 140031916288000 logging_logger.py:49] Constructing tf.data.Dataset redpajama_common_crawl for split train, from gs://rosinality-tpu-bucket/redpajama_common_crawl/1.0.0
I0512 23:42:05.362767 140339465779200 dataset_info.py:540] Load dataset info from gs://rosinality-tpu-bucket/redpajama_github/1.0.0
I0512 23:42:05.418383 139674595104768 dataset_info.py:540] Load dataset info from gs://rosinality-tpu-bucket/redpajama_github/1.0.0
I0512 23:42:05.419233 140311617681408 dataset_info.py:540] Load dataset info from gs://rosinality-tpu-bucket/redpajama_github/1.0.0
I0512 23:42:05.468835 140440593786880 dataset_info.py:540] Load dataset info from gs://rosinality-tpu-bucket/redpajama_github/1.0.0
I0512 23:42:05.491971 140095893518336 dataset_info.py:540] Load dataset info from gs://rosinality-tpu-bucket/redpajama_github/1.0.0
I0512 23:42:05.528627 140595318249472 dataset_info.py:540] Load dataset info from gs://rosinality-tpu-bucket/redpajama_github/1.0.0
I0512 23:42:05.542639 140529434355712 dataset_info.py:540] Load dataset info from gs://rosinality-tpu-bucket/redpajama_github/1.0.0
I0512 23:42:05.568903 140339465779200 dataset_providers.py:1597] Sharding at the data source: 7 of 8
I0512 23:42:05.612135 140031916288000 dataset_info.py:540] Load dataset info from gs://rosinality-tpu-bucket/redpajama_github/1.0.0
I0512 23:42:05.608813 140311617681408 dataset_providers.py:1597] Sharding at the data source: 3 of 8
I0512 23:42:05.641311 139674595104768 dataset_providers.py:1597] Sharding at the data source: 2 of 8
I0512 23:42:05.640694 140339465779200 dataset_info.py:540] Load dataset info from gs://rosinality-tpu-bucket/redpajama_github/1.0.0
I0512 23:42:05.676518 140311617681408 dataset_info.py:540] Load dataset info from gs://rosinality-tpu-bucket/redpajama_github/1.0.0
I0512 23:42:05.698642 139674595104768 dataset_info.py:540] Load dataset info from gs://rosinality-tpu-bucket/redpajama_github/1.0.0
I0512 23:42:05.703267 140095893518336 dataset_providers.py:1597] Sharding at the data source: 6 of 8
I0512 23:42:05.715382 140440593786880 dataset_providers.py:1597] Sharding at the data source: 8 of 8
I0512 23:42:05.748502 140595318249472 dataset_providers.py:1597] Sharding at the data source: 5 of 8
I0512 23:42:05.748321 140529434355712 dataset_providers.py:1597] Sharding at the data source: 1 of 8
I0512 23:42:05.779487 140440593786880 dataset_info.py:540] Load dataset info from gs://rosinality-tpu-bucket/redpajama_github/1.0.0
I0512 23:42:05.784854 140095893518336 dataset_info.py:540] Load dataset info from gs://rosinality-tpu-bucket/redpajama_github/1.0.0
I0512 23:42:05.803626 140031916288000 dataset_providers.py:1597] Sharding at the data source: 4 of 8
I0512 23:42:05.806612 140595318249472 dataset_info.py:540] Load dataset info from gs://rosinality-tpu-bucket/redpajama_github/1.0.0
I0512 23:42:05.807220 140529434355712 dataset_info.py:540] Load dataset info from gs://rosinality-tpu-bucket/redpajama_github/1.0.0
I0512 23:42:05.839001 140339465779200 dataset_builder.py:421] Reusing dataset redpajama_github (gs://rosinality-tpu-bucket/redpajama_github/1.0.0)
I0512 23:42:05.865174 140031916288000 dataset_info.py:540] Load dataset info from gs://rosinality-tpu-bucket/redpajama_github/1.0.0
I0512 23:42:05.888571 140311617681408 dataset_builder.py:421] Reusing dataset redpajama_github (gs://rosinality-tpu-bucket/redpajama_github/1.0.0)
I0512 23:42:05.911942 140339465779200 logging_logger.py:49] Constructing tf.data.Dataset redpajama_github for split train, from gs://rosinality-tpu-bucket/redpajama_github/1.0.0
I0512 23:42:05.938201 139674595104768 dataset_builder.py:421] Reusing dataset redpajama_github (gs://rosinality-tpu-bucket/redpajama_github/1.0.0)
I0512 23:42:05.959014 140311617681408 logging_logger.py:49] Constructing tf.data.Dataset redpajama_github for split train, from gs://rosinality-tpu-bucket/redpajama_github/1.0.0
I0512 23:42:06.005925 140440593786880 dataset_builder.py:421] Reusing dataset redpajama_github (gs://rosinality-tpu-bucket/redpajama_github/1.0.0)
I0512 23:42:06.014748 139674595104768 logging_logger.py:49] Constructing tf.data.Dataset redpajama_github for split train, from gs://rosinality-tpu-bucket/redpajama_github/1.0.0
I0512 23:42:06.021341 140595318249472 dataset_builder.py:421] Reusing dataset redpajama_github (gs://rosinality-tpu-bucket/redpajama_github/1.0.0)
I0512 23:42:06.026187 140095893518336 dataset_builder.py:421] Reusing dataset redpajama_github (gs://rosinality-tpu-bucket/redpajama_github/1.0.0)
I0512 23:42:06.074571 140031916288000 dataset_builder.py:421] Reusing dataset redpajama_github (gs://rosinality-tpu-bucket/redpajama_github/1.0.0)
I0512 23:42:06.075495 140440593786880 logging_logger.py:49] Constructing tf.data.Dataset redpajama_github for split train, from gs://rosinality-tpu-bucket/redpajama_github/1.0.0
I0512 23:42:06.092646 140595318249472 logging_logger.py:49] Constructing tf.data.Dataset redpajama_github for split train, from gs://rosinality-tpu-bucket/redpajama_github/1.0.0
I0512 23:42:06.097027 140095893518336 logging_logger.py:49] Constructing tf.data.Dataset redpajama_github for split train, from gs://rosinality-tpu-bucket/redpajama_github/1.0.0
I0512 23:42:06.082745 140529434355712 dataset_builder.py:421] Reusing dataset redpajama_github (gs://rosinality-tpu-bucket/redpajama_github/1.0.0)
I0512 23:42:06.145335 140031916288000 logging_logger.py:49] Constructing tf.data.Dataset redpajama_github for split train, from gs://rosinality-tpu-bucket/redpajama_github/1.0.0
I0512 23:42:06.155402 140529434355712 logging_logger.py:49] Constructing tf.data.Dataset redpajama_github for split train, from gs://rosinality-tpu-bucket/redpajama_github/1.0.0
I0512 23:42:06.357738 140311617681408 dataset_info.py:540] Load dataset info from gs://rosinality-tpu-bucket/redpajama_stackexchange/1.0.0
I0512 23:42:06.399984 140339465779200 dataset_info.py:540] Load dataset info from gs://rosinality-tpu-bucket/redpajama_stackexchange/1.0.0
I0512 23:42:06.405576 139674595104768 dataset_info.py:540] Load dataset info from gs://rosinality-tpu-bucket/redpajama_stackexchange/1.0.0
I0512 23:42:06.472360 140440593786880 dataset_info.py:540] Load dataset info from gs://rosinality-tpu-bucket/redpajama_stackexchange/1.0.0
I0512 23:42:06.490749 140095893518336 dataset_info.py:540] Load dataset info from gs://rosinality-tpu-bucket/redpajama_stackexchange/1.0.0
I0512 23:42:06.499034 140595318249472 dataset_info.py:540] Load dataset info from gs://rosinality-tpu-bucket/redpajama_stackexchange/1.0.0
I0512 23:42:06.563530 140031916288000 dataset_info.py:540] Load dataset info from gs://rosinality-tpu-bucket/redpajama_stackexchange/1.0.0
I0512 23:42:06.580637 140529434355712 dataset_info.py:540] Load dataset info from gs://rosinality-tpu-bucket/redpajama_stackexchange/1.0.0
I0512 23:42:06.622068 139674595104768 dataset_providers.py:1597] Sharding at the data source: 2 of 8
I0512 23:42:06.624189 140339465779200 dataset_providers.py:1597] Sharding at the data source: 7 of 8
I0512 23:42:06.637141 140311617681408 dataset_providers.py:1597] Sharding at the data source: 3 of 8
I0512 23:42:06.678595 140339465779200 dataset_info.py:540] Load dataset info from gs://rosinality-tpu-bucket/redpajama_stackexchange/1.0.0
I0512 23:42:06.684016 139674595104768 dataset_info.py:540] Load dataset info from gs://rosinality-tpu-bucket/redpajama_stackexchange/1.0.0
I0512 23:42:06.707847 140095893518336 dataset_providers.py:1597] Sharding at the data source: 6 of 8
I0512 23:42:06.706240 140595318249472 dataset_providers.py:1597] Sharding at the data source: 5 of 8
I0512 23:42:06.695506 140311617681408 dataset_info.py:540] Load dataset info from gs://rosinality-tpu-bucket/redpajama_stackexchange/1.0.0
I0512 23:42:06.731741 140440593786880 dataset_providers.py:1597] Sharding at the data source: 8 of 8
I0512 23:42:06.763600 140095893518336 dataset_info.py:540] Load dataset info from gs://rosinality-tpu-bucket/redpajama_stackexchange/1.0.0
I0512 23:42:06.768611 140595318249472 dataset_info.py:540] Load dataset info from gs://rosinality-tpu-bucket/redpajama_stackexchange/1.0.0
I0512 23:42:06.792063 140031916288000 dataset_providers.py:1597] Sharding at the data source: 4 of 8
I0512 23:42:06.792251 140529434355712 dataset_providers.py:1597] Sharding at the data source: 1 of 8
I0512 23:42:06.830008 140440593786880 dataset_info.py:540] Load dataset info from gs://rosinality-tpu-bucket/redpajama_stackexchange/1.0.0
I0512 23:42:06.851076 140031916288000 dataset_info.py:540] Load dataset info from gs://rosinality-tpu-bucket/redpajama_stackexchange/1.0.0
I0512 23:42:06.850515 140529434355712 dataset_info.py:540] Load dataset info from gs://rosinality-tpu-bucket/redpajama_stackexchange/1.0.0
I0512 23:42:06.891232 140339465779200 dataset_builder.py:421] Reusing dataset redpajama_stackexchange (gs://rosinality-tpu-bucket/redpajama_stackexchange/1.0.0)
I0512 23:42:06.914689 139674595104768 dataset_builder.py:421] Reusing dataset redpajama_stackexchange (gs://rosinality-tpu-bucket/redpajama_stackexchange/1.0.0)
I0512 23:42:06.928405 140311617681408 dataset_builder.py:421] Reusing dataset redpajama_stackexchange (gs://rosinality-tpu-bucket/redpajama_stackexchange/1.0.0)
I0512 23:42:06.965877 140339465779200 logging_logger.py:49] Constructing tf.data.Dataset redpajama_stackexchange for split train, from gs://rosinality-tpu-bucket/redpajama_stackexchange/1.0.0
I0512 23:42:06.986198 139674595104768 logging_logger.py:49] Constructing tf.data.Dataset redpajama_stackexchange for split train, from gs://rosinality-tpu-bucket/redpajama_stackexchange/1.0.0
I0512 23:42:06.994069 140595318249472 dataset_builder.py:421] Reusing dataset redpajama_stackexchange (gs://rosinality-tpu-bucket/redpajama_stackexchange/1.0.0)
I0512 23:42:07.002021 140095893518336 dataset_builder.py:421] Reusing dataset redpajama_stackexchange (gs://rosinality-tpu-bucket/redpajama_stackexchange/1.0.0)
I0512 23:42:06.997496 140311617681408 logging_logger.py:49] Constructing tf.data.Dataset redpajama_stackexchange for split train, from gs://rosinality-tpu-bucket/redpajama_stackexchange/1.0.0
I0512 23:42:07.034901 140440593786880 dataset_builder.py:421] Reusing dataset redpajama_stackexchange (gs://rosinality-tpu-bucket/redpajama_stackexchange/1.0.0)
I0512 23:42:07.063911 140031916288000 dataset_builder.py:421] Reusing dataset redpajama_stackexchange (gs://rosinality-tpu-bucket/redpajama_stackexchange/1.0.0)
I0512 23:42:07.068159 140595318249472 logging_logger.py:49] Constructing tf.data.Dataset redpajama_stackexchange for split train, from gs://rosinality-tpu-bucket/redpajama_stackexchange/1.0.0
I0512 23:42:07.075397 140095893518336 logging_logger.py:49] Constructing tf.data.Dataset redpajama_stackexchange for split train, from gs://rosinality-tpu-bucket/redpajama_stackexchange/1.0.0
I0512 23:42:07.076859 140529434355712 dataset_builder.py:421] Reusing dataset redpajama_stackexchange (gs://rosinality-tpu-bucket/redpajama_stackexchange/1.0.0)
I0512 23:42:07.107152 140440593786880 logging_logger.py:49] Constructing tf.data.Dataset redpajama_stackexchange for split train, from gs://rosinality-tpu-bucket/redpajama_stackexchange/1.0.0
I0512 23:42:07.134024 140031916288000 logging_logger.py:49] Constructing tf.data.Dataset redpajama_stackexchange for split train, from gs://rosinality-tpu-bucket/redpajama_stackexchange/1.0.0
I0512 23:42:07.148267 140529434355712 logging_logger.py:49] Constructing tf.data.Dataset redpajama_stackexchange for split train, from gs://rosinality-tpu-bucket/redpajama_stackexchange/1.0.0
I0512 23:42:07.352911 140339465779200 dataset_info.py:540] Load dataset info from gs://rosinality-tpu-bucket/redpajama_wikipedia/1.0.0
I0512 23:42:07.394694 139674595104768 dataset_info.py:540] Load dataset info from gs://rosinality-tpu-bucket/redpajama_wikipedia/1.0.0
I0512 23:42:07.386626 140311617681408 dataset_info.py:540] Load dataset info from gs://rosinality-tpu-bucket/redpajama_wikipedia/1.0.0
I0512 23:42:07.473774 140095893518336 dataset_info.py:540] Load dataset info from gs://rosinality-tpu-bucket/redpajama_wikipedia/1.0.0
I0512 23:42:07.474150 140595318249472 dataset_info.py:540] Load dataset info from gs://rosinality-tpu-bucket/redpajama_wikipedia/1.0.0
I0512 23:42:07.486162 140440593786880 dataset_info.py:540] Load dataset info from gs://rosinality-tpu-bucket/redpajama_wikipedia/1.0.0
I0512 23:42:07.573266 140031916288000 dataset_info.py:540] Load dataset info from gs://rosinality-tpu-bucket/redpajama_wikipedia/1.0.0
I0512 23:42:07.575100 140529434355712 dataset_info.py:540] Load dataset info from gs://rosinality-tpu-bucket/redpajama_wikipedia/1.0.0
I0512 23:42:07.594918 140339465779200 dataset_providers.py:1597] Sharding at the data source: 7 of 8
I0512 23:42:07.604486 139674595104768 dataset_providers.py:1597] Sharding at the data source: 2 of 8
I0512 23:42:07.626858 140311617681408 dataset_providers.py:1597] Sharding at the data source: 3 of 8
I0512 23:42:07.656213 140339465779200 dataset_info.py:540] Load dataset info from gs://rosinality-tpu-bucket/redpajama_wikipedia/1.0.0
I0512 23:42:07.660533 139674595104768 dataset_info.py:540] Load dataset info from gs://rosinality-tpu-bucket/redpajama_wikipedia/1.0.0
I0512 23:42:07.690568 140595318249472 dataset_providers.py:1597] Sharding at the data source: 5 of 8
I0512 23:42:07.695997 140095893518336 dataset_providers.py:1597] Sharding at the data source: 6 of 8
I0512 23:42:07.695406 140440593786880 dataset_providers.py:1597] Sharding at the data source: 8 of 8
I0512 23:42:07.683342 140311617681408 dataset_info.py:540] Load dataset info from gs://rosinality-tpu-bucket/redpajama_wikipedia/1.0.0
I0512 23:42:07.759534 140095893518336 dataset_info.py:540] Load dataset info from gs://rosinality-tpu-bucket/redpajama_wikipedia/1.0.0
I0512 23:42:07.766815 140595318249472 dataset_info.py:540] Load dataset info from gs://rosinality-tpu-bucket/redpajama_wikipedia/1.0.0
I0512 23:42:07.770270 140440593786880 dataset_info.py:540] Load dataset info from gs://rosinality-tpu-bucket/redpajama_wikipedia/1.0.0
I0512 23:42:07.770215 140529434355712 dataset_providers.py:1597] Sharding at the data source: 1 of 8
I0512 23:42:07.789589 140031916288000 dataset_providers.py:1597] Sharding at the data source: 4 of 8
I0512 23:42:07.846890 140031916288000 dataset_info.py:540] Load dataset info from gs://rosinality-tpu-bucket/redpajama_wikipedia/1.0.0
I0512 23:42:07.829644 140529434355712 dataset_info.py:540] Load dataset info from gs://rosinality-tpu-bucket/redpajama_wikipedia/1.0.0
I0512 23:42:07.851814 140339465779200 dataset_builder.py:421] Reusing dataset redpajama_wikipedia (gs://rosinality-tpu-bucket/redpajama_wikipedia/1.0.0)
I0512 23:42:07.858935 139674595104768 dataset_builder.py:421] Reusing dataset redpajama_wikipedia (gs://rosinality-tpu-bucket/redpajama_wikipedia/1.0.0)
I0512 23:42:07.892122 140311617681408 dataset_builder.py:421] Reusing dataset redpajama_wikipedia (gs://rosinality-tpu-bucket/redpajama_wikipedia/1.0.0)
I0512 23:42:07.929296 139674595104768 logging_logger.py:49] Constructing tf.data.Dataset redpajama_wikipedia for split train, from gs://rosinality-tpu-bucket/redpajama_wikipedia/1.0.0
I0512 23:42:07.929427 140339465779200 logging_logger.py:49] Constructing tf.data.Dataset redpajama_wikipedia for split train, from gs://rosinality-tpu-bucket/redpajama_wikipedia/1.0.0
I0512 23:42:07.973981 140595318249472 dataset_builder.py:421] Reusing dataset redpajama_wikipedia (gs://rosinality-tpu-bucket/redpajama_wikipedia/1.0.0)
I0512 23:42:07.966435 140311617681408 logging_logger.py:49] Constructing tf.data.Dataset redpajama_wikipedia for split train, from gs://rosinality-tpu-bucket/redpajama_wikipedia/1.0.0
I0512 23:42:07.994848 140440593786880 dataset_builder.py:421] Reusing dataset redpajama_wikipedia (gs://rosinality-tpu-bucket/redpajama_wikipedia/1.0.0)
I0512 23:42:08.003374 140095893518336 dataset_builder.py:421] Reusing dataset redpajama_wikipedia (gs://rosinality-tpu-bucket/redpajama_wikipedia/1.0.0)
I0512 23:42:08.039570 140031916288000 dataset_builder.py:421] Reusing dataset redpajama_wikipedia (gs://rosinality-tpu-bucket/redpajama_wikipedia/1.0.0)
I0512 23:42:08.042908 140595318249472 logging_logger.py:49] Constructing tf.data.Dataset redpajama_wikipedia for split train, from gs://rosinality-tpu-bucket/redpajama_wikipedia/1.0.0
I0512 23:42:08.028384 140529434355712 dataset_builder.py:421] Reusing dataset redpajama_wikipedia (gs://rosinality-tpu-bucket/redpajama_wikipedia/1.0.0)
I0512 23:42:08.065058 140440593786880 logging_logger.py:49] Constructing tf.data.Dataset redpajama_wikipedia for split train, from gs://rosinality-tpu-bucket/redpajama_wikipedia/1.0.0
I0512 23:42:08.077427 140095893518336 logging_logger.py:49] Constructing tf.data.Dataset redpajama_wikipedia for split train, from gs://rosinality-tpu-bucket/redpajama_wikipedia/1.0.0
I0512 23:42:08.109761 140031916288000 logging_logger.py:49] Constructing tf.data.Dataset redpajama_wikipedia for split train, from gs://rosinality-tpu-bucket/redpajama_wikipedia/1.0.0
I0512 23:42:08.104376 140529434355712 logging_logger.py:49] Constructing tf.data.Dataset redpajama_wikipedia for split train, from gs://rosinality-tpu-bucket/redpajama_wikipedia/1.0.0
I0512 23:42:08.315241 140339465779200 dataset_info.py:540] Load dataset info from gs://rosinality-tpu-bucket/the_stack_dedup/1.0.0
I0512 23:42:08.346865 139674595104768 dataset_info.py:540] Load dataset info from gs://rosinality-tpu-bucket/the_stack_dedup/1.0.0
I0512 23:42:08.349152 140311617681408 dataset_info.py:540] Load dataset info from gs://rosinality-tpu-bucket/the_stack_dedup/1.0.0
I0512 23:42:08.425972 140595318249472 dataset_info.py:540] Load dataset info from gs://rosinality-tpu-bucket/the_stack_dedup/1.0.0
I0512 23:42:08.450344 140440593786880 dataset_info.py:540] Load dataset info from gs://rosinality-tpu-bucket/the_stack_dedup/1.0.0
I0512 23:42:08.464451 140095893518336 dataset_info.py:540] Load dataset info from gs://rosinality-tpu-bucket/the_stack_dedup/1.0.0
I0512 23:42:08.519408 140529434355712 dataset_info.py:540] Load dataset info from gs://rosinality-tpu-bucket/the_stack_dedup/1.0.0
I0512 23:42:08.539533 140031916288000 dataset_info.py:540] Load dataset info from gs://rosinality-tpu-bucket/the_stack_dedup/1.0.0
I0512 23:42:08.665100 140339465779200 dataset_providers.py:1597] Sharding at the data source: 7 of 8
I0512 23:42:08.691798 139674595104768 dataset_providers.py:1597] Sharding at the data source: 2 of 8
I0512 23:42:08.690739 140311617681408 dataset_providers.py:1597] Sharding at the data source: 3 of 8
I0512 23:42:08.727456 140339465779200 dataset_info.py:540] Load dataset info from gs://rosinality-tpu-bucket/the_stack_dedup/1.0.0
I0512 23:42:08.749849 139674595104768 dataset_info.py:540] Load dataset info from gs://rosinality-tpu-bucket/the_stack_dedup/1.0.0
I0512 23:42:08.784848 140440593786880 dataset_providers.py:1597] Sharding at the data source: 8 of 8
I0512 23:42:08.792291 140311617681408 dataset_info.py:540] Load dataset info from gs://rosinality-tpu-bucket/the_stack_dedup/1.0.0
I0512 23:42:08.812697 140595318249472 dataset_providers.py:1597] Sharding at the data source: 5 of 8
I0512 23:42:08.823418 140095893518336 dataset_providers.py:1597] Sharding at the data source: 6 of 8
I0512 23:42:08.844540 140440593786880 dataset_info.py:540] Load dataset info from gs://rosinality-tpu-bucket/the_stack_dedup/1.0.0
I0512 23:42:08.887726 140095893518336 dataset_info.py:540] Load dataset info from gs://rosinality-tpu-bucket/the_stack_dedup/1.0.0
I0512 23:42:08.871001 140529434355712 dataset_providers.py:1597] Sharding at the data source: 1 of 8
I0512 23:42:08.890807 140031916288000 dataset_providers.py:1597] Sharding at the data source: 4 of 8
I0512 23:42:08.900384 140595318249472 dataset_info.py:540] Load dataset info from gs://rosinality-tpu-bucket/the_stack_dedup/1.0.0
I0512 23:42:08.951577 140031916288000 dataset_info.py:540] Load dataset info from gs://rosinality-tpu-bucket/the_stack_dedup/1.0.0
I0512 23:42:08.934896 140529434355712 dataset_info.py:540] Load dataset info from gs://rosinality-tpu-bucket/the_stack_dedup/1.0.0
I0512 23:42:09.034593 140339465779200 dataset_builder.py:421] Reusing dataset the_stack_dedup (gs://rosinality-tpu-bucket/the_stack_dedup/1.0.0)
I0512 23:42:09.062328 139674595104768 dataset_builder.py:421] Reusing dataset the_stack_dedup (gs://rosinality-tpu-bucket/the_stack_dedup/1.0.0)
I0512 23:42:09.147678 140440593786880 dataset_builder.py:421] Reusing dataset the_stack_dedup (gs://rosinality-tpu-bucket/the_stack_dedup/1.0.0)
I0512 23:42:09.147204 140339465779200 logging_logger.py:49] Constructing tf.data.Dataset the_stack_dedup for split train, from gs://rosinality-tpu-bucket/the_stack_dedup/1.0.0
I0512 23:42:09.170988 139674595104768 logging_logger.py:49] Constructing tf.data.Dataset the_stack_dedup for split train, from gs://rosinality-tpu-bucket/the_stack_dedup/1.0.0
I0512 23:42:09.155513 140311617681408 dataset_builder.py:421] Reusing dataset the_stack_dedup (gs://rosinality-tpu-bucket/the_stack_dedup/1.0.0)
I0512 23:42:09.193026 140595318249472 dataset_builder.py:421] Reusing dataset the_stack_dedup (gs://rosinality-tpu-bucket/the_stack_dedup/1.0.0)
I0512 23:42:09.203116 140095893518336 dataset_builder.py:421] Reusing dataset the_stack_dedup (gs://rosinality-tpu-bucket/the_stack_dedup/1.0.0)
I0512 23:42:09.258908 140440593786880 logging_logger.py:49] Constructing tf.data.Dataset the_stack_dedup for split train, from gs://rosinality-tpu-bucket/the_stack_dedup/1.0.0
I0512 23:42:09.264966 140529434355712 dataset_builder.py:421] Reusing dataset the_stack_dedup (gs://rosinality-tpu-bucket/the_stack_dedup/1.0.0)
I0512 23:42:09.283203 140031916288000 dataset_builder.py:421] Reusing dataset the_stack_dedup (gs://rosinality-tpu-bucket/the_stack_dedup/1.0.0)
I0512 23:42:09.273903 140311617681408 logging_logger.py:49] Constructing tf.data.Dataset the_stack_dedup for split train, from gs://rosinality-tpu-bucket/the_stack_dedup/1.0.0
I0512 23:42:09.303434 140595318249472 logging_logger.py:49] Constructing tf.data.Dataset the_stack_dedup for split train, from gs://rosinality-tpu-bucket/the_stack_dedup/1.0.0
I0512 23:42:09.316259 140095893518336 logging_logger.py:49] Constructing tf.data.Dataset the_stack_dedup for split train, from gs://rosinality-tpu-bucket/the_stack_dedup/1.0.0
I0512 23:42:09.393425 140031916288000 logging_logger.py:49] Constructing tf.data.Dataset the_stack_dedup for split train, from gs://rosinality-tpu-bucket/the_stack_dedup/1.0.0
I0512 23:42:09.382364 140529434355712 logging_logger.py:49] Constructing tf.data.Dataset the_stack_dedup for split train, from gs://rosinality-tpu-bucket/the_stack_dedup/1.0.0
I0512 23:42:10.890235 140339465779200 dataset_providers.py:2432] The output dataset from seqio.get_dataset has the following features
I0512 23:42:10.890403 140339465779200 dataset_providers.py:2438] feature: decoder_target_tokens 	 shape: [48, 2048] 	 dtype: int32
I0512 23:42:10.890458 140339465779200 dataset_providers.py:2438] feature: decoder_input_tokens 	 shape: [48, 2048] 	 dtype: int32
I0512 23:42:10.890498 140339465779200 dataset_providers.py:2438] feature: decoder_loss_weights 	 shape: [48, 2048] 	 dtype: int32
I0512 23:42:10.890537 140339465779200 dataset_providers.py:2438] feature: decoder_segment_ids 	 shape: [48, 2048] 	 dtype: int32
I0512 23:42:10.890575 140339465779200 dataset_providers.py:2438] feature: decoder_positions 	 shape: [48, 2048] 	 dtype: int32
I0512 23:42:10.914972 139674595104768 dataset_providers.py:2432] The output dataset from seqio.get_dataset has the following features
I0512 23:42:10.915147 139674595104768 dataset_providers.py:2438] feature: decoder_target_tokens 	 shape: [48, 2048] 	 dtype: int32
I0512 23:42:10.915200 139674595104768 dataset_providers.py:2438] feature: decoder_input_tokens 	 shape: [48, 2048] 	 dtype: int32
I0512 23:42:10.915241 139674595104768 dataset_providers.py:2438] feature: decoder_loss_weights 	 shape: [48, 2048] 	 dtype: int32
I0512 23:42:10.915280 139674595104768 dataset_providers.py:2438] feature: decoder_segment_ids 	 shape: [48, 2048] 	 dtype: int32
I0512 23:42:10.915347 139674595104768 dataset_providers.py:2438] feature: decoder_positions 	 shape: [48, 2048] 	 dtype: int32
I0512 23:42:11.002593 140440593786880 dataset_providers.py:2432] The output dataset from seqio.get_dataset has the following features
I0512 23:42:11.002741 140440593786880 dataset_providers.py:2438] feature: decoder_target_tokens 	 shape: [48, 2048] 	 dtype: int32
I0512 23:42:11.002796 140440593786880 dataset_providers.py:2438] feature: decoder_input_tokens 	 shape: [48, 2048] 	 dtype: int32
I0512 23:42:11.002839 140440593786880 dataset_providers.py:2438] feature: decoder_loss_weights 	 shape: [48, 2048] 	 dtype: int32
I0512 23:42:11.002879 140440593786880 dataset_providers.py:2438] feature: decoder_segment_ids 	 shape: [48, 2048] 	 dtype: int32
I0512 23:42:11.002929 140440593786880 dataset_providers.py:2438] feature: decoder_positions 	 shape: [48, 2048] 	 dtype: int32
I0512 23:42:11.021886 140311617681408 dataset_providers.py:2432] The output dataset from seqio.get_dataset has the following features
I0512 23:42:11.022037 140311617681408 dataset_providers.py:2438] feature: decoder_target_tokens 	 shape: [48, 2048] 	 dtype: int32
I0512 23:42:11.022091 140311617681408 dataset_providers.py:2438] feature: decoder_input_tokens 	 shape: [48, 2048] 	 dtype: int32
I0512 23:42:11.022134 140311617681408 dataset_providers.py:2438] feature: decoder_loss_weights 	 shape: [48, 2048] 	 dtype: int32
I0512 23:42:11.022173 140311617681408 dataset_providers.py:2438] feature: decoder_segment_ids 	 shape: [48, 2048] 	 dtype: int32
I0512 23:42:11.022212 140311617681408 dataset_providers.py:2438] feature: decoder_positions 	 shape: [48, 2048] 	 dtype: int32
I0512 23:42:11.070678 140095893518336 dataset_providers.py:2432] The output dataset from seqio.get_dataset has the following features
I0512 23:42:11.070850 140095893518336 dataset_providers.py:2438] feature: decoder_target_tokens 	 shape: [48, 2048] 	 dtype: int32
I0512 23:42:11.070901 140095893518336 dataset_providers.py:2438] feature: decoder_input_tokens 	 shape: [48, 2048] 	 dtype: int32
I0512 23:42:11.070940 140095893518336 dataset_providers.py:2438] feature: decoder_loss_weights 	 shape: [48, 2048] 	 dtype: int32
I0512 23:42:11.070976 140095893518336 dataset_providers.py:2438] feature: decoder_segment_ids 	 shape: [48, 2048] 	 dtype: int32
I0512 23:42:11.071011 140095893518336 dataset_providers.py:2438] feature: decoder_positions 	 shape: [48, 2048] 	 dtype: int32
I0512 23:42:11.074334 140595318249472 dataset_providers.py:2432] The output dataset from seqio.get_dataset has the following features
I0512 23:42:11.074507 140595318249472 dataset_providers.py:2438] feature: decoder_target_tokens 	 shape: [48, 2048] 	 dtype: int32
I0512 23:42:11.074558 140595318249472 dataset_providers.py:2438] feature: decoder_input_tokens 	 shape: [48, 2048] 	 dtype: int32
I0512 23:42:11.074598 140595318249472 dataset_providers.py:2438] feature: decoder_loss_weights 	 shape: [48, 2048] 	 dtype: int32
I0512 23:42:11.074635 140595318249472 dataset_providers.py:2438] feature: decoder_segment_ids 	 shape: [48, 2048] 	 dtype: int32
I0512 23:42:11.074670 140595318249472 dataset_providers.py:2438] feature: decoder_positions 	 shape: [48, 2048] 	 dtype: int32
I0512 23:42:11.128455 140529434355712 dataset_providers.py:2432] The output dataset from seqio.get_dataset has the following features
I0512 23:42:11.128622 140529434355712 dataset_providers.py:2438] feature: decoder_target_tokens 	 shape: [48, 2048] 	 dtype: int32
I0512 23:42:11.128675 140529434355712 dataset_providers.py:2438] feature: decoder_input_tokens 	 shape: [48, 2048] 	 dtype: int32
I0512 23:42:11.128716 140529434355712 dataset_providers.py:2438] feature: decoder_loss_weights 	 shape: [48, 2048] 	 dtype: int32
I0512 23:42:11.128754 140529434355712 dataset_providers.py:2438] feature: decoder_segment_ids 	 shape: [48, 2048] 	 dtype: int32
I0512 23:42:11.128792 140529434355712 dataset_providers.py:2438] feature: decoder_positions 	 shape: [48, 2048] 	 dtype: int32
I0512 23:42:11.148379 140031916288000 dataset_providers.py:2432] The output dataset from seqio.get_dataset has the following features
I0512 23:42:11.148540 140031916288000 dataset_providers.py:2438] feature: decoder_target_tokens 	 shape: [48, 2048] 	 dtype: int32
I0512 23:42:11.148592 140031916288000 dataset_providers.py:2438] feature: decoder_input_tokens 	 shape: [48, 2048] 	 dtype: int32
I0512 23:42:11.148642 140031916288000 dataset_providers.py:2438] feature: decoder_loss_weights 	 shape: [48, 2048] 	 dtype: int32
I0512 23:42:11.148680 140031916288000 dataset_providers.py:2438] feature: decoder_segment_ids 	 shape: [48, 2048] 	 dtype: int32
I0512 23:42:11.148724 140031916288000 dataset_providers.py:2438] feature: decoder_positions 	 shape: [48, 2048] 	 dtype: int32
W0512 23:42:11.844262 140339465779200 activation_partitioning.py:145] In /home/rosinality/t5x/flaxformer/flaxformer/architectures/t5/t5_architecture.py:520, activation_partitioning_dims was set, but it is deprecated and will be removed soon.
W0512 23:42:11.851405 140339465779200 activation_partitioning.py:145] In /home/rosinality/t5x/flaxformer/flaxformer/architectures/t5/t5_architecture.py:586, activation_partitioning_dims was set, but it is deprecated and will be removed soon.
W0512 23:42:11.866870 139674595104768 activation_partitioning.py:145] In /home/rosinality/t5x/flaxformer/flaxformer/architectures/t5/t5_architecture.py:520, activation_partitioning_dims was set, but it is deprecated and will be removed soon.
W0512 23:42:11.874069 139674595104768 activation_partitioning.py:145] In /home/rosinality/t5x/flaxformer/flaxformer/architectures/t5/t5_architecture.py:586, activation_partitioning_dims was set, but it is deprecated and will be removed soon.
W0512 23:42:11.904312 140339465779200 activation_partitioning.py:145] In /home/rosinality/t5x/flaxformer/flaxformer/architectures/t5/t5_architecture.py:623, activation_partitioning_dims was set, but it is deprecated and will be removed soon.
W0512 23:42:11.907499 140339465779200 activation_partitioning.py:145] In /home/rosinality/t5x/flaxformer/flaxformer/architectures/t5/t5_architecture.py:663, activation_partitioning_dims was set, but it is deprecated and will be removed soon.
W0512 23:42:11.926918 139674595104768 activation_partitioning.py:145] In /home/rosinality/t5x/flaxformer/flaxformer/architectures/t5/t5_architecture.py:623, activation_partitioning_dims was set, but it is deprecated and will be removed soon.
W0512 23:42:11.930426 139674595104768 activation_partitioning.py:145] In /home/rosinality/t5x/flaxformer/flaxformer/architectures/t5/t5_architecture.py:663, activation_partitioning_dims was set, but it is deprecated and will be removed soon.
W0512 23:42:11.947353 140440593786880 activation_partitioning.py:145] In /home/rosinality/t5x/flaxformer/flaxformer/architectures/t5/t5_architecture.py:520, activation_partitioning_dims was set, but it is deprecated and will be removed soon.
W0512 23:42:11.951791 140339465779200 activation_partitioning.py:145] In /home/rosinality/t5x/flaxformer/flaxformer/architectures/t5/t5_architecture.py:678, activation_partitioning_dims was set, but it is deprecated and will be removed soon.
W0512 23:42:11.954599 140440593786880 activation_partitioning.py:145] In /home/rosinality/t5x/flaxformer/flaxformer/architectures/t5/t5_architecture.py:586, activation_partitioning_dims was set, but it is deprecated and will be removed soon.
W0512 23:42:11.956110 140339465779200 activation_partitioning.py:145] In /home/rosinality/t5x/flaxformer/flaxformer/architectures/t5/t5_architecture.py:520, activation_partitioning_dims was set, but it is deprecated and will be removed soon.
W0512 23:42:11.959286 140339465779200 activation_partitioning.py:145] In /home/rosinality/t5x/flaxformer/flaxformer/architectures/t5/t5_architecture.py:586, activation_partitioning_dims was set, but it is deprecated and will be removed soon.
W0512 23:42:11.976789 139674595104768 activation_partitioning.py:145] In /home/rosinality/t5x/flaxformer/flaxformer/architectures/t5/t5_architecture.py:678, activation_partitioning_dims was set, but it is deprecated and will be removed soon.
W0512 23:42:11.981186 139674595104768 activation_partitioning.py:145] In /home/rosinality/t5x/flaxformer/flaxformer/architectures/t5/t5_architecture.py:520, activation_partitioning_dims was set, but it is deprecated and will be removed soon.
W0512 23:42:11.981770 140339465779200 activation_partitioning.py:145] In /home/rosinality/t5x/flaxformer/flaxformer/architectures/t5/t5_architecture.py:623, activation_partitioning_dims was set, but it is deprecated and will be removed soon.
W0512 23:42:11.984453 139674595104768 activation_partitioning.py:145] In /home/rosinality/t5x/flaxformer/flaxformer/architectures/t5/t5_architecture.py:586, activation_partitioning_dims was set, but it is deprecated and will be removed soon.
W0512 23:42:11.984982 140339465779200 activation_partitioning.py:145] In /home/rosinality/t5x/flaxformer/flaxformer/architectures/t5/t5_architecture.py:663, activation_partitioning_dims was set, but it is deprecated and will be removed soon.
W0512 23:42:11.970917 140311617681408 activation_partitioning.py:145] In /home/rosinality/t5x/flaxformer/flaxformer/architectures/t5/t5_architecture.py:520, activation_partitioning_dims was set, but it is deprecated and will be removed soon.
W0512 23:42:11.978145 140311617681408 activation_partitioning.py:145] In /home/rosinality/t5x/flaxformer/flaxformer/architectures/t5/t5_architecture.py:586, activation_partitioning_dims was set, but it is deprecated and will be removed soon.
W0512 23:42:11.997948 140339465779200 activation_partitioning.py:145] In /home/rosinality/t5x/flaxformer/flaxformer/architectures/t5/t5_architecture.py:678, activation_partitioning_dims was set, but it is deprecated and will be removed soon.
W0512 23:42:12.007601 139674595104768 activation_partitioning.py:145] In /home/rosinality/t5x/flaxformer/flaxformer/architectures/t5/t5_architecture.py:623, activation_partitioning_dims was set, but it is deprecated and will be removed soon.
W0512 23:42:12.007958 140440593786880 activation_partitioning.py:145] In /home/rosinality/t5x/flaxformer/flaxformer/architectures/t5/t5_architecture.py:623, activation_partitioning_dims was set, but it is deprecated and will be removed soon.
W0512 23:42:12.010885 139674595104768 activation_partitioning.py:145] In /home/rosinality/t5x/flaxformer/flaxformer/architectures/t5/t5_architecture.py:663, activation_partitioning_dims was set, but it is deprecated and will be removed soon.
W0512 23:42:12.011238 140440593786880 activation_partitioning.py:145] In /home/rosinality/t5x/flaxformer/flaxformer/architectures/t5/t5_architecture.py:663, activation_partitioning_dims was set, but it is deprecated and will be removed soon.
W0512 23:42:12.022729 140095893518336 activation_partitioning.py:145] In /home/rosinality/t5x/flaxformer/flaxformer/architectures/t5/t5_architecture.py:520, activation_partitioning_dims was set, but it is deprecated and will be removed soon.
W0512 23:42:12.024216 139674595104768 activation_partitioning.py:145] In /home/rosinality/t5x/flaxformer/flaxformer/architectures/t5/t5_architecture.py:678, activation_partitioning_dims was set, but it is deprecated and will be removed soon.
W0512 23:42:12.029887 140095893518336 activation_partitioning.py:145] In /home/rosinality/t5x/flaxformer/flaxformer/architectures/t5/t5_architecture.py:586, activation_partitioning_dims was set, but it is deprecated and will be removed soon.
W0512 23:42:12.039426 140595318249472 activation_partitioning.py:145] In /home/rosinality/t5x/flaxformer/flaxformer/architectures/t5/t5_architecture.py:520, activation_partitioning_dims was set, but it is deprecated and will be removed soon.
W0512 23:42:12.046824 140595318249472 activation_partitioning.py:145] In /home/rosinality/t5x/flaxformer/flaxformer/architectures/t5/t5_architecture.py:586, activation_partitioning_dims was set, but it is deprecated and will be removed soon.
W0512 23:42:12.030624 140311617681408 activation_partitioning.py:145] In /home/rosinality/t5x/flaxformer/flaxformer/architectures/t5/t5_architecture.py:623, activation_partitioning_dims was set, but it is deprecated and will be removed soon.
W0512 23:42:12.033853 140311617681408 activation_partitioning.py:145] In /home/rosinality/t5x/flaxformer/flaxformer/architectures/t5/t5_architecture.py:663, activation_partitioning_dims was set, but it is deprecated and will be removed soon.
W0512 23:42:12.056311 140440593786880 activation_partitioning.py:145] In /home/rosinality/t5x/flaxformer/flaxformer/architectures/t5/t5_architecture.py:678, activation_partitioning_dims was set, but it is deprecated and will be removed soon.
W0512 23:42:12.060762 140440593786880 activation_partitioning.py:145] In /home/rosinality/t5x/flaxformer/flaxformer/architectures/t5/t5_architecture.py:520, activation_partitioning_dims was set, but it is deprecated and will be removed soon.
W0512 23:42:12.063980 140440593786880 activation_partitioning.py:145] In /home/rosinality/t5x/flaxformer/flaxformer/architectures/t5/t5_architecture.py:586, activation_partitioning_dims was set, but it is deprecated and will be removed soon.
W0512 23:42:12.082227 140095893518336 activation_partitioning.py:145] In /home/rosinality/t5x/flaxformer/flaxformer/architectures/t5/t5_architecture.py:623, activation_partitioning_dims was set, but it is deprecated and will be removed soon.
W0512 23:42:12.085437 140095893518336 activation_partitioning.py:145] In /home/rosinality/t5x/flaxformer/flaxformer/architectures/t5/t5_architecture.py:663, activation_partitioning_dims was set, but it is deprecated and will be removed soon.
W0512 23:42:12.086822 140440593786880 activation_partitioning.py:145] In /home/rosinality/t5x/flaxformer/flaxformer/architectures/t5/t5_architecture.py:623, activation_partitioning_dims was set, but it is deprecated and will be removed soon.
W0512 23:42:12.090079 140440593786880 activation_partitioning.py:145] In /home/rosinality/t5x/flaxformer/flaxformer/architectures/t5/t5_architecture.py:663, activation_partitioning_dims was set, but it is deprecated and will be removed soon.
W0512 23:42:12.076459 140529434355712 activation_partitioning.py:145] In /home/rosinality/t5x/flaxformer/flaxformer/architectures/t5/t5_architecture.py:520, activation_partitioning_dims was set, but it is deprecated and will be removed soon.
W0512 23:42:12.078667 140311617681408 activation_partitioning.py:145] In /home/rosinality/t5x/flaxformer/flaxformer/architectures/t5/t5_architecture.py:678, activation_partitioning_dims was set, but it is deprecated and will be removed soon.
W0512 23:42:12.099564 140595318249472 activation_partitioning.py:145] In /home/rosinality/t5x/flaxformer/flaxformer/architectures/t5/t5_architecture.py:623, activation_partitioning_dims was set, but it is deprecated and will be removed soon.
W0512 23:42:12.083682 140529434355712 activation_partitioning.py:145] In /home/rosinality/t5x/flaxformer/flaxformer/architectures/t5/t5_architecture.py:586, activation_partitioning_dims was set, but it is deprecated and will be removed soon.
W0512 23:42:12.102761 140595318249472 activation_partitioning.py:145] In /home/rosinality/t5x/flaxformer/flaxformer/architectures/t5/t5_architecture.py:663, activation_partitioning_dims was set, but it is deprecated and will be removed soon.
W0512 23:42:12.082983 140311617681408 activation_partitioning.py:145] In /home/rosinality/t5x/flaxformer/flaxformer/architectures/t5/t5_architecture.py:520, activation_partitioning_dims was set, but it is deprecated and will be removed soon.
W0512 23:42:12.103382 140440593786880 activation_partitioning.py:145] In /home/rosinality/t5x/flaxformer/flaxformer/architectures/t5/t5_architecture.py:678, activation_partitioning_dims was set, but it is deprecated and will be removed soon.
W0512 23:42:12.086172 140311617681408 activation_partitioning.py:145] In /home/rosinality/t5x/flaxformer/flaxformer/architectures/t5/t5_architecture.py:586, activation_partitioning_dims was set, but it is deprecated and will be removed soon.
W0512 23:42:12.107845 140031916288000 activation_partitioning.py:145] In /home/rosinality/t5x/flaxformer/flaxformer/architectures/t5/t5_architecture.py:520, activation_partitioning_dims was set, but it is deprecated and will be removed soon.
W0512 23:42:12.115235 140031916288000 activation_partitioning.py:145] In /home/rosinality/t5x/flaxformer/flaxformer/architectures/t5/t5_architecture.py:586, activation_partitioning_dims was set, but it is deprecated and will be removed soon.
W0512 23:42:12.129988 140095893518336 activation_partitioning.py:145] In /home/rosinality/t5x/flaxformer/flaxformer/architectures/t5/t5_architecture.py:678, activation_partitioning_dims was set, but it is deprecated and will be removed soon.
W0512 23:42:12.108816 140311617681408 activation_partitioning.py:145] In /home/rosinality/t5x/flaxformer/flaxformer/architectures/t5/t5_architecture.py:623, activation_partitioning_dims was set, but it is deprecated and will be removed soon.
W0512 23:42:12.112022 140311617681408 activation_partitioning.py:145] In /home/rosinality/t5x/flaxformer/flaxformer/architectures/t5/t5_architecture.py:663, activation_partitioning_dims was set, but it is deprecated and will be removed soon.
W0512 23:42:12.134409 140095893518336 activation_partitioning.py:145] In /home/rosinality/t5x/flaxformer/flaxformer/architectures/t5/t5_architecture.py:520, activation_partitioning_dims was set, but it is deprecated and will be removed soon.
W0512 23:42:12.137620 140095893518336 activation_partitioning.py:145] In /home/rosinality/t5x/flaxformer/flaxformer/architectures/t5/t5_architecture.py:586, activation_partitioning_dims was set, but it is deprecated and will be removed soon.
W0512 23:42:12.125140 140311617681408 activation_partitioning.py:145] In /home/rosinality/t5x/flaxformer/flaxformer/architectures/t5/t5_architecture.py:678, activation_partitioning_dims was set, but it is deprecated and will be removed soon.
W0512 23:42:12.147816 140595318249472 activation_partitioning.py:145] In /home/rosinality/t5x/flaxformer/flaxformer/architectures/t5/t5_architecture.py:678, activation_partitioning_dims was set, but it is deprecated and will be removed soon.
W0512 23:42:12.152185 140595318249472 activation_partitioning.py:145] In /home/rosinality/t5x/flaxformer/flaxformer/architectures/t5/t5_architecture.py:520, activation_partitioning_dims was set, but it is deprecated and will be removed soon.
W0512 23:42:12.155384 140595318249472 activation_partitioning.py:145] In /home/rosinality/t5x/flaxformer/flaxformer/architectures/t5/t5_architecture.py:586, activation_partitioning_dims was set, but it is deprecated and will be removed soon.
W0512 23:42:12.137671 140529434355712 activation_partitioning.py:145] In /home/rosinality/t5x/flaxformer/flaxformer/architectures/t5/t5_architecture.py:623, activation_partitioning_dims was set, but it is deprecated and will be removed soon.
W0512 23:42:12.160473 140095893518336 activation_partitioning.py:145] In /home/rosinality/t5x/flaxformer/flaxformer/architectures/t5/t5_architecture.py:623, activation_partitioning_dims was set, but it is deprecated and will be removed soon.
W0512 23:42:12.140964 140529434355712 activation_partitioning.py:145] In /home/rosinality/t5x/flaxformer/flaxformer/architectures/t5/t5_architecture.py:663, activation_partitioning_dims was set, but it is deprecated and will be removed soon.
W0512 23:42:12.163715 140095893518336 activation_partitioning.py:145] In /home/rosinality/t5x/flaxformer/flaxformer/architectures/t5/t5_architecture.py:663, activation_partitioning_dims was set, but it is deprecated and will be removed soon.
I0512 23:42:12.167677 140339465779200 moe_layers.py:777] Selected group_size=4096 and num_groups=192 for input num_tokens=786432, max_group_size=4096, num_experts=8 and num_expert_replicas=1
W0512 23:42:12.169045 140031916288000 activation_partitioning.py:145] In /home/rosinality/t5x/flaxformer/flaxformer/architectures/t5/t5_architecture.py:623, activation_partitioning_dims was set, but it is deprecated and will be removed soon.
W0512 23:42:12.172301 140031916288000 activation_partitioning.py:145] In /home/rosinality/t5x/flaxformer/flaxformer/architectures/t5/t5_architecture.py:663, activation_partitioning_dims was set, but it is deprecated and will be removed soon.
W0512 23:42:12.176981 140095893518336 activation_partitioning.py:145] In /home/rosinality/t5x/flaxformer/flaxformer/architectures/t5/t5_architecture.py:678, activation_partitioning_dims was set, but it is deprecated and will be removed soon.
W0512 23:42:12.178182 140595318249472 activation_partitioning.py:145] In /home/rosinality/t5x/flaxformer/flaxformer/architectures/t5/t5_architecture.py:623, activation_partitioning_dims was set, but it is deprecated and will be removed soon.
W0512 23:42:12.181389 140595318249472 activation_partitioning.py:145] In /home/rosinality/t5x/flaxformer/flaxformer/architectures/t5/t5_architecture.py:663, activation_partitioning_dims was set, but it is deprecated and will be removed soon.
W0512 23:42:12.194550 140595318249472 activation_partitioning.py:145] In /home/rosinality/t5x/flaxformer/flaxformer/architectures/t5/t5_architecture.py:678, activation_partitioning_dims was set, but it is deprecated and will be removed soon.
I0512 23:42:12.196474 139674595104768 moe_layers.py:777] Selected group_size=4096 and num_groups=192 for input num_tokens=786432, max_group_size=4096, num_experts=8 and num_expert_replicas=1
W0512 23:42:12.186008 140529434355712 activation_partitioning.py:145] In /home/rosinality/t5x/flaxformer/flaxformer/architectures/t5/t5_architecture.py:678, activation_partitioning_dims was set, but it is deprecated and will be removed soon.
W0512 23:42:12.190491 140529434355712 activation_partitioning.py:145] In /home/rosinality/t5x/flaxformer/flaxformer/architectures/t5/t5_architecture.py:520, activation_partitioning_dims was set, but it is deprecated and will be removed soon.
W0512 23:42:12.193768 140529434355712 activation_partitioning.py:145] In /home/rosinality/t5x/flaxformer/flaxformer/architectures/t5/t5_architecture.py:586, activation_partitioning_dims was set, but it is deprecated and will be removed soon.
W0512 23:42:12.217789 140031916288000 activation_partitioning.py:145] In /home/rosinality/t5x/flaxformer/flaxformer/architectures/t5/t5_architecture.py:678, activation_partitioning_dims was set, but it is deprecated and will be removed soon.
W0512 23:42:12.222144 140031916288000 activation_partitioning.py:145] In /home/rosinality/t5x/flaxformer/flaxformer/architectures/t5/t5_architecture.py:520, activation_partitioning_dims was set, but it is deprecated and will be removed soon.
W0512 23:42:12.225438 140031916288000 activation_partitioning.py:145] In /home/rosinality/t5x/flaxformer/flaxformer/architectures/t5/t5_architecture.py:586, activation_partitioning_dims was set, but it is deprecated and will be removed soon.
W0512 23:42:12.217068 140529434355712 activation_partitioning.py:145] In /home/rosinality/t5x/flaxformer/flaxformer/architectures/t5/t5_architecture.py:623, activation_partitioning_dims was set, but it is deprecated and will be removed soon.
W0512 23:42:12.220297 140529434355712 activation_partitioning.py:145] In /home/rosinality/t5x/flaxformer/flaxformer/architectures/t5/t5_architecture.py:663, activation_partitioning_dims was set, but it is deprecated and will be removed soon.
W0512 23:42:12.248595 140031916288000 activation_partitioning.py:145] In /home/rosinality/t5x/flaxformer/flaxformer/architectures/t5/t5_architecture.py:623, activation_partitioning_dims was set, but it is deprecated and will be removed soon.
W0512 23:42:12.233566 140529434355712 activation_partitioning.py:145] In /home/rosinality/t5x/flaxformer/flaxformer/architectures/t5/t5_architecture.py:678, activation_partitioning_dims was set, but it is deprecated and will be removed soon.
W0512 23:42:12.251837 140031916288000 activation_partitioning.py:145] In /home/rosinality/t5x/flaxformer/flaxformer/architectures/t5/t5_architecture.py:663, activation_partitioning_dims was set, but it is deprecated and will be removed soon.
W0512 23:42:12.265045 140031916288000 activation_partitioning.py:145] In /home/rosinality/t5x/flaxformer/flaxformer/architectures/t5/t5_architecture.py:678, activation_partitioning_dims was set, but it is deprecated and will be removed soon.
I0512 23:42:12.276020 140440593786880 moe_layers.py:777] Selected group_size=4096 and num_groups=192 for input num_tokens=786432, max_group_size=4096, num_experts=8 and num_expert_replicas=1
I0512 23:42:12.295787 140311617681408 moe_layers.py:777] Selected group_size=4096 and num_groups=192 for input num_tokens=786432, max_group_size=4096, num_experts=8 and num_expert_replicas=1
I0512 23:42:12.349023 140095893518336 moe_layers.py:777] Selected group_size=4096 and num_groups=192 for input num_tokens=786432, max_group_size=4096, num_experts=8 and num_expert_replicas=1
I0512 23:42:12.366300 140595318249472 moe_layers.py:777] Selected group_size=4096 and num_groups=192 for input num_tokens=786432, max_group_size=4096, num_experts=8 and num_expert_replicas=1
I0512 23:42:12.406697 140529434355712 moe_layers.py:777] Selected group_size=4096 and num_groups=192 for input num_tokens=786432, max_group_size=4096, num_experts=8 and num_expert_replicas=1
I0512 23:42:12.437280 140031916288000 moe_layers.py:777] Selected group_size=4096 and num_groups=192 for input num_tokens=786432, max_group_size=4096, num_experts=8 and num_expert_replicas=1
I0512 23:42:12.547577 140339465779200 moe_layers.py:777] Selected group_size=4096 and num_groups=192 for input num_tokens=786432, max_group_size=4096, num_experts=8 and num_expert_replicas=1
I0512 23:42:12.584470 139674595104768 moe_layers.py:777] Selected group_size=4096 and num_groups=192 for input num_tokens=786432, max_group_size=4096, num_experts=8 and num_expert_replicas=1
I0512 23:42:12.662450 140440593786880 moe_layers.py:777] Selected group_size=4096 and num_groups=192 for input num_tokens=786432, max_group_size=4096, num_experts=8 and num_expert_replicas=1
I0512 23:42:12.677663 140311617681408 moe_layers.py:777] Selected group_size=4096 and num_groups=192 for input num_tokens=786432, max_group_size=4096, num_experts=8 and num_expert_replicas=1
I0512 23:42:12.730577 140095893518336 moe_layers.py:777] Selected group_size=4096 and num_groups=192 for input num_tokens=786432, max_group_size=4096, num_experts=8 and num_expert_replicas=1
I0512 23:42:12.751024 140595318249472 moe_layers.py:777] Selected group_size=4096 and num_groups=192 for input num_tokens=786432, max_group_size=4096, num_experts=8 and num_expert_replicas=1
I0512 23:42:12.790199 140529434355712 moe_layers.py:777] Selected group_size=4096 and num_groups=192 for input num_tokens=786432, max_group_size=4096, num_experts=8 and num_expert_replicas=1
I0512 23:42:12.820064 140031916288000 moe_layers.py:777] Selected group_size=4096 and num_groups=192 for input num_tokens=786432, max_group_size=4096, num_experts=8 and num_expert_replicas=1
I0512 23:42:12.867415 140339465779200 moe_layers.py:777] Selected group_size=4096 and num_groups=192 for input num_tokens=786432, max_group_size=4096, num_experts=8 and num_expert_replicas=1
I0512 23:42:12.909567 139674595104768 moe_layers.py:777] Selected group_size=4096 and num_groups=192 for input num_tokens=786432, max_group_size=4096, num_experts=8 and num_expert_replicas=1
I0512 23:42:12.987277 140440593786880 moe_layers.py:777] Selected group_size=4096 and num_groups=192 for input num_tokens=786432, max_group_size=4096, num_experts=8 and num_expert_replicas=1
I0512 23:42:12.997421 140311617681408 moe_layers.py:777] Selected group_size=4096 and num_groups=192 for input num_tokens=786432, max_group_size=4096, num_experts=8 and num_expert_replicas=1
I0512 23:42:13.051518 140095893518336 moe_layers.py:777] Selected group_size=4096 and num_groups=192 for input num_tokens=786432, max_group_size=4096, num_experts=8 and num_expert_replicas=1
I0512 23:42:13.073098 140595318249472 moe_layers.py:777] Selected group_size=4096 and num_groups=192 for input num_tokens=786432, max_group_size=4096, num_experts=8 and num_expert_replicas=1
I0512 23:42:13.114774 140529434355712 moe_layers.py:777] Selected group_size=4096 and num_groups=192 for input num_tokens=786432, max_group_size=4096, num_experts=8 and num_expert_replicas=1
I0512 23:42:13.142279 140031916288000 moe_layers.py:777] Selected group_size=4096 and num_groups=192 for input num_tokens=786432, max_group_size=4096, num_experts=8 and num_expert_replicas=1
I0512 23:42:13.186003 140339465779200 moe_layers.py:777] Selected group_size=4096 and num_groups=192 for input num_tokens=786432, max_group_size=4096, num_experts=8 and num_expert_replicas=1
I0512 23:42:13.235170 139674595104768 moe_layers.py:777] Selected group_size=4096 and num_groups=192 for input num_tokens=786432, max_group_size=4096, num_experts=8 and num_expert_replicas=1
I0512 23:42:13.311189 140440593786880 moe_layers.py:777] Selected group_size=4096 and num_groups=192 for input num_tokens=786432, max_group_size=4096, num_experts=8 and num_expert_replicas=1
I0512 23:42:13.316615 140311617681408 moe_layers.py:777] Selected group_size=4096 and num_groups=192 for input num_tokens=786432, max_group_size=4096, num_experts=8 and num_expert_replicas=1
I0512 23:42:13.373096 140095893518336 moe_layers.py:777] Selected group_size=4096 and num_groups=192 for input num_tokens=786432, max_group_size=4096, num_experts=8 and num_expert_replicas=1
I0512 23:42:13.393779 140595318249472 moe_layers.py:777] Selected group_size=4096 and num_groups=192 for input num_tokens=786432, max_group_size=4096, num_experts=8 and num_expert_replicas=1
I0512 23:42:13.440882 140529434355712 moe_layers.py:777] Selected group_size=4096 and num_groups=192 for input num_tokens=786432, max_group_size=4096, num_experts=8 and num_expert_replicas=1
I0512 23:42:13.465318 140031916288000 moe_layers.py:777] Selected group_size=4096 and num_groups=192 for input num_tokens=786432, max_group_size=4096, num_experts=8 and num_expert_replicas=1
W0512 23:42:13.511089 140339465779200 adafactor.py:358] Since rank of parameter decoder/decoder_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:13.512150 140339465779200 adafactor.py:358] Since rank of parameter decoder/layers_0/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.513377 140339465779200 adafactor.py:358] Since rank of parameter decoder/layers_0/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.514496 140339465779200 adafactor.py:358] Since rank of parameter decoder/layers_0/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:13.515729 140339465779200 adafactor.py:358] Since rank of parameter decoder/layers_0/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:13.516674 140339465779200 adafactor.py:358] Since rank of parameter decoder/layers_0/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:13.517584 140339465779200 adafactor.py:358] Since rank of parameter decoder/layers_0/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.518728 140339465779200 adafactor.py:358] Since rank of parameter decoder/layers_0/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:13.519921 140339465779200 adafactor.py:358] Since rank of parameter decoder/layers_0/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.521030 140339465779200 adafactor.py:358] Since rank of parameter decoder/layers_0/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.522113 140339465779200 adafactor.py:358] Since rank of parameter decoder/layers_1/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.523242 140339465779200 adafactor.py:358] Since rank of parameter decoder/layers_1/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.524367 140339465779200 adafactor.py:358] Since rank of parameter decoder/layers_1/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:13.525438 140339465779200 adafactor.py:358] Since rank of parameter decoder/layers_1/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:13.526314 140339465779200 adafactor.py:358] Since rank of parameter decoder/layers_1/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:13.527234 140339465779200 adafactor.py:358] Since rank of parameter decoder/layers_1/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.528350 140339465779200 adafactor.py:358] Since rank of parameter decoder/layers_1/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:13.529420 140339465779200 adafactor.py:358] Since rank of parameter decoder/layers_1/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.530485 140339465779200 adafactor.py:358] Since rank of parameter decoder/layers_1/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.531601 140339465779200 adafactor.py:358] Since rank of parameter decoder/layers_10/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.532720 140339465779200 adafactor.py:358] Since rank of parameter decoder/layers_10/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.533824 140339465779200 adafactor.py:358] Since rank of parameter decoder/layers_10/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:13.534938 140339465779200 adafactor.py:358] Since rank of parameter decoder/layers_10/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:13.535815 140339465779200 adafactor.py:358] Since rank of parameter decoder/layers_10/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:13.536720 140339465779200 adafactor.py:358] Since rank of parameter decoder/layers_10/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.537798 140339465779200 adafactor.py:358] Since rank of parameter decoder/layers_10/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:13.538910 140339465779200 adafactor.py:358] Since rank of parameter decoder/layers_10/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.539975 140339465779200 adafactor.py:358] Since rank of parameter decoder/layers_10/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.541077 140339465779200 adafactor.py:358] Since rank of parameter decoder/layers_11/extra_mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.542127 140339465779200 adafactor.py:358] Since rank of parameter decoder/layers_11/extra_mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.543770 140339465779200 adafactor.py:358] Since rank of parameter decoder/layers_11/extra_mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:13.547813 140339465779200 adafactor.py:358] Since rank of parameter decoder/layers_11/mlp/router/router_weights/w/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>, <FactorDim.NONE: None>) is ignored.
W0512 23:42:13.548732 140339465779200 adafactor.py:358] Since rank of parameter decoder/layers_11/pre_extra_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:13.549596 140339465779200 adafactor.py:358] Since rank of parameter decoder/layers_11/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:13.550514 140339465779200 adafactor.py:358] Since rank of parameter decoder/layers_11/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:13.551375 140339465779200 adafactor.py:358] Since rank of parameter decoder/layers_11/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.552472 140339465779200 adafactor.py:358] Since rank of parameter decoder/layers_11/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:13.553538 140339465779200 adafactor.py:358] Since rank of parameter decoder/layers_11/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.554664 140339465779200 adafactor.py:358] Since rank of parameter decoder/layers_11/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.555783 140339465779200 adafactor.py:358] Since rank of parameter decoder/layers_12/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.556876 140339465779200 adafactor.py:358] Since rank of parameter decoder/layers_12/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.557949 140339465779200 adafactor.py:358] Since rank of parameter decoder/layers_12/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:13.559080 140339465779200 adafactor.py:358] Since rank of parameter decoder/layers_12/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:13.559958 140339465779200 adafactor.py:358] Since rank of parameter decoder/layers_12/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:13.560880 140339465779200 adafactor.py:358] Since rank of parameter decoder/layers_12/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.561971 140339465779200 adafactor.py:358] Since rank of parameter decoder/layers_12/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:13.564482 139674595104768 adafactor.py:358] Since rank of parameter decoder/decoder_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:13.565541 139674595104768 adafactor.py:358] Since rank of parameter decoder/layers_0/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.563129 140339465779200 adafactor.py:358] Since rank of parameter decoder/layers_12/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.566726 139674595104768 adafactor.py:358] Since rank of parameter decoder/layers_0/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.564279 140339465779200 adafactor.py:358] Since rank of parameter decoder/layers_12/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.567904 139674595104768 adafactor.py:358] Since rank of parameter decoder/layers_0/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:13.565392 140339465779200 adafactor.py:358] Since rank of parameter decoder/layers_13/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.569169 139674595104768 adafactor.py:358] Since rank of parameter decoder/layers_0/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:13.566554 140339465779200 adafactor.py:358] Since rank of parameter decoder/layers_13/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.570090 139674595104768 adafactor.py:358] Since rank of parameter decoder/layers_0/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:13.567662 140339465779200 adafactor.py:358] Since rank of parameter decoder/layers_13/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:13.570981 139674595104768 adafactor.py:358] Since rank of parameter decoder/layers_0/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.568815 140339465779200 adafactor.py:358] Since rank of parameter decoder/layers_13/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:13.572098 139674595104768 adafactor.py:358] Since rank of parameter decoder/layers_0/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:13.570081 140339465779200 adafactor.py:358] Since rank of parameter decoder/layers_13/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:13.573242 139674595104768 adafactor.py:358] Since rank of parameter decoder/layers_0/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.571045 140339465779200 adafactor.py:358] Since rank of parameter decoder/layers_13/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.574322 139674595104768 adafactor.py:358] Since rank of parameter decoder/layers_0/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.572194 140339465779200 adafactor.py:358] Since rank of parameter decoder/layers_13/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:13.575455 139674595104768 adafactor.py:358] Since rank of parameter decoder/layers_1/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.573299 140339465779200 adafactor.py:358] Since rank of parameter decoder/layers_13/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.576602 139674595104768 adafactor.py:358] Since rank of parameter decoder/layers_1/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.574409 140339465779200 adafactor.py:358] Since rank of parameter decoder/layers_13/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.577681 139674595104768 adafactor.py:358] Since rank of parameter decoder/layers_1/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:13.575578 140339465779200 adafactor.py:358] Since rank of parameter decoder/layers_14/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.578768 139674595104768 adafactor.py:358] Since rank of parameter decoder/layers_1/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:13.576731 140339465779200 adafactor.py:358] Since rank of parameter decoder/layers_14/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.579683 139674595104768 adafactor.py:358] Since rank of parameter decoder/layers_1/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:13.577843 140339465779200 adafactor.py:358] Since rank of parameter decoder/layers_14/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:13.580630 139674595104768 adafactor.py:358] Since rank of parameter decoder/layers_1/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.579008 140339465779200 adafactor.py:358] Since rank of parameter decoder/layers_14/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:13.581707 139674595104768 adafactor.py:358] Since rank of parameter decoder/layers_1/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:13.579918 140339465779200 adafactor.py:358] Since rank of parameter decoder/layers_14/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:13.582828 139674595104768 adafactor.py:358] Since rank of parameter decoder/layers_1/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.580866 140339465779200 adafactor.py:358] Since rank of parameter decoder/layers_14/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.583964 139674595104768 adafactor.py:358] Since rank of parameter decoder/layers_1/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.585104 139674595104768 adafactor.py:358] Since rank of parameter decoder/layers_10/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.586168 139674595104768 adafactor.py:358] Since rank of parameter decoder/layers_10/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.587268 139674595104768 adafactor.py:358] Since rank of parameter decoder/layers_10/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:13.588437 139674595104768 adafactor.py:358] Since rank of parameter decoder/layers_10/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:13.589318 139674595104768 adafactor.py:358] Since rank of parameter decoder/layers_10/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:13.590230 139674595104768 adafactor.py:358] Since rank of parameter decoder/layers_10/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.591378 139674595104768 adafactor.py:358] Since rank of parameter decoder/layers_10/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:13.592527 139674595104768 adafactor.py:358] Since rank of parameter decoder/layers_10/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.593596 139674595104768 adafactor.py:358] Since rank of parameter decoder/layers_10/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.594663 139674595104768 adafactor.py:358] Since rank of parameter decoder/layers_11/extra_mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.595779 139674595104768 adafactor.py:358] Since rank of parameter decoder/layers_11/extra_mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.597438 139674595104768 adafactor.py:358] Since rank of parameter decoder/layers_11/extra_mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:13.601580 139674595104768 adafactor.py:358] Since rank of parameter decoder/layers_11/mlp/router/router_weights/w/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>, <FactorDim.NONE: None>) is ignored.
W0512 23:42:13.602519 139674595104768 adafactor.py:358] Since rank of parameter decoder/layers_11/pre_extra_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:13.603445 139674595104768 adafactor.py:358] Since rank of parameter decoder/layers_11/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:13.604384 139674595104768 adafactor.py:358] Since rank of parameter decoder/layers_11/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:13.605268 139674595104768 adafactor.py:358] Since rank of parameter decoder/layers_11/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.606350 139674595104768 adafactor.py:358] Since rank of parameter decoder/layers_11/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:13.607475 139674595104768 adafactor.py:358] Since rank of parameter decoder/layers_11/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.608604 139674595104768 adafactor.py:358] Since rank of parameter decoder/layers_11/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.609757 139674595104768 adafactor.py:358] Since rank of parameter decoder/layers_12/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.610845 139674595104768 adafactor.py:358] Since rank of parameter decoder/layers_12/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.611960 139674595104768 adafactor.py:358] Since rank of parameter decoder/layers_12/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:13.613094 139674595104768 adafactor.py:358] Since rank of parameter decoder/layers_12/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:13.613972 139674595104768 adafactor.py:358] Since rank of parameter decoder/layers_12/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:13.614863 139674595104768 adafactor.py:358] Since rank of parameter decoder/layers_12/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.615976 139674595104768 adafactor.py:358] Since rank of parameter decoder/layers_12/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:13.617115 139674595104768 adafactor.py:358] Since rank of parameter decoder/layers_12/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.618185 139674595104768 adafactor.py:358] Since rank of parameter decoder/layers_12/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.619261 139674595104768 adafactor.py:358] Since rank of parameter decoder/layers_13/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.620431 139674595104768 adafactor.py:358] Since rank of parameter decoder/layers_13/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.621501 139674595104768 adafactor.py:358] Since rank of parameter decoder/layers_13/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:13.622611 139674595104768 adafactor.py:358] Since rank of parameter decoder/layers_13/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:13.624085 139674595104768 adafactor.py:358] Since rank of parameter decoder/layers_13/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:13.625061 139674595104768 adafactor.py:358] Since rank of parameter decoder/layers_13/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.626192 139674595104768 adafactor.py:358] Since rank of parameter decoder/layers_13/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:13.627346 139674595104768 adafactor.py:358] Since rank of parameter decoder/layers_13/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.628454 139674595104768 adafactor.py:358] Since rank of parameter decoder/layers_13/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.641122 140440593786880 adafactor.py:358] Since rank of parameter decoder/decoder_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:13.642200 140440593786880 adafactor.py:358] Since rank of parameter decoder/layers_0/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.643419 140440593786880 adafactor.py:358] Since rank of parameter decoder/layers_0/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.644621 140440593786880 adafactor.py:358] Since rank of parameter decoder/layers_0/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:13.645872 140440593786880 adafactor.py:358] Since rank of parameter decoder/layers_0/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:13.646810 140440593786880 adafactor.py:358] Since rank of parameter decoder/layers_0/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:13.647750 140440593786880 adafactor.py:358] Since rank of parameter decoder/layers_0/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.648931 140440593786880 adafactor.py:358] Since rank of parameter decoder/layers_0/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:13.650109 140440593786880 adafactor.py:358] Since rank of parameter decoder/layers_0/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.651255 140440593786880 adafactor.py:358] Since rank of parameter decoder/layers_0/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.652362 140440593786880 adafactor.py:358] Since rank of parameter decoder/layers_1/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.653560 140440593786880 adafactor.py:358] Since rank of parameter decoder/layers_1/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.654666 140440593786880 adafactor.py:358] Since rank of parameter decoder/layers_1/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:13.655766 140440593786880 adafactor.py:358] Since rank of parameter decoder/layers_1/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:13.656703 140440593786880 adafactor.py:358] Since rank of parameter decoder/layers_1/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:13.657659 140440593786880 adafactor.py:358] Since rank of parameter decoder/layers_1/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.658794 140440593786880 adafactor.py:358] Since rank of parameter decoder/layers_1/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:13.659911 140440593786880 adafactor.py:358] Since rank of parameter decoder/layers_1/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.661034 140440593786880 adafactor.py:358] Since rank of parameter decoder/layers_1/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.641376 140311617681408 adafactor.py:358] Since rank of parameter decoder/decoder_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:13.662182 140440593786880 adafactor.py:358] Since rank of parameter decoder/layers_10/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.642456 140311617681408 adafactor.py:358] Since rank of parameter decoder/layers_0/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.663330 140440593786880 adafactor.py:358] Since rank of parameter decoder/layers_10/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.643652 140311617681408 adafactor.py:358] Since rank of parameter decoder/layers_0/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.664485 140440593786880 adafactor.py:358] Since rank of parameter decoder/layers_10/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:13.644792 140311617681408 adafactor.py:358] Since rank of parameter decoder/layers_0/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:13.665647 140440593786880 adafactor.py:358] Since rank of parameter decoder/layers_10/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:13.646053 140311617681408 adafactor.py:358] Since rank of parameter decoder/layers_0/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:13.666552 140440593786880 adafactor.py:358] Since rank of parameter decoder/layers_10/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:13.646981 140311617681408 adafactor.py:358] Since rank of parameter decoder/layers_0/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:13.667454 140440593786880 adafactor.py:358] Since rank of parameter decoder/layers_10/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.647905 140311617681408 adafactor.py:358] Since rank of parameter decoder/layers_0/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.668589 140440593786880 adafactor.py:358] Since rank of parameter decoder/layers_10/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:13.649030 140311617681408 adafactor.py:358] Since rank of parameter decoder/layers_0/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:13.669739 140440593786880 adafactor.py:358] Since rank of parameter decoder/layers_10/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.650258 140311617681408 adafactor.py:358] Since rank of parameter decoder/layers_0/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.670841 140440593786880 adafactor.py:358] Since rank of parameter decoder/layers_10/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.651390 140311617681408 adafactor.py:358] Since rank of parameter decoder/layers_0/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.671947 140440593786880 adafactor.py:358] Since rank of parameter decoder/layers_11/extra_mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.652559 140311617681408 adafactor.py:358] Since rank of parameter decoder/layers_1/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.673070 140440593786880 adafactor.py:358] Since rank of parameter decoder/layers_11/extra_mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.653807 140311617681408 adafactor.py:358] Since rank of parameter decoder/layers_1/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.674713 140440593786880 adafactor.py:358] Since rank of parameter decoder/layers_11/extra_mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:13.654960 140311617681408 adafactor.py:358] Since rank of parameter decoder/layers_1/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:13.656084 140311617681408 adafactor.py:358] Since rank of parameter decoder/layers_1/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:13.657001 140311617681408 adafactor.py:358] Since rank of parameter decoder/layers_1/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:13.658012 140311617681408 adafactor.py:358] Since rank of parameter decoder/layers_1/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.678913 140440593786880 adafactor.py:358] Since rank of parameter decoder/layers_11/mlp/router/router_weights/w/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>, <FactorDim.NONE: None>) is ignored.
W0512 23:42:13.659134 140311617681408 adafactor.py:358] Since rank of parameter decoder/layers_1/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:13.679839 140440593786880 adafactor.py:358] Since rank of parameter decoder/layers_11/pre_extra_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:13.660258 140311617681408 adafactor.py:358] Since rank of parameter decoder/layers_1/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.680830 140440593786880 adafactor.py:358] Since rank of parameter decoder/layers_11/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:13.661325 140311617681408 adafactor.py:358] Since rank of parameter decoder/layers_1/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.681856 140440593786880 adafactor.py:358] Since rank of parameter decoder/layers_11/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:13.581978 140339465779200 adafactor.py:358] Since rank of parameter decoder/layers_14/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:13.583150 140339465779200 adafactor.py:358] Since rank of parameter decoder/layers_14/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.584292 140339465779200 adafactor.py:358] Since rank of parameter decoder/layers_14/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.585360 140339465779200 adafactor.py:358] Since rank of parameter decoder/layers_15/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.586415 140339465779200 adafactor.py:358] Since rank of parameter decoder/layers_15/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.587536 140339465779200 adafactor.py:358] Since rank of parameter decoder/layers_15/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:13.588645 140339465779200 adafactor.py:358] Since rank of parameter decoder/layers_15/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:13.589518 140339465779200 adafactor.py:358] Since rank of parameter decoder/layers_15/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:13.662481 140311617681408 adafactor.py:358] Since rank of parameter decoder/layers_10/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.590974 140339465779200 adafactor.py:358] Since rank of parameter decoder/layers_15/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.592074 140339465779200 adafactor.py:358] Since rank of parameter decoder/layers_15/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:13.593189 140339465779200 adafactor.py:358] Since rank of parameter decoder/layers_15/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.594254 140339465779200 adafactor.py:358] Since rank of parameter decoder/layers_15/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.595370 140339465779200 adafactor.py:358] Since rank of parameter decoder/layers_16/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.596456 140339465779200 adafactor.py:358] Since rank of parameter decoder/layers_16/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.597520 140339465779200 adafactor.py:358] Since rank of parameter decoder/layers_16/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:13.598587 140339465779200 adafactor.py:358] Since rank of parameter decoder/layers_16/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:13.599504 140339465779200 adafactor.py:358] Since rank of parameter decoder/layers_16/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:13.682852 140440593786880 adafactor.py:358] Since rank of parameter decoder/layers_11/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.663555 140311617681408 adafactor.py:358] Since rank of parameter decoder/layers_10/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.600413 140339465779200 adafactor.py:358] Since rank of parameter decoder/layers_16/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.601499 140339465779200 adafactor.py:358] Since rank of parameter decoder/layers_16/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:13.602555 140339465779200 adafactor.py:358] Since rank of parameter decoder/layers_16/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.603660 140339465779200 adafactor.py:358] Since rank of parameter decoder/layers_16/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.604808 140339465779200 adafactor.py:358] Since rank of parameter decoder/layers_17/extra_mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.605872 140339465779200 adafactor.py:358] Since rank of parameter decoder/layers_17/extra_mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.606934 140339465779200 adafactor.py:358] Since rank of parameter decoder/layers_17/extra_mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:13.610976 140339465779200 adafactor.py:358] Since rank of parameter decoder/layers_17/mlp/router/router_weights/w/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>, <FactorDim.NONE: None>) is ignored.
W0512 23:42:13.684030 140440593786880 adafactor.py:358] Since rank of parameter decoder/layers_11/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:13.664633 140311617681408 adafactor.py:358] Since rank of parameter decoder/layers_10/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:13.611864 140339465779200 adafactor.py:358] Since rank of parameter decoder/layers_17/pre_extra_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:13.612816 140339465779200 adafactor.py:358] Since rank of parameter decoder/layers_17/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:13.613692 140339465779200 adafactor.py:358] Since rank of parameter decoder/layers_17/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:13.614575 140339465779200 adafactor.py:358] Since rank of parameter decoder/layers_17/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.615654 140339465779200 adafactor.py:358] Since rank of parameter decoder/layers_17/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:13.616809 140339465779200 adafactor.py:358] Since rank of parameter decoder/layers_17/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.617876 140339465779200 adafactor.py:358] Since rank of parameter decoder/layers_17/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.618948 140339465779200 adafactor.py:358] Since rank of parameter decoder/layers_18/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.620005 140339465779200 adafactor.py:358] Since rank of parameter decoder/layers_18/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.685266 140440593786880 adafactor.py:358] Since rank of parameter decoder/layers_11/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.665809 140311617681408 adafactor.py:358] Since rank of parameter decoder/layers_10/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:13.621161 140339465779200 adafactor.py:358] Since rank of parameter decoder/layers_18/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:13.622223 140339465779200 adafactor.py:358] Since rank of parameter decoder/layers_18/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:13.623098 140339465779200 adafactor.py:358] Since rank of parameter decoder/layers_18/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:13.624009 140339465779200 adafactor.py:358] Since rank of parameter decoder/layers_18/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.686481 140440593786880 adafactor.py:358] Since rank of parameter decoder/layers_11/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.625157 140339465779200 adafactor.py:358] Since rank of parameter decoder/layers_18/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:13.626233 140339465779200 adafactor.py:358] Since rank of parameter decoder/layers_18/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.627308 140339465779200 adafactor.py:358] Since rank of parameter decoder/layers_18/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.628413 140339465779200 adafactor.py:358] Since rank of parameter decoder/layers_19/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.629538 140339465779200 adafactor.py:358] Since rank of parameter decoder/layers_19/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.666753 140311617681408 adafactor.py:358] Since rank of parameter decoder/layers_10/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:13.630615 140339465779200 adafactor.py:358] Since rank of parameter decoder/layers_19/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:13.631686 140339465779200 adafactor.py:358] Since rank of parameter decoder/layers_19/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:13.632588 140339465779200 adafactor.py:358] Since rank of parameter decoder/layers_19/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:13.633464 140339465779200 adafactor.py:358] Since rank of parameter decoder/layers_19/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.634605 140339465779200 adafactor.py:358] Since rank of parameter decoder/layers_19/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:13.635684 140339465779200 adafactor.py:358] Since rank of parameter decoder/layers_19/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.636793 140339465779200 adafactor.py:358] Since rank of parameter decoder/layers_19/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.637922 140339465779200 adafactor.py:358] Since rank of parameter decoder/layers_2/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.667620 140311617681408 adafactor.py:358] Since rank of parameter decoder/layers_10/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.687622 140440593786880 adafactor.py:358] Since rank of parameter decoder/layers_12/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.639002 140339465779200 adafactor.py:358] Since rank of parameter decoder/layers_2/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.640071 140339465779200 adafactor.py:358] Since rank of parameter decoder/layers_2/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:13.641163 140339465779200 adafactor.py:358] Since rank of parameter decoder/layers_2/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:13.642049 140339465779200 adafactor.py:358] Since rank of parameter decoder/layers_2/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:13.643415 140339465779200 adafactor.py:358] Since rank of parameter decoder/layers_2/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.644547 140339465779200 adafactor.py:358] Since rank of parameter decoder/layers_2/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:13.645628 140339465779200 adafactor.py:358] Since rank of parameter decoder/layers_2/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.646696 140339465779200 adafactor.py:358] Since rank of parameter decoder/layers_2/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.647827 140339465779200 adafactor.py:358] Since rank of parameter decoder/layers_20/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.668685 140311617681408 adafactor.py:358] Since rank of parameter decoder/layers_10/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:13.688751 140440593786880 adafactor.py:358] Since rank of parameter decoder/layers_12/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.648924 140339465779200 adafactor.py:358] Since rank of parameter decoder/layers_20/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.649990 140339465779200 adafactor.py:358] Since rank of parameter decoder/layers_20/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:13.651051 140339465779200 adafactor.py:358] Since rank of parameter decoder/layers_20/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:13.652001 140339465779200 adafactor.py:358] Since rank of parameter decoder/layers_20/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:13.652916 140339465779200 adafactor.py:358] Since rank of parameter decoder/layers_20/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.653971 140339465779200 adafactor.py:358] Since rank of parameter decoder/layers_20/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:13.655040 140339465779200 adafactor.py:358] Since rank of parameter decoder/layers_20/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.656183 140339465779200 adafactor.py:358] Since rank of parameter decoder/layers_20/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.657253 140339465779200 adafactor.py:358] Since rank of parameter decoder/layers_21/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.669847 140311617681408 adafactor.py:358] Since rank of parameter decoder/layers_10/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.689841 140440593786880 adafactor.py:358] Since rank of parameter decoder/layers_12/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:13.658324 140339465779200 adafactor.py:358] Since rank of parameter decoder/layers_21/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.659399 140339465779200 adafactor.py:358] Since rank of parameter decoder/layers_21/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:13.660547 140339465779200 adafactor.py:358] Since rank of parameter decoder/layers_21/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:13.661417 140339465779200 adafactor.py:358] Since rank of parameter decoder/layers_21/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:13.662289 140339465779200 adafactor.py:358] Since rank of parameter decoder/layers_21/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.663368 140339465779200 adafactor.py:358] Since rank of parameter decoder/layers_21/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:13.664541 140339465779200 adafactor.py:358] Since rank of parameter decoder/layers_21/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.665622 140339465779200 adafactor.py:358] Since rank of parameter decoder/layers_21/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.670921 140311617681408 adafactor.py:358] Since rank of parameter decoder/layers_10/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.691001 140440593786880 adafactor.py:358] Since rank of parameter decoder/layers_12/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:13.666676 140339465779200 adafactor.py:358] Since rank of parameter decoder/layers_22/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.667743 140339465779200 adafactor.py:358] Since rank of parameter decoder/layers_22/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.668895 140339465779200 adafactor.py:358] Since rank of parameter decoder/layers_22/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:13.669959 140339465779200 adafactor.py:358] Since rank of parameter decoder/layers_22/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:13.670836 140339465779200 adafactor.py:358] Since rank of parameter decoder/layers_22/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:13.671717 140339465779200 adafactor.py:358] Since rank of parameter decoder/layers_22/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.672865 140339465779200 adafactor.py:358] Since rank of parameter decoder/layers_22/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:13.673929 140339465779200 adafactor.py:358] Since rank of parameter decoder/layers_22/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.674991 140339465779200 adafactor.py:358] Since rank of parameter decoder/layers_22/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.691890 140440593786880 adafactor.py:358] Since rank of parameter decoder/layers_12/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:13.671991 140311617681408 adafactor.py:358] Since rank of parameter decoder/layers_11/extra_mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.676060 140339465779200 adafactor.py:358] Since rank of parameter decoder/layers_23/extra_mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.677211 140339465779200 adafactor.py:358] Since rank of parameter decoder/layers_23/extra_mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.678266 140339465779200 adafactor.py:358] Since rank of parameter decoder/layers_23/extra_mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:13.692824 140440593786880 adafactor.py:358] Since rank of parameter decoder/layers_12/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.673070 140311617681408 adafactor.py:358] Since rank of parameter decoder/layers_11/extra_mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.682277 140339465779200 adafactor.py:358] Since rank of parameter decoder/layers_23/mlp/router/router_weights/w/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>, <FactorDim.NONE: None>) is ignored.
W0512 23:42:13.683160 140339465779200 adafactor.py:358] Since rank of parameter decoder/layers_23/pre_extra_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:13.684051 140339465779200 adafactor.py:358] Since rank of parameter decoder/layers_23/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:13.684954 140339465779200 adafactor.py:358] Since rank of parameter decoder/layers_23/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:13.685879 140339465779200 adafactor.py:358] Since rank of parameter decoder/layers_23/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.686942 140339465779200 adafactor.py:358] Since rank of parameter decoder/layers_23/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:13.688193 140339465779200 adafactor.py:358] Since rank of parameter decoder/layers_23/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.689258 140339465779200 adafactor.py:358] Since rank of parameter decoder/layers_23/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.693914 140440593786880 adafactor.py:358] Since rank of parameter decoder/layers_12/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:13.674759 140311617681408 adafactor.py:358] Since rank of parameter decoder/layers_11/extra_mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:13.690370 140339465779200 adafactor.py:358] Since rank of parameter decoder/layers_3/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.691441 140339465779200 adafactor.py:358] Since rank of parameter decoder/layers_3/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.692529 140339465779200 adafactor.py:358] Since rank of parameter decoder/layers_3/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:13.693584 140339465779200 adafactor.py:358] Since rank of parameter decoder/layers_3/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:13.695091 140440593786880 adafactor.py:358] Since rank of parameter decoder/layers_12/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.694936 140339465779200 adafactor.py:358] Since rank of parameter decoder/layers_3/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:13.696189 140440593786880 adafactor.py:358] Since rank of parameter decoder/layers_12/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.695832 140339465779200 adafactor.py:358] Since rank of parameter decoder/layers_3/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.697324 140440593786880 adafactor.py:358] Since rank of parameter decoder/layers_13/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.697011 140339465779200 adafactor.py:358] Since rank of parameter decoder/layers_3/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:13.698510 140440593786880 adafactor.py:358] Since rank of parameter decoder/layers_13/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.678855 140311617681408 adafactor.py:358] Since rank of parameter decoder/layers_11/mlp/router/router_weights/w/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>, <FactorDim.NONE: None>) is ignored.
W0512 23:42:13.698157 140339465779200 adafactor.py:358] Since rank of parameter decoder/layers_3/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.679751 140311617681408 adafactor.py:358] Since rank of parameter decoder/layers_11/pre_extra_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:13.699610 140440593786880 adafactor.py:358] Since rank of parameter decoder/layers_13/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:13.701671 140095893518336 adafactor.py:358] Since rank of parameter decoder/decoder_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:13.702766 140095893518336 adafactor.py:358] Since rank of parameter decoder/layers_0/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.699358 140339465779200 adafactor.py:358] Since rank of parameter decoder/layers_3/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.700755 140440593786880 adafactor.py:358] Since rank of parameter decoder/layers_13/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:13.680618 140311617681408 adafactor.py:358] Since rank of parameter decoder/layers_11/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:13.681555 140311617681408 adafactor.py:358] Since rank of parameter decoder/layers_11/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:13.703960 140095893518336 adafactor.py:358] Since rank of parameter decoder/layers_0/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.700529 140339465779200 adafactor.py:358] Since rank of parameter decoder/layers_4/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.702143 140440593786880 adafactor.py:358] Since rank of parameter decoder/layers_13/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:13.682459 140311617681408 adafactor.py:358] Since rank of parameter decoder/layers_11/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.705103 140095893518336 adafactor.py:358] Since rank of parameter decoder/layers_0/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:13.703089 140440593786880 adafactor.py:358] Since rank of parameter decoder/layers_13/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.701675 140339465779200 adafactor.py:358] Since rank of parameter decoder/layers_4/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.683542 140311617681408 adafactor.py:358] Since rank of parameter decoder/layers_11/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:13.704199 140440593786880 adafactor.py:358] Since rank of parameter decoder/layers_13/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:13.706345 140095893518336 adafactor.py:358] Since rank of parameter decoder/layers_0/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:13.702808 140339465779200 adafactor.py:358] Since rank of parameter decoder/layers_4/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:13.684610 140311617681408 adafactor.py:358] Since rank of parameter decoder/layers_11/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.707253 140095893518336 adafactor.py:358] Since rank of parameter decoder/layers_0/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:13.705342 140440593786880 adafactor.py:358] Since rank of parameter decoder/layers_13/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.704012 140339465779200 adafactor.py:358] Since rank of parameter decoder/layers_4/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:13.685750 140311617681408 adafactor.py:358] Since rank of parameter decoder/layers_11/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.708163 140095893518336 adafactor.py:358] Since rank of parameter decoder/layers_0/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.704974 140339465779200 adafactor.py:358] Since rank of parameter decoder/layers_4/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:13.706486 140440593786880 adafactor.py:358] Since rank of parameter decoder/layers_13/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.686885 140311617681408 adafactor.py:358] Since rank of parameter decoder/layers_12/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.709279 140095893518336 adafactor.py:358] Since rank of parameter decoder/layers_0/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:13.705899 140339465779200 adafactor.py:358] Since rank of parameter decoder/layers_4/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.707674 140440593786880 adafactor.py:358] Since rank of parameter decoder/layers_14/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.687960 140311617681408 adafactor.py:358] Since rank of parameter decoder/layers_12/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.707040 140339465779200 adafactor.py:358] Since rank of parameter decoder/layers_4/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:13.710484 140095893518336 adafactor.py:358] Since rank of parameter decoder/layers_0/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.708838 140440593786880 adafactor.py:358] Since rank of parameter decoder/layers_14/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.689036 140311617681408 adafactor.py:358] Since rank of parameter decoder/layers_12/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:13.711576 140095893518336 adafactor.py:358] Since rank of parameter decoder/layers_0/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.708257 140339465779200 adafactor.py:358] Since rank of parameter decoder/layers_4/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.709967 140440593786880 adafactor.py:358] Since rank of parameter decoder/layers_14/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:13.690194 140311617681408 adafactor.py:358] Since rank of parameter decoder/layers_12/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:13.712664 140095893518336 adafactor.py:358] Since rank of parameter decoder/layers_1/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.709390 140339465779200 adafactor.py:358] Since rank of parameter decoder/layers_4/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.691072 140311617681408 adafactor.py:358] Since rank of parameter decoder/layers_12/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:13.711172 140440593786880 adafactor.py:358] Since rank of parameter decoder/layers_14/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:13.713803 140095893518336 adafactor.py:358] Since rank of parameter decoder/layers_1/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.691946 140311617681408 adafactor.py:358] Since rank of parameter decoder/layers_12/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.710515 140339465779200 adafactor.py:358] Since rank of parameter decoder/layers_5/extra_mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.712096 140440593786880 adafactor.py:358] Since rank of parameter decoder/layers_14/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:13.714937 140095893518336 adafactor.py:358] Since rank of parameter decoder/layers_1/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:13.693009 140311617681408 adafactor.py:358] Since rank of parameter decoder/layers_12/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:13.711665 140339465779200 adafactor.py:358] Since rank of parameter decoder/layers_5/extra_mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.716011 140095893518336 adafactor.py:358] Since rank of parameter decoder/layers_1/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:13.694200 140311617681408 adafactor.py:358] Since rank of parameter decoder/layers_12/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.712896 140339465779200 adafactor.py:358] Since rank of parameter decoder/layers_5/extra_mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:13.716894 140095893518336 adafactor.py:358] Since rank of parameter decoder/layers_1/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:13.695276 140311617681408 adafactor.py:358] Since rank of parameter decoder/layers_12/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.717832 140095893518336 adafactor.py:358] Since rank of parameter decoder/layers_1/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.696348 140311617681408 adafactor.py:358] Since rank of parameter decoder/layers_13/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.718939 140095893518336 adafactor.py:358] Since rank of parameter decoder/layers_1/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:13.697474 140311617681408 adafactor.py:358] Since rank of parameter decoder/layers_13/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.720007 140095893518336 adafactor.py:358] Since rank of parameter decoder/layers_1/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.717242 140339465779200 adafactor.py:358] Since rank of parameter decoder/layers_5/mlp/router/router_weights/w/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>, <FactorDim.NONE: None>) is ignored.
W0512 23:42:13.698588 140311617681408 adafactor.py:358] Since rank of parameter decoder/layers_13/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:13.721093 140095893518336 adafactor.py:358] Since rank of parameter decoder/layers_1/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.718197 140339465779200 adafactor.py:358] Since rank of parameter decoder/layers_5/pre_extra_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:13.699671 140311617681408 adafactor.py:358] Since rank of parameter decoder/layers_13/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:13.722209 140095893518336 adafactor.py:358] Since rank of parameter decoder/layers_10/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.719126 140339465779200 adafactor.py:358] Since rank of parameter decoder/layers_5/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:13.701202 140311617681408 adafactor.py:358] Since rank of parameter decoder/layers_13/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:13.720042 140339465779200 adafactor.py:358] Since rank of parameter decoder/layers_5/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:13.723299 140095893518336 adafactor.py:358] Since rank of parameter decoder/layers_10/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.702169 140311617681408 adafactor.py:358] Since rank of parameter decoder/layers_13/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.721021 140339465779200 adafactor.py:358] Since rank of parameter decoder/layers_5/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.724370 140095893518336 adafactor.py:358] Since rank of parameter decoder/layers_10/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:13.703299 140311617681408 adafactor.py:358] Since rank of parameter decoder/layers_13/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:13.722225 140339465779200 adafactor.py:358] Since rank of parameter decoder/layers_5/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:13.725474 140095893518336 adafactor.py:358] Since rank of parameter decoder/layers_10/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:13.726366 140095893518336 adafactor.py:358] Since rank of parameter decoder/layers_10/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:13.704421 140311617681408 adafactor.py:358] Since rank of parameter decoder/layers_13/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.723331 140339465779200 adafactor.py:358] Since rank of parameter decoder/layers_5/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.727232 140095893518336 adafactor.py:358] Since rank of parameter decoder/layers_10/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.705543 140311617681408 adafactor.py:358] Since rank of parameter decoder/layers_13/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.724461 140339465779200 adafactor.py:358] Since rank of parameter decoder/layers_5/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.728291 140095893518336 adafactor.py:358] Since rank of parameter decoder/layers_10/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:13.706764 140311617681408 adafactor.py:358] Since rank of parameter decoder/layers_14/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.725516 140339465779200 adafactor.py:358] Since rank of parameter decoder/layers_6/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.729406 140095893518336 adafactor.py:358] Since rank of parameter decoder/layers_10/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.707892 140311617681408 adafactor.py:358] Since rank of parameter decoder/layers_14/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.727972 140595318249472 adafactor.py:358] Since rank of parameter decoder/decoder_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:13.726640 140339465779200 adafactor.py:358] Since rank of parameter decoder/layers_6/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.730493 140095893518336 adafactor.py:358] Since rank of parameter decoder/layers_10/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.709007 140311617681408 adafactor.py:358] Since rank of parameter decoder/layers_14/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:13.729046 140595318249472 adafactor.py:358] Since rank of parameter decoder/layers_0/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.727730 140339465779200 adafactor.py:358] Since rank of parameter decoder/layers_6/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:13.731562 140095893518336 adafactor.py:358] Since rank of parameter decoder/layers_11/extra_mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.629574 139674595104768 adafactor.py:358] Since rank of parameter decoder/layers_14/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.630656 139674595104768 adafactor.py:358] Since rank of parameter decoder/layers_14/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.631790 139674595104768 adafactor.py:358] Since rank of parameter decoder/layers_14/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:13.632947 139674595104768 adafactor.py:358] Since rank of parameter decoder/layers_14/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:13.633828 139674595104768 adafactor.py:358] Since rank of parameter decoder/layers_14/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:13.634705 139674595104768 adafactor.py:358] Since rank of parameter decoder/layers_14/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.635815 139674595104768 adafactor.py:358] Since rank of parameter decoder/layers_14/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:13.636974 139674595104768 adafactor.py:358] Since rank of parameter decoder/layers_14/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.710209 140311617681408 adafactor.py:358] Since rank of parameter decoder/layers_14/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:13.728834 140339465779200 adafactor.py:358] Since rank of parameter decoder/layers_6/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:13.730246 140595318249472 adafactor.py:358] Since rank of parameter decoder/layers_0/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.638064 139674595104768 adafactor.py:358] Since rank of parameter decoder/layers_14/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.639148 139674595104768 adafactor.py:358] Since rank of parameter decoder/layers_15/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.640254 139674595104768 adafactor.py:358] Since rank of parameter decoder/layers_15/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.641381 139674595104768 adafactor.py:358] Since rank of parameter decoder/layers_15/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:13.642484 139674595104768 adafactor.py:358] Since rank of parameter decoder/layers_15/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:13.643407 139674595104768 adafactor.py:358] Since rank of parameter decoder/layers_15/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:13.644973 139674595104768 adafactor.py:358] Since rank of parameter decoder/layers_15/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.646086 139674595104768 adafactor.py:358] Since rank of parameter decoder/layers_15/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:13.647194 139674595104768 adafactor.py:358] Since rank of parameter decoder/layers_15/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.732630 140095893518336 adafactor.py:358] Since rank of parameter decoder/layers_11/extra_mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.729703 140339465779200 adafactor.py:358] Since rank of parameter decoder/layers_6/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:13.711131 140311617681408 adafactor.py:358] Since rank of parameter decoder/layers_14/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:13.731338 140595318249472 adafactor.py:358] Since rank of parameter decoder/layers_0/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:13.648307 139674595104768 adafactor.py:358] Since rank of parameter decoder/layers_15/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.649432 139674595104768 adafactor.py:358] Since rank of parameter decoder/layers_16/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.650521 139674595104768 adafactor.py:358] Since rank of parameter decoder/layers_16/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.651630 139674595104768 adafactor.py:358] Since rank of parameter decoder/layers_16/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:13.652707 139674595104768 adafactor.py:358] Since rank of parameter decoder/layers_16/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:13.653647 139674595104768 adafactor.py:358] Since rank of parameter decoder/layers_16/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:13.654527 139674595104768 adafactor.py:358] Since rank of parameter decoder/layers_16/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.655637 139674595104768 adafactor.py:358] Since rank of parameter decoder/layers_16/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:13.656721 139674595104768 adafactor.py:358] Since rank of parameter decoder/layers_16/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.730630 140339465779200 adafactor.py:358] Since rank of parameter decoder/layers_6/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.734277 140095893518336 adafactor.py:358] Since rank of parameter decoder/layers_11/extra_mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:13.732537 140595318249472 adafactor.py:358] Since rank of parameter decoder/layers_0/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:13.657803 139674595104768 adafactor.py:358] Since rank of parameter decoder/layers_16/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.658929 139674595104768 adafactor.py:358] Since rank of parameter decoder/layers_17/extra_mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.660061 139674595104768 adafactor.py:358] Since rank of parameter decoder/layers_17/extra_mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.661129 139674595104768 adafactor.py:358] Since rank of parameter decoder/layers_17/extra_mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:13.665222 139674595104768 adafactor.py:358] Since rank of parameter decoder/layers_17/mlp/router/router_weights/w/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>, <FactorDim.NONE: None>) is ignored.
W0512 23:42:13.666128 139674595104768 adafactor.py:358] Since rank of parameter decoder/layers_17/pre_extra_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:13.667066 139674595104768 adafactor.py:358] Since rank of parameter decoder/layers_17/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:13.667982 139674595104768 adafactor.py:358] Since rank of parameter decoder/layers_17/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:13.731708 140339465779200 adafactor.py:358] Since rank of parameter decoder/layers_6/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:13.733455 140595318249472 adafactor.py:358] Since rank of parameter decoder/layers_0/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:13.668860 139674595104768 adafactor.py:358] Since rank of parameter decoder/layers_17/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.669955 139674595104768 adafactor.py:358] Since rank of parameter decoder/layers_17/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:13.671085 139674595104768 adafactor.py:358] Since rank of parameter decoder/layers_17/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.672204 139674595104768 adafactor.py:358] Since rank of parameter decoder/layers_17/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.673278 139674595104768 adafactor.py:358] Since rank of parameter decoder/layers_18/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.674356 139674595104768 adafactor.py:358] Since rank of parameter decoder/layers_18/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.675527 139674595104768 adafactor.py:358] Since rank of parameter decoder/layers_18/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:13.676602 139674595104768 adafactor.py:358] Since rank of parameter decoder/layers_18/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:13.677489 139674595104768 adafactor.py:358] Since rank of parameter decoder/layers_18/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:13.734343 140595318249472 adafactor.py:358] Since rank of parameter decoder/layers_0/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.732810 140339465779200 adafactor.py:358] Since rank of parameter decoder/layers_6/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.678383 139674595104768 adafactor.py:358] Since rank of parameter decoder/layers_18/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.679591 139674595104768 adafactor.py:358] Since rank of parameter decoder/layers_18/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:13.680679 139674595104768 adafactor.py:358] Since rank of parameter decoder/layers_18/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.681762 139674595104768 adafactor.py:358] Since rank of parameter decoder/layers_18/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.682881 139674595104768 adafactor.py:358] Since rank of parameter decoder/layers_19/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.684048 139674595104768 adafactor.py:358] Since rank of parameter decoder/layers_19/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.685137 139674595104768 adafactor.py:358] Since rank of parameter decoder/layers_19/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:13.686224 139674595104768 adafactor.py:358] Since rank of parameter decoder/layers_19/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:13.687114 139674595104768 adafactor.py:358] Since rank of parameter decoder/layers_19/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:13.735455 140595318249472 adafactor.py:358] Since rank of parameter decoder/layers_0/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:13.733883 140339465779200 adafactor.py:358] Since rank of parameter decoder/layers_6/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.738311 140095893518336 adafactor.py:358] Since rank of parameter decoder/layers_11/mlp/router/router_weights/w/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>, <FactorDim.NONE: None>) is ignored.
W0512 23:42:13.688029 139674595104768 adafactor.py:358] Since rank of parameter decoder/layers_19/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.689180 139674595104768 adafactor.py:358] Since rank of parameter decoder/layers_19/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:13.690264 139674595104768 adafactor.py:358] Since rank of parameter decoder/layers_19/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.691373 139674595104768 adafactor.py:358] Since rank of parameter decoder/layers_19/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.692519 139674595104768 adafactor.py:358] Since rank of parameter decoder/layers_2/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.693609 139674595104768 adafactor.py:358] Since rank of parameter decoder/layers_2/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.694684 139674595104768 adafactor.py:358] Since rank of parameter decoder/layers_2/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:13.695788 139674595104768 adafactor.py:358] Since rank of parameter decoder/layers_2/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:13.736607 140595318249472 adafactor.py:358] Since rank of parameter decoder/layers_0/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.734987 140339465779200 adafactor.py:358] Since rank of parameter decoder/layers_7/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.739190 140095893518336 adafactor.py:358] Since rank of parameter decoder/layers_11/pre_extra_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:13.696675 139674595104768 adafactor.py:358] Since rank of parameter decoder/layers_2/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:13.698042 139674595104768 adafactor.py:358] Since rank of parameter decoder/layers_2/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.699122 139674595104768 adafactor.py:358] Since rank of parameter decoder/layers_2/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:13.700228 139674595104768 adafactor.py:358] Since rank of parameter decoder/layers_2/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.701302 139674595104768 adafactor.py:358] Since rank of parameter decoder/layers_2/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.702476 139674595104768 adafactor.py:358] Since rank of parameter decoder/layers_20/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.703595 139674595104768 adafactor.py:358] Since rank of parameter decoder/layers_20/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.704670 139674595104768 adafactor.py:358] Since rank of parameter decoder/layers_20/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:13.705762 139674595104768 adafactor.py:358] Since rank of parameter decoder/layers_20/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:13.736049 140339465779200 adafactor.py:358] Since rank of parameter decoder/layers_7/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.737717 140595318249472 adafactor.py:358] Since rank of parameter decoder/layers_0/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.740061 140095893518336 adafactor.py:358] Since rank of parameter decoder/layers_11/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:13.706703 139674595104768 adafactor.py:358] Since rank of parameter decoder/layers_20/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:13.707623 139674595104768 adafactor.py:358] Since rank of parameter decoder/layers_20/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.708714 139674595104768 adafactor.py:358] Since rank of parameter decoder/layers_20/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:13.709793 139674595104768 adafactor.py:358] Since rank of parameter decoder/layers_20/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.710939 139674595104768 adafactor.py:358] Since rank of parameter decoder/layers_20/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.712062 139674595104768 adafactor.py:358] Since rank of parameter decoder/layers_21/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.713149 139674595104768 adafactor.py:358] Since rank of parameter decoder/layers_21/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.714234 139674595104768 adafactor.py:358] Since rank of parameter decoder/layers_21/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:13.715408 139674595104768 adafactor.py:358] Since rank of parameter decoder/layers_21/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:13.737146 140339465779200 adafactor.py:358] Since rank of parameter decoder/layers_7/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:13.740974 140095893518336 adafactor.py:358] Since rank of parameter decoder/layers_11/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:13.738801 140595318249472 adafactor.py:358] Since rank of parameter decoder/layers_1/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.716289 139674595104768 adafactor.py:358] Since rank of parameter decoder/layers_21/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:13.717175 139674595104768 adafactor.py:358] Since rank of parameter decoder/layers_21/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.718263 139674595104768 adafactor.py:358] Since rank of parameter decoder/layers_21/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:13.719427 139674595104768 adafactor.py:358] Since rank of parameter decoder/layers_21/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.720517 139674595104768 adafactor.py:358] Since rank of parameter decoder/layers_21/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.721591 139674595104768 adafactor.py:358] Since rank of parameter decoder/layers_22/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.722695 139674595104768 adafactor.py:358] Since rank of parameter decoder/layers_22/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.723863 139674595104768 adafactor.py:358] Since rank of parameter decoder/layers_22/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:13.738206 140339465779200 adafactor.py:358] Since rank of parameter decoder/layers_7/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:13.741860 140095893518336 adafactor.py:358] Since rank of parameter decoder/layers_11/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.739923 140595318249472 adafactor.py:358] Since rank of parameter decoder/layers_1/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.724949 139674595104768 adafactor.py:358] Since rank of parameter decoder/layers_22/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:13.725837 139674595104768 adafactor.py:358] Since rank of parameter decoder/layers_22/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:13.726718 139674595104768 adafactor.py:358] Since rank of parameter decoder/layers_22/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.727887 139674595104768 adafactor.py:358] Since rank of parameter decoder/layers_22/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:13.728959 139674595104768 adafactor.py:358] Since rank of parameter decoder/layers_22/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.730027 139674595104768 adafactor.py:358] Since rank of parameter decoder/layers_22/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.739135 140339465779200 adafactor.py:358] Since rank of parameter decoder/layers_7/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:13.742958 140095893518336 adafactor.py:358] Since rank of parameter decoder/layers_11/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:13.741015 140595318249472 adafactor.py:358] Since rank of parameter decoder/layers_1/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:13.740003 140339465779200 adafactor.py:358] Since rank of parameter decoder/layers_7/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.731098 139674595104768 adafactor.py:358] Since rank of parameter decoder/layers_23/extra_mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.732242 139674595104768 adafactor.py:358] Since rank of parameter decoder/layers_23/extra_mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.733318 139674595104768 adafactor.py:358] Since rank of parameter decoder/layers_23/extra_mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:13.737699 139674595104768 adafactor.py:358] Since rank of parameter decoder/layers_23/mlp/router/router_weights/w/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>, <FactorDim.NONE: None>) is ignored.
W0512 23:42:13.738601 139674595104768 adafactor.py:358] Since rank of parameter decoder/layers_23/pre_extra_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:13.739530 139674595104768 adafactor.py:358] Since rank of parameter decoder/layers_23/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:13.740406 139674595104768 adafactor.py:358] Since rank of parameter decoder/layers_23/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:13.741340 139674595104768 adafactor.py:358] Since rank of parameter decoder/layers_23/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.744047 140095893518336 adafactor.py:358] Since rank of parameter decoder/layers_11/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.742083 140595318249472 adafactor.py:358] Since rank of parameter decoder/layers_1/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:13.741098 140339465779200 adafactor.py:358] Since rank of parameter decoder/layers_7/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:13.742450 139674595104768 adafactor.py:358] Since rank of parameter decoder/layers_23/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:13.743700 139674595104768 adafactor.py:358] Since rank of parameter decoder/layers_23/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.742954 140595318249472 adafactor.py:358] Since rank of parameter decoder/layers_1/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:13.745164 140095893518336 adafactor.py:358] Since rank of parameter decoder/layers_11/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.744812 139674595104768 adafactor.py:358] Since rank of parameter decoder/layers_23/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.742153 140339465779200 adafactor.py:358] Since rank of parameter decoder/layers_7/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.743924 140595318249472 adafactor.py:358] Since rank of parameter decoder/layers_1/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.746303 140095893518336 adafactor.py:358] Since rank of parameter decoder/layers_12/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.745976 139674595104768 adafactor.py:358] Since rank of parameter decoder/layers_3/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.743270 140339465779200 adafactor.py:358] Since rank of parameter decoder/layers_7/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.745043 140595318249472 adafactor.py:358] Since rank of parameter decoder/layers_1/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:13.747376 140095893518336 adafactor.py:358] Since rank of parameter decoder/layers_12/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.747088 139674595104768 adafactor.py:358] Since rank of parameter decoder/layers_3/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.744399 140339465779200 adafactor.py:358] Since rank of parameter decoder/layers_8/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.746127 140595318249472 adafactor.py:358] Since rank of parameter decoder/layers_1/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.748450 140095893518336 adafactor.py:358] Since rank of parameter decoder/layers_12/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:13.748183 139674595104768 adafactor.py:358] Since rank of parameter decoder/layers_3/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:13.745460 140339465779200 adafactor.py:358] Since rank of parameter decoder/layers_8/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.747204 140595318249472 adafactor.py:358] Since rank of parameter decoder/layers_1/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.749577 140095893518336 adafactor.py:358] Since rank of parameter decoder/layers_12/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:13.749252 139674595104768 adafactor.py:358] Since rank of parameter decoder/layers_3/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:13.746530 140339465779200 adafactor.py:358] Since rank of parameter decoder/layers_8/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:13.750475 140095893518336 adafactor.py:358] Since rank of parameter decoder/layers_12/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:13.748344 140595318249472 adafactor.py:358] Since rank of parameter decoder/layers_10/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.751351 140095893518336 adafactor.py:358] Since rank of parameter decoder/layers_12/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.750681 139674595104768 adafactor.py:358] Since rank of parameter decoder/layers_3/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:13.748086 140339465779200 adafactor.py:358] Since rank of parameter decoder/layers_8/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:13.749457 140595318249472 adafactor.py:358] Since rank of parameter decoder/layers_10/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.748984 140339465779200 adafactor.py:358] Since rank of parameter decoder/layers_8/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:13.752414 140095893518336 adafactor.py:358] Since rank of parameter decoder/layers_12/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:13.751587 139674595104768 adafactor.py:358] Since rank of parameter decoder/layers_3/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.750542 140595318249472 adafactor.py:358] Since rank of parameter decoder/layers_10/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:13.749877 140339465779200 adafactor.py:358] Since rank of parameter decoder/layers_8/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.752657 139674595104768 adafactor.py:358] Since rank of parameter decoder/layers_3/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:13.753533 140095893518336 adafactor.py:358] Since rank of parameter decoder/layers_12/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.751675 140595318249472 adafactor.py:358] Since rank of parameter decoder/layers_10/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:13.750950 140339465779200 adafactor.py:358] Since rank of parameter decoder/layers_8/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:13.753722 139674595104768 adafactor.py:358] Since rank of parameter decoder/layers_3/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.752547 140595318249472 adafactor.py:358] Since rank of parameter decoder/layers_10/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:13.754654 140095893518336 adafactor.py:358] Since rank of parameter decoder/layers_12/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.753454 140595318249472 adafactor.py:358] Since rank of parameter decoder/layers_10/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.752018 140339465779200 adafactor.py:358] Since rank of parameter decoder/layers_8/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.754882 139674595104768 adafactor.py:358] Since rank of parameter decoder/layers_3/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.755727 140095893518336 adafactor.py:358] Since rank of parameter decoder/layers_13/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.754523 140595318249472 adafactor.py:358] Since rank of parameter decoder/layers_10/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:13.753154 140339465779200 adafactor.py:358] Since rank of parameter decoder/layers_8/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.756021 139674595104768 adafactor.py:358] Since rank of parameter decoder/layers_4/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.756844 140095893518336 adafactor.py:358] Since rank of parameter decoder/layers_13/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.755645 140595318249472 adafactor.py:358] Since rank of parameter decoder/layers_10/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.754241 140339465779200 adafactor.py:358] Since rank of parameter decoder/layers_9/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.757139 139674595104768 adafactor.py:358] Since rank of parameter decoder/layers_4/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.757901 140095893518336 adafactor.py:358] Since rank of parameter decoder/layers_13/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:13.756721 140595318249472 adafactor.py:358] Since rank of parameter decoder/layers_10/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.758244 139674595104768 adafactor.py:358] Since rank of parameter decoder/layers_4/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:13.759002 140095893518336 adafactor.py:358] Since rank of parameter decoder/layers_13/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:13.755297 140339465779200 adafactor.py:358] Since rank of parameter decoder/layers_9/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.757822 140595318249472 adafactor.py:358] Since rank of parameter decoder/layers_11/extra_mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.759392 139674595104768 adafactor.py:358] Since rank of parameter decoder/layers_4/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:13.760329 140095893518336 adafactor.py:358] Since rank of parameter decoder/layers_13/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:13.756435 140339465779200 adafactor.py:358] Since rank of parameter decoder/layers_9/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:13.760286 139674595104768 adafactor.py:358] Since rank of parameter decoder/layers_4/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:13.758892 140595318249472 adafactor.py:358] Since rank of parameter decoder/layers_11/extra_mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.761285 140095893518336 adafactor.py:358] Since rank of parameter decoder/layers_13/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.757488 140339465779200 adafactor.py:358] Since rank of parameter decoder/layers_9/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:13.758351 140339465779200 adafactor.py:358] Since rank of parameter decoder/layers_9/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:13.761159 139674595104768 adafactor.py:358] Since rank of parameter decoder/layers_4/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.762438 140095893518336 adafactor.py:358] Since rank of parameter decoder/layers_13/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:13.759223 140339465779200 adafactor.py:358] Since rank of parameter decoder/layers_9/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.760547 140595318249472 adafactor.py:358] Since rank of parameter decoder/layers_11/extra_mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:13.762242 139674595104768 adafactor.py:358] Since rank of parameter decoder/layers_4/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:13.763555 140095893518336 adafactor.py:358] Since rank of parameter decoder/layers_13/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.760321 140339465779200 adafactor.py:358] Since rank of parameter decoder/layers_9/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:13.763385 139674595104768 adafactor.py:358] Since rank of parameter decoder/layers_4/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.764643 140095893518336 adafactor.py:358] Since rank of parameter decoder/layers_13/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.761430 140339465779200 adafactor.py:358] Since rank of parameter decoder/layers_9/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.764451 139674595104768 adafactor.py:358] Since rank of parameter decoder/layers_4/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.765764 140095893518336 adafactor.py:358] Since rank of parameter decoder/layers_14/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.762483 140339465779200 adafactor.py:358] Since rank of parameter decoder/layers_9/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.765516 139674595104768 adafactor.py:358] Since rank of parameter decoder/layers_5/extra_mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.764705 140595318249472 adafactor.py:358] Since rank of parameter decoder/layers_11/mlp/router/router_weights/w/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>, <FactorDim.NONE: None>) is ignored.
W0512 23:42:13.766858 140095893518336 adafactor.py:358] Since rank of parameter decoder/layers_14/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.763570 140339465779200 adafactor.py:358] Since rank of parameter decoder/logits_dense/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.766569 139674595104768 adafactor.py:358] Since rank of parameter decoder/layers_5/extra_mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.765653 140595318249472 adafactor.py:358] Since rank of parameter decoder/layers_11/pre_extra_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:13.767924 140095893518336 adafactor.py:358] Since rank of parameter decoder/layers_14/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:13.764678 140339465779200 adafactor.py:358] Since rank of parameter token_embedder/embedding 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:13.767735 139674595104768 adafactor.py:358] Since rank of parameter decoder/layers_5/extra_mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:13.766543 140595318249472 adafactor.py:358] Since rank of parameter decoder/layers_11/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:13.769045 140095893518336 adafactor.py:358] Since rank of parameter decoder/layers_14/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:13.767478 140595318249472 adafactor.py:358] Since rank of parameter decoder/layers_11/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:13.769915 140095893518336 adafactor.py:358] Since rank of parameter decoder/layers_14/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:13.768352 140595318249472 adafactor.py:358] Since rank of parameter decoder/layers_11/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.769469 140595318249472 adafactor.py:358] Since rank of parameter decoder/layers_11/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:13.771751 139674595104768 adafactor.py:358] Since rank of parameter decoder/layers_5/mlp/router/router_weights/w/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>, <FactorDim.NONE: None>) is ignored.
W0512 23:42:13.770569 140595318249472 adafactor.py:358] Since rank of parameter decoder/layers_11/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.772670 139674595104768 adafactor.py:358] Since rank of parameter decoder/layers_5/pre_extra_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:13.771716 140595318249472 adafactor.py:358] Since rank of parameter decoder/layers_11/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.773535 139674595104768 adafactor.py:358] Since rank of parameter decoder/layers_5/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:13.772867 140595318249472 adafactor.py:358] Since rank of parameter decoder/layers_12/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.774406 139674595104768 adafactor.py:358] Since rank of parameter decoder/layers_5/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:13.775309 139674595104768 adafactor.py:358] Since rank of parameter decoder/layers_5/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.773975 140595318249472 adafactor.py:358] Since rank of parameter decoder/layers_12/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.775062 140595318249472 adafactor.py:358] Since rank of parameter decoder/layers_12/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:13.776445 139674595104768 adafactor.py:358] Since rank of parameter decoder/layers_5/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:13.776208 140595318249472 adafactor.py:358] Since rank of parameter decoder/layers_12/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:13.777524 139674595104768 adafactor.py:358] Since rank of parameter decoder/layers_5/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.777106 140595318249472 adafactor.py:358] Since rank of parameter decoder/layers_12/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:13.778609 139674595104768 adafactor.py:358] Since rank of parameter decoder/layers_5/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.777998 140595318249472 adafactor.py:358] Since rank of parameter decoder/layers_12/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.779723 139674595104768 adafactor.py:358] Since rank of parameter decoder/layers_6/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.779074 140595318249472 adafactor.py:358] Since rank of parameter decoder/layers_12/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:13.780856 139674595104768 adafactor.py:358] Since rank of parameter decoder/layers_6/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.780228 140595318249472 adafactor.py:358] Since rank of parameter decoder/layers_12/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.781927 139674595104768 adafactor.py:358] Since rank of parameter decoder/layers_6/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:13.781332 140595318249472 adafactor.py:358] Since rank of parameter decoder/layers_12/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.783051 139674595104768 adafactor.py:358] Since rank of parameter decoder/layers_6/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:13.782401 140595318249472 adafactor.py:358] Since rank of parameter decoder/layers_13/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.783970 139674595104768 adafactor.py:358] Since rank of parameter decoder/layers_6/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:13.784914 139674595104768 adafactor.py:358] Since rank of parameter decoder/layers_6/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.783555 140595318249472 adafactor.py:358] Since rank of parameter decoder/layers_13/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.784684 140595318249472 adafactor.py:358] Since rank of parameter decoder/layers_13/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:13.785988 139674595104768 adafactor.py:358] Since rank of parameter decoder/layers_6/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:13.787056 139674595104768 adafactor.py:358] Since rank of parameter decoder/layers_6/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.785807 140595318249472 adafactor.py:358] Since rank of parameter decoder/layers_13/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:13.788172 139674595104768 adafactor.py:358] Since rank of parameter decoder/layers_6/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.787202 140595318249472 adafactor.py:358] Since rank of parameter decoder/layers_13/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:13.788115 140595318249472 adafactor.py:358] Since rank of parameter decoder/layers_13/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.789296 139674595104768 adafactor.py:358] Since rank of parameter decoder/layers_7/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.790365 139674595104768 adafactor.py:358] Since rank of parameter decoder/layers_7/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.789224 140595318249472 adafactor.py:358] Since rank of parameter decoder/layers_13/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:13.790305 140595318249472 adafactor.py:358] Since rank of parameter decoder/layers_13/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.791475 139674595104768 adafactor.py:358] Since rank of parameter decoder/layers_7/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:13.772404 140529434355712 adafactor.py:358] Since rank of parameter decoder/decoder_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:13.792558 139674595104768 adafactor.py:358] Since rank of parameter decoder/layers_7/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:13.791382 140595318249472 adafactor.py:358] Since rank of parameter decoder/layers_13/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.773816 140529434355712 adafactor.py:358] Since rank of parameter decoder/layers_0/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.793503 139674595104768 adafactor.py:358] Since rank of parameter decoder/layers_7/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:13.792505 140595318249472 adafactor.py:358] Since rank of parameter decoder/layers_14/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.794384 139674595104768 adafactor.py:358] Since rank of parameter decoder/layers_7/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.775091 140529434355712 adafactor.py:358] Since rank of parameter decoder/layers_0/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.795497 139674595104768 adafactor.py:358] Since rank of parameter decoder/layers_7/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:13.776204 140529434355712 adafactor.py:358] Since rank of parameter decoder/layers_0/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:13.793622 140595318249472 adafactor.py:358] Since rank of parameter decoder/layers_14/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.794687 140595318249472 adafactor.py:358] Since rank of parameter decoder/layers_14/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:13.796574 139674595104768 adafactor.py:358] Since rank of parameter decoder/layers_7/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.794661 140031916288000 adafactor.py:358] Since rank of parameter decoder/decoder_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:13.777376 140529434355712 adafactor.py:358] Since rank of parameter decoder/layers_0/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:13.795798 140595318249472 adafactor.py:358] Since rank of parameter decoder/layers_14/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:13.795729 140031916288000 adafactor.py:358] Since rank of parameter decoder/layers_0/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.778303 140529434355712 adafactor.py:358] Since rank of parameter decoder/layers_0/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:13.797702 139674595104768 adafactor.py:358] Since rank of parameter decoder/layers_7/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.796685 140595318249472 adafactor.py:358] Since rank of parameter decoder/layers_14/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:13.779183 140529434355712 adafactor.py:358] Since rank of parameter decoder/layers_0/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.797605 140595318249472 adafactor.py:358] Since rank of parameter decoder/layers_14/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.796930 140031916288000 adafactor.py:358] Since rank of parameter decoder/layers_0/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.798773 139674595104768 adafactor.py:358] Since rank of parameter decoder/layers_8/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.780259 140529434355712 adafactor.py:358] Since rank of parameter decoder/layers_0/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:13.799888 139674595104768 adafactor.py:358] Since rank of parameter decoder/layers_8/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.798106 140031916288000 adafactor.py:358] Since rank of parameter decoder/layers_0/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:13.781407 140529434355712 adafactor.py:358] Since rank of parameter decoder/layers_0/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.800970 139674595104768 adafactor.py:358] Since rank of parameter decoder/layers_8/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:13.799353 140031916288000 adafactor.py:358] Since rank of parameter decoder/layers_0/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:13.782504 140529434355712 adafactor.py:358] Since rank of parameter decoder/layers_0/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.800280 140031916288000 adafactor.py:358] Since rank of parameter decoder/layers_0/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:13.802673 139674595104768 adafactor.py:358] Since rank of parameter decoder/layers_8/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:13.783594 140529434355712 adafactor.py:358] Since rank of parameter decoder/layers_1/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.801204 140031916288000 adafactor.py:358] Since rank of parameter decoder/layers_0/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.803598 139674595104768 adafactor.py:358] Since rank of parameter decoder/layers_8/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:13.784720 140529434355712 adafactor.py:358] Since rank of parameter decoder/layers_1/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.802340 140031916288000 adafactor.py:358] Since rank of parameter decoder/layers_0/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:13.804487 139674595104768 adafactor.py:358] Since rank of parameter decoder/layers_8/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.785828 140529434355712 adafactor.py:358] Since rank of parameter decoder/layers_1/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:13.803500 140031916288000 adafactor.py:358] Since rank of parameter decoder/layers_0/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.805606 139674595104768 adafactor.py:358] Since rank of parameter decoder/layers_8/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:13.786890 140529434355712 adafactor.py:358] Since rank of parameter decoder/layers_1/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:13.804598 140031916288000 adafactor.py:358] Since rank of parameter decoder/layers_0/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.806734 139674595104768 adafactor.py:358] Since rank of parameter decoder/layers_8/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.787769 140529434355712 adafactor.py:358] Since rank of parameter decoder/layers_1/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:13.805701 140031916288000 adafactor.py:358] Since rank of parameter decoder/layers_1/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.807934 139674595104768 adafactor.py:358] Since rank of parameter decoder/layers_8/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.788724 140529434355712 adafactor.py:358] Since rank of parameter decoder/layers_1/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.806870 140031916288000 adafactor.py:358] Since rank of parameter decoder/layers_1/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.809072 139674595104768 adafactor.py:358] Since rank of parameter decoder/layers_9/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.789840 140529434355712 adafactor.py:358] Since rank of parameter decoder/layers_1/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:13.807955 140031916288000 adafactor.py:358] Since rank of parameter decoder/layers_1/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:13.810191 139674595104768 adafactor.py:358] Since rank of parameter decoder/layers_9/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.790908 140529434355712 adafactor.py:358] Since rank of parameter decoder/layers_1/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.809045 140031916288000 adafactor.py:358] Since rank of parameter decoder/layers_1/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:13.713052 140440593786880 adafactor.py:358] Since rank of parameter decoder/layers_14/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.714203 140440593786880 adafactor.py:358] Since rank of parameter decoder/layers_14/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:13.715406 140440593786880 adafactor.py:358] Since rank of parameter decoder/layers_14/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.716523 140440593786880 adafactor.py:358] Since rank of parameter decoder/layers_14/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.717617 140440593786880 adafactor.py:358] Since rank of parameter decoder/layers_15/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.718730 140440593786880 adafactor.py:358] Since rank of parameter decoder/layers_15/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.719870 140440593786880 adafactor.py:358] Since rank of parameter decoder/layers_15/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:13.721004 140440593786880 adafactor.py:358] Since rank of parameter decoder/layers_15/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:13.811323 139674595104768 adafactor.py:358] Since rank of parameter decoder/layers_9/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:13.791996 140529434355712 adafactor.py:358] Since rank of parameter decoder/layers_1/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.809927 140031916288000 adafactor.py:358] Since rank of parameter decoder/layers_1/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:13.721901 140440593786880 adafactor.py:358] Since rank of parameter decoder/layers_15/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:13.723401 140440593786880 adafactor.py:358] Since rank of parameter decoder/layers_15/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.724554 140440593786880 adafactor.py:358] Since rank of parameter decoder/layers_15/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:13.725683 140440593786880 adafactor.py:358] Since rank of parameter decoder/layers_15/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.726787 140440593786880 adafactor.py:358] Since rank of parameter decoder/layers_15/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.727926 140440593786880 adafactor.py:358] Since rank of parameter decoder/layers_16/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.729086 140440593786880 adafactor.py:358] Since rank of parameter decoder/layers_16/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.730196 140440593786880 adafactor.py:358] Since rank of parameter decoder/layers_16/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:13.731308 140440593786880 adafactor.py:358] Since rank of parameter decoder/layers_16/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:13.732298 140440593786880 adafactor.py:358] Since rank of parameter decoder/layers_16/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:13.733229 140440593786880 adafactor.py:358] Since rank of parameter decoder/layers_16/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.734335 140440593786880 adafactor.py:358] Since rank of parameter decoder/layers_16/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:13.735434 140440593786880 adafactor.py:358] Since rank of parameter decoder/layers_16/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.736568 140440593786880 adafactor.py:358] Since rank of parameter decoder/layers_16/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.737720 140440593786880 adafactor.py:358] Since rank of parameter decoder/layers_17/extra_mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.738867 140440593786880 adafactor.py:358] Since rank of parameter decoder/layers_17/extra_mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.739967 140440593786880 adafactor.py:358] Since rank of parameter decoder/layers_17/extra_mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:13.812402 139674595104768 adafactor.py:358] Since rank of parameter decoder/layers_9/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:13.793176 140529434355712 adafactor.py:358] Since rank of parameter decoder/layers_10/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.810895 140031916288000 adafactor.py:358] Since rank of parameter decoder/layers_1/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.744125 140440593786880 adafactor.py:358] Since rank of parameter decoder/layers_17/mlp/router/router_weights/w/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>, <FactorDim.NONE: None>) is ignored.
W0512 23:42:13.745072 140440593786880 adafactor.py:358] Since rank of parameter decoder/layers_17/pre_extra_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:13.746021 140440593786880 adafactor.py:358] Since rank of parameter decoder/layers_17/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:13.746911 140440593786880 adafactor.py:358] Since rank of parameter decoder/layers_17/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:13.747811 140440593786880 adafactor.py:358] Since rank of parameter decoder/layers_17/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.748950 140440593786880 adafactor.py:358] Since rank of parameter decoder/layers_17/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:13.750095 140440593786880 adafactor.py:358] Since rank of parameter decoder/layers_17/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.751211 140440593786880 adafactor.py:358] Since rank of parameter decoder/layers_17/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.752324 140440593786880 adafactor.py:358] Since rank of parameter decoder/layers_18/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.813271 139674595104768 adafactor.py:358] Since rank of parameter decoder/layers_9/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:13.794333 140529434355712 adafactor.py:358] Since rank of parameter decoder/layers_10/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.811982 140031916288000 adafactor.py:358] Since rank of parameter decoder/layers_1/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:13.814150 139674595104768 adafactor.py:358] Since rank of parameter decoder/layers_9/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.753448 140440593786880 adafactor.py:358] Since rank of parameter decoder/layers_18/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.754615 140440593786880 adafactor.py:358] Since rank of parameter decoder/layers_18/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:13.755722 140440593786880 adafactor.py:358] Since rank of parameter decoder/layers_18/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:13.756647 140440593786880 adafactor.py:358] Since rank of parameter decoder/layers_18/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:13.757555 140440593786880 adafactor.py:358] Since rank of parameter decoder/layers_18/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.758745 140440593786880 adafactor.py:358] Since rank of parameter decoder/layers_18/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:13.759856 140440593786880 adafactor.py:358] Since rank of parameter decoder/layers_18/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.760996 140440593786880 adafactor.py:358] Since rank of parameter decoder/layers_18/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.762096 140440593786880 adafactor.py:358] Since rank of parameter decoder/layers_19/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.813075 140031916288000 adafactor.py:358] Since rank of parameter decoder/layers_1/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.815226 139674595104768 adafactor.py:358] Since rank of parameter decoder/layers_9/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:13.763256 140440593786880 adafactor.py:358] Since rank of parameter decoder/layers_19/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.764393 140440593786880 adafactor.py:358] Since rank of parameter decoder/layers_19/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:13.765529 140440593786880 adafactor.py:358] Since rank of parameter decoder/layers_19/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:13.766428 140440593786880 adafactor.py:358] Since rank of parameter decoder/layers_19/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:13.767327 140440593786880 adafactor.py:358] Since rank of parameter decoder/layers_19/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.768519 140440593786880 adafactor.py:358] Since rank of parameter decoder/layers_19/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:13.769613 140440593786880 adafactor.py:358] Since rank of parameter decoder/layers_19/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.770740 140440593786880 adafactor.py:358] Since rank of parameter decoder/layers_19/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.795587 140529434355712 adafactor.py:358] Since rank of parameter decoder/layers_10/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:13.814183 140031916288000 adafactor.py:358] Since rank of parameter decoder/layers_1/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.771882 140440593786880 adafactor.py:358] Since rank of parameter decoder/layers_2/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.773013 140440593786880 adafactor.py:358] Since rank of parameter decoder/layers_2/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.774113 140440593786880 adafactor.py:358] Since rank of parameter decoder/layers_2/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:13.775233 140440593786880 adafactor.py:358] Since rank of parameter decoder/layers_2/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:13.776124 140440593786880 adafactor.py:358] Since rank of parameter decoder/layers_2/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:13.796873 140529434355712 adafactor.py:358] Since rank of parameter decoder/layers_10/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:13.777566 140440593786880 adafactor.py:358] Since rank of parameter decoder/layers_2/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.778705 140440593786880 adafactor.py:358] Since rank of parameter decoder/layers_2/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:13.779808 140440593786880 adafactor.py:358] Since rank of parameter decoder/layers_2/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.780944 140440593786880 adafactor.py:358] Since rank of parameter decoder/layers_2/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.816359 139674595104768 adafactor.py:358] Since rank of parameter decoder/layers_9/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.797788 140529434355712 adafactor.py:358] Since rank of parameter decoder/layers_10/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:13.815306 140031916288000 adafactor.py:358] Since rank of parameter decoder/layers_10/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.817442 139674595104768 adafactor.py:358] Since rank of parameter decoder/layers_9/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.782105 140440593786880 adafactor.py:358] Since rank of parameter decoder/layers_20/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.783212 140440593786880 adafactor.py:358] Since rank of parameter decoder/layers_20/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.784315 140440593786880 adafactor.py:358] Since rank of parameter decoder/layers_20/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:13.785452 140440593786880 adafactor.py:358] Since rank of parameter decoder/layers_20/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:13.786423 140440593786880 adafactor.py:358] Since rank of parameter decoder/layers_20/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:13.787323 140440593786880 adafactor.py:358] Since rank of parameter decoder/layers_20/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.788435 140440593786880 adafactor.py:358] Since rank of parameter decoder/layers_20/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:13.789537 140440593786880 adafactor.py:358] Since rank of parameter decoder/layers_20/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.790693 140440593786880 adafactor.py:358] Since rank of parameter decoder/layers_20/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.798681 140529434355712 adafactor.py:358] Since rank of parameter decoder/layers_10/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.816387 140031916288000 adafactor.py:358] Since rank of parameter decoder/layers_10/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.818520 139674595104768 adafactor.py:358] Since rank of parameter decoder/logits_dense/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.791784 140440593786880 adafactor.py:358] Since rank of parameter decoder/layers_21/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.792914 140440593786880 adafactor.py:358] Since rank of parameter decoder/layers_21/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.794006 140440593786880 adafactor.py:358] Since rank of parameter decoder/layers_21/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:13.795167 140440593786880 adafactor.py:358] Since rank of parameter decoder/layers_21/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:13.796064 140440593786880 adafactor.py:358] Since rank of parameter decoder/layers_21/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:13.796991 140440593786880 adafactor.py:358] Since rank of parameter decoder/layers_21/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.798092 140440593786880 adafactor.py:358] Since rank of parameter decoder/layers_21/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:13.799278 140440593786880 adafactor.py:358] Since rank of parameter decoder/layers_21/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.799788 140529434355712 adafactor.py:358] Since rank of parameter decoder/layers_10/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:13.817488 140031916288000 adafactor.py:358] Since rank of parameter decoder/layers_10/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:13.819640 139674595104768 adafactor.py:358] Since rank of parameter token_embedder/embedding 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:13.800372 140440593786880 adafactor.py:358] Since rank of parameter decoder/layers_21/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.801504 140440593786880 adafactor.py:358] Since rank of parameter decoder/layers_22/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.802591 140440593786880 adafactor.py:358] Since rank of parameter decoder/layers_22/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.803738 140440593786880 adafactor.py:358] Since rank of parameter decoder/layers_22/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:13.804869 140440593786880 adafactor.py:358] Since rank of parameter decoder/layers_22/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:13.805765 140440593786880 adafactor.py:358] Since rank of parameter decoder/layers_22/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:13.806663 140440593786880 adafactor.py:358] Since rank of parameter decoder/layers_22/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.807812 140440593786880 adafactor.py:358] Since rank of parameter decoder/layers_22/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:13.808946 140440593786880 adafactor.py:358] Since rank of parameter decoder/layers_22/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.800963 140529434355712 adafactor.py:358] Since rank of parameter decoder/layers_10/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.818679 140031916288000 adafactor.py:358] Since rank of parameter decoder/layers_10/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:13.810045 140440593786880 adafactor.py:358] Since rank of parameter decoder/layers_22/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.811151 140440593786880 adafactor.py:358] Since rank of parameter decoder/layers_23/extra_mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.812295 140440593786880 adafactor.py:358] Since rank of parameter decoder/layers_23/extra_mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.813427 140440593786880 adafactor.py:358] Since rank of parameter decoder/layers_23/extra_mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:13.817576 140440593786880 adafactor.py:358] Since rank of parameter decoder/layers_23/mlp/router/router_weights/w/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>, <FactorDim.NONE: None>) is ignored.
W0512 23:42:13.818517 140440593786880 adafactor.py:358] Since rank of parameter decoder/layers_23/pre_extra_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:13.819422 140440593786880 adafactor.py:358] Since rank of parameter decoder/layers_23/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:13.802098 140529434355712 adafactor.py:358] Since rank of parameter decoder/layers_10/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.819551 140031916288000 adafactor.py:358] Since rank of parameter decoder/layers_10/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:13.820327 140440593786880 adafactor.py:358] Since rank of parameter decoder/layers_23/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:13.820428 140031916288000 adafactor.py:358] Since rank of parameter decoder/layers_10/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.821317 140440593786880 adafactor.py:358] Since rank of parameter decoder/layers_23/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.803185 140529434355712 adafactor.py:358] Since rank of parameter decoder/layers_11/extra_mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.821497 140031916288000 adafactor.py:358] Since rank of parameter decoder/layers_10/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:13.822419 140440593786880 adafactor.py:358] Since rank of parameter decoder/layers_23/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:13.804271 140529434355712 adafactor.py:358] Since rank of parameter decoder/layers_11/extra_mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.822673 140031916288000 adafactor.py:358] Since rank of parameter decoder/layers_10/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.823665 140440593786880 adafactor.py:358] Since rank of parameter decoder/layers_23/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.806038 140529434355712 adafactor.py:358] Since rank of parameter decoder/layers_11/extra_mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:13.823759 140031916288000 adafactor.py:358] Since rank of parameter decoder/layers_10/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.824785 140440593786880 adafactor.py:358] Since rank of parameter decoder/layers_23/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.824848 140031916288000 adafactor.py:358] Since rank of parameter decoder/layers_11/extra_mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.825915 140440593786880 adafactor.py:358] Since rank of parameter decoder/layers_3/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.825936 140031916288000 adafactor.py:358] Since rank of parameter decoder/layers_11/extra_mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.826995 140440593786880 adafactor.py:358] Since rank of parameter decoder/layers_3/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.828064 140440593786880 adafactor.py:358] Since rank of parameter decoder/layers_3/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:13.827594 140031916288000 adafactor.py:358] Since rank of parameter decoder/layers_11/extra_mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:13.810133 140529434355712 adafactor.py:358] Since rank of parameter decoder/layers_11/mlp/router/router_weights/w/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>, <FactorDim.NONE: None>) is ignored.
W0512 23:42:13.811020 140529434355712 adafactor.py:358] Since rank of parameter decoder/layers_11/pre_extra_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:13.829246 140440593786880 adafactor.py:358] Since rank of parameter decoder/layers_3/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:13.811901 140529434355712 adafactor.py:358] Since rank of parameter decoder/layers_11/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:13.830703 140440593786880 adafactor.py:358] Since rank of parameter decoder/layers_3/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:13.812840 140529434355712 adafactor.py:358] Since rank of parameter decoder/layers_11/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:13.831705 140440593786880 adafactor.py:358] Since rank of parameter decoder/layers_3/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.813753 140529434355712 adafactor.py:358] Since rank of parameter decoder/layers_11/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.831707 140031916288000 adafactor.py:358] Since rank of parameter decoder/layers_11/mlp/router/router_weights/w/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>, <FactorDim.NONE: None>) is ignored.
W0512 23:42:13.814853 140529434355712 adafactor.py:358] Since rank of parameter decoder/layers_11/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:13.832942 140440593786880 adafactor.py:358] Since rank of parameter decoder/layers_3/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:13.832606 140031916288000 adafactor.py:358] Since rank of parameter decoder/layers_11/pre_extra_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:13.834173 140440593786880 adafactor.py:358] Since rank of parameter decoder/layers_3/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.815926 140529434355712 adafactor.py:358] Since rank of parameter decoder/layers_11/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.833488 140031916288000 adafactor.py:358] Since rank of parameter decoder/layers_11/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:13.834462 140031916288000 adafactor.py:358] Since rank of parameter decoder/layers_11/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:13.817100 140529434355712 adafactor.py:358] Since rank of parameter decoder/layers_11/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.835438 140440593786880 adafactor.py:358] Since rank of parameter decoder/layers_3/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.835372 140031916288000 adafactor.py:358] Since rank of parameter decoder/layers_11/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.818263 140529434355712 adafactor.py:358] Since rank of parameter decoder/layers_12/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.836706 140440593786880 adafactor.py:358] Since rank of parameter decoder/layers_4/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.836542 140031916288000 adafactor.py:358] Since rank of parameter decoder/layers_11/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:13.819328 140529434355712 adafactor.py:358] Since rank of parameter decoder/layers_12/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.837900 140440593786880 adafactor.py:358] Since rank of parameter decoder/layers_4/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.837660 140031916288000 adafactor.py:358] Since rank of parameter decoder/layers_11/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.820425 140529434355712 adafactor.py:358] Since rank of parameter decoder/layers_12/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:13.839095 140440593786880 adafactor.py:358] Since rank of parameter decoder/layers_4/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:13.838834 140031916288000 adafactor.py:358] Since rank of parameter decoder/layers_11/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.821644 140529434355712 adafactor.py:358] Since rank of parameter decoder/layers_12/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:13.840335 140440593786880 adafactor.py:358] Since rank of parameter decoder/layers_4/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:13.839977 140031916288000 adafactor.py:358] Since rank of parameter decoder/layers_12/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.822710 140529434355712 adafactor.py:358] Since rank of parameter decoder/layers_12/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:13.841332 140440593786880 adafactor.py:358] Since rank of parameter decoder/layers_4/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:13.841161 140031916288000 adafactor.py:358] Since rank of parameter decoder/layers_12/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.823935 140529434355712 adafactor.py:358] Since rank of parameter decoder/layers_12/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.842307 140440593786880 adafactor.py:358] Since rank of parameter decoder/layers_4/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.842325 140031916288000 adafactor.py:358] Since rank of parameter decoder/layers_12/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:13.843480 140440593786880 adafactor.py:358] Since rank of parameter decoder/layers_4/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:13.825265 140529434355712 adafactor.py:358] Since rank of parameter decoder/layers_12/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:13.843481 140031916288000 adafactor.py:358] Since rank of parameter decoder/layers_12/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:13.844738 140440593786880 adafactor.py:358] Since rank of parameter decoder/layers_4/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.826611 140529434355712 adafactor.py:358] Since rank of parameter decoder/layers_12/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.844400 140031916288000 adafactor.py:358] Since rank of parameter decoder/layers_12/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:13.845941 140440593786880 adafactor.py:358] Since rank of parameter decoder/layers_4/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.845289 140031916288000 adafactor.py:358] Since rank of parameter decoder/layers_12/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.827761 140529434355712 adafactor.py:358] Since rank of parameter decoder/layers_12/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.847137 140440593786880 adafactor.py:358] Since rank of parameter decoder/layers_5/extra_mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.828878 140529434355712 adafactor.py:358] Since rank of parameter decoder/layers_13/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.846411 140031916288000 adafactor.py:358] Since rank of parameter decoder/layers_12/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:13.847572 140031916288000 adafactor.py:358] Since rank of parameter decoder/layers_12/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.830086 140529434355712 adafactor.py:358] Since rank of parameter decoder/layers_13/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.848311 140440593786880 adafactor.py:358] Since rank of parameter decoder/layers_5/extra_mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.712008 140311617681408 adafactor.py:358] Since rank of parameter decoder/layers_14/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.713069 140311617681408 adafactor.py:358] Since rank of parameter decoder/layers_14/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:13.714268 140311617681408 adafactor.py:358] Since rank of parameter decoder/layers_14/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.715345 140311617681408 adafactor.py:358] Since rank of parameter decoder/layers_14/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.716427 140311617681408 adafactor.py:358] Since rank of parameter decoder/layers_15/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.717511 140311617681408 adafactor.py:358] Since rank of parameter decoder/layers_15/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.718660 140311617681408 adafactor.py:358] Since rank of parameter decoder/layers_15/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:13.719740 140311617681408 adafactor.py:358] Since rank of parameter decoder/layers_15/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:13.848667 140031916288000 adafactor.py:358] Since rank of parameter decoder/layers_12/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.720629 140311617681408 adafactor.py:358] Since rank of parameter decoder/layers_15/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:13.722142 140311617681408 adafactor.py:358] Since rank of parameter decoder/layers_15/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.723267 140311617681408 adafactor.py:358] Since rank of parameter decoder/layers_15/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:13.724374 140311617681408 adafactor.py:358] Since rank of parameter decoder/layers_15/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.725461 140311617681408 adafactor.py:358] Since rank of parameter decoder/layers_15/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.726629 140311617681408 adafactor.py:358] Since rank of parameter decoder/layers_16/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.727729 140311617681408 adafactor.py:358] Since rank of parameter decoder/layers_16/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.728809 140311617681408 adafactor.py:358] Since rank of parameter decoder/layers_16/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:13.729912 140311617681408 adafactor.py:358] Since rank of parameter decoder/layers_16/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:13.831321 140529434355712 adafactor.py:358] Since rank of parameter decoder/layers_13/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:13.849552 140440593786880 adafactor.py:358] Since rank of parameter decoder/layers_5/extra_mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:13.849758 140031916288000 adafactor.py:358] Since rank of parameter decoder/layers_13/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.832419 140529434355712 adafactor.py:358] Since rank of parameter decoder/layers_13/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:13.730850 140311617681408 adafactor.py:358] Since rank of parameter decoder/layers_16/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:13.731729 140311617681408 adafactor.py:358] Since rank of parameter decoder/layers_16/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.732807 140311617681408 adafactor.py:358] Since rank of parameter decoder/layers_16/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:13.733938 140311617681408 adafactor.py:358] Since rank of parameter decoder/layers_16/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.735017 140311617681408 adafactor.py:358] Since rank of parameter decoder/layers_16/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.736140 140311617681408 adafactor.py:358] Since rank of parameter decoder/layers_17/extra_mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.737229 140311617681408 adafactor.py:358] Since rank of parameter decoder/layers_17/extra_mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.738338 140311617681408 adafactor.py:358] Since rank of parameter decoder/layers_17/extra_mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:13.742408 140311617681408 adafactor.py:358] Since rank of parameter decoder/layers_17/mlp/router/router_weights/w/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>, <FactorDim.NONE: None>) is ignored.
W0512 23:42:13.743315 140311617681408 adafactor.py:358] Since rank of parameter decoder/layers_17/pre_extra_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:13.744254 140311617681408 adafactor.py:358] Since rank of parameter decoder/layers_17/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:13.745152 140311617681408 adafactor.py:358] Since rank of parameter decoder/layers_17/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:13.746048 140311617681408 adafactor.py:358] Since rank of parameter decoder/layers_17/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.747143 140311617681408 adafactor.py:358] Since rank of parameter decoder/layers_17/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:13.748278 140311617681408 adafactor.py:358] Since rank of parameter decoder/layers_17/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.749352 140311617681408 adafactor.py:358] Since rank of parameter decoder/layers_17/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.750462 140311617681408 adafactor.py:358] Since rank of parameter decoder/layers_18/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.850938 140031916288000 adafactor.py:358] Since rank of parameter decoder/layers_13/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.833886 140529434355712 adafactor.py:358] Since rank of parameter decoder/layers_13/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:13.852017 140031916288000 adafactor.py:358] Since rank of parameter decoder/layers_13/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:13.834850 140529434355712 adafactor.py:358] Since rank of parameter decoder/layers_13/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.751542 140311617681408 adafactor.py:358] Since rank of parameter decoder/layers_18/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.752690 140311617681408 adafactor.py:358] Since rank of parameter decoder/layers_18/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:13.753826 140311617681408 adafactor.py:358] Since rank of parameter decoder/layers_18/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:13.754711 140311617681408 adafactor.py:358] Since rank of parameter decoder/layers_18/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:13.755594 140311617681408 adafactor.py:358] Since rank of parameter decoder/layers_18/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.756713 140311617681408 adafactor.py:358] Since rank of parameter decoder/layers_18/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:13.757819 140311617681408 adafactor.py:358] Since rank of parameter decoder/layers_18/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.758913 140311617681408 adafactor.py:358] Since rank of parameter decoder/layers_18/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.759991 140311617681408 adafactor.py:358] Since rank of parameter decoder/layers_19/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.853121 140031916288000 adafactor.py:358] Since rank of parameter decoder/layers_13/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:13.853989 140440593786880 adafactor.py:358] Since rank of parameter decoder/layers_5/mlp/router/router_weights/w/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>, <FactorDim.NONE: None>) is ignored.
W0512 23:42:13.761122 140311617681408 adafactor.py:358] Since rank of parameter decoder/layers_19/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.762228 140311617681408 adafactor.py:358] Since rank of parameter decoder/layers_19/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:13.763318 140311617681408 adafactor.py:358] Since rank of parameter decoder/layers_19/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:13.764191 140311617681408 adafactor.py:358] Since rank of parameter decoder/layers_19/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:13.765066 140311617681408 adafactor.py:358] Since rank of parameter decoder/layers_19/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.766227 140311617681408 adafactor.py:358] Since rank of parameter decoder/layers_19/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:13.767312 140311617681408 adafactor.py:358] Since rank of parameter decoder/layers_19/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.768383 140311617681408 adafactor.py:358] Since rank of parameter decoder/layers_19/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.835961 140529434355712 adafactor.py:358] Since rank of parameter decoder/layers_13/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:13.769510 140311617681408 adafactor.py:358] Since rank of parameter decoder/layers_2/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.770622 140311617681408 adafactor.py:358] Since rank of parameter decoder/layers_2/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.771687 140311617681408 adafactor.py:358] Since rank of parameter decoder/layers_2/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:13.772764 140311617681408 adafactor.py:358] Since rank of parameter decoder/layers_2/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:13.773620 140311617681408 adafactor.py:358] Since rank of parameter decoder/layers_2/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:13.775028 140311617681408 adafactor.py:358] Since rank of parameter decoder/layers_2/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.776109 140311617681408 adafactor.py:358] Since rank of parameter decoder/layers_2/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:13.777190 140311617681408 adafactor.py:358] Since rank of parameter decoder/layers_2/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.778280 140311617681408 adafactor.py:358] Since rank of parameter decoder/layers_2/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.854959 140440593786880 adafactor.py:358] Since rank of parameter decoder/layers_5/pre_extra_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:13.854531 140031916288000 adafactor.py:358] Since rank of parameter decoder/layers_13/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:13.837118 140529434355712 adafactor.py:358] Since rank of parameter decoder/layers_13/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.855902 140440593786880 adafactor.py:358] Since rank of parameter decoder/layers_5/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:13.779415 140311617681408 adafactor.py:358] Since rank of parameter decoder/layers_20/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.780479 140311617681408 adafactor.py:358] Since rank of parameter decoder/layers_20/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.781556 140311617681408 adafactor.py:358] Since rank of parameter decoder/layers_20/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:13.782658 140311617681408 adafactor.py:358] Since rank of parameter decoder/layers_20/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:13.783610 140311617681408 adafactor.py:358] Since rank of parameter decoder/layers_20/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:13.784491 140311617681408 adafactor.py:358] Since rank of parameter decoder/layers_20/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.785567 140311617681408 adafactor.py:358] Since rank of parameter decoder/layers_20/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:13.786669 140311617681408 adafactor.py:358] Since rank of parameter decoder/layers_20/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.787803 140311617681408 adafactor.py:358] Since rank of parameter decoder/layers_20/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.855514 140031916288000 adafactor.py:358] Since rank of parameter decoder/layers_13/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.838235 140529434355712 adafactor.py:358] Since rank of parameter decoder/layers_13/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.856826 140440593786880 adafactor.py:358] Since rank of parameter decoder/layers_5/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:13.788869 140311617681408 adafactor.py:358] Since rank of parameter decoder/layers_21/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.789979 140311617681408 adafactor.py:358] Since rank of parameter decoder/layers_21/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.791052 140311617681408 adafactor.py:358] Since rank of parameter decoder/layers_21/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:13.792196 140311617681408 adafactor.py:358] Since rank of parameter decoder/layers_21/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:13.793081 140311617681408 adafactor.py:358] Since rank of parameter decoder/layers_21/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:13.794017 140311617681408 adafactor.py:358] Since rank of parameter decoder/layers_21/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.795144 140311617681408 adafactor.py:358] Since rank of parameter decoder/layers_21/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:13.796274 140311617681408 adafactor.py:358] Since rank of parameter decoder/layers_21/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.856668 140031916288000 adafactor.py:358] Since rank of parameter decoder/layers_13/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:13.839367 140529434355712 adafactor.py:358] Since rank of parameter decoder/layers_14/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.857716 140440593786880 adafactor.py:358] Since rank of parameter decoder/layers_5/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.797367 140311617681408 adafactor.py:358] Since rank of parameter decoder/layers_21/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.798488 140311617681408 adafactor.py:358] Since rank of parameter decoder/layers_22/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.799576 140311617681408 adafactor.py:358] Since rank of parameter decoder/layers_22/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.800709 140311617681408 adafactor.py:358] Since rank of parameter decoder/layers_22/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:13.801815 140311617681408 adafactor.py:358] Since rank of parameter decoder/layers_22/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:13.802703 140311617681408 adafactor.py:358] Since rank of parameter decoder/layers_22/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:13.803583 140311617681408 adafactor.py:358] Since rank of parameter decoder/layers_22/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.804714 140311617681408 adafactor.py:358] Since rank of parameter decoder/layers_22/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:13.805814 140311617681408 adafactor.py:358] Since rank of parameter decoder/layers_22/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.857810 140031916288000 adafactor.py:358] Since rank of parameter decoder/layers_13/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.840446 140529434355712 adafactor.py:358] Since rank of parameter decoder/layers_14/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.858913 140440593786880 adafactor.py:358] Since rank of parameter decoder/layers_5/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:13.806897 140311617681408 adafactor.py:358] Since rank of parameter decoder/layers_22/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.807967 140311617681408 adafactor.py:358] Since rank of parameter decoder/layers_23/extra_mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.809087 140311617681408 adafactor.py:358] Since rank of parameter decoder/layers_23/extra_mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.810197 140311617681408 adafactor.py:358] Since rank of parameter decoder/layers_23/extra_mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:13.814253 140311617681408 adafactor.py:358] Since rank of parameter decoder/layers_23/mlp/router/router_weights/w/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>, <FactorDim.NONE: None>) is ignored.
W0512 23:42:13.815140 140311617681408 adafactor.py:358] Since rank of parameter decoder/layers_23/pre_extra_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:13.816010 140311617681408 adafactor.py:358] Since rank of parameter decoder/layers_23/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:13.816879 140311617681408 adafactor.py:358] Since rank of parameter decoder/layers_23/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:13.817838 140311617681408 adafactor.py:358] Since rank of parameter decoder/layers_23/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.858974 140031916288000 adafactor.py:358] Since rank of parameter decoder/layers_13/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.841515 140529434355712 adafactor.py:358] Since rank of parameter decoder/layers_14/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:13.860013 140440593786880 adafactor.py:358] Since rank of parameter decoder/layers_5/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.860116 140031916288000 adafactor.py:358] Since rank of parameter decoder/layers_14/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.842663 140529434355712 adafactor.py:358] Since rank of parameter decoder/layers_14/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:13.818902 140311617681408 adafactor.py:358] Since rank of parameter decoder/layers_23/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:13.820117 140311617681408 adafactor.py:358] Since rank of parameter decoder/layers_23/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.821178 140311617681408 adafactor.py:358] Since rank of parameter decoder/layers_23/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.822318 140311617681408 adafactor.py:358] Since rank of parameter decoder/layers_3/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.823396 140311617681408 adafactor.py:358] Since rank of parameter decoder/layers_3/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.824442 140311617681408 adafactor.py:358] Since rank of parameter decoder/layers_3/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:13.825496 140311617681408 adafactor.py:358] Since rank of parameter decoder/layers_3/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:13.826883 140311617681408 adafactor.py:358] Since rank of parameter decoder/layers_3/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:13.861137 140440593786880 adafactor.py:358] Since rank of parameter decoder/layers_5/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.861214 140031916288000 adafactor.py:358] Since rank of parameter decoder/layers_14/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.827748 140311617681408 adafactor.py:358] Since rank of parameter decoder/layers_3/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.862233 140440593786880 adafactor.py:358] Since rank of parameter decoder/layers_6/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.862342 140031916288000 adafactor.py:358] Since rank of parameter decoder/layers_14/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:13.828804 140311617681408 adafactor.py:358] Since rank of parameter decoder/layers_3/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:13.829883 140311617681408 adafactor.py:358] Since rank of parameter decoder/layers_3/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.831002 140311617681408 adafactor.py:358] Since rank of parameter decoder/layers_3/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.832046 140311617681408 adafactor.py:358] Since rank of parameter decoder/layers_4/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.833101 140311617681408 adafactor.py:358] Since rank of parameter decoder/layers_4/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.834228 140311617681408 adafactor.py:358] Since rank of parameter decoder/layers_4/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:13.835353 140311617681408 adafactor.py:358] Since rank of parameter decoder/layers_4/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:13.836231 140311617681408 adafactor.py:358] Since rank of parameter decoder/layers_4/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:13.863387 140440593786880 adafactor.py:358] Since rank of parameter decoder/layers_6/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.837108 140311617681408 adafactor.py:358] Since rank of parameter decoder/layers_4/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.838211 140311617681408 adafactor.py:358] Since rank of parameter decoder/layers_4/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:13.839326 140311617681408 adafactor.py:358] Since rank of parameter decoder/layers_4/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.840398 140311617681408 adafactor.py:358] Since rank of parameter decoder/layers_4/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.841479 140311617681408 adafactor.py:358] Since rank of parameter decoder/layers_5/extra_mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.842573 140311617681408 adafactor.py:358] Since rank of parameter decoder/layers_5/extra_mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.843695 140311617681408 adafactor.py:358] Since rank of parameter decoder/layers_5/extra_mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:13.863492 140031916288000 adafactor.py:358] Since rank of parameter decoder/layers_14/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:13.864495 140440593786880 adafactor.py:358] Since rank of parameter decoder/layers_6/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:13.864414 140031916288000 adafactor.py:358] Since rank of parameter decoder/layers_14/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:13.770817 140095893518336 adafactor.py:358] Since rank of parameter decoder/layers_14/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.771870 140095893518336 adafactor.py:358] Since rank of parameter decoder/layers_14/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:13.772999 140095893518336 adafactor.py:358] Since rank of parameter decoder/layers_14/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.774060 140095893518336 adafactor.py:358] Since rank of parameter decoder/layers_14/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.775176 140095893518336 adafactor.py:358] Since rank of parameter decoder/layers_15/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.776243 140095893518336 adafactor.py:358] Since rank of parameter decoder/layers_15/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.777365 140095893518336 adafactor.py:358] Since rank of parameter decoder/layers_15/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:13.778461 140095893518336 adafactor.py:358] Since rank of parameter decoder/layers_15/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:13.865622 140440593786880 adafactor.py:358] Since rank of parameter decoder/layers_6/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:13.779338 140095893518336 adafactor.py:358] Since rank of parameter decoder/layers_15/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:13.780807 140095893518336 adafactor.py:358] Since rank of parameter decoder/layers_15/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.781891 140095893518336 adafactor.py:358] Since rank of parameter decoder/layers_15/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:13.782994 140095893518336 adafactor.py:358] Since rank of parameter decoder/layers_15/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.784060 140095893518336 adafactor.py:358] Since rank of parameter decoder/layers_15/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.785160 140095893518336 adafactor.py:358] Since rank of parameter decoder/layers_16/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.786220 140095893518336 adafactor.py:358] Since rank of parameter decoder/layers_16/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.787300 140095893518336 adafactor.py:358] Since rank of parameter decoder/layers_16/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:13.788364 140095893518336 adafactor.py:358] Since rank of parameter decoder/layers_16/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:13.865312 140031916288000 adafactor.py:358] Since rank of parameter decoder/layers_14/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.866518 140440593786880 adafactor.py:358] Since rank of parameter decoder/layers_6/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:13.789281 140095893518336 adafactor.py:358] Since rank of parameter decoder/layers_16/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:13.790147 140095893518336 adafactor.py:358] Since rank of parameter decoder/layers_16/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.791226 140095893518336 adafactor.py:358] Since rank of parameter decoder/layers_16/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:13.792288 140095893518336 adafactor.py:358] Since rank of parameter decoder/layers_16/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.793352 140095893518336 adafactor.py:358] Since rank of parameter decoder/layers_16/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.794499 140095893518336 adafactor.py:358] Since rank of parameter decoder/layers_17/extra_mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.795599 140095893518336 adafactor.py:358] Since rank of parameter decoder/layers_17/extra_mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.796677 140095893518336 adafactor.py:358] Since rank of parameter decoder/layers_17/extra_mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:13.867468 140440593786880 adafactor.py:358] Since rank of parameter decoder/layers_6/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.847693 140311617681408 adafactor.py:358] Since rank of parameter decoder/layers_5/mlp/router/router_weights/w/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>, <FactorDim.NONE: None>) is ignored.
W0512 23:42:13.800714 140095893518336 adafactor.py:358] Since rank of parameter decoder/layers_17/mlp/router/router_weights/w/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>, <FactorDim.NONE: None>) is ignored.
W0512 23:42:13.801602 140095893518336 adafactor.py:358] Since rank of parameter decoder/layers_17/pre_extra_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:13.802558 140095893518336 adafactor.py:358] Since rank of parameter decoder/layers_17/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:13.803445 140095893518336 adafactor.py:358] Since rank of parameter decoder/layers_17/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:13.804324 140095893518336 adafactor.py:358] Since rank of parameter decoder/layers_17/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.805386 140095893518336 adafactor.py:358] Since rank of parameter decoder/layers_17/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:13.806545 140095893518336 adafactor.py:358] Since rank of parameter decoder/layers_17/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.807615 140095893518336 adafactor.py:358] Since rank of parameter decoder/layers_17/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.808710 140095893518336 adafactor.py:358] Since rank of parameter decoder/layers_18/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.848583 140311617681408 adafactor.py:358] Since rank of parameter decoder/layers_5/pre_extra_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:13.868611 140440593786880 adafactor.py:358] Since rank of parameter decoder/layers_6/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:13.869709 140440593786880 adafactor.py:358] Since rank of parameter decoder/layers_6/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.849445 140311617681408 adafactor.py:358] Since rank of parameter decoder/layers_5/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:13.809789 140095893518336 adafactor.py:358] Since rank of parameter decoder/layers_18/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.810952 140095893518336 adafactor.py:358] Since rank of parameter decoder/layers_18/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:13.812023 140095893518336 adafactor.py:358] Since rank of parameter decoder/layers_18/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:13.812883 140095893518336 adafactor.py:358] Since rank of parameter decoder/layers_18/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:13.813761 140095893518336 adafactor.py:358] Since rank of parameter decoder/layers_18/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.814938 140095893518336 adafactor.py:358] Since rank of parameter decoder/layers_18/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:13.816010 140095893518336 adafactor.py:358] Since rank of parameter decoder/layers_18/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.817091 140095893518336 adafactor.py:358] Since rank of parameter decoder/layers_18/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.818158 140095893518336 adafactor.py:358] Since rank of parameter decoder/layers_19/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.819302 140095893518336 adafactor.py:358] Since rank of parameter decoder/layers_19/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.820381 140095893518336 adafactor.py:358] Since rank of parameter decoder/layers_19/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:13.821454 140095893518336 adafactor.py:358] Since rank of parameter decoder/layers_19/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:13.822351 140095893518336 adafactor.py:358] Since rank of parameter decoder/layers_19/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:13.823227 140095893518336 adafactor.py:358] Since rank of parameter decoder/layers_19/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.824357 140095893518336 adafactor.py:358] Since rank of parameter decoder/layers_19/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:13.825417 140095893518336 adafactor.py:358] Since rank of parameter decoder/layers_19/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.826519 140095893518336 adafactor.py:358] Since rank of parameter decoder/layers_19/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.850349 140311617681408 adafactor.py:358] Since rank of parameter decoder/layers_5/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:13.870815 140440593786880 adafactor.py:358] Since rank of parameter decoder/layers_6/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.827639 140095893518336 adafactor.py:358] Since rank of parameter decoder/layers_2/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.828711 140095893518336 adafactor.py:358] Since rank of parameter decoder/layers_2/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.829776 140095893518336 adafactor.py:358] Since rank of parameter decoder/layers_2/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:13.830864 140095893518336 adafactor.py:358] Since rank of parameter decoder/layers_2/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:13.831731 140095893518336 adafactor.py:358] Since rank of parameter decoder/layers_2/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:13.833064 140095893518336 adafactor.py:358] Since rank of parameter decoder/layers_2/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.834141 140095893518336 adafactor.py:358] Since rank of parameter decoder/layers_2/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:13.835256 140095893518336 adafactor.py:358] Since rank of parameter decoder/layers_2/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.836321 140095893518336 adafactor.py:358] Since rank of parameter decoder/layers_2/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.851220 140311617681408 adafactor.py:358] Since rank of parameter decoder/layers_5/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.871963 140440593786880 adafactor.py:358] Since rank of parameter decoder/layers_7/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.852341 140311617681408 adafactor.py:358] Since rank of parameter decoder/layers_5/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:13.837449 140095893518336 adafactor.py:358] Since rank of parameter decoder/layers_20/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.838537 140095893518336 adafactor.py:358] Since rank of parameter decoder/layers_20/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.839613 140095893518336 adafactor.py:358] Since rank of parameter decoder/layers_20/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:13.840681 140095893518336 adafactor.py:358] Since rank of parameter decoder/layers_20/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:13.841624 140095893518336 adafactor.py:358] Since rank of parameter decoder/layers_20/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:13.842536 140095893518336 adafactor.py:358] Since rank of parameter decoder/layers_20/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.843607 140095893518336 adafactor.py:358] Since rank of parameter decoder/layers_20/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:13.844672 140095893518336 adafactor.py:358] Since rank of parameter decoder/layers_20/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.845790 140095893518336 adafactor.py:358] Since rank of parameter decoder/layers_20/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.873090 140440593786880 adafactor.py:358] Since rank of parameter decoder/layers_7/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.853400 140311617681408 adafactor.py:358] Since rank of parameter decoder/layers_5/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.846887 140095893518336 adafactor.py:358] Since rank of parameter decoder/layers_21/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.847934 140095893518336 adafactor.py:358] Since rank of parameter decoder/layers_21/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.849003 140095893518336 adafactor.py:358] Since rank of parameter decoder/layers_21/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:13.850136 140095893518336 adafactor.py:358] Since rank of parameter decoder/layers_21/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:13.851031 140095893518336 adafactor.py:358] Since rank of parameter decoder/layers_21/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:13.851898 140095893518336 adafactor.py:358] Since rank of parameter decoder/layers_21/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.852962 140095893518336 adafactor.py:358] Since rank of parameter decoder/layers_21/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:13.854071 140095893518336 adafactor.py:358] Since rank of parameter decoder/layers_21/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.874181 140440593786880 adafactor.py:358] Since rank of parameter decoder/layers_7/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:13.854508 140311617681408 adafactor.py:358] Since rank of parameter decoder/layers_5/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.855197 140095893518336 adafactor.py:358] Since rank of parameter decoder/layers_21/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.856267 140095893518336 adafactor.py:358] Since rank of parameter decoder/layers_22/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.857339 140095893518336 adafactor.py:358] Since rank of parameter decoder/layers_22/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.858489 140095893518336 adafactor.py:358] Since rank of parameter decoder/layers_22/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:13.859561 140095893518336 adafactor.py:358] Since rank of parameter decoder/layers_22/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:13.860439 140095893518336 adafactor.py:358] Since rank of parameter decoder/layers_22/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:13.861316 140095893518336 adafactor.py:358] Since rank of parameter decoder/layers_22/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.862453 140095893518336 adafactor.py:358] Since rank of parameter decoder/layers_22/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:13.863519 140095893518336 adafactor.py:358] Since rank of parameter decoder/layers_22/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.875274 140440593786880 adafactor.py:358] Since rank of parameter decoder/layers_7/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:13.855577 140311617681408 adafactor.py:358] Since rank of parameter decoder/layers_6/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.864590 140095893518336 adafactor.py:358] Since rank of parameter decoder/layers_22/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.865664 140095893518336 adafactor.py:358] Since rank of parameter decoder/layers_23/extra_mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.866802 140095893518336 adafactor.py:358] Since rank of parameter decoder/layers_23/extra_mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.867864 140095893518336 adafactor.py:358] Since rank of parameter decoder/layers_23/extra_mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:13.871881 140095893518336 adafactor.py:358] Since rank of parameter decoder/layers_23/mlp/router/router_weights/w/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>, <FactorDim.NONE: None>) is ignored.
W0512 23:42:13.872770 140095893518336 adafactor.py:358] Since rank of parameter decoder/layers_23/pre_extra_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:13.873637 140095893518336 adafactor.py:358] Since rank of parameter decoder/layers_23/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:13.874556 140095893518336 adafactor.py:358] Since rank of parameter decoder/layers_23/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:13.875488 140095893518336 adafactor.py:358] Since rank of parameter decoder/layers_23/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.876225 140440593786880 adafactor.py:358] Since rank of parameter decoder/layers_7/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:13.856707 140311617681408 adafactor.py:358] Since rank of parameter decoder/layers_6/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.876553 140095893518336 adafactor.py:358] Since rank of parameter decoder/layers_23/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:13.877766 140095893518336 adafactor.py:358] Since rank of parameter decoder/layers_23/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.877142 140440593786880 adafactor.py:358] Since rank of parameter decoder/layers_7/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.857791 140311617681408 adafactor.py:358] Since rank of parameter decoder/layers_6/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:13.878895 140095893518336 adafactor.py:358] Since rank of parameter decoder/layers_23/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.880048 140095893518336 adafactor.py:358] Since rank of parameter decoder/layers_3/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.878236 140440593786880 adafactor.py:358] Since rank of parameter decoder/layers_7/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:13.858874 140311617681408 adafactor.py:358] Since rank of parameter decoder/layers_6/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:13.881151 140095893518336 adafactor.py:358] Since rank of parameter decoder/layers_3/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.879357 140440593786880 adafactor.py:358] Since rank of parameter decoder/layers_7/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.859742 140311617681408 adafactor.py:358] Since rank of parameter decoder/layers_6/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:13.882275 140095893518336 adafactor.py:358] Since rank of parameter decoder/layers_3/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:13.860658 140311617681408 adafactor.py:358] Since rank of parameter decoder/layers_6/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.880513 140440593786880 adafactor.py:358] Since rank of parameter decoder/layers_7/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.883375 140095893518336 adafactor.py:358] Since rank of parameter decoder/layers_3/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:13.861732 140311617681408 adafactor.py:358] Since rank of parameter decoder/layers_6/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:13.881616 140440593786880 adafactor.py:358] Since rank of parameter decoder/layers_8/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.884722 140095893518336 adafactor.py:358] Since rank of parameter decoder/layers_3/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:13.862830 140311617681408 adafactor.py:358] Since rank of parameter decoder/layers_6/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.882703 140440593786880 adafactor.py:358] Since rank of parameter decoder/layers_8/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.885576 140095893518336 adafactor.py:358] Since rank of parameter decoder/layers_3/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.883794 140440593786880 adafactor.py:358] Since rank of parameter decoder/layers_8/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:13.863909 140311617681408 adafactor.py:358] Since rank of parameter decoder/layers_6/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.886658 140095893518336 adafactor.py:358] Since rank of parameter decoder/layers_3/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:13.865022 140311617681408 adafactor.py:358] Since rank of parameter decoder/layers_7/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.885421 140440593786880 adafactor.py:358] Since rank of parameter decoder/layers_8/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:13.887717 140095893518336 adafactor.py:358] Since rank of parameter decoder/layers_3/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.866128 140311617681408 adafactor.py:358] Since rank of parameter decoder/layers_7/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.886365 140440593786880 adafactor.py:358] Since rank of parameter decoder/layers_8/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:13.888834 140095893518336 adafactor.py:358] Since rank of parameter decoder/layers_3/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.887275 140440593786880 adafactor.py:358] Since rank of parameter decoder/layers_8/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.867188 140311617681408 adafactor.py:358] Since rank of parameter decoder/layers_7/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:13.889886 140095893518336 adafactor.py:358] Since rank of parameter decoder/layers_4/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.888371 140440593786880 adafactor.py:358] Since rank of parameter decoder/layers_8/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:13.868254 140311617681408 adafactor.py:358] Since rank of parameter decoder/layers_7/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:13.869178 140311617681408 adafactor.py:358] Since rank of parameter decoder/layers_7/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:13.890992 140095893518336 adafactor.py:358] Since rank of parameter decoder/layers_4/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.889500 140440593786880 adafactor.py:358] Since rank of parameter decoder/layers_8/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.892047 140095893518336 adafactor.py:358] Since rank of parameter decoder/layers_4/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:13.870086 140311617681408 adafactor.py:358] Since rank of parameter decoder/layers_7/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.890644 140440593786880 adafactor.py:358] Since rank of parameter decoder/layers_8/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.893166 140095893518336 adafactor.py:358] Since rank of parameter decoder/layers_4/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:13.871157 140311617681408 adafactor.py:358] Since rank of parameter decoder/layers_7/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:13.891757 140440593786880 adafactor.py:358] Since rank of parameter decoder/layers_9/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.894036 140095893518336 adafactor.py:358] Since rank of parameter decoder/layers_4/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:13.872246 140311617681408 adafactor.py:358] Since rank of parameter decoder/layers_7/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.894938 140095893518336 adafactor.py:358] Since rank of parameter decoder/layers_4/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.892874 140440593786880 adafactor.py:358] Since rank of parameter decoder/layers_9/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.873351 140311617681408 adafactor.py:358] Since rank of parameter decoder/layers_7/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.895997 140095893518336 adafactor.py:358] Since rank of parameter decoder/layers_4/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:13.894028 140440593786880 adafactor.py:358] Since rank of parameter decoder/layers_9/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:13.874489 140311617681408 adafactor.py:358] Since rank of parameter decoder/layers_8/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.897117 140095893518336 adafactor.py:358] Since rank of parameter decoder/layers_4/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.895139 140440593786880 adafactor.py:358] Since rank of parameter decoder/layers_9/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:13.875567 140311617681408 adafactor.py:358] Since rank of parameter decoder/layers_8/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.798668 140595318249472 adafactor.py:358] Since rank of parameter decoder/layers_14/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:13.799810 140595318249472 adafactor.py:358] Since rank of parameter decoder/layers_14/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.800894 140595318249472 adafactor.py:358] Since rank of parameter decoder/layers_14/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.801994 140595318249472 adafactor.py:358] Since rank of parameter decoder/layers_15/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.803064 140595318249472 adafactor.py:358] Since rank of parameter decoder/layers_15/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.804232 140595318249472 adafactor.py:358] Since rank of parameter decoder/layers_15/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:13.805331 140595318249472 adafactor.py:358] Since rank of parameter decoder/layers_15/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:13.806220 140595318249472 adafactor.py:358] Since rank of parameter decoder/layers_15/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:13.896023 140440593786880 adafactor.py:358] Since rank of parameter decoder/layers_9/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:13.898174 140095893518336 adafactor.py:358] Since rank of parameter decoder/layers_4/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.807677 140595318249472 adafactor.py:358] Since rank of parameter decoder/layers_15/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.808772 140595318249472 adafactor.py:358] Since rank of parameter decoder/layers_15/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:13.809897 140595318249472 adafactor.py:358] Since rank of parameter decoder/layers_15/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.810974 140595318249472 adafactor.py:358] Since rank of parameter decoder/layers_15/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.812097 140595318249472 adafactor.py:358] Since rank of parameter decoder/layers_16/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.813203 140595318249472 adafactor.py:358] Since rank of parameter decoder/layers_16/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.814264 140595318249472 adafactor.py:358] Since rank of parameter decoder/layers_16/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:13.815327 140595318249472 adafactor.py:358] Since rank of parameter decoder/layers_16/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:13.816256 140595318249472 adafactor.py:358] Since rank of parameter decoder/layers_16/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:13.876638 140311617681408 adafactor.py:358] Since rank of parameter decoder/layers_8/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:13.896943 140440593786880 adafactor.py:358] Since rank of parameter decoder/layers_9/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.899279 140095893518336 adafactor.py:358] Since rank of parameter decoder/layers_5/extra_mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.817166 140595318249472 adafactor.py:358] Since rank of parameter decoder/layers_16/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.818235 140595318249472 adafactor.py:358] Since rank of parameter decoder/layers_16/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:13.819303 140595318249472 adafactor.py:358] Since rank of parameter decoder/layers_16/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.820385 140595318249472 adafactor.py:358] Since rank of parameter decoder/layers_16/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.821556 140595318249472 adafactor.py:358] Since rank of parameter decoder/layers_17/extra_mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.822640 140595318249472 adafactor.py:358] Since rank of parameter decoder/layers_17/extra_mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.823744 140595318249472 adafactor.py:358] Since rank of parameter decoder/layers_17/extra_mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:13.827836 140595318249472 adafactor.py:358] Since rank of parameter decoder/layers_17/mlp/router/router_weights/w/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>, <FactorDim.NONE: None>) is ignored.
W0512 23:42:13.898055 140440593786880 adafactor.py:358] Since rank of parameter decoder/layers_9/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:13.878210 140311617681408 adafactor.py:358] Since rank of parameter decoder/layers_8/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:13.900341 140095893518336 adafactor.py:358] Since rank of parameter decoder/layers_5/extra_mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.828736 140595318249472 adafactor.py:358] Since rank of parameter decoder/layers_17/pre_extra_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:13.829707 140595318249472 adafactor.py:358] Since rank of parameter decoder/layers_17/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:13.830600 140595318249472 adafactor.py:358] Since rank of parameter decoder/layers_17/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:13.831486 140595318249472 adafactor.py:358] Since rank of parameter decoder/layers_17/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.832561 140595318249472 adafactor.py:358] Since rank of parameter decoder/layers_17/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:13.833739 140595318249472 adafactor.py:358] Since rank of parameter decoder/layers_17/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.834825 140595318249472 adafactor.py:358] Since rank of parameter decoder/layers_17/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.835906 140595318249472 adafactor.py:358] Since rank of parameter decoder/layers_18/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.837008 140595318249472 adafactor.py:358] Since rank of parameter decoder/layers_18/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.879101 140311617681408 adafactor.py:358] Since rank of parameter decoder/layers_8/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:13.899235 140440593786880 adafactor.py:358] Since rank of parameter decoder/layers_9/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.901458 140095893518336 adafactor.py:358] Since rank of parameter decoder/layers_5/extra_mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:13.838167 140595318249472 adafactor.py:358] Since rank of parameter decoder/layers_18/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:13.839241 140595318249472 adafactor.py:358] Since rank of parameter decoder/layers_18/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:13.840126 140595318249472 adafactor.py:358] Since rank of parameter decoder/layers_18/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:13.841032 140595318249472 adafactor.py:358] Since rank of parameter decoder/layers_18/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.842169 140595318249472 adafactor.py:358] Since rank of parameter decoder/layers_18/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:13.843240 140595318249472 adafactor.py:358] Since rank of parameter decoder/layers_18/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.844362 140595318249472 adafactor.py:358] Since rank of parameter decoder/layers_18/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.845480 140595318249472 adafactor.py:358] Since rank of parameter decoder/layers_19/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.846618 140595318249472 adafactor.py:358] Since rank of parameter decoder/layers_19/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.880017 140311617681408 adafactor.py:358] Since rank of parameter decoder/layers_8/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.900334 140440593786880 adafactor.py:358] Since rank of parameter decoder/layers_9/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.847709 140595318249472 adafactor.py:358] Since rank of parameter decoder/layers_19/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:13.848795 140595318249472 adafactor.py:358] Since rank of parameter decoder/layers_19/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:13.849707 140595318249472 adafactor.py:358] Since rank of parameter decoder/layers_19/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:13.850600 140595318249472 adafactor.py:358] Since rank of parameter decoder/layers_19/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.851738 140595318249472 adafactor.py:358] Since rank of parameter decoder/layers_19/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:13.852807 140595318249472 adafactor.py:358] Since rank of parameter decoder/layers_19/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.853941 140595318249472 adafactor.py:358] Since rank of parameter decoder/layers_19/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.855067 140595318249472 adafactor.py:358] Since rank of parameter decoder/layers_2/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.881120 140311617681408 adafactor.py:358] Since rank of parameter decoder/layers_8/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:13.856148 140595318249472 adafactor.py:358] Since rank of parameter decoder/layers_2/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.857278 140595318249472 adafactor.py:358] Since rank of parameter decoder/layers_2/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:13.858352 140595318249472 adafactor.py:358] Since rank of parameter decoder/layers_2/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:13.859226 140595318249472 adafactor.py:358] Since rank of parameter decoder/layers_2/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:13.860589 140595318249472 adafactor.py:358] Since rank of parameter decoder/layers_2/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.861709 140595318249472 adafactor.py:358] Since rank of parameter decoder/layers_2/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:13.862789 140595318249472 adafactor.py:358] Since rank of parameter decoder/layers_2/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.863892 140595318249472 adafactor.py:358] Since rank of parameter decoder/layers_2/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.865064 140595318249472 adafactor.py:358] Since rank of parameter decoder/layers_20/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.901454 140440593786880 adafactor.py:358] Since rank of parameter decoder/logits_dense/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.882281 140311617681408 adafactor.py:358] Since rank of parameter decoder/layers_8/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.902561 140440593786880 adafactor.py:358] Since rank of parameter token_embedder/embedding 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:13.866142 140595318249472 adafactor.py:358] Since rank of parameter decoder/layers_20/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.867233 140595318249472 adafactor.py:358] Since rank of parameter decoder/layers_20/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:13.868325 140595318249472 adafactor.py:358] Since rank of parameter decoder/layers_20/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:13.869307 140595318249472 adafactor.py:358] Since rank of parameter decoder/layers_20/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:13.870195 140595318249472 adafactor.py:358] Since rank of parameter decoder/layers_20/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.871272 140595318249472 adafactor.py:358] Since rank of parameter decoder/layers_20/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:13.872351 140595318249472 adafactor.py:358] Since rank of parameter decoder/layers_20/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.873521 140595318249472 adafactor.py:358] Since rank of parameter decoder/layers_20/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.874599 140595318249472 adafactor.py:358] Since rank of parameter decoder/layers_21/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.905466 140095893518336 adafactor.py:358] Since rank of parameter decoder/layers_5/mlp/router/router_weights/w/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>, <FactorDim.NONE: None>) is ignored.
W0512 23:42:13.883432 140311617681408 adafactor.py:358] Since rank of parameter decoder/layers_8/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.875673 140595318249472 adafactor.py:358] Since rank of parameter decoder/layers_21/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.876749 140595318249472 adafactor.py:358] Since rank of parameter decoder/layers_21/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:13.877922 140595318249472 adafactor.py:358] Since rank of parameter decoder/layers_21/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:13.878805 140595318249472 adafactor.py:358] Since rank of parameter decoder/layers_21/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:13.879694 140595318249472 adafactor.py:358] Since rank of parameter decoder/layers_21/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.880781 140595318249472 adafactor.py:358] Since rank of parameter decoder/layers_21/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:13.881948 140595318249472 adafactor.py:358] Since rank of parameter decoder/layers_21/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.883028 140595318249472 adafactor.py:358] Since rank of parameter decoder/layers_21/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.906385 140095893518336 adafactor.py:358] Since rank of parameter decoder/layers_5/pre_extra_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:13.884567 140311617681408 adafactor.py:358] Since rank of parameter decoder/layers_9/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.884146 140595318249472 adafactor.py:358] Since rank of parameter decoder/layers_22/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.885256 140595318249472 adafactor.py:358] Since rank of parameter decoder/layers_22/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.886378 140595318249472 adafactor.py:358] Since rank of parameter decoder/layers_22/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:13.887467 140595318249472 adafactor.py:358] Since rank of parameter decoder/layers_22/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:13.888328 140595318249472 adafactor.py:358] Since rank of parameter decoder/layers_22/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:13.889225 140595318249472 adafactor.py:358] Since rank of parameter decoder/layers_22/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.890350 140595318249472 adafactor.py:358] Since rank of parameter decoder/layers_22/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:13.891431 140595318249472 adafactor.py:358] Since rank of parameter decoder/layers_22/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.892511 140595318249472 adafactor.py:358] Since rank of parameter decoder/layers_22/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.907259 140095893518336 adafactor.py:358] Since rank of parameter decoder/layers_5/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:13.885684 140311617681408 adafactor.py:358] Since rank of parameter decoder/layers_9/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.908123 140095893518336 adafactor.py:358] Since rank of parameter decoder/layers_5/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:13.893633 140595318249472 adafactor.py:358] Since rank of parameter decoder/layers_23/extra_mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.894763 140595318249472 adafactor.py:358] Since rank of parameter decoder/layers_23/extra_mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.908992 140095893518336 adafactor.py:358] Since rank of parameter decoder/layers_5/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.886878 140311617681408 adafactor.py:358] Since rank of parameter decoder/layers_9/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:13.895831 140595318249472 adafactor.py:358] Since rank of parameter decoder/layers_23/extra_mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:13.899899 140595318249472 adafactor.py:358] Since rank of parameter decoder/layers_23/mlp/router/router_weights/w/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>, <FactorDim.NONE: None>) is ignored.
W0512 23:42:13.900810 140595318249472 adafactor.py:358] Since rank of parameter decoder/layers_23/pre_extra_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:13.901717 140595318249472 adafactor.py:358] Since rank of parameter decoder/layers_23/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:13.902596 140595318249472 adafactor.py:358] Since rank of parameter decoder/layers_23/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:13.903535 140595318249472 adafactor.py:358] Since rank of parameter decoder/layers_23/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.904641 140595318249472 adafactor.py:358] Since rank of parameter decoder/layers_23/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:13.905891 140595318249472 adafactor.py:358] Since rank of parameter decoder/layers_23/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.888008 140311617681408 adafactor.py:358] Since rank of parameter decoder/layers_9/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:13.910130 140095893518336 adafactor.py:358] Since rank of parameter decoder/layers_5/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:13.906953 140595318249472 adafactor.py:358] Since rank of parameter decoder/layers_23/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.908066 140595318249472 adafactor.py:358] Since rank of parameter decoder/layers_3/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.888916 140311617681408 adafactor.py:358] Since rank of parameter decoder/layers_9/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:13.911221 140095893518336 adafactor.py:358] Since rank of parameter decoder/layers_5/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.909171 140595318249472 adafactor.py:358] Since rank of parameter decoder/layers_3/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.889854 140311617681408 adafactor.py:358] Since rank of parameter decoder/layers_9/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.912281 140095893518336 adafactor.py:358] Since rank of parameter decoder/layers_5/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.910231 140595318249472 adafactor.py:358] Since rank of parameter decoder/layers_3/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:13.890988 140311617681408 adafactor.py:358] Since rank of parameter decoder/layers_9/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:13.911285 140595318249472 adafactor.py:358] Since rank of parameter decoder/layers_3/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:13.913341 140095893518336 adafactor.py:358] Since rank of parameter decoder/layers_6/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.892152 140311617681408 adafactor.py:358] Since rank of parameter decoder/layers_9/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.914478 140095893518336 adafactor.py:358] Since rank of parameter decoder/layers_6/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.912673 140595318249472 adafactor.py:358] Since rank of parameter decoder/layers_3/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:13.893277 140311617681408 adafactor.py:358] Since rank of parameter decoder/layers_9/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.915562 140095893518336 adafactor.py:358] Since rank of parameter decoder/layers_6/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:13.913591 140595318249472 adafactor.py:358] Since rank of parameter decoder/layers_3/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.916638 140095893518336 adafactor.py:358] Since rank of parameter decoder/layers_6/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:13.914662 140595318249472 adafactor.py:358] Since rank of parameter decoder/layers_3/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:13.894454 140311617681408 adafactor.py:358] Since rank of parameter decoder/logits_dense/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.917511 140095893518336 adafactor.py:358] Since rank of parameter decoder/layers_6/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:13.895586 140311617681408 adafactor.py:358] Since rank of parameter token_embedder/embedding 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:13.915724 140595318249472 adafactor.py:358] Since rank of parameter decoder/layers_3/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.918461 140095893518336 adafactor.py:358] Since rank of parameter decoder/layers_6/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.916846 140595318249472 adafactor.py:358] Since rank of parameter decoder/layers_3/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.919542 140095893518336 adafactor.py:358] Since rank of parameter decoder/layers_6/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:13.917932 140595318249472 adafactor.py:358] Since rank of parameter decoder/layers_4/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.920606 140095893518336 adafactor.py:358] Since rank of parameter decoder/layers_6/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.919003 140595318249472 adafactor.py:358] Since rank of parameter decoder/layers_4/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.921666 140095893518336 adafactor.py:358] Since rank of parameter decoder/layers_6/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.920049 140595318249472 adafactor.py:358] Since rank of parameter decoder/layers_4/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:13.922826 140095893518336 adafactor.py:358] Since rank of parameter decoder/layers_7/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.921201 140595318249472 adafactor.py:358] Since rank of parameter decoder/layers_4/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:13.923892 140095893518336 adafactor.py:358] Since rank of parameter decoder/layers_7/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.922071 140595318249472 adafactor.py:358] Since rank of parameter decoder/layers_4/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:13.924947 140095893518336 adafactor.py:358] Since rank of parameter decoder/layers_7/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:13.922940 140595318249472 adafactor.py:358] Since rank of parameter decoder/layers_4/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.926002 140095893518336 adafactor.py:358] Since rank of parameter decoder/layers_7/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:13.924048 140595318249472 adafactor.py:358] Since rank of parameter decoder/layers_4/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:13.926960 140095893518336 adafactor.py:358] Since rank of parameter decoder/layers_7/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:13.925208 140595318249472 adafactor.py:358] Since rank of parameter decoder/layers_4/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.927830 140095893518336 adafactor.py:358] Since rank of parameter decoder/layers_7/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.926282 140595318249472 adafactor.py:358] Since rank of parameter decoder/layers_4/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.928897 140095893518336 adafactor.py:358] Since rank of parameter decoder/layers_7/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:13.927352 140595318249472 adafactor.py:358] Since rank of parameter decoder/layers_5/extra_mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.929951 140095893518336 adafactor.py:358] Since rank of parameter decoder/layers_7/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.928445 140595318249472 adafactor.py:358] Since rank of parameter decoder/layers_5/extra_mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.931093 140095893518336 adafactor.py:358] Since rank of parameter decoder/layers_7/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.929608 140595318249472 adafactor.py:358] Since rank of parameter decoder/layers_5/extra_mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:13.932177 140095893518336 adafactor.py:358] Since rank of parameter decoder/layers_8/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.933232 140095893518336 adafactor.py:358] Since rank of parameter decoder/layers_8/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.934320 140095893518336 adafactor.py:358] Since rank of parameter decoder/layers_8/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:13.933656 140595318249472 adafactor.py:358] Since rank of parameter decoder/layers_5/mlp/router/router_weights/w/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>, <FactorDim.NONE: None>) is ignored.
W0512 23:42:13.935900 140095893518336 adafactor.py:358] Since rank of parameter decoder/layers_8/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:13.934555 140595318249472 adafactor.py:358] Since rank of parameter decoder/layers_5/pre_extra_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:13.936781 140095893518336 adafactor.py:358] Since rank of parameter decoder/layers_8/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:13.937666 140095893518336 adafactor.py:358] Since rank of parameter decoder/layers_8/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.935429 140595318249472 adafactor.py:358] Since rank of parameter decoder/layers_5/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:13.936289 140595318249472 adafactor.py:358] Since rank of parameter decoder/layers_5/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:13.938762 140095893518336 adafactor.py:358] Since rank of parameter decoder/layers_8/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:13.937203 140595318249472 adafactor.py:358] Since rank of parameter decoder/layers_5/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.939831 140095893518336 adafactor.py:358] Since rank of parameter decoder/layers_8/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.938342 140595318249472 adafactor.py:358] Since rank of parameter decoder/layers_5/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:13.940940 140095893518336 adafactor.py:358] Since rank of parameter decoder/layers_8/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.939421 140595318249472 adafactor.py:358] Since rank of parameter decoder/layers_5/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.942031 140095893518336 adafactor.py:358] Since rank of parameter decoder/layers_9/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.940496 140595318249472 adafactor.py:358] Since rank of parameter decoder/layers_5/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.943123 140095893518336 adafactor.py:358] Since rank of parameter decoder/layers_9/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.941605 140595318249472 adafactor.py:358] Since rank of parameter decoder/layers_6/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.944233 140095893518336 adafactor.py:358] Since rank of parameter decoder/layers_9/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:13.942735 140595318249472 adafactor.py:358] Since rank of parameter decoder/layers_6/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.945301 140095893518336 adafactor.py:358] Since rank of parameter decoder/layers_9/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:13.946171 140095893518336 adafactor.py:358] Since rank of parameter decoder/layers_9/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:13.943839 140595318249472 adafactor.py:358] Since rank of parameter decoder/layers_6/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:13.947066 140095893518336 adafactor.py:358] Since rank of parameter decoder/layers_9/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.944936 140595318249472 adafactor.py:358] Since rank of parameter decoder/layers_6/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:13.945845 140595318249472 adafactor.py:358] Since rank of parameter decoder/layers_6/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:13.948137 140095893518336 adafactor.py:358] Since rank of parameter decoder/layers_9/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:13.946778 140595318249472 adafactor.py:358] Since rank of parameter decoder/layers_6/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.949234 140095893518336 adafactor.py:358] Since rank of parameter decoder/layers_9/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.947859 140595318249472 adafactor.py:358] Since rank of parameter decoder/layers_6/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:13.950319 140095893518336 adafactor.py:358] Since rank of parameter decoder/layers_9/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.948930 140595318249472 adafactor.py:358] Since rank of parameter decoder/layers_6/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.951385 140095893518336 adafactor.py:358] Since rank of parameter decoder/logits_dense/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.950040 140595318249472 adafactor.py:358] Since rank of parameter decoder/layers_6/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.952464 140095893518336 adafactor.py:358] Since rank of parameter token_embedder/embedding 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:13.951181 140595318249472 adafactor.py:358] Since rank of parameter decoder/layers_7/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.952260 140595318249472 adafactor.py:358] Since rank of parameter decoder/layers_7/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.953372 140595318249472 adafactor.py:358] Since rank of parameter decoder/layers_7/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:13.954453 140595318249472 adafactor.py:358] Since rank of parameter decoder/layers_7/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:13.955381 140595318249472 adafactor.py:358] Since rank of parameter decoder/layers_7/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:13.956258 140595318249472 adafactor.py:358] Since rank of parameter decoder/layers_7/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.957381 140595318249472 adafactor.py:358] Since rank of parameter decoder/layers_7/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:13.958458 140595318249472 adafactor.py:358] Since rank of parameter decoder/layers_7/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.959574 140595318249472 adafactor.py:358] Since rank of parameter decoder/layers_7/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.960657 140595318249472 adafactor.py:358] Since rank of parameter decoder/layers_8/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.961772 140595318249472 adafactor.py:358] Since rank of parameter decoder/layers_8/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.962851 140595318249472 adafactor.py:358] Since rank of parameter decoder/layers_8/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:13.866425 140031916288000 adafactor.py:358] Since rank of parameter decoder/layers_14/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:13.867569 140031916288000 adafactor.py:358] Since rank of parameter decoder/layers_14/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.868657 140031916288000 adafactor.py:358] Since rank of parameter decoder/layers_14/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.869751 140031916288000 adafactor.py:358] Since rank of parameter decoder/layers_15/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.870864 140031916288000 adafactor.py:358] Since rank of parameter decoder/layers_15/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.872009 140031916288000 adafactor.py:358] Since rank of parameter decoder/layers_15/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:13.873092 140031916288000 adafactor.py:358] Since rank of parameter decoder/layers_15/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:13.873998 140031916288000 adafactor.py:358] Since rank of parameter decoder/layers_15/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:13.964496 140595318249472 adafactor.py:358] Since rank of parameter decoder/layers_8/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:13.875541 140031916288000 adafactor.py:358] Since rank of parameter decoder/layers_15/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.876653 140031916288000 adafactor.py:358] Since rank of parameter decoder/layers_15/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:13.877757 140031916288000 adafactor.py:358] Since rank of parameter decoder/layers_15/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.878877 140031916288000 adafactor.py:358] Since rank of parameter decoder/layers_15/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.880016 140031916288000 adafactor.py:358] Since rank of parameter decoder/layers_16/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.881115 140031916288000 adafactor.py:358] Since rank of parameter decoder/layers_16/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.882229 140031916288000 adafactor.py:358] Since rank of parameter decoder/layers_16/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:13.883318 140031916288000 adafactor.py:358] Since rank of parameter decoder/layers_16/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:13.884290 140031916288000 adafactor.py:358] Since rank of parameter decoder/layers_16/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:13.965550 140595318249472 adafactor.py:358] Since rank of parameter decoder/layers_8/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:13.885185 140031916288000 adafactor.py:358] Since rank of parameter decoder/layers_16/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.886340 140031916288000 adafactor.py:358] Since rank of parameter decoder/layers_16/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:13.887438 140031916288000 adafactor.py:358] Since rank of parameter decoder/layers_16/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.888532 140031916288000 adafactor.py:358] Since rank of parameter decoder/layers_16/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.889669 140031916288000 adafactor.py:358] Since rank of parameter decoder/layers_17/extra_mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.890792 140031916288000 adafactor.py:358] Since rank of parameter decoder/layers_17/extra_mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.891872 140031916288000 adafactor.py:358] Since rank of parameter decoder/layers_17/extra_mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:13.896012 140031916288000 adafactor.py:358] Since rank of parameter decoder/layers_17/mlp/router/router_weights/w/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>, <FactorDim.NONE: None>) is ignored.
W0512 23:42:13.966440 140595318249472 adafactor.py:358] Since rank of parameter decoder/layers_8/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.896924 140031916288000 adafactor.py:358] Since rank of parameter decoder/layers_17/pre_extra_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:13.897870 140031916288000 adafactor.py:358] Since rank of parameter decoder/layers_17/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:13.898802 140031916288000 adafactor.py:358] Since rank of parameter decoder/layers_17/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:13.899699 140031916288000 adafactor.py:358] Since rank of parameter decoder/layers_17/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.900790 140031916288000 adafactor.py:358] Since rank of parameter decoder/layers_17/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:13.901935 140031916288000 adafactor.py:358] Since rank of parameter decoder/layers_17/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.903060 140031916288000 adafactor.py:358] Since rank of parameter decoder/layers_17/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.904179 140031916288000 adafactor.py:358] Since rank of parameter decoder/layers_18/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.905269 140031916288000 adafactor.py:358] Since rank of parameter decoder/layers_18/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.967520 140595318249472 adafactor.py:358] Since rank of parameter decoder/layers_8/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:13.906445 140031916288000 adafactor.py:358] Since rank of parameter decoder/layers_18/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:13.907527 140031916288000 adafactor.py:358] Since rank of parameter decoder/layers_18/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:13.908433 140031916288000 adafactor.py:358] Since rank of parameter decoder/layers_18/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:13.909333 140031916288000 adafactor.py:358] Since rank of parameter decoder/layers_18/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.910493 140031916288000 adafactor.py:358] Since rank of parameter decoder/layers_18/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:13.911581 140031916288000 adafactor.py:358] Since rank of parameter decoder/layers_18/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.912677 140031916288000 adafactor.py:358] Since rank of parameter decoder/layers_18/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.913771 140031916288000 adafactor.py:358] Since rank of parameter decoder/layers_19/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.914946 140031916288000 adafactor.py:358] Since rank of parameter decoder/layers_19/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.968591 140595318249472 adafactor.py:358] Since rank of parameter decoder/layers_8/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.916049 140031916288000 adafactor.py:358] Since rank of parameter decoder/layers_19/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:13.917136 140031916288000 adafactor.py:358] Since rank of parameter decoder/layers_19/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:13.918030 140031916288000 adafactor.py:358] Since rank of parameter decoder/layers_19/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:13.918946 140031916288000 adafactor.py:358] Since rank of parameter decoder/layers_19/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.920131 140031916288000 adafactor.py:358] Since rank of parameter decoder/layers_19/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:13.921226 140031916288000 adafactor.py:358] Since rank of parameter decoder/layers_19/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.922364 140031916288000 adafactor.py:358] Since rank of parameter decoder/layers_19/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.923501 140031916288000 adafactor.py:358] Since rank of parameter decoder/layers_2/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.969739 140595318249472 adafactor.py:358] Since rank of parameter decoder/layers_8/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.924631 140031916288000 adafactor.py:358] Since rank of parameter decoder/layers_2/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.925721 140031916288000 adafactor.py:358] Since rank of parameter decoder/layers_2/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:13.926831 140031916288000 adafactor.py:358] Since rank of parameter decoder/layers_2/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:13.927723 140031916288000 adafactor.py:358] Since rank of parameter decoder/layers_2/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:13.929095 140031916288000 adafactor.py:358] Since rank of parameter decoder/layers_2/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.930222 140031916288000 adafactor.py:358] Since rank of parameter decoder/layers_2/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:13.931317 140031916288000 adafactor.py:358] Since rank of parameter decoder/layers_2/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.932412 140031916288000 adafactor.py:358] Since rank of parameter decoder/layers_2/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.933558 140031916288000 adafactor.py:358] Since rank of parameter decoder/layers_20/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.970841 140595318249472 adafactor.py:358] Since rank of parameter decoder/layers_9/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.934683 140031916288000 adafactor.py:358] Since rank of parameter decoder/layers_20/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.935784 140031916288000 adafactor.py:358] Since rank of parameter decoder/layers_20/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:13.936874 140031916288000 adafactor.py:358] Since rank of parameter decoder/layers_20/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:13.937986 140031916288000 adafactor.py:358] Since rank of parameter decoder/layers_20/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:13.938905 140031916288000 adafactor.py:358] Since rank of parameter decoder/layers_20/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.939992 140031916288000 adafactor.py:358] Since rank of parameter decoder/layers_20/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:13.941075 140031916288000 adafactor.py:358] Since rank of parameter decoder/layers_20/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.942229 140031916288000 adafactor.py:358] Since rank of parameter decoder/layers_20/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.943326 140031916288000 adafactor.py:358] Since rank of parameter decoder/layers_21/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.971912 140595318249472 adafactor.py:358] Since rank of parameter decoder/layers_9/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.944433 140031916288000 adafactor.py:358] Since rank of parameter decoder/layers_21/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.945517 140031916288000 adafactor.py:358] Since rank of parameter decoder/layers_21/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:13.946728 140031916288000 adafactor.py:358] Since rank of parameter decoder/layers_21/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:13.947599 140031916288000 adafactor.py:358] Since rank of parameter decoder/layers_21/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:13.948472 140031916288000 adafactor.py:358] Since rank of parameter decoder/layers_21/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.949555 140031916288000 adafactor.py:358] Since rank of parameter decoder/layers_21/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:13.950721 140031916288000 adafactor.py:358] Since rank of parameter decoder/layers_21/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.951797 140031916288000 adafactor.py:358] Since rank of parameter decoder/layers_21/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.973196 140595318249472 adafactor.py:358] Since rank of parameter decoder/layers_9/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:13.952888 140031916288000 adafactor.py:358] Since rank of parameter decoder/layers_22/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.953975 140031916288000 adafactor.py:358] Since rank of parameter decoder/layers_22/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.955135 140031916288000 adafactor.py:358] Since rank of parameter decoder/layers_22/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:13.956219 140031916288000 adafactor.py:358] Since rank of parameter decoder/layers_22/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:13.957120 140031916288000 adafactor.py:358] Since rank of parameter decoder/layers_22/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:13.958001 140031916288000 adafactor.py:358] Since rank of parameter decoder/layers_22/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.959162 140031916288000 adafactor.py:358] Since rank of parameter decoder/layers_22/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:13.960233 140031916288000 adafactor.py:358] Since rank of parameter decoder/layers_22/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.961319 140031916288000 adafactor.py:358] Since rank of parameter decoder/layers_22/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.974282 140595318249472 adafactor.py:358] Since rank of parameter decoder/layers_9/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:13.962434 140031916288000 adafactor.py:358] Since rank of parameter decoder/layers_23/extra_mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.963570 140031916288000 adafactor.py:358] Since rank of parameter decoder/layers_23/extra_mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.964684 140031916288000 adafactor.py:358] Since rank of parameter decoder/layers_23/extra_mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:13.968761 140031916288000 adafactor.py:358] Since rank of parameter decoder/layers_23/mlp/router/router_weights/w/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>, <FactorDim.NONE: None>) is ignored.
W0512 23:42:13.969658 140031916288000 adafactor.py:358] Since rank of parameter decoder/layers_23/pre_extra_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:13.970564 140031916288000 adafactor.py:358] Since rank of parameter decoder/layers_23/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:13.971456 140031916288000 adafactor.py:358] Since rank of parameter decoder/layers_23/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:13.972410 140031916288000 adafactor.py:358] Since rank of parameter decoder/layers_23/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.975150 140595318249472 adafactor.py:358] Since rank of parameter decoder/layers_9/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:13.976027 140595318249472 adafactor.py:358] Since rank of parameter decoder/layers_9/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.973497 140031916288000 adafactor.py:358] Since rank of parameter decoder/layers_23/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:13.974785 140031916288000 adafactor.py:358] Since rank of parameter decoder/layers_23/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.975850 140031916288000 adafactor.py:358] Since rank of parameter decoder/layers_23/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.977128 140595318249472 adafactor.py:358] Since rank of parameter decoder/layers_9/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:13.976984 140031916288000 adafactor.py:358] Since rank of parameter decoder/layers_3/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.843578 140529434355712 adafactor.py:358] Since rank of parameter decoder/layers_14/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:13.844473 140529434355712 adafactor.py:358] Since rank of parameter decoder/layers_14/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.845612 140529434355712 adafactor.py:358] Since rank of parameter decoder/layers_14/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:13.846791 140529434355712 adafactor.py:358] Since rank of parameter decoder/layers_14/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.847911 140529434355712 adafactor.py:358] Since rank of parameter decoder/layers_14/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.849017 140529434355712 adafactor.py:358] Since rank of parameter decoder/layers_15/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.850166 140529434355712 adafactor.py:358] Since rank of parameter decoder/layers_15/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.851336 140529434355712 adafactor.py:358] Since rank of parameter decoder/layers_15/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:13.978243 140595318249472 adafactor.py:358] Since rank of parameter decoder/layers_9/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.978062 140031916288000 adafactor.py:358] Since rank of parameter decoder/layers_3/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.852404 140529434355712 adafactor.py:358] Since rank of parameter decoder/layers_15/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:13.853296 140529434355712 adafactor.py:358] Since rank of parameter decoder/layers_15/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:13.855003 140529434355712 adafactor.py:358] Since rank of parameter decoder/layers_15/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.856173 140529434355712 adafactor.py:358] Since rank of parameter decoder/layers_15/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:13.857300 140529434355712 adafactor.py:358] Since rank of parameter decoder/layers_15/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.858448 140529434355712 adafactor.py:358] Since rank of parameter decoder/layers_15/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.859606 140529434355712 adafactor.py:358] Since rank of parameter decoder/layers_16/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.860721 140529434355712 adafactor.py:358] Since rank of parameter decoder/layers_16/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.861841 140529434355712 adafactor.py:358] Since rank of parameter decoder/layers_16/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:13.979313 140595318249472 adafactor.py:358] Since rank of parameter decoder/layers_9/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.979157 140031916288000 adafactor.py:358] Since rank of parameter decoder/layers_3/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:13.980386 140595318249472 adafactor.py:358] Since rank of parameter decoder/logits_dense/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.862917 140529434355712 adafactor.py:358] Since rank of parameter decoder/layers_16/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:13.863838 140529434355712 adafactor.py:358] Since rank of parameter decoder/layers_16/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:13.864715 140529434355712 adafactor.py:358] Since rank of parameter decoder/layers_16/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.865825 140529434355712 adafactor.py:358] Since rank of parameter decoder/layers_16/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:13.866898 140529434355712 adafactor.py:358] Since rank of parameter decoder/layers_16/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.867978 140529434355712 adafactor.py:358] Since rank of parameter decoder/layers_16/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.869100 140529434355712 adafactor.py:358] Since rank of parameter decoder/layers_17/extra_mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.870213 140529434355712 adafactor.py:358] Since rank of parameter decoder/layers_17/extra_mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.980226 140031916288000 adafactor.py:358] Since rank of parameter decoder/layers_3/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:13.871282 140529434355712 adafactor.py:358] Since rank of parameter decoder/layers_17/extra_mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:13.875373 140529434355712 adafactor.py:358] Since rank of parameter decoder/layers_17/mlp/router/router_weights/w/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>, <FactorDim.NONE: None>) is ignored.
W0512 23:42:13.876307 140529434355712 adafactor.py:358] Since rank of parameter decoder/layers_17/pre_extra_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:13.877236 140529434355712 adafactor.py:358] Since rank of parameter decoder/layers_17/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:13.878148 140529434355712 adafactor.py:358] Since rank of parameter decoder/layers_17/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:13.879019 140529434355712 adafactor.py:358] Since rank of parameter decoder/layers_17/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.880112 140529434355712 adafactor.py:358] Since rank of parameter decoder/layers_17/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:13.881244 140529434355712 adafactor.py:358] Since rank of parameter decoder/layers_17/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.882350 140529434355712 adafactor.py:358] Since rank of parameter decoder/layers_17/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.981497 140595318249472 adafactor.py:358] Since rank of parameter token_embedder/embedding 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:13.981597 140031916288000 adafactor.py:358] Since rank of parameter decoder/layers_3/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:13.883713 140529434355712 adafactor.py:358] Since rank of parameter decoder/layers_18/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.885152 140529434355712 adafactor.py:358] Since rank of parameter decoder/layers_18/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.886350 140529434355712 adafactor.py:358] Since rank of parameter decoder/layers_18/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:13.887428 140529434355712 adafactor.py:358] Since rank of parameter decoder/layers_18/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:13.888321 140529434355712 adafactor.py:358] Since rank of parameter decoder/layers_18/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:13.889204 140529434355712 adafactor.py:358] Since rank of parameter decoder/layers_18/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.890399 140529434355712 adafactor.py:358] Since rank of parameter decoder/layers_18/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:13.891517 140529434355712 adafactor.py:358] Since rank of parameter decoder/layers_18/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.892635 140529434355712 adafactor.py:358] Since rank of parameter decoder/layers_18/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.982503 140031916288000 adafactor.py:358] Since rank of parameter decoder/layers_3/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.893779 140529434355712 adafactor.py:358] Since rank of parameter decoder/layers_19/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.894932 140529434355712 adafactor.py:358] Since rank of parameter decoder/layers_19/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.896026 140529434355712 adafactor.py:358] Since rank of parameter decoder/layers_19/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:13.897118 140529434355712 adafactor.py:358] Since rank of parameter decoder/layers_19/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:13.898026 140529434355712 adafactor.py:358] Since rank of parameter decoder/layers_19/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:13.898904 140529434355712 adafactor.py:358] Since rank of parameter decoder/layers_19/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.900043 140529434355712 adafactor.py:358] Since rank of parameter decoder/layers_19/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:13.901112 140529434355712 adafactor.py:358] Since rank of parameter decoder/layers_19/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.902230 140529434355712 adafactor.py:358] Since rank of parameter decoder/layers_19/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.903357 140529434355712 adafactor.py:358] Since rank of parameter decoder/layers_2/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.904424 140529434355712 adafactor.py:358] Since rank of parameter decoder/layers_2/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.905504 140529434355712 adafactor.py:358] Since rank of parameter decoder/layers_2/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:13.906615 140529434355712 adafactor.py:358] Since rank of parameter decoder/layers_2/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:13.907480 140529434355712 adafactor.py:358] Since rank of parameter decoder/layers_2/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:13.908860 140529434355712 adafactor.py:358] Since rank of parameter decoder/layers_2/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.909964 140529434355712 adafactor.py:358] Since rank of parameter decoder/layers_2/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:13.911029 140529434355712 adafactor.py:358] Since rank of parameter decoder/layers_2/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.983585 140031916288000 adafactor.py:358] Since rank of parameter decoder/layers_3/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:13.912100 140529434355712 adafactor.py:358] Since rank of parameter decoder/layers_2/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.913226 140529434355712 adafactor.py:358] Since rank of parameter decoder/layers_20/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.914324 140529434355712 adafactor.py:358] Since rank of parameter decoder/layers_20/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.915404 140529434355712 adafactor.py:358] Since rank of parameter decoder/layers_20/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:13.916509 140529434355712 adafactor.py:358] Since rank of parameter decoder/layers_20/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:13.917454 140529434355712 adafactor.py:358] Since rank of parameter decoder/layers_20/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:13.918363 140529434355712 adafactor.py:358] Since rank of parameter decoder/layers_20/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.919430 140529434355712 adafactor.py:358] Since rank of parameter decoder/layers_20/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:13.920503 140529434355712 adafactor.py:358] Since rank of parameter decoder/layers_20/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.984701 140031916288000 adafactor.py:358] Since rank of parameter decoder/layers_3/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.921675 140529434355712 adafactor.py:358] Since rank of parameter decoder/layers_20/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.922760 140529434355712 adafactor.py:358] Since rank of parameter decoder/layers_21/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.923854 140529434355712 adafactor.py:358] Since rank of parameter decoder/layers_21/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.924918 140529434355712 adafactor.py:358] Since rank of parameter decoder/layers_21/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:13.926076 140529434355712 adafactor.py:358] Since rank of parameter decoder/layers_21/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:13.926937 140529434355712 adafactor.py:358] Since rank of parameter decoder/layers_21/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:13.927806 140529434355712 adafactor.py:358] Since rank of parameter decoder/layers_21/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.928888 140529434355712 adafactor.py:358] Since rank of parameter decoder/layers_21/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:13.985832 140031916288000 adafactor.py:358] Since rank of parameter decoder/layers_3/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.986940 140031916288000 adafactor.py:358] Since rank of parameter decoder/layers_4/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.930034 140529434355712 adafactor.py:358] Since rank of parameter decoder/layers_21/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.931117 140529434355712 adafactor.py:358] Since rank of parameter decoder/layers_21/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.932184 140529434355712 adafactor.py:358] Since rank of parameter decoder/layers_22/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.933256 140529434355712 adafactor.py:358] Since rank of parameter decoder/layers_22/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.934411 140529434355712 adafactor.py:358] Since rank of parameter decoder/layers_22/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:13.935486 140529434355712 adafactor.py:358] Since rank of parameter decoder/layers_22/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:13.936410 140529434355712 adafactor.py:358] Since rank of parameter decoder/layers_22/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:13.937284 140529434355712 adafactor.py:358] Since rank of parameter decoder/layers_22/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.938435 140529434355712 adafactor.py:358] Since rank of parameter decoder/layers_22/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:13.988031 140031916288000 adafactor.py:358] Since rank of parameter decoder/layers_4/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.939502 140529434355712 adafactor.py:358] Since rank of parameter decoder/layers_22/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.940584 140529434355712 adafactor.py:358] Since rank of parameter decoder/layers_22/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.941677 140529434355712 adafactor.py:358] Since rank of parameter decoder/layers_23/extra_mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.942796 140529434355712 adafactor.py:358] Since rank of parameter decoder/layers_23/extra_mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.943853 140529434355712 adafactor.py:358] Since rank of parameter decoder/layers_23/extra_mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:13.947927 140529434355712 adafactor.py:358] Since rank of parameter decoder/layers_23/mlp/router/router_weights/w/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>, <FactorDim.NONE: None>) is ignored.
W0512 23:42:13.948815 140529434355712 adafactor.py:358] Since rank of parameter decoder/layers_23/pre_extra_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:13.949731 140529434355712 adafactor.py:358] Since rank of parameter decoder/layers_23/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:13.950619 140529434355712 adafactor.py:358] Since rank of parameter decoder/layers_23/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:13.951575 140529434355712 adafactor.py:358] Since rank of parameter decoder/layers_23/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.952639 140529434355712 adafactor.py:358] Since rank of parameter decoder/layers_23/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:13.953904 140529434355712 adafactor.py:358] Since rank of parameter decoder/layers_23/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.954968 140529434355712 adafactor.py:358] Since rank of parameter decoder/layers_23/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.956115 140529434355712 adafactor.py:358] Since rank of parameter decoder/layers_3/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.957176 140529434355712 adafactor.py:358] Since rank of parameter decoder/layers_3/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.958267 140529434355712 adafactor.py:358] Since rank of parameter decoder/layers_3/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:13.959341 140529434355712 adafactor.py:358] Since rank of parameter decoder/layers_3/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:13.989116 140031916288000 adafactor.py:358] Since rank of parameter decoder/layers_4/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:13.960693 140529434355712 adafactor.py:358] Since rank of parameter decoder/layers_3/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:13.961595 140529434355712 adafactor.py:358] Since rank of parameter decoder/layers_3/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.962668 140529434355712 adafactor.py:358] Since rank of parameter decoder/layers_3/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:13.963719 140529434355712 adafactor.py:358] Since rank of parameter decoder/layers_3/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.964876 140529434355712 adafactor.py:358] Since rank of parameter decoder/layers_3/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.965951 140529434355712 adafactor.py:358] Since rank of parameter decoder/layers_4/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.967009 140529434355712 adafactor.py:358] Since rank of parameter decoder/layers_4/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.968046 140529434355712 adafactor.py:358] Since rank of parameter decoder/layers_4/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:13.990272 140031916288000 adafactor.py:358] Since rank of parameter decoder/layers_4/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:13.991158 140031916288000 adafactor.py:358] Since rank of parameter decoder/layers_4/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:13.969144 140529434355712 adafactor.py:358] Since rank of parameter decoder/layers_4/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:13.970165 140529434355712 adafactor.py:358] Since rank of parameter decoder/layers_4/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:13.971021 140529434355712 adafactor.py:358] Since rank of parameter decoder/layers_4/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.972084 140529434355712 adafactor.py:358] Since rank of parameter decoder/layers_4/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:13.973187 140529434355712 adafactor.py:358] Since rank of parameter decoder/layers_4/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.992042 140031916288000 adafactor.py:358] Since rank of parameter decoder/layers_4/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.974272 140529434355712 adafactor.py:358] Since rank of parameter decoder/layers_4/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.975351 140529434355712 adafactor.py:358] Since rank of parameter decoder/layers_5/extra_mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.993138 140031916288000 adafactor.py:358] Since rank of parameter decoder/layers_4/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:13.976440 140529434355712 adafactor.py:358] Since rank of parameter decoder/layers_5/extra_mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.994307 140031916288000 adafactor.py:358] Since rank of parameter decoder/layers_4/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.977611 140529434355712 adafactor.py:358] Since rank of parameter decoder/layers_5/extra_mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:13.995393 140031916288000 adafactor.py:358] Since rank of parameter decoder/layers_4/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.996476 140031916288000 adafactor.py:358] Since rank of parameter decoder/layers_5/extra_mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.997558 140031916288000 adafactor.py:358] Since rank of parameter decoder/layers_5/extra_mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.998728 140031916288000 adafactor.py:358] Since rank of parameter decoder/layers_5/extra_mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:13.981750 140529434355712 adafactor.py:358] Since rank of parameter decoder/layers_5/mlp/router/router_weights/w/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>, <FactorDim.NONE: None>) is ignored.
W0512 23:42:13.982663 140529434355712 adafactor.py:358] Since rank of parameter decoder/layers_5/pre_extra_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:13.983550 140529434355712 adafactor.py:358] Since rank of parameter decoder/layers_5/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:13.984420 140529434355712 adafactor.py:358] Since rank of parameter decoder/layers_5/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:13.985300 140529434355712 adafactor.py:358] Since rank of parameter decoder/layers_5/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:14.002788 140031916288000 adafactor.py:358] Since rank of parameter decoder/layers_5/mlp/router/router_weights/w/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>, <FactorDim.NONE: None>) is ignored.
W0512 23:42:14.003690 140031916288000 adafactor.py:358] Since rank of parameter decoder/layers_5/pre_extra_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:13.986467 140529434355712 adafactor.py:358] Since rank of parameter decoder/layers_5/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:14.004580 140031916288000 adafactor.py:358] Since rank of parameter decoder/layers_5/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:13.987606 140529434355712 adafactor.py:358] Since rank of parameter decoder/layers_5/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:14.005464 140031916288000 adafactor.py:358] Since rank of parameter decoder/layers_5/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:13.988722 140529434355712 adafactor.py:358] Since rank of parameter decoder/layers_5/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:14.006371 140031916288000 adafactor.py:358] Since rank of parameter decoder/layers_5/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.989897 140529434355712 adafactor.py:358] Since rank of parameter decoder/layers_6/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:14.007511 140031916288000 adafactor.py:358] Since rank of parameter decoder/layers_5/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:14.008593 140031916288000 adafactor.py:358] Since rank of parameter decoder/layers_5/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.991091 140529434355712 adafactor.py:358] Since rank of parameter decoder/layers_6/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:14.009681 140031916288000 adafactor.py:358] Since rank of parameter decoder/layers_5/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.992189 140529434355712 adafactor.py:358] Since rank of parameter decoder/layers_6/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:14.010817 140031916288000 adafactor.py:358] Since rank of parameter decoder/layers_6/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.993291 140529434355712 adafactor.py:358] Since rank of parameter decoder/layers_6/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:13.994204 140529434355712 adafactor.py:358] Since rank of parameter decoder/layers_6/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:14.011966 140031916288000 adafactor.py:358] Since rank of parameter decoder/layers_6/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.995146 140529434355712 adafactor.py:358] Since rank of parameter decoder/layers_6/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:14.013045 140031916288000 adafactor.py:358] Since rank of parameter decoder/layers_6/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:13.996299 140529434355712 adafactor.py:358] Since rank of parameter decoder/layers_6/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:14.014156 140031916288000 adafactor.py:358] Since rank of parameter decoder/layers_6/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:13.997404 140529434355712 adafactor.py:358] Since rank of parameter decoder/layers_6/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:14.015044 140031916288000 adafactor.py:358] Since rank of parameter decoder/layers_6/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:14.015982 140031916288000 adafactor.py:358] Since rank of parameter decoder/layers_6/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:13.998557 140529434355712 adafactor.py:358] Since rank of parameter decoder/layers_6/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:14.017079 140031916288000 adafactor.py:358] Since rank of parameter decoder/layers_6/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:13.999718 140529434355712 adafactor.py:358] Since rank of parameter decoder/layers_7/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:14.000834 140529434355712 adafactor.py:358] Since rank of parameter decoder/layers_7/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:14.018183 140031916288000 adafactor.py:358] Since rank of parameter decoder/layers_6/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:14.019266 140031916288000 adafactor.py:358] Since rank of parameter decoder/layers_6/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:14.001945 140529434355712 adafactor.py:358] Since rank of parameter decoder/layers_7/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:14.020404 140031916288000 adafactor.py:358] Since rank of parameter decoder/layers_7/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:14.003040 140529434355712 adafactor.py:358] Since rank of parameter decoder/layers_7/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:14.003970 140529434355712 adafactor.py:358] Since rank of parameter decoder/layers_7/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:14.021488 140031916288000 adafactor.py:358] Since rank of parameter decoder/layers_7/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:14.004839 140529434355712 adafactor.py:358] Since rank of parameter decoder/layers_7/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:14.022590 140031916288000 adafactor.py:358] Since rank of parameter decoder/layers_7/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:14.005937 140529434355712 adafactor.py:358] Since rank of parameter decoder/layers_7/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:14.023677 140031916288000 adafactor.py:358] Since rank of parameter decoder/layers_7/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:14.007009 140529434355712 adafactor.py:358] Since rank of parameter decoder/layers_7/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:14.024643 140031916288000 adafactor.py:358] Since rank of parameter decoder/layers_7/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:14.025523 140031916288000 adafactor.py:358] Since rank of parameter decoder/layers_7/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:14.008139 140529434355712 adafactor.py:358] Since rank of parameter decoder/layers_7/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:14.026645 140031916288000 adafactor.py:358] Since rank of parameter decoder/layers_7/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:14.009233 140529434355712 adafactor.py:358] Since rank of parameter decoder/layers_8/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:14.027727 140031916288000 adafactor.py:358] Since rank of parameter decoder/layers_7/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:14.010328 140529434355712 adafactor.py:358] Since rank of parameter decoder/layers_8/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:14.028856 140031916288000 adafactor.py:358] Since rank of parameter decoder/layers_7/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:14.011420 140529434355712 adafactor.py:358] Since rank of parameter decoder/layers_8/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:14.029940 140031916288000 adafactor.py:358] Since rank of parameter decoder/layers_8/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:14.013085 140529434355712 adafactor.py:358] Since rank of parameter decoder/layers_8/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:14.031059 140031916288000 adafactor.py:358] Since rank of parameter decoder/layers_8/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:14.014025 140529434355712 adafactor.py:358] Since rank of parameter decoder/layers_8/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:14.032140 140031916288000 adafactor.py:358] Since rank of parameter decoder/layers_8/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:14.014936 140529434355712 adafactor.py:358] Since rank of parameter decoder/layers_8/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:14.016089 140529434355712 adafactor.py:358] Since rank of parameter decoder/layers_8/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:14.033722 140031916288000 adafactor.py:358] Since rank of parameter decoder/layers_8/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:14.017195 140529434355712 adafactor.py:358] Since rank of parameter decoder/layers_8/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:14.034827 140031916288000 adafactor.py:358] Since rank of parameter decoder/layers_8/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:14.035721 140031916288000 adafactor.py:358] Since rank of parameter decoder/layers_8/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:14.018375 140529434355712 adafactor.py:358] Since rank of parameter decoder/layers_8/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:14.036797 140031916288000 adafactor.py:358] Since rank of parameter decoder/layers_8/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:14.019516 140529434355712 adafactor.py:358] Since rank of parameter decoder/layers_9/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:14.037876 140031916288000 adafactor.py:358] Since rank of parameter decoder/layers_8/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:14.020627 140529434355712 adafactor.py:358] Since rank of parameter decoder/layers_9/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:14.039022 140031916288000 adafactor.py:358] Since rank of parameter decoder/layers_8/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:14.021823 140529434355712 adafactor.py:358] Since rank of parameter decoder/layers_9/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:14.040131 140031916288000 adafactor.py:358] Since rank of parameter decoder/layers_9/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:14.022943 140529434355712 adafactor.py:358] Since rank of parameter decoder/layers_9/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:14.023841 140529434355712 adafactor.py:358] Since rank of parameter decoder/layers_9/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:14.041206 140031916288000 adafactor.py:358] Since rank of parameter decoder/layers_9/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:14.024741 140529434355712 adafactor.py:358] Since rank of parameter decoder/layers_9/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:14.042354 140031916288000 adafactor.py:358] Since rank of parameter decoder/layers_9/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:14.025886 140529434355712 adafactor.py:358] Since rank of parameter decoder/layers_9/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:14.043461 140031916288000 adafactor.py:358] Since rank of parameter decoder/layers_9/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:14.044374 140031916288000 adafactor.py:358] Since rank of parameter decoder/layers_9/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:14.027045 140529434355712 adafactor.py:358] Since rank of parameter decoder/layers_9/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:14.045257 140031916288000 adafactor.py:358] Since rank of parameter decoder/layers_9/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:14.028171 140529434355712 adafactor.py:358] Since rank of parameter decoder/layers_9/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:14.046374 140031916288000 adafactor.py:358] Since rank of parameter decoder/layers_9/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:14.029294 140529434355712 adafactor.py:358] Since rank of parameter decoder/logits_dense/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:14.047490 140031916288000 adafactor.py:358] Since rank of parameter decoder/layers_9/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:14.030430 140529434355712 adafactor.py:358] Since rank of parameter token_embedder/embedding 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:14.048563 140031916288000 adafactor.py:358] Since rank of parameter decoder/layers_9/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:14.049643 140031916288000 adafactor.py:358] Since rank of parameter decoder/logits_dense/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:14.050777 140031916288000 adafactor.py:358] Since rank of parameter token_embedder/embedding 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
I0512 23:42:14.747720 140339465779200 utils.py:952] Using latest T5X checkpoint.
I0512 23:42:14.817999 139674595104768 utils.py:952] Using latest T5X checkpoint.
I0512 23:42:14.850789 140311617681408 utils.py:952] Using latest T5X checkpoint.
I0512 23:42:14.875505 140440593786880 utils.py:952] Using latest T5X checkpoint.
I0512 23:42:14.978005 140595318249472 utils.py:952] Using latest T5X checkpoint.
I0512 23:42:14.986636 140095893518336 utils.py:952] Using latest T5X checkpoint.
I0512 23:42:15.004810 140031916288000 utils.py:952] Using latest T5X checkpoint.
I0512 23:42:15.010971 140529434355712 utils.py:952] Using latest T5X checkpoint.
I0512 23:42:15.634646 140339465779200 utils.py:442] Initializing parameters from specific T5X checkpoint gs://rosinality-tpu-bucket/openmoe_8b/training/checkpoint_32500
I0512 23:42:15.709557 139674595104768 utils.py:442] Initializing parameters from specific T5X checkpoint gs://rosinality-tpu-bucket/openmoe_8b/training/checkpoint_32500
I0512 23:42:15.759644 140440593786880 utils.py:442] Initializing parameters from specific T5X checkpoint gs://rosinality-tpu-bucket/openmoe_8b/training/checkpoint_32500
I0512 23:42:15.744291 140311617681408 utils.py:442] Initializing parameters from specific T5X checkpoint gs://rosinality-tpu-bucket/openmoe_8b/training/checkpoint_32500
I0512 23:42:15.863947 140595318249472 utils.py:442] Initializing parameters from specific T5X checkpoint gs://rosinality-tpu-bucket/openmoe_8b/training/checkpoint_32500
I0512 23:42:15.873412 140095893518336 utils.py:442] Initializing parameters from specific T5X checkpoint gs://rosinality-tpu-bucket/openmoe_8b/training/checkpoint_32500
I0512 23:42:15.883119 140031916288000 utils.py:442] Initializing parameters from specific T5X checkpoint gs://rosinality-tpu-bucket/openmoe_8b/training/checkpoint_32500
I0512 23:42:15.908143 140529434355712 utils.py:442] Initializing parameters from specific T5X checkpoint gs://rosinality-tpu-bucket/openmoe_8b/training/checkpoint_32500
I0512 23:42:15.968644 140339465779200 checkpoints.py:1054] Restoring from checkpoint: gs://rosinality-tpu-bucket/openmoe_8b/training/checkpoint_32500/checkpoint
I0512 23:42:16.067246 139674595104768 checkpoints.py:1054] Restoring from checkpoint: gs://rosinality-tpu-bucket/openmoe_8b/training/checkpoint_32500/checkpoint
I0512 23:42:16.087978 140311617681408 checkpoints.py:1054] Restoring from checkpoint: gs://rosinality-tpu-bucket/openmoe_8b/training/checkpoint_32500/checkpoint
I0512 23:42:16.124168 140440593786880 checkpoints.py:1054] Restoring from checkpoint: gs://rosinality-tpu-bucket/openmoe_8b/training/checkpoint_32500/checkpoint
I0512 23:42:16.145256 140095893518336 checkpoints.py:1054] Restoring from checkpoint: gs://rosinality-tpu-bucket/openmoe_8b/training/checkpoint_32500/checkpoint
I0512 23:42:16.143710 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/decoder_norm/scale/m
I0512 23:42:16.143877 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/decoder_norm/scale/v
I0512 23:42:16.143917 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/decoder_norm/scale/v_col
I0512 23:42:16.143953 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/decoder_norm/scale/v_row
I0512 23:42:16.143988 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_0/mlp/wi_0/kernel/m
I0512 23:42:16.144023 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_0/mlp/wi_0/kernel/v
I0512 23:42:16.144058 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_0/mlp/wi_0/kernel/v_col
I0512 23:42:16.144092 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_0/mlp/wi_0/kernel/v_row
I0512 23:42:16.144126 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_0/mlp/wi_1/kernel/m
I0512 23:42:16.144161 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_0/mlp/wi_1/kernel/v
I0512 23:42:16.144221 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_0/mlp/wi_1/kernel/v_col
I0512 23:42:16.144257 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_0/mlp/wi_1/kernel/v_row
I0512 23:42:16.144291 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_0/mlp/wo/kernel/m
I0512 23:42:16.144326 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_0/mlp/wo/kernel/v
I0512 23:42:16.144370 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_0/mlp/wo/kernel/v_col
I0512 23:42:16.144407 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_0/mlp/wo/kernel/v_row
I0512 23:42:16.144441 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_0/pre_mlp_layer_norm/scale/m
I0512 23:42:16.144475 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_0/pre_mlp_layer_norm/scale/v
I0512 23:42:16.144510 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_0/pre_mlp_layer_norm/scale/v_col
I0512 23:42:16.144544 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_0/pre_mlp_layer_norm/scale/v_row
I0512 23:42:16.144578 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_0/pre_self_attention_layer_norm/scale/m
I0512 23:42:16.144613 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_0/pre_self_attention_layer_norm/scale/v
I0512 23:42:16.144649 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_0/pre_self_attention_layer_norm/scale/v_col
I0512 23:42:16.144683 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_0/pre_self_attention_layer_norm/scale/v_row
I0512 23:42:16.144718 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_0/self_attention/key/kernel/m
I0512 23:42:16.144752 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_0/self_attention/key/kernel/v
I0512 23:42:16.144786 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_0/self_attention/key/kernel/v_col
I0512 23:42:16.144820 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_0/self_attention/key/kernel/v_row
I0512 23:42:16.144854 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_0/self_attention/out/kernel/m
I0512 23:42:16.144888 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_0/self_attention/out/kernel/v
I0512 23:42:16.144922 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_0/self_attention/out/kernel/v_col
I0512 23:42:16.144956 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_0/self_attention/out/kernel/v_row
I0512 23:42:16.144989 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_0/self_attention/query/kernel/m
I0512 23:42:16.145023 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_0/self_attention/query/kernel/v
I0512 23:42:16.145056 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_0/self_attention/query/kernel/v_col
I0512 23:42:16.145091 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_0/self_attention/query/kernel/v_row
I0512 23:42:16.145125 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_0/self_attention/value/kernel/m
I0512 23:42:16.145159 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_0/self_attention/value/kernel/v
I0512 23:42:16.145192 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_0/self_attention/value/kernel/v_col
I0512 23:42:16.145226 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_0/self_attention/value/kernel/v_row
I0512 23:42:16.145260 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_1/mlp/wi_0/kernel/m
I0512 23:42:16.145294 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_1/mlp/wi_0/kernel/v
I0512 23:42:16.145328 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_1/mlp/wi_0/kernel/v_col
I0512 23:42:16.145368 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_1/mlp/wi_0/kernel/v_row
I0512 23:42:16.145404 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_1/mlp/wi_1/kernel/m
I0512 23:42:16.145438 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_1/mlp/wi_1/kernel/v
I0512 23:42:16.145472 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_1/mlp/wi_1/kernel/v_col
I0512 23:42:16.145506 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_1/mlp/wi_1/kernel/v_row
I0512 23:42:16.145540 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_1/mlp/wo/kernel/m
I0512 23:42:16.145574 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_1/mlp/wo/kernel/v
I0512 23:42:16.145608 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_1/mlp/wo/kernel/v_col
I0512 23:42:16.145645 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_1/mlp/wo/kernel/v_row
I0512 23:42:16.145679 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_1/pre_mlp_layer_norm/scale/m
I0512 23:42:16.145713 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_1/pre_mlp_layer_norm/scale/v
I0512 23:42:16.145747 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_1/pre_mlp_layer_norm/scale/v_col
I0512 23:42:16.145781 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_1/pre_mlp_layer_norm/scale/v_row
I0512 23:42:16.145816 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_1/pre_self_attention_layer_norm/scale/m
I0512 23:42:16.145850 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_1/pre_self_attention_layer_norm/scale/v
I0512 23:42:16.145884 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_1/pre_self_attention_layer_norm/scale/v_col
I0512 23:42:16.145919 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_1/pre_self_attention_layer_norm/scale/v_row
I0512 23:42:16.145954 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_1/self_attention/key/kernel/m
I0512 23:42:16.145988 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_1/self_attention/key/kernel/v
I0512 23:42:16.146022 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_1/self_attention/key/kernel/v_col
I0512 23:42:16.146056 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_1/self_attention/key/kernel/v_row
I0512 23:42:16.146090 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_1/self_attention/out/kernel/m
I0512 23:42:16.146124 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_1/self_attention/out/kernel/v
I0512 23:42:16.146157 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_1/self_attention/out/kernel/v_col
I0512 23:42:16.146191 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_1/self_attention/out/kernel/v_row
I0512 23:42:16.146225 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_1/self_attention/query/kernel/m
I0512 23:42:16.146259 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_1/self_attention/query/kernel/v
I0512 23:42:16.146293 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_1/self_attention/query/kernel/v_col
I0512 23:42:16.146327 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_1/self_attention/query/kernel/v_row
I0512 23:42:16.146367 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_1/self_attention/value/kernel/m
I0512 23:42:16.146403 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_1/self_attention/value/kernel/v
I0512 23:42:16.146437 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_1/self_attention/value/kernel/v_col
I0512 23:42:16.146471 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_1/self_attention/value/kernel/v_row
I0512 23:42:16.146506 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_10/mlp/wi_0/kernel/m
I0512 23:42:16.146539 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_10/mlp/wi_0/kernel/v
I0512 23:42:16.146574 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_10/mlp/wi_0/kernel/v_col
I0512 23:42:16.146608 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_10/mlp/wi_0/kernel/v_row
I0512 23:42:16.146644 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_10/mlp/wi_1/kernel/m
I0512 23:42:16.146679 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_10/mlp/wi_1/kernel/v
I0512 23:42:16.146712 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_10/mlp/wi_1/kernel/v_col
I0512 23:42:16.146746 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_10/mlp/wi_1/kernel/v_row
I0512 23:42:16.146780 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_10/mlp/wo/kernel/m
I0512 23:42:16.146814 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_10/mlp/wo/kernel/v
I0512 23:42:16.146848 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_10/mlp/wo/kernel/v_col
I0512 23:42:16.146882 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_10/mlp/wo/kernel/v_row
I0512 23:42:16.146916 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_10/pre_mlp_layer_norm/scale/m
I0512 23:42:16.146950 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_10/pre_mlp_layer_norm/scale/v
I0512 23:42:16.146983 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_10/pre_mlp_layer_norm/scale/v_col
I0512 23:42:16.147017 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_10/pre_mlp_layer_norm/scale/v_row
I0512 23:42:16.147052 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_10/pre_self_attention_layer_norm/scale/m
I0512 23:42:16.147086 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_10/pre_self_attention_layer_norm/scale/v
I0512 23:42:16.147120 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_10/pre_self_attention_layer_norm/scale/v_col
I0512 23:42:16.147153 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_10/pre_self_attention_layer_norm/scale/v_row
I0512 23:42:16.147187 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_10/self_attention/key/kernel/m
I0512 23:42:16.147221 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_10/self_attention/key/kernel/v
I0512 23:42:16.147255 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_10/self_attention/key/kernel/v_col
I0512 23:42:16.147289 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_10/self_attention/key/kernel/v_row
I0512 23:42:16.147323 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_10/self_attention/out/kernel/m
I0512 23:42:16.147362 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_10/self_attention/out/kernel/v
I0512 23:42:16.147397 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_10/self_attention/out/kernel/v_col
I0512 23:42:16.147430 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_10/self_attention/out/kernel/v_row
I0512 23:42:16.147464 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_10/self_attention/query/kernel/m
I0512 23:42:16.147499 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_10/self_attention/query/kernel/v
I0512 23:42:16.147532 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_10/self_attention/query/kernel/v_col
I0512 23:42:16.147566 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_10/self_attention/query/kernel/v_row
I0512 23:42:16.147600 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_10/self_attention/value/kernel/m
I0512 23:42:16.147636 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_10/self_attention/value/kernel/v
I0512 23:42:16.147671 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_10/self_attention/value/kernel/v_col
I0512 23:42:16.147705 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_10/self_attention/value/kernel/v_row
I0512 23:42:16.147739 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/extra_mlp/wi_0/kernel/m
I0512 23:42:16.147773 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/extra_mlp/wi_0/kernel/v
I0512 23:42:16.147808 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/extra_mlp/wi_0/kernel/v_col
I0512 23:42:16.147841 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/extra_mlp/wi_0/kernel/v_row
I0512 23:42:16.147875 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/extra_mlp/wi_1/kernel/m
I0512 23:42:16.147908 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/extra_mlp/wi_1/kernel/v
I0512 23:42:16.147942 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/extra_mlp/wi_1/kernel/v_col
I0512 23:42:16.147976 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/extra_mlp/wi_1/kernel/v_row
I0512 23:42:16.148010 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/extra_mlp/wo/kernel/m
I0512 23:42:16.148044 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/extra_mlp/wo/kernel/v
I0512 23:42:16.148078 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/extra_mlp/wo/kernel/v_col
I0512 23:42:16.148112 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/extra_mlp/wo/kernel/v_row
I0512 23:42:16.148146 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/mlp/expert/wi_0/kernel/m
I0512 23:42:16.148201 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/mlp/expert/wi_0/kernel/v
I0512 23:42:16.148238 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/mlp/expert/wi_0/kernel/v_col
I0512 23:42:16.148273 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/mlp/expert/wi_0/kernel/v_row
I0512 23:42:16.148307 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/mlp/expert/wi_1/kernel/m
I0512 23:42:16.148341 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/mlp/expert/wi_1/kernel/v
I0512 23:42:16.148380 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/mlp/expert/wi_1/kernel/v_col
I0512 23:42:16.148415 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/mlp/expert/wi_1/kernel/v_row
I0512 23:42:16.148449 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/mlp/expert/wo/kernel/m
I0512 23:42:16.148483 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/mlp/expert/wo/kernel/v
I0512 23:42:16.148517 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/mlp/expert/wo/kernel/v_col
I0512 23:42:16.148551 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/mlp/expert/wo/kernel/v_row
I0512 23:42:16.148584 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/mlp/router/router_weights/w/kernel/m
I0512 23:42:16.148619 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/mlp/router/router_weights/w/kernel/v
I0512 23:42:16.148654 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/mlp/router/router_weights/w/kernel/v_col
I0512 23:42:16.148689 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/mlp/router/router_weights/w/kernel/v_row
I0512 23:42:16.148723 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/pre_extra_mlp_layer_norm/scale/m
I0512 23:42:16.148756 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/pre_extra_mlp_layer_norm/scale/v
I0512 23:42:16.148791 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/pre_extra_mlp_layer_norm/scale/v_col
I0512 23:42:16.148824 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/pre_extra_mlp_layer_norm/scale/v_row
I0512 23:42:16.148858 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/pre_mlp_layer_norm/scale/m
I0512 23:42:16.148892 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/pre_mlp_layer_norm/scale/v
I0512 23:42:16.148926 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/pre_mlp_layer_norm/scale/v_col
I0512 23:42:16.148960 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/pre_mlp_layer_norm/scale/v_row
I0512 23:42:16.148993 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/pre_self_attention_layer_norm/scale/m
I0512 23:42:16.149027 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/pre_self_attention_layer_norm/scale/v
I0512 23:42:16.149061 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/pre_self_attention_layer_norm/scale/v_col
I0512 23:42:16.149095 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/pre_self_attention_layer_norm/scale/v_row
I0512 23:42:16.149129 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/self_attention/key/kernel/m
I0512 23:42:16.149163 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/self_attention/key/kernel/v
I0512 23:42:16.149197 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/self_attention/key/kernel/v_col
I0512 23:42:16.149230 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/self_attention/key/kernel/v_row
I0512 23:42:16.149264 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/self_attention/out/kernel/m
I0512 23:42:16.149297 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/self_attention/out/kernel/v
I0512 23:42:16.149331 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/self_attention/out/kernel/v_col
I0512 23:42:16.149371 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/self_attention/out/kernel/v_row
I0512 23:42:16.149406 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/self_attention/query/kernel/m
I0512 23:42:16.149441 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/self_attention/query/kernel/v
I0512 23:42:16.149474 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/self_attention/query/kernel/v_col
I0512 23:42:16.149508 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/self_attention/query/kernel/v_row
I0512 23:42:16.149542 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/self_attention/value/kernel/m
I0512 23:42:16.149576 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/self_attention/value/kernel/v
I0512 23:42:16.149610 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/self_attention/value/kernel/v_col
I0512 23:42:16.149646 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/self_attention/value/kernel/v_row
I0512 23:42:16.149680 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_12/mlp/wi_0/kernel/m
I0512 23:42:16.149714 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_12/mlp/wi_0/kernel/v
I0512 23:42:16.149748 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_12/mlp/wi_0/kernel/v_col
I0512 23:42:16.149782 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_12/mlp/wi_0/kernel/v_row
I0512 23:42:16.149816 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_12/mlp/wi_1/kernel/m
I0512 23:42:16.149850 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_12/mlp/wi_1/kernel/v
I0512 23:42:16.149884 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_12/mlp/wi_1/kernel/v_col
I0512 23:42:16.149917 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_12/mlp/wi_1/kernel/v_row
I0512 23:42:16.149951 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_12/mlp/wo/kernel/m
I0512 23:42:16.149985 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_12/mlp/wo/kernel/v
I0512 23:42:16.150020 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_12/mlp/wo/kernel/v_col
I0512 23:42:16.150054 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_12/mlp/wo/kernel/v_row
I0512 23:42:16.150088 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_12/pre_mlp_layer_norm/scale/m
I0512 23:42:16.150122 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_12/pre_mlp_layer_norm/scale/v
I0512 23:42:16.150156 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_12/pre_mlp_layer_norm/scale/v_col
I0512 23:42:16.150190 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_12/pre_mlp_layer_norm/scale/v_row
I0512 23:42:16.150224 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_12/pre_self_attention_layer_norm/scale/m
I0512 23:42:16.150257 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_12/pre_self_attention_layer_norm/scale/v
I0512 23:42:16.150291 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_12/pre_self_attention_layer_norm/scale/v_col
I0512 23:42:16.150325 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_12/pre_self_attention_layer_norm/scale/v_row
I0512 23:42:16.150365 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_12/self_attention/key/kernel/m
I0512 23:42:16.150402 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_12/self_attention/key/kernel/v
I0512 23:42:16.150436 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_12/self_attention/key/kernel/v_col
I0512 23:42:16.150469 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_12/self_attention/key/kernel/v_row
I0512 23:42:16.150503 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_12/self_attention/out/kernel/m
I0512 23:42:16.150537 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_12/self_attention/out/kernel/v
I0512 23:42:16.150571 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_12/self_attention/out/kernel/v_col
I0512 23:42:16.150604 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_12/self_attention/out/kernel/v_row
I0512 23:42:16.150640 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_12/self_attention/query/kernel/m
I0512 23:42:16.150674 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_12/self_attention/query/kernel/v
I0512 23:42:16.150707 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_12/self_attention/query/kernel/v_col
I0512 23:42:16.150741 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_12/self_attention/query/kernel/v_row
I0512 23:42:16.150774 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_12/self_attention/value/kernel/m
I0512 23:42:16.150809 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_12/self_attention/value/kernel/v
I0512 23:42:16.150846 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_12/self_attention/value/kernel/v_col
I0512 23:42:16.150881 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_12/self_attention/value/kernel/v_row
I0512 23:42:16.150915 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_13/mlp/wi_0/kernel/m
I0512 23:42:16.150950 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_13/mlp/wi_0/kernel/v
I0512 23:42:16.150985 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_13/mlp/wi_0/kernel/v_col
I0512 23:42:16.151019 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_13/mlp/wi_0/kernel/v_row
I0512 23:42:16.151053 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_13/mlp/wi_1/kernel/m
I0512 23:42:16.151087 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_13/mlp/wi_1/kernel/v
I0512 23:42:16.151122 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_13/mlp/wi_1/kernel/v_col
I0512 23:42:16.151156 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_13/mlp/wi_1/kernel/v_row
I0512 23:42:16.151190 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_13/mlp/wo/kernel/m
I0512 23:42:16.151224 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_13/mlp/wo/kernel/v
I0512 23:42:16.151259 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_13/mlp/wo/kernel/v_col
I0512 23:42:16.151293 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_13/mlp/wo/kernel/v_row
I0512 23:42:16.151327 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_13/pre_mlp_layer_norm/scale/m
I0512 23:42:16.151367 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_13/pre_mlp_layer_norm/scale/v
I0512 23:42:16.151404 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_13/pre_mlp_layer_norm/scale/v_col
I0512 23:42:16.151438 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_13/pre_mlp_layer_norm/scale/v_row
I0512 23:42:16.151472 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_13/pre_self_attention_layer_norm/scale/m
I0512 23:42:16.151506 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_13/pre_self_attention_layer_norm/scale/v
I0512 23:42:16.151540 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_13/pre_self_attention_layer_norm/scale/v_col
I0512 23:42:16.151575 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_13/pre_self_attention_layer_norm/scale/v_row
I0512 23:42:16.151610 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_13/self_attention/key/kernel/m
I0512 23:42:16.151646 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_13/self_attention/key/kernel/v
I0512 23:42:16.151680 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_13/self_attention/key/kernel/v_col
I0512 23:42:16.151714 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_13/self_attention/key/kernel/v_row
I0512 23:42:16.151748 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_13/self_attention/out/kernel/m
I0512 23:42:16.151782 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_13/self_attention/out/kernel/v
I0512 23:42:16.151815 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_13/self_attention/out/kernel/v_col
I0512 23:42:16.151849 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_13/self_attention/out/kernel/v_row
I0512 23:42:16.151883 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_13/self_attention/query/kernel/m
I0512 23:42:16.151917 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_13/self_attention/query/kernel/v
I0512 23:42:16.151951 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_13/self_attention/query/kernel/v_col
I0512 23:42:16.151985 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_13/self_attention/query/kernel/v_row
I0512 23:42:16.152018 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_13/self_attention/value/kernel/m
I0512 23:42:16.152052 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_13/self_attention/value/kernel/v
I0512 23:42:16.152086 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_13/self_attention/value/kernel/v_col
I0512 23:42:16.152119 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_13/self_attention/value/kernel/v_row
I0512 23:42:16.152154 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_14/mlp/wi_0/kernel/m
I0512 23:42:16.152210 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_14/mlp/wi_0/kernel/v
I0512 23:42:16.152247 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_14/mlp/wi_0/kernel/v_col
I0512 23:42:16.152281 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_14/mlp/wi_0/kernel/v_row
I0512 23:42:16.152316 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_14/mlp/wi_1/kernel/m
I0512 23:42:16.152350 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_14/mlp/wi_1/kernel/v
I0512 23:42:16.152390 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_14/mlp/wi_1/kernel/v_col
I0512 23:42:16.152425 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_14/mlp/wi_1/kernel/v_row
I0512 23:42:16.152459 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_14/mlp/wo/kernel/m
I0512 23:42:16.152493 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_14/mlp/wo/kernel/v
I0512 23:42:16.152527 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_14/mlp/wo/kernel/v_col
I0512 23:42:16.152561 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_14/mlp/wo/kernel/v_row
I0512 23:42:16.152595 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_14/pre_mlp_layer_norm/scale/m
I0512 23:42:16.152630 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_14/pre_mlp_layer_norm/scale/v
I0512 23:42:16.152665 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_14/pre_mlp_layer_norm/scale/v_col
I0512 23:42:16.152700 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_14/pre_mlp_layer_norm/scale/v_row
I0512 23:42:16.152734 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_14/pre_self_attention_layer_norm/scale/m
I0512 23:42:16.152767 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_14/pre_self_attention_layer_norm/scale/v
I0512 23:42:16.152801 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_14/pre_self_attention_layer_norm/scale/v_col
I0512 23:42:16.152836 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_14/pre_self_attention_layer_norm/scale/v_row
I0512 23:42:16.152870 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_14/self_attention/key/kernel/m
I0512 23:42:16.152904 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_14/self_attention/key/kernel/v
I0512 23:42:16.152939 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_14/self_attention/key/kernel/v_col
I0512 23:42:16.152972 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_14/self_attention/key/kernel/v_row
I0512 23:42:16.153006 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_14/self_attention/out/kernel/m
I0512 23:42:16.153040 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_14/self_attention/out/kernel/v
I0512 23:42:16.153074 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_14/self_attention/out/kernel/v_col
I0512 23:42:16.153107 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_14/self_attention/out/kernel/v_row
I0512 23:42:16.153142 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_14/self_attention/query/kernel/m
I0512 23:42:16.153175 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_14/self_attention/query/kernel/v
I0512 23:42:16.153209 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_14/self_attention/query/kernel/v_col
I0512 23:42:16.153243 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_14/self_attention/query/kernel/v_row
I0512 23:42:16.153277 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_14/self_attention/value/kernel/m
I0512 23:42:16.153311 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_14/self_attention/value/kernel/v
I0512 23:42:16.153344 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_14/self_attention/value/kernel/v_col
I0512 23:42:16.153385 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_14/self_attention/value/kernel/v_row
I0512 23:42:16.153419 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_15/mlp/wi_0/kernel/m
I0512 23:42:16.153454 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_15/mlp/wi_0/kernel/v
I0512 23:42:16.153488 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_15/mlp/wi_0/kernel/v_col
I0512 23:42:16.153522 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_15/mlp/wi_0/kernel/v_row
I0512 23:42:16.153556 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_15/mlp/wi_1/kernel/m
I0512 23:42:16.153590 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_15/mlp/wi_1/kernel/v
I0512 23:42:16.153626 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_15/mlp/wi_1/kernel/v_col
I0512 23:42:16.153661 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_15/mlp/wi_1/kernel/v_row
I0512 23:42:16.153696 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_15/mlp/wo/kernel/m
I0512 23:42:16.153730 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_15/mlp/wo/kernel/v
I0512 23:42:16.153765 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_15/mlp/wo/kernel/v_col
I0512 23:42:16.153799 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_15/mlp/wo/kernel/v_row
I0512 23:42:16.153838 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_15/pre_mlp_layer_norm/scale/m
I0512 23:42:16.153872 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_15/pre_mlp_layer_norm/scale/v
I0512 23:42:16.153906 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_15/pre_mlp_layer_norm/scale/v_col
I0512 23:42:16.153940 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_15/pre_mlp_layer_norm/scale/v_row
I0512 23:42:16.153974 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_15/pre_self_attention_layer_norm/scale/m
I0512 23:42:16.154008 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_15/pre_self_attention_layer_norm/scale/v
I0512 23:42:16.154042 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_15/pre_self_attention_layer_norm/scale/v_col
I0512 23:42:16.154076 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_15/pre_self_attention_layer_norm/scale/v_row
I0512 23:42:16.154111 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_15/self_attention/key/kernel/m
I0512 23:42:16.154145 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_15/self_attention/key/kernel/v
I0512 23:42:16.154179 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_15/self_attention/key/kernel/v_col
I0512 23:42:16.154213 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_15/self_attention/key/kernel/v_row
I0512 23:42:16.154246 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_15/self_attention/out/kernel/m
I0512 23:42:16.154280 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_15/self_attention/out/kernel/v
I0512 23:42:16.154314 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_15/self_attention/out/kernel/v_col
I0512 23:42:16.154348 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_15/self_attention/out/kernel/v_row
I0512 23:42:16.154388 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_15/self_attention/query/kernel/m
I0512 23:42:16.154423 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_15/self_attention/query/kernel/v
I0512 23:42:16.154457 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_15/self_attention/query/kernel/v_col
I0512 23:42:16.154491 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_15/self_attention/query/kernel/v_row
I0512 23:42:16.154525 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_15/self_attention/value/kernel/m
I0512 23:42:16.154558 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_15/self_attention/value/kernel/v
I0512 23:42:16.154592 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_15/self_attention/value/kernel/v_col
I0512 23:42:16.154628 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_15/self_attention/value/kernel/v_row
I0512 23:42:16.154668 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_16/mlp/wi_0/kernel/m
I0512 23:42:16.154707 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_16/mlp/wi_0/kernel/v
I0512 23:42:16.154744 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_16/mlp/wi_0/kernel/v_col
I0512 23:42:16.154779 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_16/mlp/wi_0/kernel/v_row
I0512 23:42:16.154819 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_16/mlp/wi_1/kernel/m
I0512 23:42:16.154855 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_16/mlp/wi_1/kernel/v
I0512 23:42:16.154889 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_16/mlp/wi_1/kernel/v_col
I0512 23:42:16.154923 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_16/mlp/wi_1/kernel/v_row
I0512 23:42:16.154957 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_16/mlp/wo/kernel/m
I0512 23:42:16.154990 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_16/mlp/wo/kernel/v
I0512 23:42:16.155023 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_16/mlp/wo/kernel/v_col
I0512 23:42:16.155057 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_16/mlp/wo/kernel/v_row
I0512 23:42:16.155090 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_16/pre_mlp_layer_norm/scale/m
I0512 23:42:16.155124 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_16/pre_mlp_layer_norm/scale/v
I0512 23:42:16.155157 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_16/pre_mlp_layer_norm/scale/v_col
I0512 23:42:16.155190 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_16/pre_mlp_layer_norm/scale/v_row
I0512 23:42:16.155223 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_16/pre_self_attention_layer_norm/scale/m
I0512 23:42:16.155256 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_16/pre_self_attention_layer_norm/scale/v
I0512 23:42:16.155290 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_16/pre_self_attention_layer_norm/scale/v_col
I0512 23:42:16.155324 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_16/pre_self_attention_layer_norm/scale/v_row
I0512 23:42:16.155364 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_16/self_attention/key/kernel/m
I0512 23:42:16.155399 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_16/self_attention/key/kernel/v
I0512 23:42:16.155432 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_16/self_attention/key/kernel/v_col
I0512 23:42:16.155466 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_16/self_attention/key/kernel/v_row
I0512 23:42:16.155498 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_16/self_attention/out/kernel/m
I0512 23:42:16.155531 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_16/self_attention/out/kernel/v
I0512 23:42:16.155563 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_16/self_attention/out/kernel/v_col
I0512 23:42:16.155596 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_16/self_attention/out/kernel/v_row
I0512 23:42:16.155631 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_16/self_attention/query/kernel/m
I0512 23:42:16.155664 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_16/self_attention/query/kernel/v
I0512 23:42:16.155698 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_16/self_attention/query/kernel/v_col
I0512 23:42:16.155731 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_16/self_attention/query/kernel/v_row
I0512 23:42:16.155764 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_16/self_attention/value/kernel/m
I0512 23:42:16.155797 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_16/self_attention/value/kernel/v
I0512 23:42:16.155829 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_16/self_attention/value/kernel/v_col
I0512 23:42:16.155862 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_16/self_attention/value/kernel/v_row
I0512 23:42:16.155895 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/extra_mlp/wi_0/kernel/m
I0512 23:42:16.155927 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/extra_mlp/wi_0/kernel/v
I0512 23:42:16.155960 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/extra_mlp/wi_0/kernel/v_col
I0512 23:42:16.155992 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/extra_mlp/wi_0/kernel/v_row
I0512 23:42:16.156026 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/extra_mlp/wi_1/kernel/m
I0512 23:42:16.156058 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/extra_mlp/wi_1/kernel/v
I0512 23:42:16.156091 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/extra_mlp/wi_1/kernel/v_col
I0512 23:42:16.156124 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/extra_mlp/wi_1/kernel/v_row
I0512 23:42:16.156157 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/extra_mlp/wo/kernel/m
I0512 23:42:16.156212 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/extra_mlp/wo/kernel/v
I0512 23:42:16.156247 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/extra_mlp/wo/kernel/v_col
I0512 23:42:16.156280 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/extra_mlp/wo/kernel/v_row
I0512 23:42:16.156314 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/mlp/expert/wi_0/kernel/m
I0512 23:42:16.156346 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/mlp/expert/wi_0/kernel/v
I0512 23:42:16.156387 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/mlp/expert/wi_0/kernel/v_col
I0512 23:42:16.156420 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/mlp/expert/wi_0/kernel/v_row
I0512 23:42:16.156453 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/mlp/expert/wi_1/kernel/m
I0512 23:42:16.156485 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/mlp/expert/wi_1/kernel/v
I0512 23:42:16.156518 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/mlp/expert/wi_1/kernel/v_col
I0512 23:42:16.156550 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/mlp/expert/wi_1/kernel/v_row
I0512 23:42:16.156584 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/mlp/expert/wo/kernel/m
I0512 23:42:16.156618 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/mlp/expert/wo/kernel/v
I0512 23:42:16.156653 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/mlp/expert/wo/kernel/v_col
I0512 23:42:16.156686 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/mlp/expert/wo/kernel/v_row
I0512 23:42:16.156720 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/mlp/router/router_weights/w/kernel/m
I0512 23:42:16.156752 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/mlp/router/router_weights/w/kernel/v
I0512 23:42:16.156785 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/mlp/router/router_weights/w/kernel/v_col
I0512 23:42:16.156819 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/mlp/router/router_weights/w/kernel/v_row
I0512 23:42:16.156852 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/pre_extra_mlp_layer_norm/scale/m
I0512 23:42:16.156885 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/pre_extra_mlp_layer_norm/scale/v
I0512 23:42:16.156918 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/pre_extra_mlp_layer_norm/scale/v_col
I0512 23:42:16.156951 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/pre_extra_mlp_layer_norm/scale/v_row
I0512 23:42:16.156984 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/pre_mlp_layer_norm/scale/m
I0512 23:42:16.157016 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/pre_mlp_layer_norm/scale/v
I0512 23:42:16.157049 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/pre_mlp_layer_norm/scale/v_col
I0512 23:42:16.157082 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/pre_mlp_layer_norm/scale/v_row
I0512 23:42:16.157115 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/pre_self_attention_layer_norm/scale/m
I0512 23:42:16.157147 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/pre_self_attention_layer_norm/scale/v
I0512 23:42:16.157180 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/pre_self_attention_layer_norm/scale/v_col
I0512 23:42:16.157213 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/pre_self_attention_layer_norm/scale/v_row
I0512 23:42:16.157246 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/self_attention/key/kernel/m
I0512 23:42:16.157279 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/self_attention/key/kernel/v
I0512 23:42:16.157312 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/self_attention/key/kernel/v_col
I0512 23:42:16.157345 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/self_attention/key/kernel/v_row
I0512 23:42:16.157386 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/self_attention/out/kernel/m
I0512 23:42:16.157419 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/self_attention/out/kernel/v
I0512 23:42:16.157453 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/self_attention/out/kernel/v_col
I0512 23:42:16.157486 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/self_attention/out/kernel/v_row
I0512 23:42:16.157519 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/self_attention/query/kernel/m
I0512 23:42:16.157551 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/self_attention/query/kernel/v
I0512 23:42:16.157584 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/self_attention/query/kernel/v_col
I0512 23:42:16.157618 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/self_attention/query/kernel/v_row
I0512 23:42:16.157653 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/self_attention/value/kernel/m
I0512 23:42:16.157685 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/self_attention/value/kernel/v
I0512 23:42:16.157718 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/self_attention/value/kernel/v_col
I0512 23:42:16.157751 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/self_attention/value/kernel/v_row
I0512 23:42:16.157783 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_18/mlp/wi_0/kernel/m
I0512 23:42:16.157817 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_18/mlp/wi_0/kernel/v
I0512 23:42:16.157850 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_18/mlp/wi_0/kernel/v_col
I0512 23:42:16.157883 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_18/mlp/wi_0/kernel/v_row
I0512 23:42:16.157917 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_18/mlp/wi_1/kernel/m
I0512 23:42:16.157950 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_18/mlp/wi_1/kernel/v
I0512 23:42:16.157983 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_18/mlp/wi_1/kernel/v_col
I0512 23:42:16.158016 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_18/mlp/wi_1/kernel/v_row
I0512 23:42:16.158049 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_18/mlp/wo/kernel/m
I0512 23:42:16.158082 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_18/mlp/wo/kernel/v
I0512 23:42:16.158114 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_18/mlp/wo/kernel/v_col
I0512 23:42:16.158147 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_18/mlp/wo/kernel/v_row
I0512 23:42:16.158179 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_18/pre_mlp_layer_norm/scale/m
I0512 23:42:16.158212 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_18/pre_mlp_layer_norm/scale/v
I0512 23:42:16.158245 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_18/pre_mlp_layer_norm/scale/v_col
I0512 23:42:16.158278 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_18/pre_mlp_layer_norm/scale/v_row
I0512 23:42:16.158311 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_18/pre_self_attention_layer_norm/scale/m
I0512 23:42:16.158344 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_18/pre_self_attention_layer_norm/scale/v
I0512 23:42:16.158383 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_18/pre_self_attention_layer_norm/scale/v_col
I0512 23:42:16.158416 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_18/pre_self_attention_layer_norm/scale/v_row
I0512 23:42:16.158449 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_18/self_attention/key/kernel/m
I0512 23:42:16.158481 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_18/self_attention/key/kernel/v
I0512 23:42:16.158514 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_18/self_attention/key/kernel/v_col
I0512 23:42:16.158547 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_18/self_attention/key/kernel/v_row
I0512 23:42:16.158580 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_18/self_attention/out/kernel/m
I0512 23:42:16.158614 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_18/self_attention/out/kernel/v
I0512 23:42:16.158648 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_18/self_attention/out/kernel/v_col
I0512 23:42:16.158681 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_18/self_attention/out/kernel/v_row
I0512 23:42:16.158714 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_18/self_attention/query/kernel/m
I0512 23:42:16.158747 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_18/self_attention/query/kernel/v
I0512 23:42:16.158780 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_18/self_attention/query/kernel/v_col
I0512 23:42:16.158813 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_18/self_attention/query/kernel/v_row
I0512 23:42:16.158846 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_18/self_attention/value/kernel/m
I0512 23:42:16.158879 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_18/self_attention/value/kernel/v
I0512 23:42:16.158911 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_18/self_attention/value/kernel/v_col
I0512 23:42:16.158944 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_18/self_attention/value/kernel/v_row
I0512 23:42:16.158977 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_19/mlp/wi_0/kernel/m
I0512 23:42:16.159011 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_19/mlp/wi_0/kernel/v
I0512 23:42:16.159044 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_19/mlp/wi_0/kernel/v_col
I0512 23:42:16.159077 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_19/mlp/wi_0/kernel/v_row
I0512 23:42:16.159110 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_19/mlp/wi_1/kernel/m
I0512 23:42:16.159142 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_19/mlp/wi_1/kernel/v
I0512 23:42:16.159175 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_19/mlp/wi_1/kernel/v_col
I0512 23:42:16.159208 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_19/mlp/wi_1/kernel/v_row
I0512 23:42:16.159241 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_19/mlp/wo/kernel/m
I0512 23:42:16.159274 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_19/mlp/wo/kernel/v
I0512 23:42:16.159307 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_19/mlp/wo/kernel/v_col
I0512 23:42:16.159340 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_19/mlp/wo/kernel/v_row
I0512 23:42:16.159379 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_19/pre_mlp_layer_norm/scale/m
I0512 23:42:16.159413 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_19/pre_mlp_layer_norm/scale/v
I0512 23:42:16.159446 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_19/pre_mlp_layer_norm/scale/v_col
I0512 23:42:16.159479 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_19/pre_mlp_layer_norm/scale/v_row
I0512 23:42:16.159511 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_19/pre_self_attention_layer_norm/scale/m
I0512 23:42:16.159544 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_19/pre_self_attention_layer_norm/scale/v
I0512 23:42:16.159577 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_19/pre_self_attention_layer_norm/scale/v_col
I0512 23:42:16.159611 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_19/pre_self_attention_layer_norm/scale/v_row
I0512 23:42:16.159645 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_19/self_attention/key/kernel/m
I0512 23:42:16.159678 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_19/self_attention/key/kernel/v
I0512 23:42:16.159711 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_19/self_attention/key/kernel/v_col
I0512 23:42:16.159744 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_19/self_attention/key/kernel/v_row
I0512 23:42:16.159776 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_19/self_attention/out/kernel/m
I0512 23:42:16.159810 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_19/self_attention/out/kernel/v
I0512 23:42:16.159842 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_19/self_attention/out/kernel/v_col
I0512 23:42:16.159876 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_19/self_attention/out/kernel/v_row
I0512 23:42:16.159908 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_19/self_attention/query/kernel/m
I0512 23:42:16.159941 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_19/self_attention/query/kernel/v
I0512 23:42:16.159974 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_19/self_attention/query/kernel/v_col
I0512 23:42:16.160007 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_19/self_attention/query/kernel/v_row
I0512 23:42:16.160039 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_19/self_attention/value/kernel/m
I0512 23:42:16.160073 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_19/self_attention/value/kernel/v
I0512 23:42:16.160105 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_19/self_attention/value/kernel/v_col
I0512 23:42:16.160138 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_19/self_attention/value/kernel/v_row
I0512 23:42:16.160189 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_2/mlp/wi_0/kernel/m
I0512 23:42:16.160227 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_2/mlp/wi_0/kernel/v
I0512 23:42:16.160261 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_2/mlp/wi_0/kernel/v_col
I0512 23:42:16.160294 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_2/mlp/wi_0/kernel/v_row
I0512 23:42:16.160327 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_2/mlp/wi_1/kernel/m
I0512 23:42:16.160366 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_2/mlp/wi_1/kernel/v
I0512 23:42:16.160400 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_2/mlp/wi_1/kernel/v_col
I0512 23:42:16.160433 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_2/mlp/wi_1/kernel/v_row
I0512 23:42:16.160466 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_2/mlp/wo/kernel/m
I0512 23:42:16.160499 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_2/mlp/wo/kernel/v
I0512 23:42:16.160532 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_2/mlp/wo/kernel/v_col
I0512 23:42:16.160565 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_2/mlp/wo/kernel/v_row
I0512 23:42:16.160598 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_2/pre_mlp_layer_norm/scale/m
I0512 23:42:16.160633 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_2/pre_mlp_layer_norm/scale/v
I0512 23:42:16.160667 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_2/pre_mlp_layer_norm/scale/v_col
I0512 23:42:16.160700 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_2/pre_mlp_layer_norm/scale/v_row
I0512 23:42:16.160733 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_2/pre_self_attention_layer_norm/scale/m
I0512 23:42:16.160767 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_2/pre_self_attention_layer_norm/scale/v
I0512 23:42:16.160799 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_2/pre_self_attention_layer_norm/scale/v_col
I0512 23:42:16.160833 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_2/pre_self_attention_layer_norm/scale/v_row
I0512 23:42:16.160866 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_2/self_attention/key/kernel/m
I0512 23:42:16.160899 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_2/self_attention/key/kernel/v
I0512 23:42:16.160932 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_2/self_attention/key/kernel/v_col
I0512 23:42:16.160965 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_2/self_attention/key/kernel/v_row
I0512 23:42:16.160998 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_2/self_attention/out/kernel/m
I0512 23:42:16.161031 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_2/self_attention/out/kernel/v
I0512 23:42:16.161064 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_2/self_attention/out/kernel/v_col
I0512 23:42:16.161097 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_2/self_attention/out/kernel/v_row
I0512 23:42:16.161130 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_2/self_attention/query/kernel/m
I0512 23:42:16.161163 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_2/self_attention/query/kernel/v
I0512 23:42:16.161195 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_2/self_attention/query/kernel/v_col
I0512 23:42:16.161228 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_2/self_attention/query/kernel/v_row
I0512 23:42:16.161261 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_2/self_attention/value/kernel/m
I0512 23:42:16.161294 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_2/self_attention/value/kernel/v
I0512 23:42:16.161327 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_2/self_attention/value/kernel/v_col
I0512 23:42:16.161365 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_2/self_attention/value/kernel/v_row
I0512 23:42:16.161400 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_20/mlp/wi_0/kernel/m
I0512 23:42:16.161433 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_20/mlp/wi_0/kernel/v
I0512 23:42:16.161466 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_20/mlp/wi_0/kernel/v_col
I0512 23:42:16.161500 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_20/mlp/wi_0/kernel/v_row
I0512 23:42:16.161533 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_20/mlp/wi_1/kernel/m
I0512 23:42:16.161566 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_20/mlp/wi_1/kernel/v
I0512 23:42:16.161598 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_20/mlp/wi_1/kernel/v_col
I0512 23:42:16.161634 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_20/mlp/wi_1/kernel/v_row
I0512 23:42:16.161668 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_20/mlp/wo/kernel/m
I0512 23:42:16.161701 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_20/mlp/wo/kernel/v
I0512 23:42:16.161734 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_20/mlp/wo/kernel/v_col
I0512 23:42:16.161767 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_20/mlp/wo/kernel/v_row
I0512 23:42:16.161800 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_20/pre_mlp_layer_norm/scale/m
I0512 23:42:16.161833 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_20/pre_mlp_layer_norm/scale/v
I0512 23:42:16.161866 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_20/pre_mlp_layer_norm/scale/v_col
I0512 23:42:16.161899 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_20/pre_mlp_layer_norm/scale/v_row
I0512 23:42:16.161932 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_20/pre_self_attention_layer_norm/scale/m
I0512 23:42:16.161965 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_20/pre_self_attention_layer_norm/scale/v
I0512 23:42:16.161997 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_20/pre_self_attention_layer_norm/scale/v_col
I0512 23:42:16.162030 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_20/pre_self_attention_layer_norm/scale/v_row
I0512 23:42:16.162064 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_20/self_attention/key/kernel/m
I0512 23:42:16.162097 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_20/self_attention/key/kernel/v
I0512 23:42:16.162130 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_20/self_attention/key/kernel/v_col
I0512 23:42:16.162163 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_20/self_attention/key/kernel/v_row
I0512 23:42:16.162196 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_20/self_attention/out/kernel/m
I0512 23:42:16.162229 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_20/self_attention/out/kernel/v
I0512 23:42:16.162262 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_20/self_attention/out/kernel/v_col
I0512 23:42:16.162295 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_20/self_attention/out/kernel/v_row
I0512 23:42:16.162328 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_20/self_attention/query/kernel/m
I0512 23:42:16.162366 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_20/self_attention/query/kernel/v
I0512 23:42:16.162401 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_20/self_attention/query/kernel/v_col
I0512 23:42:16.162434 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_20/self_attention/query/kernel/v_row
I0512 23:42:16.162467 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_20/self_attention/value/kernel/m
I0512 23:42:16.162500 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_20/self_attention/value/kernel/v
I0512 23:42:16.162533 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_20/self_attention/value/kernel/v_col
I0512 23:42:16.162565 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_20/self_attention/value/kernel/v_row
I0512 23:42:16.162598 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_21/mlp/wi_0/kernel/m
I0512 23:42:16.162633 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_21/mlp/wi_0/kernel/v
I0512 23:42:16.162667 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_21/mlp/wi_0/kernel/v_col
I0512 23:42:16.162700 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_21/mlp/wi_0/kernel/v_row
I0512 23:42:16.162734 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_21/mlp/wi_1/kernel/m
I0512 23:42:16.162767 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_21/mlp/wi_1/kernel/v
I0512 23:42:16.162800 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_21/mlp/wi_1/kernel/v_col
I0512 23:42:16.162833 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_21/mlp/wi_1/kernel/v_row
I0512 23:42:16.162866 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_21/mlp/wo/kernel/m
I0512 23:42:16.162899 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_21/mlp/wo/kernel/v
I0512 23:42:16.162932 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_21/mlp/wo/kernel/v_col
I0512 23:42:16.162965 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_21/mlp/wo/kernel/v_row
I0512 23:42:16.162998 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_21/pre_mlp_layer_norm/scale/m
I0512 23:42:16.163031 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_21/pre_mlp_layer_norm/scale/v
I0512 23:42:16.163064 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_21/pre_mlp_layer_norm/scale/v_col
I0512 23:42:16.163097 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_21/pre_mlp_layer_norm/scale/v_row
I0512 23:42:16.163130 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_21/pre_self_attention_layer_norm/scale/m
I0512 23:42:16.163163 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_21/pre_self_attention_layer_norm/scale/v
I0512 23:42:16.163196 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_21/pre_self_attention_layer_norm/scale/v_col
I0512 23:42:16.163228 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_21/pre_self_attention_layer_norm/scale/v_row
I0512 23:42:16.163261 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_21/self_attention/key/kernel/m
I0512 23:42:16.163295 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_21/self_attention/key/kernel/v
I0512 23:42:16.163327 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_21/self_attention/key/kernel/v_col
I0512 23:42:16.163366 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_21/self_attention/key/kernel/v_row
I0512 23:42:16.163400 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_21/self_attention/out/kernel/m
I0512 23:42:16.163433 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_21/self_attention/out/kernel/v
I0512 23:42:16.163466 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_21/self_attention/out/kernel/v_col
I0512 23:42:16.163517 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_21/self_attention/out/kernel/v_row
I0512 23:42:16.163552 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_21/self_attention/query/kernel/m
I0512 23:42:16.163585 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_21/self_attention/query/kernel/v
I0512 23:42:16.163619 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_21/self_attention/query/kernel/v_col
I0512 23:42:16.163653 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_21/self_attention/query/kernel/v_row
I0512 23:42:16.163687 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_21/self_attention/value/kernel/m
I0512 23:42:16.163719 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_21/self_attention/value/kernel/v
I0512 23:42:16.163752 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_21/self_attention/value/kernel/v_col
I0512 23:42:16.163785 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_21/self_attention/value/kernel/v_row
I0512 23:42:16.163818 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_22/mlp/wi_0/kernel/m
I0512 23:42:16.163851 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_22/mlp/wi_0/kernel/v
I0512 23:42:16.163884 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_22/mlp/wi_0/kernel/v_col
I0512 23:42:16.163917 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_22/mlp/wi_0/kernel/v_row
I0512 23:42:16.163949 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_22/mlp/wi_1/kernel/m
I0512 23:42:16.163982 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_22/mlp/wi_1/kernel/v
I0512 23:42:16.164015 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_22/mlp/wi_1/kernel/v_col
I0512 23:42:16.164047 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_22/mlp/wi_1/kernel/v_row
I0512 23:42:16.164080 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_22/mlp/wo/kernel/m
I0512 23:42:16.164113 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_22/mlp/wo/kernel/v
I0512 23:42:16.164146 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_22/mlp/wo/kernel/v_col
I0512 23:42:16.164211 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_22/mlp/wo/kernel/v_row
I0512 23:42:16.164247 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_22/pre_mlp_layer_norm/scale/m
I0512 23:42:16.164280 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_22/pre_mlp_layer_norm/scale/v
I0512 23:42:16.164313 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_22/pre_mlp_layer_norm/scale/v_col
I0512 23:42:16.164346 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_22/pre_mlp_layer_norm/scale/v_row
I0512 23:42:16.164385 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_22/pre_self_attention_layer_norm/scale/m
I0512 23:42:16.164419 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_22/pre_self_attention_layer_norm/scale/v
I0512 23:42:16.164452 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_22/pre_self_attention_layer_norm/scale/v_col
I0512 23:42:16.164485 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_22/pre_self_attention_layer_norm/scale/v_row
I0512 23:42:16.164518 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_22/self_attention/key/kernel/m
I0512 23:42:16.164550 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_22/self_attention/key/kernel/v
I0512 23:42:16.164583 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_22/self_attention/key/kernel/v_col
I0512 23:42:16.164618 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_22/self_attention/key/kernel/v_row
I0512 23:42:16.164652 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_22/self_attention/out/kernel/m
I0512 23:42:16.164685 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_22/self_attention/out/kernel/v
I0512 23:42:16.164719 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_22/self_attention/out/kernel/v_col
I0512 23:42:16.164752 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_22/self_attention/out/kernel/v_row
I0512 23:42:16.164784 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_22/self_attention/query/kernel/m
I0512 23:42:16.164816 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_22/self_attention/query/kernel/v
I0512 23:42:16.164849 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_22/self_attention/query/kernel/v_col
I0512 23:42:16.164882 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_22/self_attention/query/kernel/v_row
I0512 23:42:16.164914 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_22/self_attention/value/kernel/m
I0512 23:42:16.164947 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_22/self_attention/value/kernel/v
I0512 23:42:16.164980 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_22/self_attention/value/kernel/v_col
I0512 23:42:16.165013 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_22/self_attention/value/kernel/v_row
I0512 23:42:16.165045 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/extra_mlp/wi_0/kernel/m
I0512 23:42:16.165078 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/extra_mlp/wi_0/kernel/v
I0512 23:42:16.165111 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/extra_mlp/wi_0/kernel/v_col
I0512 23:42:16.165143 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/extra_mlp/wi_0/kernel/v_row
I0512 23:42:16.165176 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/extra_mlp/wi_1/kernel/m
I0512 23:42:16.165209 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/extra_mlp/wi_1/kernel/v
I0512 23:42:16.165242 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/extra_mlp/wi_1/kernel/v_col
I0512 23:42:16.165275 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/extra_mlp/wi_1/kernel/v_row
I0512 23:42:16.165308 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/extra_mlp/wo/kernel/m
I0512 23:42:16.165341 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/extra_mlp/wo/kernel/v
I0512 23:42:16.165381 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/extra_mlp/wo/kernel/v_col
I0512 23:42:16.165415 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/extra_mlp/wo/kernel/v_row
I0512 23:42:16.165447 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/mlp/expert/wi_0/kernel/m
I0512 23:42:16.165480 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/mlp/expert/wi_0/kernel/v
I0512 23:42:16.165513 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/mlp/expert/wi_0/kernel/v_col
I0512 23:42:16.165546 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/mlp/expert/wi_0/kernel/v_row
I0512 23:42:16.165579 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/mlp/expert/wi_1/kernel/m
I0512 23:42:16.165613 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/mlp/expert/wi_1/kernel/v
I0512 23:42:16.165647 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/mlp/expert/wi_1/kernel/v_col
I0512 23:42:16.165680 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/mlp/expert/wi_1/kernel/v_row
I0512 23:42:16.165712 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/mlp/expert/wo/kernel/m
I0512 23:42:16.165745 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/mlp/expert/wo/kernel/v
I0512 23:42:16.165778 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/mlp/expert/wo/kernel/v_col
I0512 23:42:16.165812 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/mlp/expert/wo/kernel/v_row
I0512 23:42:16.165844 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/mlp/router/router_weights/w/kernel/m
I0512 23:42:16.165877 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/mlp/router/router_weights/w/kernel/v
I0512 23:42:16.165910 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/mlp/router/router_weights/w/kernel/v_col
I0512 23:42:16.165943 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/mlp/router/router_weights/w/kernel/v_row
I0512 23:42:16.165976 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/pre_extra_mlp_layer_norm/scale/m
I0512 23:42:16.166009 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/pre_extra_mlp_layer_norm/scale/v
I0512 23:42:16.166042 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/pre_extra_mlp_layer_norm/scale/v_col
I0512 23:42:16.166075 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/pre_extra_mlp_layer_norm/scale/v_row
I0512 23:42:16.166108 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/pre_mlp_layer_norm/scale/m
I0512 23:42:16.166141 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/pre_mlp_layer_norm/scale/v
I0512 23:42:16.166173 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/pre_mlp_layer_norm/scale/v_col
I0512 23:42:16.166206 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/pre_mlp_layer_norm/scale/v_row
I0512 23:42:16.166239 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/pre_self_attention_layer_norm/scale/m
I0512 23:42:16.166271 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/pre_self_attention_layer_norm/scale/v
I0512 23:42:16.166304 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/pre_self_attention_layer_norm/scale/v_col
I0512 23:42:16.166337 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/pre_self_attention_layer_norm/scale/v_row
I0512 23:42:16.166376 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/self_attention/key/kernel/m
I0512 23:42:16.166409 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/self_attention/key/kernel/v
I0512 23:42:16.166443 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/self_attention/key/kernel/v_col
I0512 23:42:16.166476 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/self_attention/key/kernel/v_row
I0512 23:42:16.166508 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/self_attention/out/kernel/m
I0512 23:42:16.166541 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/self_attention/out/kernel/v
I0512 23:42:16.166574 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/self_attention/out/kernel/v_col
I0512 23:42:16.166607 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/self_attention/out/kernel/v_row
I0512 23:42:16.166642 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/self_attention/query/kernel/m
I0512 23:42:16.166675 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/self_attention/query/kernel/v
I0512 23:42:16.166708 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/self_attention/query/kernel/v_col
I0512 23:42:16.166741 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/self_attention/query/kernel/v_row
I0512 23:42:16.166774 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/self_attention/value/kernel/m
I0512 23:42:16.166806 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/self_attention/value/kernel/v
I0512 23:42:16.166840 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/self_attention/value/kernel/v_col
I0512 23:42:16.166872 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/self_attention/value/kernel/v_row
I0512 23:42:16.166905 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_3/mlp/wi_0/kernel/m
I0512 23:42:16.166939 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_3/mlp/wi_0/kernel/v
I0512 23:42:16.166971 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_3/mlp/wi_0/kernel/v_col
I0512 23:42:16.167004 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_3/mlp/wi_0/kernel/v_row
I0512 23:42:16.167037 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_3/mlp/wi_1/kernel/m
I0512 23:42:16.167070 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_3/mlp/wi_1/kernel/v
I0512 23:42:16.167103 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_3/mlp/wi_1/kernel/v_col
I0512 23:42:16.167135 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_3/mlp/wi_1/kernel/v_row
I0512 23:42:16.167168 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_3/mlp/wo/kernel/m
I0512 23:42:16.167201 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_3/mlp/wo/kernel/v
I0512 23:42:16.167234 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_3/mlp/wo/kernel/v_col
I0512 23:42:16.167267 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_3/mlp/wo/kernel/v_row
I0512 23:42:16.167300 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_3/pre_mlp_layer_norm/scale/m
I0512 23:42:16.167333 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_3/pre_mlp_layer_norm/scale/v
I0512 23:42:16.167371 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_3/pre_mlp_layer_norm/scale/v_col
I0512 23:42:16.167405 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_3/pre_mlp_layer_norm/scale/v_row
I0512 23:42:16.167438 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_3/pre_self_attention_layer_norm/scale/m
I0512 23:42:16.167471 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_3/pre_self_attention_layer_norm/scale/v
I0512 23:42:16.167505 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_3/pre_self_attention_layer_norm/scale/v_col
I0512 23:42:16.167538 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_3/pre_self_attention_layer_norm/scale/v_row
I0512 23:42:16.167571 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_3/self_attention/key/kernel/m
I0512 23:42:16.167604 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_3/self_attention/key/kernel/v
I0512 23:42:16.167639 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_3/self_attention/key/kernel/v_col
I0512 23:42:16.167673 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_3/self_attention/key/kernel/v_row
I0512 23:42:16.167706 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_3/self_attention/out/kernel/m
I0512 23:42:16.167739 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_3/self_attention/out/kernel/v
I0512 23:42:16.167772 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_3/self_attention/out/kernel/v_col
I0512 23:42:16.167804 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_3/self_attention/out/kernel/v_row
I0512 23:42:16.167837 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_3/self_attention/query/kernel/m
I0512 23:42:16.167870 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_3/self_attention/query/kernel/v
I0512 23:42:16.167902 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_3/self_attention/query/kernel/v_col
I0512 23:42:16.167935 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_3/self_attention/query/kernel/v_row
I0512 23:42:16.167968 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_3/self_attention/value/kernel/m
I0512 23:42:16.168000 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_3/self_attention/value/kernel/v
I0512 23:42:16.168033 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_3/self_attention/value/kernel/v_col
I0512 23:42:16.168065 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_3/self_attention/value/kernel/v_row
I0512 23:42:16.168098 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_4/mlp/wi_0/kernel/m
I0512 23:42:16.168131 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_4/mlp/wi_0/kernel/v
I0512 23:42:16.168183 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_4/mlp/wi_0/kernel/v_col
I0512 23:42:16.168222 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_4/mlp/wi_0/kernel/v_row
I0512 23:42:16.168256 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_4/mlp/wi_1/kernel/m
I0512 23:42:16.168289 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_4/mlp/wi_1/kernel/v
I0512 23:42:16.168322 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_4/mlp/wi_1/kernel/v_col
I0512 23:42:16.168355 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_4/mlp/wi_1/kernel/v_row
I0512 23:42:16.168394 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_4/mlp/wo/kernel/m
I0512 23:42:16.168427 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_4/mlp/wo/kernel/v
I0512 23:42:16.168460 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_4/mlp/wo/kernel/v_col
I0512 23:42:16.168493 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_4/mlp/wo/kernel/v_row
I0512 23:42:16.168525 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_4/pre_mlp_layer_norm/scale/m
I0512 23:42:16.168558 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_4/pre_mlp_layer_norm/scale/v
I0512 23:42:16.168591 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_4/pre_mlp_layer_norm/scale/v_col
I0512 23:42:16.168626 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_4/pre_mlp_layer_norm/scale/v_row
I0512 23:42:16.168660 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_4/pre_self_attention_layer_norm/scale/m
I0512 23:42:16.168693 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_4/pre_self_attention_layer_norm/scale/v
I0512 23:42:16.168726 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_4/pre_self_attention_layer_norm/scale/v_col
I0512 23:42:16.168759 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_4/pre_self_attention_layer_norm/scale/v_row
I0512 23:42:16.168792 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_4/self_attention/key/kernel/m
I0512 23:42:16.168825 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_4/self_attention/key/kernel/v
I0512 23:42:16.168857 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_4/self_attention/key/kernel/v_col
I0512 23:42:16.168890 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_4/self_attention/key/kernel/v_row
I0512 23:42:16.168923 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_4/self_attention/out/kernel/m
I0512 23:42:16.168956 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_4/self_attention/out/kernel/v
I0512 23:42:16.168988 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_4/self_attention/out/kernel/v_col
I0512 23:42:16.169021 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_4/self_attention/out/kernel/v_row
I0512 23:42:16.169053 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_4/self_attention/query/kernel/m
I0512 23:42:16.169086 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_4/self_attention/query/kernel/v
I0512 23:42:16.169119 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_4/self_attention/query/kernel/v_col
I0512 23:42:16.169151 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_4/self_attention/query/kernel/v_row
I0512 23:42:16.169184 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_4/self_attention/value/kernel/m
I0512 23:42:16.169217 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_4/self_attention/value/kernel/v
I0512 23:42:16.169250 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_4/self_attention/value/kernel/v_col
I0512 23:42:16.169282 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_4/self_attention/value/kernel/v_row
I0512 23:42:16.169314 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/extra_mlp/wi_0/kernel/m
I0512 23:42:16.169347 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/extra_mlp/wi_0/kernel/v
I0512 23:42:16.169387 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/extra_mlp/wi_0/kernel/v_col
I0512 23:42:16.169421 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/extra_mlp/wi_0/kernel/v_row
I0512 23:42:16.169454 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/extra_mlp/wi_1/kernel/m
I0512 23:42:16.169486 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/extra_mlp/wi_1/kernel/v
I0512 23:42:16.169519 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/extra_mlp/wi_1/kernel/v_col
I0512 23:42:16.169552 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/extra_mlp/wi_1/kernel/v_row
I0512 23:42:16.169585 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/extra_mlp/wo/kernel/m
I0512 23:42:16.169619 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/extra_mlp/wo/kernel/v
I0512 23:42:16.169653 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/extra_mlp/wo/kernel/v_col
I0512 23:42:16.169687 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/extra_mlp/wo/kernel/v_row
I0512 23:42:16.169720 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/mlp/expert/wi_0/kernel/m
I0512 23:42:16.169752 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/mlp/expert/wi_0/kernel/v
I0512 23:42:16.169786 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/mlp/expert/wi_0/kernel/v_col
I0512 23:42:16.169818 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/mlp/expert/wi_0/kernel/v_row
I0512 23:42:16.169851 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/mlp/expert/wi_1/kernel/m
I0512 23:42:16.169884 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/mlp/expert/wi_1/kernel/v
I0512 23:42:16.169916 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/mlp/expert/wi_1/kernel/v_col
I0512 23:42:16.169949 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/mlp/expert/wi_1/kernel/v_row
I0512 23:42:16.169982 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/mlp/expert/wo/kernel/m
I0512 23:42:16.170015 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/mlp/expert/wo/kernel/v
I0512 23:42:16.170048 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/mlp/expert/wo/kernel/v_col
I0512 23:42:16.170081 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/mlp/expert/wo/kernel/v_row
I0512 23:42:16.170114 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/mlp/router/router_weights/w/kernel/m
I0512 23:42:16.170148 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/mlp/router/router_weights/w/kernel/v
I0512 23:42:16.170180 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/mlp/router/router_weights/w/kernel/v_col
I0512 23:42:16.170213 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/mlp/router/router_weights/w/kernel/v_row
I0512 23:42:16.170246 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/pre_extra_mlp_layer_norm/scale/m
I0512 23:42:16.170279 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/pre_extra_mlp_layer_norm/scale/v
I0512 23:42:16.170312 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/pre_extra_mlp_layer_norm/scale/v_col
I0512 23:42:16.170345 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/pre_extra_mlp_layer_norm/scale/v_row
I0512 23:42:16.170384 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/pre_mlp_layer_norm/scale/m
I0512 23:42:16.170418 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/pre_mlp_layer_norm/scale/v
I0512 23:42:16.170450 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/pre_mlp_layer_norm/scale/v_col
I0512 23:42:16.170483 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/pre_mlp_layer_norm/scale/v_row
I0512 23:42:16.170516 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/pre_self_attention_layer_norm/scale/m
I0512 23:42:16.170548 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/pre_self_attention_layer_norm/scale/v
I0512 23:42:16.170581 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/pre_self_attention_layer_norm/scale/v_col
I0512 23:42:16.170614 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/pre_self_attention_layer_norm/scale/v_row
I0512 23:42:16.170649 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/self_attention/key/kernel/m
I0512 23:42:16.170682 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/self_attention/key/kernel/v
I0512 23:42:16.170715 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/self_attention/key/kernel/v_col
I0512 23:42:16.170748 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/self_attention/key/kernel/v_row
I0512 23:42:16.170781 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/self_attention/out/kernel/m
I0512 23:42:16.170813 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/self_attention/out/kernel/v
I0512 23:42:16.170846 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/self_attention/out/kernel/v_col
I0512 23:42:16.170879 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/self_attention/out/kernel/v_row
I0512 23:42:16.170912 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/self_attention/query/kernel/m
I0512 23:42:16.170944 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/self_attention/query/kernel/v
I0512 23:42:16.170978 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/self_attention/query/kernel/v_col
I0512 23:42:16.171010 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/self_attention/query/kernel/v_row
I0512 23:42:16.171043 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/self_attention/value/kernel/m
I0512 23:42:16.171076 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/self_attention/value/kernel/v
I0512 23:42:16.171108 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/self_attention/value/kernel/v_col
I0512 23:42:16.171141 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/self_attention/value/kernel/v_row
I0512 23:42:16.171174 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_6/mlp/wi_0/kernel/m
I0512 23:42:16.171207 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_6/mlp/wi_0/kernel/v
I0512 23:42:16.171240 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_6/mlp/wi_0/kernel/v_col
I0512 23:42:16.171273 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_6/mlp/wi_0/kernel/v_row
I0512 23:42:16.171306 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_6/mlp/wi_1/kernel/m
I0512 23:42:16.171339 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_6/mlp/wi_1/kernel/v
I0512 23:42:16.171378 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_6/mlp/wi_1/kernel/v_col
I0512 23:42:16.171411 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_6/mlp/wi_1/kernel/v_row
I0512 23:42:16.171444 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_6/mlp/wo/kernel/m
I0512 23:42:16.171477 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_6/mlp/wo/kernel/v
I0512 23:42:16.171509 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_6/mlp/wo/kernel/v_col
I0512 23:42:16.171542 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_6/mlp/wo/kernel/v_row
I0512 23:42:16.171575 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_6/pre_mlp_layer_norm/scale/m
I0512 23:42:16.171608 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_6/pre_mlp_layer_norm/scale/v
I0512 23:42:16.171643 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_6/pre_mlp_layer_norm/scale/v_col
I0512 23:42:16.171677 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_6/pre_mlp_layer_norm/scale/v_row
I0512 23:42:16.171710 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_6/pre_self_attention_layer_norm/scale/m
I0512 23:42:16.171742 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_6/pre_self_attention_layer_norm/scale/v
I0512 23:42:16.171776 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_6/pre_self_attention_layer_norm/scale/v_col
I0512 23:42:16.171808 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_6/pre_self_attention_layer_norm/scale/v_row
I0512 23:42:16.171842 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_6/self_attention/key/kernel/m
I0512 23:42:16.171875 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_6/self_attention/key/kernel/v
I0512 23:42:16.171908 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_6/self_attention/key/kernel/v_col
I0512 23:42:16.171941 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_6/self_attention/key/kernel/v_row
I0512 23:42:16.171974 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_6/self_attention/out/kernel/m
I0512 23:42:16.172007 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_6/self_attention/out/kernel/v
I0512 23:42:16.172040 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_6/self_attention/out/kernel/v_col
I0512 23:42:16.172072 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_6/self_attention/out/kernel/v_row
I0512 23:42:16.172106 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_6/self_attention/query/kernel/m
I0512 23:42:16.172138 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_6/self_attention/query/kernel/v
I0512 23:42:16.172190 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_6/self_attention/query/kernel/v_col
I0512 23:42:16.172231 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_6/self_attention/query/kernel/v_row
I0512 23:42:16.172266 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_6/self_attention/value/kernel/m
I0512 23:42:16.172301 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_6/self_attention/value/kernel/v
I0512 23:42:16.172336 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_6/self_attention/value/kernel/v_col
I0512 23:42:16.172376 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_6/self_attention/value/kernel/v_row
I0512 23:42:16.172410 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_7/mlp/wi_0/kernel/m
I0512 23:42:16.172443 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_7/mlp/wi_0/kernel/v
I0512 23:42:16.172477 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_7/mlp/wi_0/kernel/v_col
I0512 23:42:16.172510 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_7/mlp/wi_0/kernel/v_row
I0512 23:42:16.172544 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_7/mlp/wi_1/kernel/m
I0512 23:42:16.172577 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_7/mlp/wi_1/kernel/v
I0512 23:42:16.172610 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_7/mlp/wi_1/kernel/v_col
I0512 23:42:16.172645 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_7/mlp/wi_1/kernel/v_row
I0512 23:42:16.172679 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_7/mlp/wo/kernel/m
I0512 23:42:16.172712 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_7/mlp/wo/kernel/v
I0512 23:42:16.172744 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_7/mlp/wo/kernel/v_col
I0512 23:42:16.172777 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_7/mlp/wo/kernel/v_row
I0512 23:42:16.172811 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_7/pre_mlp_layer_norm/scale/m
I0512 23:42:16.172843 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_7/pre_mlp_layer_norm/scale/v
I0512 23:42:16.172876 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_7/pre_mlp_layer_norm/scale/v_col
I0512 23:42:16.172909 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_7/pre_mlp_layer_norm/scale/v_row
I0512 23:42:16.172942 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_7/pre_self_attention_layer_norm/scale/m
I0512 23:42:16.172976 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_7/pre_self_attention_layer_norm/scale/v
I0512 23:42:16.173009 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_7/pre_self_attention_layer_norm/scale/v_col
I0512 23:42:16.173042 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_7/pre_self_attention_layer_norm/scale/v_row
I0512 23:42:16.173075 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_7/self_attention/key/kernel/m
I0512 23:42:16.173108 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_7/self_attention/key/kernel/v
I0512 23:42:16.173140 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_7/self_attention/key/kernel/v_col
I0512 23:42:16.173173 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_7/self_attention/key/kernel/v_row
I0512 23:42:16.173206 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_7/self_attention/out/kernel/m
I0512 23:42:16.173238 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_7/self_attention/out/kernel/v
I0512 23:42:16.173271 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_7/self_attention/out/kernel/v_col
I0512 23:42:16.173304 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_7/self_attention/out/kernel/v_row
I0512 23:42:16.173337 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_7/self_attention/query/kernel/m
I0512 23:42:16.173376 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_7/self_attention/query/kernel/v
I0512 23:42:16.173409 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_7/self_attention/query/kernel/v_col
I0512 23:42:16.173442 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_7/self_attention/query/kernel/v_row
I0512 23:42:16.173475 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_7/self_attention/value/kernel/m
I0512 23:42:16.173507 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_7/self_attention/value/kernel/v
I0512 23:42:16.173540 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_7/self_attention/value/kernel/v_col
I0512 23:42:16.173572 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_7/self_attention/value/kernel/v_row
I0512 23:42:16.173605 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_8/mlp/wi_0/kernel/m
I0512 23:42:16.173640 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_8/mlp/wi_0/kernel/v
I0512 23:42:16.173674 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_8/mlp/wi_0/kernel/v_col
I0512 23:42:16.173707 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_8/mlp/wi_0/kernel/v_row
I0512 23:42:16.173741 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_8/mlp/wi_1/kernel/m
I0512 23:42:16.173774 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_8/mlp/wi_1/kernel/v
I0512 23:42:16.173807 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_8/mlp/wi_1/kernel/v_col
I0512 23:42:16.173840 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_8/mlp/wi_1/kernel/v_row
I0512 23:42:16.173872 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_8/mlp/wo/kernel/m
I0512 23:42:16.173905 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_8/mlp/wo/kernel/v
I0512 23:42:16.173938 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_8/mlp/wo/kernel/v_col
I0512 23:42:16.173970 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_8/mlp/wo/kernel/v_row
I0512 23:42:16.174003 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_8/pre_mlp_layer_norm/scale/m
I0512 23:42:16.174036 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_8/pre_mlp_layer_norm/scale/v
I0512 23:42:16.174069 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_8/pre_mlp_layer_norm/scale/v_col
I0512 23:42:16.174102 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_8/pre_mlp_layer_norm/scale/v_row
I0512 23:42:16.174135 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_8/pre_self_attention_layer_norm/scale/m
I0512 23:42:16.174168 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_8/pre_self_attention_layer_norm/scale/v
I0512 23:42:16.174200 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_8/pre_self_attention_layer_norm/scale/v_col
I0512 23:42:16.174233 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_8/pre_self_attention_layer_norm/scale/v_row
I0512 23:42:16.174266 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_8/self_attention/key/kernel/m
I0512 23:42:16.174299 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_8/self_attention/key/kernel/v
I0512 23:42:16.174331 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_8/self_attention/key/kernel/v_col
I0512 23:42:16.174370 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_8/self_attention/key/kernel/v_row
I0512 23:42:16.174404 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_8/self_attention/out/kernel/m
I0512 23:42:16.174437 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_8/self_attention/out/kernel/v
I0512 23:42:16.174470 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_8/self_attention/out/kernel/v_col
I0512 23:42:16.174503 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_8/self_attention/out/kernel/v_row
I0512 23:42:16.174535 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_8/self_attention/query/kernel/m
I0512 23:42:16.174568 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_8/self_attention/query/kernel/v
I0512 23:42:16.174600 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_8/self_attention/query/kernel/v_col
I0512 23:42:16.174635 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_8/self_attention/query/kernel/v_row
I0512 23:42:16.174669 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_8/self_attention/value/kernel/m
I0512 23:42:16.174702 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_8/self_attention/value/kernel/v
I0512 23:42:16.174735 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_8/self_attention/value/kernel/v_col
I0512 23:42:16.174767 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_8/self_attention/value/kernel/v_row
I0512 23:42:16.174800 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_9/mlp/wi_0/kernel/m
I0512 23:42:16.174833 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_9/mlp/wi_0/kernel/v
I0512 23:42:16.174866 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_9/mlp/wi_0/kernel/v_col
I0512 23:42:16.174899 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_9/mlp/wi_0/kernel/v_row
I0512 23:42:16.174932 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_9/mlp/wi_1/kernel/m
I0512 23:42:16.174965 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_9/mlp/wi_1/kernel/v
I0512 23:42:16.174998 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_9/mlp/wi_1/kernel/v_col
I0512 23:42:16.175030 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_9/mlp/wi_1/kernel/v_row
I0512 23:42:16.175063 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_9/mlp/wo/kernel/m
I0512 23:42:16.175096 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_9/mlp/wo/kernel/v
I0512 23:42:16.175129 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_9/mlp/wo/kernel/v_col
I0512 23:42:16.175162 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_9/mlp/wo/kernel/v_row
I0512 23:42:16.175194 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_9/pre_mlp_layer_norm/scale/m
I0512 23:42:16.175227 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_9/pre_mlp_layer_norm/scale/v
I0512 23:42:16.175260 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_9/pre_mlp_layer_norm/scale/v_col
I0512 23:42:16.175293 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_9/pre_mlp_layer_norm/scale/v_row
I0512 23:42:16.175326 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_9/pre_self_attention_layer_norm/scale/m
I0512 23:42:16.175364 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_9/pre_self_attention_layer_norm/scale/v
I0512 23:42:16.175399 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_9/pre_self_attention_layer_norm/scale/v_col
I0512 23:42:16.175433 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_9/pre_self_attention_layer_norm/scale/v_row
I0512 23:42:16.175466 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_9/self_attention/key/kernel/m
I0512 23:42:16.175499 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_9/self_attention/key/kernel/v
I0512 23:42:16.175532 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_9/self_attention/key/kernel/v_col
I0512 23:42:16.175565 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_9/self_attention/key/kernel/v_row
I0512 23:42:16.175598 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_9/self_attention/out/kernel/m
I0512 23:42:16.175633 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_9/self_attention/out/kernel/v
I0512 23:42:16.175667 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_9/self_attention/out/kernel/v_col
I0512 23:42:16.175700 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_9/self_attention/out/kernel/v_row
I0512 23:42:16.175732 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_9/self_attention/query/kernel/m
I0512 23:42:16.175765 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_9/self_attention/query/kernel/v
I0512 23:42:16.175797 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_9/self_attention/query/kernel/v_col
I0512 23:42:16.175830 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_9/self_attention/query/kernel/v_row
I0512 23:42:16.175863 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_9/self_attention/value/kernel/m
I0512 23:42:16.175896 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_9/self_attention/value/kernel/v
I0512 23:42:16.175928 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_9/self_attention/value/kernel/v_col
I0512 23:42:16.175961 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_9/self_attention/value/kernel/v_row
I0512 23:42:16.175994 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/logits_dense/kernel/m
I0512 23:42:16.176027 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/logits_dense/kernel/v
I0512 23:42:16.176059 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/logits_dense/kernel/v_col
I0512 23:42:16.176093 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/logits_dense/kernel/v_row
I0512 23:42:16.176126 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/token_embedder/embedding/m
I0512 23:42:16.176158 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/token_embedder/embedding/v
I0512 23:42:16.176212 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/token_embedder/embedding/v_col
I0512 23:42:16.176246 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/param_states/token_embedder/embedding/v_row
I0512 23:42:16.176279 140339465779200 checkpoints.py:1100] Restoring key from ckpt: state/step
I0512 23:42:16.176313 140339465779200 checkpoints.py:1100] Restoring key from ckpt: target/decoder/decoder_norm/scale
I0512 23:42:16.176346 140339465779200 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_0/mlp/wi_0/kernel
I0512 23:42:16.176387 140339465779200 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_0/mlp/wi_1/kernel
I0512 23:42:16.176421 140339465779200 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_0/mlp/wo/kernel
I0512 23:42:16.176453 140339465779200 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_0/pre_mlp_layer_norm/scale
I0512 23:42:16.176487 140339465779200 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_0/pre_self_attention_layer_norm/scale
I0512 23:42:16.176520 140339465779200 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_0/self_attention/key/kernel
I0512 23:42:16.176553 140339465779200 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_0/self_attention/out/kernel
I0512 23:42:16.176585 140339465779200 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_0/self_attention/query/kernel
I0512 23:42:16.176619 140339465779200 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_0/self_attention/value/kernel
I0512 23:42:16.176653 140339465779200 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_1/mlp/wi_0/kernel
I0512 23:42:16.176687 140339465779200 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_1/mlp/wi_1/kernel
I0512 23:42:16.176719 140339465779200 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_1/mlp/wo/kernel
I0512 23:42:16.176753 140339465779200 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_1/pre_mlp_layer_norm/scale
I0512 23:42:16.176786 140339465779200 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_1/pre_self_attention_layer_norm/scale
I0512 23:42:16.176818 140339465779200 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_1/self_attention/key/kernel
I0512 23:42:16.176851 140339465779200 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_1/self_attention/out/kernel
I0512 23:42:16.176883 140339465779200 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_1/self_attention/query/kernel
I0512 23:42:16.176916 140339465779200 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_1/self_attention/value/kernel
I0512 23:42:16.176949 140339465779200 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_10/mlp/wi_0/kernel
I0512 23:42:16.176982 140339465779200 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_10/mlp/wi_1/kernel
I0512 23:42:16.177015 140339465779200 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_10/mlp/wo/kernel
I0512 23:42:16.177048 140339465779200 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_10/pre_mlp_layer_norm/scale
I0512 23:42:16.177081 140339465779200 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_10/pre_self_attention_layer_norm/scale
I0512 23:42:16.177115 140339465779200 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_10/self_attention/key/kernel
I0512 23:42:16.177148 140339465779200 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_10/self_attention/out/kernel
I0512 23:42:16.177181 140339465779200 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_10/self_attention/query/kernel
I0512 23:42:16.177214 140339465779200 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_10/self_attention/value/kernel
I0512 23:42:16.177247 140339465779200 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_11/extra_mlp/wi_0/kernel
I0512 23:42:16.177279 140339465779200 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_11/extra_mlp/wi_1/kernel
I0512 23:42:16.177312 140339465779200 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_11/extra_mlp/wo/kernel
I0512 23:42:16.177345 140339465779200 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_11/mlp/expert/wi_0/kernel
I0512 23:42:16.177385 140339465779200 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_11/mlp/expert/wi_1/kernel
I0512 23:42:16.177418 140339465779200 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_11/mlp/expert/wo/kernel
I0512 23:42:16.177451 140339465779200 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_11/mlp/router/router_weights/w/kernel
I0512 23:42:16.177484 140339465779200 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_11/pre_extra_mlp_layer_norm/scale
I0512 23:42:16.177517 140339465779200 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_11/pre_mlp_layer_norm/scale
I0512 23:42:16.177550 140339465779200 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_11/pre_self_attention_layer_norm/scale
I0512 23:42:16.177583 140339465779200 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_11/self_attention/key/kernel
I0512 23:42:16.177617 140339465779200 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_11/self_attention/out/kernel
I0512 23:42:16.177651 140339465779200 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_11/self_attention/query/kernel
I0512 23:42:16.177685 140339465779200 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_11/self_attention/value/kernel
I0512 23:42:16.177717 140339465779200 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_12/mlp/wi_0/kernel
I0512 23:42:16.177750 140339465779200 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_12/mlp/wi_1/kernel
I0512 23:42:16.177783 140339465779200 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_12/mlp/wo/kernel
I0512 23:42:16.177817 140339465779200 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_12/pre_mlp_layer_norm/scale
I0512 23:42:16.177850 140339465779200 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_12/pre_self_attention_layer_norm/scale
I0512 23:42:16.177883 140339465779200 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_12/self_attention/key/kernel
I0512 23:42:16.177916 140339465779200 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_12/self_attention/out/kernel
I0512 23:42:16.177949 140339465779200 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_12/self_attention/query/kernel
I0512 23:42:16.177982 140339465779200 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_12/self_attention/value/kernel
I0512 23:42:16.178014 140339465779200 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_13/mlp/wi_0/kernel
I0512 23:42:16.178047 140339465779200 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_13/mlp/wi_1/kernel
I0512 23:42:16.178080 140339465779200 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_13/mlp/wo/kernel
I0512 23:42:16.178113 140339465779200 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_13/pre_mlp_layer_norm/scale
I0512 23:42:16.178147 140339465779200 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_13/pre_self_attention_layer_norm/scale
I0512 23:42:16.178180 140339465779200 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_13/self_attention/key/kernel
I0512 23:42:16.178213 140339465779200 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_13/self_attention/out/kernel
I0512 23:42:16.178246 140339465779200 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_13/self_attention/query/kernel
I0512 23:42:16.178279 140339465779200 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_13/self_attention/value/kernel
I0512 23:42:16.178311 140339465779200 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_14/mlp/wi_0/kernel
I0512 23:42:16.178344 140339465779200 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_14/mlp/wi_1/kernel
I0512 23:42:16.178383 140339465779200 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_14/mlp/wo/kernel
I0512 23:42:16.178417 140339465779200 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_14/pre_mlp_layer_norm/scale
I0512 23:42:16.178451 140339465779200 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_14/pre_self_attention_layer_norm/scale
I0512 23:42:16.178484 140339465779200 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_14/self_attention/key/kernel
I0512 23:42:16.178517 140339465779200 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_14/self_attention/out/kernel
I0512 23:42:16.178550 140339465779200 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_14/self_attention/query/kernel
I0512 23:42:16.178583 140339465779200 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_14/self_attention/value/kernel
I0512 23:42:16.178616 140339465779200 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_15/mlp/wi_0/kernel
I0512 23:42:16.178650 140339465779200 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_15/mlp/wi_1/kernel
I0512 23:42:16.178684 140339465779200 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_15/mlp/wo/kernel
I0512 23:42:16.178717 140339465779200 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_15/pre_mlp_layer_norm/scale
I0512 23:42:16.178750 140339465779200 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_15/pre_self_attention_layer_norm/scale
I0512 23:42:16.178783 140339465779200 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_15/self_attention/key/kernel
I0512 23:42:16.178816 140339465779200 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_15/self_attention/out/kernel
I0512 23:42:16.178849 140339465779200 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_15/self_attention/query/kernel
I0512 23:42:16.178882 140339465779200 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_15/self_attention/value/kernel
I0512 23:42:16.178915 140339465779200 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_16/mlp/wi_0/kernel
I0512 23:42:16.178948 140339465779200 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_16/mlp/wi_1/kernel
I0512 23:42:16.178981 140339465779200 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_16/mlp/wo/kernel
I0512 23:42:16.179014 140339465779200 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_16/pre_mlp_layer_norm/scale
I0512 23:42:16.179047 140339465779200 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_16/pre_self_attention_layer_norm/scale
I0512 23:42:16.179080 140339465779200 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_16/self_attention/key/kernel
I0512 23:42:16.179114 140339465779200 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_16/self_attention/out/kernel
I0512 23:42:16.179147 140339465779200 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_16/self_attention/query/kernel
I0512 23:42:16.179179 140339465779200 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_16/self_attention/value/kernel
I0512 23:42:16.179212 140339465779200 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_17/extra_mlp/wi_0/kernel
I0512 23:42:16.179244 140339465779200 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_17/extra_mlp/wi_1/kernel
I0512 23:42:16.179277 140339465779200 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_17/extra_mlp/wo/kernel
I0512 23:42:16.179310 140339465779200 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_17/mlp/expert/wi_0/kernel
I0512 23:42:16.179342 140339465779200 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_17/mlp/expert/wi_1/kernel
I0512 23:42:16.179382 140339465779200 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_17/mlp/expert/wo/kernel
I0512 23:42:16.179415 140339465779200 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_17/mlp/router/router_weights/w/kernel
I0512 23:42:16.179448 140339465779200 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_17/pre_extra_mlp_layer_norm/scale
I0512 23:42:16.179481 140339465779200 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_17/pre_mlp_layer_norm/scale
I0512 23:42:16.179514 140339465779200 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_17/pre_self_attention_layer_norm/scale
I0512 23:42:16.179547 140339465779200 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_17/self_attention/key/kernel
I0512 23:42:16.179580 140339465779200 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_17/self_attention/out/kernel
I0512 23:42:16.179614 140339465779200 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_17/self_attention/query/kernel
I0512 23:42:16.179648 140339465779200 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_17/self_attention/value/kernel
I0512 23:42:16.179681 140339465779200 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_18/mlp/wi_0/kernel
I0512 23:42:16.179714 140339465779200 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_18/mlp/wi_1/kernel
I0512 23:42:16.179747 140339465779200 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_18/mlp/wo/kernel
I0512 23:42:16.179781 140339465779200 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_18/pre_mlp_layer_norm/scale
I0512 23:42:16.179814 140339465779200 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_18/pre_self_attention_layer_norm/scale
I0512 23:42:16.179847 140339465779200 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_18/self_attention/key/kernel
I0512 23:42:16.179881 140339465779200 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_18/self_attention/out/kernel
I0512 23:42:16.179913 140339465779200 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_18/self_attention/query/kernel
I0512 23:42:16.179946 140339465779200 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_18/self_attention/value/kernel
I0512 23:42:16.179978 140339465779200 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_19/mlp/wi_0/kernel
I0512 23:42:16.180011 140339465779200 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_19/mlp/wi_1/kernel
I0512 23:42:16.180043 140339465779200 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_19/mlp/wo/kernel
I0512 23:42:16.180077 140339465779200 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_19/pre_mlp_layer_norm/scale
I0512 23:42:16.180109 140339465779200 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_19/pre_self_attention_layer_norm/scale
I0512 23:42:16.180143 140339465779200 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_19/self_attention/key/kernel
I0512 23:42:16.180196 140339465779200 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_19/self_attention/out/kernel
I0512 23:42:16.180232 140339465779200 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_19/self_attention/query/kernel
I0512 23:42:16.180265 140339465779200 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_19/self_attention/value/kernel
I0512 23:42:16.180298 140339465779200 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_2/mlp/wi_0/kernel
I0512 23:42:16.180331 140339465779200 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_2/mlp/wi_1/kernel
I0512 23:42:16.180370 140339465779200 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_2/mlp/wo/kernel
I0512 23:42:16.180404 140339465779200 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_2/pre_mlp_layer_norm/scale
I0512 23:42:16.180438 140339465779200 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_2/pre_self_attention_layer_norm/scale
I0512 23:42:16.180471 140339465779200 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_2/self_attention/key/kernel
I0512 23:42:16.180504 140339465779200 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_2/self_attention/out/kernel
I0512 23:42:16.180537 140339465779200 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_2/self_attention/query/kernel
I0512 23:42:16.180570 140339465779200 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_2/self_attention/value/kernel
I0512 23:42:16.180602 140339465779200 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_20/mlp/wi_0/kernel
I0512 23:42:16.180638 140339465779200 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_20/mlp/wi_1/kernel
I0512 23:42:16.180672 140339465779200 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_20/mlp/wo/kernel
I0512 23:42:16.180706 140339465779200 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_20/pre_mlp_layer_norm/scale
I0512 23:42:16.180739 140339465779200 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_20/pre_self_attention_layer_norm/scale
I0512 23:42:16.180772 140339465779200 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_20/self_attention/key/kernel
I0512 23:42:16.180805 140339465779200 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_20/self_attention/out/kernel
I0512 23:42:16.180839 140339465779200 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_20/self_attention/query/kernel
I0512 23:42:16.180872 140339465779200 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_20/self_attention/value/kernel
I0512 23:42:16.180905 140339465779200 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_21/mlp/wi_0/kernel
I0512 23:42:16.180938 140339465779200 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_21/mlp/wi_1/kernel
I0512 23:42:16.180971 140339465779200 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_21/mlp/wo/kernel
I0512 23:42:16.181004 140339465779200 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_21/pre_mlp_layer_norm/scale
I0512 23:42:16.181037 140339465779200 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_21/pre_self_attention_layer_norm/scale
I0512 23:42:16.181070 140339465779200 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_21/self_attention/key/kernel
I0512 23:42:16.181103 140339465779200 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_21/self_attention/out/kernel
I0512 23:42:16.181136 140339465779200 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_21/self_attention/query/kernel
I0512 23:42:16.181169 140339465779200 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_21/self_attention/value/kernel
I0512 23:42:16.181202 140339465779200 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_22/mlp/wi_0/kernel
I0512 23:42:16.181235 140339465779200 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_22/mlp/wi_1/kernel
I0512 23:42:16.181268 140339465779200 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_22/mlp/wo/kernel
I0512 23:42:16.181301 140339465779200 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_22/pre_mlp_layer_norm/scale
I0512 23:42:16.181334 140339465779200 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_22/pre_self_attention_layer_norm/scale
I0512 23:42:16.181373 140339465779200 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_22/self_attention/key/kernel
I0512 23:42:16.181407 140339465779200 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_22/self_attention/out/kernel
I0512 23:42:16.181441 140339465779200 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_22/self_attention/query/kernel
I0512 23:42:16.181473 140339465779200 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_22/self_attention/value/kernel
I0512 23:42:16.181507 140339465779200 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_23/extra_mlp/wi_0/kernel
I0512 23:42:16.181539 140339465779200 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_23/extra_mlp/wi_1/kernel
I0512 23:42:16.181572 140339465779200 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_23/extra_mlp/wo/kernel
I0512 23:42:16.181605 140339465779200 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_23/mlp/expert/wi_0/kernel
I0512 23:42:16.181640 140339465779200 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_23/mlp/expert/wi_1/kernel
I0512 23:42:16.181674 140339465779200 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_23/mlp/expert/wo/kernel
I0512 23:42:16.181706 140339465779200 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_23/mlp/router/router_weights/w/kernel
I0512 23:42:16.181740 140339465779200 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_23/pre_extra_mlp_layer_norm/scale
I0512 23:42:16.181773 140339465779200 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_23/pre_mlp_layer_norm/scale
I0512 23:42:16.181806 140339465779200 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_23/pre_self_attention_layer_norm/scale
I0512 23:42:16.181839 140339465779200 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_23/self_attention/key/kernel
I0512 23:42:16.181872 140339465779200 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_23/self_attention/out/kernel
I0512 23:42:16.181905 140339465779200 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_23/self_attention/query/kernel
I0512 23:42:16.181938 140339465779200 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_23/self_attention/value/kernel
I0512 23:42:16.181971 140339465779200 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_3/mlp/wi_0/kernel
I0512 23:42:16.182004 140339465779200 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_3/mlp/wi_1/kernel
I0512 23:42:16.182037 140339465779200 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_3/mlp/wo/kernel
I0512 23:42:16.182070 140339465779200 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_3/pre_mlp_layer_norm/scale
I0512 23:42:16.182103 140339465779200 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_3/pre_self_attention_layer_norm/scale
I0512 23:42:16.182137 140339465779200 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_3/self_attention/key/kernel
I0512 23:42:16.182169 140339465779200 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_3/self_attention/out/kernel
I0512 23:42:16.182202 140339465779200 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_3/self_attention/query/kernel
I0512 23:42:16.182234 140339465779200 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_3/self_attention/value/kernel
I0512 23:42:16.182267 140339465779200 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_4/mlp/wi_0/kernel
I0512 23:42:16.182300 140339465779200 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_4/mlp/wi_1/kernel
I0512 23:42:16.182333 140339465779200 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_4/mlp/wo/kernel
I0512 23:42:16.182372 140339465779200 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_4/pre_mlp_layer_norm/scale
I0512 23:42:16.182407 140339465779200 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_4/pre_self_attention_layer_norm/scale
I0512 23:42:16.182440 140339465779200 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_4/self_attention/key/kernel
I0512 23:42:16.182473 140339465779200 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_4/self_attention/out/kernel
I0512 23:42:16.182507 140339465779200 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_4/self_attention/query/kernel
I0512 23:42:16.182539 140339465779200 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_4/self_attention/value/kernel
I0512 23:42:16.182573 140339465779200 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_5/extra_mlp/wi_0/kernel
I0512 23:42:16.182605 140339465779200 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_5/extra_mlp/wi_1/kernel
I0512 23:42:16.182641 140339465779200 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_5/extra_mlp/wo/kernel
I0512 23:42:16.182674 140339465779200 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_5/mlp/expert/wi_0/kernel
I0512 23:42:16.182708 140339465779200 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_5/mlp/expert/wi_1/kernel
I0512 23:42:16.182740 140339465779200 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_5/mlp/expert/wo/kernel
I0512 23:42:16.182773 140339465779200 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_5/mlp/router/router_weights/w/kernel
I0512 23:42:16.182806 140339465779200 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_5/pre_extra_mlp_layer_norm/scale
I0512 23:42:16.182839 140339465779200 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_5/pre_mlp_layer_norm/scale
I0512 23:42:16.182872 140339465779200 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_5/pre_self_attention_layer_norm/scale
I0512 23:42:16.182905 140339465779200 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_5/self_attention/key/kernel
I0512 23:42:16.182938 140339465779200 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_5/self_attention/out/kernel
I0512 23:42:16.182971 140339465779200 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_5/self_attention/query/kernel
I0512 23:42:16.183003 140339465779200 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_5/self_attention/value/kernel
I0512 23:42:16.183036 140339465779200 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_6/mlp/wi_0/kernel
I0512 23:42:16.183068 140339465779200 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_6/mlp/wi_1/kernel
I0512 23:42:16.183101 140339465779200 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_6/mlp/wo/kernel
I0512 23:42:16.183134 140339465779200 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_6/pre_mlp_layer_norm/scale
I0512 23:42:16.183168 140339465779200 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_6/pre_self_attention_layer_norm/scale
I0512 23:42:16.183201 140339465779200 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_6/self_attention/key/kernel
I0512 23:42:16.183234 140339465779200 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_6/self_attention/out/kernel
I0512 23:42:16.183267 140339465779200 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_6/self_attention/query/kernel
I0512 23:42:16.183300 140339465779200 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_6/self_attention/value/kernel
I0512 23:42:16.183332 140339465779200 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_7/mlp/wi_0/kernel
I0512 23:42:16.183371 140339465779200 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_7/mlp/wi_1/kernel
I0512 23:42:16.183405 140339465779200 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_7/mlp/wo/kernel
I0512 23:42:16.183439 140339465779200 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_7/pre_mlp_layer_norm/scale
I0512 23:42:16.183472 140339465779200 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_7/pre_self_attention_layer_norm/scale
I0512 23:42:16.183523 140339465779200 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_7/self_attention/key/kernel
I0512 23:42:16.183558 140339465779200 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_7/self_attention/out/kernel
I0512 23:42:16.183591 140339465779200 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_7/self_attention/query/kernel
I0512 23:42:16.183625 140339465779200 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_7/self_attention/value/kernel
I0512 23:42:16.183659 140339465779200 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_8/mlp/wi_0/kernel
I0512 23:42:16.183692 140339465779200 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_8/mlp/wi_1/kernel
I0512 23:42:16.183725 140339465779200 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_8/mlp/wo/kernel
I0512 23:42:16.183758 140339465779200 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_8/pre_mlp_layer_norm/scale
I0512 23:42:16.183791 140339465779200 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_8/pre_self_attention_layer_norm/scale
I0512 23:42:16.183824 140339465779200 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_8/self_attention/key/kernel
I0512 23:42:16.183857 140339465779200 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_8/self_attention/out/kernel
I0512 23:42:16.183889 140339465779200 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_8/self_attention/query/kernel
I0512 23:42:16.183922 140339465779200 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_8/self_attention/value/kernel
I0512 23:42:16.183955 140339465779200 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_9/mlp/wi_0/kernel
I0512 23:42:16.183988 140339465779200 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_9/mlp/wi_1/kernel
I0512 23:42:16.184021 140339465779200 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_9/mlp/wo/kernel
I0512 23:42:16.184054 140339465779200 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_9/pre_mlp_layer_norm/scale
I0512 23:42:16.184087 140339465779200 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_9/pre_self_attention_layer_norm/scale
I0512 23:42:16.184121 140339465779200 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_9/self_attention/key/kernel
I0512 23:42:16.184153 140339465779200 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_9/self_attention/out/kernel
I0512 23:42:16.184208 140339465779200 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_9/self_attention/query/kernel
I0512 23:42:16.184243 140339465779200 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_9/self_attention/value/kernel
I0512 23:42:16.184277 140339465779200 checkpoints.py:1100] Restoring key from ckpt: target/decoder/logits_dense/kernel
I0512 23:42:16.184310 140339465779200 checkpoints.py:1100] Restoring key from ckpt: target/token_embedder/embedding
I0512 23:42:16.189471 140595318249472 checkpoints.py:1054] Restoring from checkpoint: gs://rosinality-tpu-bucket/openmoe_8b/training/checkpoint_32500/checkpoint
I0512 23:42:16.201210 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/decoder_norm/scale/m
I0512 23:42:16.201367 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/decoder_norm/scale/v
I0512 23:42:16.201407 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/decoder_norm/scale/v_col
I0512 23:42:16.201442 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/decoder_norm/scale/v_row
I0512 23:42:16.201477 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_0/mlp/wi_0/kernel/m
I0512 23:42:16.201511 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_0/mlp/wi_0/kernel/v
I0512 23:42:16.201545 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_0/mlp/wi_0/kernel/v_col
I0512 23:42:16.201588 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_0/mlp/wi_0/kernel/v_row
I0512 23:42:16.201631 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_0/mlp/wi_1/kernel/m
I0512 23:42:16.201668 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_0/mlp/wi_1/kernel/v
I0512 23:42:16.201702 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_0/mlp/wi_1/kernel/v_col
I0512 23:42:16.201740 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_0/mlp/wi_1/kernel/v_row
I0512 23:42:16.201777 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_0/mlp/wo/kernel/m
I0512 23:42:16.201810 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_0/mlp/wo/kernel/v
I0512 23:42:16.201853 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_0/mlp/wo/kernel/v_col
I0512 23:42:16.201889 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_0/mlp/wo/kernel/v_row
I0512 23:42:16.201924 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_0/pre_mlp_layer_norm/scale/m
I0512 23:42:16.201957 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_0/pre_mlp_layer_norm/scale/v
I0512 23:42:16.201991 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_0/pre_mlp_layer_norm/scale/v_col
I0512 23:42:16.202023 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_0/pre_mlp_layer_norm/scale/v_row
I0512 23:42:16.202057 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_0/pre_self_attention_layer_norm/scale/m
I0512 23:42:16.202091 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_0/pre_self_attention_layer_norm/scale/v
I0512 23:42:16.202123 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_0/pre_self_attention_layer_norm/scale/v_col
I0512 23:42:16.202158 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_0/pre_self_attention_layer_norm/scale/v_row
I0512 23:42:16.202227 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_0/self_attention/key/kernel/m
I0512 23:42:16.202262 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_0/self_attention/key/kernel/v
I0512 23:42:16.202298 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_0/self_attention/key/kernel/v_col
I0512 23:42:16.202332 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_0/self_attention/key/kernel/v_row
I0512 23:42:16.202363 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_0/self_attention/out/kernel/m
I0512 23:42:16.202397 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_0/self_attention/out/kernel/v
I0512 23:42:16.202428 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_0/self_attention/out/kernel/v_col
I0512 23:42:16.202461 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_0/self_attention/out/kernel/v_row
I0512 23:42:16.202495 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_0/self_attention/query/kernel/m
I0512 23:42:16.202526 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_0/self_attention/query/kernel/v
I0512 23:42:16.202559 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_0/self_attention/query/kernel/v_col
I0512 23:42:16.202591 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_0/self_attention/query/kernel/v_row
I0512 23:42:16.202628 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_0/self_attention/value/kernel/m
I0512 23:42:16.202662 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_0/self_attention/value/kernel/v
I0512 23:42:16.202693 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_0/self_attention/value/kernel/v_col
I0512 23:42:16.202727 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_0/self_attention/value/kernel/v_row
I0512 23:42:16.202761 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_1/mlp/wi_0/kernel/m
I0512 23:42:16.202792 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_1/mlp/wi_0/kernel/v
I0512 23:42:16.202826 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_1/mlp/wi_0/kernel/v_col
I0512 23:42:16.202866 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_1/mlp/wi_0/kernel/v_row
I0512 23:42:16.202898 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_1/mlp/wi_1/kernel/m
I0512 23:42:16.202929 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_1/mlp/wi_1/kernel/v
I0512 23:42:16.202959 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_1/mlp/wi_1/kernel/v_col
I0512 23:42:16.202990 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_1/mlp/wi_1/kernel/v_row
I0512 23:42:16.203021 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_1/mlp/wo/kernel/m
I0512 23:42:16.203052 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_1/mlp/wo/kernel/v
I0512 23:42:16.203082 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_1/mlp/wo/kernel/v_col
I0512 23:42:16.203113 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_1/mlp/wo/kernel/v_row
I0512 23:42:16.203144 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_1/pre_mlp_layer_norm/scale/m
I0512 23:42:16.203175 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_1/pre_mlp_layer_norm/scale/v
I0512 23:42:16.203206 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_1/pre_mlp_layer_norm/scale/v_col
I0512 23:42:16.203236 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_1/pre_mlp_layer_norm/scale/v_row
I0512 23:42:16.203268 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_1/pre_self_attention_layer_norm/scale/m
I0512 23:42:16.203322 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_1/pre_self_attention_layer_norm/scale/v
I0512 23:42:16.203357 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_1/pre_self_attention_layer_norm/scale/v_col
I0512 23:42:16.203389 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_1/pre_self_attention_layer_norm/scale/v_row
I0512 23:42:16.203420 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_1/self_attention/key/kernel/m
I0512 23:42:16.203451 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_1/self_attention/key/kernel/v
I0512 23:42:16.203483 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_1/self_attention/key/kernel/v_col
I0512 23:42:16.203514 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_1/self_attention/key/kernel/v_row
I0512 23:42:16.203545 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_1/self_attention/out/kernel/m
I0512 23:42:16.203576 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_1/self_attention/out/kernel/v
I0512 23:42:16.203609 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_1/self_attention/out/kernel/v_col
I0512 23:42:16.203641 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_1/self_attention/out/kernel/v_row
I0512 23:42:16.203671 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_1/self_attention/query/kernel/m
I0512 23:42:16.203702 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_1/self_attention/query/kernel/v
I0512 23:42:16.203733 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_1/self_attention/query/kernel/v_col
I0512 23:42:16.203763 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_1/self_attention/query/kernel/v_row
I0512 23:42:16.203794 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_1/self_attention/value/kernel/m
I0512 23:42:16.203825 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_1/self_attention/value/kernel/v
I0512 23:42:16.203862 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_1/self_attention/value/kernel/v_col
I0512 23:42:16.203893 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_1/self_attention/value/kernel/v_row
I0512 23:42:16.203924 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_10/mlp/wi_0/kernel/m
I0512 23:42:16.203955 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_10/mlp/wi_0/kernel/v
I0512 23:42:16.203986 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_10/mlp/wi_0/kernel/v_col
I0512 23:42:16.204017 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_10/mlp/wi_0/kernel/v_row
I0512 23:42:16.204048 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_10/mlp/wi_1/kernel/m
I0512 23:42:16.204079 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_10/mlp/wi_1/kernel/v
I0512 23:42:16.204110 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_10/mlp/wi_1/kernel/v_col
I0512 23:42:16.204141 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_10/mlp/wi_1/kernel/v_row
I0512 23:42:16.204171 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_10/mlp/wo/kernel/m
I0512 23:42:16.204202 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_10/mlp/wo/kernel/v
I0512 23:42:16.204233 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_10/mlp/wo/kernel/v_col
I0512 23:42:16.204264 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_10/mlp/wo/kernel/v_row
I0512 23:42:16.204294 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_10/pre_mlp_layer_norm/scale/m
I0512 23:42:16.204325 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_10/pre_mlp_layer_norm/scale/v
I0512 23:42:16.204356 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_10/pre_mlp_layer_norm/scale/v_col
I0512 23:42:16.204387 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_10/pre_mlp_layer_norm/scale/v_row
I0512 23:42:16.204417 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_10/pre_self_attention_layer_norm/scale/m
I0512 23:42:16.204448 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_10/pre_self_attention_layer_norm/scale/v
I0512 23:42:16.204479 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_10/pre_self_attention_layer_norm/scale/v_col
I0512 23:42:16.204510 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_10/pre_self_attention_layer_norm/scale/v_row
I0512 23:42:16.204540 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_10/self_attention/key/kernel/m
I0512 23:42:16.204571 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_10/self_attention/key/kernel/v
I0512 23:42:16.204604 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_10/self_attention/key/kernel/v_col
I0512 23:42:16.204636 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_10/self_attention/key/kernel/v_row
I0512 23:42:16.204668 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_10/self_attention/out/kernel/m
I0512 23:42:16.204699 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_10/self_attention/out/kernel/v
I0512 23:42:16.204729 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_10/self_attention/out/kernel/v_col
I0512 23:42:16.204760 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_10/self_attention/out/kernel/v_row
I0512 23:42:16.204791 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_10/self_attention/query/kernel/m
I0512 23:42:16.204822 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_10/self_attention/query/kernel/v
I0512 23:42:16.204858 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_10/self_attention/query/kernel/v_col
I0512 23:42:16.204889 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_10/self_attention/query/kernel/v_row
I0512 23:42:16.204920 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_10/self_attention/value/kernel/m
I0512 23:42:16.204951 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_10/self_attention/value/kernel/v
I0512 23:42:16.204982 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_10/self_attention/value/kernel/v_col
I0512 23:42:16.205013 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_10/self_attention/value/kernel/v_row
I0512 23:42:16.205044 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/extra_mlp/wi_0/kernel/m
I0512 23:42:16.205074 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/extra_mlp/wi_0/kernel/v
I0512 23:42:16.205105 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/extra_mlp/wi_0/kernel/v_col
I0512 23:42:16.205136 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/extra_mlp/wi_0/kernel/v_row
I0512 23:42:16.205167 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/extra_mlp/wi_1/kernel/m
I0512 23:42:16.205198 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/extra_mlp/wi_1/kernel/v
I0512 23:42:16.205229 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/extra_mlp/wi_1/kernel/v_col
I0512 23:42:16.205260 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/extra_mlp/wi_1/kernel/v_row
I0512 23:42:16.205291 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/extra_mlp/wo/kernel/m
I0512 23:42:16.205322 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/extra_mlp/wo/kernel/v
I0512 23:42:16.205353 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/extra_mlp/wo/kernel/v_col
I0512 23:42:16.205384 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/extra_mlp/wo/kernel/v_row
I0512 23:42:16.205414 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/mlp/expert/wi_0/kernel/m
I0512 23:42:16.205445 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/mlp/expert/wi_0/kernel/v
I0512 23:42:16.205476 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/mlp/expert/wi_0/kernel/v_col
I0512 23:42:16.205507 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/mlp/expert/wi_0/kernel/v_row
I0512 23:42:16.205538 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/mlp/expert/wi_1/kernel/m
I0512 23:42:16.205569 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/mlp/expert/wi_1/kernel/v
I0512 23:42:16.205601 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/mlp/expert/wi_1/kernel/v_col
I0512 23:42:16.205633 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/mlp/expert/wi_1/kernel/v_row
I0512 23:42:16.205665 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/mlp/expert/wo/kernel/m
I0512 23:42:16.205696 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/mlp/expert/wo/kernel/v
I0512 23:42:16.205726 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/mlp/expert/wo/kernel/v_col
I0512 23:42:16.205757 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/mlp/expert/wo/kernel/v_row
I0512 23:42:16.205788 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/mlp/router/router_weights/w/kernel/m
I0512 23:42:16.205819 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/mlp/router/router_weights/w/kernel/v
I0512 23:42:16.205855 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/mlp/router/router_weights/w/kernel/v_col
I0512 23:42:16.205887 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/mlp/router/router_weights/w/kernel/v_row
I0512 23:42:16.205918 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/pre_extra_mlp_layer_norm/scale/m
I0512 23:42:16.205949 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/pre_extra_mlp_layer_norm/scale/v
I0512 23:42:16.205979 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/pre_extra_mlp_layer_norm/scale/v_col
I0512 23:42:16.206010 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/pre_extra_mlp_layer_norm/scale/v_row
I0512 23:42:16.206041 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/pre_mlp_layer_norm/scale/m
I0512 23:42:16.206072 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/pre_mlp_layer_norm/scale/v
I0512 23:42:16.206102 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/pre_mlp_layer_norm/scale/v_col
I0512 23:42:16.206133 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/pre_mlp_layer_norm/scale/v_row
I0512 23:42:16.206164 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/pre_self_attention_layer_norm/scale/m
I0512 23:42:16.206195 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/pre_self_attention_layer_norm/scale/v
I0512 23:42:16.206225 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/pre_self_attention_layer_norm/scale/v_col
I0512 23:42:16.206256 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/pre_self_attention_layer_norm/scale/v_row
I0512 23:42:16.206287 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/self_attention/key/kernel/m
I0512 23:42:16.206318 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/self_attention/key/kernel/v
I0512 23:42:16.206349 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/self_attention/key/kernel/v_col
I0512 23:42:16.206380 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/self_attention/key/kernel/v_row
I0512 23:42:16.206411 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/self_attention/out/kernel/m
I0512 23:42:16.206441 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/self_attention/out/kernel/v
I0512 23:42:16.206472 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/self_attention/out/kernel/v_col
I0512 23:42:16.206503 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/self_attention/out/kernel/v_row
I0512 23:42:16.206533 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/self_attention/query/kernel/m
I0512 23:42:16.206564 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/self_attention/query/kernel/v
I0512 23:42:16.206597 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/self_attention/query/kernel/v_col
I0512 23:42:16.206629 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/self_attention/query/kernel/v_row
I0512 23:42:16.206660 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/self_attention/value/kernel/m
I0512 23:42:16.206691 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/self_attention/value/kernel/v
I0512 23:42:16.206722 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/self_attention/value/kernel/v_col
I0512 23:42:16.206753 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/self_attention/value/kernel/v_row
I0512 23:42:16.206784 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_12/mlp/wi_0/kernel/m
I0512 23:42:16.206815 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_12/mlp/wi_0/kernel/v
I0512 23:42:16.206851 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_12/mlp/wi_0/kernel/v_col
I0512 23:42:16.206883 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_12/mlp/wi_0/kernel/v_row
I0512 23:42:16.206915 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_12/mlp/wi_1/kernel/m
I0512 23:42:16.206946 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_12/mlp/wi_1/kernel/v
I0512 23:42:16.206977 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_12/mlp/wi_1/kernel/v_col
I0512 23:42:16.207007 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_12/mlp/wi_1/kernel/v_row
I0512 23:42:16.207038 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_12/mlp/wo/kernel/m
I0512 23:42:16.207069 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_12/mlp/wo/kernel/v
I0512 23:42:16.207101 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_12/mlp/wo/kernel/v_col
I0512 23:42:16.207132 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_12/mlp/wo/kernel/v_row
I0512 23:42:16.207163 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_12/pre_mlp_layer_norm/scale/m
I0512 23:42:16.207194 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_12/pre_mlp_layer_norm/scale/v
I0512 23:42:16.207225 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_12/pre_mlp_layer_norm/scale/v_col
I0512 23:42:16.207256 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_12/pre_mlp_layer_norm/scale/v_row
I0512 23:42:16.207315 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_12/pre_self_attention_layer_norm/scale/m
I0512 23:42:16.207354 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_12/pre_self_attention_layer_norm/scale/v
I0512 23:42:16.207387 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_12/pre_self_attention_layer_norm/scale/v_col
I0512 23:42:16.207418 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_12/pre_self_attention_layer_norm/scale/v_row
I0512 23:42:16.207449 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_12/self_attention/key/kernel/m
I0512 23:42:16.207479 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_12/self_attention/key/kernel/v
I0512 23:42:16.207510 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_12/self_attention/key/kernel/v_col
I0512 23:42:16.207541 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_12/self_attention/key/kernel/v_row
I0512 23:42:16.207572 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_12/self_attention/out/kernel/m
I0512 23:42:16.207606 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_12/self_attention/out/kernel/v
I0512 23:42:16.207637 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_12/self_attention/out/kernel/v_col
I0512 23:42:16.207668 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_12/self_attention/out/kernel/v_row
I0512 23:42:16.207699 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_12/self_attention/query/kernel/m
I0512 23:42:16.207730 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_12/self_attention/query/kernel/v
I0512 23:42:16.207761 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_12/self_attention/query/kernel/v_col
I0512 23:42:16.207791 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_12/self_attention/query/kernel/v_row
I0512 23:42:16.207822 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_12/self_attention/value/kernel/m
I0512 23:42:16.207859 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_12/self_attention/value/kernel/v
I0512 23:42:16.207890 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_12/self_attention/value/kernel/v_col
I0512 23:42:16.207923 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_12/self_attention/value/kernel/v_row
I0512 23:42:16.207956 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_13/mlp/wi_0/kernel/m
I0512 23:42:16.207987 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_13/mlp/wi_0/kernel/v
I0512 23:42:16.208018 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_13/mlp/wi_0/kernel/v_col
I0512 23:42:16.208050 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_13/mlp/wi_0/kernel/v_row
I0512 23:42:16.208081 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_13/mlp/wi_1/kernel/m
I0512 23:42:16.208112 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_13/mlp/wi_1/kernel/v
I0512 23:42:16.208143 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_13/mlp/wi_1/kernel/v_col
I0512 23:42:16.208173 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_13/mlp/wi_1/kernel/v_row
I0512 23:42:16.208205 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_13/mlp/wo/kernel/m
I0512 23:42:16.208235 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_13/mlp/wo/kernel/v
I0512 23:42:16.208266 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_13/mlp/wo/kernel/v_col
I0512 23:42:16.208297 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_13/mlp/wo/kernel/v_row
I0512 23:42:16.208328 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_13/pre_mlp_layer_norm/scale/m
I0512 23:42:16.208359 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_13/pre_mlp_layer_norm/scale/v
I0512 23:42:16.208389 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_13/pre_mlp_layer_norm/scale/v_col
I0512 23:42:16.208420 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_13/pre_mlp_layer_norm/scale/v_row
I0512 23:42:16.208451 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_13/pre_self_attention_layer_norm/scale/m
I0512 23:42:16.208482 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_13/pre_self_attention_layer_norm/scale/v
I0512 23:42:16.208513 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_13/pre_self_attention_layer_norm/scale/v_col
I0512 23:42:16.208544 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_13/pre_self_attention_layer_norm/scale/v_row
I0512 23:42:16.208575 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_13/self_attention/key/kernel/m
I0512 23:42:16.208608 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_13/self_attention/key/kernel/v
I0512 23:42:16.208640 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_13/self_attention/key/kernel/v_col
I0512 23:42:16.208671 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_13/self_attention/key/kernel/v_row
I0512 23:42:16.208701 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_13/self_attention/out/kernel/m
I0512 23:42:16.208732 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_13/self_attention/out/kernel/v
I0512 23:42:16.208763 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_13/self_attention/out/kernel/v_col
I0512 23:42:16.208794 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_13/self_attention/out/kernel/v_row
I0512 23:42:16.208825 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_13/self_attention/query/kernel/m
I0512 23:42:16.208862 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_13/self_attention/query/kernel/v
I0512 23:42:16.208894 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_13/self_attention/query/kernel/v_col
I0512 23:42:16.208925 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_13/self_attention/query/kernel/v_row
I0512 23:42:16.208956 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_13/self_attention/value/kernel/m
I0512 23:42:16.208986 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_13/self_attention/value/kernel/v
I0512 23:42:16.209017 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_13/self_attention/value/kernel/v_col
I0512 23:42:16.209048 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_13/self_attention/value/kernel/v_row
I0512 23:42:16.209079 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_14/mlp/wi_0/kernel/m
I0512 23:42:16.209110 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_14/mlp/wi_0/kernel/v
I0512 23:42:16.209141 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_14/mlp/wi_0/kernel/v_col
I0512 23:42:16.209172 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_14/mlp/wi_0/kernel/v_row
I0512 23:42:16.209203 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_14/mlp/wi_1/kernel/m
I0512 23:42:16.209233 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_14/mlp/wi_1/kernel/v
I0512 23:42:16.209264 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_14/mlp/wi_1/kernel/v_col
I0512 23:42:16.209296 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_14/mlp/wi_1/kernel/v_row
I0512 23:42:16.209326 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_14/mlp/wo/kernel/m
I0512 23:42:16.209357 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_14/mlp/wo/kernel/v
I0512 23:42:16.209388 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_14/mlp/wo/kernel/v_col
I0512 23:42:16.209422 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_14/mlp/wo/kernel/v_row
I0512 23:42:16.209456 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_14/pre_mlp_layer_norm/scale/m
I0512 23:42:16.209486 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_14/pre_mlp_layer_norm/scale/v
I0512 23:42:16.209518 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_14/pre_mlp_layer_norm/scale/v_col
I0512 23:42:16.209548 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_14/pre_mlp_layer_norm/scale/v_row
I0512 23:42:16.209580 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_14/pre_self_attention_layer_norm/scale/m
I0512 23:42:16.209613 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_14/pre_self_attention_layer_norm/scale/v
I0512 23:42:16.209644 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_14/pre_self_attention_layer_norm/scale/v_col
I0512 23:42:16.209675 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_14/pre_self_attention_layer_norm/scale/v_row
I0512 23:42:16.209707 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_14/self_attention/key/kernel/m
I0512 23:42:16.209738 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_14/self_attention/key/kernel/v
I0512 23:42:16.209768 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_14/self_attention/key/kernel/v_col
I0512 23:42:16.209799 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_14/self_attention/key/kernel/v_row
I0512 23:42:16.209835 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_14/self_attention/out/kernel/m
I0512 23:42:16.209867 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_14/self_attention/out/kernel/v
I0512 23:42:16.209898 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_14/self_attention/out/kernel/v_col
I0512 23:42:16.209929 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_14/self_attention/out/kernel/v_row
I0512 23:42:16.209960 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_14/self_attention/query/kernel/m
I0512 23:42:16.209991 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_14/self_attention/query/kernel/v
I0512 23:42:16.210022 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_14/self_attention/query/kernel/v_col
I0512 23:42:16.210052 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_14/self_attention/query/kernel/v_row
I0512 23:42:16.210083 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_14/self_attention/value/kernel/m
I0512 23:42:16.210114 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_14/self_attention/value/kernel/v
I0512 23:42:16.210145 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_14/self_attention/value/kernel/v_col
I0512 23:42:16.210176 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_14/self_attention/value/kernel/v_row
I0512 23:42:16.210207 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_15/mlp/wi_0/kernel/m
I0512 23:42:16.210238 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_15/mlp/wi_0/kernel/v
I0512 23:42:16.210269 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_15/mlp/wi_0/kernel/v_col
I0512 23:42:16.210300 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_15/mlp/wi_0/kernel/v_row
I0512 23:42:16.210331 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_15/mlp/wi_1/kernel/m
I0512 23:42:16.210362 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_15/mlp/wi_1/kernel/v
I0512 23:42:16.210392 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_15/mlp/wi_1/kernel/v_col
I0512 23:42:16.210423 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_15/mlp/wi_1/kernel/v_row
I0512 23:42:16.210454 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_15/mlp/wo/kernel/m
I0512 23:42:16.210485 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_15/mlp/wo/kernel/v
I0512 23:42:16.210516 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_15/mlp/wo/kernel/v_col
I0512 23:42:16.210547 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_15/mlp/wo/kernel/v_row
I0512 23:42:16.210577 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_15/pre_mlp_layer_norm/scale/m
I0512 23:42:16.210610 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_15/pre_mlp_layer_norm/scale/v
I0512 23:42:16.210641 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_15/pre_mlp_layer_norm/scale/v_col
I0512 23:42:16.210672 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_15/pre_mlp_layer_norm/scale/v_row
I0512 23:42:16.210704 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_15/pre_self_attention_layer_norm/scale/m
I0512 23:42:16.210734 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_15/pre_self_attention_layer_norm/scale/v
I0512 23:42:16.210765 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_15/pre_self_attention_layer_norm/scale/v_col
I0512 23:42:16.210796 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_15/pre_self_attention_layer_norm/scale/v_row
I0512 23:42:16.210827 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_15/self_attention/key/kernel/m
I0512 23:42:16.210866 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_15/self_attention/key/kernel/v
I0512 23:42:16.210897 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_15/self_attention/key/kernel/v_col
I0512 23:42:16.210928 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_15/self_attention/key/kernel/v_row
I0512 23:42:16.210959 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_15/self_attention/out/kernel/m
I0512 23:42:16.210989 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_15/self_attention/out/kernel/v
I0512 23:42:16.211020 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_15/self_attention/out/kernel/v_col
I0512 23:42:16.211051 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_15/self_attention/out/kernel/v_row
I0512 23:42:16.211082 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_15/self_attention/query/kernel/m
I0512 23:42:16.211113 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_15/self_attention/query/kernel/v
I0512 23:42:16.211144 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_15/self_attention/query/kernel/v_col
I0512 23:42:16.211175 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_15/self_attention/query/kernel/v_row
I0512 23:42:16.211206 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_15/self_attention/value/kernel/m
I0512 23:42:16.211236 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_15/self_attention/value/kernel/v
I0512 23:42:16.211267 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_15/self_attention/value/kernel/v_col
I0512 23:42:16.211322 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_15/self_attention/value/kernel/v_row
I0512 23:42:16.211362 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_16/mlp/wi_0/kernel/m
I0512 23:42:16.211400 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_16/mlp/wi_0/kernel/v
I0512 23:42:16.211433 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_16/mlp/wi_0/kernel/v_col
I0512 23:42:16.211466 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_16/mlp/wi_0/kernel/v_row
I0512 23:42:16.211504 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_16/mlp/wi_1/kernel/m
I0512 23:42:16.211536 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_16/mlp/wi_1/kernel/v
I0512 23:42:16.211567 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_16/mlp/wi_1/kernel/v_col
I0512 23:42:16.211600 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_16/mlp/wi_1/kernel/v_row
I0512 23:42:16.211631 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_16/mlp/wo/kernel/m
I0512 23:42:16.211662 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_16/mlp/wo/kernel/v
I0512 23:42:16.211692 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_16/mlp/wo/kernel/v_col
I0512 23:42:16.211722 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_16/mlp/wo/kernel/v_row
I0512 23:42:16.211752 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_16/pre_mlp_layer_norm/scale/m
I0512 23:42:16.211783 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_16/pre_mlp_layer_norm/scale/v
I0512 23:42:16.211813 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_16/pre_mlp_layer_norm/scale/v_col
I0512 23:42:16.211852 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_16/pre_mlp_layer_norm/scale/v_row
I0512 23:42:16.211883 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_16/pre_self_attention_layer_norm/scale/m
I0512 23:42:16.211913 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_16/pre_self_attention_layer_norm/scale/v
I0512 23:42:16.211945 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_16/pre_self_attention_layer_norm/scale/v_col
I0512 23:42:16.211977 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_16/pre_self_attention_layer_norm/scale/v_row
I0512 23:42:16.212007 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_16/self_attention/key/kernel/m
I0512 23:42:16.212038 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_16/self_attention/key/kernel/v
I0512 23:42:16.212068 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_16/self_attention/key/kernel/v_col
I0512 23:42:16.212098 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_16/self_attention/key/kernel/v_row
I0512 23:42:16.212128 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_16/self_attention/out/kernel/m
I0512 23:42:16.212157 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_16/self_attention/out/kernel/v
I0512 23:42:16.212188 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_16/self_attention/out/kernel/v_col
I0512 23:42:16.212217 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_16/self_attention/out/kernel/v_row
I0512 23:42:16.212247 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_16/self_attention/query/kernel/m
I0512 23:42:16.212276 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_16/self_attention/query/kernel/v
I0512 23:42:16.212306 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_16/self_attention/query/kernel/v_col
I0512 23:42:16.212336 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_16/self_attention/query/kernel/v_row
I0512 23:42:16.212366 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_16/self_attention/value/kernel/m
I0512 23:42:16.212396 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_16/self_attention/value/kernel/v
I0512 23:42:16.212428 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_16/self_attention/value/kernel/v_col
I0512 23:42:16.212460 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_16/self_attention/value/kernel/v_row
I0512 23:42:16.212490 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/extra_mlp/wi_0/kernel/m
I0512 23:42:16.212519 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/extra_mlp/wi_0/kernel/v
I0512 23:42:16.212549 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/extra_mlp/wi_0/kernel/v_col
I0512 23:42:16.212579 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/extra_mlp/wi_0/kernel/v_row
I0512 23:42:16.212611 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/extra_mlp/wi_1/kernel/m
I0512 23:42:16.212642 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/extra_mlp/wi_1/kernel/v
I0512 23:42:16.212671 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/extra_mlp/wi_1/kernel/v_col
I0512 23:42:16.212701 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/extra_mlp/wi_1/kernel/v_row
I0512 23:42:16.212731 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/extra_mlp/wo/kernel/m
I0512 23:42:16.212761 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/extra_mlp/wo/kernel/v
I0512 23:42:16.212791 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/extra_mlp/wo/kernel/v_col
I0512 23:42:16.212820 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/extra_mlp/wo/kernel/v_row
I0512 23:42:16.212858 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/mlp/expert/wi_0/kernel/m
I0512 23:42:16.212889 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/mlp/expert/wi_0/kernel/v
I0512 23:42:16.212919 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/mlp/expert/wi_0/kernel/v_col
I0512 23:42:16.212949 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/mlp/expert/wi_0/kernel/v_row
I0512 23:42:16.212978 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/mlp/expert/wi_1/kernel/m
I0512 23:42:16.213008 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/mlp/expert/wi_1/kernel/v
I0512 23:42:16.213037 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/mlp/expert/wi_1/kernel/v_col
I0512 23:42:16.213068 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/mlp/expert/wi_1/kernel/v_row
I0512 23:42:16.213097 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/mlp/expert/wo/kernel/m
I0512 23:42:16.213127 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/mlp/expert/wo/kernel/v
I0512 23:42:16.213158 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/mlp/expert/wo/kernel/v_col
I0512 23:42:16.213188 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/mlp/expert/wo/kernel/v_row
I0512 23:42:16.213217 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/mlp/router/router_weights/w/kernel/m
I0512 23:42:16.213247 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/mlp/router/router_weights/w/kernel/v
I0512 23:42:16.213277 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/mlp/router/router_weights/w/kernel/v_col
I0512 23:42:16.213307 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/mlp/router/router_weights/w/kernel/v_row
I0512 23:42:16.213337 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/pre_extra_mlp_layer_norm/scale/m
I0512 23:42:16.213367 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/pre_extra_mlp_layer_norm/scale/v
I0512 23:42:16.213397 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/pre_extra_mlp_layer_norm/scale/v_col
I0512 23:42:16.213426 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/pre_extra_mlp_layer_norm/scale/v_row
I0512 23:42:16.213456 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/pre_mlp_layer_norm/scale/m
I0512 23:42:16.213486 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/pre_mlp_layer_norm/scale/v
I0512 23:42:16.213516 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/pre_mlp_layer_norm/scale/v_col
I0512 23:42:16.213546 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/pre_mlp_layer_norm/scale/v_row
I0512 23:42:16.213576 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/pre_self_attention_layer_norm/scale/m
I0512 23:42:16.213608 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/pre_self_attention_layer_norm/scale/v
I0512 23:42:16.213639 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/pre_self_attention_layer_norm/scale/v_col
I0512 23:42:16.213669 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/pre_self_attention_layer_norm/scale/v_row
I0512 23:42:16.213699 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/self_attention/key/kernel/m
I0512 23:42:16.213729 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/self_attention/key/kernel/v
I0512 23:42:16.213759 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/self_attention/key/kernel/v_col
I0512 23:42:16.213789 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/self_attention/key/kernel/v_row
I0512 23:42:16.213819 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/self_attention/out/kernel/m
I0512 23:42:16.213855 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/self_attention/out/kernel/v
I0512 23:42:16.213885 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/self_attention/out/kernel/v_col
I0512 23:42:16.213915 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/self_attention/out/kernel/v_row
I0512 23:42:16.213945 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/self_attention/query/kernel/m
I0512 23:42:16.213974 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/self_attention/query/kernel/v
I0512 23:42:16.214004 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/self_attention/query/kernel/v_col
I0512 23:42:16.214035 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/self_attention/query/kernel/v_row
I0512 23:42:16.214064 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/self_attention/value/kernel/m
I0512 23:42:16.214094 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/self_attention/value/kernel/v
I0512 23:42:16.214124 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/self_attention/value/kernel/v_col
I0512 23:42:16.214154 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/self_attention/value/kernel/v_row
I0512 23:42:16.214183 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_18/mlp/wi_0/kernel/m
I0512 23:42:16.214213 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_18/mlp/wi_0/kernel/v
I0512 23:42:16.214244 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_18/mlp/wi_0/kernel/v_col
I0512 23:42:16.214274 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_18/mlp/wi_0/kernel/v_row
I0512 23:42:16.214304 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_18/mlp/wi_1/kernel/m
I0512 23:42:16.214334 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_18/mlp/wi_1/kernel/v
I0512 23:42:16.214364 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_18/mlp/wi_1/kernel/v_col
I0512 23:42:16.214394 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_18/mlp/wi_1/kernel/v_row
I0512 23:42:16.214424 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_18/mlp/wo/kernel/m
I0512 23:42:16.214454 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_18/mlp/wo/kernel/v
I0512 23:42:16.214484 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_18/mlp/wo/kernel/v_col
I0512 23:42:16.214514 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_18/mlp/wo/kernel/v_row
I0512 23:42:16.214544 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_18/pre_mlp_layer_norm/scale/m
I0512 23:42:16.214574 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_18/pre_mlp_layer_norm/scale/v
I0512 23:42:16.214606 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_18/pre_mlp_layer_norm/scale/v_col
I0512 23:42:16.214637 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_18/pre_mlp_layer_norm/scale/v_row
I0512 23:42:16.214667 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_18/pre_self_attention_layer_norm/scale/m
I0512 23:42:16.214697 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_18/pre_self_attention_layer_norm/scale/v
I0512 23:42:16.214727 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_18/pre_self_attention_layer_norm/scale/v_col
I0512 23:42:16.214757 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_18/pre_self_attention_layer_norm/scale/v_row
I0512 23:42:16.214787 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_18/self_attention/key/kernel/m
I0512 23:42:16.214817 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_18/self_attention/key/kernel/v
I0512 23:42:16.214852 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_18/self_attention/key/kernel/v_col
I0512 23:42:16.214883 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_18/self_attention/key/kernel/v_row
I0512 23:42:16.214913 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_18/self_attention/out/kernel/m
I0512 23:42:16.214942 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_18/self_attention/out/kernel/v
I0512 23:42:16.214972 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_18/self_attention/out/kernel/v_col
I0512 23:42:16.215002 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_18/self_attention/out/kernel/v_row
I0512 23:42:16.215032 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_18/self_attention/query/kernel/m
I0512 23:42:16.215062 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_18/self_attention/query/kernel/v
I0512 23:42:16.215092 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_18/self_attention/query/kernel/v_col
I0512 23:42:16.215121 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_18/self_attention/query/kernel/v_row
I0512 23:42:16.215152 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_18/self_attention/value/kernel/m
I0512 23:42:16.215182 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_18/self_attention/value/kernel/v
I0512 23:42:16.215212 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_18/self_attention/value/kernel/v_col
I0512 23:42:16.215241 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_18/self_attention/value/kernel/v_row
I0512 23:42:16.215271 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_19/mlp/wi_0/kernel/m
I0512 23:42:16.215325 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_19/mlp/wi_0/kernel/v
I0512 23:42:16.215359 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_19/mlp/wi_0/kernel/v_col
I0512 23:42:16.215389 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_19/mlp/wi_0/kernel/v_row
I0512 23:42:16.215420 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_19/mlp/wi_1/kernel/m
I0512 23:42:16.215450 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_19/mlp/wi_1/kernel/v
I0512 23:42:16.215480 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_19/mlp/wi_1/kernel/v_col
I0512 23:42:16.215509 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_19/mlp/wi_1/kernel/v_row
I0512 23:42:16.215540 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_19/mlp/wo/kernel/m
I0512 23:42:16.215569 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_19/mlp/wo/kernel/v
I0512 23:42:16.215601 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_19/mlp/wo/kernel/v_col
I0512 23:42:16.215632 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_19/mlp/wo/kernel/v_row
I0512 23:42:16.215662 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_19/pre_mlp_layer_norm/scale/m
I0512 23:42:16.215692 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_19/pre_mlp_layer_norm/scale/v
I0512 23:42:16.215722 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_19/pre_mlp_layer_norm/scale/v_col
I0512 23:42:16.215751 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_19/pre_mlp_layer_norm/scale/v_row
I0512 23:42:16.215781 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_19/pre_self_attention_layer_norm/scale/m
I0512 23:42:16.215812 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_19/pre_self_attention_layer_norm/scale/v
I0512 23:42:16.215848 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_19/pre_self_attention_layer_norm/scale/v_col
I0512 23:42:16.215879 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_19/pre_self_attention_layer_norm/scale/v_row
I0512 23:42:16.215909 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_19/self_attention/key/kernel/m
I0512 23:42:16.215939 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_19/self_attention/key/kernel/v
I0512 23:42:16.215969 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_19/self_attention/key/kernel/v_col
I0512 23:42:16.215998 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_19/self_attention/key/kernel/v_row
I0512 23:42:16.216027 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_19/self_attention/out/kernel/m
I0512 23:42:16.216057 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_19/self_attention/out/kernel/v
I0512 23:42:16.216087 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_19/self_attention/out/kernel/v_col
I0512 23:42:16.216116 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_19/self_attention/out/kernel/v_row
I0512 23:42:16.216146 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_19/self_attention/query/kernel/m
I0512 23:42:16.216176 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_19/self_attention/query/kernel/v
I0512 23:42:16.216206 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_19/self_attention/query/kernel/v_col
I0512 23:42:16.216235 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_19/self_attention/query/kernel/v_row
I0512 23:42:16.216265 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_19/self_attention/value/kernel/m
I0512 23:42:16.216295 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_19/self_attention/value/kernel/v
I0512 23:42:16.216325 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_19/self_attention/value/kernel/v_col
I0512 23:42:16.216355 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_19/self_attention/value/kernel/v_row
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
I0000 00:00:1715557336.213845  449796 gcs_resource.cc:109] Using default AdmissionQueue with limit 32
I0512 23:42:16.216385 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_2/mlp/wi_0/kernel/m
I0512 23:42:16.216415 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_2/mlp/wi_0/kernel/v
I0512 23:42:16.216445 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_2/mlp/wi_0/kernel/v_col
I0512 23:42:16.216475 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_2/mlp/wi_0/kernel/v_row
I0512 23:42:16.216504 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_2/mlp/wi_1/kernel/m
I0512 23:42:16.216534 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_2/mlp/wi_1/kernel/v
I0512 23:42:16.216564 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_2/mlp/wi_1/kernel/v_col
I0512 23:42:16.216595 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_2/mlp/wi_1/kernel/v_row
I0512 23:42:16.216626 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_2/mlp/wo/kernel/m
I0512 23:42:16.216657 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_2/mlp/wo/kernel/v
I0512 23:42:16.216687 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_2/mlp/wo/kernel/v_col
I0512 23:42:16.216717 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_2/mlp/wo/kernel/v_row
I0512 23:42:16.216747 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_2/pre_mlp_layer_norm/scale/m
I0512 23:42:16.216777 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_2/pre_mlp_layer_norm/scale/v
I0512 23:42:16.216807 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_2/pre_mlp_layer_norm/scale/v_col
I0512 23:42:16.216842 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_2/pre_mlp_layer_norm/scale/v_row
I0512 23:42:16.216873 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_2/pre_self_attention_layer_norm/scale/m
I0512 23:42:16.216903 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_2/pre_self_attention_layer_norm/scale/v
I0512 23:42:16.216933 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_2/pre_self_attention_layer_norm/scale/v_col
I0512 23:42:16.216962 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_2/pre_self_attention_layer_norm/scale/v_row
I0512 23:42:16.216992 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_2/self_attention/key/kernel/m
I0512 23:42:16.217022 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_2/self_attention/key/kernel/v
I0512 23:42:16.217052 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_2/self_attention/key/kernel/v_col
I0512 23:42:16.217081 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_2/self_attention/key/kernel/v_row
I0512 23:42:16.217111 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_2/self_attention/out/kernel/m
I0512 23:42:16.217141 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_2/self_attention/out/kernel/v
I0512 23:42:16.217171 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_2/self_attention/out/kernel/v_col
I0512 23:42:16.217201 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_2/self_attention/out/kernel/v_row
I0512 23:42:16.217230 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_2/self_attention/query/kernel/m
I0512 23:42:16.217260 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_2/self_attention/query/kernel/v
I0512 23:42:16.217289 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_2/self_attention/query/kernel/v_col
I0512 23:42:16.217319 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_2/self_attention/query/kernel/v_row
I0512 23:42:16.217349 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_2/self_attention/value/kernel/m
I0512 23:42:16.217379 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_2/self_attention/value/kernel/v
I0512 23:42:16.217409 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_2/self_attention/value/kernel/v_col
I0512 23:42:16.217438 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_2/self_attention/value/kernel/v_row
I0512 23:42:16.217468 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_20/mlp/wi_0/kernel/m
I0512 23:42:16.217498 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_20/mlp/wi_0/kernel/v
I0512 23:42:16.217528 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_20/mlp/wi_0/kernel/v_col
I0512 23:42:16.217559 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_20/mlp/wi_0/kernel/v_row
I0512 23:42:16.217590 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_20/mlp/wi_1/kernel/m
I0512 23:42:16.217622 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_20/mlp/wi_1/kernel/v
I0512 23:42:16.217652 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_20/mlp/wi_1/kernel/v_col
I0512 23:42:16.217681 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_20/mlp/wi_1/kernel/v_row
I0512 23:42:16.217711 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_20/mlp/wo/kernel/m
I0512 23:42:16.217741 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_20/mlp/wo/kernel/v
I0512 23:42:16.217771 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_20/mlp/wo/kernel/v_col
I0512 23:42:16.217801 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_20/mlp/wo/kernel/v_row
I0512 23:42:16.217836 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_20/pre_mlp_layer_norm/scale/m
I0512 23:42:16.217867 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_20/pre_mlp_layer_norm/scale/v
I0512 23:42:16.217897 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_20/pre_mlp_layer_norm/scale/v_col
I0512 23:42:16.217927 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_20/pre_mlp_layer_norm/scale/v_row
I0512 23:42:16.217957 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_20/pre_self_attention_layer_norm/scale/m
I0512 23:42:16.217987 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_20/pre_self_attention_layer_norm/scale/v
I0512 23:42:16.218016 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_20/pre_self_attention_layer_norm/scale/v_col
I0512 23:42:16.218046 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_20/pre_self_attention_layer_norm/scale/v_row
I0512 23:42:16.218076 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_20/self_attention/key/kernel/m
I0512 23:42:16.218106 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_20/self_attention/key/kernel/v
I0512 23:42:16.218136 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_20/self_attention/key/kernel/v_col
I0512 23:42:16.218166 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_20/self_attention/key/kernel/v_row
I0512 23:42:16.218195 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_20/self_attention/out/kernel/m
I0512 23:42:16.218225 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_20/self_attention/out/kernel/v
I0512 23:42:16.218255 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_20/self_attention/out/kernel/v_col
I0512 23:42:16.218284 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_20/self_attention/out/kernel/v_row
I0512 23:42:16.218314 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_20/self_attention/query/kernel/m
I0512 23:42:16.218344 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_20/self_attention/query/kernel/v
I0512 23:42:16.218374 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_20/self_attention/query/kernel/v_col
I0512 23:42:16.218403 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_20/self_attention/query/kernel/v_row
I0512 23:42:16.218434 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_20/self_attention/value/kernel/m
I0512 23:42:16.218463 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_20/self_attention/value/kernel/v
I0512 23:42:16.218493 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_20/self_attention/value/kernel/v_col
I0512 23:42:16.218522 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_20/self_attention/value/kernel/v_row
I0512 23:42:16.218552 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_21/mlp/wi_0/kernel/m
I0512 23:42:16.218582 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_21/mlp/wi_0/kernel/v
I0512 23:42:16.218615 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_21/mlp/wi_0/kernel/v_col
I0512 23:42:16.218645 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_21/mlp/wi_0/kernel/v_row
I0512 23:42:16.218675 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_21/mlp/wi_1/kernel/m
I0512 23:42:16.218705 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_21/mlp/wi_1/kernel/v
I0512 23:42:16.218735 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_21/mlp/wi_1/kernel/v_col
I0512 23:42:16.218765 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_21/mlp/wi_1/kernel/v_row
I0512 23:42:16.218795 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_21/mlp/wo/kernel/m
I0512 23:42:16.218825 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_21/mlp/wo/kernel/v
I0512 23:42:16.218864 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_21/mlp/wo/kernel/v_col
I0512 23:42:16.218895 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_21/mlp/wo/kernel/v_row
I0512 23:42:16.218925 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_21/pre_mlp_layer_norm/scale/m
I0512 23:42:16.218955 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_21/pre_mlp_layer_norm/scale/v
I0512 23:42:16.218985 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_21/pre_mlp_layer_norm/scale/v_col
I0512 23:42:16.219015 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_21/pre_mlp_layer_norm/scale/v_row
I0512 23:42:16.219045 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_21/pre_self_attention_layer_norm/scale/m
I0512 23:42:16.219074 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_21/pre_self_attention_layer_norm/scale/v
I0512 23:42:16.219105 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_21/pre_self_attention_layer_norm/scale/v_col
I0512 23:42:16.219135 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_21/pre_self_attention_layer_norm/scale/v_row
I0512 23:42:16.219164 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_21/self_attention/key/kernel/m
I0512 23:42:16.219195 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_21/self_attention/key/kernel/v
I0512 23:42:16.219225 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_21/self_attention/key/kernel/v_col
I0512 23:42:16.219254 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_21/self_attention/key/kernel/v_row
I0512 23:42:16.219285 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_21/self_attention/out/kernel/m
I0512 23:42:16.219341 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_21/self_attention/out/kernel/v
I0512 23:42:16.219372 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_21/self_attention/out/kernel/v_col
I0512 23:42:16.219403 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_21/self_attention/out/kernel/v_row
I0512 23:42:16.219432 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_21/self_attention/query/kernel/m
I0512 23:42:16.219462 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_21/self_attention/query/kernel/v
I0512 23:42:16.219492 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_21/self_attention/query/kernel/v_col
I0512 23:42:16.219522 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_21/self_attention/query/kernel/v_row
I0512 23:42:16.219552 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_21/self_attention/value/kernel/m
I0512 23:42:16.219582 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_21/self_attention/value/kernel/v
I0512 23:42:16.219614 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_21/self_attention/value/kernel/v_col
I0512 23:42:16.219645 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_21/self_attention/value/kernel/v_row
I0512 23:42:16.219675 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_22/mlp/wi_0/kernel/m
I0512 23:42:16.219705 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_22/mlp/wi_0/kernel/v
I0512 23:42:16.219736 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_22/mlp/wi_0/kernel/v_col
I0512 23:42:16.219766 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_22/mlp/wi_0/kernel/v_row
I0512 23:42:16.219796 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_22/mlp/wi_1/kernel/m
I0512 23:42:16.219826 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_22/mlp/wi_1/kernel/v
I0512 23:42:16.219862 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_22/mlp/wi_1/kernel/v_col
I0512 23:42:16.219893 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_22/mlp/wi_1/kernel/v_row
I0512 23:42:16.219923 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_22/mlp/wo/kernel/m
I0512 23:42:16.219953 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_22/mlp/wo/kernel/v
I0512 23:42:16.219983 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_22/mlp/wo/kernel/v_col
I0512 23:42:16.220013 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_22/mlp/wo/kernel/v_row
I0512 23:42:16.220043 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_22/pre_mlp_layer_norm/scale/m
I0512 23:42:16.220073 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_22/pre_mlp_layer_norm/scale/v
I0512 23:42:16.220103 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_22/pre_mlp_layer_norm/scale/v_col
I0512 23:42:16.220133 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_22/pre_mlp_layer_norm/scale/v_row
I0512 23:42:16.220163 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_22/pre_self_attention_layer_norm/scale/m
I0512 23:42:16.220193 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_22/pre_self_attention_layer_norm/scale/v
I0512 23:42:16.220223 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_22/pre_self_attention_layer_norm/scale/v_col
I0512 23:42:16.220253 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_22/pre_self_attention_layer_norm/scale/v_row
I0512 23:42:16.220283 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_22/self_attention/key/kernel/m
I0512 23:42:16.220313 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_22/self_attention/key/kernel/v
I0512 23:42:16.220343 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_22/self_attention/key/kernel/v_col
I0512 23:42:16.220372 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_22/self_attention/key/kernel/v_row
I0512 23:42:16.220402 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_22/self_attention/out/kernel/m
I0512 23:42:16.220432 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_22/self_attention/out/kernel/v
I0512 23:42:16.220462 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_22/self_attention/out/kernel/v_col
I0512 23:42:16.220492 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_22/self_attention/out/kernel/v_row
I0512 23:42:16.220521 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_22/self_attention/query/kernel/m
I0512 23:42:16.220551 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_22/self_attention/query/kernel/v
I0512 23:42:16.220581 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_22/self_attention/query/kernel/v_col
I0512 23:42:16.220613 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_22/self_attention/query/kernel/v_row
I0512 23:42:16.220643 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_22/self_attention/value/kernel/m
I0512 23:42:16.220674 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_22/self_attention/value/kernel/v
I0512 23:42:16.220704 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_22/self_attention/value/kernel/v_col
I0512 23:42:16.220733 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_22/self_attention/value/kernel/v_row
I0512 23:42:16.220763 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/extra_mlp/wi_0/kernel/m
I0512 23:42:16.220793 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/extra_mlp/wi_0/kernel/v
I0512 23:42:16.220823 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/extra_mlp/wi_0/kernel/v_col
I0000 00:00:1715557336.218604  451959 google_auth_provider.cc:180] Running on GCE, using service account 1081612546678-compute@developer.gserviceaccount.com
I0512 23:42:16.220859 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/extra_mlp/wi_0/kernel/v_row
I0512 23:42:16.220889 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/extra_mlp/wi_1/kernel/m
I0512 23:42:16.220919 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/extra_mlp/wi_1/kernel/v
I0512 23:42:16.220948 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/extra_mlp/wi_1/kernel/v_col
I0512 23:42:16.220978 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/extra_mlp/wi_1/kernel/v_row
I0512 23:42:16.221008 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/extra_mlp/wo/kernel/m
I0512 23:42:16.221038 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/extra_mlp/wo/kernel/v
I0512 23:42:16.221068 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/extra_mlp/wo/kernel/v_col
I0512 23:42:16.221098 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/extra_mlp/wo/kernel/v_row
I0512 23:42:16.221128 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/mlp/expert/wi_0/kernel/m
I0512 23:42:16.221157 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/mlp/expert/wi_0/kernel/v
I0512 23:42:16.221187 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/mlp/expert/wi_0/kernel/v_col
I0512 23:42:16.221217 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/mlp/expert/wi_0/kernel/v_row
I0512 23:42:16.221247 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/mlp/expert/wi_1/kernel/m
I0512 23:42:16.221277 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/mlp/expert/wi_1/kernel/v
I0512 23:42:16.221307 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/mlp/expert/wi_1/kernel/v_col
I0512 23:42:16.221337 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/mlp/expert/wi_1/kernel/v_row
I0512 23:42:16.221367 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/mlp/expert/wo/kernel/m
I0512 23:42:16.221396 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/mlp/expert/wo/kernel/v
I0512 23:42:16.221426 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/mlp/expert/wo/kernel/v_col
I0512 23:42:16.221456 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/mlp/expert/wo/kernel/v_row
I0512 23:42:16.221486 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/mlp/router/router_weights/w/kernel/m
I0512 23:42:16.221516 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/mlp/router/router_weights/w/kernel/v
I0512 23:42:16.221545 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/mlp/router/router_weights/w/kernel/v_col
I0512 23:42:16.221575 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/mlp/router/router_weights/w/kernel/v_row
I0512 23:42:16.221607 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/pre_extra_mlp_layer_norm/scale/m
I0512 23:42:16.221638 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/pre_extra_mlp_layer_norm/scale/v
I0512 23:42:16.221668 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/pre_extra_mlp_layer_norm/scale/v_col
I0512 23:42:16.221697 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/pre_extra_mlp_layer_norm/scale/v_row
I0512 23:42:16.221727 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/pre_mlp_layer_norm/scale/m
I0512 23:42:16.221757 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/pre_mlp_layer_norm/scale/v
I0512 23:42:16.221787 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/pre_mlp_layer_norm/scale/v_col
I0512 23:42:16.221816 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/pre_mlp_layer_norm/scale/v_row
I0512 23:42:16.221852 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/pre_self_attention_layer_norm/scale/m
I0512 23:42:16.221883 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/pre_self_attention_layer_norm/scale/v
I0512 23:42:16.221913 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/pre_self_attention_layer_norm/scale/v_col
I0512 23:42:16.221943 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/pre_self_attention_layer_norm/scale/v_row
I0512 23:42:16.221972 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/self_attention/key/kernel/m
I0512 23:42:16.222002 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/self_attention/key/kernel/v
I0512 23:42:16.222032 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/self_attention/key/kernel/v_col
I0512 23:42:16.222062 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/self_attention/key/kernel/v_row
I0512 23:42:16.222091 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/self_attention/out/kernel/m
I0512 23:42:16.222121 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/self_attention/out/kernel/v
I0512 23:42:16.222151 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/self_attention/out/kernel/v_col
I0512 23:42:16.222180 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/self_attention/out/kernel/v_row
I0512 23:42:16.222236 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/self_attention/query/kernel/m
I0512 23:42:16.222267 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/self_attention/query/kernel/v
I0512 23:42:16.222297 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/self_attention/query/kernel/v_col
I0512 23:42:16.222327 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/self_attention/query/kernel/v_row
I0512 23:42:16.222357 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/self_attention/value/kernel/m
I0512 23:42:16.222387 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/self_attention/value/kernel/v
I0512 23:42:16.222417 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/self_attention/value/kernel/v_col
I0512 23:42:16.222446 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/self_attention/value/kernel/v_row
I0512 23:42:16.222476 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_3/mlp/wi_0/kernel/m
I0512 23:42:16.222506 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_3/mlp/wi_0/kernel/v
I0512 23:42:16.222537 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_3/mlp/wi_0/kernel/v_col
I0512 23:42:16.222566 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_3/mlp/wi_0/kernel/v_row
I0512 23:42:16.222599 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_3/mlp/wi_1/kernel/m
I0512 23:42:16.222630 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_3/mlp/wi_1/kernel/v
I0512 23:42:16.222660 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_3/mlp/wi_1/kernel/v_col
I0512 23:42:16.222690 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_3/mlp/wi_1/kernel/v_row
I0512 23:42:16.222720 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_3/mlp/wo/kernel/m
I0512 23:42:16.222750 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_3/mlp/wo/kernel/v
I0512 23:42:16.222780 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_3/mlp/wo/kernel/v_col
I0512 23:42:16.222810 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_3/mlp/wo/kernel/v_row
I0512 23:42:16.222845 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_3/pre_mlp_layer_norm/scale/m
I0512 23:42:16.222876 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_3/pre_mlp_layer_norm/scale/v
I0512 23:42:16.222906 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_3/pre_mlp_layer_norm/scale/v_col
I0512 23:42:16.222936 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_3/pre_mlp_layer_norm/scale/v_row
I0512 23:42:16.222966 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_3/pre_self_attention_layer_norm/scale/m
I0512 23:42:16.222996 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_3/pre_self_attention_layer_norm/scale/v
I0512 23:42:16.221778 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/decoder_norm/scale/m
I0512 23:42:16.221934 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/decoder_norm/scale/v
I0512 23:42:16.221977 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/decoder_norm/scale/v_col
I0512 23:42:16.223026 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_3/pre_self_attention_layer_norm/scale/v_col
I0512 23:42:16.223055 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_3/pre_self_attention_layer_norm/scale/v_row
I0512 23:42:16.223085 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_3/self_attention/key/kernel/m
I0512 23:42:16.223115 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_3/self_attention/key/kernel/v
I0512 23:42:16.223145 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_3/self_attention/key/kernel/v_col
I0512 23:42:16.223175 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_3/self_attention/key/kernel/v_row
I0512 23:42:16.223204 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_3/self_attention/out/kernel/m
I0512 23:42:16.223234 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_3/self_attention/out/kernel/v
I0512 23:42:16.223264 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_3/self_attention/out/kernel/v_col
I0512 23:42:16.223318 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_3/self_attention/out/kernel/v_row
I0512 23:42:16.223354 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_3/self_attention/query/kernel/m
I0512 23:42:16.223384 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_3/self_attention/query/kernel/v
I0512 23:42:16.223415 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_3/self_attention/query/kernel/v_col
I0512 23:42:16.222014 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/decoder_norm/scale/v_row
I0512 23:42:16.222050 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_0/mlp/wi_0/kernel/m
I0512 23:42:16.222085 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_0/mlp/wi_0/kernel/v
I0512 23:42:16.222121 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_0/mlp/wi_0/kernel/v_col
I0512 23:42:16.222163 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_0/mlp/wi_0/kernel/v_row
I0512 23:42:16.222199 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_0/mlp/wi_1/kernel/m
I0512 23:42:16.222235 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_0/mlp/wi_1/kernel/v
I0512 23:42:16.222270 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_0/mlp/wi_1/kernel/v_col
I0512 23:42:16.222305 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_0/mlp/wi_1/kernel/v_row
I0512 23:42:16.222340 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_0/mlp/wo/kernel/m
I0512 23:42:16.223444 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_3/self_attention/query/kernel/v_row
I0512 23:42:16.223474 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_3/self_attention/value/kernel/m
I0512 23:42:16.223504 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_3/self_attention/value/kernel/v
I0512 23:42:16.223533 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_3/self_attention/value/kernel/v_col
I0512 23:42:16.223563 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_3/self_attention/value/kernel/v_row
I0512 23:42:16.223594 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_4/mlp/wi_0/kernel/m
I0512 23:42:16.223626 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_4/mlp/wi_0/kernel/v
I0512 23:42:16.223656 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_4/mlp/wi_0/kernel/v_col
I0512 23:42:16.223687 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_4/mlp/wi_0/kernel/v_row
I0512 23:42:16.223717 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_4/mlp/wi_1/kernel/m
I0512 23:42:16.223747 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_4/mlp/wi_1/kernel/v
I0512 23:42:16.223776 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_4/mlp/wi_1/kernel/v_col
I0512 23:42:16.223806 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_4/mlp/wi_1/kernel/v_row
I0512 23:42:16.223841 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_4/mlp/wo/kernel/m
I0512 23:42:16.222376 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_0/mlp/wo/kernel/v
I0512 23:42:16.222411 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_0/mlp/wo/kernel/v_col
I0512 23:42:16.222447 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_0/mlp/wo/kernel/v_row
I0512 23:42:16.222482 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_0/pre_mlp_layer_norm/scale/m
I0512 23:42:16.222517 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_0/pre_mlp_layer_norm/scale/v
I0512 23:42:16.222552 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_0/pre_mlp_layer_norm/scale/v_col
I0512 23:42:16.222587 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_0/pre_mlp_layer_norm/scale/v_row
I0512 23:42:16.222622 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_0/pre_self_attention_layer_norm/scale/m
I0512 23:42:16.222658 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_0/pre_self_attention_layer_norm/scale/v
I0512 23:42:16.222693 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_0/pre_self_attention_layer_norm/scale/v_col
I0512 23:42:16.223873 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_4/mlp/wo/kernel/v
I0512 23:42:16.223903 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_4/mlp/wo/kernel/v_col
I0512 23:42:16.223933 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_4/mlp/wo/kernel/v_row
I0512 23:42:16.223963 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_4/pre_mlp_layer_norm/scale/m
I0512 23:42:16.223993 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_4/pre_mlp_layer_norm/scale/v
I0512 23:42:16.224023 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_4/pre_mlp_layer_norm/scale/v_col
I0512 23:42:16.224053 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_4/pre_mlp_layer_norm/scale/v_row
I0512 23:42:16.224083 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_4/pre_self_attention_layer_norm/scale/m
I0512 23:42:16.224113 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_4/pre_self_attention_layer_norm/scale/v
I0512 23:42:16.224143 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_4/pre_self_attention_layer_norm/scale/v_col
I0512 23:42:16.224173 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_4/pre_self_attention_layer_norm/scale/v_row
I0512 23:42:16.224203 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_4/self_attention/key/kernel/m
I0512 23:42:16.224233 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_4/self_attention/key/kernel/v
I0512 23:42:16.224263 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_4/self_attention/key/kernel/v_col
I0512 23:42:16.222728 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_0/pre_self_attention_layer_norm/scale/v_row
I0512 23:42:16.222764 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_0/self_attention/key/kernel/m
I0512 23:42:16.222800 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_0/self_attention/key/kernel/v
I0512 23:42:16.222835 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_0/self_attention/key/kernel/v_col
I0512 23:42:16.222870 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_0/self_attention/key/kernel/v_row
I0512 23:42:16.222908 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_0/self_attention/out/kernel/m
I0512 23:42:16.222943 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_0/self_attention/out/kernel/v
I0512 23:42:16.222979 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_0/self_attention/out/kernel/v_col
I0512 23:42:16.223014 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_0/self_attention/out/kernel/v_row
I0512 23:42:16.223048 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_0/self_attention/query/kernel/m
I0512 23:42:16.224292 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_4/self_attention/key/kernel/v_row
I0512 23:42:16.224322 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_4/self_attention/out/kernel/m
I0512 23:42:16.224352 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_4/self_attention/out/kernel/v
I0512 23:42:16.224382 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_4/self_attention/out/kernel/v_col
I0512 23:42:16.224412 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_4/self_attention/out/kernel/v_row
I0512 23:42:16.224442 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_4/self_attention/query/kernel/m
I0512 23:42:16.224472 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_4/self_attention/query/kernel/v
I0512 23:42:16.224502 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_4/self_attention/query/kernel/v_col
I0512 23:42:16.224531 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_4/self_attention/query/kernel/v_row
I0512 23:42:16.224561 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_4/self_attention/value/kernel/m
I0512 23:42:16.224592 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_4/self_attention/value/kernel/v
I0512 23:42:16.224623 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_4/self_attention/value/kernel/v_col
I0512 23:42:16.224653 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_4/self_attention/value/kernel/v_row
I0512 23:42:16.224683 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/extra_mlp/wi_0/kernel/m
I0512 23:42:16.223083 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_0/self_attention/query/kernel/v
I0512 23:42:16.223119 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_0/self_attention/query/kernel/v_col
I0512 23:42:16.223160 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_0/self_attention/query/kernel/v_row
I0512 23:42:16.223196 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_0/self_attention/value/kernel/m
I0512 23:42:16.223232 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_0/self_attention/value/kernel/v
I0512 23:42:16.223267 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_0/self_attention/value/kernel/v_col
I0512 23:42:16.223302 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_0/self_attention/value/kernel/v_row
I0512 23:42:16.223337 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_1/mlp/wi_0/kernel/m
I0512 23:42:16.223373 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_1/mlp/wi_0/kernel/v
I0512 23:42:16.223408 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_1/mlp/wi_0/kernel/v_col
I0512 23:42:16.223443 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_1/mlp/wi_0/kernel/v_row
I0512 23:42:16.223479 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_1/mlp/wi_1/kernel/m
I0512 23:42:16.223514 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_1/mlp/wi_1/kernel/v
I0512 23:42:16.223549 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_1/mlp/wi_1/kernel/v_col
I0512 23:42:16.223585 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_1/mlp/wi_1/kernel/v_row
I0512 23:42:16.223620 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_1/mlp/wo/kernel/m
I0512 23:42:16.223655 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_1/mlp/wo/kernel/v
I0512 23:42:16.223690 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_1/mlp/wo/kernel/v_col
I0512 23:42:16.223726 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_1/mlp/wo/kernel/v_row
I0512 23:42:16.223761 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_1/pre_mlp_layer_norm/scale/m
I0512 23:42:16.223796 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_1/pre_mlp_layer_norm/scale/v
I0512 23:42:16.223831 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_1/pre_mlp_layer_norm/scale/v_col
I0512 23:42:16.223866 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_1/pre_mlp_layer_norm/scale/v_row
I0512 23:42:16.224713 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/extra_mlp/wi_0/kernel/v
I0512 23:42:16.224743 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/extra_mlp/wi_0/kernel/v_col
I0512 23:42:16.224773 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/extra_mlp/wi_0/kernel/v_row
I0512 23:42:16.224802 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/extra_mlp/wi_1/kernel/m
I0512 23:42:16.224838 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/extra_mlp/wi_1/kernel/v
I0512 23:42:16.224869 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/extra_mlp/wi_1/kernel/v_col
I0512 23:42:16.224899 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/extra_mlp/wi_1/kernel/v_row
I0512 23:42:16.224928 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/extra_mlp/wo/kernel/m
I0512 23:42:16.224958 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/extra_mlp/wo/kernel/v
I0512 23:42:16.224988 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/extra_mlp/wo/kernel/v_col
I0512 23:42:16.225018 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/extra_mlp/wo/kernel/v_row
I0512 23:42:16.225047 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/mlp/expert/wi_0/kernel/m
I0512 23:42:16.225077 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/mlp/expert/wi_0/kernel/v
I0512 23:42:16.225106 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/mlp/expert/wi_0/kernel/v_col
I0512 23:42:16.225136 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/mlp/expert/wi_0/kernel/v_row
I0512 23:42:16.223903 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_1/pre_self_attention_layer_norm/scale/m
I0512 23:42:16.223940 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_1/pre_self_attention_layer_norm/scale/v
I0512 23:42:16.223975 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_1/pre_self_attention_layer_norm/scale/v_col
I0512 23:42:16.224011 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_1/pre_self_attention_layer_norm/scale/v_row
I0512 23:42:16.224046 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_1/self_attention/key/kernel/m
I0512 23:42:16.224081 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_1/self_attention/key/kernel/v
I0512 23:42:16.224117 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_1/self_attention/key/kernel/v_col
I0512 23:42:16.224158 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_1/self_attention/key/kernel/v_row
I0512 23:42:16.224193 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_1/self_attention/out/kernel/m
I0512 23:42:16.224229 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_1/self_attention/out/kernel/v
I0512 23:42:16.224264 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_1/self_attention/out/kernel/v_col
I0512 23:42:16.224300 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_1/self_attention/out/kernel/v_row
I0512 23:42:16.225166 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/mlp/expert/wi_1/kernel/m
I0512 23:42:16.225196 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/mlp/expert/wi_1/kernel/v
I0512 23:42:16.225225 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/mlp/expert/wi_1/kernel/v_col
I0512 23:42:16.225255 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/mlp/expert/wi_1/kernel/v_row
I0512 23:42:16.225285 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/mlp/expert/wo/kernel/m
I0512 23:42:16.225315 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/mlp/expert/wo/kernel/v
I0512 23:42:16.225345 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/mlp/expert/wo/kernel/v_col
I0512 23:42:16.225375 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/mlp/expert/wo/kernel/v_row
I0512 23:42:16.225404 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/mlp/router/router_weights/w/kernel/m
I0512 23:42:16.225434 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/mlp/router/router_weights/w/kernel/v
I0512 23:42:16.225464 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/mlp/router/router_weights/w/kernel/v_col
I0512 23:42:16.225494 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/mlp/router/router_weights/w/kernel/v_row
I0512 23:42:16.225524 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/pre_extra_mlp_layer_norm/scale/m
I0512 23:42:16.225554 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/pre_extra_mlp_layer_norm/scale/v
I0512 23:42:16.225585 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/pre_extra_mlp_layer_norm/scale/v_col
I0512 23:42:16.225617 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/pre_extra_mlp_layer_norm/scale/v_row
I0512 23:42:16.225647 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/pre_mlp_layer_norm/scale/m
I0512 23:42:16.225677 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/pre_mlp_layer_norm/scale/v
I0512 23:42:16.225707 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/pre_mlp_layer_norm/scale/v_col
I0512 23:42:16.225737 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/pre_mlp_layer_norm/scale/v_row
I0512 23:42:16.225767 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/pre_self_attention_layer_norm/scale/m
I0512 23:42:16.225797 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/pre_self_attention_layer_norm/scale/v
I0512 23:42:16.225827 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/pre_self_attention_layer_norm/scale/v_col
I0512 23:42:16.225863 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/pre_self_attention_layer_norm/scale/v_row
I0512 23:42:16.225894 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/self_attention/key/kernel/m
I0512 23:42:16.225924 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/self_attention/key/kernel/v
I0512 23:42:16.225954 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/self_attention/key/kernel/v_col
I0512 23:42:16.225984 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/self_attention/key/kernel/v_row
I0512 23:42:16.226013 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/self_attention/out/kernel/m
I0512 23:42:16.224335 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_1/self_attention/query/kernel/m
I0512 23:42:16.224370 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_1/self_attention/query/kernel/v
I0512 23:42:16.224433 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_1/self_attention/query/kernel/v_col
I0512 23:42:16.224475 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_1/self_attention/query/kernel/v_row
I0512 23:42:16.224511 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_1/self_attention/value/kernel/m
I0512 23:42:16.224546 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_1/self_attention/value/kernel/v
I0512 23:42:16.224582 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_1/self_attention/value/kernel/v_col
I0512 23:42:16.224617 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_1/self_attention/value/kernel/v_row
I0512 23:42:16.224652 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_10/mlp/wi_0/kernel/m
I0512 23:42:16.224687 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_10/mlp/wi_0/kernel/v
I0512 23:42:16.224722 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_10/mlp/wi_0/kernel/v_col
I0512 23:42:16.224758 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_10/mlp/wi_0/kernel/v_row
I0512 23:42:16.224793 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_10/mlp/wi_1/kernel/m
I0512 23:42:16.226043 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/self_attention/out/kernel/v
I0512 23:42:16.226073 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/self_attention/out/kernel/v_col
I0512 23:42:16.226103 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/self_attention/out/kernel/v_row
I0512 23:42:16.226133 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/self_attention/query/kernel/m
I0512 23:42:16.226163 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/self_attention/query/kernel/v
I0512 23:42:16.226193 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/self_attention/query/kernel/v_col
I0512 23:42:16.226223 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/self_attention/query/kernel/v_row
I0512 23:42:16.226253 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/self_attention/value/kernel/m
I0512 23:42:16.226282 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/self_attention/value/kernel/v
I0512 23:42:16.226312 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/self_attention/value/kernel/v_col
I0512 23:42:16.226342 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/self_attention/value/kernel/v_row
I0512 23:42:16.226372 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_6/mlp/wi_0/kernel/m
I0512 23:42:16.226402 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_6/mlp/wi_0/kernel/v
I0512 23:42:16.226432 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_6/mlp/wi_0/kernel/v_col
I0512 23:42:16.226462 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_6/mlp/wi_0/kernel/v_row
I0512 23:42:16.224828 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_10/mlp/wi_1/kernel/v
I0512 23:42:16.224863 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_10/mlp/wi_1/kernel/v_col
I0512 23:42:16.224900 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_10/mlp/wi_1/kernel/v_row
I0512 23:42:16.224936 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_10/mlp/wo/kernel/m
I0512 23:42:16.224971 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_10/mlp/wo/kernel/v
I0512 23:42:16.225005 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_10/mlp/wo/kernel/v_col
I0512 23:42:16.225040 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_10/mlp/wo/kernel/v_row
I0512 23:42:16.225076 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_10/pre_mlp_layer_norm/scale/m
I0512 23:42:16.225111 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_10/pre_mlp_layer_norm/scale/v
I0512 23:42:16.225151 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_10/pre_mlp_layer_norm/scale/v_col
I0512 23:42:16.225187 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_10/pre_mlp_layer_norm/scale/v_row
I0512 23:42:16.225222 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_10/pre_self_attention_layer_norm/scale/m
I0512 23:42:16.225258 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_10/pre_self_attention_layer_norm/scale/v
I0512 23:42:16.225293 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_10/pre_self_attention_layer_norm/scale/v_col
I0512 23:42:16.226492 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_6/mlp/wi_1/kernel/m
I0512 23:42:16.226522 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_6/mlp/wi_1/kernel/v
I0512 23:42:16.226552 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_6/mlp/wi_1/kernel/v_col
I0512 23:42:16.226582 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_6/mlp/wi_1/kernel/v_row
I0512 23:42:16.226614 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_6/mlp/wo/kernel/m
I0512 23:42:16.226644 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_6/mlp/wo/kernel/v
I0512 23:42:16.226674 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_6/mlp/wo/kernel/v_col
I0512 23:42:16.226704 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_6/mlp/wo/kernel/v_row
I0512 23:42:16.226734 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_6/pre_mlp_layer_norm/scale/m
I0512 23:42:16.226764 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_6/pre_mlp_layer_norm/scale/v
I0512 23:42:16.226794 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_6/pre_mlp_layer_norm/scale/v_col
I0512 23:42:16.226824 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_6/pre_mlp_layer_norm/scale/v_row
I0512 23:42:16.226860 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_6/pre_self_attention_layer_norm/scale/m
I0512 23:42:16.226891 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_6/pre_self_attention_layer_norm/scale/v
I0512 23:42:16.226921 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_6/pre_self_attention_layer_norm/scale/v_col
I0512 23:42:16.225328 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_10/pre_self_attention_layer_norm/scale/v_row
I0512 23:42:16.225363 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_10/self_attention/key/kernel/m
I0512 23:42:16.225398 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_10/self_attention/key/kernel/v
I0512 23:42:16.225434 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_10/self_attention/key/kernel/v_col
I0512 23:42:16.225469 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_10/self_attention/key/kernel/v_row
I0512 23:42:16.225504 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_10/self_attention/out/kernel/m
I0512 23:42:16.225539 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_10/self_attention/out/kernel/v
I0512 23:42:16.225574 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_10/self_attention/out/kernel/v_col
I0512 23:42:16.225609 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_10/self_attention/out/kernel/v_row
I0512 23:42:16.225644 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_10/self_attention/query/kernel/m
I0512 23:42:16.225679 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_10/self_attention/query/kernel/v
I0512 23:42:16.225714 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_10/self_attention/query/kernel/v_col
I0512 23:42:16.225749 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_10/self_attention/query/kernel/v_row
I0512 23:42:16.225784 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_10/self_attention/value/kernel/m
I0512 23:42:16.226951 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_6/pre_self_attention_layer_norm/scale/v_row
I0512 23:42:16.226981 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_6/self_attention/key/kernel/m
I0512 23:42:16.227010 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_6/self_attention/key/kernel/v
I0512 23:42:16.227041 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_6/self_attention/key/kernel/v_col
I0512 23:42:16.227070 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_6/self_attention/key/kernel/v_row
I0512 23:42:16.227100 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_6/self_attention/out/kernel/m
I0512 23:42:16.227130 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_6/self_attention/out/kernel/v
I0512 23:42:16.227159 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_6/self_attention/out/kernel/v_col
I0512 23:42:16.227189 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_6/self_attention/out/kernel/v_row
I0512 23:42:16.227219 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_6/self_attention/query/kernel/m
I0512 23:42:16.227248 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_6/self_attention/query/kernel/v
I0512 23:42:16.227278 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_6/self_attention/query/kernel/v_col
I0512 23:42:16.227330 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_6/self_attention/query/kernel/v_row
I0512 23:42:16.227363 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_6/self_attention/value/kernel/m
I0512 23:42:16.225819 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_10/self_attention/value/kernel/v
I0512 23:42:16.225854 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_10/self_attention/value/kernel/v_col
I0512 23:42:16.225891 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_10/self_attention/value/kernel/v_row
I0512 23:42:16.225927 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/extra_mlp/wi_0/kernel/m
I0512 23:42:16.225963 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/extra_mlp/wi_0/kernel/v
I0512 23:42:16.225998 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/extra_mlp/wi_0/kernel/v_col
I0512 23:42:16.226033 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/extra_mlp/wi_0/kernel/v_row
I0512 23:42:16.226068 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/extra_mlp/wi_1/kernel/m
I0512 23:42:16.226103 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/extra_mlp/wi_1/kernel/v
I0512 23:42:16.226144 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/extra_mlp/wi_1/kernel/v_col
I0512 23:42:16.226180 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/extra_mlp/wi_1/kernel/v_row
I0512 23:42:16.226215 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/extra_mlp/wo/kernel/m
I0512 23:42:16.226250 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/extra_mlp/wo/kernel/v
I0512 23:42:16.226285 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/extra_mlp/wo/kernel/v_col
I0512 23:42:16.227393 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_6/self_attention/value/kernel/v
I0512 23:42:16.227423 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_6/self_attention/value/kernel/v_col
I0512 23:42:16.227453 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_6/self_attention/value/kernel/v_row
I0512 23:42:16.227483 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_7/mlp/wi_0/kernel/m
I0512 23:42:16.227513 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_7/mlp/wi_0/kernel/v
I0512 23:42:16.227543 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_7/mlp/wi_0/kernel/v_col
I0512 23:42:16.227573 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_7/mlp/wi_0/kernel/v_row
I0512 23:42:16.227605 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_7/mlp/wi_1/kernel/m
I0512 23:42:16.227636 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_7/mlp/wi_1/kernel/v
I0512 23:42:16.227665 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_7/mlp/wi_1/kernel/v_col
I0512 23:42:16.227695 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_7/mlp/wi_1/kernel/v_row
I0512 23:42:16.227725 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_7/mlp/wo/kernel/m
I0512 23:42:16.227755 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_7/mlp/wo/kernel/v
I0512 23:42:16.227785 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_7/mlp/wo/kernel/v_col
I0512 23:42:16.227815 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_7/mlp/wo/kernel/v_row
I0512 23:42:16.227851 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_7/pre_mlp_layer_norm/scale/m
I0512 23:42:16.206591 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/decoder_norm/scale/m
I0512 23:42:16.206768 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/decoder_norm/scale/v
I0512 23:42:16.206807 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/decoder_norm/scale/v_col
I0512 23:42:16.206844 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/decoder_norm/scale/v_row
I0512 23:42:16.206877 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_0/mlp/wi_0/kernel/m
I0512 23:42:16.226321 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/extra_mlp/wo/kernel/v_row
I0512 23:42:16.226356 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/mlp/expert/wi_0/kernel/m
I0512 23:42:16.226391 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/mlp/expert/wi_0/kernel/v
I0512 23:42:16.226426 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/mlp/expert/wi_0/kernel/v_col
I0512 23:42:16.226461 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/mlp/expert/wi_0/kernel/v_row
I0512 23:42:16.226496 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/mlp/expert/wi_1/kernel/m
I0512 23:42:16.226531 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/mlp/expert/wi_1/kernel/v
I0512 23:42:16.226566 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/mlp/expert/wi_1/kernel/v_col
I0512 23:42:16.226601 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/mlp/expert/wi_1/kernel/v_row
I0512 23:42:16.226636 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/mlp/expert/wo/kernel/m
I0512 23:42:16.226671 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/mlp/expert/wo/kernel/v
I0512 23:42:16.226706 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/mlp/expert/wo/kernel/v_col
I0512 23:42:16.226741 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/mlp/expert/wo/kernel/v_row
I0512 23:42:16.226776 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/mlp/router/router_weights/w/kernel/m
I0512 23:42:16.226811 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/mlp/router/router_weights/w/kernel/v
I0512 23:42:16.227881 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_7/pre_mlp_layer_norm/scale/v
I0512 23:42:16.227912 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_7/pre_mlp_layer_norm/scale/v_col
I0512 23:42:16.227942 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_7/pre_mlp_layer_norm/scale/v_row
I0512 23:42:16.227972 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_7/pre_self_attention_layer_norm/scale/m
I0512 23:42:16.228002 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_7/pre_self_attention_layer_norm/scale/v
I0512 23:42:16.228032 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_7/pre_self_attention_layer_norm/scale/v_col
I0512 23:42:16.228061 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_7/pre_self_attention_layer_norm/scale/v_row
I0512 23:42:16.228091 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_7/self_attention/key/kernel/m
I0512 23:42:16.228121 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_7/self_attention/key/kernel/v
I0512 23:42:16.228151 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_7/self_attention/key/kernel/v_col
I0512 23:42:16.228180 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_7/self_attention/key/kernel/v_row
I0512 23:42:16.228210 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_7/self_attention/out/kernel/m
I0512 23:42:16.228240 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_7/self_attention/out/kernel/v
I0512 23:42:16.228270 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_7/self_attention/out/kernel/v_col
I0512 23:42:16.206910 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_0/mlp/wi_0/kernel/v
I0512 23:42:16.206942 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_0/mlp/wi_0/kernel/v_col
I0512 23:42:16.206974 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_0/mlp/wi_0/kernel/v_row
I0512 23:42:16.207006 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_0/mlp/wi_1/kernel/m
I0512 23:42:16.207039 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_0/mlp/wi_1/kernel/v
I0512 23:42:16.207071 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_0/mlp/wi_1/kernel/v_col
I0512 23:42:16.207103 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_0/mlp/wi_1/kernel/v_row
I0512 23:42:16.207134 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_0/mlp/wo/kernel/m
I0512 23:42:16.207166 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_0/mlp/wo/kernel/v
I0512 23:42:16.207198 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_0/mlp/wo/kernel/v_col
I0512 23:42:16.207231 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_0/mlp/wo/kernel/v_row
I0512 23:42:16.207262 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_0/pre_mlp_layer_norm/scale/m
I0512 23:42:16.226846 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/mlp/router/router_weights/w/kernel/v_col
I0512 23:42:16.226881 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/mlp/router/router_weights/w/kernel/v_row
I0512 23:42:16.226919 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/pre_extra_mlp_layer_norm/scale/m
I0512 23:42:16.226954 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/pre_extra_mlp_layer_norm/scale/v
I0512 23:42:16.226989 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/pre_extra_mlp_layer_norm/scale/v_col
I0512 23:42:16.227024 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/pre_extra_mlp_layer_norm/scale/v_row
I0512 23:42:16.227059 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/pre_mlp_layer_norm/scale/m
I0512 23:42:16.227095 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/pre_mlp_layer_norm/scale/v
I0512 23:42:16.227134 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/pre_mlp_layer_norm/scale/v_col
I0512 23:42:16.227171 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/pre_mlp_layer_norm/scale/v_row
I0512 23:42:16.227206 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/pre_self_attention_layer_norm/scale/m
I0512 23:42:16.227242 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/pre_self_attention_layer_norm/scale/v
I0512 23:42:16.227277 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/pre_self_attention_layer_norm/scale/v_col
I0512 23:42:16.227312 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/pre_self_attention_layer_norm/scale/v_row
I0512 23:42:16.226489 140031916288000 checkpoints.py:1054] Restoring from checkpoint: gs://rosinality-tpu-bucket/openmoe_8b/training/checkpoint_32500/checkpoint
I0512 23:42:16.228299 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_7/self_attention/out/kernel/v_row
I0512 23:42:16.228329 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_7/self_attention/query/kernel/m
I0512 23:42:16.228359 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_7/self_attention/query/kernel/v
I0512 23:42:16.228389 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_7/self_attention/query/kernel/v_col
I0512 23:42:16.228419 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_7/self_attention/query/kernel/v_row
I0512 23:42:16.228449 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_7/self_attention/value/kernel/m
I0512 23:42:16.228479 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_7/self_attention/value/kernel/v
I0512 23:42:16.228508 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_7/self_attention/value/kernel/v_col
I0512 23:42:16.228538 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_7/self_attention/value/kernel/v_row
I0512 23:42:16.228568 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_8/mlp/wi_0/kernel/m
I0512 23:42:16.228600 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_8/mlp/wi_0/kernel/v
I0512 23:42:16.228631 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_8/mlp/wi_0/kernel/v_col
I0512 23:42:16.228662 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_8/mlp/wi_0/kernel/v_row
I0512 23:42:16.228692 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_8/mlp/wi_1/kernel/m
I0512 23:42:16.228722 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_8/mlp/wi_1/kernel/v
I0512 23:42:16.207295 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_0/pre_mlp_layer_norm/scale/v
I0512 23:42:16.207327 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_0/pre_mlp_layer_norm/scale/v_col
I0512 23:42:16.207359 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_0/pre_mlp_layer_norm/scale/v_row
I0512 23:42:16.207391 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_0/pre_self_attention_layer_norm/scale/m
I0512 23:42:16.207426 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_0/pre_self_attention_layer_norm/scale/v
I0512 23:42:16.207459 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_0/pre_self_attention_layer_norm/scale/v_col
I0512 23:42:16.207492 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_0/pre_self_attention_layer_norm/scale/v_row
I0512 23:42:16.207525 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_0/self_attention/key/kernel/m
I0512 23:42:16.207557 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_0/self_attention/key/kernel/v
I0512 23:42:16.207589 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_0/self_attention/key/kernel/v_col
I0512 23:42:16.207621 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_0/self_attention/key/kernel/v_row
I0512 23:42:16.207658 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_0/self_attention/out/kernel/m
I0512 23:42:16.227347 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/self_attention/key/kernel/m
I0512 23:42:16.227382 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/self_attention/key/kernel/v
I0512 23:42:16.227418 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/self_attention/key/kernel/v_col
I0512 23:42:16.227452 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/self_attention/key/kernel/v_row
I0512 23:42:16.227488 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/self_attention/out/kernel/m
I0512 23:42:16.227522 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/self_attention/out/kernel/v
I0512 23:42:16.227557 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/self_attention/out/kernel/v_col
I0512 23:42:16.227592 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/self_attention/out/kernel/v_row
I0512 23:42:16.227627 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/self_attention/query/kernel/m
I0512 23:42:16.227662 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/self_attention/query/kernel/v
I0512 23:42:16.227697 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/self_attention/query/kernel/v_col
I0512 23:42:16.227732 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/self_attention/query/kernel/v_row
I0512 23:42:16.227767 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/self_attention/value/kernel/m
I0512 23:42:16.227802 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/self_attention/value/kernel/v
I0512 23:42:16.227837 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/self_attention/value/kernel/v_col
I0512 23:42:16.207692 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_0/self_attention/out/kernel/v
I0512 23:42:16.207724 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_0/self_attention/out/kernel/v_col
I0512 23:42:16.207756 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_0/self_attention/out/kernel/v_row
I0512 23:42:16.207788 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_0/self_attention/query/kernel/m
I0512 23:42:16.207820 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_0/self_attention/query/kernel/v
I0512 23:42:16.207852 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_0/self_attention/query/kernel/v_col
I0512 23:42:16.207884 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_0/self_attention/query/kernel/v_row
I0512 23:42:16.207916 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_0/self_attention/value/kernel/m
I0512 23:42:16.207947 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_0/self_attention/value/kernel/v
I0512 23:42:16.207979 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_0/self_attention/value/kernel/v_col
I0512 23:42:16.208011 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_0/self_attention/value/kernel/v_row
I0512 23:42:16.208044 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_1/mlp/wi_0/kernel/m
I0512 23:42:16.208076 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_1/mlp/wi_0/kernel/v
I0512 23:42:16.228752 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_8/mlp/wi_1/kernel/v_col
I0512 23:42:16.228782 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_8/mlp/wi_1/kernel/v_row
I0512 23:42:16.228811 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_8/mlp/wo/kernel/m
I0512 23:42:16.228847 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_8/mlp/wo/kernel/v
I0512 23:42:16.228878 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_8/mlp/wo/kernel/v_col
I0512 23:42:16.228908 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_8/mlp/wo/kernel/v_row
I0512 23:42:16.228938 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_8/pre_mlp_layer_norm/scale/m
I0512 23:42:16.228968 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_8/pre_mlp_layer_norm/scale/v
I0512 23:42:16.228998 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_8/pre_mlp_layer_norm/scale/v_col
I0512 23:42:16.229028 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_8/pre_mlp_layer_norm/scale/v_row
I0512 23:42:16.229058 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_8/pre_self_attention_layer_norm/scale/m
I0512 23:42:16.229088 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_8/pre_self_attention_layer_norm/scale/v
I0512 23:42:16.229118 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_8/pre_self_attention_layer_norm/scale/v_col
I0512 23:42:16.229148 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_8/pre_self_attention_layer_norm/scale/v_row
I0512 23:42:16.229178 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_8/self_attention/key/kernel/m
I0512 23:42:16.229208 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_8/self_attention/key/kernel/v
I0512 23:42:16.229238 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_8/self_attention/key/kernel/v_col
I0512 23:42:16.229268 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_8/self_attention/key/kernel/v_row
I0512 23:42:16.229297 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_8/self_attention/out/kernel/m
I0512 23:42:16.229327 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_8/self_attention/out/kernel/v
I0512 23:42:16.229357 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_8/self_attention/out/kernel/v_col
I0512 23:42:16.229387 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_8/self_attention/out/kernel/v_row
I0512 23:42:16.229417 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_8/self_attention/query/kernel/m
I0512 23:42:16.229447 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_8/self_attention/query/kernel/v
I0512 23:42:16.229477 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_8/self_attention/query/kernel/v_col
I0512 23:42:16.229506 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_8/self_attention/query/kernel/v_row
I0512 23:42:16.229537 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_8/self_attention/value/kernel/m
I0512 23:42:16.229566 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_8/self_attention/value/kernel/v
I0512 23:42:16.229598 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_8/self_attention/value/kernel/v_col
I0512 23:42:16.229629 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_8/self_attention/value/kernel/v_row
I0512 23:42:16.227873 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/self_attention/value/kernel/v_row
I0512 23:42:16.227909 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_12/mlp/wi_0/kernel/m
I0512 23:42:16.227946 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_12/mlp/wi_0/kernel/v
I0512 23:42:16.227981 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_12/mlp/wi_0/kernel/v_col
I0512 23:42:16.228016 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_12/mlp/wi_0/kernel/v_row
I0512 23:42:16.228051 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_12/mlp/wi_1/kernel/m
I0512 23:42:16.228086 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_12/mlp/wi_1/kernel/v
I0512 23:42:16.228121 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_12/mlp/wi_1/kernel/v_col
I0512 23:42:16.228163 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_12/mlp/wi_1/kernel/v_row
I0512 23:42:16.228198 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_12/mlp/wo/kernel/m
I0512 23:42:16.228233 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_12/mlp/wo/kernel/v
I0512 23:42:16.228269 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_12/mlp/wo/kernel/v_col
I0512 23:42:16.228304 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_12/mlp/wo/kernel/v_row
I0512 23:42:16.228339 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_12/pre_mlp_layer_norm/scale/m
I0512 23:42:16.228374 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_12/pre_mlp_layer_norm/scale/v
I0512 23:42:16.208107 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_1/mlp/wi_0/kernel/v_col
I0512 23:42:16.208140 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_1/mlp/wi_0/kernel/v_row
I0512 23:42:16.208171 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_1/mlp/wi_1/kernel/m
I0512 23:42:16.208203 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_1/mlp/wi_1/kernel/v
I0512 23:42:16.208235 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_1/mlp/wi_1/kernel/v_col
I0512 23:42:16.208267 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_1/mlp/wi_1/kernel/v_row
I0512 23:42:16.208299 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_1/mlp/wo/kernel/m
I0512 23:42:16.208331 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_1/mlp/wo/kernel/v
I0512 23:42:16.208362 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_1/mlp/wo/kernel/v_col
I0512 23:42:16.208394 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_1/mlp/wo/kernel/v_row
I0512 23:42:16.208428 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_1/pre_mlp_layer_norm/scale/m
I0512 23:42:16.208461 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_1/pre_mlp_layer_norm/scale/v
I0512 23:42:16.208493 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_1/pre_mlp_layer_norm/scale/v_col
I0512 23:42:16.208525 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_1/pre_mlp_layer_norm/scale/v_row
I0512 23:42:16.229659 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_9/mlp/wi_0/kernel/m
I0512 23:42:16.229690 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_9/mlp/wi_0/kernel/v
I0512 23:42:16.229720 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_9/mlp/wi_0/kernel/v_col
I0512 23:42:16.229749 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_9/mlp/wi_0/kernel/v_row
I0512 23:42:16.229779 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_9/mlp/wi_1/kernel/m
I0512 23:42:16.229809 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_9/mlp/wi_1/kernel/v
I0512 23:42:16.229845 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_9/mlp/wi_1/kernel/v_col
I0512 23:42:16.229877 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_9/mlp/wi_1/kernel/v_row
I0512 23:42:16.229907 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_9/mlp/wo/kernel/m
I0512 23:42:16.229937 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_9/mlp/wo/kernel/v
I0512 23:42:16.229967 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_9/mlp/wo/kernel/v_col
I0512 23:42:16.229997 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_9/mlp/wo/kernel/v_row
I0512 23:42:16.230026 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_9/pre_mlp_layer_norm/scale/m
I0512 23:42:16.230057 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_9/pre_mlp_layer_norm/scale/v
I0512 23:42:16.230086 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_9/pre_mlp_layer_norm/scale/v_col
I0512 23:42:16.208558 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_1/pre_self_attention_layer_norm/scale/m
I0512 23:42:16.208591 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_1/pre_self_attention_layer_norm/scale/v
I0512 23:42:16.208622 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_1/pre_self_attention_layer_norm/scale/v_col
I0512 23:42:16.208659 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_1/pre_self_attention_layer_norm/scale/v_row
I0512 23:42:16.208693 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_1/self_attention/key/kernel/m
I0512 23:42:16.208725 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_1/self_attention/key/kernel/v
I0512 23:42:16.208757 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_1/self_attention/key/kernel/v_col
I0512 23:42:16.208789 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_1/self_attention/key/kernel/v_row
I0512 23:42:16.208821 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_1/self_attention/out/kernel/m
I0512 23:42:16.208853 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_1/self_attention/out/kernel/v
I0512 23:42:16.208884 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_1/self_attention/out/kernel/v_col
I0512 23:42:16.208916 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_1/self_attention/out/kernel/v_row
I0512 23:42:16.208949 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_1/self_attention/query/kernel/m
I0512 23:42:16.208981 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_1/self_attention/query/kernel/v
I0512 23:42:16.228426 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_12/pre_mlp_layer_norm/scale/v_col
I0512 23:42:16.228466 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_12/pre_mlp_layer_norm/scale/v_row
I0512 23:42:16.228501 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_12/pre_self_attention_layer_norm/scale/m
I0512 23:42:16.228536 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_12/pre_self_attention_layer_norm/scale/v
I0512 23:42:16.228571 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_12/pre_self_attention_layer_norm/scale/v_col
I0512 23:42:16.228606 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_12/pre_self_attention_layer_norm/scale/v_row
I0512 23:42:16.228642 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_12/self_attention/key/kernel/m
I0512 23:42:16.228677 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_12/self_attention/key/kernel/v
I0512 23:42:16.228712 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_12/self_attention/key/kernel/v_col
I0512 23:42:16.228747 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_12/self_attention/key/kernel/v_row
I0512 23:42:16.228782 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_12/self_attention/out/kernel/m
I0512 23:42:16.228817 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_12/self_attention/out/kernel/v
I0512 23:42:16.228852 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_12/self_attention/out/kernel/v_col
I0512 23:42:16.228888 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_12/self_attention/out/kernel/v_row
I0512 23:42:16.228924 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_12/self_attention/query/kernel/m
I0512 23:42:16.209012 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_1/self_attention/query/kernel/v_col
I0512 23:42:16.209044 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_1/self_attention/query/kernel/v_row
I0512 23:42:16.209076 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_1/self_attention/value/kernel/m
I0512 23:42:16.209108 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_1/self_attention/value/kernel/v
I0512 23:42:16.209140 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_1/self_attention/value/kernel/v_col
I0512 23:42:16.209172 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_1/self_attention/value/kernel/v_row
I0512 23:42:16.209203 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_10/mlp/wi_0/kernel/m
I0512 23:42:16.209235 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_10/mlp/wi_0/kernel/v
I0512 23:42:16.209267 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_10/mlp/wi_0/kernel/v_col
I0512 23:42:16.209299 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_10/mlp/wi_0/kernel/v_row
I0512 23:42:16.209331 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_10/mlp/wi_1/kernel/m
I0512 23:42:16.209363 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_10/mlp/wi_1/kernel/v
I0512 23:42:16.209395 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_10/mlp/wi_1/kernel/v_col
I0512 23:42:16.209429 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_10/mlp/wi_1/kernel/v_row
I0512 23:42:16.209463 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_10/mlp/wo/kernel/m
I0512 23:42:16.228959 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_12/self_attention/query/kernel/v
I0512 23:42:16.228994 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_12/self_attention/query/kernel/v_col
I0512 23:42:16.229029 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_12/self_attention/query/kernel/v_row
I0512 23:42:16.229064 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_12/self_attention/value/kernel/m
I0512 23:42:16.229099 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_12/self_attention/value/kernel/v
I0512 23:42:16.229139 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_12/self_attention/value/kernel/v_col
I0512 23:42:16.229177 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_12/self_attention/value/kernel/v_row
I0512 23:42:16.229213 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_13/mlp/wi_0/kernel/m
I0512 23:42:16.229249 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_13/mlp/wi_0/kernel/v
I0512 23:42:16.229284 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_13/mlp/wi_0/kernel/v_col
I0512 23:42:16.229319 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_13/mlp/wi_0/kernel/v_row
I0512 23:42:16.229354 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_13/mlp/wi_1/kernel/m
I0512 23:42:16.229388 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_13/mlp/wi_1/kernel/v
I0512 23:42:16.229423 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_13/mlp/wi_1/kernel/v_col
I0512 23:42:16.230116 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_9/pre_mlp_layer_norm/scale/v_row
I0512 23:42:16.230146 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_9/pre_self_attention_layer_norm/scale/m
I0512 23:42:16.230176 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_9/pre_self_attention_layer_norm/scale/v
I0512 23:42:16.230206 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_9/pre_self_attention_layer_norm/scale/v_col
I0512 23:42:16.230235 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_9/pre_self_attention_layer_norm/scale/v_row
I0512 23:42:16.230265 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_9/self_attention/key/kernel/m
I0512 23:42:16.230295 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_9/self_attention/key/kernel/v
I0512 23:42:16.230325 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_9/self_attention/key/kernel/v_col
I0512 23:42:16.230355 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_9/self_attention/key/kernel/v_row
I0512 23:42:16.230385 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_9/self_attention/out/kernel/m
I0512 23:42:16.230415 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_9/self_attention/out/kernel/v
I0512 23:42:16.230444 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_9/self_attention/out/kernel/v_col
I0512 23:42:16.230474 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_9/self_attention/out/kernel/v_row
I0512 23:42:16.230504 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_9/self_attention/query/kernel/m
I0512 23:42:16.230534 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_9/self_attention/query/kernel/v
I0512 23:42:16.230564 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_9/self_attention/query/kernel/v_col
I0512 23:42:16.230595 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_9/self_attention/query/kernel/v_row
I0512 23:42:16.230626 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_9/self_attention/value/kernel/m
I0512 23:42:16.230657 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_9/self_attention/value/kernel/v
I0512 23:42:16.230686 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_9/self_attention/value/kernel/v_col
I0512 23:42:16.230716 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_9/self_attention/value/kernel/v_row
I0512 23:42:16.230746 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/logits_dense/kernel/m
I0512 23:42:16.230777 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/logits_dense/kernel/v
I0512 23:42:16.230807 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/logits_dense/kernel/v_col
I0512 23:42:16.230842 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/logits_dense/kernel/v_row
I0512 23:42:16.230874 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/token_embedder/embedding/m
I0512 23:42:16.230904 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/token_embedder/embedding/v
I0512 23:42:16.230934 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/token_embedder/embedding/v_col
I0512 23:42:16.230964 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/param_states/token_embedder/embedding/v_row
I0512 23:42:16.230994 139674595104768 checkpoints.py:1100] Restoring key from ckpt: state/step
I0512 23:42:16.209495 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_10/mlp/wo/kernel/v
I0512 23:42:16.209527 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_10/mlp/wo/kernel/v_col
I0512 23:42:16.209559 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_10/mlp/wo/kernel/v_row
I0512 23:42:16.209591 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_10/pre_mlp_layer_norm/scale/m
I0512 23:42:16.209623 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_10/pre_mlp_layer_norm/scale/v
I0512 23:42:16.209659 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_10/pre_mlp_layer_norm/scale/v_col
I0512 23:42:16.209693 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_10/pre_mlp_layer_norm/scale/v_row
I0512 23:42:16.209725 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_10/pre_self_attention_layer_norm/scale/m
I0512 23:42:16.209757 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_10/pre_self_attention_layer_norm/scale/v
I0512 23:42:16.209810 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_10/pre_self_attention_layer_norm/scale/v_col
I0512 23:42:16.209845 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_10/pre_self_attention_layer_norm/scale/v_row
I0512 23:42:16.209877 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_10/self_attention/key/kernel/m
I0512 23:42:16.209910 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_10/self_attention/key/kernel/v
I0512 23:42:16.209942 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_10/self_attention/key/kernel/v_col
I0512 23:42:16.229459 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_13/mlp/wi_1/kernel/v_row
I0512 23:42:16.229494 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_13/mlp/wo/kernel/m
I0512 23:42:16.229529 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_13/mlp/wo/kernel/v
I0512 23:42:16.229564 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_13/mlp/wo/kernel/v_col
I0512 23:42:16.229598 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_13/mlp/wo/kernel/v_row
I0512 23:42:16.229634 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_13/pre_mlp_layer_norm/scale/m
I0512 23:42:16.229669 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_13/pre_mlp_layer_norm/scale/v
I0512 23:42:16.229704 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_13/pre_mlp_layer_norm/scale/v_col
I0512 23:42:16.229739 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_13/pre_mlp_layer_norm/scale/v_row
I0512 23:42:16.229774 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_13/pre_self_attention_layer_norm/scale/m
I0512 23:42:16.229809 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_13/pre_self_attention_layer_norm/scale/v
I0512 23:42:16.229844 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_13/pre_self_attention_layer_norm/scale/v_col
I0512 23:42:16.229879 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_13/pre_self_attention_layer_norm/scale/v_row
I0512 23:42:16.229917 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_13/self_attention/key/kernel/m
I0512 23:42:16.229952 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_13/self_attention/key/kernel/v
I0512 23:42:16.229987 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_13/self_attention/key/kernel/v_col
I0512 23:42:16.230022 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_13/self_attention/key/kernel/v_row
I0512 23:42:16.230058 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_13/self_attention/out/kernel/m
I0512 23:42:16.230093 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_13/self_attention/out/kernel/v
I0512 23:42:16.230128 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_13/self_attention/out/kernel/v_col
I0512 23:42:16.230169 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_13/self_attention/out/kernel/v_row
I0512 23:42:16.230204 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_13/self_attention/query/kernel/m
I0512 23:42:16.230239 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_13/self_attention/query/kernel/v
I0512 23:42:16.230274 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_13/self_attention/query/kernel/v_col
I0512 23:42:16.230309 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_13/self_attention/query/kernel/v_row
I0512 23:42:16.230344 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_13/self_attention/value/kernel/m
I0512 23:42:16.230379 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_13/self_attention/value/kernel/v
I0512 23:42:16.230414 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_13/self_attention/value/kernel/v_col
I0512 23:42:16.230449 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_13/self_attention/value/kernel/v_row
I0512 23:42:16.230484 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_14/mlp/wi_0/kernel/m
I0512 23:42:16.209974 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_10/self_attention/key/kernel/v_row
I0512 23:42:16.210006 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_10/self_attention/out/kernel/m
I0512 23:42:16.210038 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_10/self_attention/out/kernel/v
I0512 23:42:16.210070 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_10/self_attention/out/kernel/v_col
I0512 23:42:16.210102 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_10/self_attention/out/kernel/v_row
I0512 23:42:16.210134 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_10/self_attention/query/kernel/m
I0512 23:42:16.210165 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_10/self_attention/query/kernel/v
I0512 23:42:16.210197 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_10/self_attention/query/kernel/v_col
I0512 23:42:16.210230 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_10/self_attention/query/kernel/v_row
I0512 23:42:16.210261 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_10/self_attention/value/kernel/m
I0512 23:42:16.210293 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_10/self_attention/value/kernel/v
I0512 23:42:16.210325 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_10/self_attention/value/kernel/v_col
I0512 23:42:16.210357 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_10/self_attention/value/kernel/v_row
I0512 23:42:16.210389 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/extra_mlp/wi_0/kernel/m
I0512 23:42:16.210422 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/extra_mlp/wi_0/kernel/v
I0512 23:42:16.231024 139674595104768 checkpoints.py:1100] Restoring key from ckpt: target/decoder/decoder_norm/scale
I0512 23:42:16.231055 139674595104768 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_0/mlp/wi_0/kernel
I0512 23:42:16.231085 139674595104768 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_0/mlp/wi_1/kernel
I0512 23:42:16.231114 139674595104768 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_0/mlp/wo/kernel
I0512 23:42:16.231144 139674595104768 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_0/pre_mlp_layer_norm/scale
I0512 23:42:16.231173 139674595104768 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_0/pre_self_attention_layer_norm/scale
I0512 23:42:16.231203 139674595104768 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_0/self_attention/key/kernel
I0512 23:42:16.231233 139674595104768 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_0/self_attention/out/kernel
I0512 23:42:16.231263 139674595104768 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_0/self_attention/query/kernel
I0512 23:42:16.231553 139674595104768 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_0/self_attention/value/kernel
I0512 23:42:16.231592 139674595104768 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_1/mlp/wi_0/kernel
I0512 23:42:16.231625 139674595104768 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_1/mlp/wi_1/kernel
I0512 23:42:16.231656 139674595104768 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_1/mlp/wo/kernel
I0512 23:42:16.231686 139674595104768 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_1/pre_mlp_layer_norm/scale
I0512 23:42:16.231716 139674595104768 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_1/pre_self_attention_layer_norm/scale
I0512 23:42:16.231746 139674595104768 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_1/self_attention/key/kernel
I0512 23:42:16.231776 139674595104768 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_1/self_attention/out/kernel
I0512 23:42:16.210456 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/extra_mlp/wi_0/kernel/v_col
I0512 23:42:16.210488 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/extra_mlp/wi_0/kernel/v_row
I0512 23:42:16.210520 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/extra_mlp/wi_1/kernel/m
I0512 23:42:16.210552 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/extra_mlp/wi_1/kernel/v
I0512 23:42:16.210584 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/extra_mlp/wi_1/kernel/v_col
I0512 23:42:16.210616 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/extra_mlp/wi_1/kernel/v_row
I0512 23:42:16.210648 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/extra_mlp/wo/kernel/m
I0512 23:42:16.210687 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/extra_mlp/wo/kernel/v
I0512 23:42:16.210720 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/extra_mlp/wo/kernel/v_col
I0512 23:42:16.210752 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/extra_mlp/wo/kernel/v_row
I0512 23:42:16.210784 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/mlp/expert/wi_0/kernel/m
I0512 23:42:16.210816 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/mlp/expert/wi_0/kernel/v
I0512 23:42:16.210848 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/mlp/expert/wi_0/kernel/v_col
I0512 23:42:16.210880 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/mlp/expert/wi_0/kernel/v_row
I0512 23:42:16.210912 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/mlp/expert/wi_1/kernel/m
I0512 23:42:16.230519 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_14/mlp/wi_0/kernel/v
I0512 23:42:16.230554 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_14/mlp/wi_0/kernel/v_col
I0512 23:42:16.230589 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_14/mlp/wi_0/kernel/v_row
I0512 23:42:16.230624 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_14/mlp/wi_1/kernel/m
I0512 23:42:16.230659 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_14/mlp/wi_1/kernel/v
I0512 23:42:16.230694 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_14/mlp/wi_1/kernel/v_col
I0512 23:42:16.230729 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_14/mlp/wi_1/kernel/v_row
I0512 23:42:16.230764 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_14/mlp/wo/kernel/m
I0512 23:42:16.230799 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_14/mlp/wo/kernel/v
I0512 23:42:16.230834 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_14/mlp/wo/kernel/v_col
I0512 23:42:16.230868 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_14/mlp/wo/kernel/v_row
I0512 23:42:16.230905 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_14/pre_mlp_layer_norm/scale/m
I0512 23:42:16.230942 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_14/pre_mlp_layer_norm/scale/v
I0512 23:42:16.230976 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_14/pre_mlp_layer_norm/scale/v_col
I0512 23:42:16.231011 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_14/pre_mlp_layer_norm/scale/v_row
I0512 23:42:16.231806 139674595104768 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_1/self_attention/query/kernel
I0512 23:42:16.231841 139674595104768 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_1/self_attention/value/kernel
I0512 23:42:16.231872 139674595104768 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_10/mlp/wi_0/kernel
I0512 23:42:16.231903 139674595104768 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_10/mlp/wi_1/kernel
I0512 23:42:16.231933 139674595104768 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_10/mlp/wo/kernel
I0512 23:42:16.231963 139674595104768 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_10/pre_mlp_layer_norm/scale
I0512 23:42:16.231993 139674595104768 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_10/pre_self_attention_layer_norm/scale
I0512 23:42:16.232023 139674595104768 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_10/self_attention/key/kernel
I0512 23:42:16.232053 139674595104768 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_10/self_attention/out/kernel
I0512 23:42:16.232083 139674595104768 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_10/self_attention/query/kernel
I0512 23:42:16.232113 139674595104768 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_10/self_attention/value/kernel
I0512 23:42:16.232143 139674595104768 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_11/extra_mlp/wi_0/kernel
I0512 23:42:16.232174 139674595104768 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_11/extra_mlp/wi_1/kernel
I0512 23:42:16.232203 139674595104768 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_11/extra_mlp/wo/kernel
I0512 23:42:16.232234 139674595104768 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_11/mlp/expert/wi_0/kernel
I0512 23:42:16.232264 139674595104768 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_11/mlp/expert/wi_1/kernel
I0512 23:42:16.210944 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/mlp/expert/wi_1/kernel/v
I0512 23:42:16.210976 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/mlp/expert/wi_1/kernel/v_col
I0512 23:42:16.211008 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/mlp/expert/wi_1/kernel/v_row
I0512 23:42:16.211040 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/mlp/expert/wo/kernel/m
I0512 23:42:16.211072 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/mlp/expert/wo/kernel/v
I0512 23:42:16.211104 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/mlp/expert/wo/kernel/v_col
I0512 23:42:16.211136 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/mlp/expert/wo/kernel/v_row
I0512 23:42:16.211168 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/mlp/router/router_weights/w/kernel/m
I0512 23:42:16.211200 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/mlp/router/router_weights/w/kernel/v
I0512 23:42:16.211233 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/mlp/router/router_weights/w/kernel/v_col
I0512 23:42:16.211265 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/mlp/router/router_weights/w/kernel/v_row
I0512 23:42:16.211297 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/pre_extra_mlp_layer_norm/scale/m
I0512 23:42:16.211329 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/pre_extra_mlp_layer_norm/scale/v
I0512 23:42:16.211361 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/pre_extra_mlp_layer_norm/scale/v_col
I0512 23:42:16.231046 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_14/pre_self_attention_layer_norm/scale/m
I0512 23:42:16.231081 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_14/pre_self_attention_layer_norm/scale/v
I0512 23:42:16.231117 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_14/pre_self_attention_layer_norm/scale/v_col
I0512 23:42:16.231157 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_14/pre_self_attention_layer_norm/scale/v_row
I0512 23:42:16.231193 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_14/self_attention/key/kernel/m
I0512 23:42:16.231228 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_14/self_attention/key/kernel/v
I0512 23:42:16.231263 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_14/self_attention/key/kernel/v_col
I0512 23:42:16.231298 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_14/self_attention/key/kernel/v_row
I0512 23:42:16.231333 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_14/self_attention/out/kernel/m
I0512 23:42:16.231368 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_14/self_attention/out/kernel/v
I0512 23:42:16.231403 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_14/self_attention/out/kernel/v_col
I0512 23:42:16.231438 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_14/self_attention/out/kernel/v_row
I0512 23:42:16.231473 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_14/self_attention/query/kernel/m
I0512 23:42:16.231508 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_14/self_attention/query/kernel/v
I0512 23:42:16.232294 139674595104768 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_11/mlp/expert/wo/kernel
I0512 23:42:16.232324 139674595104768 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_11/mlp/router/router_weights/w/kernel
I0512 23:42:16.232354 139674595104768 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_11/pre_extra_mlp_layer_norm/scale
I0512 23:42:16.232384 139674595104768 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_11/pre_mlp_layer_norm/scale
I0512 23:42:16.232414 139674595104768 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_11/pre_self_attention_layer_norm/scale
I0512 23:42:16.232444 139674595104768 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_11/self_attention/key/kernel
I0512 23:42:16.232474 139674595104768 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_11/self_attention/out/kernel
I0512 23:42:16.232504 139674595104768 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_11/self_attention/query/kernel
I0512 23:42:16.232534 139674595104768 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_11/self_attention/value/kernel
I0512 23:42:16.232564 139674595104768 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_12/mlp/wi_0/kernel
I0512 23:42:16.232595 139674595104768 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_12/mlp/wi_1/kernel
I0512 23:42:16.232627 139674595104768 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_12/mlp/wo/kernel
I0512 23:42:16.232657 139674595104768 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_12/pre_mlp_layer_norm/scale
I0512 23:42:16.232686 139674595104768 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_12/pre_self_attention_layer_norm/scale
I0512 23:42:16.232716 139674595104768 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_12/self_attention/key/kernel
I0512 23:42:16.232746 139674595104768 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_12/self_attention/out/kernel
I0512 23:42:16.211393 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/pre_extra_mlp_layer_norm/scale/v_row
I0512 23:42:16.211426 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/pre_mlp_layer_norm/scale/m
I0512 23:42:16.211460 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/pre_mlp_layer_norm/scale/v
I0512 23:42:16.211492 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/pre_mlp_layer_norm/scale/v_col
I0512 23:42:16.211525 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/pre_mlp_layer_norm/scale/v_row
I0512 23:42:16.211556 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/pre_self_attention_layer_norm/scale/m
I0512 23:42:16.211588 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/pre_self_attention_layer_norm/scale/v
I0512 23:42:16.211620 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/pre_self_attention_layer_norm/scale/v_col
I0512 23:42:16.211652 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/pre_self_attention_layer_norm/scale/v_row
I0512 23:42:16.211691 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/self_attention/key/kernel/m
I0512 23:42:16.211723 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/self_attention/key/kernel/v
I0512 23:42:16.211755 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/self_attention/key/kernel/v_col
I0512 23:42:16.211787 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/self_attention/key/kernel/v_row
I0512 23:42:16.211818 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/self_attention/out/kernel/m
I0512 23:42:16.211851 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/self_attention/out/kernel/v
I0512 23:42:16.232776 139674595104768 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_12/self_attention/query/kernel
I0512 23:42:16.232806 139674595104768 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_12/self_attention/value/kernel
I0512 23:42:16.232841 139674595104768 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_13/mlp/wi_0/kernel
I0512 23:42:16.232872 139674595104768 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_13/mlp/wi_1/kernel
I0512 23:42:16.232903 139674595104768 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_13/mlp/wo/kernel
I0512 23:42:16.232933 139674595104768 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_13/pre_mlp_layer_norm/scale
I0512 23:42:16.232963 139674595104768 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_13/pre_self_attention_layer_norm/scale
I0512 23:42:16.232993 139674595104768 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_13/self_attention/key/kernel
I0512 23:42:16.233022 139674595104768 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_13/self_attention/out/kernel
I0512 23:42:16.233053 139674595104768 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_13/self_attention/query/kernel
I0512 23:42:16.233083 139674595104768 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_13/self_attention/value/kernel
I0512 23:42:16.233112 139674595104768 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_14/mlp/wi_0/kernel
I0512 23:42:16.233143 139674595104768 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_14/mlp/wi_1/kernel
I0512 23:42:16.233172 139674595104768 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_14/mlp/wo/kernel
I0512 23:42:16.233202 139674595104768 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_14/pre_mlp_layer_norm/scale
I0512 23:42:16.233232 139674595104768 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_14/pre_self_attention_layer_norm/scale
I0512 23:42:16.231543 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_14/self_attention/query/kernel/v_col
I0512 23:42:16.231578 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_14/self_attention/query/kernel/v_row
I0512 23:42:16.231613 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_14/self_attention/value/kernel/m
I0512 23:42:16.231648 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_14/self_attention/value/kernel/v
I0512 23:42:16.231683 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_14/self_attention/value/kernel/v_col
I0512 23:42:16.231718 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_14/self_attention/value/kernel/v_row
I0512 23:42:16.231753 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_15/mlp/wi_0/kernel/m
I0512 23:42:16.231788 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_15/mlp/wi_0/kernel/v
I0512 23:42:16.231823 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_15/mlp/wi_0/kernel/v_col
I0512 23:42:16.231858 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_15/mlp/wi_0/kernel/v_row
I0512 23:42:16.231894 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_15/mlp/wi_1/kernel/m
I0512 23:42:16.231930 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_15/mlp/wi_1/kernel/v
I0512 23:42:16.231966 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_15/mlp/wi_1/kernel/v_col
I0512 23:42:16.232001 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_15/mlp/wi_1/kernel/v_row
I0512 23:42:16.232036 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_15/mlp/wo/kernel/m
I0512 23:42:16.211883 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/self_attention/out/kernel/v_col
I0512 23:42:16.211915 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/self_attention/out/kernel/v_row
I0512 23:42:16.211947 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/self_attention/query/kernel/m
I0512 23:42:16.211979 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/self_attention/query/kernel/v
I0512 23:42:16.212011 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/self_attention/query/kernel/v_col
I0512 23:42:16.212043 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/self_attention/query/kernel/v_row
I0512 23:42:16.212074 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/self_attention/value/kernel/m
I0512 23:42:16.212106 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/self_attention/value/kernel/v
I0512 23:42:16.212138 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/self_attention/value/kernel/v_col
I0512 23:42:16.212170 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/self_attention/value/kernel/v_row
I0512 23:42:16.212202 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_12/mlp/wi_0/kernel/m
I0512 23:42:16.212234 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_12/mlp/wi_0/kernel/v
I0512 23:42:16.212266 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_12/mlp/wi_0/kernel/v_col
I0512 23:42:16.212298 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_12/mlp/wi_0/kernel/v_row
I0512 23:42:16.233262 139674595104768 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_14/self_attention/key/kernel
I0512 23:42:16.233292 139674595104768 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_14/self_attention/out/kernel
I0512 23:42:16.233322 139674595104768 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_14/self_attention/query/kernel
I0512 23:42:16.233352 139674595104768 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_14/self_attention/value/kernel
I0512 23:42:16.233382 139674595104768 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_15/mlp/wi_0/kernel
I0512 23:42:16.233412 139674595104768 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_15/mlp/wi_1/kernel
I0512 23:42:16.233442 139674595104768 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_15/mlp/wo/kernel
I0512 23:42:16.233472 139674595104768 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_15/pre_mlp_layer_norm/scale
I0512 23:42:16.233502 139674595104768 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_15/pre_self_attention_layer_norm/scale
I0512 23:42:16.233532 139674595104768 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_15/self_attention/key/kernel
I0512 23:42:16.233562 139674595104768 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_15/self_attention/out/kernel
I0512 23:42:16.233593 139674595104768 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_15/self_attention/query/kernel
I0512 23:42:16.233624 139674595104768 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_15/self_attention/value/kernel
I0512 23:42:16.233654 139674595104768 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_16/mlp/wi_0/kernel
I0512 23:42:16.233685 139674595104768 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_16/mlp/wi_1/kernel
I0512 23:42:16.233715 139674595104768 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_16/mlp/wo/kernel
I0512 23:42:16.232071 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_15/mlp/wo/kernel/v
I0512 23:42:16.232106 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_15/mlp/wo/kernel/v_col
I0512 23:42:16.232150 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_15/mlp/wo/kernel/v_row
I0512 23:42:16.232186 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_15/pre_mlp_layer_norm/scale/m
I0512 23:42:16.232221 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_15/pre_mlp_layer_norm/scale/v
I0512 23:42:16.232256 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_15/pre_mlp_layer_norm/scale/v_col
I0512 23:42:16.232291 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_15/pre_mlp_layer_norm/scale/v_row
I0512 23:42:16.232326 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_15/pre_self_attention_layer_norm/scale/m
I0512 23:42:16.232361 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_15/pre_self_attention_layer_norm/scale/v
I0512 23:42:16.232397 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_15/pre_self_attention_layer_norm/scale/v_col
I0512 23:42:16.232454 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_15/pre_self_attention_layer_norm/scale/v_row
I0512 23:42:16.232491 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_15/self_attention/key/kernel/m
I0512 23:42:16.232526 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_15/self_attention/key/kernel/v
I0512 23:42:16.232561 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_15/self_attention/key/kernel/v_col
I0512 23:42:16.232596 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_15/self_attention/key/kernel/v_row
I0512 23:42:16.212330 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_12/mlp/wi_1/kernel/m
I0512 23:42:16.212362 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_12/mlp/wi_1/kernel/v
I0512 23:42:16.212394 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_12/mlp/wi_1/kernel/v_col
I0512 23:42:16.212427 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_12/mlp/wi_1/kernel/v_row
I0512 23:42:16.212460 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_12/mlp/wo/kernel/m
I0512 23:42:16.212492 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_12/mlp/wo/kernel/v
I0512 23:42:16.212525 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_12/mlp/wo/kernel/v_col
I0512 23:42:16.212557 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_12/mlp/wo/kernel/v_row
I0512 23:42:16.212589 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_12/pre_mlp_layer_norm/scale/m
I0512 23:42:16.212621 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_12/pre_mlp_layer_norm/scale/v
I0512 23:42:16.212658 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_12/pre_mlp_layer_norm/scale/v_col
I0512 23:42:16.212692 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_12/pre_mlp_layer_norm/scale/v_row
I0512 23:42:16.212724 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_12/pre_self_attention_layer_norm/scale/m
I0512 23:42:16.212756 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_12/pre_self_attention_layer_norm/scale/v
I0512 23:42:16.212788 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_12/pre_self_attention_layer_norm/scale/v_col
I0512 23:42:16.233744 139674595104768 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_16/pre_mlp_layer_norm/scale
I0512 23:42:16.233774 139674595104768 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_16/pre_self_attention_layer_norm/scale
I0512 23:42:16.233805 139674595104768 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_16/self_attention/key/kernel
I0512 23:42:16.233839 139674595104768 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_16/self_attention/out/kernel
I0512 23:42:16.233871 139674595104768 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_16/self_attention/query/kernel
I0512 23:42:16.233901 139674595104768 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_16/self_attention/value/kernel
I0512 23:42:16.233930 139674595104768 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_17/extra_mlp/wi_0/kernel
I0512 23:42:16.233960 139674595104768 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_17/extra_mlp/wi_1/kernel
I0512 23:42:16.233990 139674595104768 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_17/extra_mlp/wo/kernel
I0512 23:42:16.234020 139674595104768 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_17/mlp/expert/wi_0/kernel
I0512 23:42:16.234050 139674595104768 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_17/mlp/expert/wi_1/kernel
I0512 23:42:16.234080 139674595104768 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_17/mlp/expert/wo/kernel
I0512 23:42:16.234110 139674595104768 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_17/mlp/router/router_weights/w/kernel
I0512 23:42:16.234139 139674595104768 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_17/pre_extra_mlp_layer_norm/scale
I0512 23:42:16.234169 139674595104768 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_17/pre_mlp_layer_norm/scale
I0512 23:42:16.234199 139674595104768 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_17/pre_self_attention_layer_norm/scale
I0512 23:42:16.232631 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_15/self_attention/out/kernel/m
I0512 23:42:16.232666 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_15/self_attention/out/kernel/v
I0512 23:42:16.232702 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_15/self_attention/out/kernel/v_col
I0512 23:42:16.232737 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_15/self_attention/out/kernel/v_row
I0512 23:42:16.232772 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_15/self_attention/query/kernel/m
I0512 23:42:16.232806 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_15/self_attention/query/kernel/v
I0512 23:42:16.232841 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_15/self_attention/query/kernel/v_col
I0512 23:42:16.212820 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_12/pre_self_attention_layer_norm/scale/v_row
I0512 23:42:16.212852 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_12/self_attention/key/kernel/m
I0512 23:42:16.212884 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_12/self_attention/key/kernel/v
I0512 23:42:16.212916 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_12/self_attention/key/kernel/v_col
I0512 23:42:16.212948 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_12/self_attention/key/kernel/v_row
I0512 23:42:16.212980 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_12/self_attention/out/kernel/m
I0512 23:42:16.213012 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_12/self_attention/out/kernel/v
I0512 23:42:16.232876 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_15/self_attention/query/kernel/v_row
I0512 23:42:16.232913 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_15/self_attention/value/kernel/m
I0512 23:42:16.232949 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_15/self_attention/value/kernel/v
I0512 23:42:16.232984 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_15/self_attention/value/kernel/v_col
I0512 23:42:16.233019 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_15/self_attention/value/kernel/v_row
I0512 23:42:16.233060 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_16/mlp/wi_0/kernel/m
I0512 23:42:16.233100 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_16/mlp/wi_0/kernel/v
I0512 23:42:16.213043 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_12/self_attention/out/kernel/v_col
I0512 23:42:16.213075 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_12/self_attention/out/kernel/v_row
I0512 23:42:16.213108 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_12/self_attention/query/kernel/m
I0512 23:42:16.213139 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_12/self_attention/query/kernel/v
I0512 23:42:16.213171 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_12/self_attention/query/kernel/v_col
I0512 23:42:16.213203 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_12/self_attention/query/kernel/v_row
I0512 23:42:16.213235 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_12/self_attention/value/kernel/m
I0512 23:42:16.213267 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_12/self_attention/value/kernel/v
I0512 23:42:16.234229 139674595104768 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_17/self_attention/key/kernel
I0512 23:42:16.234259 139674595104768 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_17/self_attention/out/kernel
I0512 23:42:16.234289 139674595104768 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_17/self_attention/query/kernel
I0512 23:42:16.234319 139674595104768 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_17/self_attention/value/kernel
I0512 23:42:16.234348 139674595104768 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_18/mlp/wi_0/kernel
I0512 23:42:16.234378 139674595104768 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_18/mlp/wi_1/kernel
I0512 23:42:16.234408 139674595104768 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_18/mlp/wo/kernel
I0512 23:42:16.234438 139674595104768 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_18/pre_mlp_layer_norm/scale
I0512 23:42:16.234467 139674595104768 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_18/pre_self_attention_layer_norm/scale
I0512 23:42:16.234497 139674595104768 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_18/self_attention/key/kernel
I0512 23:42:16.234527 139674595104768 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_18/self_attention/out/kernel
I0512 23:42:16.234557 139674595104768 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_18/self_attention/query/kernel
I0512 23:42:16.234588 139674595104768 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_18/self_attention/value/kernel
I0512 23:42:16.234619 139674595104768 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_19/mlp/wi_0/kernel
I0512 23:42:16.234649 139674595104768 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_19/mlp/wi_1/kernel
I0512 23:42:16.234679 139674595104768 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_19/mlp/wo/kernel
I0512 23:42:16.233142 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_16/mlp/wi_0/kernel/v_col
I0512 23:42:16.233179 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_16/mlp/wi_0/kernel/v_row
I0512 23:42:16.233219 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_16/mlp/wi_1/kernel/m
I0512 23:42:16.233256 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_16/mlp/wi_1/kernel/v
I0512 23:42:16.233290 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_16/mlp/wi_1/kernel/v_col
I0512 23:42:16.233324 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_16/mlp/wi_1/kernel/v_row
I0512 23:42:16.233358 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_16/mlp/wo/kernel/m
I0512 23:42:16.233392 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_16/mlp/wo/kernel/v
I0512 23:42:16.233426 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_16/mlp/wo/kernel/v_col
I0512 23:42:16.233461 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_16/mlp/wo/kernel/v_row
I0512 23:42:16.233494 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_16/pre_mlp_layer_norm/scale/m
I0512 23:42:16.233529 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_16/pre_mlp_layer_norm/scale/v
I0512 23:42:16.233562 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_16/pre_mlp_layer_norm/scale/v_col
I0512 23:42:16.233596 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_16/pre_mlp_layer_norm/scale/v_row
I0512 23:42:16.233630 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_16/pre_self_attention_layer_norm/scale/m
I0512 23:42:16.213299 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_12/self_attention/value/kernel/v_col
I0512 23:42:16.213332 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_12/self_attention/value/kernel/v_row
I0512 23:42:16.213365 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_13/mlp/wi_0/kernel/m
I0512 23:42:16.213397 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_13/mlp/wi_0/kernel/v
I0512 23:42:16.213431 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_13/mlp/wi_0/kernel/v_col
I0512 23:42:16.213464 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_13/mlp/wi_0/kernel/v_row
I0512 23:42:16.213495 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_13/mlp/wi_1/kernel/m
I0512 23:42:16.213528 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_13/mlp/wi_1/kernel/v
I0512 23:42:16.213560 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_13/mlp/wi_1/kernel/v_col
I0512 23:42:16.213592 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_13/mlp/wi_1/kernel/v_row
I0512 23:42:16.213624 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_13/mlp/wo/kernel/m
I0512 23:42:16.213660 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_13/mlp/wo/kernel/v
I0512 23:42:16.213694 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_13/mlp/wo/kernel/v_col
I0512 23:42:16.213726 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_13/mlp/wo/kernel/v_row
I0512 23:42:16.213805 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_13/pre_mlp_layer_norm/scale/m
I0512 23:42:16.234709 139674595104768 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_19/pre_mlp_layer_norm/scale
I0512 23:42:16.234738 139674595104768 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_19/pre_self_attention_layer_norm/scale
I0512 23:42:16.234768 139674595104768 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_19/self_attention/key/kernel
I0512 23:42:16.234798 139674595104768 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_19/self_attention/out/kernel
I0512 23:42:16.234827 139674595104768 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_19/self_attention/query/kernel
I0512 23:42:16.234863 139674595104768 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_19/self_attention/value/kernel
I0512 23:42:16.234894 139674595104768 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_2/mlp/wi_0/kernel
I0512 23:42:16.234923 139674595104768 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_2/mlp/wi_1/kernel
I0512 23:42:16.234953 139674595104768 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_2/mlp/wo/kernel
I0512 23:42:16.234983 139674595104768 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_2/pre_mlp_layer_norm/scale
I0512 23:42:16.235013 139674595104768 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_2/pre_self_attention_layer_norm/scale
I0512 23:42:16.235043 139674595104768 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_2/self_attention/key/kernel
I0512 23:42:16.235073 139674595104768 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_2/self_attention/out/kernel
I0512 23:42:16.235103 139674595104768 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_2/self_attention/query/kernel
I0512 23:42:16.235133 139674595104768 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_2/self_attention/value/kernel
I0512 23:42:16.235164 139674595104768 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_20/mlp/wi_0/kernel
I0512 23:42:16.213842 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_13/pre_mlp_layer_norm/scale/v
I0512 23:42:16.213875 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_13/pre_mlp_layer_norm/scale/v_col
I0512 23:42:16.213909 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_13/pre_mlp_layer_norm/scale/v_row
I0512 23:42:16.213944 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_13/pre_self_attention_layer_norm/scale/m
I0512 23:42:16.213976 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_13/pre_self_attention_layer_norm/scale/v
I0512 23:42:16.214008 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_13/pre_self_attention_layer_norm/scale/v_col
I0512 23:42:16.214040 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_13/pre_self_attention_layer_norm/scale/v_row
I0512 23:42:16.214071 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_13/self_attention/key/kernel/m
I0512 23:42:16.214104 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_13/self_attention/key/kernel/v
I0512 23:42:16.214136 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_13/self_attention/key/kernel/v_col
I0512 23:42:16.214168 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_13/self_attention/key/kernel/v_row
I0512 23:42:16.214199 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_13/self_attention/out/kernel/m
I0512 23:42:16.214231 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_13/self_attention/out/kernel/v
I0512 23:42:16.214262 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_13/self_attention/out/kernel/v_col
I0512 23:42:16.233664 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_16/pre_self_attention_layer_norm/scale/v
I0512 23:42:16.233699 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_16/pre_self_attention_layer_norm/scale/v_col
I0512 23:42:16.233734 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_16/pre_self_attention_layer_norm/scale/v_row
I0512 23:42:16.233769 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_16/self_attention/key/kernel/m
I0512 23:42:16.233803 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_16/self_attention/key/kernel/v
I0512 23:42:16.233837 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_16/self_attention/key/kernel/v_col
I0512 23:42:16.233870 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_16/self_attention/key/kernel/v_row
I0512 23:42:16.233906 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_16/self_attention/out/kernel/m
I0512 23:42:16.233939 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_16/self_attention/out/kernel/v
I0512 23:42:16.233973 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_16/self_attention/out/kernel/v_col
I0512 23:42:16.234006 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_16/self_attention/out/kernel/v_row
I0512 23:42:16.234040 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_16/self_attention/query/kernel/m
I0512 23:42:16.234073 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_16/self_attention/query/kernel/v
I0512 23:42:16.234107 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_16/self_attention/query/kernel/v_col
I0512 23:42:16.234146 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_16/self_attention/query/kernel/v_row
I0512 23:42:16.235194 139674595104768 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_20/mlp/wi_1/kernel
I0512 23:42:16.235225 139674595104768 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_20/mlp/wo/kernel
I0512 23:42:16.235255 139674595104768 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_20/pre_mlp_layer_norm/scale
I0512 23:42:16.235303 139674595104768 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_20/pre_self_attention_layer_norm/scale
I0512 23:42:16.235340 139674595104768 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_20/self_attention/key/kernel
I0512 23:42:16.235372 139674595104768 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_20/self_attention/out/kernel
I0512 23:42:16.235403 139674595104768 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_20/self_attention/query/kernel
I0512 23:42:16.235433 139674595104768 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_20/self_attention/value/kernel
I0512 23:42:16.235463 139674595104768 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_21/mlp/wi_0/kernel
I0512 23:42:16.235494 139674595104768 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_21/mlp/wi_1/kernel
I0512 23:42:16.235524 139674595104768 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_21/mlp/wo/kernel
I0512 23:42:16.235554 139674595104768 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_21/pre_mlp_layer_norm/scale
I0512 23:42:16.235586 139674595104768 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_21/pre_self_attention_layer_norm/scale
I0512 23:42:16.235618 139674595104768 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_21/self_attention/key/kernel
I0512 23:42:16.235649 139674595104768 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_21/self_attention/out/kernel
I0512 23:42:16.235679 139674595104768 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_21/self_attention/query/kernel
I0512 23:42:16.234181 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_16/self_attention/value/kernel/m
I0512 23:42:16.234214 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_16/self_attention/value/kernel/v
I0512 23:42:16.234248 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_16/self_attention/value/kernel/v_col
I0512 23:42:16.234281 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_16/self_attention/value/kernel/v_row
I0512 23:42:16.234315 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/extra_mlp/wi_0/kernel/m
I0512 23:42:16.234349 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/extra_mlp/wi_0/kernel/v
I0512 23:42:16.234382 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/extra_mlp/wi_0/kernel/v_col
I0512 23:42:16.214294 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_13/self_attention/out/kernel/v_row
I0512 23:42:16.214325 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_13/self_attention/query/kernel/m
I0512 23:42:16.214357 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_13/self_attention/query/kernel/v
I0512 23:42:16.214389 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_13/self_attention/query/kernel/v_col
I0512 23:42:16.214422 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_13/self_attention/query/kernel/v_row
I0512 23:42:16.214455 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_13/self_attention/value/kernel/m
I0512 23:42:16.214486 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_13/self_attention/value/kernel/v
I0512 23:42:16.234416 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/extra_mlp/wi_0/kernel/v_row
I0512 23:42:16.234450 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/extra_mlp/wi_1/kernel/m
I0512 23:42:16.234483 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/extra_mlp/wi_1/kernel/v
I0512 23:42:16.234517 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/extra_mlp/wi_1/kernel/v_col
I0512 23:42:16.234550 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/extra_mlp/wi_1/kernel/v_row
I0512 23:42:16.234584 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/extra_mlp/wo/kernel/m
I0512 23:42:16.234618 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/extra_mlp/wo/kernel/v
I0512 23:42:16.234652 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/extra_mlp/wo/kernel/v_col
I0512 23:42:16.214518 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_13/self_attention/value/kernel/v_col
I0512 23:42:16.214550 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_13/self_attention/value/kernel/v_row
I0512 23:42:16.214581 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_14/mlp/wi_0/kernel/m
I0512 23:42:16.214613 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_14/mlp/wi_0/kernel/v
I0512 23:42:16.214644 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_14/mlp/wi_0/kernel/v_col
I0512 23:42:16.214682 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_14/mlp/wi_0/kernel/v_row
I0512 23:42:16.214715 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_14/mlp/wi_1/kernel/m
I0512 23:42:16.214747 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_14/mlp/wi_1/kernel/v
I0512 23:42:16.235709 139674595104768 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_21/self_attention/value/kernel
I0512 23:42:16.235739 139674595104768 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_22/mlp/wi_0/kernel
I0512 23:42:16.235770 139674595104768 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_22/mlp/wi_1/kernel
I0512 23:42:16.235800 139674595104768 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_22/mlp/wo/kernel
I0512 23:42:16.235836 139674595104768 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_22/pre_mlp_layer_norm/scale
I0512 23:42:16.235868 139674595104768 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_22/pre_self_attention_layer_norm/scale
I0512 23:42:16.235899 139674595104768 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_22/self_attention/key/kernel
I0512 23:42:16.235929 139674595104768 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_22/self_attention/out/kernel
I0512 23:42:16.235959 139674595104768 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_22/self_attention/query/kernel
I0512 23:42:16.235990 139674595104768 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_22/self_attention/value/kernel
I0512 23:42:16.236021 139674595104768 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_23/extra_mlp/wi_0/kernel
I0512 23:42:16.236051 139674595104768 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_23/extra_mlp/wi_1/kernel
I0512 23:42:16.236081 139674595104768 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_23/extra_mlp/wo/kernel
I0512 23:42:16.236112 139674595104768 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_23/mlp/expert/wi_0/kernel
I0512 23:42:16.236142 139674595104768 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_23/mlp/expert/wi_1/kernel
I0512 23:42:16.236172 139674595104768 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_23/mlp/expert/wo/kernel
I0512 23:42:16.236202 139674595104768 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_23/mlp/router/router_weights/w/kernel
I0512 23:42:16.234686 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/extra_mlp/wo/kernel/v_row
I0512 23:42:16.234720 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/mlp/expert/wi_0/kernel/m
I0512 23:42:16.234753 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/mlp/expert/wi_0/kernel/v
I0512 23:42:16.234786 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/mlp/expert/wi_0/kernel/v_col
I0512 23:42:16.234820 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/mlp/expert/wi_0/kernel/v_row
I0512 23:42:16.234854 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/mlp/expert/wi_1/kernel/m
I0512 23:42:16.234889 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/mlp/expert/wi_1/kernel/v
I0512 23:42:16.234924 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/mlp/expert/wi_1/kernel/v_col
I0512 23:42:16.234958 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/mlp/expert/wi_1/kernel/v_row
I0512 23:42:16.234992 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/mlp/expert/wo/kernel/m
I0512 23:42:16.235026 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/mlp/expert/wo/kernel/v
I0512 23:42:16.235060 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/mlp/expert/wo/kernel/v_col
I0512 23:42:16.235093 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/mlp/expert/wo/kernel/v_row
I0512 23:42:16.235127 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/mlp/router/router_weights/w/kernel/m
I0512 23:42:16.235167 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/mlp/router/router_weights/w/kernel/v
I0512 23:42:16.214780 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_14/mlp/wi_1/kernel/v_col
I0512 23:42:16.214812 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_14/mlp/wi_1/kernel/v_row
I0512 23:42:16.214844 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_14/mlp/wo/kernel/m
I0512 23:42:16.214876 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_14/mlp/wo/kernel/v
I0512 23:42:16.214908 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_14/mlp/wo/kernel/v_col
I0512 23:42:16.214939 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_14/mlp/wo/kernel/v_row
I0512 23:42:16.214971 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_14/pre_mlp_layer_norm/scale/m
I0512 23:42:16.215003 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_14/pre_mlp_layer_norm/scale/v
I0512 23:42:16.215035 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_14/pre_mlp_layer_norm/scale/v_col
I0512 23:42:16.215067 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_14/pre_mlp_layer_norm/scale/v_row
I0512 23:42:16.215099 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_14/pre_self_attention_layer_norm/scale/m
I0512 23:42:16.215130 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_14/pre_self_attention_layer_norm/scale/v
I0512 23:42:16.215162 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_14/pre_self_attention_layer_norm/scale/v_col
I0512 23:42:16.215194 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_14/pre_self_attention_layer_norm/scale/v_row
I0512 23:42:16.215226 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_14/self_attention/key/kernel/m
I0512 23:42:16.236233 139674595104768 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_23/pre_extra_mlp_layer_norm/scale
I0512 23:42:16.236263 139674595104768 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_23/pre_mlp_layer_norm/scale
I0512 23:42:16.236294 139674595104768 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_23/pre_self_attention_layer_norm/scale
I0512 23:42:16.236324 139674595104768 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_23/self_attention/key/kernel
I0512 23:42:16.236355 139674595104768 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_23/self_attention/out/kernel
I0512 23:42:16.236385 139674595104768 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_23/self_attention/query/kernel
I0512 23:42:16.236416 139674595104768 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_23/self_attention/value/kernel
I0512 23:42:16.236446 139674595104768 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_3/mlp/wi_0/kernel
I0512 23:42:16.236476 139674595104768 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_3/mlp/wi_1/kernel
I0512 23:42:16.236507 139674595104768 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_3/mlp/wo/kernel
I0512 23:42:16.236537 139674595104768 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_3/pre_mlp_layer_norm/scale
I0512 23:42:16.236567 139674595104768 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_3/pre_self_attention_layer_norm/scale
I0512 23:42:16.236600 139674595104768 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_3/self_attention/key/kernel
I0512 23:42:16.236632 139674595104768 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_3/self_attention/out/kernel
I0512 23:42:16.236663 139674595104768 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_3/self_attention/query/kernel
I0512 23:42:16.236694 139674595104768 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_3/self_attention/value/kernel
I0512 23:42:16.215258 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_14/self_attention/key/kernel/v
I0512 23:42:16.215289 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_14/self_attention/key/kernel/v_col
I0512 23:42:16.215321 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_14/self_attention/key/kernel/v_row
I0512 23:42:16.215353 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_14/self_attention/out/kernel/m
I0512 23:42:16.215384 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_14/self_attention/out/kernel/v
I0512 23:42:16.215417 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_14/self_attention/out/kernel/v_col
I0512 23:42:16.215450 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_14/self_attention/out/kernel/v_row
I0512 23:42:16.215482 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_14/self_attention/query/kernel/m
I0512 23:42:16.215513 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_14/self_attention/query/kernel/v
I0512 23:42:16.215545 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_14/self_attention/query/kernel/v_col
I0512 23:42:16.215577 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_14/self_attention/query/kernel/v_row
I0512 23:42:16.215608 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_14/self_attention/value/kernel/m
I0512 23:42:16.215640 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_14/self_attention/value/kernel/v
I0512 23:42:16.215677 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_14/self_attention/value/kernel/v_col
I0512 23:42:16.235201 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/mlp/router/router_weights/w/kernel/v_col
I0512 23:42:16.235235 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/mlp/router/router_weights/w/kernel/v_row
I0512 23:42:16.235269 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/pre_extra_mlp_layer_norm/scale/m
I0512 23:42:16.235302 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/pre_extra_mlp_layer_norm/scale/v
I0512 23:42:16.235336 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/pre_extra_mlp_layer_norm/scale/v_col
I0512 23:42:16.235370 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/pre_extra_mlp_layer_norm/scale/v_row
I0512 23:42:16.235404 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/pre_mlp_layer_norm/scale/m
I0512 23:42:16.235437 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/pre_mlp_layer_norm/scale/v
I0512 23:42:16.235471 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/pre_mlp_layer_norm/scale/v_col
I0512 23:42:16.235504 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/pre_mlp_layer_norm/scale/v_row
I0512 23:42:16.235538 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/pre_self_attention_layer_norm/scale/m
I0512 23:42:16.235571 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/pre_self_attention_layer_norm/scale/v
I0512 23:42:16.235605 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/pre_self_attention_layer_norm/scale/v_col
I0512 23:42:16.235639 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/pre_self_attention_layer_norm/scale/v_row
I0512 23:42:16.236725 139674595104768 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_4/mlp/wi_0/kernel
I0512 23:42:16.236755 139674595104768 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_4/mlp/wi_1/kernel
I0512 23:42:16.236786 139674595104768 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_4/mlp/wo/kernel
I0512 23:42:16.236816 139674595104768 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_4/pre_mlp_layer_norm/scale
I0512 23:42:16.236851 139674595104768 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_4/pre_self_attention_layer_norm/scale
I0512 23:42:16.236882 139674595104768 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_4/self_attention/key/kernel
I0512 23:42:16.236912 139674595104768 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_4/self_attention/out/kernel
I0512 23:42:16.236941 139674595104768 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_4/self_attention/query/kernel
I0512 23:42:16.236971 139674595104768 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_4/self_attention/value/kernel
I0512 23:42:16.237001 139674595104768 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_5/extra_mlp/wi_0/kernel
I0512 23:42:16.237031 139674595104768 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_5/extra_mlp/wi_1/kernel
I0512 23:42:16.237060 139674595104768 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_5/extra_mlp/wo/kernel
I0512 23:42:16.237091 139674595104768 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_5/mlp/expert/wi_0/kernel
I0512 23:42:16.237121 139674595104768 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_5/mlp/expert/wi_1/kernel
I0512 23:42:16.237151 139674595104768 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_5/mlp/expert/wo/kernel
I0512 23:42:16.237181 139674595104768 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_5/mlp/router/router_weights/w/kernel
I0512 23:42:16.235673 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/self_attention/key/kernel/m
I0512 23:42:16.235707 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/self_attention/key/kernel/v
I0512 23:42:16.235740 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/self_attention/key/kernel/v_col
I0512 23:42:16.235774 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/self_attention/key/kernel/v_row
I0512 23:42:16.235807 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/self_attention/out/kernel/m
I0512 23:42:16.235841 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/self_attention/out/kernel/v
I0512 23:42:16.235875 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/self_attention/out/kernel/v_col
I0512 23:42:16.235911 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/self_attention/out/kernel/v_row
I0512 23:42:16.235945 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/self_attention/query/kernel/m
I0512 23:42:16.235979 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/self_attention/query/kernel/v
I0512 23:42:16.236012 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/self_attention/query/kernel/v_col
I0512 23:42:16.236046 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/self_attention/query/kernel/v_row
I0512 23:42:16.236080 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/self_attention/value/kernel/m
I0512 23:42:16.236113 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/self_attention/value/kernel/v
I0512 23:42:16.215710 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_14/self_attention/value/kernel/v_row
I0512 23:42:16.215741 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_15/mlp/wi_0/kernel/m
I0512 23:42:16.215773 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_15/mlp/wi_0/kernel/v
I0512 23:42:16.215805 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_15/mlp/wi_0/kernel/v_col
I0512 23:42:16.215837 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_15/mlp/wi_0/kernel/v_row
I0512 23:42:16.215868 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_15/mlp/wi_1/kernel/m
I0512 23:42:16.215900 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_15/mlp/wi_1/kernel/v
I0512 23:42:16.215932 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_15/mlp/wi_1/kernel/v_col
I0512 23:42:16.215964 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_15/mlp/wi_1/kernel/v_row
I0512 23:42:16.215996 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_15/mlp/wo/kernel/m
I0512 23:42:16.216027 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_15/mlp/wo/kernel/v
I0512 23:42:16.216059 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_15/mlp/wo/kernel/v_col
I0512 23:42:16.216091 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_15/mlp/wo/kernel/v_row
I0512 23:42:16.216122 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_15/pre_mlp_layer_norm/scale/m
I0512 23:42:16.216154 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_15/pre_mlp_layer_norm/scale/v
I0512 23:42:16.216186 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_15/pre_mlp_layer_norm/scale/v_col
I0512 23:42:16.237211 139674595104768 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_5/pre_extra_mlp_layer_norm/scale
I0512 23:42:16.237241 139674595104768 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_5/pre_mlp_layer_norm/scale
I0512 23:42:16.237271 139674595104768 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_5/pre_self_attention_layer_norm/scale
I0512 23:42:16.237300 139674595104768 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_5/self_attention/key/kernel
I0512 23:42:16.237330 139674595104768 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_5/self_attention/out/kernel
I0512 23:42:16.237360 139674595104768 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_5/self_attention/query/kernel
I0512 23:42:16.237390 139674595104768 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_5/self_attention/value/kernel
I0512 23:42:16.237420 139674595104768 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_6/mlp/wi_0/kernel
I0512 23:42:16.237450 139674595104768 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_6/mlp/wi_1/kernel
I0512 23:42:16.237480 139674595104768 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_6/mlp/wo/kernel
I0512 23:42:16.237509 139674595104768 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_6/pre_mlp_layer_norm/scale
I0512 23:42:16.237539 139674595104768 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_6/pre_self_attention_layer_norm/scale
I0512 23:42:16.237569 139674595104768 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_6/self_attention/key/kernel
I0512 23:42:16.237601 139674595104768 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_6/self_attention/out/kernel
I0512 23:42:16.237632 139674595104768 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_6/self_attention/query/kernel
I0512 23:42:16.237661 139674595104768 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_6/self_attention/value/kernel
I0512 23:42:16.216217 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_15/pre_mlp_layer_norm/scale/v_row
I0512 23:42:16.216249 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_15/pre_self_attention_layer_norm/scale/m
I0512 23:42:16.216281 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_15/pre_self_attention_layer_norm/scale/v
I0512 23:42:16.216313 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_15/pre_self_attention_layer_norm/scale/v_col
I0512 23:42:16.216345 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_15/pre_self_attention_layer_norm/scale/v_row
I0512 23:42:16.216376 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_15/self_attention/key/kernel/m
I0512 23:42:16.216409 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_15/self_attention/key/kernel/v
I0512 23:42:16.216443 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_15/self_attention/key/kernel/v_col
I0512 23:42:16.216475 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_15/self_attention/key/kernel/v_row
I0512 23:42:16.216507 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_15/self_attention/out/kernel/m
I0512 23:42:16.216539 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_15/self_attention/out/kernel/v
I0512 23:42:16.216570 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_15/self_attention/out/kernel/v_col
I0512 23:42:16.216602 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_15/self_attention/out/kernel/v_row
I0512 23:42:16.216634 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_15/self_attention/query/kernel/m
I0512 23:42:16.236152 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/self_attention/value/kernel/v_col
I0512 23:42:16.236187 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/self_attention/value/kernel/v_row
I0512 23:42:16.236220 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_18/mlp/wi_0/kernel/m
I0512 23:42:16.236254 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_18/mlp/wi_0/kernel/v
I0512 23:42:16.236288 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_18/mlp/wi_0/kernel/v_col
I0512 23:42:16.236321 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_18/mlp/wi_0/kernel/v_row
I0512 23:42:16.236355 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_18/mlp/wi_1/kernel/m
I0512 23:42:16.236389 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_18/mlp/wi_1/kernel/v
I0512 23:42:16.236443 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_18/mlp/wi_1/kernel/v_col
I0512 23:42:16.236479 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_18/mlp/wi_1/kernel/v_row
I0512 23:42:16.236513 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_18/mlp/wo/kernel/m
I0512 23:42:16.236547 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_18/mlp/wo/kernel/v
I0512 23:42:16.236580 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_18/mlp/wo/kernel/v_col
I0512 23:42:16.236614 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_18/mlp/wo/kernel/v_row
I0512 23:42:16.236647 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_18/pre_mlp_layer_norm/scale/m
I0512 23:42:16.236681 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_18/pre_mlp_layer_norm/scale/v
I0512 23:42:16.216670 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_15/self_attention/query/kernel/v
I0512 23:42:16.216703 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_15/self_attention/query/kernel/v_col
I0512 23:42:16.216735 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_15/self_attention/query/kernel/v_row
I0512 23:42:16.216767 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_15/self_attention/value/kernel/m
I0512 23:42:16.216799 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_15/self_attention/value/kernel/v
I0512 23:42:16.216830 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_15/self_attention/value/kernel/v_col
I0512 23:42:16.216861 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_15/self_attention/value/kernel/v_row
I0512 23:42:16.216900 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_16/mlp/wi_0/kernel/m
I0512 23:42:16.216943 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_16/mlp/wi_0/kernel/v
I0512 23:42:16.237691 139674595104768 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_7/mlp/wi_0/kernel
I0512 23:42:16.237721 139674595104768 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_7/mlp/wi_1/kernel
I0512 23:42:16.237750 139674595104768 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_7/mlp/wo/kernel
I0512 23:42:16.237780 139674595104768 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_7/pre_mlp_layer_norm/scale
I0512 23:42:16.237810 139674595104768 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_7/pre_self_attention_layer_norm/scale
I0512 23:42:16.237844 139674595104768 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_7/self_attention/key/kernel
I0512 23:42:16.237875 139674595104768 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_7/self_attention/out/kernel
I0512 23:42:16.237906 139674595104768 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_7/self_attention/query/kernel
I0512 23:42:16.237935 139674595104768 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_7/self_attention/value/kernel
I0512 23:42:16.237965 139674595104768 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_8/mlp/wi_0/kernel
I0512 23:42:16.237994 139674595104768 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_8/mlp/wi_1/kernel
I0512 23:42:16.238024 139674595104768 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_8/mlp/wo/kernel
I0512 23:42:16.238054 139674595104768 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_8/pre_mlp_layer_norm/scale
I0512 23:42:16.238084 139674595104768 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_8/pre_self_attention_layer_norm/scale
I0512 23:42:16.238114 139674595104768 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_8/self_attention/key/kernel
I0512 23:42:16.238143 139674595104768 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_8/self_attention/out/kernel
I0512 23:42:16.236715 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_18/pre_mlp_layer_norm/scale/v_col
I0512 23:42:16.236749 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_18/pre_mlp_layer_norm/scale/v_row
I0512 23:42:16.236782 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_18/pre_self_attention_layer_norm/scale/m
I0512 23:42:16.236816 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_18/pre_self_attention_layer_norm/scale/v
I0512 23:42:16.236850 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_18/pre_self_attention_layer_norm/scale/v_col
I0512 23:42:16.236884 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_18/pre_self_attention_layer_norm/scale/v_row
I0512 23:42:16.236920 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_18/self_attention/key/kernel/m
I0512 23:42:16.236954 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_18/self_attention/key/kernel/v
I0512 23:42:16.236988 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_18/self_attention/key/kernel/v_col
I0512 23:42:16.237022 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_18/self_attention/key/kernel/v_row
I0512 23:42:16.237056 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_18/self_attention/out/kernel/m
I0512 23:42:16.237089 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_18/self_attention/out/kernel/v
I0512 23:42:16.237123 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_18/self_attention/out/kernel/v_col
I0512 23:42:16.237163 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_18/self_attention/out/kernel/v_row
I0512 23:42:16.216977 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_16/mlp/wi_0/kernel/v_col
I0512 23:42:16.217009 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_16/mlp/wi_0/kernel/v_row
I0512 23:42:16.217048 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_16/mlp/wi_1/kernel/m
I0512 23:42:16.217082 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_16/mlp/wi_1/kernel/v
I0512 23:42:16.217114 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_16/mlp/wi_1/kernel/v_col
I0512 23:42:16.217146 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_16/mlp/wi_1/kernel/v_row
I0512 23:42:16.217177 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_16/mlp/wo/kernel/m
I0512 23:42:16.217208 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_16/mlp/wo/kernel/v
I0512 23:42:16.217239 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_16/mlp/wo/kernel/v_col
I0512 23:42:16.217271 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_16/mlp/wo/kernel/v_row
I0512 23:42:16.217302 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_16/pre_mlp_layer_norm/scale/m
I0512 23:42:16.217333 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_16/pre_mlp_layer_norm/scale/v
I0512 23:42:16.217364 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_16/pre_mlp_layer_norm/scale/v_col
I0512 23:42:16.217396 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_16/pre_mlp_layer_norm/scale/v_row
I0512 23:42:16.217429 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_16/pre_self_attention_layer_norm/scale/m
I0512 23:42:16.217461 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_16/pre_self_attention_layer_norm/scale/v
I0512 23:42:16.217494 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_16/pre_self_attention_layer_norm/scale/v_col
I0512 23:42:16.217527 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_16/pre_self_attention_layer_norm/scale/v_row
I0512 23:42:16.217559 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_16/self_attention/key/kernel/m
I0512 23:42:16.217590 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_16/self_attention/key/kernel/v
I0512 23:42:16.237196 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_18/self_attention/query/kernel/m
I0512 23:42:16.237230 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_18/self_attention/query/kernel/v
I0512 23:42:16.237263 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_18/self_attention/query/kernel/v_col
I0512 23:42:16.237298 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_18/self_attention/query/kernel/v_row
I0512 23:42:16.237331 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_18/self_attention/value/kernel/m
I0512 23:42:16.237365 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_18/self_attention/value/kernel/v
I0512 23:42:16.237398 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_18/self_attention/value/kernel/v_col
I0512 23:42:16.237431 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_18/self_attention/value/kernel/v_row
I0512 23:42:16.237465 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_19/mlp/wi_0/kernel/m
I0512 23:42:16.237499 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_19/mlp/wi_0/kernel/v
I0512 23:42:16.237533 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_19/mlp/wi_0/kernel/v_col
I0512 23:42:16.237566 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_19/mlp/wi_0/kernel/v_row
I0512 23:42:16.237600 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_19/mlp/wi_1/kernel/m
I0512 23:42:16.237634 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_19/mlp/wi_1/kernel/v
I0512 23:42:16.237668 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_19/mlp/wi_1/kernel/v_col
I0512 23:42:16.217621 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_16/self_attention/key/kernel/v_col
I0512 23:42:16.217651 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_16/self_attention/key/kernel/v_row
I0512 23:42:16.217689 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_16/self_attention/out/kernel/m
I0512 23:42:16.217720 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_16/self_attention/out/kernel/v
I0512 23:42:16.217751 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_16/self_attention/out/kernel/v_col
I0512 23:42:16.217804 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_16/self_attention/out/kernel/v_row
I0512 23:42:16.217838 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_16/self_attention/query/kernel/m
I0512 23:42:16.217870 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_16/self_attention/query/kernel/v
I0512 23:42:16.217901 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_16/self_attention/query/kernel/v_col
I0512 23:42:16.217932 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_16/self_attention/query/kernel/v_row
I0512 23:42:16.217963 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_16/self_attention/value/kernel/m
I0512 23:42:16.238173 139674595104768 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_8/self_attention/query/kernel
I0512 23:42:16.238202 139674595104768 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_8/self_attention/value/kernel
I0512 23:42:16.238232 139674595104768 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_9/mlp/wi_0/kernel
I0512 23:42:16.238262 139674595104768 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_9/mlp/wi_1/kernel
I0512 23:42:16.238292 139674595104768 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_9/mlp/wo/kernel
I0512 23:42:16.238322 139674595104768 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_9/pre_mlp_layer_norm/scale
I0512 23:42:16.238351 139674595104768 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_9/pre_self_attention_layer_norm/scale
I0512 23:42:16.238381 139674595104768 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_9/self_attention/key/kernel
I0512 23:42:16.238411 139674595104768 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_9/self_attention/out/kernel
I0512 23:42:16.238440 139674595104768 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_9/self_attention/query/kernel
I0512 23:42:16.238470 139674595104768 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_9/self_attention/value/kernel
I0512 23:42:16.238499 139674595104768 checkpoints.py:1100] Restoring key from ckpt: target/decoder/logits_dense/kernel
I0512 23:42:16.238529 139674595104768 checkpoints.py:1100] Restoring key from ckpt: target/token_embedder/embedding
I0512 23:42:16.237701 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_19/mlp/wi_1/kernel/v_row
I0512 23:42:16.237735 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_19/mlp/wo/kernel/m
I0512 23:42:16.237769 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_19/mlp/wo/kernel/v
I0512 23:42:16.237802 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_19/mlp/wo/kernel/v_col
I0512 23:42:16.237836 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_19/mlp/wo/kernel/v_row
I0512 23:42:16.237869 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_19/pre_mlp_layer_norm/scale/m
I0512 23:42:16.237905 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_19/pre_mlp_layer_norm/scale/v
I0512 23:42:16.217994 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_16/self_attention/value/kernel/v
I0512 23:42:16.218025 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_16/self_attention/value/kernel/v_col
I0512 23:42:16.218056 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_16/self_attention/value/kernel/v_row
I0512 23:42:16.218087 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/extra_mlp/wi_0/kernel/m
I0512 23:42:16.218118 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/extra_mlp/wi_0/kernel/v
I0512 23:42:16.218149 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/extra_mlp/wi_0/kernel/v_col
I0512 23:42:16.218180 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/extra_mlp/wi_0/kernel/v_row
I0512 23:42:16.237940 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_19/pre_mlp_layer_norm/scale/v_col
I0512 23:42:16.237973 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_19/pre_mlp_layer_norm/scale/v_row
I0512 23:42:16.238008 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_19/pre_self_attention_layer_norm/scale/m
I0512 23:42:16.238041 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_19/pre_self_attention_layer_norm/scale/v
I0512 23:42:16.238075 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_19/pre_self_attention_layer_norm/scale/v_col
I0512 23:42:16.238109 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_19/pre_self_attention_layer_norm/scale/v_row
I0512 23:42:16.238147 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_19/self_attention/key/kernel/m
I0512 23:42:16.218211 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/extra_mlp/wi_1/kernel/m
I0512 23:42:16.218241 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/extra_mlp/wi_1/kernel/v
I0512 23:42:16.218272 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/extra_mlp/wi_1/kernel/v_col
I0512 23:42:16.218303 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/extra_mlp/wi_1/kernel/v_row
I0512 23:42:16.218334 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/extra_mlp/wo/kernel/m
I0512 23:42:16.218366 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/extra_mlp/wo/kernel/v
I0512 23:42:16.218398 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/extra_mlp/wo/kernel/v_col
I0512 23:42:16.218432 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/extra_mlp/wo/kernel/v_row
I0512 23:42:16.218465 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/mlp/expert/wi_0/kernel/m
I0512 23:42:16.218496 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/mlp/expert/wi_0/kernel/v
I0512 23:42:16.218527 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/mlp/expert/wi_0/kernel/v_col
I0512 23:42:16.218558 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/mlp/expert/wi_0/kernel/v_row
I0512 23:42:16.218589 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/mlp/expert/wi_1/kernel/m
I0512 23:42:16.218620 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/mlp/expert/wi_1/kernel/v
I0512 23:42:16.218651 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/mlp/expert/wi_1/kernel/v_col
I0512 23:42:16.218690 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/mlp/expert/wi_1/kernel/v_row
I0512 23:42:16.218721 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/mlp/expert/wo/kernel/m
I0512 23:42:16.218753 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/mlp/expert/wo/kernel/v
I0512 23:42:16.238182 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_19/self_attention/key/kernel/v
I0512 23:42:16.238216 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_19/self_attention/key/kernel/v_col
I0512 23:42:16.238250 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_19/self_attention/key/kernel/v_row
I0512 23:42:16.238283 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_19/self_attention/out/kernel/m
I0512 23:42:16.238317 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_19/self_attention/out/kernel/v
I0512 23:42:16.238350 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_19/self_attention/out/kernel/v_col
I0512 23:42:16.238384 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_19/self_attention/out/kernel/v_row
I0512 23:42:16.238418 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_19/self_attention/query/kernel/m
I0512 23:42:16.238451 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_19/self_attention/query/kernel/v
I0512 23:42:16.238510 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_19/self_attention/query/kernel/v_col
I0512 23:42:16.238545 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_19/self_attention/query/kernel/v_row
I0512 23:42:16.238579 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_19/self_attention/value/kernel/m
I0512 23:42:16.238612 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_19/self_attention/value/kernel/v
I0512 23:42:16.238646 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_19/self_attention/value/kernel/v_col
I0512 23:42:16.238680 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_19/self_attention/value/kernel/v_row
I0512 23:42:16.240871 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/decoder_norm/scale/m
I0512 23:42:16.241042 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/decoder_norm/scale/v
I0512 23:42:16.241083 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/decoder_norm/scale/v_col
I0512 23:42:16.218784 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/mlp/expert/wo/kernel/v_col
I0512 23:42:16.218815 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/mlp/expert/wo/kernel/v_row
I0512 23:42:16.218847 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/mlp/router/router_weights/w/kernel/m
I0512 23:42:16.218878 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/mlp/router/router_weights/w/kernel/v
I0512 23:42:16.218909 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/mlp/router/router_weights/w/kernel/v_col
I0512 23:42:16.218940 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/mlp/router/router_weights/w/kernel/v_row
I0512 23:42:16.218971 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/pre_extra_mlp_layer_norm/scale/m
I0512 23:42:16.241118 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/decoder_norm/scale/v_row
I0512 23:42:16.241152 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_0/mlp/wi_0/kernel/m
I0512 23:42:16.241186 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_0/mlp/wi_0/kernel/v
I0512 23:42:16.241219 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_0/mlp/wi_0/kernel/v_col
I0512 23:42:16.241253 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_0/mlp/wi_0/kernel/v_row
I0512 23:42:16.241290 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_0/mlp/wi_1/kernel/m
I0512 23:42:16.241324 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_0/mlp/wi_1/kernel/v
I0512 23:42:16.219002 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/pre_extra_mlp_layer_norm/scale/v
I0512 23:42:16.219033 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/pre_extra_mlp_layer_norm/scale/v_col
I0512 23:42:16.219064 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/pre_extra_mlp_layer_norm/scale/v_row
I0512 23:42:16.219095 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/pre_mlp_layer_norm/scale/m
I0512 23:42:16.219126 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/pre_mlp_layer_norm/scale/v
I0512 23:42:16.219157 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/pre_mlp_layer_norm/scale/v_col
I0512 23:42:16.238713 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_2/mlp/wi_0/kernel/m
I0512 23:42:16.238747 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_2/mlp/wi_0/kernel/v
I0512 23:42:16.238781 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_2/mlp/wi_0/kernel/v_col
I0512 23:42:16.238816 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_2/mlp/wi_0/kernel/v_row
I0512 23:42:16.238850 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_2/mlp/wi_1/kernel/m
I0512 23:42:16.238884 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_2/mlp/wi_1/kernel/v
I0512 23:42:16.238919 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_2/mlp/wi_1/kernel/v_col
I0512 23:42:16.238953 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_2/mlp/wi_1/kernel/v_row
I0512 23:42:16.238986 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_2/mlp/wo/kernel/m
I0512 23:42:16.239020 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_2/mlp/wo/kernel/v
I0512 23:42:16.239053 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_2/mlp/wo/kernel/v_col
I0512 23:42:16.239087 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_2/mlp/wo/kernel/v_row
I0512 23:42:16.239120 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_2/pre_mlp_layer_norm/scale/m
I0512 23:42:16.239161 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_2/pre_mlp_layer_norm/scale/v
I0512 23:42:16.239195 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_2/pre_mlp_layer_norm/scale/v_col
I0512 23:42:16.241357 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_0/mlp/wi_1/kernel/v_col
I0512 23:42:16.241390 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_0/mlp/wi_1/kernel/v_row
I0512 23:42:16.241423 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_0/mlp/wo/kernel/m
I0512 23:42:16.241457 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_0/mlp/wo/kernel/v
I0512 23:42:16.241491 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_0/mlp/wo/kernel/v_col
I0512 23:42:16.219189 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/pre_mlp_layer_norm/scale/v_row
I0512 23:42:16.219220 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/pre_self_attention_layer_norm/scale/m
I0512 23:42:16.219251 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/pre_self_attention_layer_norm/scale/v
I0512 23:42:16.219282 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/pre_self_attention_layer_norm/scale/v_col
I0512 23:42:16.219313 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/pre_self_attention_layer_norm/scale/v_row
I0512 23:42:16.219345 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/self_attention/key/kernel/m
I0512 23:42:16.219376 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/self_attention/key/kernel/v
I0512 23:42:16.219408 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/self_attention/key/kernel/v_col
I0512 23:42:16.219441 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/self_attention/key/kernel/v_row
I0512 23:42:16.219472 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/self_attention/out/kernel/m
I0512 23:42:16.219504 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/self_attention/out/kernel/v
I0512 23:42:16.219535 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/self_attention/out/kernel/v_col
I0512 23:42:16.219566 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/self_attention/out/kernel/v_row
I0512 23:42:16.241524 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_0/mlp/wo/kernel/v_row
I0512 23:42:16.241557 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_0/pre_mlp_layer_norm/scale/m
I0512 23:42:16.241590 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_0/pre_mlp_layer_norm/scale/v
I0512 23:42:16.241624 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_0/pre_mlp_layer_norm/scale/v_col
I0512 23:42:16.241657 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_0/pre_mlp_layer_norm/scale/v_row
I0512 23:42:16.241690 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_0/pre_self_attention_layer_norm/scale/m
I0512 23:42:16.241724 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_0/pre_self_attention_layer_norm/scale/v
I0512 23:42:16.241757 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_0/pre_self_attention_layer_norm/scale/v_col
I0512 23:42:16.241790 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_0/pre_self_attention_layer_norm/scale/v_row
I0512 23:42:16.241823 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_0/self_attention/key/kernel/m
I0512 23:42:16.239229 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_2/pre_mlp_layer_norm/scale/v_row
I0512 23:42:16.239263 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_2/pre_self_attention_layer_norm/scale/m
I0512 23:42:16.239296 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_2/pre_self_attention_layer_norm/scale/v
I0512 23:42:16.239330 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_2/pre_self_attention_layer_norm/scale/v_col
I0512 23:42:16.239363 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_2/pre_self_attention_layer_norm/scale/v_row
I0512 23:42:16.239397 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_2/self_attention/key/kernel/m
I0512 23:42:16.239431 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_2/self_attention/key/kernel/v
I0512 23:42:16.239465 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_2/self_attention/key/kernel/v_col
I0512 23:42:16.239499 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_2/self_attention/key/kernel/v_row
I0512 23:42:16.239532 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_2/self_attention/out/kernel/m
I0512 23:42:16.239565 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_2/self_attention/out/kernel/v
I0512 23:42:16.239598 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_2/self_attention/out/kernel/v_col
I0512 23:42:16.239632 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_2/self_attention/out/kernel/v_row
I0512 23:42:16.239666 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_2/self_attention/query/kernel/m
I0512 23:42:16.239700 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_2/self_attention/query/kernel/v
I0512 23:42:16.241857 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_0/self_attention/key/kernel/v
I0512 23:42:16.241890 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_0/self_attention/key/kernel/v_col
I0512 23:42:16.241923 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_0/self_attention/key/kernel/v_row
I0512 23:42:16.241956 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_0/self_attention/out/kernel/m
I0512 23:42:16.241989 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_0/self_attention/out/kernel/v
I0512 23:42:16.242028 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_0/self_attention/out/kernel/v_col
I0512 23:42:16.219597 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/self_attention/query/kernel/m
I0512 23:42:16.219628 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/self_attention/query/kernel/v
I0512 23:42:16.219664 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/self_attention/query/kernel/v_col
I0512 23:42:16.219697 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/self_attention/query/kernel/v_row
I0512 23:42:16.219728 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/self_attention/value/kernel/m
I0512 23:42:16.219760 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/self_attention/value/kernel/v
I0512 23:42:16.219791 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/self_attention/value/kernel/v_col
I0512 23:42:16.219822 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/self_attention/value/kernel/v_row
I0512 23:42:16.219853 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_18/mlp/wi_0/kernel/m
I0512 23:42:16.219884 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_18/mlp/wi_0/kernel/v
I0512 23:42:16.219916 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_18/mlp/wi_0/kernel/v_col
I0512 23:42:16.219946 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_18/mlp/wi_0/kernel/v_row
I0512 23:42:16.219977 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_18/mlp/wi_1/kernel/m
I0512 23:42:16.220008 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_18/mlp/wi_1/kernel/v
I0512 23:42:16.242063 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_0/self_attention/out/kernel/v_row
I0512 23:42:16.242097 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_0/self_attention/query/kernel/m
I0512 23:42:16.242130 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_0/self_attention/query/kernel/v
I0512 23:42:16.242163 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_0/self_attention/query/kernel/v_col
I0512 23:42:16.242196 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_0/self_attention/query/kernel/v_row
I0512 23:42:16.242229 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_0/self_attention/value/kernel/m
I0512 23:42:16.242282 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_0/self_attention/value/kernel/v
I0512 23:42:16.239733 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_2/self_attention/query/kernel/v_col
I0512 23:42:16.239766 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_2/self_attention/query/kernel/v_row
I0512 23:42:16.239801 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_2/self_attention/value/kernel/m
I0512 23:42:16.239835 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_2/self_attention/value/kernel/v
I0512 23:42:16.239868 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_2/self_attention/value/kernel/v_col
I0512 23:42:16.239904 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_2/self_attention/value/kernel/v_row
I0512 23:42:16.239938 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_20/mlp/wi_0/kernel/m
I0512 23:42:16.239972 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_20/mlp/wi_0/kernel/v
I0512 23:42:16.240006 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_20/mlp/wi_0/kernel/v_col
I0512 23:42:16.240040 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_20/mlp/wi_0/kernel/v_row
I0512 23:42:16.240073 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_20/mlp/wi_1/kernel/m
I0512 23:42:16.240107 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_20/mlp/wi_1/kernel/v
I0512 23:42:16.240144 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_20/mlp/wi_1/kernel/v_col
I0512 23:42:16.240179 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_20/mlp/wi_1/kernel/v_row
I0512 23:42:16.240212 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_20/mlp/wo/kernel/m
I0512 23:42:16.220038 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_18/mlp/wi_1/kernel/v_col
I0512 23:42:16.220069 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_18/mlp/wi_1/kernel/v_row
I0512 23:42:16.220100 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_18/mlp/wo/kernel/m
I0512 23:42:16.220131 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_18/mlp/wo/kernel/v
I0512 23:42:16.220161 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_18/mlp/wo/kernel/v_col
I0512 23:42:16.220192 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_18/mlp/wo/kernel/v_row
I0512 23:42:16.220222 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_18/pre_mlp_layer_norm/scale/m
I0512 23:42:16.220253 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_18/pre_mlp_layer_norm/scale/v
I0512 23:42:16.220284 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_18/pre_mlp_layer_norm/scale/v_col
I0512 23:42:16.220315 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_18/pre_mlp_layer_norm/scale/v_row
I0512 23:42:16.220345 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_18/pre_self_attention_layer_norm/scale/m
I0512 23:42:16.220376 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_18/pre_self_attention_layer_norm/scale/v
I0512 23:42:16.220408 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_18/pre_self_attention_layer_norm/scale/v_col
I0512 23:42:16.220441 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_18/pre_self_attention_layer_norm/scale/v_row
I0512 23:42:16.242320 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_0/self_attention/value/kernel/v_col
I0512 23:42:16.242354 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_0/self_attention/value/kernel/v_row
I0512 23:42:16.242387 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_1/mlp/wi_0/kernel/m
I0512 23:42:16.242421 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_1/mlp/wi_0/kernel/v
I0512 23:42:16.242454 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_1/mlp/wi_0/kernel/v_col
I0512 23:42:16.242487 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_1/mlp/wi_0/kernel/v_row
I0512 23:42:16.242520 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_1/mlp/wi_1/kernel/m
I0512 23:42:16.242553 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_1/mlp/wi_1/kernel/v
I0512 23:42:16.242586 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_1/mlp/wi_1/kernel/v_col
I0512 23:42:16.242619 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_1/mlp/wi_1/kernel/v_row
I0512 23:42:16.242653 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_1/mlp/wo/kernel/m
I0512 23:42:16.242686 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_1/mlp/wo/kernel/v
I0512 23:42:16.242719 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_1/mlp/wo/kernel/v_col
I0512 23:42:16.242752 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_1/mlp/wo/kernel/v_row
I0512 23:42:16.242785 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_1/pre_mlp_layer_norm/scale/m
I0512 23:42:16.242819 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_1/pre_mlp_layer_norm/scale/v
I0512 23:42:16.242852 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_1/pre_mlp_layer_norm/scale/v_col
I0512 23:42:16.240245 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_20/mlp/wo/kernel/v
I0512 23:42:16.240279 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_20/mlp/wo/kernel/v_col
I0512 23:42:16.240313 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_20/mlp/wo/kernel/v_row
I0512 23:42:16.240346 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_20/pre_mlp_layer_norm/scale/m
I0512 23:42:16.240380 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_20/pre_mlp_layer_norm/scale/v
I0512 23:42:16.240432 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_20/pre_mlp_layer_norm/scale/v_col
I0512 23:42:16.240469 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_20/pre_mlp_layer_norm/scale/v_row
I0512 23:42:16.240504 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_20/pre_self_attention_layer_norm/scale/m
I0512 23:42:16.240538 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_20/pre_self_attention_layer_norm/scale/v
I0512 23:42:16.240572 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_20/pre_self_attention_layer_norm/scale/v_col
I0512 23:42:16.240605 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_20/pre_self_attention_layer_norm/scale/v_row
I0512 23:42:16.240639 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_20/self_attention/key/kernel/m
I0512 23:42:16.240673 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_20/self_attention/key/kernel/v
I0512 23:42:16.240707 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_20/self_attention/key/kernel/v_col
I0512 23:42:16.240740 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_20/self_attention/key/kernel/v_row
I0512 23:42:16.220472 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_18/self_attention/key/kernel/m
I0512 23:42:16.220503 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_18/self_attention/key/kernel/v
I0512 23:42:16.220535 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_18/self_attention/key/kernel/v_col
I0512 23:42:16.220565 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_18/self_attention/key/kernel/v_row
I0512 23:42:16.220597 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_18/self_attention/out/kernel/m
I0512 23:42:16.220628 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_18/self_attention/out/kernel/v
I0512 23:42:16.220663 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_18/self_attention/out/kernel/v_col
I0512 23:42:16.220696 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_18/self_attention/out/kernel/v_row
I0512 23:42:16.220728 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_18/self_attention/query/kernel/m
I0512 23:42:16.220759 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_18/self_attention/query/kernel/v
I0512 23:42:16.220790 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_18/self_attention/query/kernel/v_col
I0512 23:42:16.220821 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_18/self_attention/query/kernel/v_row
I0512 23:42:16.220852 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_18/self_attention/value/kernel/m
I0512 23:42:16.220883 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_18/self_attention/value/kernel/v
I0512 23:42:16.242885 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_1/pre_mlp_layer_norm/scale/v_row
I0512 23:42:16.242919 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_1/pre_self_attention_layer_norm/scale/m
I0512 23:42:16.242952 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_1/pre_self_attention_layer_norm/scale/v
I0512 23:42:16.242986 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_1/pre_self_attention_layer_norm/scale/v_col
I0512 23:42:16.243025 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_1/pre_self_attention_layer_norm/scale/v_row
I0512 23:42:16.243060 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_1/self_attention/key/kernel/m
I0512 23:42:16.243093 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_1/self_attention/key/kernel/v
I0512 23:42:16.243127 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_1/self_attention/key/kernel/v_col
I0512 23:42:16.243160 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_1/self_attention/key/kernel/v_row
I0512 23:42:16.220914 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_18/self_attention/value/kernel/v_col
I0512 23:42:16.220945 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_18/self_attention/value/kernel/v_row
I0512 23:42:16.220976 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_19/mlp/wi_0/kernel/m
I0512 23:42:16.221008 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_19/mlp/wi_0/kernel/v
I0512 23:42:16.221039 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_19/mlp/wi_0/kernel/v_col
I0512 23:42:16.221070 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_19/mlp/wi_0/kernel/v_row
I0512 23:42:16.221101 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_19/mlp/wi_1/kernel/m
I0512 23:42:16.221132 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_19/mlp/wi_1/kernel/v
I0512 23:42:16.221163 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_19/mlp/wi_1/kernel/v_col
I0512 23:42:16.221194 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_19/mlp/wi_1/kernel/v_row
I0512 23:42:16.221225 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_19/mlp/wo/kernel/m
I0512 23:42:16.221255 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_19/mlp/wo/kernel/v
I0512 23:42:16.221286 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_19/mlp/wo/kernel/v_col
I0512 23:42:16.221317 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_19/mlp/wo/kernel/v_row
I0512 23:42:16.221347 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_19/pre_mlp_layer_norm/scale/m
I0512 23:42:16.240774 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_20/self_attention/out/kernel/m
I0512 23:42:16.240808 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_20/self_attention/out/kernel/v
I0512 23:42:16.240841 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_20/self_attention/out/kernel/v_col
I0512 23:42:16.240875 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_20/self_attention/out/kernel/v_row
I0512 23:42:16.240911 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_20/self_attention/query/kernel/m
I0512 23:42:16.240945 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_20/self_attention/query/kernel/v
I0512 23:42:16.240978 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_20/self_attention/query/kernel/v_col
I0512 23:42:16.241012 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_20/self_attention/query/kernel/v_row
I0512 23:42:16.241045 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_20/self_attention/value/kernel/m
I0512 23:42:16.241079 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_20/self_attention/value/kernel/v
I0512 23:42:16.241112 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_20/self_attention/value/kernel/v_col
I0512 23:42:16.241152 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_20/self_attention/value/kernel/v_row
I0512 23:42:16.241186 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_21/mlp/wi_0/kernel/m
I0512 23:42:16.241220 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_21/mlp/wi_0/kernel/v
I0512 23:42:16.243194 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_1/self_attention/out/kernel/m
I0512 23:42:16.243227 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_1/self_attention/out/kernel/v
I0512 23:42:16.243260 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_1/self_attention/out/kernel/v_col
I0512 23:42:16.243298 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_1/self_attention/out/kernel/v_row
I0512 23:42:16.243332 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_1/self_attention/query/kernel/m
I0512 23:42:16.243365 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_1/self_attention/query/kernel/v
I0512 23:42:16.243398 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_1/self_attention/query/kernel/v_col
I0512 23:42:16.243432 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_1/self_attention/query/kernel/v_row
I0512 23:42:16.243465 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_1/self_attention/value/kernel/m
I0512 23:42:16.243498 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_1/self_attention/value/kernel/v
I0512 23:42:16.221378 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_19/pre_mlp_layer_norm/scale/v
I0512 23:42:16.221410 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_19/pre_mlp_layer_norm/scale/v_col
I0512 23:42:16.221443 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_19/pre_mlp_layer_norm/scale/v_row
I0512 23:42:16.221475 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_19/pre_self_attention_layer_norm/scale/m
I0512 23:42:16.221506 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_19/pre_self_attention_layer_norm/scale/v
I0512 23:42:16.221537 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_19/pre_self_attention_layer_norm/scale/v_col
I0512 23:42:16.221568 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_19/pre_self_attention_layer_norm/scale/v_row
I0512 23:42:16.221599 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_19/self_attention/key/kernel/m
I0512 23:42:16.221629 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_19/self_attention/key/kernel/v
I0512 23:42:16.221665 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_19/self_attention/key/kernel/v_col
I0512 23:42:16.221698 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_19/self_attention/key/kernel/v_row
I0512 23:42:16.221729 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_19/self_attention/out/kernel/m
I0512 23:42:16.221760 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_19/self_attention/out/kernel/v
I0512 23:42:16.221810 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_19/self_attention/out/kernel/v_col
I0512 23:42:16.243531 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_1/self_attention/value/kernel/v_col
I0512 23:42:16.243565 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_1/self_attention/value/kernel/v_row
I0512 23:42:16.243597 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_10/mlp/wi_0/kernel/m
I0512 23:42:16.243631 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_10/mlp/wi_0/kernel/v
I0512 23:42:16.243664 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_10/mlp/wi_0/kernel/v_col
I0512 23:42:16.243698 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_10/mlp/wi_0/kernel/v_row
I0512 23:42:16.243731 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_10/mlp/wi_1/kernel/m
I0512 23:42:16.243764 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_10/mlp/wi_1/kernel/v
I0512 23:42:16.243798 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_10/mlp/wi_1/kernel/v_col
I0512 23:42:16.243831 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_10/mlp/wi_1/kernel/v_row
I0512 23:42:16.243864 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_10/mlp/wo/kernel/m
I0512 23:42:16.241254 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_21/mlp/wi_0/kernel/v_col
I0512 23:42:16.241288 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_21/mlp/wi_0/kernel/v_row
I0512 23:42:16.241322 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_21/mlp/wi_1/kernel/m
I0512 23:42:16.241355 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_21/mlp/wi_1/kernel/v
I0512 23:42:16.241388 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_21/mlp/wi_1/kernel/v_col
I0512 23:42:16.241422 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_21/mlp/wi_1/kernel/v_row
I0512 23:42:16.241455 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_21/mlp/wo/kernel/m
I0512 23:42:16.241489 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_21/mlp/wo/kernel/v
I0512 23:42:16.241523 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_21/mlp/wo/kernel/v_col
I0512 23:42:16.241556 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_21/mlp/wo/kernel/v_row
I0512 23:42:16.241589 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_21/pre_mlp_layer_norm/scale/m
I0512 23:42:16.241622 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_21/pre_mlp_layer_norm/scale/v
I0512 23:42:16.241656 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_21/pre_mlp_layer_norm/scale/v_col
I0512 23:42:16.241689 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_21/pre_mlp_layer_norm/scale/v_row
I0512 23:42:16.241723 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_21/pre_self_attention_layer_norm/scale/m
I0512 23:42:16.221843 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_19/self_attention/out/kernel/v_row
I0512 23:42:16.221874 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_19/self_attention/query/kernel/m
I0512 23:42:16.221905 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_19/self_attention/query/kernel/v
I0512 23:42:16.221937 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_19/self_attention/query/kernel/v_col
I0512 23:42:16.221968 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_19/self_attention/query/kernel/v_row
I0512 23:42:16.221998 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_19/self_attention/value/kernel/m
I0512 23:42:16.222029 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_19/self_attention/value/kernel/v
I0512 23:42:16.222060 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_19/self_attention/value/kernel/v_col
I0512 23:42:16.222091 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_19/self_attention/value/kernel/v_row
I0512 23:42:16.222121 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_2/mlp/wi_0/kernel/m
I0512 23:42:16.222153 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_2/mlp/wi_0/kernel/v
I0512 23:42:16.222184 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_2/mlp/wi_0/kernel/v_col
I0512 23:42:16.222215 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_2/mlp/wi_0/kernel/v_row
I0512 23:42:16.222246 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_2/mlp/wi_1/kernel/m
I0512 23:42:16.222276 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_2/mlp/wi_1/kernel/v
I0512 23:42:16.241756 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_21/pre_self_attention_layer_norm/scale/v
I0512 23:42:16.241790 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_21/pre_self_attention_layer_norm/scale/v_col
I0512 23:42:16.241823 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_21/pre_self_attention_layer_norm/scale/v_row
I0512 23:42:16.241857 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_21/self_attention/key/kernel/m
I0512 23:42:16.241893 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_21/self_attention/key/kernel/v
I0512 23:42:16.241927 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_21/self_attention/key/kernel/v_col
I0512 23:42:16.241961 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_21/self_attention/key/kernel/v_row
I0512 23:42:16.241995 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_21/self_attention/out/kernel/m
I0512 23:42:16.242028 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_21/self_attention/out/kernel/v
I0512 23:42:16.242062 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_21/self_attention/out/kernel/v_col
I0512 23:42:16.242096 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_21/self_attention/out/kernel/v_row
I0512 23:42:16.242134 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_21/self_attention/query/kernel/m
I0512 23:42:16.242169 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_21/self_attention/query/kernel/v
I0512 23:42:16.242203 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_21/self_attention/query/kernel/v_col
I0512 23:42:16.242237 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_21/self_attention/query/kernel/v_row
I0512 23:42:16.243897 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_10/mlp/wo/kernel/v
I0512 23:42:16.243930 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_10/mlp/wo/kernel/v_col
I0512 23:42:16.243963 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_10/mlp/wo/kernel/v_row
I0512 23:42:16.243997 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_10/pre_mlp_layer_norm/scale/m
I0512 23:42:16.244040 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_10/pre_mlp_layer_norm/scale/v
I0512 23:42:16.244075 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_10/pre_mlp_layer_norm/scale/v_col
I0512 23:42:16.244108 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_10/pre_mlp_layer_norm/scale/v_row
I0512 23:42:16.244141 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_10/pre_self_attention_layer_norm/scale/m
I0512 23:42:16.244174 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_10/pre_self_attention_layer_norm/scale/v
I0512 23:42:16.244207 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_10/pre_self_attention_layer_norm/scale/v_col
I0512 23:42:16.244241 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_10/pre_self_attention_layer_norm/scale/v_row
I0512 23:42:16.244275 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_10/self_attention/key/kernel/m
I0512 23:42:16.244310 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_10/self_attention/key/kernel/v
I0512 23:42:16.244343 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_10/self_attention/key/kernel/v_col
I0512 23:42:16.244377 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_10/self_attention/key/kernel/v_row
I0512 23:42:16.244410 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_10/self_attention/out/kernel/m
I0512 23:42:16.244443 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_10/self_attention/out/kernel/v
I0512 23:42:16.244476 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_10/self_attention/out/kernel/v_col
I0512 23:42:16.244509 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_10/self_attention/out/kernel/v_row
I0512 23:42:16.244542 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_10/self_attention/query/kernel/m
I0512 23:42:16.244575 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_10/self_attention/query/kernel/v
I0512 23:42:16.244609 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_10/self_attention/query/kernel/v_col
I0512 23:42:16.244642 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_10/self_attention/query/kernel/v_row
I0512 23:42:16.222307 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_2/mlp/wi_1/kernel/v_col
I0512 23:42:16.222338 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_2/mlp/wi_1/kernel/v_row
I0512 23:42:16.222368 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_2/mlp/wo/kernel/m
I0512 23:42:16.222399 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_2/mlp/wo/kernel/v
I0512 23:42:16.222433 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_2/mlp/wo/kernel/v_col
I0512 23:42:16.222464 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_2/mlp/wo/kernel/v_row
I0512 23:42:16.222495 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_2/pre_mlp_layer_norm/scale/m
I0512 23:42:16.222526 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_2/pre_mlp_layer_norm/scale/v
I0512 23:42:16.222557 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_2/pre_mlp_layer_norm/scale/v_col
I0512 23:42:16.222588 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_2/pre_mlp_layer_norm/scale/v_row
I0512 23:42:16.222619 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_2/pre_self_attention_layer_norm/scale/m
I0512 23:42:16.222650 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_2/pre_self_attention_layer_norm/scale/v
I0512 23:42:16.222688 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_2/pre_self_attention_layer_norm/scale/v_col
I0512 23:42:16.222719 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_2/pre_self_attention_layer_norm/scale/v_row
I0512 23:42:16.222750 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_2/self_attention/key/kernel/m
I0512 23:42:16.242271 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_21/self_attention/value/kernel/m
I0512 23:42:16.242304 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_21/self_attention/value/kernel/v
I0512 23:42:16.242338 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_21/self_attention/value/kernel/v_col
I0512 23:42:16.242371 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_21/self_attention/value/kernel/v_row
I0512 23:42:16.242404 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_22/mlp/wi_0/kernel/m
I0512 23:42:16.242438 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_22/mlp/wi_0/kernel/v
I0512 23:42:16.242471 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_22/mlp/wi_0/kernel/v_col
I0512 23:42:16.242505 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_22/mlp/wi_0/kernel/v_row
I0512 23:42:16.242538 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_22/mlp/wi_1/kernel/m
I0512 23:42:16.242572 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_22/mlp/wi_1/kernel/v
I0512 23:42:16.242605 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_22/mlp/wi_1/kernel/v_col
I0512 23:42:16.242639 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_22/mlp/wi_1/kernel/v_row
I0512 23:42:16.242672 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_22/mlp/wo/kernel/m
I0512 23:42:16.242705 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_22/mlp/wo/kernel/v
I0512 23:42:16.242738 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_22/mlp/wo/kernel/v_col
I0512 23:42:16.244675 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_10/self_attention/value/kernel/m
I0512 23:42:16.244708 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_10/self_attention/value/kernel/v
I0512 23:42:16.244741 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_10/self_attention/value/kernel/v_col
I0512 23:42:16.244774 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_10/self_attention/value/kernel/v_row
I0512 23:42:16.244807 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/extra_mlp/wi_0/kernel/m
I0512 23:42:16.244840 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/extra_mlp/wi_0/kernel/v
I0512 23:42:16.244873 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/extra_mlp/wi_0/kernel/v_col
I0512 23:42:16.244906 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/extra_mlp/wi_0/kernel/v_row
I0512 23:42:16.244939 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/extra_mlp/wi_1/kernel/m
I0512 23:42:16.244972 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/extra_mlp/wi_1/kernel/v
I0512 23:42:16.245006 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/extra_mlp/wi_1/kernel/v_col
I0512 23:42:16.245045 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/extra_mlp/wi_1/kernel/v_row
I0512 23:42:16.245079 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/extra_mlp/wo/kernel/m
I0512 23:42:16.222781 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_2/self_attention/key/kernel/v
I0512 23:42:16.222812 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_2/self_attention/key/kernel/v_col
I0512 23:42:16.222843 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_2/self_attention/key/kernel/v_row
I0512 23:42:16.222874 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_2/self_attention/out/kernel/m
I0512 23:42:16.222905 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_2/self_attention/out/kernel/v
I0512 23:42:16.222936 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_2/self_attention/out/kernel/v_col
I0512 23:42:16.222966 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_2/self_attention/out/kernel/v_row
I0512 23:42:16.222997 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_2/self_attention/query/kernel/m
I0512 23:42:16.223028 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_2/self_attention/query/kernel/v
I0512 23:42:16.223059 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_2/self_attention/query/kernel/v_col
I0512 23:42:16.223089 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_2/self_attention/query/kernel/v_row
I0512 23:42:16.223120 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_2/self_attention/value/kernel/m
I0512 23:42:16.223151 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_2/self_attention/value/kernel/v
I0512 23:42:16.223182 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_2/self_attention/value/kernel/v_col
I0512 23:42:16.242771 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_22/mlp/wo/kernel/v_row
I0512 23:42:16.242805 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_22/pre_mlp_layer_norm/scale/m
I0512 23:42:16.242838 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_22/pre_mlp_layer_norm/scale/v
I0512 23:42:16.242871 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_22/pre_mlp_layer_norm/scale/v_col
I0512 23:42:16.242907 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_22/pre_mlp_layer_norm/scale/v_row
I0512 23:42:16.242941 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_22/pre_self_attention_layer_norm/scale/m
I0512 23:42:16.242975 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_22/pre_self_attention_layer_norm/scale/v
I0512 23:42:16.243009 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_22/pre_self_attention_layer_norm/scale/v_col
I0512 23:42:16.243043 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_22/pre_self_attention_layer_norm/scale/v_row
I0512 23:42:16.243076 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_22/self_attention/key/kernel/m
I0512 23:42:16.243109 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_22/self_attention/key/kernel/v
I0512 23:42:16.243150 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_22/self_attention/key/kernel/v_col
I0512 23:42:16.243185 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_22/self_attention/key/kernel/v_row
I0512 23:42:16.243218 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_22/self_attention/out/kernel/m
I0512 23:42:16.245113 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/extra_mlp/wo/kernel/v
I0512 23:42:16.245146 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/extra_mlp/wo/kernel/v_col
I0512 23:42:16.245180 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/extra_mlp/wo/kernel/v_row
I0512 23:42:16.245213 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/mlp/expert/wi_0/kernel/m
I0512 23:42:16.245245 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/mlp/expert/wi_0/kernel/v
I0512 23:42:16.245280 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/mlp/expert/wi_0/kernel/v_col
I0512 23:42:16.245315 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/mlp/expert/wi_0/kernel/v_row
I0512 23:42:16.245348 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/mlp/expert/wi_1/kernel/m
I0512 23:42:16.245381 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/mlp/expert/wi_1/kernel/v
I0512 23:42:16.245414 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/mlp/expert/wi_1/kernel/v_col
I0512 23:42:16.245448 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/mlp/expert/wi_1/kernel/v_row
I0512 23:42:16.245481 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/mlp/expert/wo/kernel/m
I0512 23:42:16.245514 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/mlp/expert/wo/kernel/v
I0512 23:42:16.245547 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/mlp/expert/wo/kernel/v_col
I0512 23:42:16.223212 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_2/self_attention/value/kernel/v_row
I0512 23:42:16.223243 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_20/mlp/wi_0/kernel/m
I0512 23:42:16.223274 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_20/mlp/wi_0/kernel/v
I0512 23:42:16.223305 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_20/mlp/wi_0/kernel/v_col
I0512 23:42:16.223336 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_20/mlp/wi_0/kernel/v_row
I0512 23:42:16.223367 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_20/mlp/wi_1/kernel/m
I0512 23:42:16.223398 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_20/mlp/wi_1/kernel/v
I0512 23:42:16.223431 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_20/mlp/wi_1/kernel/v_col
I0512 23:42:16.223462 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_20/mlp/wi_1/kernel/v_row
I0512 23:42:16.223493 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_20/mlp/wo/kernel/m
I0512 23:42:16.223524 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_20/mlp/wo/kernel/v
I0512 23:42:16.223554 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_20/mlp/wo/kernel/v_col
I0512 23:42:16.223584 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_20/mlp/wo/kernel/v_row
I0512 23:42:16.223615 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_20/pre_mlp_layer_norm/scale/m
I0512 23:42:16.223646 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_20/pre_mlp_layer_norm/scale/v
I0512 23:42:16.223685 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_20/pre_mlp_layer_norm/scale/v_col
I0512 23:42:16.243251 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_22/self_attention/out/kernel/v
I0512 23:42:16.243284 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_22/self_attention/out/kernel/v_col
I0512 23:42:16.243317 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_22/self_attention/out/kernel/v_row
I0512 23:42:16.243350 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_22/self_attention/query/kernel/m
I0512 23:42:16.243383 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_22/self_attention/query/kernel/v
I0512 23:42:16.243416 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_22/self_attention/query/kernel/v_col
I0512 23:42:16.243449 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_22/self_attention/query/kernel/v_row
I0512 23:42:16.243482 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_22/self_attention/value/kernel/m
I0512 23:42:16.243515 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_22/self_attention/value/kernel/v
I0512 23:42:16.243549 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_22/self_attention/value/kernel/v_col
I0512 23:42:16.243582 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_22/self_attention/value/kernel/v_row
I0512 23:42:16.243615 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/extra_mlp/wi_0/kernel/m
I0512 23:42:16.243649 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/extra_mlp/wi_0/kernel/v
I0512 23:42:16.243682 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/extra_mlp/wi_0/kernel/v_col
I0512 23:42:16.243715 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/extra_mlp/wi_0/kernel/v_row
I0512 23:42:16.245581 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/mlp/expert/wo/kernel/v_row
I0512 23:42:16.245614 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/mlp/router/router_weights/w/kernel/m
I0512 23:42:16.245647 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/mlp/router/router_weights/w/kernel/v
I0512 23:42:16.245680 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/mlp/router/router_weights/w/kernel/v_col
I0512 23:42:16.245714 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/mlp/router/router_weights/w/kernel/v_row
I0512 23:42:16.245747 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/pre_extra_mlp_layer_norm/scale/m
I0512 23:42:16.245780 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/pre_extra_mlp_layer_norm/scale/v
I0512 23:42:16.245814 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/pre_extra_mlp_layer_norm/scale/v_col
I0512 23:42:16.245846 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/pre_extra_mlp_layer_norm/scale/v_row
I0512 23:42:16.245879 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/pre_mlp_layer_norm/scale/m
I0512 23:42:16.245913 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/pre_mlp_layer_norm/scale/v
I0512 23:42:16.245945 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/pre_mlp_layer_norm/scale/v_col
I0512 23:42:16.245979 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/pre_mlp_layer_norm/scale/v_row
I0512 23:42:16.246011 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/pre_self_attention_layer_norm/scale/m
I0512 23:42:16.223717 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_20/pre_mlp_layer_norm/scale/v_row
I0512 23:42:16.223748 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_20/pre_self_attention_layer_norm/scale/m
I0512 23:42:16.223779 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_20/pre_self_attention_layer_norm/scale/v
I0512 23:42:16.223810 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_20/pre_self_attention_layer_norm/scale/v_col
I0512 23:42:16.223841 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_20/pre_self_attention_layer_norm/scale/v_row
I0512 23:42:16.223871 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_20/self_attention/key/kernel/m
I0512 23:42:16.223902 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_20/self_attention/key/kernel/v
I0512 23:42:16.223933 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_20/self_attention/key/kernel/v_col
I0512 23:42:16.223964 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_20/self_attention/key/kernel/v_row
I0512 23:42:16.223994 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_20/self_attention/out/kernel/m
I0512 23:42:16.224025 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_20/self_attention/out/kernel/v
I0512 23:42:16.224056 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_20/self_attention/out/kernel/v_col
I0512 23:42:16.224086 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_20/self_attention/out/kernel/v_row
I0512 23:42:16.224117 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_20/self_attention/query/kernel/m
I0512 23:42:16.243748 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/extra_mlp/wi_1/kernel/m
I0512 23:42:16.243782 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/extra_mlp/wi_1/kernel/v
I0512 23:42:16.243816 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/extra_mlp/wi_1/kernel/v_col
I0512 23:42:16.243849 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/extra_mlp/wi_1/kernel/v_row
I0512 23:42:16.243883 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/extra_mlp/wo/kernel/m
I0512 23:42:16.243919 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/extra_mlp/wo/kernel/v
I0512 23:42:16.243953 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/extra_mlp/wo/kernel/v_col
I0512 23:42:16.243987 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/extra_mlp/wo/kernel/v_row
I0512 23:42:16.244020 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/mlp/expert/wi_0/kernel/m
I0512 23:42:16.244054 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/mlp/expert/wi_0/kernel/v
I0512 23:42:16.244086 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/mlp/expert/wi_0/kernel/v_col
I0512 23:42:16.244120 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/mlp/expert/wi_0/kernel/v_row
I0512 23:42:16.244160 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/mlp/expert/wi_1/kernel/m
I0512 23:42:16.244195 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/mlp/expert/wi_1/kernel/v
I0512 23:42:16.244228 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/mlp/expert/wi_1/kernel/v_col
I0512 23:42:16.246051 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/pre_self_attention_layer_norm/scale/v
I0512 23:42:16.246085 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/pre_self_attention_layer_norm/scale/v_col
I0512 23:42:16.246118 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/pre_self_attention_layer_norm/scale/v_row
I0512 23:42:16.246151 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/self_attention/key/kernel/m
I0512 23:42:16.246185 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/self_attention/key/kernel/v
I0512 23:42:16.246217 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/self_attention/key/kernel/v_col
I0512 23:42:16.246251 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/self_attention/key/kernel/v_row
I0512 23:42:16.246307 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/self_attention/out/kernel/m
I0512 23:42:16.246341 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/self_attention/out/kernel/v
I0512 23:42:16.246375 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/self_attention/out/kernel/v_col
I0512 23:42:16.246409 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/self_attention/out/kernel/v_row
I0512 23:42:16.246442 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/self_attention/query/kernel/m
I0512 23:42:16.246475 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/self_attention/query/kernel/v
I0512 23:42:16.246509 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/self_attention/query/kernel/v_col
I0512 23:42:16.224148 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_20/self_attention/query/kernel/v
I0512 23:42:16.224179 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_20/self_attention/query/kernel/v_col
I0512 23:42:16.224209 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_20/self_attention/query/kernel/v_row
I0512 23:42:16.224240 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_20/self_attention/value/kernel/m
I0512 23:42:16.224271 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_20/self_attention/value/kernel/v
I0512 23:42:16.224302 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_20/self_attention/value/kernel/v_col
I0512 23:42:16.224332 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_20/self_attention/value/kernel/v_row
I0512 23:42:16.224363 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_21/mlp/wi_0/kernel/m
I0512 23:42:16.224394 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_21/mlp/wi_0/kernel/v
I0512 23:42:16.224427 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_21/mlp/wi_0/kernel/v_col
I0512 23:42:16.224460 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_21/mlp/wi_0/kernel/v_row
I0512 23:42:16.224491 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_21/mlp/wi_1/kernel/m
I0512 23:42:16.224521 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_21/mlp/wi_1/kernel/v
I0512 23:42:16.224552 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_21/mlp/wi_1/kernel/v_col
I0512 23:42:16.224583 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_21/mlp/wi_1/kernel/v_row
I0512 23:42:16.246541 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/self_attention/query/kernel/v_row
I0512 23:42:16.246572 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/self_attention/value/kernel/m
I0512 23:42:16.246603 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/self_attention/value/kernel/v
I0512 23:42:16.246634 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/self_attention/value/kernel/v_col
I0512 23:42:16.246665 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/self_attention/value/kernel/v_row
I0512 23:42:16.246696 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_12/mlp/wi_0/kernel/m
I0512 23:42:16.246727 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_12/mlp/wi_0/kernel/v
I0512 23:42:16.246758 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_12/mlp/wi_0/kernel/v_col
I0512 23:42:16.246792 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_12/mlp/wi_0/kernel/v_row
I0512 23:42:16.246826 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_12/mlp/wi_1/kernel/m
I0512 23:42:16.246856 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_12/mlp/wi_1/kernel/v
I0512 23:42:16.246888 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_12/mlp/wi_1/kernel/v_col
I0512 23:42:16.246919 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_12/mlp/wi_1/kernel/v_row
I0512 23:42:16.246950 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_12/mlp/wo/kernel/m
I0512 23:42:16.246981 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_12/mlp/wo/kernel/v
I0512 23:42:16.224613 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_21/mlp/wo/kernel/m
I0512 23:42:16.224644 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_21/mlp/wo/kernel/v
I0512 23:42:16.224681 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_21/mlp/wo/kernel/v_col
I0512 23:42:16.224712 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_21/mlp/wo/kernel/v_row
I0512 23:42:16.224743 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_21/pre_mlp_layer_norm/scale/m
I0512 23:42:16.224774 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_21/pre_mlp_layer_norm/scale/v
I0512 23:42:16.224805 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_21/pre_mlp_layer_norm/scale/v_col
I0512 23:42:16.224835 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_21/pre_mlp_layer_norm/scale/v_row
I0512 23:42:16.224866 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_21/pre_self_attention_layer_norm/scale/m
I0512 23:42:16.224897 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_21/pre_self_attention_layer_norm/scale/v
I0512 23:42:16.224927 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_21/pre_self_attention_layer_norm/scale/v_col
I0512 23:42:16.224958 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_21/pre_self_attention_layer_norm/scale/v_row
I0512 23:42:16.224989 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_21/self_attention/key/kernel/m
I0512 23:42:16.225020 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_21/self_attention/key/kernel/v
I0512 23:42:16.225051 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_21/self_attention/key/kernel/v_col
I0512 23:42:16.244262 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/mlp/expert/wi_1/kernel/v_row
I0512 23:42:16.244296 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/mlp/expert/wo/kernel/m
I0512 23:42:16.244330 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/mlp/expert/wo/kernel/v
I0512 23:42:16.244364 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/mlp/expert/wo/kernel/v_col
I0512 23:42:16.244398 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/mlp/expert/wo/kernel/v_row
I0512 23:42:16.244458 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/mlp/router/router_weights/w/kernel/m
I0512 23:42:16.244493 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/mlp/router/router_weights/w/kernel/v
I0512 23:42:16.244527 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/mlp/router/router_weights/w/kernel/v_col
I0512 23:42:16.244561 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/mlp/router/router_weights/w/kernel/v_row
I0512 23:42:16.244594 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/pre_extra_mlp_layer_norm/scale/m
I0512 23:42:16.244628 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/pre_extra_mlp_layer_norm/scale/v
I0512 23:42:16.244662 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/pre_extra_mlp_layer_norm/scale/v_col
I0512 23:42:16.244695 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/pre_extra_mlp_layer_norm/scale/v_row
I0512 23:42:16.244728 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/pre_mlp_layer_norm/scale/m
I0512 23:42:16.244762 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/pre_mlp_layer_norm/scale/v
I0512 23:42:16.244795 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/pre_mlp_layer_norm/scale/v_col
I0512 23:42:16.244829 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/pre_mlp_layer_norm/scale/v_row
I0512 23:42:16.244862 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/pre_self_attention_layer_norm/scale/m
I0512 23:42:16.244897 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/pre_self_attention_layer_norm/scale/v
I0512 23:42:16.244931 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/pre_self_attention_layer_norm/scale/v_col
I0512 23:42:16.244965 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/pre_self_attention_layer_norm/scale/v_row
I0512 23:42:16.244998 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/self_attention/key/kernel/m
I0512 23:42:16.245031 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/self_attention/key/kernel/v
I0512 23:42:16.245064 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/self_attention/key/kernel/v_col
I0512 23:42:16.245098 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/self_attention/key/kernel/v_row
I0512 23:42:16.245137 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/self_attention/out/kernel/m
I0512 23:42:16.245172 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/self_attention/out/kernel/v
I0512 23:42:16.245205 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/self_attention/out/kernel/v_col
I0512 23:42:16.245238 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/self_attention/out/kernel/v_row
I0512 23:42:16.247018 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_12/mlp/wo/kernel/v_col
I0512 23:42:16.247051 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_12/mlp/wo/kernel/v_row
I0512 23:42:16.247082 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_12/pre_mlp_layer_norm/scale/m
I0512 23:42:16.247113 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_12/pre_mlp_layer_norm/scale/v
I0512 23:42:16.247145 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_12/pre_mlp_layer_norm/scale/v_col
I0512 23:42:16.247176 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_12/pre_mlp_layer_norm/scale/v_row
I0512 23:42:16.247207 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_12/pre_self_attention_layer_norm/scale/m
I0512 23:42:16.225082 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_21/self_attention/key/kernel/v_row
I0512 23:42:16.225113 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_21/self_attention/out/kernel/m
I0512 23:42:16.225145 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_21/self_attention/out/kernel/v
I0512 23:42:16.225176 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_21/self_attention/out/kernel/v_col
I0512 23:42:16.225206 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_21/self_attention/out/kernel/v_row
I0512 23:42:16.225236 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_21/self_attention/query/kernel/m
I0512 23:42:16.225267 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_21/self_attention/query/kernel/v
I0512 23:42:16.225297 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_21/self_attention/query/kernel/v_col
I0512 23:42:16.225328 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_21/self_attention/query/kernel/v_row
I0512 23:42:16.225358 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_21/self_attention/value/kernel/m
I0512 23:42:16.225389 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_21/self_attention/value/kernel/v
I0512 23:42:16.225422 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_21/self_attention/value/kernel/v_col
I0512 23:42:16.225453 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_21/self_attention/value/kernel/v_row
I0512 23:42:16.225484 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_22/mlp/wi_0/kernel/m
I0512 23:42:16.247238 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_12/pre_self_attention_layer_norm/scale/v
I0512 23:42:16.247269 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_12/pre_self_attention_layer_norm/scale/v_col
I0512 23:42:16.247303 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_12/pre_self_attention_layer_norm/scale/v_row
I0512 23:42:16.247334 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_12/self_attention/key/kernel/m
I0512 23:42:16.247366 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_12/self_attention/key/kernel/v
I0512 23:42:16.247397 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_12/self_attention/key/kernel/v_col
I0512 23:42:16.247428 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_12/self_attention/key/kernel/v_row
I0512 23:42:16.247459 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_12/self_attention/out/kernel/m
I0512 23:42:16.245272 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/self_attention/query/kernel/m
I0512 23:42:16.245305 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/self_attention/query/kernel/v
I0512 23:42:16.245338 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/self_attention/query/kernel/v_col
I0512 23:42:16.245371 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/self_attention/query/kernel/v_row
I0512 23:42:16.245404 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/self_attention/value/kernel/m
I0512 23:42:16.245438 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/self_attention/value/kernel/v
I0512 23:42:16.245471 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/self_attention/value/kernel/v_col
I0512 23:42:16.245504 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/self_attention/value/kernel/v_row
I0512 23:42:16.245537 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_3/mlp/wi_0/kernel/m
I0512 23:42:16.245571 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_3/mlp/wi_0/kernel/v
I0512 23:42:16.245604 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_3/mlp/wi_0/kernel/v_col
I0512 23:42:16.245638 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_3/mlp/wi_0/kernel/v_row
I0512 23:42:16.245671 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_3/mlp/wi_1/kernel/m
I0512 23:42:16.245705 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_3/mlp/wi_1/kernel/v
I0512 23:42:16.245738 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_3/mlp/wi_1/kernel/v_col
I0512 23:42:16.247490 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_12/self_attention/out/kernel/v
I0512 23:42:16.247521 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_12/self_attention/out/kernel/v_col
I0512 23:42:16.247553 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_12/self_attention/out/kernel/v_row
I0512 23:42:16.247584 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_12/self_attention/query/kernel/m
I0512 23:42:16.247614 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_12/self_attention/query/kernel/v
I0512 23:42:16.247646 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_12/self_attention/query/kernel/v_col
I0512 23:42:16.247677 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_12/self_attention/query/kernel/v_row
I0512 23:42:16.247708 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_12/self_attention/value/kernel/m
I0512 23:42:16.247739 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_12/self_attention/value/kernel/v
I0512 23:42:16.247770 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_12/self_attention/value/kernel/v_col
I0512 23:42:16.247805 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_12/self_attention/value/kernel/v_row
I0512 23:42:16.247838 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_13/mlp/wi_0/kernel/m
I0512 23:42:16.247870 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_13/mlp/wi_0/kernel/v
I0512 23:42:16.247902 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_13/mlp/wi_0/kernel/v_col
I0512 23:42:16.225515 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_22/mlp/wi_0/kernel/v
I0512 23:42:16.225545 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_22/mlp/wi_0/kernel/v_col
I0512 23:42:16.225576 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_22/mlp/wi_0/kernel/v_row
I0512 23:42:16.225607 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_22/mlp/wi_1/kernel/m
I0512 23:42:16.225637 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_22/mlp/wi_1/kernel/v
I0512 23:42:16.225673 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_22/mlp/wi_1/kernel/v_col
I0512 23:42:16.225705 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_22/mlp/wi_1/kernel/v_row
I0512 23:42:16.225736 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_22/mlp/wo/kernel/m
I0512 23:42:16.225783 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_22/mlp/wo/kernel/v
I0512 23:42:16.225820 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_22/mlp/wo/kernel/v_col
I0512 23:42:16.225853 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_22/mlp/wo/kernel/v_row
I0512 23:42:16.225884 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_22/pre_mlp_layer_norm/scale/m
I0512 23:42:16.225915 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_22/pre_mlp_layer_norm/scale/v
I0512 23:42:16.225946 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_22/pre_mlp_layer_norm/scale/v_col
I0512 23:42:16.225977 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_22/pre_mlp_layer_norm/scale/v_row
I0512 23:42:16.247933 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_13/mlp/wi_0/kernel/v_row
I0512 23:42:16.247964 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_13/mlp/wi_1/kernel/m
I0512 23:42:16.247996 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_13/mlp/wi_1/kernel/v
I0512 23:42:16.248033 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_13/mlp/wi_1/kernel/v_col
I0512 23:42:16.248065 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_13/mlp/wi_1/kernel/v_row
I0512 23:42:16.248097 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_13/mlp/wo/kernel/m
I0512 23:42:16.248128 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_13/mlp/wo/kernel/v
I0512 23:42:16.248159 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_13/mlp/wo/kernel/v_col
I0512 23:42:16.248190 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_13/mlp/wo/kernel/v_row
I0512 23:42:16.248221 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_13/pre_mlp_layer_norm/scale/m
I0512 23:42:16.248253 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_13/pre_mlp_layer_norm/scale/v
I0512 23:42:16.248286 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_13/pre_mlp_layer_norm/scale/v_col
I0512 23:42:16.248319 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_13/pre_mlp_layer_norm/scale/v_row
I0512 23:42:16.248350 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_13/pre_self_attention_layer_norm/scale/m
I0512 23:42:16.248381 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_13/pre_self_attention_layer_norm/scale/v
I0512 23:42:16.245771 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_3/mlp/wi_1/kernel/v_row
I0512 23:42:16.245805 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_3/mlp/wo/kernel/m
I0512 23:42:16.245838 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_3/mlp/wo/kernel/v
I0512 23:42:16.245871 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_3/mlp/wo/kernel/v_col
I0512 23:42:16.245906 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_3/mlp/wo/kernel/v_row
I0512 23:42:16.245939 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_3/pre_mlp_layer_norm/scale/m
I0512 23:42:16.245973 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_3/pre_mlp_layer_norm/scale/v
I0512 23:42:16.246006 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_3/pre_mlp_layer_norm/scale/v_col
I0512 23:42:16.246040 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_3/pre_mlp_layer_norm/scale/v_row
I0512 23:42:16.246073 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_3/pre_self_attention_layer_norm/scale/m
I0512 23:42:16.246107 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_3/pre_self_attention_layer_norm/scale/v
I0512 23:42:16.246146 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_3/pre_self_attention_layer_norm/scale/v_col
I0512 23:42:16.246181 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_3/pre_self_attention_layer_norm/scale/v_row
I0512 23:42:16.246214 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_3/self_attention/key/kernel/m
I0512 23:42:16.226008 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_22/pre_self_attention_layer_norm/scale/m
I0512 23:42:16.226039 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_22/pre_self_attention_layer_norm/scale/v
I0512 23:42:16.226070 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_22/pre_self_attention_layer_norm/scale/v_col
I0512 23:42:16.226101 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_22/pre_self_attention_layer_norm/scale/v_row
I0512 23:42:16.226132 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_22/self_attention/key/kernel/m
I0512 23:42:16.226162 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_22/self_attention/key/kernel/v
I0512 23:42:16.226193 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_22/self_attention/key/kernel/v_col
I0512 23:42:16.226223 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_22/self_attention/key/kernel/v_row
I0512 23:42:16.226254 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_22/self_attention/out/kernel/m
I0512 23:42:16.226284 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_22/self_attention/out/kernel/v
I0512 23:42:16.226315 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_22/self_attention/out/kernel/v_col
I0512 23:42:16.226346 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_22/self_attention/out/kernel/v_row
I0512 23:42:16.226377 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_22/self_attention/query/kernel/m
I0512 23:42:16.226408 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_22/self_attention/query/kernel/v
I0512 23:42:16.226441 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_22/self_attention/query/kernel/v_col
I0512 23:42:16.246247 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_3/self_attention/key/kernel/v
I0512 23:42:16.246281 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_3/self_attention/key/kernel/v_col
I0512 23:42:16.246314 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_3/self_attention/key/kernel/v_row
I0512 23:42:16.246348 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_3/self_attention/out/kernel/m
I0512 23:42:16.246381 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_3/self_attention/out/kernel/v
I0512 23:42:16.246414 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_3/self_attention/out/kernel/v_col
I0512 23:42:16.246447 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_3/self_attention/out/kernel/v_row
I0512 23:42:16.246481 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_3/self_attention/query/kernel/m
I0512 23:42:16.246514 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_3/self_attention/query/kernel/v
I0512 23:42:16.246547 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_3/self_attention/query/kernel/v_col
I0512 23:42:16.246580 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_3/self_attention/query/kernel/v_row
I0512 23:42:16.246613 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_3/self_attention/value/kernel/m
I0512 23:42:16.246647 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_3/self_attention/value/kernel/v
I0512 23:42:16.246680 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_3/self_attention/value/kernel/v_col
I0512 23:42:16.246713 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_3/self_attention/value/kernel/v_row
I0512 23:42:16.248412 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_13/pre_self_attention_layer_norm/scale/v_col
I0512 23:42:16.248443 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_13/pre_self_attention_layer_norm/scale/v_row
I0512 23:42:16.248474 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_13/self_attention/key/kernel/m
I0512 23:42:16.248505 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_13/self_attention/key/kernel/v
I0512 23:42:16.248537 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_13/self_attention/key/kernel/v_col
I0512 23:42:16.248568 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_13/self_attention/key/kernel/v_row
I0512 23:42:16.248599 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_13/self_attention/out/kernel/m
I0512 23:42:16.248631 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_13/self_attention/out/kernel/v
I0512 23:42:16.248662 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_13/self_attention/out/kernel/v_col
I0512 23:42:16.248693 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_13/self_attention/out/kernel/v_row
I0512 23:42:16.248724 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_13/self_attention/query/kernel/m
I0512 23:42:16.248755 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_13/self_attention/query/kernel/v
I0512 23:42:16.248786 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_13/self_attention/query/kernel/v_col
I0512 23:42:16.248818 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_13/self_attention/query/kernel/v_row
I0512 23:42:16.248849 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_13/self_attention/value/kernel/m
I0512 23:42:16.226472 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_22/self_attention/query/kernel/v_row
I0512 23:42:16.226503 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_22/self_attention/value/kernel/m
I0512 23:42:16.226533 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_22/self_attention/value/kernel/v
I0512 23:42:16.226564 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_22/self_attention/value/kernel/v_col
I0512 23:42:16.226595 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_22/self_attention/value/kernel/v_row
I0512 23:42:16.226625 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/extra_mlp/wi_0/kernel/m
I0512 23:42:16.226661 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/extra_mlp/wi_0/kernel/v
I0512 23:42:16.226693 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/extra_mlp/wi_0/kernel/v_col
I0512 23:42:16.226724 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/extra_mlp/wi_0/kernel/v_row
I0512 23:42:16.226755 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/extra_mlp/wi_1/kernel/m
I0512 23:42:16.226785 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/extra_mlp/wi_1/kernel/v
I0512 23:42:16.226815 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/extra_mlp/wi_1/kernel/v_col
I0512 23:42:16.226846 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/extra_mlp/wi_1/kernel/v_row
I0512 23:42:16.226876 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/extra_mlp/wo/kernel/m
I0512 23:42:16.226907 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/extra_mlp/wo/kernel/v
I0512 23:42:16.226938 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/extra_mlp/wo/kernel/v_col
I0512 23:42:16.226969 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/extra_mlp/wo/kernel/v_row
I0512 23:42:16.226999 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/mlp/expert/wi_0/kernel/m
I0512 23:42:16.227030 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/mlp/expert/wi_0/kernel/v
I0512 23:42:16.227061 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/mlp/expert/wi_0/kernel/v_col
I0512 23:42:16.227092 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/mlp/expert/wi_0/kernel/v_row
I0512 23:42:16.227122 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/mlp/expert/wi_1/kernel/m
I0512 23:42:16.227153 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/mlp/expert/wi_1/kernel/v
I0512 23:42:16.227184 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/mlp/expert/wi_1/kernel/v_col
I0512 23:42:16.248880 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_13/self_attention/value/kernel/v
I0512 23:42:16.248911 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_13/self_attention/value/kernel/v_col
I0512 23:42:16.248942 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_13/self_attention/value/kernel/v_row
I0512 23:42:16.248973 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_14/mlp/wi_0/kernel/m
I0512 23:42:16.249004 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_14/mlp/wi_0/kernel/v
I0512 23:42:16.249042 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_14/mlp/wi_0/kernel/v_col
I0512 23:42:16.249073 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_14/mlp/wi_0/kernel/v_row
I0512 23:42:16.249105 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_14/mlp/wi_1/kernel/m
I0512 23:42:16.249136 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_14/mlp/wi_1/kernel/v
I0512 23:42:16.249168 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_14/mlp/wi_1/kernel/v_col
I0512 23:42:16.249199 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_14/mlp/wi_1/kernel/v_row
I0512 23:42:16.249230 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_14/mlp/wo/kernel/m
I0512 23:42:16.249261 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_14/mlp/wo/kernel/v
I0512 23:42:16.249295 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_14/mlp/wo/kernel/v_col
I0512 23:42:16.249327 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_14/mlp/wo/kernel/v_row
I0512 23:42:16.246746 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_4/mlp/wi_0/kernel/m
I0512 23:42:16.246779 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_4/mlp/wi_0/kernel/v
I0512 23:42:16.246813 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_4/mlp/wi_0/kernel/v_col
I0512 23:42:16.246847 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_4/mlp/wi_0/kernel/v_row
I0512 23:42:16.246880 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_4/mlp/wi_1/kernel/m
I0512 23:42:16.246916 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_4/mlp/wi_1/kernel/v
I0512 23:42:16.246950 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_4/mlp/wi_1/kernel/v_col
I0512 23:42:16.246984 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_4/mlp/wi_1/kernel/v_row
I0512 23:42:16.247018 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_4/mlp/wo/kernel/m
I0512 23:42:16.247051 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_4/mlp/wo/kernel/v
I0512 23:42:16.247085 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_4/mlp/wo/kernel/v_col
I0512 23:42:16.247118 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_4/mlp/wo/kernel/v_row
I0512 23:42:16.247158 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_4/pre_mlp_layer_norm/scale/m
I0512 23:42:16.247192 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_4/pre_mlp_layer_norm/scale/v
I0512 23:42:16.247225 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_4/pre_mlp_layer_norm/scale/v_col
I0512 23:42:16.247259 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_4/pre_mlp_layer_norm/scale/v_row
I0512 23:42:16.227214 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/mlp/expert/wi_1/kernel/v_row
I0512 23:42:16.227245 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/mlp/expert/wo/kernel/m
I0512 23:42:16.227276 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/mlp/expert/wo/kernel/v
I0512 23:42:16.227307 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/mlp/expert/wo/kernel/v_col
I0512 23:42:16.227338 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/mlp/expert/wo/kernel/v_row
I0512 23:42:16.227368 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/mlp/router/router_weights/w/kernel/m
I0512 23:42:16.227398 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/mlp/router/router_weights/w/kernel/v
I0512 23:42:16.227432 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/mlp/router/router_weights/w/kernel/v_col
I0512 23:42:16.227463 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/mlp/router/router_weights/w/kernel/v_row
I0512 23:42:16.227494 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/pre_extra_mlp_layer_norm/scale/m
I0512 23:42:16.227525 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/pre_extra_mlp_layer_norm/scale/v
I0512 23:42:16.227555 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/pre_extra_mlp_layer_norm/scale/v_col
I0512 23:42:16.227586 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/pre_extra_mlp_layer_norm/scale/v_row
I0512 23:42:16.227617 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/pre_mlp_layer_norm/scale/m
I0512 23:42:16.227648 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/pre_mlp_layer_norm/scale/v
I0512 23:42:16.227688 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/pre_mlp_layer_norm/scale/v_col
I0512 23:42:16.227719 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/pre_mlp_layer_norm/scale/v_row
I0512 23:42:16.227750 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/pre_self_attention_layer_norm/scale/m
I0512 23:42:16.249358 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_14/pre_mlp_layer_norm/scale/m
I0512 23:42:16.249390 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_14/pre_mlp_layer_norm/scale/v
I0512 23:42:16.249421 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_14/pre_mlp_layer_norm/scale/v_col
I0512 23:42:16.249453 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_14/pre_mlp_layer_norm/scale/v_row
I0512 23:42:16.249484 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_14/pre_self_attention_layer_norm/scale/m
I0512 23:42:16.249515 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_14/pre_self_attention_layer_norm/scale/v
I0512 23:42:16.249546 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_14/pre_self_attention_layer_norm/scale/v_col
I0512 23:42:16.247293 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_4/pre_self_attention_layer_norm/scale/m
I0512 23:42:16.247327 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_4/pre_self_attention_layer_norm/scale/v
I0512 23:42:16.247360 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_4/pre_self_attention_layer_norm/scale/v_col
I0512 23:42:16.247394 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_4/pre_self_attention_layer_norm/scale/v_row
I0512 23:42:16.247427 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_4/self_attention/key/kernel/m
I0512 23:42:16.247461 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_4/self_attention/key/kernel/v
I0512 23:42:16.247494 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_4/self_attention/key/kernel/v_col
I0512 23:42:16.249577 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_14/pre_self_attention_layer_norm/scale/v_row
I0512 23:42:16.249608 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_14/self_attention/key/kernel/m
I0512 23:42:16.249640 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_14/self_attention/key/kernel/v
I0512 23:42:16.249671 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_14/self_attention/key/kernel/v_col
I0512 23:42:16.249702 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_14/self_attention/key/kernel/v_row
I0512 23:42:16.249733 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_14/self_attention/out/kernel/m
I0512 23:42:16.249764 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_14/self_attention/out/kernel/v
I0512 23:42:16.247527 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_4/self_attention/key/kernel/v_row
I0512 23:42:16.247561 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_4/self_attention/out/kernel/m
I0512 23:42:16.247594 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_4/self_attention/out/kernel/v
I0512 23:42:16.247627 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_4/self_attention/out/kernel/v_col
I0512 23:42:16.247662 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_4/self_attention/out/kernel/v_row
I0512 23:42:16.247695 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_4/self_attention/query/kernel/m
I0512 23:42:16.247728 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_4/self_attention/query/kernel/v
I0512 23:42:16.227781 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/pre_self_attention_layer_norm/scale/v
I0512 23:42:16.227811 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/pre_self_attention_layer_norm/scale/v_col
I0512 23:42:16.227842 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/pre_self_attention_layer_norm/scale/v_row
I0512 23:42:16.227873 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/self_attention/key/kernel/m
I0512 23:42:16.227903 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/self_attention/key/kernel/v
I0512 23:42:16.227933 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/self_attention/key/kernel/v_col
I0512 23:42:16.227964 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/self_attention/key/kernel/v_row
I0512 23:42:16.227995 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/self_attention/out/kernel/m
I0512 23:42:16.228025 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/self_attention/out/kernel/v
I0512 23:42:16.228056 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/self_attention/out/kernel/v_col
I0512 23:42:16.249796 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_14/self_attention/out/kernel/v_col
I0512 23:42:16.249827 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_14/self_attention/out/kernel/v_row
I0512 23:42:16.249858 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_14/self_attention/query/kernel/m
I0512 23:42:16.249889 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_14/self_attention/query/kernel/v
I0512 23:42:16.249921 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_14/self_attention/query/kernel/v_col
I0512 23:42:16.249952 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_14/self_attention/query/kernel/v_row
I0512 23:42:16.249983 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_14/self_attention/value/kernel/m
I0512 23:42:16.250019 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_14/self_attention/value/kernel/v
I0512 23:42:16.250052 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_14/self_attention/value/kernel/v_col
I0512 23:42:16.250084 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_14/self_attention/value/kernel/v_row
I0512 23:42:16.250115 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_15/mlp/wi_0/kernel/m
I0512 23:42:16.250147 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_15/mlp/wi_0/kernel/v
I0512 23:42:16.250178 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_15/mlp/wi_0/kernel/v_col
I0512 23:42:16.250209 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_15/mlp/wi_0/kernel/v_row
I0512 23:42:16.250241 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_15/mlp/wi_1/kernel/m
I0512 23:42:16.247762 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_4/self_attention/query/kernel/v_col
I0512 23:42:16.247795 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_4/self_attention/query/kernel/v_row
I0512 23:42:16.247828 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_4/self_attention/value/kernel/m
I0512 23:42:16.247862 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_4/self_attention/value/kernel/v
I0512 23:42:16.247896 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_4/self_attention/value/kernel/v_col
I0512 23:42:16.247931 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_4/self_attention/value/kernel/v_row
I0512 23:42:16.247964 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/extra_mlp/wi_0/kernel/m
I0512 23:42:16.247998 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/extra_mlp/wi_0/kernel/v
I0512 23:42:16.248032 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/extra_mlp/wi_0/kernel/v_col
I0512 23:42:16.248065 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/extra_mlp/wi_0/kernel/v_row
I0512 23:42:16.248099 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/extra_mlp/wi_1/kernel/m
I0512 23:42:16.248138 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/extra_mlp/wi_1/kernel/v
I0512 23:42:16.248174 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/extra_mlp/wi_1/kernel/v_col
I0512 23:42:16.248207 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/extra_mlp/wi_1/kernel/v_row
I0512 23:42:16.248241 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/extra_mlp/wo/kernel/m
I0512 23:42:16.228087 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/self_attention/out/kernel/v_row
I0512 23:42:16.228118 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/self_attention/query/kernel/m
I0512 23:42:16.228148 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/self_attention/query/kernel/v
I0512 23:42:16.228179 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/self_attention/query/kernel/v_col
I0512 23:42:16.228210 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/self_attention/query/kernel/v_row
I0512 23:42:16.228241 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/self_attention/value/kernel/m
I0512 23:42:16.228272 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/self_attention/value/kernel/v
I0512 23:42:16.228303 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/self_attention/value/kernel/v_col
I0512 23:42:16.228333 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/self_attention/value/kernel/v_row
I0512 23:42:16.228364 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_3/mlp/wi_0/kernel/m
I0512 23:42:16.228395 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_3/mlp/wi_0/kernel/v
I0512 23:42:16.228428 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_3/mlp/wi_0/kernel/v_col
I0512 23:42:16.228460 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_3/mlp/wi_0/kernel/v_row
I0512 23:42:16.228491 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_3/mlp/wi_1/kernel/m
I0512 23:42:16.228522 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_3/mlp/wi_1/kernel/v
I0512 23:42:16.228552 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_3/mlp/wi_1/kernel/v_col
I0512 23:42:16.228582 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_3/mlp/wi_1/kernel/v_row
I0512 23:42:16.228613 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_3/mlp/wo/kernel/m
I0512 23:42:16.228644 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_3/mlp/wo/kernel/v
I0512 23:42:16.228679 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_3/mlp/wo/kernel/v_col
I0512 23:42:16.228711 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_3/mlp/wo/kernel/v_row
I0512 23:42:16.228742 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_3/pre_mlp_layer_norm/scale/m
I0512 23:42:16.228772 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_3/pre_mlp_layer_norm/scale/v
I0512 23:42:16.250296 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_15/mlp/wi_1/kernel/v
I0512 23:42:16.250334 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_15/mlp/wi_1/kernel/v_col
I0512 23:42:16.250367 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_15/mlp/wi_1/kernel/v_row
I0512 23:42:16.250398 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_15/mlp/wo/kernel/m
I0512 23:42:16.250430 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_15/mlp/wo/kernel/v
I0512 23:42:16.250461 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_15/mlp/wo/kernel/v_col
I0512 23:42:16.250492 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_15/mlp/wo/kernel/v_row
I0512 23:42:16.250523 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_15/pre_mlp_layer_norm/scale/m
I0512 23:42:16.250555 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_15/pre_mlp_layer_norm/scale/v
I0512 23:42:16.250586 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_15/pre_mlp_layer_norm/scale/v_col
I0512 23:42:16.250618 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_15/pre_mlp_layer_norm/scale/v_row
I0512 23:42:16.250648 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_15/pre_self_attention_layer_norm/scale/m
I0512 23:42:16.250680 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_15/pre_self_attention_layer_norm/scale/v
I0512 23:42:16.250711 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_15/pre_self_attention_layer_norm/scale/v_col
I0512 23:42:16.250743 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_15/pre_self_attention_layer_norm/scale/v_row
I0512 23:42:16.248275 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/extra_mlp/wo/kernel/v
I0512 23:42:16.248308 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/extra_mlp/wo/kernel/v_col
I0512 23:42:16.248342 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/extra_mlp/wo/kernel/v_row
I0512 23:42:16.248376 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/mlp/expert/wi_0/kernel/m
I0512 23:42:16.248430 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/mlp/expert/wi_0/kernel/v
I0512 23:42:16.248468 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/mlp/expert/wi_0/kernel/v_col
I0512 23:42:16.248503 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/mlp/expert/wi_0/kernel/v_row
I0512 23:42:16.248537 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/mlp/expert/wi_1/kernel/m
I0512 23:42:16.248571 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/mlp/expert/wi_1/kernel/v
I0512 23:42:16.248604 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/mlp/expert/wi_1/kernel/v_col
I0512 23:42:16.248638 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/mlp/expert/wi_1/kernel/v_row
I0512 23:42:16.248671 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/mlp/expert/wo/kernel/m
I0512 23:42:16.248705 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/mlp/expert/wo/kernel/v
I0512 23:42:16.248739 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/mlp/expert/wo/kernel/v_col
I0512 23:42:16.248773 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/mlp/expert/wo/kernel/v_row
I0512 23:42:16.228803 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_3/pre_mlp_layer_norm/scale/v_col
I0512 23:42:16.228834 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_3/pre_mlp_layer_norm/scale/v_row
I0512 23:42:16.228865 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_3/pre_self_attention_layer_norm/scale/m
I0512 23:42:16.228896 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_3/pre_self_attention_layer_norm/scale/v
I0512 23:42:16.228926 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_3/pre_self_attention_layer_norm/scale/v_col
I0512 23:42:16.228957 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_3/pre_self_attention_layer_norm/scale/v_row
I0512 23:42:16.228988 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_3/self_attention/key/kernel/m
I0512 23:42:16.229018 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_3/self_attention/key/kernel/v
I0512 23:42:16.229049 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_3/self_attention/key/kernel/v_col
I0512 23:42:16.229079 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_3/self_attention/key/kernel/v_row
I0512 23:42:16.229110 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_3/self_attention/out/kernel/m
I0512 23:42:16.229141 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_3/self_attention/out/kernel/v
I0512 23:42:16.250774 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_15/self_attention/key/kernel/m
I0512 23:42:16.250806 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_15/self_attention/key/kernel/v
I0512 23:42:16.250837 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_15/self_attention/key/kernel/v_col
I0512 23:42:16.250869 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_15/self_attention/key/kernel/v_row
I0512 23:42:16.250900 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_15/self_attention/out/kernel/m
I0512 23:42:16.250931 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_15/self_attention/out/kernel/v
I0512 23:42:16.250962 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_15/self_attention/out/kernel/v_col
I0512 23:42:16.250993 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_15/self_attention/out/kernel/v_row
I0512 23:42:16.251030 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_15/self_attention/query/kernel/m
I0512 23:42:16.251063 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_15/self_attention/query/kernel/v
I0512 23:42:16.251094 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_15/self_attention/query/kernel/v_col
I0512 23:42:16.251126 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_15/self_attention/query/kernel/v_row
I0512 23:42:16.251157 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_15/self_attention/value/kernel/m
I0512 23:42:16.251188 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_15/self_attention/value/kernel/v
I0512 23:42:16.248806 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/mlp/router/router_weights/w/kernel/m
I0512 23:42:16.248840 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/mlp/router/router_weights/w/kernel/v
I0512 23:42:16.248874 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/mlp/router/router_weights/w/kernel/v_col
I0512 23:42:16.248910 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/mlp/router/router_weights/w/kernel/v_row
I0512 23:42:16.248944 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/pre_extra_mlp_layer_norm/scale/m
I0512 23:42:16.248978 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/pre_extra_mlp_layer_norm/scale/v
I0512 23:42:16.249011 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/pre_extra_mlp_layer_norm/scale/v_col
I0512 23:42:16.249045 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/pre_extra_mlp_layer_norm/scale/v_row
I0512 23:42:16.249079 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/pre_mlp_layer_norm/scale/m
I0512 23:42:16.249112 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/pre_mlp_layer_norm/scale/v
I0512 23:42:16.249152 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/pre_mlp_layer_norm/scale/v_col
I0512 23:42:16.249186 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/pre_mlp_layer_norm/scale/v_row
I0512 23:42:16.249220 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/pre_self_attention_layer_norm/scale/m
I0512 23:42:16.249253 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/pre_self_attention_layer_norm/scale/v
I0512 23:42:16.229171 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_3/self_attention/out/kernel/v_col
I0512 23:42:16.229202 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_3/self_attention/out/kernel/v_row
I0512 23:42:16.229233 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_3/self_attention/query/kernel/m
I0512 23:42:16.229263 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_3/self_attention/query/kernel/v
I0512 23:42:16.229294 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_3/self_attention/query/kernel/v_col
I0512 23:42:16.229324 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_3/self_attention/query/kernel/v_row
I0512 23:42:16.229355 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_3/self_attention/value/kernel/m
I0512 23:42:16.229386 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_3/self_attention/value/kernel/v
I0512 23:42:16.229418 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_3/self_attention/value/kernel/v_col
I0512 23:42:16.229450 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_3/self_attention/value/kernel/v_row
I0512 23:42:16.229481 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_4/mlp/wi_0/kernel/m
I0512 23:42:16.229512 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_4/mlp/wi_0/kernel/v
I0512 23:42:16.229543 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_4/mlp/wi_0/kernel/v_col
I0512 23:42:16.251219 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_15/self_attention/value/kernel/v_col
I0512 23:42:16.251250 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_15/self_attention/value/kernel/v_row
I0512 23:42:16.251289 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_16/mlp/wi_0/kernel/m
I0512 23:42:16.251327 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_16/mlp/wi_0/kernel/v
I0512 23:42:16.251361 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_16/mlp/wi_0/kernel/v_col
I0512 23:42:16.251392 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_16/mlp/wi_0/kernel/v_row
I0512 23:42:16.251430 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_16/mlp/wi_1/kernel/m
I0512 23:42:16.251465 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_16/mlp/wi_1/kernel/v
I0512 23:42:16.251496 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_16/mlp/wi_1/kernel/v_col
I0512 23:42:16.251527 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_16/mlp/wi_1/kernel/v_row
I0512 23:42:16.251557 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_16/mlp/wo/kernel/m
I0512 23:42:16.251588 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_16/mlp/wo/kernel/v
I0512 23:42:16.251618 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_16/mlp/wo/kernel/v_col
I0512 23:42:16.251649 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_16/mlp/wo/kernel/v_row
I0512 23:42:16.251679 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_16/pre_mlp_layer_norm/scale/m
I0512 23:42:16.249286 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/pre_self_attention_layer_norm/scale/v_col
I0512 23:42:16.249320 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/pre_self_attention_layer_norm/scale/v_row
I0512 23:42:16.249354 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/self_attention/key/kernel/m
I0512 23:42:16.249387 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/self_attention/key/kernel/v
I0512 23:42:16.249421 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/self_attention/key/kernel/v_col
I0512 23:42:16.249455 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/self_attention/key/kernel/v_row
I0512 23:42:16.249488 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/self_attention/out/kernel/m
I0512 23:42:16.249521 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/self_attention/out/kernel/v
I0512 23:42:16.249554 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/self_attention/out/kernel/v_col
I0512 23:42:16.249588 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/self_attention/out/kernel/v_row
I0512 23:42:16.249621 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/self_attention/query/kernel/m
I0512 23:42:16.249655 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/self_attention/query/kernel/v
I0512 23:42:16.249688 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/self_attention/query/kernel/v_col
I0512 23:42:16.249721 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/self_attention/query/kernel/v_row
I0512 23:42:16.249755 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/self_attention/value/kernel/m
I0512 23:42:16.229574 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_4/mlp/wi_0/kernel/v_row
I0512 23:42:16.229605 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_4/mlp/wi_1/kernel/m
I0512 23:42:16.229636 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_4/mlp/wi_1/kernel/v
I0512 23:42:16.229671 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_4/mlp/wi_1/kernel/v_col
I0512 23:42:16.229703 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_4/mlp/wi_1/kernel/v_row
I0512 23:42:16.229734 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_4/mlp/wo/kernel/m
I0512 23:42:16.229782 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_4/mlp/wo/kernel/v
I0512 23:42:16.229819 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_4/mlp/wo/kernel/v_col
I0512 23:42:16.229852 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_4/mlp/wo/kernel/v_row
I0512 23:42:16.229883 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_4/pre_mlp_layer_norm/scale/m
I0512 23:42:16.229914 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_4/pre_mlp_layer_norm/scale/v
I0512 23:42:16.229945 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_4/pre_mlp_layer_norm/scale/v_col
I0512 23:42:16.229976 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_4/pre_mlp_layer_norm/scale/v_row
I0512 23:42:16.230007 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_4/pre_self_attention_layer_norm/scale/m
I0512 23:42:16.251710 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_16/pre_mlp_layer_norm/scale/v
I0512 23:42:16.251741 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_16/pre_mlp_layer_norm/scale/v_col
I0512 23:42:16.251771 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_16/pre_mlp_layer_norm/scale/v_row
I0512 23:42:16.251801 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_16/pre_self_attention_layer_norm/scale/m
I0512 23:42:16.251831 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_16/pre_self_attention_layer_norm/scale/v
I0512 23:42:16.251863 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_16/pre_self_attention_layer_norm/scale/v_col
I0512 23:42:16.251895 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_16/pre_self_attention_layer_norm/scale/v_row
I0512 23:42:16.251926 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_16/self_attention/key/kernel/m
I0512 23:42:16.251956 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_16/self_attention/key/kernel/v
I0512 23:42:16.251986 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_16/self_attention/key/kernel/v_col
I0512 23:42:16.252022 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_16/self_attention/key/kernel/v_row
I0512 23:42:16.252053 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_16/self_attention/out/kernel/m
I0512 23:42:16.252083 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_16/self_attention/out/kernel/v
I0512 23:42:16.252112 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_16/self_attention/out/kernel/v_col
I0512 23:42:16.252141 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_16/self_attention/out/kernel/v_row
I0512 23:42:16.230038 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_4/pre_self_attention_layer_norm/scale/v
I0512 23:42:16.230069 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_4/pre_self_attention_layer_norm/scale/v_col
I0512 23:42:16.230100 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_4/pre_self_attention_layer_norm/scale/v_row
I0512 23:42:16.230131 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_4/self_attention/key/kernel/m
I0512 23:42:16.230162 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_4/self_attention/key/kernel/v
I0512 23:42:16.230193 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_4/self_attention/key/kernel/v_col
I0512 23:42:16.230224 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_4/self_attention/key/kernel/v_row
I0512 23:42:16.230255 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_4/self_attention/out/kernel/m
I0512 23:42:16.230285 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_4/self_attention/out/kernel/v
I0512 23:42:16.230316 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_4/self_attention/out/kernel/v_col
I0512 23:42:16.230347 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_4/self_attention/out/kernel/v_row
I0512 23:42:16.230377 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_4/self_attention/query/kernel/m
I0512 23:42:16.230410 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_4/self_attention/query/kernel/v
I0512 23:42:16.230442 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_4/self_attention/query/kernel/v_col
I0512 23:42:16.249788 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/self_attention/value/kernel/v
I0512 23:42:16.249822 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/self_attention/value/kernel/v_col
I0512 23:42:16.249855 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/self_attention/value/kernel/v_row
I0512 23:42:16.249890 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_6/mlp/wi_0/kernel/m
I0512 23:42:16.249925 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_6/mlp/wi_0/kernel/v
I0512 23:42:16.249959 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_6/mlp/wi_0/kernel/v_col
I0512 23:42:16.249992 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_6/mlp/wi_0/kernel/v_row
I0512 23:42:16.250026 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_6/mlp/wi_1/kernel/m
I0512 23:42:16.250059 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_6/mlp/wi_1/kernel/v
I0512 23:42:16.250092 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_6/mlp/wi_1/kernel/v_col
I0512 23:42:16.250126 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_6/mlp/wi_1/kernel/v_row
I0512 23:42:16.250166 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_6/mlp/wo/kernel/m
I0512 23:42:16.250200 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_6/mlp/wo/kernel/v
I0512 23:42:16.250233 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_6/mlp/wo/kernel/v_col
I0512 23:42:16.250267 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_6/mlp/wo/kernel/v_row
I0512 23:42:16.252170 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_16/self_attention/query/kernel/m
I0512 23:42:16.252199 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_16/self_attention/query/kernel/v
I0512 23:42:16.252228 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_16/self_attention/query/kernel/v_col
I0512 23:42:16.252258 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_16/self_attention/query/kernel/v_row
I0512 23:42:16.252289 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_16/self_attention/value/kernel/m
I0512 23:42:16.252319 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_16/self_attention/value/kernel/v
I0512 23:42:16.252348 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_16/self_attention/value/kernel/v_col
I0512 23:42:16.252378 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_16/self_attention/value/kernel/v_row
I0512 23:42:16.252407 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/extra_mlp/wi_0/kernel/m
I0512 23:42:16.252437 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/extra_mlp/wi_0/kernel/v
I0512 23:42:16.252466 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/extra_mlp/wi_0/kernel/v_col
I0512 23:42:16.252496 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/extra_mlp/wi_0/kernel/v_row
I0512 23:42:16.252525 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/extra_mlp/wi_1/kernel/m
I0512 23:42:16.252554 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/extra_mlp/wi_1/kernel/v
I0512 23:42:16.230473 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_4/self_attention/query/kernel/v_row
I0512 23:42:16.230504 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_4/self_attention/value/kernel/m
I0512 23:42:16.230535 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_4/self_attention/value/kernel/v
I0512 23:42:16.230566 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_4/self_attention/value/kernel/v_col
I0512 23:42:16.230597 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_4/self_attention/value/kernel/v_row
I0512 23:42:16.230627 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/extra_mlp/wi_0/kernel/m
I0512 23:42:16.230664 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/extra_mlp/wi_0/kernel/v
I0512 23:42:16.230697 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/extra_mlp/wi_0/kernel/v_col
I0512 23:42:16.230728 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/extra_mlp/wi_0/kernel/v_row
I0512 23:42:16.230759 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/extra_mlp/wi_1/kernel/m
I0512 23:42:16.230790 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/extra_mlp/wi_1/kernel/v
I0512 23:42:16.230821 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/extra_mlp/wi_1/kernel/v_col
I0512 23:42:16.230851 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/extra_mlp/wi_1/kernel/v_row
I0512 23:42:16.230882 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/extra_mlp/wo/kernel/m
I0512 23:42:16.250300 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_6/pre_mlp_layer_norm/scale/m
I0512 23:42:16.250334 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_6/pre_mlp_layer_norm/scale/v
I0512 23:42:16.250369 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_6/pre_mlp_layer_norm/scale/v_col
I0512 23:42:16.250403 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_6/pre_mlp_layer_norm/scale/v_row
I0512 23:42:16.250436 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_6/pre_self_attention_layer_norm/scale/m
I0512 23:42:16.250470 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_6/pre_self_attention_layer_norm/scale/v
I0512 23:42:16.250504 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_6/pre_self_attention_layer_norm/scale/v_col
I0512 23:42:16.250537 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_6/pre_self_attention_layer_norm/scale/v_row
I0512 23:42:16.250570 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_6/self_attention/key/kernel/m
I0512 23:42:16.250604 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_6/self_attention/key/kernel/v
I0512 23:42:16.250638 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_6/self_attention/key/kernel/v_col
I0512 23:42:16.250671 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_6/self_attention/key/kernel/v_row
I0512 23:42:16.250705 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_6/self_attention/out/kernel/m
I0512 23:42:16.250739 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_6/self_attention/out/kernel/v
I0512 23:42:16.250773 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_6/self_attention/out/kernel/v_col
I0512 23:42:16.252584 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/extra_mlp/wi_1/kernel/v_col
I0512 23:42:16.252613 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/extra_mlp/wi_1/kernel/v_row
I0512 23:42:16.252643 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/extra_mlp/wo/kernel/m
I0512 23:42:16.252673 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/extra_mlp/wo/kernel/v
I0512 23:42:16.252703 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/extra_mlp/wo/kernel/v_col
I0512 23:42:16.252733 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/extra_mlp/wo/kernel/v_row
I0512 23:42:16.252762 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/mlp/expert/wi_0/kernel/m
I0512 23:42:16.252794 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/mlp/expert/wi_0/kernel/v
I0512 23:42:16.252826 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/mlp/expert/wi_0/kernel/v_col
I0512 23:42:16.252855 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/mlp/expert/wi_0/kernel/v_row
I0512 23:42:16.252885 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/mlp/expert/wi_1/kernel/m
I0512 23:42:16.252913 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/mlp/expert/wi_1/kernel/v
I0512 23:42:16.252943 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/mlp/expert/wi_1/kernel/v_col
I0512 23:42:16.252972 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/mlp/expert/wi_1/kernel/v_row
I0512 23:42:16.253001 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/mlp/expert/wo/kernel/m
I0512 23:42:16.230913 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/extra_mlp/wo/kernel/v
I0512 23:42:16.230944 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/extra_mlp/wo/kernel/v_col
I0512 23:42:16.230975 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/extra_mlp/wo/kernel/v_row
I0512 23:42:16.231006 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/mlp/expert/wi_0/kernel/m
I0512 23:42:16.231036 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/mlp/expert/wi_0/kernel/v
I0512 23:42:16.231067 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/mlp/expert/wi_0/kernel/v_col
I0512 23:42:16.231098 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/mlp/expert/wi_0/kernel/v_row
I0512 23:42:16.231129 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/mlp/expert/wi_1/kernel/m
I0512 23:42:16.231159 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/mlp/expert/wi_1/kernel/v
I0512 23:42:16.231190 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/mlp/expert/wi_1/kernel/v_col
I0512 23:42:16.231220 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/mlp/expert/wi_1/kernel/v_row
I0512 23:42:16.231251 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/mlp/expert/wo/kernel/m
I0512 23:42:16.231282 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/mlp/expert/wo/kernel/v
I0512 23:42:16.231313 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/mlp/expert/wo/kernel/v_col
I0512 23:42:16.231343 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/mlp/expert/wo/kernel/v_row
I0512 23:42:16.250807 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_6/self_attention/out/kernel/v_row
I0512 23:42:16.250840 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_6/self_attention/query/kernel/m
I0512 23:42:16.250874 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_6/self_attention/query/kernel/v
I0512 23:42:16.250910 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_6/self_attention/query/kernel/v_col
I0512 23:42:16.250943 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_6/self_attention/query/kernel/v_row
I0512 23:42:16.250977 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_6/self_attention/value/kernel/m
I0512 23:42:16.251011 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_6/self_attention/value/kernel/v
I0512 23:42:16.251044 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_6/self_attention/value/kernel/v_col
I0512 23:42:16.251078 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_6/self_attention/value/kernel/v_row
I0512 23:42:16.251111 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_7/mlp/wi_0/kernel/m
I0512 23:42:16.251151 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_7/mlp/wi_0/kernel/v
I0512 23:42:16.251185 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_7/mlp/wi_0/kernel/v_col
I0512 23:42:16.251219 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_7/mlp/wi_0/kernel/v_row
I0512 23:42:16.251252 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_7/mlp/wi_1/kernel/m
I0512 23:42:16.251286 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_7/mlp/wi_1/kernel/v
I0512 23:42:16.253037 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/mlp/expert/wo/kernel/v
I0512 23:42:16.253068 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/mlp/expert/wo/kernel/v_col
I0512 23:42:16.253098 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/mlp/expert/wo/kernel/v_row
I0512 23:42:16.253127 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/mlp/router/router_weights/w/kernel/m
I0512 23:42:16.253157 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/mlp/router/router_weights/w/kernel/v
I0512 23:42:16.253186 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/mlp/router/router_weights/w/kernel/v_col
I0512 23:42:16.253216 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/mlp/router/router_weights/w/kernel/v_row
I0512 23:42:16.253245 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/pre_extra_mlp_layer_norm/scale/m
I0512 23:42:16.253276 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/pre_extra_mlp_layer_norm/scale/v
I0512 23:42:16.253306 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/pre_extra_mlp_layer_norm/scale/v_col
I0512 23:42:16.253335 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/pre_extra_mlp_layer_norm/scale/v_row
I0512 23:42:16.253365 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/pre_mlp_layer_norm/scale/m
I0512 23:42:16.253394 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/pre_mlp_layer_norm/scale/v
I0512 23:42:16.253424 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/pre_mlp_layer_norm/scale/v_col
I0512 23:42:16.253453 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/pre_mlp_layer_norm/scale/v_row
I0512 23:42:16.253482 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/pre_self_attention_layer_norm/scale/m
I0512 23:42:16.253511 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/pre_self_attention_layer_norm/scale/v
I0512 23:42:16.253540 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/pre_self_attention_layer_norm/scale/v_col
I0512 23:42:16.253570 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/pre_self_attention_layer_norm/scale/v_row
I0512 23:42:16.253598 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/self_attention/key/kernel/m
I0512 23:42:16.253628 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/self_attention/key/kernel/v
I0512 23:42:16.253657 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/self_attention/key/kernel/v_col
I0512 23:42:16.251320 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_7/mlp/wi_1/kernel/v_col
I0512 23:42:16.251354 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_7/mlp/wi_1/kernel/v_row
I0512 23:42:16.251387 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_7/mlp/wo/kernel/m
I0512 23:42:16.251421 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_7/mlp/wo/kernel/v
I0512 23:42:16.251455 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_7/mlp/wo/kernel/v_col
I0512 23:42:16.251488 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_7/mlp/wo/kernel/v_row
I0512 23:42:16.251522 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_7/pre_mlp_layer_norm/scale/m
I0512 23:42:16.251556 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_7/pre_mlp_layer_norm/scale/v
I0512 23:42:16.231374 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/mlp/router/router_weights/w/kernel/m
I0512 23:42:16.231405 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/mlp/router/router_weights/w/kernel/v
I0512 23:42:16.231438 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/mlp/router/router_weights/w/kernel/v_col
I0512 23:42:16.231470 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/mlp/router/router_weights/w/kernel/v_row
I0512 23:42:16.231501 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/pre_extra_mlp_layer_norm/scale/m
I0512 23:42:16.231531 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/pre_extra_mlp_layer_norm/scale/v
I0512 23:42:16.231562 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/pre_extra_mlp_layer_norm/scale/v_col
I0512 23:42:16.231593 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/pre_extra_mlp_layer_norm/scale/v_row
I0512 23:42:16.231623 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/pre_mlp_layer_norm/scale/m
I0512 23:42:16.231660 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/pre_mlp_layer_norm/scale/v
I0512 23:42:16.231692 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/pre_mlp_layer_norm/scale/v_col
I0512 23:42:16.231723 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/pre_mlp_layer_norm/scale/v_row
I0512 23:42:16.231754 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/pre_self_attention_layer_norm/scale/m
I0512 23:42:16.231785 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/pre_self_attention_layer_norm/scale/v
I0512 23:42:16.231816 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/pre_self_attention_layer_norm/scale/v_col
I0512 23:42:16.253687 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/self_attention/key/kernel/v_row
I0512 23:42:16.253716 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/self_attention/out/kernel/m
I0512 23:42:16.253746 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/self_attention/out/kernel/v
I0512 23:42:16.253775 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/self_attention/out/kernel/v_col
I0512 23:42:16.253808 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/self_attention/out/kernel/v_row
I0512 23:42:16.253839 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/self_attention/query/kernel/m
I0512 23:42:16.253868 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/self_attention/query/kernel/v
I0512 23:42:16.251590 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_7/pre_mlp_layer_norm/scale/v_col
I0512 23:42:16.251623 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_7/pre_mlp_layer_norm/scale/v_row
I0512 23:42:16.251657 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_7/pre_self_attention_layer_norm/scale/m
I0512 23:42:16.251690 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_7/pre_self_attention_layer_norm/scale/v
I0512 23:42:16.251724 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_7/pre_self_attention_layer_norm/scale/v_col
I0512 23:42:16.251758 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_7/pre_self_attention_layer_norm/scale/v_row
I0512 23:42:16.251791 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_7/self_attention/key/kernel/m
I0512 23:42:16.231846 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/pre_self_attention_layer_norm/scale/v_row
I0512 23:42:16.231877 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/self_attention/key/kernel/m
I0512 23:42:16.231908 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/self_attention/key/kernel/v
I0512 23:42:16.231939 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/self_attention/key/kernel/v_col
I0512 23:42:16.231969 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/self_attention/key/kernel/v_row
I0512 23:42:16.232000 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/self_attention/out/kernel/m
I0512 23:42:16.232030 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/self_attention/out/kernel/v
I0512 23:42:16.232061 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/self_attention/out/kernel/v_col
I0512 23:42:16.232091 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/self_attention/out/kernel/v_row
I0512 23:42:16.232122 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/self_attention/query/kernel/m
I0512 23:42:16.232152 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/self_attention/query/kernel/v
I0512 23:42:16.232183 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/self_attention/query/kernel/v_col
I0512 23:42:16.232213 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/self_attention/query/kernel/v_row
I0512 23:42:16.232244 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/self_attention/value/kernel/m
I0512 23:42:16.251824 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_7/self_attention/key/kernel/v
I0512 23:42:16.251859 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_7/self_attention/key/kernel/v_col
I0512 23:42:16.251893 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_7/self_attention/key/kernel/v_row
I0512 23:42:16.251928 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_7/self_attention/out/kernel/m
I0512 23:42:16.251962 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_7/self_attention/out/kernel/v
I0512 23:42:16.251996 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_7/self_attention/out/kernel/v_col
I0512 23:42:16.252029 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_7/self_attention/out/kernel/v_row
I0512 23:42:16.253897 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/self_attention/query/kernel/v_col
I0512 23:42:16.253927 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/self_attention/query/kernel/v_row
I0512 23:42:16.253956 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/self_attention/value/kernel/m
I0512 23:42:16.253985 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/self_attention/value/kernel/v
I0512 23:42:16.254020 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/self_attention/value/kernel/v_col
I0512 23:42:16.254051 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/self_attention/value/kernel/v_row
I0512 23:42:16.254080 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_18/mlp/wi_0/kernel/m
I0512 23:42:16.254110 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_18/mlp/wi_0/kernel/v
I0512 23:42:16.254140 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_18/mlp/wi_0/kernel/v_col
I0512 23:42:16.254170 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_18/mlp/wi_0/kernel/v_row
I0512 23:42:16.254200 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_18/mlp/wi_1/kernel/m
I0512 23:42:16.254230 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_18/mlp/wi_1/kernel/v
I0512 23:42:16.254278 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_18/mlp/wi_1/kernel/v_col
I0512 23:42:16.254314 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_18/mlp/wi_1/kernel/v_row
I0512 23:42:16.254345 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_18/mlp/wo/kernel/m
I0512 23:42:16.252063 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_7/self_attention/query/kernel/m
I0512 23:42:16.252097 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_7/self_attention/query/kernel/v
I0512 23:42:16.252135 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_7/self_attention/query/kernel/v_col
I0512 23:42:16.252171 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_7/self_attention/query/kernel/v_row
I0512 23:42:16.252205 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_7/self_attention/value/kernel/m
I0512 23:42:16.252238 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_7/self_attention/value/kernel/v
I0512 23:42:16.252272 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_7/self_attention/value/kernel/v_col
I0512 23:42:16.252305 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_7/self_attention/value/kernel/v_row
I0512 23:42:16.252339 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_8/mlp/wi_0/kernel/m
I0512 23:42:16.252373 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_8/mlp/wi_0/kernel/v
I0512 23:42:16.252425 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_8/mlp/wi_0/kernel/v_col
I0512 23:42:16.252462 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_8/mlp/wi_0/kernel/v_row
I0512 23:42:16.252503 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_8/mlp/wi_1/kernel/m
I0512 23:42:16.252536 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_8/mlp/wi_1/kernel/v
I0512 23:42:16.252573 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_8/mlp/wi_1/kernel/v_col
I0512 23:42:16.252606 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_8/mlp/wi_1/kernel/v_row
I0512 23:42:16.252644 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_8/mlp/wo/kernel/m
I0512 23:42:16.232275 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/self_attention/value/kernel/v
I0512 23:42:16.232306 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/self_attention/value/kernel/v_col
I0512 23:42:16.232336 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/self_attention/value/kernel/v_row
I0512 23:42:16.232367 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_6/mlp/wi_0/kernel/m
I0512 23:42:16.232398 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_6/mlp/wi_0/kernel/v
I0512 23:42:16.232431 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_6/mlp/wi_0/kernel/v_col
I0512 23:42:16.232463 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_6/mlp/wi_0/kernel/v_row
I0512 23:42:16.232494 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_6/mlp/wi_1/kernel/m
I0512 23:42:16.232525 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_6/mlp/wi_1/kernel/v
I0512 23:42:16.232556 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_6/mlp/wi_1/kernel/v_col
I0512 23:42:16.232586 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_6/mlp/wi_1/kernel/v_row
I0512 23:42:16.232617 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_6/mlp/wo/kernel/m
I0512 23:42:16.232647 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_6/mlp/wo/kernel/v
I0512 23:42:16.232684 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_6/mlp/wo/kernel/v_col
I0512 23:42:16.232715 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_6/mlp/wo/kernel/v_row
I0512 23:42:16.232746 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_6/pre_mlp_layer_norm/scale/m
I0512 23:42:16.252678 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_8/mlp/wo/kernel/v
I0512 23:42:16.252712 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_8/mlp/wo/kernel/v_col
I0512 23:42:16.252745 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_8/mlp/wo/kernel/v_row
I0512 23:42:16.252779 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_8/pre_mlp_layer_norm/scale/m
I0512 23:42:16.252812 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_8/pre_mlp_layer_norm/scale/v
I0512 23:42:16.252845 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_8/pre_mlp_layer_norm/scale/v_col
I0512 23:42:16.252879 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_8/pre_mlp_layer_norm/scale/v_row
I0512 23:42:16.252915 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_8/pre_self_attention_layer_norm/scale/m
I0512 23:42:16.254375 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_18/mlp/wo/kernel/v
I0512 23:42:16.254405 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_18/mlp/wo/kernel/v_col
I0512 23:42:16.254435 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_18/mlp/wo/kernel/v_row
I0512 23:42:16.254498 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_18/pre_mlp_layer_norm/scale/m
I0512 23:42:16.254552 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_18/pre_mlp_layer_norm/scale/v
I0512 23:42:16.254587 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_18/pre_mlp_layer_norm/scale/v_col
I0512 23:42:16.254618 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_18/pre_mlp_layer_norm/scale/v_row
I0512 23:42:16.254647 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_18/pre_self_attention_layer_norm/scale/m
I0512 23:42:16.254677 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_18/pre_self_attention_layer_norm/scale/v
I0512 23:42:16.254706 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_18/pre_self_attention_layer_norm/scale/v_col
I0512 23:42:16.254736 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_18/pre_self_attention_layer_norm/scale/v_row
I0512 23:42:16.254765 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_18/self_attention/key/kernel/m
I0512 23:42:16.254795 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_18/self_attention/key/kernel/v
I0512 23:42:16.254824 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_18/self_attention/key/kernel/v_col
I0512 23:42:16.254854 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_18/self_attention/key/kernel/v_row
I0512 23:42:16.252949 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_8/pre_self_attention_layer_norm/scale/v
I0512 23:42:16.252983 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_8/pre_self_attention_layer_norm/scale/v_col
I0512 23:42:16.253017 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_8/pre_self_attention_layer_norm/scale/v_row
I0512 23:42:16.253050 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_8/self_attention/key/kernel/m
I0512 23:42:16.232777 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_6/pre_mlp_layer_norm/scale/v
I0512 23:42:16.232808 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_6/pre_mlp_layer_norm/scale/v_col
I0512 23:42:16.232838 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_6/pre_mlp_layer_norm/scale/v_row
I0512 23:42:16.232869 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_6/pre_self_attention_layer_norm/scale/m
I0512 23:42:16.232900 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_6/pre_self_attention_layer_norm/scale/v
I0512 23:42:16.232930 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_6/pre_self_attention_layer_norm/scale/v_col
I0512 23:42:16.232960 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_6/pre_self_attention_layer_norm/scale/v_row
I0512 23:42:16.232991 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_6/self_attention/key/kernel/m
I0512 23:42:16.233021 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_6/self_attention/key/kernel/v
I0512 23:42:16.233052 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_6/self_attention/key/kernel/v_col
I0512 23:42:16.233083 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_6/self_attention/key/kernel/v_row
I0512 23:42:16.233114 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_6/self_attention/out/kernel/m
I0512 23:42:16.233144 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_6/self_attention/out/kernel/v
I0512 23:42:16.233175 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_6/self_attention/out/kernel/v_col
I0512 23:42:16.254883 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_18/self_attention/out/kernel/m
I0512 23:42:16.254912 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_18/self_attention/out/kernel/v
I0512 23:42:16.254942 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_18/self_attention/out/kernel/v_col
I0512 23:42:16.254971 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_18/self_attention/out/kernel/v_row
I0512 23:42:16.255000 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_18/self_attention/query/kernel/m
I0512 23:42:16.255035 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_18/self_attention/query/kernel/v
I0512 23:42:16.255065 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_18/self_attention/query/kernel/v_col
I0512 23:42:16.255094 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_18/self_attention/query/kernel/v_row
I0512 23:42:16.255123 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_18/self_attention/value/kernel/m
I0512 23:42:16.255152 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_18/self_attention/value/kernel/v
I0512 23:42:16.255182 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_18/self_attention/value/kernel/v_col
I0512 23:42:16.255211 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_18/self_attention/value/kernel/v_row
I0512 23:42:16.255240 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_19/mlp/wi_0/kernel/m
I0512 23:42:16.255271 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_19/mlp/wi_0/kernel/v
I0512 23:42:16.253084 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_8/self_attention/key/kernel/v
I0512 23:42:16.253118 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_8/self_attention/key/kernel/v_col
I0512 23:42:16.253158 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_8/self_attention/key/kernel/v_row
I0512 23:42:16.253191 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_8/self_attention/out/kernel/m
I0512 23:42:16.253226 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_8/self_attention/out/kernel/v
I0512 23:42:16.253259 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_8/self_attention/out/kernel/v_col
I0512 23:42:16.253293 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_8/self_attention/out/kernel/v_row
I0512 23:42:16.253326 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_8/self_attention/query/kernel/m
I0512 23:42:16.253360 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_8/self_attention/query/kernel/v
I0512 23:42:16.253394 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_8/self_attention/query/kernel/v_col
I0512 23:42:16.253428 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_8/self_attention/query/kernel/v_row
I0512 23:42:16.253462 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_8/self_attention/value/kernel/m
I0512 23:42:16.253496 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_8/self_attention/value/kernel/v
I0512 23:42:16.253529 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_8/self_attention/value/kernel/v_col
I0512 23:42:16.253562 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_8/self_attention/value/kernel/v_row
I0512 23:42:16.253596 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_9/mlp/wi_0/kernel/m
I0512 23:42:16.255302 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_19/mlp/wi_0/kernel/v_col
I0512 23:42:16.255332 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_19/mlp/wi_0/kernel/v_row
I0512 23:42:16.255362 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_19/mlp/wi_1/kernel/m
I0512 23:42:16.255392 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_19/mlp/wi_1/kernel/v
I0512 23:42:16.255421 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_19/mlp/wi_1/kernel/v_col
I0512 23:42:16.255451 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_19/mlp/wi_1/kernel/v_row
I0512 23:42:16.255481 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_19/mlp/wo/kernel/m
I0512 23:42:16.255511 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_19/mlp/wo/kernel/v
I0512 23:42:16.233206 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_6/self_attention/out/kernel/v_row
I0512 23:42:16.233237 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_6/self_attention/query/kernel/m
I0512 23:42:16.233268 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_6/self_attention/query/kernel/v
I0512 23:42:16.233298 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_6/self_attention/query/kernel/v_col
I0512 23:42:16.233329 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_6/self_attention/query/kernel/v_row
I0512 23:42:16.233359 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_6/self_attention/value/kernel/m
I0512 23:42:16.233390 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_6/self_attention/value/kernel/v
I0512 23:42:16.233422 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_6/self_attention/value/kernel/v_col
I0512 23:42:16.233454 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_6/self_attention/value/kernel/v_row
I0512 23:42:16.233485 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_7/mlp/wi_0/kernel/m
I0512 23:42:16.233516 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_7/mlp/wi_0/kernel/v
I0512 23:42:16.233547 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_7/mlp/wi_0/kernel/v_col
I0512 23:42:16.233577 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_7/mlp/wi_0/kernel/v_row
I0512 23:42:16.233608 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_7/mlp/wi_1/kernel/m
I0512 23:42:16.233639 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_7/mlp/wi_1/kernel/v
I0512 23:42:16.253630 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_9/mlp/wi_0/kernel/v
I0512 23:42:16.253663 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_9/mlp/wi_0/kernel/v_col
I0512 23:42:16.253697 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_9/mlp/wi_0/kernel/v_row
I0512 23:42:16.253731 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_9/mlp/wi_1/kernel/m
I0512 23:42:16.253764 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_9/mlp/wi_1/kernel/v
I0512 23:42:16.253797 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_9/mlp/wi_1/kernel/v_col
I0512 23:42:16.253831 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_9/mlp/wi_1/kernel/v_row
I0512 23:42:16.255541 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_19/mlp/wo/kernel/v_col
I0512 23:42:16.255570 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_19/mlp/wo/kernel/v_row
I0512 23:42:16.255600 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_19/pre_mlp_layer_norm/scale/m
I0512 23:42:16.255630 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_19/pre_mlp_layer_norm/scale/v
I0512 23:42:16.255659 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_19/pre_mlp_layer_norm/scale/v_col
I0512 23:42:16.255688 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_19/pre_mlp_layer_norm/scale/v_row
I0512 23:42:16.255718 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_19/pre_self_attention_layer_norm/scale/m
I0512 23:42:16.253864 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_9/mlp/wo/kernel/m
I0512 23:42:16.253899 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_9/mlp/wo/kernel/v
I0512 23:42:16.253933 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_9/mlp/wo/kernel/v_col
I0512 23:42:16.253967 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_9/mlp/wo/kernel/v_row
I0512 23:42:16.254001 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_9/pre_mlp_layer_norm/scale/m
I0512 23:42:16.254035 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_9/pre_mlp_layer_norm/scale/v
I0512 23:42:16.254068 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_9/pre_mlp_layer_norm/scale/v_col
I0512 23:42:16.255747 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_19/pre_self_attention_layer_norm/scale/v
I0512 23:42:16.255777 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_19/pre_self_attention_layer_norm/scale/v_col
I0512 23:42:16.255806 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_19/pre_self_attention_layer_norm/scale/v_row
I0512 23:42:16.255835 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_19/self_attention/key/kernel/m
I0512 23:42:16.255864 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_19/self_attention/key/kernel/v
I0512 23:42:16.255893 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_19/self_attention/key/kernel/v_col
I0512 23:42:16.255923 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_19/self_attention/key/kernel/v_row
I0512 23:42:16.255952 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_19/self_attention/out/kernel/m
I0512 23:42:16.255981 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_19/self_attention/out/kernel/v
I0512 23:42:16.256011 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_19/self_attention/out/kernel/v_col
I0512 23:42:16.256047 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_19/self_attention/out/kernel/v_row
I0512 23:42:16.256076 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_19/self_attention/query/kernel/m
I0512 23:42:16.256105 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_19/self_attention/query/kernel/v
I0512 23:42:16.256134 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_19/self_attention/query/kernel/v_col
I0512 23:42:16.256164 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_19/self_attention/query/kernel/v_row
I0512 23:42:16.233675 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_7/mlp/wi_1/kernel/v_col
I0512 23:42:16.233708 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_7/mlp/wi_1/kernel/v_row
I0512 23:42:16.233763 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_7/mlp/wo/kernel/m
I0512 23:42:16.233821 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_7/mlp/wo/kernel/v
I0512 23:42:16.233854 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_7/mlp/wo/kernel/v_col
I0512 23:42:16.233885 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_7/mlp/wo/kernel/v_row
I0512 23:42:16.233916 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_7/pre_mlp_layer_norm/scale/m
I0512 23:42:16.233947 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_7/pre_mlp_layer_norm/scale/v
I0512 23:42:16.233979 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_7/pre_mlp_layer_norm/scale/v_col
I0512 23:42:16.234010 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_7/pre_mlp_layer_norm/scale/v_row
I0512 23:42:16.234040 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_7/pre_self_attention_layer_norm/scale/m
I0512 23:42:16.234071 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_7/pre_self_attention_layer_norm/scale/v
I0512 23:42:16.234102 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_7/pre_self_attention_layer_norm/scale/v_col
I0512 23:42:16.234133 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_7/pre_self_attention_layer_norm/scale/v_row
I0512 23:42:16.234164 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_7/self_attention/key/kernel/m
I0512 23:42:16.254101 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_9/pre_mlp_layer_norm/scale/v_row
I0512 23:42:16.254140 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_9/pre_self_attention_layer_norm/scale/m
I0512 23:42:16.254175 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_9/pre_self_attention_layer_norm/scale/v
I0512 23:42:16.254209 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_9/pre_self_attention_layer_norm/scale/v_col
I0512 23:42:16.254242 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_9/pre_self_attention_layer_norm/scale/v_row
I0512 23:42:16.254276 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_9/self_attention/key/kernel/m
I0512 23:42:16.254310 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_9/self_attention/key/kernel/v
I0512 23:42:16.256192 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_19/self_attention/value/kernel/m
I0512 23:42:16.256221 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_19/self_attention/value/kernel/v
I0512 23:42:16.256250 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_19/self_attention/value/kernel/v_col
I0512 23:42:16.256281 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_19/self_attention/value/kernel/v_row
I0512 23:42:16.256311 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_2/mlp/wi_0/kernel/m
I0512 23:42:16.256340 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_2/mlp/wi_0/kernel/v
I0512 23:42:16.256370 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_2/mlp/wi_0/kernel/v_col
I0512 23:42:16.256399 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_2/mlp/wi_0/kernel/v_row
I0512 23:42:16.256429 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_2/mlp/wi_1/kernel/m
I0512 23:42:16.256458 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_2/mlp/wi_1/kernel/v
I0512 23:42:16.256488 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_2/mlp/wi_1/kernel/v_col
I0512 23:42:16.256518 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_2/mlp/wi_1/kernel/v_row
I0512 23:42:16.256547 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_2/mlp/wo/kernel/m
I0512 23:42:16.256576 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_2/mlp/wo/kernel/v
I0512 23:42:16.256606 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_2/mlp/wo/kernel/v_col
I0512 23:42:16.254343 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_9/self_attention/key/kernel/v_col
I0512 23:42:16.254377 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_9/self_attention/key/kernel/v_row
I0512 23:42:16.254411 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_9/self_attention/out/kernel/m
I0512 23:42:16.254444 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_9/self_attention/out/kernel/v
I0512 23:42:16.254478 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_9/self_attention/out/kernel/v_col
I0512 23:42:16.254512 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_9/self_attention/out/kernel/v_row
I0512 23:42:16.254545 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_9/self_attention/query/kernel/m
I0512 23:42:16.254579 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_9/self_attention/query/kernel/v
I0512 23:42:16.234195 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_7/self_attention/key/kernel/v
I0512 23:42:16.234226 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_7/self_attention/key/kernel/v_col
I0512 23:42:16.234257 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_7/self_attention/key/kernel/v_row
I0512 23:42:16.234288 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_7/self_attention/out/kernel/m
I0512 23:42:16.234319 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_7/self_attention/out/kernel/v
I0512 23:42:16.234349 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_7/self_attention/out/kernel/v_col
I0512 23:42:16.234380 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_7/self_attention/out/kernel/v_row
I0512 23:42:16.234411 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_7/self_attention/query/kernel/m
I0512 23:42:16.234444 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_7/self_attention/query/kernel/v
I0512 23:42:16.234475 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_7/self_attention/query/kernel/v_col
I0512 23:42:16.234506 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_7/self_attention/query/kernel/v_row
I0512 23:42:16.234537 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_7/self_attention/value/kernel/m
I0512 23:42:16.234568 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_7/self_attention/value/kernel/v
I0512 23:42:16.234598 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_7/self_attention/value/kernel/v_col
I0512 23:42:16.234629 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_7/self_attention/value/kernel/v_row
I0512 23:42:16.254612 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_9/self_attention/query/kernel/v_col
I0512 23:42:16.254646 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_9/self_attention/query/kernel/v_row
I0512 23:42:16.254680 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_9/self_attention/value/kernel/m
I0512 23:42:16.254714 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_9/self_attention/value/kernel/v
I0512 23:42:16.254747 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_9/self_attention/value/kernel/v_col
I0512 23:42:16.254780 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_9/self_attention/value/kernel/v_row
I0512 23:42:16.254814 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/logits_dense/kernel/m
I0512 23:42:16.254848 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/logits_dense/kernel/v
I0512 23:42:16.254882 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/logits_dense/kernel/v_col
I0512 23:42:16.256636 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_2/mlp/wo/kernel/v_row
I0512 23:42:16.256665 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_2/pre_mlp_layer_norm/scale/m
I0512 23:42:16.256695 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_2/pre_mlp_layer_norm/scale/v
I0512 23:42:16.256725 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_2/pre_mlp_layer_norm/scale/v_col
I0512 23:42:16.256754 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_2/pre_mlp_layer_norm/scale/v_row
I0512 23:42:16.256783 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_2/pre_self_attention_layer_norm/scale/m
I0512 23:42:16.256813 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_2/pre_self_attention_layer_norm/scale/v
I0512 23:42:16.256842 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_2/pre_self_attention_layer_norm/scale/v_col
I0512 23:42:16.256871 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_2/pre_self_attention_layer_norm/scale/v_row
I0512 23:42:16.256900 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_2/self_attention/key/kernel/m
I0512 23:42:16.256929 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_2/self_attention/key/kernel/v
I0512 23:42:16.256958 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_2/self_attention/key/kernel/v_col
I0512 23:42:16.256987 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_2/self_attention/key/kernel/v_row
I0512 23:42:16.257021 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_2/self_attention/out/kernel/m
I0512 23:42:16.257052 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_2/self_attention/out/kernel/v
I0512 23:42:16.234665 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_8/mlp/wi_0/kernel/m
I0512 23:42:16.234698 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_8/mlp/wi_0/kernel/v
I0512 23:42:16.234729 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_8/mlp/wi_0/kernel/v_col
I0512 23:42:16.234760 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_8/mlp/wi_0/kernel/v_row
I0512 23:42:16.234791 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_8/mlp/wi_1/kernel/m
I0512 23:42:16.234821 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_8/mlp/wi_1/kernel/v
I0512 23:42:16.234852 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_8/mlp/wi_1/kernel/v_col
I0512 23:42:16.234883 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_8/mlp/wi_1/kernel/v_row
I0512 23:42:16.234913 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_8/mlp/wo/kernel/m
I0512 23:42:16.234944 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_8/mlp/wo/kernel/v
I0512 23:42:16.234974 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_8/mlp/wo/kernel/v_col
I0512 23:42:16.235005 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_8/mlp/wo/kernel/v_row
I0512 23:42:16.235036 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_8/pre_mlp_layer_norm/scale/m
I0512 23:42:16.235067 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_8/pre_mlp_layer_norm/scale/v
I0512 23:42:16.235097 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_8/pre_mlp_layer_norm/scale/v_col
I0512 23:42:16.254918 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/logits_dense/kernel/v_row
I0512 23:42:16.254951 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/token_embedder/embedding/m
I0512 23:42:16.254985 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/token_embedder/embedding/v
I0512 23:42:16.255019 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/token_embedder/embedding/v_col
I0512 23:42:16.255053 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/param_states/token_embedder/embedding/v_row
I0512 23:42:16.255086 140440593786880 checkpoints.py:1100] Restoring key from ckpt: state/step
I0512 23:42:16.255120 140440593786880 checkpoints.py:1100] Restoring key from ckpt: target/decoder/decoder_norm/scale
I0512 23:42:16.255160 140440593786880 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_0/mlp/wi_0/kernel
I0512 23:42:16.255195 140440593786880 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_0/mlp/wi_1/kernel
I0512 23:42:16.255228 140440593786880 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_0/mlp/wo/kernel
I0512 23:42:16.255262 140440593786880 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_0/pre_mlp_layer_norm/scale
I0512 23:42:16.257082 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_2/self_attention/out/kernel/v_col
I0512 23:42:16.257111 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_2/self_attention/out/kernel/v_row
I0512 23:42:16.257140 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_2/self_attention/query/kernel/m
I0512 23:42:16.257170 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_2/self_attention/query/kernel/v
I0512 23:42:16.257199 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_2/self_attention/query/kernel/v_col
I0512 23:42:16.257228 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_2/self_attention/query/kernel/v_row
I0512 23:42:16.257257 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_2/self_attention/value/kernel/m
I0512 23:42:16.257288 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_2/self_attention/value/kernel/v
I0512 23:42:16.257318 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_2/self_attention/value/kernel/v_col
I0512 23:42:16.257347 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_2/self_attention/value/kernel/v_row
I0512 23:42:16.257376 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_20/mlp/wi_0/kernel/m
I0512 23:42:16.257405 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_20/mlp/wi_0/kernel/v
I0512 23:42:16.257435 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_20/mlp/wi_0/kernel/v_col
I0512 23:42:16.257464 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_20/mlp/wi_0/kernel/v_row
I0512 23:42:16.257493 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_20/mlp/wi_1/kernel/m
I0512 23:42:16.235129 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_8/pre_mlp_layer_norm/scale/v_row
I0512 23:42:16.235160 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_8/pre_self_attention_layer_norm/scale/m
I0512 23:42:16.235191 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_8/pre_self_attention_layer_norm/scale/v
I0512 23:42:16.235222 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_8/pre_self_attention_layer_norm/scale/v_col
I0512 23:42:16.235253 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_8/pre_self_attention_layer_norm/scale/v_row
I0512 23:42:16.235284 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_8/self_attention/key/kernel/m
I0512 23:42:16.235315 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_8/self_attention/key/kernel/v
I0512 23:42:16.235345 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_8/self_attention/key/kernel/v_col
I0512 23:42:16.235376 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_8/self_attention/key/kernel/v_row
I0512 23:42:16.235408 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_8/self_attention/out/kernel/m
I0512 23:42:16.235441 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_8/self_attention/out/kernel/v
I0512 23:42:16.235472 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_8/self_attention/out/kernel/v_col
I0512 23:42:16.235503 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_8/self_attention/out/kernel/v_row
I0512 23:42:16.235534 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_8/self_attention/query/kernel/m
I0512 23:42:16.235566 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_8/self_attention/query/kernel/v
I0512 23:42:16.255296 140440593786880 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_0/pre_self_attention_layer_norm/scale
I0512 23:42:16.255330 140440593786880 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_0/self_attention/key/kernel
I0512 23:42:16.255364 140440593786880 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_0/self_attention/out/kernel
I0512 23:42:16.255397 140440593786880 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_0/self_attention/query/kernel
I0512 23:42:16.255431 140440593786880 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_0/self_attention/value/kernel
I0512 23:42:16.255464 140440593786880 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_1/mlp/wi_0/kernel
I0512 23:42:16.255497 140440593786880 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_1/mlp/wi_1/kernel
I0512 23:42:16.255531 140440593786880 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_1/mlp/wo/kernel
I0512 23:42:16.255565 140440593786880 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_1/pre_mlp_layer_norm/scale
I0512 23:42:16.255598 140440593786880 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_1/pre_self_attention_layer_norm/scale
I0512 23:42:16.255632 140440593786880 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_1/self_attention/key/kernel
I0512 23:42:16.257523 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_20/mlp/wi_1/kernel/v
I0512 23:42:16.257552 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_20/mlp/wi_1/kernel/v_col
I0512 23:42:16.257582 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_20/mlp/wi_1/kernel/v_row
I0512 23:42:16.257611 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_20/mlp/wo/kernel/m
I0512 23:42:16.257640 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_20/mlp/wo/kernel/v
I0512 23:42:16.257669 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_20/mlp/wo/kernel/v_col
I0512 23:42:16.257698 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_20/mlp/wo/kernel/v_row
I0512 23:42:16.257728 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_20/pre_mlp_layer_norm/scale/m
I0512 23:42:16.257757 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_20/pre_mlp_layer_norm/scale/v
I0512 23:42:16.257786 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_20/pre_mlp_layer_norm/scale/v_col
I0512 23:42:16.257815 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_20/pre_mlp_layer_norm/scale/v_row
I0512 23:42:16.257844 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_20/pre_self_attention_layer_norm/scale/m
I0512 23:42:16.257874 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_20/pre_self_attention_layer_norm/scale/v
I0512 23:42:16.257903 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_20/pre_self_attention_layer_norm/scale/v_col
I0512 23:42:16.255666 140440593786880 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_1/self_attention/out/kernel
I0512 23:42:16.255718 140440593786880 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_1/self_attention/query/kernel
I0512 23:42:16.255757 140440593786880 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_1/self_attention/value/kernel
I0512 23:42:16.255792 140440593786880 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_10/mlp/wi_0/kernel
I0512 23:42:16.255826 140440593786880 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_10/mlp/wi_1/kernel
I0512 23:42:16.255860 140440593786880 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_10/mlp/wo/kernel
I0512 23:42:16.255895 140440593786880 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_10/pre_mlp_layer_norm/scale
I0512 23:42:16.255930 140440593786880 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_10/pre_self_attention_layer_norm/scale
I0512 23:42:16.255964 140440593786880 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_10/self_attention/key/kernel
I0512 23:42:16.255997 140440593786880 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_10/self_attention/out/kernel
I0512 23:42:16.256031 140440593786880 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_10/self_attention/query/kernel
I0512 23:42:16.235597 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_8/self_attention/query/kernel/v_col
I0512 23:42:16.235628 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_8/self_attention/query/kernel/v_row
I0512 23:42:16.235664 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_8/self_attention/value/kernel/m
I0512 23:42:16.235697 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_8/self_attention/value/kernel/v
I0512 23:42:16.235728 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_8/self_attention/value/kernel/v_col
I0512 23:42:16.235759 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_8/self_attention/value/kernel/v_row
I0512 23:42:16.235790 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_9/mlp/wi_0/kernel/m
I0512 23:42:16.235821 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_9/mlp/wi_0/kernel/v
I0512 23:42:16.235853 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_9/mlp/wi_0/kernel/v_col
I0512 23:42:16.235884 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_9/mlp/wi_0/kernel/v_row
I0512 23:42:16.235915 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_9/mlp/wi_1/kernel/m
I0512 23:42:16.235946 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_9/mlp/wi_1/kernel/v
I0512 23:42:16.235977 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_9/mlp/wi_1/kernel/v_col
I0512 23:42:16.236007 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_9/mlp/wi_1/kernel/v_row
I0512 23:42:16.236039 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_9/mlp/wo/kernel/m
I0512 23:42:16.257931 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_20/pre_self_attention_layer_norm/scale/v_row
I0512 23:42:16.257961 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_20/self_attention/key/kernel/m
I0512 23:42:16.257990 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_20/self_attention/key/kernel/v
I0512 23:42:16.258024 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_20/self_attention/key/kernel/v_col
I0512 23:42:16.258055 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_20/self_attention/key/kernel/v_row
I0512 23:42:16.258084 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_20/self_attention/out/kernel/m
I0512 23:42:16.258113 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_20/self_attention/out/kernel/v
I0512 23:42:16.258142 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_20/self_attention/out/kernel/v_col
I0512 23:42:16.258172 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_20/self_attention/out/kernel/v_row
I0512 23:42:16.258201 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_20/self_attention/query/kernel/m
I0512 23:42:16.258230 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_20/self_attention/query/kernel/v
I0512 23:42:16.258279 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_20/self_attention/query/kernel/v_col
I0512 23:42:16.258316 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_20/self_attention/query/kernel/v_row
I0512 23:42:16.258346 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_20/self_attention/value/kernel/m
I0512 23:42:16.258375 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_20/self_attention/value/kernel/v
I0512 23:42:16.236070 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_9/mlp/wo/kernel/v
I0512 23:42:16.236101 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_9/mlp/wo/kernel/v_col
I0512 23:42:16.236132 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_9/mlp/wo/kernel/v_row
I0512 23:42:16.236163 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_9/pre_mlp_layer_norm/scale/m
I0512 23:42:16.236194 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_9/pre_mlp_layer_norm/scale/v
I0512 23:42:16.236224 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_9/pre_mlp_layer_norm/scale/v_col
I0512 23:42:16.236255 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_9/pre_mlp_layer_norm/scale/v_row
I0512 23:42:16.236286 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_9/pre_self_attention_layer_norm/scale/m
I0512 23:42:16.236317 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_9/pre_self_attention_layer_norm/scale/v
I0512 23:42:16.236348 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_9/pre_self_attention_layer_norm/scale/v_col
I0512 23:42:16.236379 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_9/pre_self_attention_layer_norm/scale/v_row
I0512 23:42:16.236411 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_9/self_attention/key/kernel/m
I0512 23:42:16.236443 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_9/self_attention/key/kernel/v
I0512 23:42:16.236474 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_9/self_attention/key/kernel/v_col
I0512 23:42:16.236505 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_9/self_attention/key/kernel/v_row
I0512 23:42:16.256065 140440593786880 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_10/self_attention/value/kernel
I0512 23:42:16.256098 140440593786880 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_11/extra_mlp/wi_0/kernel
I0512 23:42:16.256136 140440593786880 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_11/extra_mlp/wi_1/kernel
I0512 23:42:16.256171 140440593786880 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_11/extra_mlp/wo/kernel
I0512 23:42:16.256205 140440593786880 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_11/mlp/expert/wi_0/kernel
I0512 23:42:16.256238 140440593786880 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_11/mlp/expert/wi_1/kernel
I0512 23:42:16.256272 140440593786880 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_11/mlp/expert/wo/kernel
I0512 23:42:16.256306 140440593786880 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_11/mlp/router/router_weights/w/kernel
I0512 23:42:16.256339 140440593786880 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_11/pre_extra_mlp_layer_norm/scale
I0512 23:42:16.256373 140440593786880 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_11/pre_mlp_layer_norm/scale
I0512 23:42:16.256426 140440593786880 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_11/pre_self_attention_layer_norm/scale
I0512 23:42:16.258404 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_20/self_attention/value/kernel/v_col
I0512 23:42:16.258433 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_20/self_attention/value/kernel/v_row
I0512 23:42:16.258462 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_21/mlp/wi_0/kernel/m
I0512 23:42:16.258491 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_21/mlp/wi_0/kernel/v
I0512 23:42:16.258520 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_21/mlp/wi_0/kernel/v_col
I0512 23:42:16.258550 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_21/mlp/wi_0/kernel/v_row
I0512 23:42:16.258579 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_21/mlp/wi_1/kernel/m
I0512 23:42:16.258609 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_21/mlp/wi_1/kernel/v
I0512 23:42:16.258638 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_21/mlp/wi_1/kernel/v_col
I0512 23:42:16.258668 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_21/mlp/wi_1/kernel/v_row
I0512 23:42:16.258697 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_21/mlp/wo/kernel/m
I0512 23:42:16.258726 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_21/mlp/wo/kernel/v
I0512 23:42:16.258756 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_21/mlp/wo/kernel/v_col
I0512 23:42:16.258785 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_21/mlp/wo/kernel/v_row
I0512 23:42:16.258815 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_21/pre_mlp_layer_norm/scale/m
I0512 23:42:16.256464 140440593786880 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_11/self_attention/key/kernel
I0512 23:42:16.256500 140440593786880 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_11/self_attention/out/kernel
I0512 23:42:16.256533 140440593786880 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_11/self_attention/query/kernel
I0512 23:42:16.256567 140440593786880 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_11/self_attention/value/kernel
I0512 23:42:16.256601 140440593786880 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_12/mlp/wi_0/kernel
I0512 23:42:16.256635 140440593786880 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_12/mlp/wi_1/kernel
I0512 23:42:16.256669 140440593786880 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_12/mlp/wo/kernel
I0512 23:42:16.256702 140440593786880 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_12/pre_mlp_layer_norm/scale
I0512 23:42:16.256736 140440593786880 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_12/pre_self_attention_layer_norm/scale
I0512 23:42:16.256770 140440593786880 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_12/self_attention/key/kernel
I0512 23:42:16.256803 140440593786880 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_12/self_attention/out/kernel
I0512 23:42:16.256837 140440593786880 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_12/self_attention/query/kernel
I0512 23:42:16.236536 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_9/self_attention/out/kernel/m
I0512 23:42:16.236567 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_9/self_attention/out/kernel/v
I0512 23:42:16.236598 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_9/self_attention/out/kernel/v_col
I0512 23:42:16.236629 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_9/self_attention/out/kernel/v_row
I0512 23:42:16.236665 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_9/self_attention/query/kernel/m
I0512 23:42:16.236697 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_9/self_attention/query/kernel/v
I0512 23:42:16.236728 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_9/self_attention/query/kernel/v_col
I0512 23:42:16.236759 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_9/self_attention/query/kernel/v_row
I0512 23:42:16.236789 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_9/self_attention/value/kernel/m
I0512 23:42:16.236820 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_9/self_attention/value/kernel/v
I0512 23:42:16.236851 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_9/self_attention/value/kernel/v_col
I0512 23:42:16.236882 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_9/self_attention/value/kernel/v_row
I0512 23:42:16.236913 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/logits_dense/kernel/m
I0512 23:42:16.236944 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/logits_dense/kernel/v
I0512 23:42:16.236975 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/logits_dense/kernel/v_col
I0512 23:42:16.237006 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/logits_dense/kernel/v_row
I0512 23:42:16.237037 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/token_embedder/embedding/m
I0512 23:42:16.237068 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/token_embedder/embedding/v
I0512 23:42:16.237098 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/token_embedder/embedding/v_col
I0512 23:42:16.237129 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/param_states/token_embedder/embedding/v_row
I0512 23:42:16.237160 140311617681408 checkpoints.py:1100] Restoring key from ckpt: state/step
I0512 23:42:16.237191 140311617681408 checkpoints.py:1100] Restoring key from ckpt: target/decoder/decoder_norm/scale
I0512 23:42:16.237223 140311617681408 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_0/mlp/wi_0/kernel
I0512 23:42:16.237254 140311617681408 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_0/mlp/wi_1/kernel
I0512 23:42:16.237285 140311617681408 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_0/mlp/wo/kernel
I0512 23:42:16.258844 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_21/pre_mlp_layer_norm/scale/v
I0512 23:42:16.258873 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_21/pre_mlp_layer_norm/scale/v_col
I0512 23:42:16.258903 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_21/pre_mlp_layer_norm/scale/v_row
I0512 23:42:16.258932 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_21/pre_self_attention_layer_norm/scale/m
I0512 23:42:16.258962 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_21/pre_self_attention_layer_norm/scale/v
I0512 23:42:16.258991 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_21/pre_self_attention_layer_norm/scale/v_col
I0512 23:42:16.259026 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_21/pre_self_attention_layer_norm/scale/v_row
I0512 23:42:16.259057 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_21/self_attention/key/kernel/m
I0512 23:42:16.259087 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_21/self_attention/key/kernel/v
I0512 23:42:16.259116 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_21/self_attention/key/kernel/v_col
I0512 23:42:16.259146 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_21/self_attention/key/kernel/v_row
I0512 23:42:16.259176 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_21/self_attention/out/kernel/m
I0512 23:42:16.259206 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_21/self_attention/out/kernel/v
I0512 23:42:16.259235 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_21/self_attention/out/kernel/v_col
I0512 23:42:16.259265 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_21/self_attention/out/kernel/v_row
I0512 23:42:16.256870 140440593786880 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_12/self_attention/value/kernel
I0512 23:42:16.256907 140440593786880 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_13/mlp/wi_0/kernel
I0512 23:42:16.256941 140440593786880 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_13/mlp/wi_1/kernel
I0512 23:42:16.256975 140440593786880 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_13/mlp/wo/kernel
I0512 23:42:16.257009 140440593786880 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_13/pre_mlp_layer_norm/scale
I0512 23:42:16.257043 140440593786880 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_13/pre_self_attention_layer_norm/scale
I0512 23:42:16.257077 140440593786880 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_13/self_attention/key/kernel
I0512 23:42:16.257110 140440593786880 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_13/self_attention/out/kernel
I0512 23:42:16.257149 140440593786880 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_13/self_attention/query/kernel
I0512 23:42:16.257184 140440593786880 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_13/self_attention/value/kernel
I0512 23:42:16.257218 140440593786880 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_14/mlp/wi_0/kernel
I0512 23:42:16.257251 140440593786880 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_14/mlp/wi_1/kernel
I0512 23:42:16.257285 140440593786880 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_14/mlp/wo/kernel
I0512 23:42:16.237316 140311617681408 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_0/pre_mlp_layer_norm/scale
I0512 23:42:16.237347 140311617681408 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_0/pre_self_attention_layer_norm/scale
I0512 23:42:16.237378 140311617681408 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_0/self_attention/key/kernel
I0512 23:42:16.237410 140311617681408 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_0/self_attention/out/kernel
I0512 23:42:16.237442 140311617681408 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_0/self_attention/query/kernel
I0512 23:42:16.237474 140311617681408 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_0/self_attention/value/kernel
I0512 23:42:16.237504 140311617681408 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_1/mlp/wi_0/kernel
I0512 23:42:16.237536 140311617681408 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_1/mlp/wi_1/kernel
I0512 23:42:16.237566 140311617681408 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_1/mlp/wo/kernel
I0512 23:42:16.237597 140311617681408 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_1/pre_mlp_layer_norm/scale
I0512 23:42:16.259297 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_21/self_attention/query/kernel/m
I0512 23:42:16.259326 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_21/self_attention/query/kernel/v
I0512 23:42:16.259356 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_21/self_attention/query/kernel/v_col
I0512 23:42:16.259385 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_21/self_attention/query/kernel/v_row
I0512 23:42:16.259414 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_21/self_attention/value/kernel/m
I0512 23:42:16.259443 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_21/self_attention/value/kernel/v
I0512 23:42:16.259472 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_21/self_attention/value/kernel/v_col
I0512 23:42:16.259500 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_21/self_attention/value/kernel/v_row
I0512 23:42:16.259530 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_22/mlp/wi_0/kernel/m
I0512 23:42:16.259559 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_22/mlp/wi_0/kernel/v
I0512 23:42:16.259589 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_22/mlp/wi_0/kernel/v_col
I0512 23:42:16.259618 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_22/mlp/wi_0/kernel/v_row
I0512 23:42:16.259648 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_22/mlp/wi_1/kernel/m
I0512 23:42:16.259677 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_22/mlp/wi_1/kernel/v
I0512 23:42:16.257319 140440593786880 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_14/pre_mlp_layer_norm/scale
I0512 23:42:16.257353 140440593786880 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_14/pre_self_attention_layer_norm/scale
I0512 23:42:16.257386 140440593786880 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_14/self_attention/key/kernel
I0512 23:42:16.257420 140440593786880 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_14/self_attention/out/kernel
I0512 23:42:16.257453 140440593786880 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_14/self_attention/query/kernel
I0512 23:42:16.257487 140440593786880 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_14/self_attention/value/kernel
I0512 23:42:16.257520 140440593786880 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_15/mlp/wi_0/kernel
I0512 23:42:16.257554 140440593786880 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_15/mlp/wi_1/kernel
I0512 23:42:16.257588 140440593786880 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_15/mlp/wo/kernel
I0512 23:42:16.257622 140440593786880 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_15/pre_mlp_layer_norm/scale
I0512 23:42:16.257655 140440593786880 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_15/pre_self_attention_layer_norm/scale
I0512 23:42:16.257689 140440593786880 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_15/self_attention/key/kernel
I0512 23:42:16.257723 140440593786880 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_15/self_attention/out/kernel
I0512 23:42:16.237628 140311617681408 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_1/pre_self_attention_layer_norm/scale
I0512 23:42:16.237664 140311617681408 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_1/self_attention/key/kernel
I0512 23:42:16.237696 140311617681408 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_1/self_attention/out/kernel
I0512 23:42:16.237728 140311617681408 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_1/self_attention/query/kernel
I0512 23:42:16.237759 140311617681408 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_1/self_attention/value/kernel
I0512 23:42:16.237813 140311617681408 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_10/mlp/wi_0/kernel
I0512 23:42:16.237846 140311617681408 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_10/mlp/wi_1/kernel
I0512 23:42:16.237877 140311617681408 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_10/mlp/wo/kernel
I0512 23:42:16.237908 140311617681408 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_10/pre_mlp_layer_norm/scale
I0512 23:42:16.237938 140311617681408 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_10/pre_self_attention_layer_norm/scale
I0512 23:42:16.259707 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_22/mlp/wi_1/kernel/v_col
I0512 23:42:16.259736 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_22/mlp/wi_1/kernel/v_row
I0512 23:42:16.259765 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_22/mlp/wo/kernel/m
I0512 23:42:16.259795 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_22/mlp/wo/kernel/v
I0512 23:42:16.259824 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_22/mlp/wo/kernel/v_col
I0512 23:42:16.259854 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_22/mlp/wo/kernel/v_row
I0512 23:42:16.259883 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_22/pre_mlp_layer_norm/scale/m
I0512 23:42:16.259912 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_22/pre_mlp_layer_norm/scale/v
I0512 23:42:16.259942 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_22/pre_mlp_layer_norm/scale/v_col
I0512 23:42:16.259971 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_22/pre_mlp_layer_norm/scale/v_row
I0512 23:42:16.260001 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_22/pre_self_attention_layer_norm/scale/m
I0512 23:42:16.260036 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_22/pre_self_attention_layer_norm/scale/v
I0512 23:42:16.260066 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_22/pre_self_attention_layer_norm/scale/v_col
I0512 23:42:16.260096 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_22/pre_self_attention_layer_norm/scale/v_row
I0512 23:42:16.260125 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_22/self_attention/key/kernel/m
I0512 23:42:16.257756 140440593786880 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_15/self_attention/query/kernel
I0512 23:42:16.257791 140440593786880 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_15/self_attention/value/kernel
I0512 23:42:16.257824 140440593786880 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_16/mlp/wi_0/kernel
I0512 23:42:16.257858 140440593786880 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_16/mlp/wi_1/kernel
I0512 23:42:16.257893 140440593786880 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_16/mlp/wo/kernel
I0512 23:42:16.257928 140440593786880 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_16/pre_mlp_layer_norm/scale
I0512 23:42:16.257961 140440593786880 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_16/pre_self_attention_layer_norm/scale
I0512 23:42:16.257995 140440593786880 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_16/self_attention/key/kernel
I0512 23:42:16.258029 140440593786880 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_16/self_attention/out/kernel
I0512 23:42:16.258062 140440593786880 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_16/self_attention/query/kernel
I0512 23:42:16.258096 140440593786880 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_16/self_attention/value/kernel
I0512 23:42:16.258129 140440593786880 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_17/extra_mlp/wi_0/kernel
I0512 23:42:16.258169 140440593786880 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_17/extra_mlp/wi_1/kernel
I0512 23:42:16.258203 140440593786880 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_17/extra_mlp/wo/kernel
I0512 23:42:16.237969 140311617681408 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_10/self_attention/key/kernel
I0512 23:42:16.238000 140311617681408 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_10/self_attention/out/kernel
I0512 23:42:16.238031 140311617681408 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_10/self_attention/query/kernel
I0512 23:42:16.238062 140311617681408 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_10/self_attention/value/kernel
I0512 23:42:16.238093 140311617681408 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_11/extra_mlp/wi_0/kernel
I0512 23:42:16.238124 140311617681408 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_11/extra_mlp/wi_1/kernel
I0512 23:42:16.238154 140311617681408 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_11/extra_mlp/wo/kernel
I0512 23:42:16.238185 140311617681408 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_11/mlp/expert/wi_0/kernel
I0512 23:42:16.238215 140311617681408 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_11/mlp/expert/wi_1/kernel
I0512 23:42:16.238246 140311617681408 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_11/mlp/expert/wo/kernel
I0512 23:42:16.260155 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_22/self_attention/key/kernel/v
I0512 23:42:16.260184 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_22/self_attention/key/kernel/v_col
I0512 23:42:16.260213 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_22/self_attention/key/kernel/v_row
I0512 23:42:16.260242 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_22/self_attention/out/kernel/m
I0512 23:42:16.260272 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_22/self_attention/out/kernel/v
I0512 23:42:16.260303 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_22/self_attention/out/kernel/v_col
I0512 23:42:16.260333 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_22/self_attention/out/kernel/v_row
I0512 23:42:16.260362 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_22/self_attention/query/kernel/m
I0512 23:42:16.260391 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_22/self_attention/query/kernel/v
I0512 23:42:16.260421 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_22/self_attention/query/kernel/v_col
I0512 23:42:16.260450 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_22/self_attention/query/kernel/v_row
I0512 23:42:16.260479 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_22/self_attention/value/kernel/m
I0512 23:42:16.260508 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_22/self_attention/value/kernel/v
I0512 23:42:16.260537 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_22/self_attention/value/kernel/v_col
I0512 23:42:16.260566 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_22/self_attention/value/kernel/v_row
I0512 23:42:16.258236 140440593786880 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_17/mlp/expert/wi_0/kernel
I0512 23:42:16.258270 140440593786880 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_17/mlp/expert/wi_1/kernel
I0512 23:42:16.258303 140440593786880 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_17/mlp/expert/wo/kernel
I0512 23:42:16.258336 140440593786880 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_17/mlp/router/router_weights/w/kernel
I0512 23:42:16.258369 140440593786880 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_17/pre_extra_mlp_layer_norm/scale
I0512 23:42:16.258403 140440593786880 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_17/pre_mlp_layer_norm/scale
I0512 23:42:16.258436 140440593786880 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_17/pre_self_attention_layer_norm/scale
I0512 23:42:16.258491 140440593786880 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_17/self_attention/key/kernel
I0512 23:42:16.258527 140440593786880 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_17/self_attention/out/kernel
I0512 23:42:16.258562 140440593786880 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_17/self_attention/query/kernel
I0512 23:42:16.258595 140440593786880 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_17/self_attention/value/kernel
I0512 23:42:16.258629 140440593786880 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_18/mlp/wi_0/kernel
I0512 23:42:16.258663 140440593786880 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_18/mlp/wi_1/kernel
I0512 23:42:16.258697 140440593786880 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_18/mlp/wo/kernel
I0512 23:42:16.238276 140311617681408 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_11/mlp/router/router_weights/w/kernel
I0512 23:42:16.238307 140311617681408 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_11/pre_extra_mlp_layer_norm/scale
I0512 23:42:16.238338 140311617681408 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_11/pre_mlp_layer_norm/scale
I0512 23:42:16.238368 140311617681408 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_11/pre_self_attention_layer_norm/scale
I0512 23:42:16.238399 140311617681408 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_11/self_attention/key/kernel
I0512 23:42:16.238432 140311617681408 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_11/self_attention/out/kernel
I0512 23:42:16.238465 140311617681408 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_11/self_attention/query/kernel
I0512 23:42:16.238495 140311617681408 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_11/self_attention/value/kernel
I0512 23:42:16.238526 140311617681408 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_12/mlp/wi_0/kernel
I0512 23:42:16.238557 140311617681408 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_12/mlp/wi_1/kernel
I0512 23:42:16.260596 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/extra_mlp/wi_0/kernel/m
I0512 23:42:16.260625 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/extra_mlp/wi_0/kernel/v
I0512 23:42:16.260654 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/extra_mlp/wi_0/kernel/v_col
I0512 23:42:16.260684 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/extra_mlp/wi_0/kernel/v_row
I0512 23:42:16.260713 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/extra_mlp/wi_1/kernel/m
I0512 23:42:16.260742 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/extra_mlp/wi_1/kernel/v
I0512 23:42:16.260771 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/extra_mlp/wi_1/kernel/v_col
I0512 23:42:16.260801 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/extra_mlp/wi_1/kernel/v_row
I0512 23:42:16.260830 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/extra_mlp/wo/kernel/m
I0512 23:42:16.260859 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/extra_mlp/wo/kernel/v
I0512 23:42:16.260889 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/extra_mlp/wo/kernel/v_col
I0512 23:42:16.260918 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/extra_mlp/wo/kernel/v_row
I0512 23:42:16.260947 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/mlp/expert/wi_0/kernel/m
I0512 23:42:16.260977 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/mlp/expert/wi_0/kernel/v
I0512 23:42:16.261006 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/mlp/expert/wi_0/kernel/v_col
I0512 23:42:16.258731 140440593786880 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_18/pre_mlp_layer_norm/scale
I0512 23:42:16.258765 140440593786880 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_18/pre_self_attention_layer_norm/scale
I0512 23:42:16.258799 140440593786880 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_18/self_attention/key/kernel
I0512 23:42:16.258832 140440593786880 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_18/self_attention/out/kernel
I0512 23:42:16.258866 140440593786880 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_18/self_attention/query/kernel
I0512 23:42:16.258901 140440593786880 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_18/self_attention/value/kernel
I0512 23:42:16.258935 140440593786880 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_19/mlp/wi_0/kernel
I0512 23:42:16.258969 140440593786880 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_19/mlp/wi_1/kernel
I0512 23:42:16.259003 140440593786880 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_19/mlp/wo/kernel
I0512 23:42:16.259036 140440593786880 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_19/pre_mlp_layer_norm/scale
I0512 23:42:16.259070 140440593786880 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_19/pre_self_attention_layer_norm/scale
I0512 23:42:16.259104 140440593786880 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_19/self_attention/key/kernel
I0512 23:42:16.259143 140440593786880 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_19/self_attention/out/kernel
I0512 23:42:16.259178 140440593786880 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_19/self_attention/query/kernel
I0512 23:42:16.238587 140311617681408 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_12/mlp/wo/kernel
I0512 23:42:16.238618 140311617681408 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_12/pre_mlp_layer_norm/scale
I0512 23:42:16.238648 140311617681408 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_12/pre_self_attention_layer_norm/scale
I0512 23:42:16.238686 140311617681408 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_12/self_attention/key/kernel
I0512 23:42:16.238718 140311617681408 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_12/self_attention/out/kernel
I0512 23:42:16.238749 140311617681408 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_12/self_attention/query/kernel
I0512 23:42:16.238780 140311617681408 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_12/self_attention/value/kernel
I0512 23:42:16.238811 140311617681408 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_13/mlp/wi_0/kernel
I0512 23:42:16.238842 140311617681408 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_13/mlp/wi_1/kernel
I0512 23:42:16.238872 140311617681408 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_13/mlp/wo/kernel
I0512 23:42:16.238903 140311617681408 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_13/pre_mlp_layer_norm/scale
I0512 23:42:16.261043 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/mlp/expert/wi_0/kernel/v_row
I0512 23:42:16.261073 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/mlp/expert/wi_1/kernel/m
I0512 23:42:16.261102 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/mlp/expert/wi_1/kernel/v
I0512 23:42:16.261132 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/mlp/expert/wi_1/kernel/v_col
I0512 23:42:16.261161 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/mlp/expert/wi_1/kernel/v_row
I0512 23:42:16.261190 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/mlp/expert/wo/kernel/m
I0512 23:42:16.261220 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/mlp/expert/wo/kernel/v
I0512 23:42:16.261250 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/mlp/expert/wo/kernel/v_col
I0512 23:42:16.261281 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/mlp/expert/wo/kernel/v_row
I0512 23:42:16.261311 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/mlp/router/router_weights/w/kernel/m
I0512 23:42:16.261341 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/mlp/router/router_weights/w/kernel/v
I0512 23:42:16.238935 140311617681408 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_13/pre_self_attention_layer_norm/scale
I0512 23:42:16.238965 140311617681408 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_13/self_attention/key/kernel
I0512 23:42:16.238996 140311617681408 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_13/self_attention/out/kernel
I0512 23:42:16.239027 140311617681408 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_13/self_attention/query/kernel
I0512 23:42:16.239058 140311617681408 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_13/self_attention/value/kernel
I0512 23:42:16.239088 140311617681408 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_14/mlp/wi_0/kernel
I0512 23:42:16.239119 140311617681408 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_14/mlp/wi_1/kernel
I0512 23:42:16.239150 140311617681408 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_14/mlp/wo/kernel
I0512 23:42:16.239180 140311617681408 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_14/pre_mlp_layer_norm/scale
I0512 23:42:16.239211 140311617681408 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_14/pre_self_attention_layer_norm/scale
I0512 23:42:16.239242 140311617681408 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_14/self_attention/key/kernel
I0512 23:42:16.261370 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/mlp/router/router_weights/w/kernel/v_col
I0512 23:42:16.261399 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/mlp/router/router_weights/w/kernel/v_row
I0512 23:42:16.261429 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/pre_extra_mlp_layer_norm/scale/m
I0512 23:42:16.261458 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/pre_extra_mlp_layer_norm/scale/v
I0512 23:42:16.261487 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/pre_extra_mlp_layer_norm/scale/v_col
I0512 23:42:16.261517 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/pre_extra_mlp_layer_norm/scale/v_row
I0512 23:42:16.261546 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/pre_mlp_layer_norm/scale/m
I0512 23:42:16.261575 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/pre_mlp_layer_norm/scale/v
I0512 23:42:16.261605 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/pre_mlp_layer_norm/scale/v_col
I0512 23:42:16.261634 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/pre_mlp_layer_norm/scale/v_row
I0512 23:42:16.261663 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/pre_self_attention_layer_norm/scale/m
I0512 23:42:16.259212 140440593786880 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_19/self_attention/value/kernel
I0512 23:42:16.259245 140440593786880 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_2/mlp/wi_0/kernel
I0512 23:42:16.259279 140440593786880 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_2/mlp/wi_1/kernel
I0512 23:42:16.259313 140440593786880 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_2/mlp/wo/kernel
I0512 23:42:16.259346 140440593786880 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_2/pre_mlp_layer_norm/scale
I0512 23:42:16.259380 140440593786880 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_2/pre_self_attention_layer_norm/scale
I0512 23:42:16.259414 140440593786880 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_2/self_attention/key/kernel
I0512 23:42:16.259448 140440593786880 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_2/self_attention/out/kernel
I0512 23:42:16.259482 140440593786880 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_2/self_attention/query/kernel
I0512 23:42:16.259515 140440593786880 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_2/self_attention/value/kernel
I0512 23:42:16.259549 140440593786880 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_20/mlp/wi_0/kernel
I0512 23:42:16.259582 140440593786880 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_20/mlp/wi_1/kernel
I0512 23:42:16.259615 140440593786880 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_20/mlp/wo/kernel
I0512 23:42:16.259649 140440593786880 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_20/pre_mlp_layer_norm/scale
I0512 23:42:16.259682 140440593786880 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_20/pre_self_attention_layer_norm/scale
I0512 23:42:16.239273 140311617681408 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_14/self_attention/out/kernel
I0512 23:42:16.239304 140311617681408 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_14/self_attention/query/kernel
I0512 23:42:16.239335 140311617681408 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_14/self_attention/value/kernel
I0512 23:42:16.239366 140311617681408 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_15/mlp/wi_0/kernel
I0512 23:42:16.239397 140311617681408 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_15/mlp/wi_1/kernel
I0512 23:42:16.239430 140311617681408 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_15/mlp/wo/kernel
I0512 23:42:16.239462 140311617681408 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_15/pre_mlp_layer_norm/scale
I0512 23:42:16.239493 140311617681408 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_15/pre_self_attention_layer_norm/scale
I0512 23:42:16.239524 140311617681408 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_15/self_attention/key/kernel
I0512 23:42:16.239555 140311617681408 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_15/self_attention/out/kernel
I0512 23:42:16.239586 140311617681408 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_15/self_attention/query/kernel
I0512 23:42:16.239616 140311617681408 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_15/self_attention/value/kernel
I0512 23:42:16.261693 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/pre_self_attention_layer_norm/scale/v
I0512 23:42:16.261722 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/pre_self_attention_layer_norm/scale/v_col
I0512 23:42:16.261751 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/pre_self_attention_layer_norm/scale/v_row
I0512 23:42:16.261781 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/self_attention/key/kernel/m
I0512 23:42:16.261810 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/self_attention/key/kernel/v
I0512 23:42:16.261839 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/self_attention/key/kernel/v_col
I0512 23:42:16.261869 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/self_attention/key/kernel/v_row
I0512 23:42:16.261898 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/self_attention/out/kernel/m
I0512 23:42:16.261928 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/self_attention/out/kernel/v
I0512 23:42:16.261957 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/self_attention/out/kernel/v_col
I0512 23:42:16.261986 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/self_attention/out/kernel/v_row
I0512 23:42:16.262020 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/self_attention/query/kernel/m
I0512 23:42:16.239647 140311617681408 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_16/mlp/wi_0/kernel
I0512 23:42:16.239684 140311617681408 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_16/mlp/wi_1/kernel
I0512 23:42:16.239715 140311617681408 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_16/mlp/wo/kernel
I0512 23:42:16.239747 140311617681408 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_16/pre_mlp_layer_norm/scale
I0512 23:42:16.239778 140311617681408 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_16/pre_self_attention_layer_norm/scale
I0512 23:42:16.239809 140311617681408 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_16/self_attention/key/kernel
I0512 23:42:16.239840 140311617681408 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_16/self_attention/out/kernel
I0512 23:42:16.239870 140311617681408 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_16/self_attention/query/kernel
I0512 23:42:16.239901 140311617681408 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_16/self_attention/value/kernel
I0512 23:42:16.239932 140311617681408 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_17/extra_mlp/wi_0/kernel
I0512 23:42:16.239963 140311617681408 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_17/extra_mlp/wi_1/kernel
I0512 23:42:16.239993 140311617681408 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_17/extra_mlp/wo/kernel
I0512 23:42:16.240024 140311617681408 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_17/mlp/expert/wi_0/kernel
I0512 23:42:16.259716 140440593786880 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_20/self_attention/key/kernel
I0512 23:42:16.259750 140440593786880 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_20/self_attention/out/kernel
I0512 23:42:16.259784 140440593786880 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_20/self_attention/query/kernel
I0512 23:42:16.259817 140440593786880 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_20/self_attention/value/kernel
I0512 23:42:16.259850 140440593786880 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_21/mlp/wi_0/kernel
I0512 23:42:16.259885 140440593786880 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_21/mlp/wi_1/kernel
I0512 23:42:16.259920 140440593786880 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_21/mlp/wo/kernel
I0512 23:42:16.259954 140440593786880 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_21/pre_mlp_layer_norm/scale
I0512 23:42:16.259988 140440593786880 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_21/pre_self_attention_layer_norm/scale
I0512 23:42:16.260022 140440593786880 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_21/self_attention/key/kernel
I0512 23:42:16.260056 140440593786880 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_21/self_attention/out/kernel
I0512 23:42:16.260089 140440593786880 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_21/self_attention/query/kernel
I0512 23:42:16.260123 140440593786880 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_21/self_attention/value/kernel
I0512 23:42:16.260162 140440593786880 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_22/mlp/wi_0/kernel
I0512 23:42:16.260196 140440593786880 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_22/mlp/wi_1/kernel
I0512 23:42:16.262052 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/self_attention/query/kernel/v
I0512 23:42:16.262081 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/self_attention/query/kernel/v_col
I0512 23:42:16.262110 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/self_attention/query/kernel/v_row
I0512 23:42:16.262139 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/self_attention/value/kernel/m
I0512 23:42:16.262168 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/self_attention/value/kernel/v
I0512 23:42:16.262197 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/self_attention/value/kernel/v_col
I0512 23:42:16.262226 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/self_attention/value/kernel/v_row
I0512 23:42:16.262275 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_3/mlp/wi_0/kernel/m
I0512 23:42:16.262312 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_3/mlp/wi_0/kernel/v
I0512 23:42:16.262343 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_3/mlp/wi_0/kernel/v_col
I0512 23:42:16.262373 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_3/mlp/wi_0/kernel/v_row
I0512 23:42:16.262403 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_3/mlp/wi_1/kernel/m
I0512 23:42:16.262433 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_3/mlp/wi_1/kernel/v
I0512 23:42:16.240054 140311617681408 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_17/mlp/expert/wi_1/kernel
I0512 23:42:16.240084 140311617681408 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_17/mlp/expert/wo/kernel
I0512 23:42:16.240115 140311617681408 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_17/mlp/router/router_weights/w/kernel
I0512 23:42:16.240146 140311617681408 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_17/pre_extra_mlp_layer_norm/scale
I0512 23:42:16.240177 140311617681408 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_17/pre_mlp_layer_norm/scale
I0512 23:42:16.240207 140311617681408 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_17/pre_self_attention_layer_norm/scale
I0512 23:42:16.240238 140311617681408 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_17/self_attention/key/kernel
I0512 23:42:16.240269 140311617681408 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_17/self_attention/out/kernel
I0512 23:42:16.240300 140311617681408 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_17/self_attention/query/kernel
I0512 23:42:16.240331 140311617681408 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_17/self_attention/value/kernel
I0512 23:42:16.240361 140311617681408 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_18/mlp/wi_0/kernel
I0512 23:42:16.240392 140311617681408 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_18/mlp/wi_1/kernel
I0512 23:42:16.240425 140311617681408 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_18/mlp/wo/kernel
I0512 23:42:16.260229 140440593786880 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_22/mlp/wo/kernel
I0512 23:42:16.260263 140440593786880 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_22/pre_mlp_layer_norm/scale
I0512 23:42:16.260297 140440593786880 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_22/pre_self_attention_layer_norm/scale
I0512 23:42:16.260330 140440593786880 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_22/self_attention/key/kernel
I0512 23:42:16.260364 140440593786880 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_22/self_attention/out/kernel
I0512 23:42:16.260397 140440593786880 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_22/self_attention/query/kernel
I0512 23:42:16.260456 140440593786880 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_22/self_attention/value/kernel
I0512 23:42:16.260491 140440593786880 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_23/extra_mlp/wi_0/kernel
I0512 23:42:16.260525 140440593786880 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_23/extra_mlp/wi_1/kernel
I0512 23:42:16.260559 140440593786880 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_23/extra_mlp/wo/kernel
I0512 23:42:16.260592 140440593786880 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_23/mlp/expert/wi_0/kernel
I0512 23:42:16.260626 140440593786880 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_23/mlp/expert/wi_1/kernel
I0512 23:42:16.260659 140440593786880 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_23/mlp/expert/wo/kernel
I0512 23:42:16.260693 140440593786880 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_23/mlp/router/router_weights/w/kernel
I0512 23:42:16.260726 140440593786880 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_23/pre_extra_mlp_layer_norm/scale
I0512 23:42:16.240457 140311617681408 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_18/pre_mlp_layer_norm/scale
I0512 23:42:16.240488 140311617681408 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_18/pre_self_attention_layer_norm/scale
I0512 23:42:16.240518 140311617681408 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_18/self_attention/key/kernel
I0512 23:42:16.240549 140311617681408 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_18/self_attention/out/kernel
I0512 23:42:16.240580 140311617681408 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_18/self_attention/query/kernel
I0512 23:42:16.240611 140311617681408 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_18/self_attention/value/kernel
I0512 23:42:16.240641 140311617681408 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_19/mlp/wi_0/kernel
I0512 23:42:16.240678 140311617681408 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_19/mlp/wi_1/kernel
I0512 23:42:16.240709 140311617681408 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_19/mlp/wo/kernel
I0512 23:42:16.240740 140311617681408 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_19/pre_mlp_layer_norm/scale
I0512 23:42:16.240771 140311617681408 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_19/pre_self_attention_layer_norm/scale
I0512 23:42:16.240802 140311617681408 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_19/self_attention/key/kernel
I0512 23:42:16.240832 140311617681408 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_19/self_attention/out/kernel
I0512 23:42:16.262462 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_3/mlp/wi_1/kernel/v_col
I0512 23:42:16.262492 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_3/mlp/wi_1/kernel/v_row
I0512 23:42:16.262521 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_3/mlp/wo/kernel/m
I0512 23:42:16.262551 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_3/mlp/wo/kernel/v
I0512 23:42:16.262581 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_3/mlp/wo/kernel/v_col
I0512 23:42:16.262610 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_3/mlp/wo/kernel/v_row
I0512 23:42:16.262640 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_3/pre_mlp_layer_norm/scale/m
I0512 23:42:16.262669 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_3/pre_mlp_layer_norm/scale/v
I0512 23:42:16.262699 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_3/pre_mlp_layer_norm/scale/v_col
I0512 23:42:16.262728 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_3/pre_mlp_layer_norm/scale/v_row
I0512 23:42:16.262758 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_3/pre_self_attention_layer_norm/scale/m
I0512 23:42:16.262787 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_3/pre_self_attention_layer_norm/scale/v
I0512 23:42:16.262816 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_3/pre_self_attention_layer_norm/scale/v_col
I0512 23:42:16.262845 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_3/pre_self_attention_layer_norm/scale/v_row
I0512 23:42:16.262875 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_3/self_attention/key/kernel/m
I0512 23:42:16.262904 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_3/self_attention/key/kernel/v
I0512 23:42:16.262934 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_3/self_attention/key/kernel/v_col
I0512 23:42:16.262963 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_3/self_attention/key/kernel/v_row
I0512 23:42:16.262993 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_3/self_attention/out/kernel/m
I0512 23:42:16.263028 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_3/self_attention/out/kernel/v
I0512 23:42:16.263058 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_3/self_attention/out/kernel/v_col
I0512 23:42:16.263088 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_3/self_attention/out/kernel/v_row
I0512 23:42:16.263117 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_3/self_attention/query/kernel/m
I0512 23:42:16.263146 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_3/self_attention/query/kernel/v
I0512 23:42:16.263175 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_3/self_attention/query/kernel/v_col
I0512 23:42:16.263205 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_3/self_attention/query/kernel/v_row
I0512 23:42:16.263234 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_3/self_attention/value/kernel/m
I0512 23:42:16.263262 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_3/self_attention/value/kernel/v
I0512 23:42:16.260760 140440593786880 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_23/pre_mlp_layer_norm/scale
I0512 23:42:16.260794 140440593786880 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_23/pre_self_attention_layer_norm/scale
I0512 23:42:16.260828 140440593786880 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_23/self_attention/key/kernel
I0512 23:42:16.260861 140440593786880 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_23/self_attention/out/kernel
I0512 23:42:16.260896 140440593786880 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_23/self_attention/query/kernel
I0512 23:42:16.260931 140440593786880 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_23/self_attention/value/kernel
I0512 23:42:16.260965 140440593786880 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_3/mlp/wi_0/kernel
I0512 23:42:16.260998 140440593786880 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_3/mlp/wi_1/kernel
I0512 23:42:16.240863 140311617681408 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_19/self_attention/query/kernel
I0512 23:42:16.240894 140311617681408 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_19/self_attention/value/kernel
I0512 23:42:16.240924 140311617681408 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_2/mlp/wi_0/kernel
I0512 23:42:16.240955 140311617681408 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_2/mlp/wi_1/kernel
I0512 23:42:16.240986 140311617681408 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_2/mlp/wo/kernel
I0512 23:42:16.241017 140311617681408 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_2/pre_mlp_layer_norm/scale
I0512 23:42:16.241048 140311617681408 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_2/pre_self_attention_layer_norm/scale
I0512 23:42:16.241078 140311617681408 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_2/self_attention/key/kernel
I0512 23:42:16.241109 140311617681408 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_2/self_attention/out/kernel
I0512 23:42:16.241139 140311617681408 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_2/self_attention/query/kernel
I0512 23:42:16.241170 140311617681408 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_2/self_attention/value/kernel
I0512 23:42:16.241201 140311617681408 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_20/mlp/wi_0/kernel
I0512 23:42:16.241231 140311617681408 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_20/mlp/wi_1/kernel
I0512 23:42:16.241262 140311617681408 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_20/mlp/wo/kernel
I0512 23:42:16.261032 140440593786880 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_3/mlp/wo/kernel
I0512 23:42:16.261065 140440593786880 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_3/pre_mlp_layer_norm/scale
I0512 23:42:16.261099 140440593786880 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_3/pre_self_attention_layer_norm/scale
I0512 23:42:16.261139 140440593786880 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_3/self_attention/key/kernel
I0512 23:42:16.261175 140440593786880 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_3/self_attention/out/kernel
I0512 23:42:16.261209 140440593786880 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_3/self_attention/query/kernel
I0512 23:42:16.261242 140440593786880 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_3/self_attention/value/kernel
I0512 23:42:16.261276 140440593786880 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_4/mlp/wi_0/kernel
I0512 23:42:16.263294 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_3/self_attention/value/kernel/v_col
I0512 23:42:16.263324 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_3/self_attention/value/kernel/v_row
I0512 23:42:16.263353 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_4/mlp/wi_0/kernel/m
I0512 23:42:16.263383 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_4/mlp/wi_0/kernel/v
I0512 23:42:16.263412 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_4/mlp/wi_0/kernel/v_col
I0512 23:42:16.263441 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_4/mlp/wi_0/kernel/v_row
I0512 23:42:16.263471 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_4/mlp/wi_1/kernel/m
I0512 23:42:16.263500 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_4/mlp/wi_1/kernel/v
I0512 23:42:16.263530 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_4/mlp/wi_1/kernel/v_col
I0512 23:42:16.263559 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_4/mlp/wi_1/kernel/v_row
I0512 23:42:16.263589 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_4/mlp/wo/kernel/m
I0512 23:42:16.263618 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_4/mlp/wo/kernel/v
I0512 23:42:16.263648 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_4/mlp/wo/kernel/v_col
I0512 23:42:16.263677 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_4/mlp/wo/kernel/v_row
I0512 23:42:16.263707 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_4/pre_mlp_layer_norm/scale/m
I0512 23:42:16.241293 140311617681408 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_20/pre_mlp_layer_norm/scale
I0512 23:42:16.241324 140311617681408 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_20/pre_self_attention_layer_norm/scale
I0512 23:42:16.241355 140311617681408 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_20/self_attention/key/kernel
I0512 23:42:16.241386 140311617681408 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_20/self_attention/out/kernel
I0512 23:42:16.241418 140311617681408 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_20/self_attention/query/kernel
I0512 23:42:16.241450 140311617681408 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_20/self_attention/value/kernel
I0512 23:42:16.241481 140311617681408 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_21/mlp/wi_0/kernel
I0512 23:42:16.241512 140311617681408 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_21/mlp/wi_1/kernel
I0512 23:42:16.241543 140311617681408 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_21/mlp/wo/kernel
I0512 23:42:16.241573 140311617681408 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_21/pre_mlp_layer_norm/scale
I0512 23:42:16.241603 140311617681408 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_21/pre_self_attention_layer_norm/scale
I0512 23:42:16.241634 140311617681408 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_21/self_attention/key/kernel
I0512 23:42:16.241670 140311617681408 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_21/self_attention/out/kernel
I0512 23:42:16.241702 140311617681408 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_21/self_attention/query/kernel
I0512 23:42:16.261310 140440593786880 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_4/mlp/wi_1/kernel
I0512 23:42:16.261344 140440593786880 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_4/mlp/wo/kernel
I0512 23:42:16.261378 140440593786880 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_4/pre_mlp_layer_norm/scale
I0512 23:42:16.261411 140440593786880 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_4/pre_self_attention_layer_norm/scale
I0512 23:42:16.261445 140440593786880 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_4/self_attention/key/kernel
I0512 23:42:16.261479 140440593786880 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_4/self_attention/out/kernel
I0512 23:42:16.261512 140440593786880 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_4/self_attention/query/kernel
I0512 23:42:16.261546 140440593786880 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_4/self_attention/value/kernel
I0512 23:42:16.261580 140440593786880 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_5/extra_mlp/wi_0/kernel
I0512 23:42:16.261614 140440593786880 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_5/extra_mlp/wi_1/kernel
I0512 23:42:16.261648 140440593786880 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_5/extra_mlp/wo/kernel
I0512 23:42:16.261682 140440593786880 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_5/mlp/expert/wi_0/kernel
I0512 23:42:16.261715 140440593786880 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_5/mlp/expert/wi_1/kernel
I0512 23:42:16.261749 140440593786880 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_5/mlp/expert/wo/kernel
I0512 23:42:16.261782 140440593786880 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_5/mlp/router/router_weights/w/kernel
I0512 23:42:16.261816 140440593786880 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_5/pre_extra_mlp_layer_norm/scale
I0512 23:42:16.263736 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_4/pre_mlp_layer_norm/scale/v
I0512 23:42:16.263766 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_4/pre_mlp_layer_norm/scale/v_col
I0512 23:42:16.263795 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_4/pre_mlp_layer_norm/scale/v_row
I0512 23:42:16.263824 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_4/pre_self_attention_layer_norm/scale/m
I0512 23:42:16.263853 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_4/pre_self_attention_layer_norm/scale/v
I0512 23:42:16.263882 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_4/pre_self_attention_layer_norm/scale/v_col
I0512 23:42:16.263911 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_4/pre_self_attention_layer_norm/scale/v_row
I0512 23:42:16.263940 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_4/self_attention/key/kernel/m
I0512 23:42:16.263969 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_4/self_attention/key/kernel/v
I0512 23:42:16.263999 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_4/self_attention/key/kernel/v_col
I0512 23:42:16.264034 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_4/self_attention/key/kernel/v_row
I0512 23:42:16.264064 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_4/self_attention/out/kernel/m
I0512 23:42:16.264093 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_4/self_attention/out/kernel/v
I0512 23:42:16.264123 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_4/self_attention/out/kernel/v_col
I0512 23:42:16.241733 140311617681408 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_21/self_attention/value/kernel
I0512 23:42:16.241781 140311617681408 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_22/mlp/wi_0/kernel
I0512 23:42:16.241819 140311617681408 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_22/mlp/wi_1/kernel
I0512 23:42:16.241852 140311617681408 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_22/mlp/wo/kernel
I0512 23:42:16.241883 140311617681408 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_22/pre_mlp_layer_norm/scale
I0512 23:42:16.241914 140311617681408 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_22/pre_self_attention_layer_norm/scale
I0512 23:42:16.241944 140311617681408 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_22/self_attention/key/kernel
I0512 23:42:16.241975 140311617681408 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_22/self_attention/out/kernel
I0512 23:42:16.242006 140311617681408 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_22/self_attention/query/kernel
I0512 23:42:16.242037 140311617681408 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_22/self_attention/value/kernel
I0512 23:42:16.242068 140311617681408 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_23/extra_mlp/wi_0/kernel
I0512 23:42:16.242098 140311617681408 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_23/extra_mlp/wi_1/kernel
I0512 23:42:16.242129 140311617681408 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_23/extra_mlp/wo/kernel
I0512 23:42:16.242160 140311617681408 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_23/mlp/expert/wi_0/kernel
I0512 23:42:16.242190 140311617681408 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_23/mlp/expert/wi_1/kernel
I0512 23:42:16.261849 140440593786880 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_5/pre_mlp_layer_norm/scale
I0512 23:42:16.261883 140440593786880 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_5/pre_self_attention_layer_norm/scale
I0512 23:42:16.261919 140440593786880 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_5/self_attention/key/kernel
I0512 23:42:16.261954 140440593786880 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_5/self_attention/out/kernel
I0512 23:42:16.261987 140440593786880 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_5/self_attention/query/kernel
I0512 23:42:16.262021 140440593786880 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_5/self_attention/value/kernel
I0512 23:42:16.262055 140440593786880 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_6/mlp/wi_0/kernel
I0512 23:42:16.262088 140440593786880 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_6/mlp/wi_1/kernel
I0512 23:42:16.262122 140440593786880 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_6/mlp/wo/kernel
I0512 23:42:16.262162 140440593786880 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_6/pre_mlp_layer_norm/scale
I0512 23:42:16.262196 140440593786880 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_6/pre_self_attention_layer_norm/scale
I0512 23:42:16.262230 140440593786880 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_6/self_attention/key/kernel
I0512 23:42:16.262264 140440593786880 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_6/self_attention/out/kernel
I0512 23:42:16.262297 140440593786880 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_6/self_attention/query/kernel
I0512 23:42:16.262331 140440593786880 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_6/self_attention/value/kernel
I0512 23:42:16.262364 140440593786880 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_7/mlp/wi_0/kernel
I0512 23:42:16.262398 140440593786880 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_7/mlp/wi_1/kernel
I0512 23:42:16.262432 140440593786880 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_7/mlp/wo/kernel
I0512 23:42:16.262465 140440593786880 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_7/pre_mlp_layer_norm/scale
I0512 23:42:16.262499 140440593786880 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_7/pre_self_attention_layer_norm/scale
I0512 23:42:16.264152 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_4/self_attention/out/kernel/v_row
I0512 23:42:16.264181 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_4/self_attention/query/kernel/m
I0512 23:42:16.264211 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_4/self_attention/query/kernel/v
I0512 23:42:16.264240 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_4/self_attention/query/kernel/v_col
I0512 23:42:16.264270 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_4/self_attention/query/kernel/v_row
I0512 23:42:16.264301 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_4/self_attention/value/kernel/m
I0512 23:42:16.264330 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_4/self_attention/value/kernel/v
I0512 23:42:16.264359 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_4/self_attention/value/kernel/v_col
I0512 23:42:16.264389 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_4/self_attention/value/kernel/v_row
I0512 23:42:16.264418 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/extra_mlp/wi_0/kernel/m
I0512 23:42:16.264447 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/extra_mlp/wi_0/kernel/v
I0512 23:42:16.264477 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/extra_mlp/wi_0/kernel/v_col
I0512 23:42:16.264507 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/extra_mlp/wi_0/kernel/v_row
I0512 23:42:16.264536 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/extra_mlp/wi_1/kernel/m
I0512 23:42:16.264566 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/extra_mlp/wi_1/kernel/v
I0512 23:42:16.262533 140440593786880 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_7/self_attention/key/kernel
I0512 23:42:16.262567 140440593786880 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_7/self_attention/out/kernel
I0512 23:42:16.262600 140440593786880 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_7/self_attention/query/kernel
I0512 23:42:16.242221 140311617681408 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_23/mlp/expert/wo/kernel
I0512 23:42:16.242252 140311617681408 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_23/mlp/router/router_weights/w/kernel
I0512 23:42:16.242282 140311617681408 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_23/pre_extra_mlp_layer_norm/scale
I0512 23:42:16.242313 140311617681408 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_23/pre_mlp_layer_norm/scale
I0512 23:42:16.242344 140311617681408 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_23/pre_self_attention_layer_norm/scale
I0512 23:42:16.242375 140311617681408 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_23/self_attention/key/kernel
I0512 23:42:16.242407 140311617681408 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_23/self_attention/out/kernel
I0512 23:42:16.262634 140440593786880 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_7/self_attention/value/kernel
I0512 23:42:16.262667 140440593786880 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_8/mlp/wi_0/kernel
I0512 23:42:16.262701 140440593786880 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_8/mlp/wi_1/kernel
I0512 23:42:16.262735 140440593786880 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_8/mlp/wo/kernel
I0512 23:42:16.242440 140311617681408 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_23/self_attention/query/kernel
I0512 23:42:16.242472 140311617681408 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_23/self_attention/value/kernel
I0512 23:42:16.242503 140311617681408 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_3/mlp/wi_0/kernel
I0512 23:42:16.242533 140311617681408 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_3/mlp/wi_1/kernel
I0512 23:42:16.242564 140311617681408 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_3/mlp/wo/kernel
I0512 23:42:16.242595 140311617681408 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_3/pre_mlp_layer_norm/scale
I0512 23:42:16.242626 140311617681408 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_3/pre_self_attention_layer_norm/scale
I0512 23:42:16.242663 140311617681408 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_3/self_attention/key/kernel
I0512 23:42:16.262769 140440593786880 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_8/pre_mlp_layer_norm/scale
I0512 23:42:16.262802 140440593786880 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_8/pre_self_attention_layer_norm/scale
I0512 23:42:16.262836 140440593786880 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_8/self_attention/key/kernel
I0512 23:42:16.262869 140440593786880 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_8/self_attention/out/kernel
I0512 23:42:16.264595 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/extra_mlp/wi_1/kernel/v_col
I0512 23:42:16.264625 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/extra_mlp/wi_1/kernel/v_row
I0512 23:42:16.264654 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/extra_mlp/wo/kernel/m
I0512 23:42:16.264684 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/extra_mlp/wo/kernel/v
I0512 23:42:16.264714 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/extra_mlp/wo/kernel/v_col
I0512 23:42:16.264743 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/extra_mlp/wo/kernel/v_row
I0512 23:42:16.264772 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/mlp/expert/wi_0/kernel/m
I0512 23:42:16.262905 140440593786880 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_8/self_attention/query/kernel
I0512 23:42:16.262939 140440593786880 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_8/self_attention/value/kernel
I0512 23:42:16.262973 140440593786880 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_9/mlp/wi_0/kernel
I0512 23:42:16.263007 140440593786880 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_9/mlp/wi_1/kernel
I0512 23:42:16.263041 140440593786880 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_9/mlp/wo/kernel
I0512 23:42:16.264804 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/mlp/expert/wi_0/kernel/v
I0512 23:42:16.264836 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/mlp/expert/wi_0/kernel/v_col
I0512 23:42:16.264864 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/mlp/expert/wi_0/kernel/v_row
I0512 23:42:16.264894 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/mlp/expert/wi_1/kernel/m
I0512 23:42:16.264923 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/mlp/expert/wi_1/kernel/v
I0512 23:42:16.264952 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/mlp/expert/wi_1/kernel/v_col
I0512 23:42:16.264981 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/mlp/expert/wi_1/kernel/v_row
I0512 23:42:16.265010 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/mlp/expert/wo/kernel/m
I0512 23:42:16.263075 140440593786880 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_9/pre_mlp_layer_norm/scale
I0512 23:42:16.263109 140440593786880 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_9/pre_self_attention_layer_norm/scale
I0512 23:42:16.263149 140440593786880 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_9/self_attention/key/kernel
I0512 23:42:16.263185 140440593786880 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_9/self_attention/out/kernel
I0512 23:42:16.263219 140440593786880 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_9/self_attention/query/kernel
I0512 23:42:16.242695 140311617681408 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_3/self_attention/out/kernel
I0512 23:42:16.242726 140311617681408 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_3/self_attention/query/kernel
I0512 23:42:16.242757 140311617681408 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_3/self_attention/value/kernel
I0512 23:42:16.242788 140311617681408 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_4/mlp/wi_0/kernel
I0512 23:42:16.242819 140311617681408 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_4/mlp/wi_1/kernel
I0512 23:42:16.242850 140311617681408 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_4/mlp/wo/kernel
I0512 23:42:16.242881 140311617681408 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_4/pre_mlp_layer_norm/scale
I0512 23:42:16.242912 140311617681408 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_4/pre_self_attention_layer_norm/scale
I0512 23:42:16.242943 140311617681408 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_4/self_attention/key/kernel
I0512 23:42:16.242974 140311617681408 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_4/self_attention/out/kernel
I0512 23:42:16.243005 140311617681408 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_4/self_attention/query/kernel
I0512 23:42:16.243035 140311617681408 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_4/self_attention/value/kernel
I0512 23:42:16.243066 140311617681408 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_5/extra_mlp/wi_0/kernel
I0512 23:42:16.243097 140311617681408 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_5/extra_mlp/wi_1/kernel
I0512 23:42:16.243128 140311617681408 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_5/extra_mlp/wo/kernel
I0512 23:42:16.243159 140311617681408 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_5/mlp/expert/wi_0/kernel
I0512 23:42:16.265046 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/mlp/expert/wo/kernel/v
I0512 23:42:16.265076 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/mlp/expert/wo/kernel/v_col
I0512 23:42:16.265106 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/mlp/expert/wo/kernel/v_row
I0512 23:42:16.265135 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/mlp/router/router_weights/w/kernel/m
I0512 23:42:16.265165 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/mlp/router/router_weights/w/kernel/v
I0512 23:42:16.265194 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/mlp/router/router_weights/w/kernel/v_col
I0512 23:42:16.265223 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/mlp/router/router_weights/w/kernel/v_row
I0512 23:42:16.265252 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/pre_extra_mlp_layer_norm/scale/m
I0512 23:42:16.265284 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/pre_extra_mlp_layer_norm/scale/v
I0512 23:42:16.265314 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/pre_extra_mlp_layer_norm/scale/v_col
I0512 23:42:16.265343 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/pre_extra_mlp_layer_norm/scale/v_row
I0512 23:42:16.265372 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/pre_mlp_layer_norm/scale/m
I0512 23:42:16.265402 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/pre_mlp_layer_norm/scale/v
I0512 23:42:16.265431 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/pre_mlp_layer_norm/scale/v_col
I0512 23:42:16.243189 140311617681408 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_5/mlp/expert/wi_1/kernel
I0512 23:42:16.243220 140311617681408 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_5/mlp/expert/wo/kernel
I0512 23:42:16.243250 140311617681408 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_5/mlp/router/router_weights/w/kernel
I0512 23:42:16.243281 140311617681408 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_5/pre_extra_mlp_layer_norm/scale
I0512 23:42:16.243312 140311617681408 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_5/pre_mlp_layer_norm/scale
I0512 23:42:16.243343 140311617681408 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_5/pre_self_attention_layer_norm/scale
I0512 23:42:16.243373 140311617681408 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_5/self_attention/key/kernel
I0512 23:42:16.243405 140311617681408 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_5/self_attention/out/kernel
I0512 23:42:16.243438 140311617681408 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_5/self_attention/query/kernel
I0512 23:42:16.243469 140311617681408 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_5/self_attention/value/kernel
I0512 23:42:16.243500 140311617681408 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_6/mlp/wi_0/kernel
I0512 23:42:16.243531 140311617681408 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_6/mlp/wi_1/kernel
I0512 23:42:16.243562 140311617681408 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_6/mlp/wo/kernel
I0512 23:42:16.243593 140311617681408 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_6/pre_mlp_layer_norm/scale
I0512 23:42:16.243624 140311617681408 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_6/pre_self_attention_layer_norm/scale
I0512 23:42:16.243659 140311617681408 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_6/self_attention/key/kernel
I0512 23:42:16.265460 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/pre_mlp_layer_norm/scale/v_row
I0512 23:42:16.265489 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/pre_self_attention_layer_norm/scale/m
I0512 23:42:16.265519 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/pre_self_attention_layer_norm/scale/v
I0512 23:42:16.265547 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/pre_self_attention_layer_norm/scale/v_col
I0512 23:42:16.265577 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/pre_self_attention_layer_norm/scale/v_row
I0512 23:42:16.265607 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/self_attention/key/kernel/m
I0512 23:42:16.265636 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/self_attention/key/kernel/v
I0512 23:42:16.265666 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/self_attention/key/kernel/v_col
I0512 23:42:16.265695 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/self_attention/key/kernel/v_row
I0512 23:42:16.265725 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/self_attention/out/kernel/m
I0512 23:42:16.265754 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/self_attention/out/kernel/v
I0512 23:42:16.265783 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/self_attention/out/kernel/v_col
I0512 23:42:16.265813 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/self_attention/out/kernel/v_row
I0512 23:42:16.265842 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/self_attention/query/kernel/m
I0512 23:42:16.265871 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/self_attention/query/kernel/v
I0512 23:42:16.243692 140311617681408 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_6/self_attention/out/kernel
I0512 23:42:16.243723 140311617681408 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_6/self_attention/query/kernel
I0512 23:42:16.243754 140311617681408 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_6/self_attention/value/kernel
I0512 23:42:16.243784 140311617681408 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_7/mlp/wi_0/kernel
I0512 23:42:16.243815 140311617681408 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_7/mlp/wi_1/kernel
I0512 23:42:16.243846 140311617681408 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_7/mlp/wo/kernel
I0512 23:42:16.243877 140311617681408 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_7/pre_mlp_layer_norm/scale
I0512 23:42:16.243908 140311617681408 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_7/pre_self_attention_layer_norm/scale
I0512 23:42:16.243939 140311617681408 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_7/self_attention/key/kernel
I0512 23:42:16.243970 140311617681408 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_7/self_attention/out/kernel
I0512 23:42:16.244000 140311617681408 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_7/self_attention/query/kernel
I0512 23:42:16.244031 140311617681408 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_7/self_attention/value/kernel
I0512 23:42:16.244062 140311617681408 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_8/mlp/wi_0/kernel
I0512 23:42:16.244093 140311617681408 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_8/mlp/wi_1/kernel
I0512 23:42:16.244124 140311617681408 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_8/mlp/wo/kernel
I0512 23:42:16.244155 140311617681408 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_8/pre_mlp_layer_norm/scale
I0512 23:42:16.265900 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/self_attention/query/kernel/v_col
I0512 23:42:16.265929 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/self_attention/query/kernel/v_row
I0512 23:42:16.265958 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/self_attention/value/kernel/m
I0512 23:42:16.265987 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/self_attention/value/kernel/v
I0512 23:42:16.266021 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/self_attention/value/kernel/v_col
I0512 23:42:16.266052 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/self_attention/value/kernel/v_row
I0512 23:42:16.266082 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_6/mlp/wi_0/kernel/m
I0512 23:42:16.263252 140440593786880 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_9/self_attention/value/kernel
I0512 23:42:16.263286 140440593786880 checkpoints.py:1100] Restoring key from ckpt: target/decoder/logits_dense/kernel
I0512 23:42:16.263320 140440593786880 checkpoints.py:1100] Restoring key from ckpt: target/token_embedder/embedding
I0512 23:42:16.266111 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_6/mlp/wi_0/kernel/v
I0512 23:42:16.266140 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_6/mlp/wi_0/kernel/v_col
I0512 23:42:16.266170 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_6/mlp/wi_0/kernel/v_row
I0512 23:42:16.266200 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_6/mlp/wi_1/kernel/m
I0512 23:42:16.266229 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_6/mlp/wi_1/kernel/v
I0512 23:42:16.266276 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_6/mlp/wi_1/kernel/v_col
I0512 23:42:16.266313 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_6/mlp/wi_1/kernel/v_row
I0512 23:42:16.266343 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_6/mlp/wo/kernel/m
I0512 23:42:16.266373 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_6/mlp/wo/kernel/v
I0512 23:42:16.266402 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_6/mlp/wo/kernel/v_col
I0512 23:42:16.266432 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_6/mlp/wo/kernel/v_row
I0512 23:42:16.266461 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_6/pre_mlp_layer_norm/scale/m
I0512 23:42:16.266490 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_6/pre_mlp_layer_norm/scale/v
I0512 23:42:16.266520 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_6/pre_mlp_layer_norm/scale/v_col
I0512 23:42:16.266549 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_6/pre_mlp_layer_norm/scale/v_row
I0512 23:42:16.266578 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_6/pre_self_attention_layer_norm/scale/m
I0512 23:42:16.266607 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_6/pre_self_attention_layer_norm/scale/v
I0512 23:42:16.266637 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_6/pre_self_attention_layer_norm/scale/v_col
I0512 23:42:16.266666 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_6/pre_self_attention_layer_norm/scale/v_row
I0512 23:42:16.266695 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_6/self_attention/key/kernel/m
I0512 23:42:16.266725 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_6/self_attention/key/kernel/v
I0512 23:42:16.266754 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_6/self_attention/key/kernel/v_col
I0512 23:42:16.266784 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_6/self_attention/key/kernel/v_row
I0512 23:42:16.266818 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_6/self_attention/out/kernel/m
I0512 23:42:16.266847 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_6/self_attention/out/kernel/v
I0512 23:42:16.266877 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_6/self_attention/out/kernel/v_col
I0512 23:42:16.266906 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_6/self_attention/out/kernel/v_row
I0512 23:42:16.266935 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_6/self_attention/query/kernel/m
I0512 23:42:16.266964 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_6/self_attention/query/kernel/v
I0512 23:42:16.266993 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_6/self_attention/query/kernel/v_col
I0512 23:42:16.267028 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_6/self_attention/query/kernel/v_row
I0512 23:42:16.267059 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_6/self_attention/value/kernel/m
I0512 23:42:16.267089 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_6/self_attention/value/kernel/v
I0512 23:42:16.267118 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_6/self_attention/value/kernel/v_col
I0512 23:42:16.267147 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_6/self_attention/value/kernel/v_row
I0512 23:42:16.267176 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_7/mlp/wi_0/kernel/m
I0512 23:42:16.267205 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_7/mlp/wi_0/kernel/v
I0512 23:42:16.267235 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_7/mlp/wi_0/kernel/v_col
I0512 23:42:16.244186 140311617681408 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_8/pre_self_attention_layer_norm/scale
I0512 23:42:16.244217 140311617681408 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_8/self_attention/key/kernel
I0512 23:42:16.244248 140311617681408 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_8/self_attention/out/kernel
I0512 23:42:16.244279 140311617681408 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_8/self_attention/query/kernel
I0512 23:42:16.244309 140311617681408 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_8/self_attention/value/kernel
I0512 23:42:16.244339 140311617681408 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_9/mlp/wi_0/kernel
I0512 23:42:16.244370 140311617681408 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_9/mlp/wi_1/kernel
I0512 23:42:16.244401 140311617681408 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_9/mlp/wo/kernel
I0512 23:42:16.244434 140311617681408 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_9/pre_mlp_layer_norm/scale
I0512 23:42:16.244465 140311617681408 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_9/pre_self_attention_layer_norm/scale
I0512 23:42:16.244496 140311617681408 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_9/self_attention/key/kernel
I0512 23:42:16.244527 140311617681408 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_9/self_attention/out/kernel
I0512 23:42:16.244558 140311617681408 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_9/self_attention/query/kernel
I0512 23:42:16.244588 140311617681408 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_9/self_attention/value/kernel
I0512 23:42:16.244619 140311617681408 checkpoints.py:1100] Restoring key from ckpt: target/decoder/logits_dense/kernel
I0512 23:42:16.244650 140311617681408 checkpoints.py:1100] Restoring key from ckpt: target/token_embedder/embedding
I0512 23:42:16.267265 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_7/mlp/wi_0/kernel/v_row
I0512 23:42:16.267296 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_7/mlp/wi_1/kernel/m
I0512 23:42:16.267326 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_7/mlp/wi_1/kernel/v
I0512 23:42:16.267355 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_7/mlp/wi_1/kernel/v_col
I0512 23:42:16.267385 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_7/mlp/wi_1/kernel/v_row
I0512 23:42:16.267414 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_7/mlp/wo/kernel/m
I0512 23:42:16.267444 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_7/mlp/wo/kernel/v
I0512 23:42:16.267473 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_7/mlp/wo/kernel/v_col
I0512 23:42:16.267503 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_7/mlp/wo/kernel/v_row
I0512 23:42:16.267532 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_7/pre_mlp_layer_norm/scale/m
I0512 23:42:16.267561 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_7/pre_mlp_layer_norm/scale/v
I0512 23:42:16.267591 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_7/pre_mlp_layer_norm/scale/v_col
I0512 23:42:16.267620 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_7/pre_mlp_layer_norm/scale/v_row
I0512 23:42:16.267649 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_7/pre_self_attention_layer_norm/scale/m
I0512 23:42:16.267678 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_7/pre_self_attention_layer_norm/scale/v
I0512 23:42:16.267707 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_7/pre_self_attention_layer_norm/scale/v_col
I0512 23:42:16.267736 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_7/pre_self_attention_layer_norm/scale/v_row
I0512 23:42:16.267765 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_7/self_attention/key/kernel/m
I0512 23:42:16.267797 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_7/self_attention/key/kernel/v
I0512 23:42:16.267828 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_7/self_attention/key/kernel/v_col
I0512 23:42:16.267858 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_7/self_attention/key/kernel/v_row
I0512 23:42:16.267887 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_7/self_attention/out/kernel/m
I0512 23:42:16.267916 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_7/self_attention/out/kernel/v
I0512 23:42:16.267945 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_7/self_attention/out/kernel/v_col
I0512 23:42:16.267975 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_7/self_attention/out/kernel/v_row
I0512 23:42:16.268004 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_7/self_attention/query/kernel/m
I0512 23:42:16.268039 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_7/self_attention/query/kernel/v
I0512 23:42:16.268069 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_7/self_attention/query/kernel/v_col
I0512 23:42:16.268099 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_7/self_attention/query/kernel/v_row
I0512 23:42:16.268128 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_7/self_attention/value/kernel/m
I0512 23:42:16.268157 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_7/self_attention/value/kernel/v
I0512 23:42:16.268187 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_7/self_attention/value/kernel/v_col
I0512 23:42:16.268216 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_7/self_attention/value/kernel/v_row
I0512 23:42:16.268245 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_8/mlp/wi_0/kernel/m
I0512 23:42:16.268275 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_8/mlp/wi_0/kernel/v
I0512 23:42:16.268306 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_8/mlp/wi_0/kernel/v_col
I0512 23:42:16.268336 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_8/mlp/wi_0/kernel/v_row
I0512 23:42:16.268366 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_8/mlp/wi_1/kernel/m
I0512 23:42:16.268395 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_8/mlp/wi_1/kernel/v
I0512 23:42:16.268425 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_8/mlp/wi_1/kernel/v_col
I0512 23:42:16.268455 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_8/mlp/wi_1/kernel/v_row
I0512 23:42:16.268484 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_8/mlp/wo/kernel/m
I0512 23:42:16.268514 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_8/mlp/wo/kernel/v
I0512 23:42:16.268543 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_8/mlp/wo/kernel/v_col
I0512 23:42:16.268573 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_8/mlp/wo/kernel/v_row
I0512 23:42:16.268603 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_8/pre_mlp_layer_norm/scale/m
I0512 23:42:16.268632 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_8/pre_mlp_layer_norm/scale/v
I0512 23:42:16.268662 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_8/pre_mlp_layer_norm/scale/v_col
I0512 23:42:16.268692 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_8/pre_mlp_layer_norm/scale/v_row
I0512 23:42:16.268721 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_8/pre_self_attention_layer_norm/scale/m
I0512 23:42:16.268751 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_8/pre_self_attention_layer_norm/scale/v
I0512 23:42:16.268780 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_8/pre_self_attention_layer_norm/scale/v_col
I0512 23:42:16.268814 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_8/pre_self_attention_layer_norm/scale/v_row
I0512 23:42:16.268843 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_8/self_attention/key/kernel/m
I0512 23:42:16.268872 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_8/self_attention/key/kernel/v
I0512 23:42:16.268901 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_8/self_attention/key/kernel/v_col
I0512 23:42:16.268931 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_8/self_attention/key/kernel/v_row
I0512 23:42:16.268959 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_8/self_attention/out/kernel/m
I0512 23:42:16.268989 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_8/self_attention/out/kernel/v
I0512 23:42:16.269022 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_8/self_attention/out/kernel/v_col
I0512 23:42:16.269053 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_8/self_attention/out/kernel/v_row
I0512 23:42:16.269083 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_8/self_attention/query/kernel/m
I0512 23:42:16.269112 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_8/self_attention/query/kernel/v
I0512 23:42:16.269141 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_8/self_attention/query/kernel/v_col
I0512 23:42:16.269170 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_8/self_attention/query/kernel/v_row
I0512 23:42:16.269199 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_8/self_attention/value/kernel/m
I0512 23:42:16.269229 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_8/self_attention/value/kernel/v
I0512 23:42:16.269258 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_8/self_attention/value/kernel/v_col
I0512 23:42:16.269289 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_8/self_attention/value/kernel/v_row
I0512 23:42:16.269318 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_9/mlp/wi_0/kernel/m
I0512 23:42:16.269348 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_9/mlp/wi_0/kernel/v
I0512 23:42:16.269377 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_9/mlp/wi_0/kernel/v_col
I0512 23:42:16.269407 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_9/mlp/wi_0/kernel/v_row
I0512 23:42:16.269436 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_9/mlp/wi_1/kernel/m
I0512 23:42:16.269466 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_9/mlp/wi_1/kernel/v
I0512 23:42:16.269495 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_9/mlp/wi_1/kernel/v_col
I0512 23:42:16.269525 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_9/mlp/wi_1/kernel/v_row
I0512 23:42:16.269554 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_9/mlp/wo/kernel/m
I0512 23:42:16.269584 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_9/mlp/wo/kernel/v
I0512 23:42:16.269613 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_9/mlp/wo/kernel/v_col
I0512 23:42:16.269642 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_9/mlp/wo/kernel/v_row
I0512 23:42:16.269672 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_9/pre_mlp_layer_norm/scale/m
I0512 23:42:16.269701 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_9/pre_mlp_layer_norm/scale/v
I0512 23:42:16.269731 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_9/pre_mlp_layer_norm/scale/v_col
I0512 23:42:16.269761 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_9/pre_mlp_layer_norm/scale/v_row
I0512 23:42:16.269792 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_9/pre_self_attention_layer_norm/scale/m
I0512 23:42:16.269824 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_9/pre_self_attention_layer_norm/scale/v
I0512 23:42:16.269852 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_9/pre_self_attention_layer_norm/scale/v_col
I0512 23:42:16.269882 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_9/pre_self_attention_layer_norm/scale/v_row
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
I0000 00:00:1715557336.268957  455454 gcs_resource.cc:109] Using default AdmissionQueue with limit 32
I0512 23:42:16.269911 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_9/self_attention/key/kernel/m
I0512 23:42:16.269941 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_9/self_attention/key/kernel/v
I0512 23:42:16.269970 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_9/self_attention/key/kernel/v_col
I0512 23:42:16.269999 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_9/self_attention/key/kernel/v_row
I0512 23:42:16.270034 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_9/self_attention/out/kernel/m
I0512 23:42:16.270064 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_9/self_attention/out/kernel/v
I0512 23:42:16.270094 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_9/self_attention/out/kernel/v_col
I0512 23:42:16.270123 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_9/self_attention/out/kernel/v_row
I0512 23:42:16.270153 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_9/self_attention/query/kernel/m
I0512 23:42:16.270182 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_9/self_attention/query/kernel/v
I0512 23:42:16.270211 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_9/self_attention/query/kernel/v_col
I0512 23:42:16.270241 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_9/self_attention/query/kernel/v_row
I0512 23:42:16.270291 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_9/self_attention/value/kernel/m
I0512 23:42:16.270324 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_9/self_attention/value/kernel/v
I0512 23:42:16.270354 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_9/self_attention/value/kernel/v_col
I0512 23:42:16.270383 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_9/self_attention/value/kernel/v_row
I0512 23:42:16.270411 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/logits_dense/kernel/m
I0512 23:42:16.270441 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/logits_dense/kernel/v
I0512 23:42:16.270471 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/logits_dense/kernel/v_col
I0512 23:42:16.270500 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/logits_dense/kernel/v_row
I0512 23:42:16.270530 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/token_embedder/embedding/m
I0512 23:42:16.270559 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/token_embedder/embedding/v
I0512 23:42:16.270588 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/token_embedder/embedding/v_col
I0512 23:42:16.270618 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/param_states/token_embedder/embedding/v_row
I0512 23:42:16.270647 140095893518336 checkpoints.py:1100] Restoring key from ckpt: state/step
I0512 23:42:16.270677 140095893518336 checkpoints.py:1100] Restoring key from ckpt: target/decoder/decoder_norm/scale
I0512 23:42:16.270707 140095893518336 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_0/mlp/wi_0/kernel
I0512 23:42:16.270737 140095893518336 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_0/mlp/wi_1/kernel
I0512 23:42:16.270766 140095893518336 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_0/mlp/wo/kernel
I0512 23:42:16.270798 140095893518336 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_0/pre_mlp_layer_norm/scale
I0512 23:42:16.270830 140095893518336 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_0/pre_self_attention_layer_norm/scale
I0512 23:42:16.270860 140095893518336 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_0/self_attention/key/kernel
I0512 23:42:16.270889 140095893518336 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_0/self_attention/out/kernel
I0512 23:42:16.270919 140095893518336 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_0/self_attention/query/kernel
I0512 23:42:16.270948 140095893518336 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_0/self_attention/value/kernel
I0512 23:42:16.270977 140095893518336 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_1/mlp/wi_0/kernel
I0512 23:42:16.271007 140095893518336 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_1/mlp/wi_1/kernel
I0512 23:42:16.271043 140095893518336 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_1/mlp/wo/kernel
I0512 23:42:16.271073 140095893518336 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_1/pre_mlp_layer_norm/scale
I0512 23:42:16.271102 140095893518336 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_1/pre_self_attention_layer_norm/scale
I0512 23:42:16.271132 140095893518336 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_1/self_attention/key/kernel
I0512 23:42:16.271161 140095893518336 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_1/self_attention/out/kernel
I0512 23:42:16.271190 140095893518336 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_1/self_attention/query/kernel
I0512 23:42:16.271219 140095893518336 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_1/self_attention/value/kernel
I0512 23:42:16.271248 140095893518336 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_10/mlp/wi_0/kernel
I0512 23:42:16.271279 140095893518336 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_10/mlp/wi_1/kernel
I0512 23:42:16.271310 140095893518336 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_10/mlp/wo/kernel
I0512 23:42:16.271340 140095893518336 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_10/pre_mlp_layer_norm/scale
I0512 23:42:16.271369 140095893518336 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_10/pre_self_attention_layer_norm/scale
I0512 23:42:16.271399 140095893518336 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_10/self_attention/key/kernel
I0512 23:42:16.271428 140095893518336 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_10/self_attention/out/kernel
I0512 23:42:16.271458 140095893518336 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_10/self_attention/query/kernel
I0512 23:42:16.271487 140095893518336 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_10/self_attention/value/kernel
I0512 23:42:16.271517 140095893518336 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_11/extra_mlp/wi_0/kernel
I0512 23:42:16.271546 140095893518336 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_11/extra_mlp/wi_1/kernel
I0512 23:42:16.271576 140095893518336 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_11/extra_mlp/wo/kernel
I0512 23:42:16.271605 140095893518336 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_11/mlp/expert/wi_0/kernel
I0512 23:42:16.271634 140095893518336 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_11/mlp/expert/wi_1/kernel
I0512 23:42:16.271663 140095893518336 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_11/mlp/expert/wo/kernel
I0512 23:42:16.271692 140095893518336 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_11/mlp/router/router_weights/w/kernel
I0512 23:42:16.271722 140095893518336 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_11/pre_extra_mlp_layer_norm/scale
I0512 23:42:16.271751 140095893518336 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_11/pre_mlp_layer_norm/scale
I0512 23:42:16.271781 140095893518336 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_11/pre_self_attention_layer_norm/scale
I0512 23:42:16.271811 140095893518336 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_11/self_attention/key/kernel
I0512 23:42:16.271840 140095893518336 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_11/self_attention/out/kernel
I0512 23:42:16.271869 140095893518336 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_11/self_attention/query/kernel
I0512 23:42:16.271898 140095893518336 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_11/self_attention/value/kernel
I0512 23:42:16.271928 140095893518336 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_12/mlp/wi_0/kernel
I0512 23:42:16.271958 140095893518336 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_12/mlp/wi_1/kernel
I0512 23:42:16.271987 140095893518336 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_12/mlp/wo/kernel
I0512 23:42:16.272021 140095893518336 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_12/pre_mlp_layer_norm/scale
I0512 23:42:16.272052 140095893518336 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_12/pre_self_attention_layer_norm/scale
I0512 23:42:16.272082 140095893518336 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_12/self_attention/key/kernel
I0512 23:42:16.272112 140095893518336 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_12/self_attention/out/kernel
I0512 23:42:16.272141 140095893518336 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_12/self_attention/query/kernel
I0512 23:42:16.272170 140095893518336 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_12/self_attention/value/kernel
I0512 23:42:16.272200 140095893518336 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_13/mlp/wi_0/kernel
I0512 23:42:16.272229 140095893518336 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_13/mlp/wi_1/kernel
I0512 23:42:16.272258 140095893518336 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_13/mlp/wo/kernel
I0512 23:42:16.272290 140095893518336 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_13/pre_mlp_layer_norm/scale
I0512 23:42:16.272320 140095893518336 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_13/pre_self_attention_layer_norm/scale
I0512 23:42:16.272349 140095893518336 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_13/self_attention/key/kernel
I0512 23:42:16.272379 140095893518336 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_13/self_attention/out/kernel
I0512 23:42:16.272408 140095893518336 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_13/self_attention/query/kernel
I0512 23:42:16.272438 140095893518336 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_13/self_attention/value/kernel
I0512 23:42:16.272467 140095893518336 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_14/mlp/wi_0/kernel
I0512 23:42:16.272496 140095893518336 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_14/mlp/wi_1/kernel
I0512 23:42:16.272526 140095893518336 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_14/mlp/wo/kernel
I0512 23:42:16.272555 140095893518336 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_14/pre_mlp_layer_norm/scale
I0512 23:42:16.272584 140095893518336 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_14/pre_self_attention_layer_norm/scale
I0512 23:42:16.272614 140095893518336 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_14/self_attention/key/kernel
I0512 23:42:16.272643 140095893518336 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_14/self_attention/out/kernel
I0512 23:42:16.272673 140095893518336 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_14/self_attention/query/kernel
I0512 23:42:16.272703 140095893518336 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_14/self_attention/value/kernel
I0512 23:42:16.272732 140095893518336 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_15/mlp/wi_0/kernel
I0512 23:42:16.272762 140095893518336 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_15/mlp/wi_1/kernel
I0512 23:42:16.272791 140095893518336 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_15/mlp/wo/kernel
I0512 23:42:16.272820 140095893518336 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_15/pre_mlp_layer_norm/scale
I0512 23:42:16.272850 140095893518336 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_15/pre_self_attention_layer_norm/scale
I0512 23:42:16.272879 140095893518336 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_15/self_attention/key/kernel
I0512 23:42:16.272909 140095893518336 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_15/self_attention/out/kernel
I0512 23:42:16.272938 140095893518336 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_15/self_attention/query/kernel
I0512 23:42:16.272968 140095893518336 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_15/self_attention/value/kernel
I0512 23:42:16.272998 140095893518336 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_16/mlp/wi_0/kernel
I0512 23:42:16.273033 140095893518336 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_16/mlp/wi_1/kernel
I0512 23:42:16.273063 140095893518336 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_16/mlp/wo/kernel
I0512 23:42:16.273093 140095893518336 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_16/pre_mlp_layer_norm/scale
I0512 23:42:16.273122 140095893518336 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_16/pre_self_attention_layer_norm/scale
I0512 23:42:16.273151 140095893518336 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_16/self_attention/key/kernel
I0512 23:42:16.273180 140095893518336 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_16/self_attention/out/kernel
I0512 23:42:16.273210 140095893518336 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_16/self_attention/query/kernel
I0512 23:42:16.273240 140095893518336 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_16/self_attention/value/kernel
I0512 23:42:16.273270 140095893518336 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_17/extra_mlp/wi_0/kernel
I0512 23:42:16.273301 140095893518336 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_17/extra_mlp/wi_1/kernel
I0512 23:42:16.273331 140095893518336 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_17/extra_mlp/wo/kernel
I0512 23:42:16.273361 140095893518336 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_17/mlp/expert/wi_0/kernel
I0512 23:42:16.273390 140095893518336 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_17/mlp/expert/wi_1/kernel
I0512 23:42:16.273420 140095893518336 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_17/mlp/expert/wo/kernel
I0512 23:42:16.273449 140095893518336 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_17/mlp/router/router_weights/w/kernel
I0512 23:42:16.273479 140095893518336 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_17/pre_extra_mlp_layer_norm/scale
I0512 23:42:16.273508 140095893518336 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_17/pre_mlp_layer_norm/scale
I0512 23:42:16.273538 140095893518336 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_17/pre_self_attention_layer_norm/scale
I0512 23:42:16.273567 140095893518336 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_17/self_attention/key/kernel
I0512 23:42:16.273597 140095893518336 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_17/self_attention/out/kernel
I0512 23:42:16.273626 140095893518336 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_17/self_attention/query/kernel
I0512 23:42:16.273656 140095893518336 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_17/self_attention/value/kernel
I0512 23:42:16.273685 140095893518336 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_18/mlp/wi_0/kernel
I0512 23:42:16.273715 140095893518336 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_18/mlp/wi_1/kernel
I0512 23:42:16.273744 140095893518336 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_18/mlp/wo/kernel
I0512 23:42:16.273773 140095893518336 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_18/pre_mlp_layer_norm/scale
I0512 23:42:16.273802 140095893518336 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_18/pre_self_attention_layer_norm/scale
I0512 23:42:16.273832 140095893518336 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_18/self_attention/key/kernel
I0512 23:42:16.273862 140095893518336 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_18/self_attention/out/kernel
I0512 23:42:16.273891 140095893518336 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_18/self_attention/query/kernel
I0512 23:42:16.273921 140095893518336 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_18/self_attention/value/kernel
I0512 23:42:16.273950 140095893518336 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_19/mlp/wi_0/kernel
I0512 23:42:16.273980 140095893518336 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_19/mlp/wi_1/kernel
I0512 23:42:16.274009 140095893518336 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_19/mlp/wo/kernel
I0512 23:42:16.274045 140095893518336 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_19/pre_mlp_layer_norm/scale
I0512 23:42:16.274075 140095893518336 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_19/pre_self_attention_layer_norm/scale
I0512 23:42:16.274105 140095893518336 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_19/self_attention/key/kernel
I0512 23:42:16.274135 140095893518336 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_19/self_attention/out/kernel
I0512 23:42:16.274164 140095893518336 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_19/self_attention/query/kernel
I0512 23:42:16.274194 140095893518336 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_19/self_attention/value/kernel
I0512 23:42:16.274223 140095893518336 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_2/mlp/wi_0/kernel
I0512 23:42:16.274272 140095893518336 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_2/mlp/wi_1/kernel
I0512 23:42:16.274311 140095893518336 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_2/mlp/wo/kernel
I0512 23:42:16.274343 140095893518336 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_2/pre_mlp_layer_norm/scale
I0512 23:42:16.274372 140095893518336 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_2/pre_self_attention_layer_norm/scale
I0512 23:42:16.274402 140095893518336 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_2/self_attention/key/kernel
I0512 23:42:16.274431 140095893518336 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_2/self_attention/out/kernel
I0512 23:42:16.274461 140095893518336 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_2/self_attention/query/kernel
I0512 23:42:16.274491 140095893518336 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_2/self_attention/value/kernel
I0512 23:42:16.274536 140095893518336 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_20/mlp/wi_0/kernel
I0512 23:42:16.274568 140095893518336 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_20/mlp/wi_1/kernel
I0512 23:42:16.274597 140095893518336 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_20/mlp/wo/kernel
I0512 23:42:16.274627 140095893518336 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_20/pre_mlp_layer_norm/scale
I0512 23:42:16.274657 140095893518336 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_20/pre_self_attention_layer_norm/scale
I0512 23:42:16.274686 140095893518336 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_20/self_attention/key/kernel
I0512 23:42:16.274715 140095893518336 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_20/self_attention/out/kernel
I0512 23:42:16.274745 140095893518336 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_20/self_attention/query/kernel
I0512 23:42:16.274775 140095893518336 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_20/self_attention/value/kernel
I0512 23:42:16.274804 140095893518336 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_21/mlp/wi_0/kernel
I0512 23:42:16.274834 140095893518336 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_21/mlp/wi_1/kernel
I0512 23:42:16.274863 140095893518336 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_21/mlp/wo/kernel
I0512 23:42:16.274893 140095893518336 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_21/pre_mlp_layer_norm/scale
I0512 23:42:16.274922 140095893518336 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_21/pre_self_attention_layer_norm/scale
I0512 23:42:16.274952 140095893518336 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_21/self_attention/key/kernel
I0512 23:42:16.274981 140095893518336 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_21/self_attention/out/kernel
I0512 23:42:16.275011 140095893518336 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_21/self_attention/query/kernel
I0512 23:42:16.275050 140095893518336 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_21/self_attention/value/kernel
I0512 23:42:16.275080 140095893518336 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_22/mlp/wi_0/kernel
I0512 23:42:16.275110 140095893518336 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_22/mlp/wi_1/kernel
I0512 23:42:16.275139 140095893518336 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_22/mlp/wo/kernel
I0512 23:42:16.275169 140095893518336 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_22/pre_mlp_layer_norm/scale
I0512 23:42:16.275198 140095893518336 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_22/pre_self_attention_layer_norm/scale
I0512 23:42:16.275227 140095893518336 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_22/self_attention/key/kernel
I0512 23:42:16.275257 140095893518336 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_22/self_attention/out/kernel
I0512 23:42:16.275288 140095893518336 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_22/self_attention/query/kernel
I0512 23:42:16.275319 140095893518336 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_22/self_attention/value/kernel
I0512 23:42:16.275348 140095893518336 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_23/extra_mlp/wi_0/kernel
I0512 23:42:16.275378 140095893518336 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_23/extra_mlp/wi_1/kernel
I0512 23:42:16.275407 140095893518336 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_23/extra_mlp/wo/kernel
I0512 23:42:16.275437 140095893518336 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_23/mlp/expert/wi_0/kernel
I0512 23:42:16.275466 140095893518336 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_23/mlp/expert/wi_1/kernel
I0512 23:42:16.275495 140095893518336 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_23/mlp/expert/wo/kernel
I0512 23:42:16.275524 140095893518336 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_23/mlp/router/router_weights/w/kernel
I0512 23:42:16.275554 140095893518336 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_23/pre_extra_mlp_layer_norm/scale
I0512 23:42:16.275583 140095893518336 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_23/pre_mlp_layer_norm/scale
I0512 23:42:16.275612 140095893518336 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_23/pre_self_attention_layer_norm/scale
I0512 23:42:16.275642 140095893518336 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_23/self_attention/key/kernel
I0512 23:42:16.275671 140095893518336 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_23/self_attention/out/kernel
I0512 23:42:16.275701 140095893518336 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_23/self_attention/query/kernel
I0512 23:42:16.275730 140095893518336 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_23/self_attention/value/kernel
I0512 23:42:16.275759 140095893518336 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_3/mlp/wi_0/kernel
I0512 23:42:16.275789 140095893518336 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_3/mlp/wi_1/kernel
I0512 23:42:16.275819 140095893518336 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_3/mlp/wo/kernel
I0512 23:42:16.275848 140095893518336 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_3/pre_mlp_layer_norm/scale
I0512 23:42:16.275878 140095893518336 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_3/pre_self_attention_layer_norm/scale
I0512 23:42:16.275907 140095893518336 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_3/self_attention/key/kernel
I0512 23:42:16.275937 140095893518336 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_3/self_attention/out/kernel
I0512 23:42:16.275966 140095893518336 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_3/self_attention/query/kernel
I0512 23:42:16.275995 140095893518336 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_3/self_attention/value/kernel
I0512 23:42:16.276030 140095893518336 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_4/mlp/wi_0/kernel
I0512 23:42:16.276061 140095893518336 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_4/mlp/wi_1/kernel
I0512 23:42:16.276091 140095893518336 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_4/mlp/wo/kernel
I0512 23:42:16.276120 140095893518336 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_4/pre_mlp_layer_norm/scale
I0512 23:42:16.276150 140095893518336 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_4/pre_self_attention_layer_norm/scale
I0512 23:42:16.276179 140095893518336 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_4/self_attention/key/kernel
I0512 23:42:16.276209 140095893518336 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_4/self_attention/out/kernel
I0512 23:42:16.276238 140095893518336 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_4/self_attention/query/kernel
I0512 23:42:16.276268 140095893518336 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_4/self_attention/value/kernel
I0512 23:42:16.276299 140095893518336 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_5/extra_mlp/wi_0/kernel
I0512 23:42:16.276329 140095893518336 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_5/extra_mlp/wi_1/kernel
I0512 23:42:16.276359 140095893518336 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_5/extra_mlp/wo/kernel
I0512 23:42:16.276389 140095893518336 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_5/mlp/expert/wi_0/kernel
I0512 23:42:16.276418 140095893518336 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_5/mlp/expert/wi_1/kernel
I0512 23:42:16.276448 140095893518336 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_5/mlp/expert/wo/kernel
I0000 00:00:1715557336.275739  457594 google_auth_provider.cc:180] Running on GCE, using service account 1081612546678-compute@developer.gserviceaccount.com
I0512 23:42:16.276477 140095893518336 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_5/mlp/router/router_weights/w/kernel
I0512 23:42:16.276506 140095893518336 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_5/pre_extra_mlp_layer_norm/scale
I0512 23:42:16.276536 140095893518336 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_5/pre_mlp_layer_norm/scale
I0512 23:42:16.276566 140095893518336 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_5/pre_self_attention_layer_norm/scale
I0512 23:42:16.276595 140095893518336 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_5/self_attention/key/kernel
I0512 23:42:16.276625 140095893518336 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_5/self_attention/out/kernel
I0512 23:42:16.276654 140095893518336 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_5/self_attention/query/kernel
I0512 23:42:16.276684 140095893518336 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_5/self_attention/value/kernel
I0512 23:42:16.276713 140095893518336 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_6/mlp/wi_0/kernel
I0512 23:42:16.276742 140095893518336 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_6/mlp/wi_1/kernel
I0512 23:42:16.276772 140095893518336 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_6/mlp/wo/kernel
I0512 23:42:16.276801 140095893518336 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_6/pre_mlp_layer_norm/scale
I0512 23:42:16.276831 140095893518336 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_6/pre_self_attention_layer_norm/scale
I0512 23:42:16.276861 140095893518336 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_6/self_attention/key/kernel
I0512 23:42:16.276890 140095893518336 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_6/self_attention/out/kernel
I0512 23:42:16.276920 140095893518336 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_6/self_attention/query/kernel
I0512 23:42:16.276949 140095893518336 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_6/self_attention/value/kernel
I0512 23:42:16.276978 140095893518336 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_7/mlp/wi_0/kernel
I0512 23:42:16.277008 140095893518336 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_7/mlp/wi_1/kernel
I0512 23:42:16.277043 140095893518336 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_7/mlp/wo/kernel
I0512 23:42:16.277073 140095893518336 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_7/pre_mlp_layer_norm/scale
I0512 23:42:16.277103 140095893518336 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_7/pre_self_attention_layer_norm/scale
I0512 23:42:16.277132 140095893518336 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_7/self_attention/key/kernel
I0512 23:42:16.277162 140095893518336 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_7/self_attention/out/kernel
I0512 23:42:16.277191 140095893518336 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_7/self_attention/query/kernel
I0512 23:42:16.277221 140095893518336 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_7/self_attention/value/kernel
I0512 23:42:16.277250 140095893518336 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_8/mlp/wi_0/kernel
I0512 23:42:16.277281 140095893518336 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_8/mlp/wi_1/kernel
I0512 23:42:16.277312 140095893518336 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_8/mlp/wo/kernel
I0512 23:42:16.277342 140095893518336 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_8/pre_mlp_layer_norm/scale
I0512 23:42:16.277371 140095893518336 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_8/pre_self_attention_layer_norm/scale
I0512 23:42:16.277400 140095893518336 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_8/self_attention/key/kernel
I0512 23:42:16.277430 140095893518336 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_8/self_attention/out/kernel
I0512 23:42:16.277460 140095893518336 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_8/self_attention/query/kernel
I0512 23:42:16.277489 140095893518336 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_8/self_attention/value/kernel
I0512 23:42:16.277518 140095893518336 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_9/mlp/wi_0/kernel
I0512 23:42:16.277548 140095893518336 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_9/mlp/wi_1/kernel
I0512 23:42:16.277577 140095893518336 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_9/mlp/wo/kernel
I0512 23:42:16.277607 140095893518336 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_9/pre_mlp_layer_norm/scale
I0512 23:42:16.277637 140095893518336 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_9/pre_self_attention_layer_norm/scale
I0512 23:42:16.277667 140095893518336 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_9/self_attention/key/kernel
I0512 23:42:16.277696 140095893518336 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_9/self_attention/out/kernel
I0512 23:42:16.277725 140095893518336 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_9/self_attention/query/kernel
I0512 23:42:16.277755 140095893518336 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_9/self_attention/value/kernel
I0512 23:42:16.277784 140095893518336 checkpoints.py:1100] Restoring key from ckpt: target/decoder/logits_dense/kernel
I0512 23:42:16.277814 140095893518336 checkpoints.py:1100] Restoring key from ckpt: target/token_embedder/embedding
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
I0000 00:00:1715557336.292774  475641 gcs_resource.cc:109] Using default AdmissionQueue with limit 32
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
I0000 00:00:1715557336.274758  487766 gcs_resource.cc:109] Using default AdmissionQueue with limit 32
I0000 00:00:1715557336.299590  477807 google_auth_provider.cc:180] Running on GCE, using service account 1081612546678-compute@developer.gserviceaccount.com
I0000 00:00:1715557336.281441  489943 google_auth_provider.cc:180] Running on GCE, using service account 1081612546678-compute@developer.gserviceaccount.com
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
I0000 00:00:1715557336.307572  450615 gcs_resource.cc:109] Using default AdmissionQueue with limit 32
I0000 00:00:1715557336.311954  452786 google_auth_provider.cc:180] Running on GCE, using service account 1081612546678-compute@developer.gserviceaccount.com
I0512 23:42:16.327746 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/decoder_norm/scale/m
I0512 23:42:16.327905 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/decoder_norm/scale/v
I0512 23:42:16.327947 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/decoder_norm/scale/v_col
I0512 23:42:16.327982 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/decoder_norm/scale/v_row
I0512 23:42:16.328016 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_0/mlp/wi_0/kernel/m
I0512 23:42:16.328050 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_0/mlp/wi_0/kernel/v
I0512 23:42:16.328083 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_0/mlp/wi_0/kernel/v_col
I0512 23:42:16.328117 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_0/mlp/wi_0/kernel/v_row
I0512 23:42:16.328150 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_0/mlp/wi_1/kernel/m
I0512 23:42:16.328183 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_0/mlp/wi_1/kernel/v
I0512 23:42:16.328216 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_0/mlp/wi_1/kernel/v_col
I0512 23:42:16.328250 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_0/mlp/wi_1/kernel/v_row
I0512 23:42:16.328283 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_0/mlp/wo/kernel/m
I0512 23:42:16.328316 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_0/mlp/wo/kernel/v
I0512 23:42:16.328349 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_0/mlp/wo/kernel/v_col
I0512 23:42:16.328383 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_0/mlp/wo/kernel/v_row
I0512 23:42:16.328416 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_0/pre_mlp_layer_norm/scale/m
I0512 23:42:16.328450 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_0/pre_mlp_layer_norm/scale/v
I0512 23:42:16.328483 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_0/pre_mlp_layer_norm/scale/v_col
I0512 23:42:16.328516 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_0/pre_mlp_layer_norm/scale/v_row
I0512 23:42:16.328549 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_0/pre_self_attention_layer_norm/scale/m
I0512 23:42:16.328583 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_0/pre_self_attention_layer_norm/scale/v
I0512 23:42:16.328616 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_0/pre_self_attention_layer_norm/scale/v_col
I0512 23:42:16.328651 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_0/pre_self_attention_layer_norm/scale/v_row
I0512 23:42:16.328686 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_0/self_attention/key/kernel/m
I0512 23:42:16.328720 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_0/self_attention/key/kernel/v
I0512 23:42:16.328753 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_0/self_attention/key/kernel/v_col
I0512 23:42:16.328786 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_0/self_attention/key/kernel/v_row
I0512 23:42:16.328819 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_0/self_attention/out/kernel/m
I0512 23:42:16.328853 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_0/self_attention/out/kernel/v
I0512 23:42:16.328885 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_0/self_attention/out/kernel/v_col
I0512 23:42:16.328925 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_0/self_attention/out/kernel/v_row
I0512 23:42:16.328958 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_0/self_attention/query/kernel/m
I0512 23:42:16.328992 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_0/self_attention/query/kernel/v
I0512 23:42:16.329025 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_0/self_attention/query/kernel/v_col
I0512 23:42:16.329058 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_0/self_attention/query/kernel/v_row
I0512 23:42:16.329091 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_0/self_attention/value/kernel/m
I0512 23:42:16.329124 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_0/self_attention/value/kernel/v
I0512 23:42:16.329157 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_0/self_attention/value/kernel/v_col
I0512 23:42:16.329196 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_0/self_attention/value/kernel/v_row
I0512 23:42:16.329231 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_1/mlp/wi_0/kernel/m
I0512 23:42:16.329268 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_1/mlp/wi_0/kernel/v
I0512 23:42:16.329305 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_1/mlp/wi_0/kernel/v_col
I0512 23:42:16.329342 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_1/mlp/wi_0/kernel/v_row
I0512 23:42:16.329376 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_1/mlp/wi_1/kernel/m
I0512 23:42:16.329413 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_1/mlp/wi_1/kernel/v
I0512 23:42:16.329450 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_1/mlp/wi_1/kernel/v_col
I0512 23:42:16.329486 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_1/mlp/wi_1/kernel/v_row
I0512 23:42:16.329520 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_1/mlp/wo/kernel/m
I0512 23:42:16.329558 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_1/mlp/wo/kernel/v
I0512 23:42:16.329594 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_1/mlp/wo/kernel/v_col
I0512 23:42:16.329629 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_1/mlp/wo/kernel/v_row
I0512 23:42:16.329667 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_1/pre_mlp_layer_norm/scale/m
I0512 23:42:16.329704 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_1/pre_mlp_layer_norm/scale/v
I0512 23:42:16.329742 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_1/pre_mlp_layer_norm/scale/v_col
I0512 23:42:16.329776 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_1/pre_mlp_layer_norm/scale/v_row
I0512 23:42:16.329814 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_1/pre_self_attention_layer_norm/scale/m
I0512 23:42:16.329851 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_1/pre_self_attention_layer_norm/scale/v
I0512 23:42:16.329888 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_1/pre_self_attention_layer_norm/scale/v_col
I0512 23:42:16.329927 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_1/pre_self_attention_layer_norm/scale/v_row
I0512 23:42:16.329964 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_1/self_attention/key/kernel/m
I0512 23:42:16.330001 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_1/self_attention/key/kernel/v
I0512 23:42:16.330038 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_1/self_attention/key/kernel/v_col
I0512 23:42:16.330072 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_1/self_attention/key/kernel/v_row
I0512 23:42:16.330133 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_1/self_attention/out/kernel/m
I0512 23:42:16.330169 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_1/self_attention/out/kernel/v
I0512 23:42:16.330206 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_1/self_attention/out/kernel/v_col
I0512 23:42:16.330243 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_1/self_attention/out/kernel/v_row
I0512 23:42:16.330276 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_1/self_attention/query/kernel/m
I0512 23:42:16.330313 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_1/self_attention/query/kernel/v
I0512 23:42:16.330350 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_1/self_attention/query/kernel/v_col
I0512 23:42:16.330386 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_1/self_attention/query/kernel/v_row
I0512 23:42:16.330420 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_1/self_attention/value/kernel/m
I0512 23:42:16.330457 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_1/self_attention/value/kernel/v
I0512 23:42:16.330493 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_1/self_attention/value/kernel/v_col
I0512 23:42:16.330528 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_1/self_attention/value/kernel/v_row
I0512 23:42:16.330565 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_10/mlp/wi_0/kernel/m
I0512 23:42:16.330602 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_10/mlp/wi_0/kernel/v
I0512 23:42:16.330636 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_10/mlp/wi_0/kernel/v_col
I0512 23:42:16.330674 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_10/mlp/wi_0/kernel/v_row
I0512 23:42:16.330712 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_10/mlp/wi_1/kernel/m
I0512 23:42:16.330748 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_10/mlp/wi_1/kernel/v
I0512 23:42:16.330783 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_10/mlp/wi_1/kernel/v_col
I0512 23:42:16.330819 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_10/mlp/wi_1/kernel/v_row
I0512 23:42:16.330856 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_10/mlp/wo/kernel/m
I0512 23:42:16.330890 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_10/mlp/wo/kernel/v
I0512 23:42:16.330932 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_10/mlp/wo/kernel/v_col
I0512 23:42:16.330969 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_10/mlp/wo/kernel/v_row
I0512 23:42:16.331006 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_10/pre_mlp_layer_norm/scale/m
I0512 23:42:16.331040 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_10/pre_mlp_layer_norm/scale/v
I0512 23:42:16.331077 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_10/pre_mlp_layer_norm/scale/v_col
I0512 23:42:16.331114 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_10/pre_mlp_layer_norm/scale/v_row
I0512 23:42:16.331169 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_10/pre_self_attention_layer_norm/scale/m
I0512 23:42:16.331207 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_10/pre_self_attention_layer_norm/scale/v
I0512 23:42:16.331242 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_10/pre_self_attention_layer_norm/scale/v_col
I0512 23:42:16.331278 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_10/pre_self_attention_layer_norm/scale/v_row
I0512 23:42:16.331315 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_10/self_attention/key/kernel/m
I0512 23:42:16.331349 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_10/self_attention/key/kernel/v
I0512 23:42:16.331385 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_10/self_attention/key/kernel/v_col
I0512 23:42:16.331422 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_10/self_attention/key/kernel/v_row
I0512 23:42:16.331455 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_10/self_attention/out/kernel/m
I0512 23:42:16.331492 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_10/self_attention/out/kernel/v
I0512 23:42:16.331529 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_10/self_attention/out/kernel/v_col
I0512 23:42:16.331562 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_10/self_attention/out/kernel/v_row
I0512 23:42:16.331599 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_10/self_attention/query/kernel/m
I0512 23:42:16.331636 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_10/self_attention/query/kernel/v
I0512 23:42:16.331675 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_10/self_attention/query/kernel/v_col
I0512 23:42:16.331709 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_10/self_attention/query/kernel/v_row
I0512 23:42:16.331746 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_10/self_attention/value/kernel/m
I0512 23:42:16.331783 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_10/self_attention/value/kernel/v
I0512 23:42:16.331819 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_10/self_attention/value/kernel/v_col
I0512 23:42:16.331853 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_10/self_attention/value/kernel/v_row
I0512 23:42:16.331889 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/extra_mlp/wi_0/kernel/m
I0512 23:42:16.331931 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/extra_mlp/wi_0/kernel/v
I0512 23:42:16.331965 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/extra_mlp/wi_0/kernel/v_col
I0512 23:42:16.332003 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/extra_mlp/wi_0/kernel/v_row
I0512 23:42:16.332040 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/extra_mlp/wi_1/kernel/m
I0512 23:42:16.332073 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/extra_mlp/wi_1/kernel/v
I0512 23:42:16.332106 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/extra_mlp/wi_1/kernel/v_col
I0512 23:42:16.332143 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/extra_mlp/wi_1/kernel/v_row
I0512 23:42:16.332176 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/extra_mlp/wo/kernel/m
I0512 23:42:16.332209 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/extra_mlp/wo/kernel/v
I0512 23:42:16.332243 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/extra_mlp/wo/kernel/v_col
I0512 23:42:16.332281 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/extra_mlp/wo/kernel/v_row
I0512 23:42:16.332318 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/mlp/expert/wi_0/kernel/m
I0512 23:42:16.332354 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/mlp/expert/wi_0/kernel/v
I0512 23:42:16.332388 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/mlp/expert/wi_0/kernel/v_col
I0512 23:42:16.332424 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/mlp/expert/wi_0/kernel/v_row
I0512 23:42:16.332461 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/mlp/expert/wi_1/kernel/m
I0512 23:42:16.332497 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/mlp/expert/wi_1/kernel/v
I0512 23:42:16.332531 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/mlp/expert/wi_1/kernel/v_col
I0512 23:42:16.332568 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/mlp/expert/wi_1/kernel/v_row
I0512 23:42:16.332604 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/mlp/expert/wo/kernel/m
I0512 23:42:16.332638 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/mlp/expert/wo/kernel/v
I0512 23:42:16.332676 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/mlp/expert/wo/kernel/v_col
I0512 23:42:16.332713 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/mlp/expert/wo/kernel/v_row
I0512 23:42:16.332746 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/mlp/router/router_weights/w/kernel/m
I0512 23:42:16.332783 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/mlp/router/router_weights/w/kernel/v
I0512 23:42:16.332816 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/mlp/router/router_weights/w/kernel/v_col
I0512 23:42:16.332853 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/mlp/router/router_weights/w/kernel/v_row
I0512 23:42:16.332889 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/pre_extra_mlp_layer_norm/scale/m
I0512 23:42:16.332928 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/pre_extra_mlp_layer_norm/scale/v
I0512 23:42:16.332965 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/pre_extra_mlp_layer_norm/scale/v_col
I0512 23:42:16.332999 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/pre_extra_mlp_layer_norm/scale/v_row
I0512 23:42:16.333036 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/pre_mlp_layer_norm/scale/m
I0512 23:42:16.333072 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/pre_mlp_layer_norm/scale/v
I0512 23:42:16.333106 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/pre_mlp_layer_norm/scale/v_col
I0512 23:42:16.333142 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/pre_mlp_layer_norm/scale/v_row
I0512 23:42:16.333176 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/pre_self_attention_layer_norm/scale/m
I0512 23:42:16.333212 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/pre_self_attention_layer_norm/scale/v
I0512 23:42:16.333245 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/pre_self_attention_layer_norm/scale/v_col
I0512 23:42:16.333282 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/pre_self_attention_layer_norm/scale/v_row
I0512 23:42:16.333318 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/self_attention/key/kernel/m
I0512 23:42:16.333353 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/self_attention/key/kernel/v
I0512 23:42:16.333389 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/self_attention/key/kernel/v_col
I0512 23:42:16.333422 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/self_attention/key/kernel/v_row
I0512 23:42:16.333459 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/self_attention/out/kernel/m
I0512 23:42:16.333492 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/self_attention/out/kernel/v
I0512 23:42:16.333528 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/self_attention/out/kernel/v_col
I0512 23:42:16.333562 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/self_attention/out/kernel/v_row
I0512 23:42:16.333599 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/self_attention/query/kernel/m
I0512 23:42:16.333632 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/self_attention/query/kernel/v
I0512 23:42:16.333670 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/self_attention/query/kernel/v_col
I0512 23:42:16.333704 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/self_attention/query/kernel/v_row
I0512 23:42:16.333741 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/self_attention/value/kernel/m
I0512 23:42:16.333776 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/self_attention/value/kernel/v
I0512 23:42:16.333810 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/self_attention/value/kernel/v_col
I0512 23:42:16.333846 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/self_attention/value/kernel/v_row
I0512 23:42:16.333880 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_12/mlp/wi_0/kernel/m
I0512 23:42:16.333921 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_12/mlp/wi_0/kernel/v
I0512 23:42:16.333955 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_12/mlp/wi_0/kernel/v_col
I0512 23:42:16.333992 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_12/mlp/wi_0/kernel/v_row
I0512 23:42:16.334028 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_12/mlp/wi_1/kernel/m
I0512 23:42:16.334062 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_12/mlp/wi_1/kernel/v
I0512 23:42:16.334125 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_12/mlp/wi_1/kernel/v_col
I0512 23:42:16.334163 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_12/mlp/wi_1/kernel/v_row
I0512 23:42:16.334198 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_12/mlp/wo/kernel/m
I0512 23:42:16.334234 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_12/mlp/wo/kernel/v
I0512 23:42:16.334269 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_12/mlp/wo/kernel/v_col
I0512 23:42:16.334306 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_12/mlp/wo/kernel/v_row
I0512 23:42:16.334342 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_12/pre_mlp_layer_norm/scale/m
I0512 23:42:16.334377 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_12/pre_mlp_layer_norm/scale/v
I0512 23:42:16.334413 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_12/pre_mlp_layer_norm/scale/v_col
I0512 23:42:16.334447 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_12/pre_mlp_layer_norm/scale/v_row
I0512 23:42:16.334483 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_12/pre_self_attention_layer_norm/scale/m
I0512 23:42:16.334517 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_12/pre_self_attention_layer_norm/scale/v
I0512 23:42:16.334554 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_12/pre_self_attention_layer_norm/scale/v_col
I0512 23:42:16.334590 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_12/pre_self_attention_layer_norm/scale/v_row
I0512 23:42:16.334624 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_12/self_attention/key/kernel/m
I0512 23:42:16.334661 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_12/self_attention/key/kernel/v
I0512 23:42:16.334696 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_12/self_attention/key/kernel/v_col
I0512 23:42:16.334732 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_12/self_attention/key/kernel/v_row
I0512 23:42:16.334766 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_12/self_attention/out/kernel/m
I0512 23:42:16.334802 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_12/self_attention/out/kernel/v
I0512 23:42:16.334835 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_12/self_attention/out/kernel/v_col
I0512 23:42:16.334872 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_12/self_attention/out/kernel/v_row
I0512 23:42:16.334913 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_12/self_attention/query/kernel/m
I0512 23:42:16.334948 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_12/self_attention/query/kernel/v
I0512 23:42:16.334985 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_12/self_attention/query/kernel/v_col
I0512 23:42:16.335018 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_12/self_attention/query/kernel/v_row
I0512 23:42:16.335055 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_12/self_attention/value/kernel/m
I0512 23:42:16.335091 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_12/self_attention/value/kernel/v
I0512 23:42:16.335125 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_12/self_attention/value/kernel/v_col
I0512 23:42:16.335163 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_12/self_attention/value/kernel/v_row
I0512 23:42:16.335198 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_13/mlp/wi_0/kernel/m
I0512 23:42:16.335232 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_13/mlp/wi_0/kernel/v
I0512 23:42:16.335265 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_13/mlp/wi_0/kernel/v_col
I0512 23:42:16.335298 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_13/mlp/wi_0/kernel/v_row
I0512 23:42:16.335332 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_13/mlp/wi_1/kernel/m
I0512 23:42:16.335369 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_13/mlp/wi_1/kernel/v
I0512 23:42:16.335403 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_13/mlp/wi_1/kernel/v_col
I0512 23:42:16.335440 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_13/mlp/wi_1/kernel/v_row
I0512 23:42:16.335474 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_13/mlp/wo/kernel/m
I0512 23:42:16.335511 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_13/mlp/wo/kernel/v
I0512 23:42:16.335544 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_13/mlp/wo/kernel/v_col
I0512 23:42:16.335581 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_13/mlp/wo/kernel/v_row
I0512 23:42:16.335618 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_13/pre_mlp_layer_norm/scale/m
I0512 23:42:16.335653 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_13/pre_mlp_layer_norm/scale/v
I0512 23:42:16.335690 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_13/pre_mlp_layer_norm/scale/v_col
I0512 23:42:16.335725 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_13/pre_mlp_layer_norm/scale/v_row
I0512 23:42:16.335762 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_13/pre_self_attention_layer_norm/scale/m
I0512 23:42:16.335796 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_13/pre_self_attention_layer_norm/scale/v
I0512 23:42:16.335833 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_13/pre_self_attention_layer_norm/scale/v_col
I0512 23:42:16.335869 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_13/pre_self_attention_layer_norm/scale/v_row
I0512 23:42:16.335908 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_13/self_attention/key/kernel/m
I0512 23:42:16.335946 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_13/self_attention/key/kernel/v
I0512 23:42:16.335979 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_13/self_attention/key/kernel/v_col
I0512 23:42:16.336016 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_13/self_attention/key/kernel/v_row
I0512 23:42:16.336053 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_13/self_attention/out/kernel/m
I0512 23:42:16.336087 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_13/self_attention/out/kernel/v
I0512 23:42:16.336123 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_13/self_attention/out/kernel/v_col
I0512 23:42:16.336157 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_13/self_attention/out/kernel/v_row
I0512 23:42:16.336193 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_13/self_attention/query/kernel/m
I0512 23:42:16.336227 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_13/self_attention/query/kernel/v
I0512 23:42:16.336263 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_13/self_attention/query/kernel/v_col
I0512 23:42:16.336300 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_13/self_attention/query/kernel/v_row
I0512 23:42:16.336334 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_13/self_attention/value/kernel/m
I0512 23:42:16.336370 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_13/self_attention/value/kernel/v
I0512 23:42:16.336404 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_13/self_attention/value/kernel/v_col
I0512 23:42:16.336440 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_13/self_attention/value/kernel/v_row
I0512 23:42:16.336473 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_14/mlp/wi_0/kernel/m
I0512 23:42:16.336510 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_14/mlp/wi_0/kernel/v
I0512 23:42:16.336544 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_14/mlp/wi_0/kernel/v_col
I0512 23:42:16.336581 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_14/mlp/wi_0/kernel/v_row
I0512 23:42:16.336618 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_14/mlp/wi_1/kernel/m
I0512 23:42:16.336654 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_14/mlp/wi_1/kernel/v
I0512 23:42:16.336691 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_14/mlp/wi_1/kernel/v_col
I0512 23:42:16.336725 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_14/mlp/wi_1/kernel/v_row
I0512 23:42:16.336762 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_14/mlp/wo/kernel/m
I0512 23:42:16.336798 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_14/mlp/wo/kernel/v
I0512 23:42:16.336833 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_14/mlp/wo/kernel/v_col
I0512 23:42:16.336869 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_14/mlp/wo/kernel/v_row
I0512 23:42:16.336908 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_14/pre_mlp_layer_norm/scale/m
I0512 23:42:16.336945 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_14/pre_mlp_layer_norm/scale/v
I0512 23:42:16.336978 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_14/pre_mlp_layer_norm/scale/v_col
I0512 23:42:16.337016 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_14/pre_mlp_layer_norm/scale/v_row
I0512 23:42:16.337052 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_14/pre_self_attention_layer_norm/scale/m
I0512 23:42:16.337086 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_14/pre_self_attention_layer_norm/scale/v
I0512 23:42:16.337122 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_14/pre_self_attention_layer_norm/scale/v_col
I0512 23:42:16.337156 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_14/pre_self_attention_layer_norm/scale/v_row
I0512 23:42:16.337193 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_14/self_attention/key/kernel/m
I0512 23:42:16.337226 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_14/self_attention/key/kernel/v
I0512 23:42:16.337263 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_14/self_attention/key/kernel/v_col
I0512 23:42:16.337300 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_14/self_attention/key/kernel/v_row
I0512 23:42:16.337334 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_14/self_attention/out/kernel/m
I0512 23:42:16.337370 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_14/self_attention/out/kernel/v
I0512 23:42:16.337404 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_14/self_attention/out/kernel/v_col
I0512 23:42:16.337440 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_14/self_attention/out/kernel/v_row
I0512 23:42:16.337474 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_14/self_attention/query/kernel/m
I0512 23:42:16.337511 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_14/self_attention/query/kernel/v
I0512 23:42:16.337547 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_14/self_attention/query/kernel/v_col
I0512 23:42:16.337582 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_14/self_attention/query/kernel/v_row
I0512 23:42:16.337618 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_14/self_attention/value/kernel/m
I0512 23:42:16.337653 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_14/self_attention/value/kernel/v
I0512 23:42:16.337690 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_14/self_attention/value/kernel/v_col
I0512 23:42:16.337724 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_14/self_attention/value/kernel/v_row
I0512 23:42:16.337760 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_15/mlp/wi_0/kernel/m
I0512 23:42:16.337797 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_15/mlp/wi_0/kernel/v
I0512 23:42:16.337831 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_15/mlp/wi_0/kernel/v_col
I0512 23:42:16.337867 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_15/mlp/wi_0/kernel/v_row
I0512 23:42:16.337906 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_15/mlp/wi_1/kernel/m
I0512 23:42:16.337944 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_15/mlp/wi_1/kernel/v
I0512 23:42:16.337980 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_15/mlp/wi_1/kernel/v_col
I0512 23:42:16.338019 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_15/mlp/wi_1/kernel/v_row
I0512 23:42:16.338056 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_15/mlp/wo/kernel/m
I0512 23:42:16.338112 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_15/mlp/wo/kernel/v
I0512 23:42:16.338149 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_15/mlp/wo/kernel/v_col
I0512 23:42:16.338185 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_15/mlp/wo/kernel/v_row
I0512 23:42:16.338219 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_15/pre_mlp_layer_norm/scale/m
I0512 23:42:16.338251 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_15/pre_mlp_layer_norm/scale/v
I0512 23:42:16.338285 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_15/pre_mlp_layer_norm/scale/v_col
I0512 23:42:16.338317 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_15/pre_mlp_layer_norm/scale/v_row
I0512 23:42:16.338351 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_15/pre_self_attention_layer_norm/scale/m
I0512 23:42:16.338383 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_15/pre_self_attention_layer_norm/scale/v
I0512 23:42:16.338417 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_15/pre_self_attention_layer_norm/scale/v_col
I0512 23:42:16.338449 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_15/pre_self_attention_layer_norm/scale/v_row
I0512 23:42:16.338483 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_15/self_attention/key/kernel/m
I0512 23:42:16.338514 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_15/self_attention/key/kernel/v
I0512 23:42:16.338548 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_15/self_attention/key/kernel/v_col
I0512 23:42:16.338579 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_15/self_attention/key/kernel/v_row
I0512 23:42:16.338614 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_15/self_attention/out/kernel/m
I0512 23:42:16.338645 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_15/self_attention/out/kernel/v
I0512 23:42:16.338681 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_15/self_attention/out/kernel/v_col
I0512 23:42:16.338715 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_15/self_attention/out/kernel/v_row
I0512 23:42:16.338748 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_15/self_attention/query/kernel/m
I0512 23:42:16.338782 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_15/self_attention/query/kernel/v
I0512 23:42:16.338814 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_15/self_attention/query/kernel/v_col
I0512 23:42:16.338848 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_15/self_attention/query/kernel/v_row
I0512 23:42:16.338880 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_15/self_attention/value/kernel/m
I0512 23:42:16.338917 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_15/self_attention/value/kernel/v
I0512 23:42:16.338949 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_15/self_attention/value/kernel/v_col
I0512 23:42:16.338984 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_15/self_attention/value/kernel/v_row
I0512 23:42:16.339023 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_16/mlp/wi_0/kernel/m
I0512 23:42:16.339061 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_16/mlp/wi_0/kernel/v
I0512 23:42:16.339097 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_16/mlp/wi_0/kernel/v_col
I0512 23:42:16.339132 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_16/mlp/wi_0/kernel/v_row
I0512 23:42:16.339171 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_16/mlp/wi_1/kernel/m
I0512 23:42:16.339205 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_16/mlp/wi_1/kernel/v
I0512 23:42:16.339240 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_16/mlp/wi_1/kernel/v_col
I0512 23:42:16.339273 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_16/mlp/wi_1/kernel/v_row
I0512 23:42:16.339304 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_16/mlp/wo/kernel/m
I0512 23:42:16.339337 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_16/mlp/wo/kernel/v
I0512 23:42:16.339370 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_16/mlp/wo/kernel/v_col
I0512 23:42:16.339401 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_16/mlp/wo/kernel/v_row
I0512 23:42:16.339435 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_16/pre_mlp_layer_norm/scale/m
I0512 23:42:16.339468 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_16/pre_mlp_layer_norm/scale/v
I0512 23:42:16.339499 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_16/pre_mlp_layer_norm/scale/v_col
I0512 23:42:16.339532 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_16/pre_mlp_layer_norm/scale/v_row
I0512 23:42:16.339565 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_16/pre_self_attention_layer_norm/scale/m
I0512 23:42:16.339595 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_16/pre_self_attention_layer_norm/scale/v
I0512 23:42:16.339630 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_16/pre_self_attention_layer_norm/scale/v_col
I0512 23:42:16.339665 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_16/pre_self_attention_layer_norm/scale/v_row
I0512 23:42:16.339700 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_16/self_attention/key/kernel/m
I0512 23:42:16.339731 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_16/self_attention/key/kernel/v
I0512 23:42:16.339765 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_16/self_attention/key/kernel/v_col
I0512 23:42:16.339797 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_16/self_attention/key/kernel/v_row
I0512 23:42:16.339828 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_16/self_attention/out/kernel/m
I0512 23:42:16.339861 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_16/self_attention/out/kernel/v
I0512 23:42:16.339898 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_16/self_attention/out/kernel/v_col
I0512 23:42:16.339930 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_16/self_attention/out/kernel/v_row
I0512 23:42:16.339963 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_16/self_attention/query/kernel/m
I0512 23:42:16.339996 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_16/self_attention/query/kernel/v
I0512 23:42:16.340026 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_16/self_attention/query/kernel/v_col
I0512 23:42:16.340059 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_16/self_attention/query/kernel/v_row
I0512 23:42:16.340091 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_16/self_attention/value/kernel/m
I0512 23:42:16.340121 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_16/self_attention/value/kernel/v
I0512 23:42:16.340154 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_16/self_attention/value/kernel/v_col
I0512 23:42:16.340186 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_16/self_attention/value/kernel/v_row
I0512 23:42:16.340217 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/extra_mlp/wi_0/kernel/m
I0512 23:42:16.340249 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/extra_mlp/wi_0/kernel/v
I0512 23:42:16.340279 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/extra_mlp/wi_0/kernel/v_col
I0512 23:42:16.340313 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/extra_mlp/wi_0/kernel/v_row
I0512 23:42:16.340345 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/extra_mlp/wi_1/kernel/m
I0512 23:42:16.340376 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/extra_mlp/wi_1/kernel/v
I0512 23:42:16.340408 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/extra_mlp/wi_1/kernel/v_col
I0512 23:42:16.340440 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/extra_mlp/wi_1/kernel/v_row
I0512 23:42:16.340471 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/extra_mlp/wo/kernel/m
I0512 23:42:16.340505 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/extra_mlp/wo/kernel/v
I0512 23:42:16.340538 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/extra_mlp/wo/kernel/v_col
I0512 23:42:16.340568 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/extra_mlp/wo/kernel/v_row
I0512 23:42:16.340601 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/mlp/expert/wi_0/kernel/m
I0512 23:42:16.340634 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/mlp/expert/wi_0/kernel/v
I0512 23:42:16.340666 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/mlp/expert/wi_0/kernel/v_col
I0512 23:42:16.340700 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/mlp/expert/wi_0/kernel/v_row
I0512 23:42:16.340732 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/mlp/expert/wi_1/kernel/m
I0512 23:42:16.340763 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/mlp/expert/wi_1/kernel/v
I0512 23:42:16.340796 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/mlp/expert/wi_1/kernel/v_col
I0512 23:42:16.340828 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/mlp/expert/wi_1/kernel/v_row
I0512 23:42:16.340858 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/mlp/expert/wo/kernel/m
I0512 23:42:16.340891 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/mlp/expert/wo/kernel/v
I0512 23:42:16.340929 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/mlp/expert/wo/kernel/v_col
I0512 23:42:16.340960 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/mlp/expert/wo/kernel/v_row
I0512 23:42:16.340992 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/mlp/router/router_weights/w/kernel/m
I0512 23:42:16.341026 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/mlp/router/router_weights/w/kernel/v
I0512 23:42:16.341056 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/mlp/router/router_weights/w/kernel/v_col
I0512 23:42:16.341089 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/mlp/router/router_weights/w/kernel/v_row
I0512 23:42:16.341121 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/pre_extra_mlp_layer_norm/scale/m
I0512 23:42:16.341152 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/pre_extra_mlp_layer_norm/scale/v
I0512 23:42:16.341181 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/pre_extra_mlp_layer_norm/scale/v_col
I0512 23:42:16.341210 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/pre_extra_mlp_layer_norm/scale/v_row
I0512 23:42:16.341239 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/pre_mlp_layer_norm/scale/m
I0512 23:42:16.341269 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/pre_mlp_layer_norm/scale/v
I0512 23:42:16.341298 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/pre_mlp_layer_norm/scale/v_col
I0512 23:42:16.341327 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/pre_mlp_layer_norm/scale/v_row
I0512 23:42:16.341356 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/pre_self_attention_layer_norm/scale/m
I0512 23:42:16.341385 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/pre_self_attention_layer_norm/scale/v
I0512 23:42:16.341414 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/pre_self_attention_layer_norm/scale/v_col
I0512 23:42:16.341444 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/pre_self_attention_layer_norm/scale/v_row
I0512 23:42:16.341473 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/self_attention/key/kernel/m
I0512 23:42:16.341503 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/self_attention/key/kernel/v
I0512 23:42:16.341532 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/self_attention/key/kernel/v_col
I0512 23:42:16.341561 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/self_attention/key/kernel/v_row
I0512 23:42:16.341591 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/self_attention/out/kernel/m
I0512 23:42:16.341620 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/self_attention/out/kernel/v
I0512 23:42:16.341650 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/self_attention/out/kernel/v_col
I0512 23:42:16.341681 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/self_attention/out/kernel/v_row
I0512 23:42:16.341710 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/self_attention/query/kernel/m
I0512 23:42:16.341740 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/self_attention/query/kernel/v
I0512 23:42:16.341769 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/self_attention/query/kernel/v_col
I0512 23:42:16.341799 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/self_attention/query/kernel/v_row
I0512 23:42:16.341828 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/self_attention/value/kernel/m
I0512 23:42:16.341858 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/self_attention/value/kernel/v
I0512 23:42:16.341887 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/self_attention/value/kernel/v_col
I0512 23:42:16.341922 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/self_attention/value/kernel/v_row
I0512 23:42:16.341953 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_18/mlp/wi_0/kernel/m
I0512 23:42:16.341984 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_18/mlp/wi_0/kernel/v
I0512 23:42:16.342017 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_18/mlp/wi_0/kernel/v_col
I0512 23:42:16.342047 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_18/mlp/wi_0/kernel/v_row
I0512 23:42:16.342093 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_18/mlp/wi_1/kernel/m
I0512 23:42:16.342129 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_18/mlp/wi_1/kernel/v
I0512 23:42:16.342160 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_18/mlp/wi_1/kernel/v_col
I0512 23:42:16.342190 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_18/mlp/wi_1/kernel/v_row
I0512 23:42:16.342220 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_18/mlp/wo/kernel/m
I0512 23:42:16.342249 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_18/mlp/wo/kernel/v
I0512 23:42:16.342279 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_18/mlp/wo/kernel/v_col
I0512 23:42:16.342308 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_18/mlp/wo/kernel/v_row
I0512 23:42:16.342338 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_18/pre_mlp_layer_norm/scale/m
I0512 23:42:16.342368 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_18/pre_mlp_layer_norm/scale/v
I0512 23:42:16.342397 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_18/pre_mlp_layer_norm/scale/v_col
I0512 23:42:16.342427 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_18/pre_mlp_layer_norm/scale/v_row
I0512 23:42:16.342456 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_18/pre_self_attention_layer_norm/scale/m
I0512 23:42:16.342485 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_18/pre_self_attention_layer_norm/scale/v
I0512 23:42:16.342515 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_18/pre_self_attention_layer_norm/scale/v_col
I0512 23:42:16.342544 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_18/pre_self_attention_layer_norm/scale/v_row
I0512 23:42:16.342574 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_18/self_attention/key/kernel/m
I0512 23:42:16.342603 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_18/self_attention/key/kernel/v
I0512 23:42:16.342633 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_18/self_attention/key/kernel/v_col
I0512 23:42:16.342664 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_18/self_attention/key/kernel/v_row
I0512 23:42:16.342694 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_18/self_attention/out/kernel/m
I0512 23:42:16.342724 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_18/self_attention/out/kernel/v
I0512 23:42:16.342753 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_18/self_attention/out/kernel/v_col
I0512 23:42:16.342782 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_18/self_attention/out/kernel/v_row
I0512 23:42:16.342812 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_18/self_attention/query/kernel/m
I0512 23:42:16.342841 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_18/self_attention/query/kernel/v
I0512 23:42:16.342870 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_18/self_attention/query/kernel/v_col
I0512 23:42:16.342905 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_18/self_attention/query/kernel/v_row
I0512 23:42:16.342936 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_18/self_attention/value/kernel/m
I0512 23:42:16.342965 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_18/self_attention/value/kernel/v
I0512 23:42:16.342994 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_18/self_attention/value/kernel/v_col
I0512 23:42:16.343024 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_18/self_attention/value/kernel/v_row
I0512 23:42:16.343053 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_19/mlp/wi_0/kernel/m
I0512 23:42:16.343083 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_19/mlp/wi_0/kernel/v
I0512 23:42:16.343113 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_19/mlp/wi_0/kernel/v_col
I0512 23:42:16.343143 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_19/mlp/wi_0/kernel/v_row
I0512 23:42:16.343173 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_19/mlp/wi_1/kernel/m
I0512 23:42:16.343202 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_19/mlp/wi_1/kernel/v
I0512 23:42:16.343232 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_19/mlp/wi_1/kernel/v_col
I0512 23:42:16.343261 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_19/mlp/wi_1/kernel/v_row
I0512 23:42:16.343291 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_19/mlp/wo/kernel/m
I0512 23:42:16.343320 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_19/mlp/wo/kernel/v
I0512 23:42:16.343350 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_19/mlp/wo/kernel/v_col
I0512 23:42:16.343380 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_19/mlp/wo/kernel/v_row
I0512 23:42:16.343410 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_19/pre_mlp_layer_norm/scale/m
I0512 23:42:16.343439 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_19/pre_mlp_layer_norm/scale/v
I0512 23:42:16.343468 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_19/pre_mlp_layer_norm/scale/v_col
I0512 23:42:16.343498 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_19/pre_mlp_layer_norm/scale/v_row
I0512 23:42:16.343528 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_19/pre_self_attention_layer_norm/scale/m
I0512 23:42:16.343558 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_19/pre_self_attention_layer_norm/scale/v
I0512 23:42:16.343587 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_19/pre_self_attention_layer_norm/scale/v_col
I0512 23:42:16.343617 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_19/pre_self_attention_layer_norm/scale/v_row
I0512 23:42:16.343647 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_19/self_attention/key/kernel/m
I0512 23:42:16.343678 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_19/self_attention/key/kernel/v
I0512 23:42:16.343708 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_19/self_attention/key/kernel/v_col
I0512 23:42:16.343738 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_19/self_attention/key/kernel/v_row
I0512 23:42:16.343767 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_19/self_attention/out/kernel/m
I0512 23:42:16.343827 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_19/self_attention/out/kernel/v
I0512 23:42:16.343859 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_19/self_attention/out/kernel/v_col
I0512 23:42:16.343888 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_19/self_attention/out/kernel/v_row
I0512 23:42:16.343923 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_19/self_attention/query/kernel/m
I0512 23:42:16.343953 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_19/self_attention/query/kernel/v
I0512 23:42:16.343982 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_19/self_attention/query/kernel/v_col
I0512 23:42:16.344012 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_19/self_attention/query/kernel/v_row
I0512 23:42:16.344041 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_19/self_attention/value/kernel/m
I0512 23:42:16.344070 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_19/self_attention/value/kernel/v
I0512 23:42:16.344099 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_19/self_attention/value/kernel/v_col
I0512 23:42:16.344129 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_19/self_attention/value/kernel/v_row
I0512 23:42:16.344158 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_2/mlp/wi_0/kernel/m
I0512 23:42:16.344188 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_2/mlp/wi_0/kernel/v
I0512 23:42:16.344217 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_2/mlp/wi_0/kernel/v_col
I0512 23:42:16.344247 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_2/mlp/wi_0/kernel/v_row
I0512 23:42:16.344277 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_2/mlp/wi_1/kernel/m
I0512 23:42:16.344306 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_2/mlp/wi_1/kernel/v
I0512 23:42:16.344335 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_2/mlp/wi_1/kernel/v_col
I0512 23:42:16.344365 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_2/mlp/wi_1/kernel/v_row
I0512 23:42:16.344394 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_2/mlp/wo/kernel/m
I0512 23:42:16.344424 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_2/mlp/wo/kernel/v
I0512 23:42:16.344453 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_2/mlp/wo/kernel/v_col
I0512 23:42:16.344483 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_2/mlp/wo/kernel/v_row
I0512 23:42:16.344512 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_2/pre_mlp_layer_norm/scale/m
I0512 23:42:16.344542 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_2/pre_mlp_layer_norm/scale/v
I0512 23:42:16.344571 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_2/pre_mlp_layer_norm/scale/v_col
I0512 23:42:16.344600 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_2/pre_mlp_layer_norm/scale/v_row
I0512 23:42:16.344630 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_2/pre_self_attention_layer_norm/scale/m
I0512 23:42:16.344661 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_2/pre_self_attention_layer_norm/scale/v
I0512 23:42:16.344692 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_2/pre_self_attention_layer_norm/scale/v_col
I0512 23:42:16.344722 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_2/pre_self_attention_layer_norm/scale/v_row
I0512 23:42:16.344752 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_2/self_attention/key/kernel/m
I0512 23:42:16.344782 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_2/self_attention/key/kernel/v
I0512 23:42:16.344812 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_2/self_attention/key/kernel/v_col
I0512 23:42:16.344841 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_2/self_attention/key/kernel/v_row
I0512 23:42:16.344870 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_2/self_attention/out/kernel/m
I0512 23:42:16.344904 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_2/self_attention/out/kernel/v
I0512 23:42:16.344935 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_2/self_attention/out/kernel/v_col
I0512 23:42:16.344965 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_2/self_attention/out/kernel/v_row
I0512 23:42:16.344997 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_2/self_attention/query/kernel/m
I0512 23:42:16.345028 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_2/self_attention/query/kernel/v
I0512 23:42:16.345057 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_2/self_attention/query/kernel/v_col
I0512 23:42:16.345086 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_2/self_attention/query/kernel/v_row
I0512 23:42:16.345115 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_2/self_attention/value/kernel/m
I0512 23:42:16.345144 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_2/self_attention/value/kernel/v
I0512 23:42:16.345174 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_2/self_attention/value/kernel/v_col
I0512 23:42:16.345203 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_2/self_attention/value/kernel/v_row
I0512 23:42:16.345232 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_20/mlp/wi_0/kernel/m
I0512 23:42:16.345262 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_20/mlp/wi_0/kernel/v
I0512 23:42:16.345291 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_20/mlp/wi_0/kernel/v_col
I0512 23:42:16.345321 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_20/mlp/wi_0/kernel/v_row
I0512 23:42:16.345350 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_20/mlp/wi_1/kernel/m
I0512 23:42:16.345380 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_20/mlp/wi_1/kernel/v
I0512 23:42:16.345410 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_20/mlp/wi_1/kernel/v_col
I0512 23:42:16.345440 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_20/mlp/wi_1/kernel/v_row
I0512 23:42:16.345470 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_20/mlp/wo/kernel/m
I0512 23:42:16.345499 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_20/mlp/wo/kernel/v
I0512 23:42:16.345529 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_20/mlp/wo/kernel/v_col
I0512 23:42:16.345558 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_20/mlp/wo/kernel/v_row
I0512 23:42:16.345588 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_20/pre_mlp_layer_norm/scale/m
I0512 23:42:16.345618 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_20/pre_mlp_layer_norm/scale/v
I0512 23:42:16.345648 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_20/pre_mlp_layer_norm/scale/v_col
I0512 23:42:16.345679 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_20/pre_mlp_layer_norm/scale/v_row
I0512 23:42:16.345709 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_20/pre_self_attention_layer_norm/scale/m
I0512 23:42:16.345739 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_20/pre_self_attention_layer_norm/scale/v
I0512 23:42:16.345772 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_20/pre_self_attention_layer_norm/scale/v_col
I0512 23:42:16.345802 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_20/pre_self_attention_layer_norm/scale/v_row
I0512 23:42:16.345831 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_20/self_attention/key/kernel/m
I0512 23:42:16.345861 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_20/self_attention/key/kernel/v
I0512 23:42:16.345890 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_20/self_attention/key/kernel/v_col
I0512 23:42:16.345926 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_20/self_attention/key/kernel/v_row
I0512 23:42:16.345956 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_20/self_attention/out/kernel/m
I0512 23:42:16.345985 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_20/self_attention/out/kernel/v
I0512 23:42:16.346015 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_20/self_attention/out/kernel/v_col
I0512 23:42:16.346044 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_20/self_attention/out/kernel/v_row
I0512 23:42:16.346074 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_20/self_attention/query/kernel/m
I0512 23:42:16.346125 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_20/self_attention/query/kernel/v
I0512 23:42:16.346157 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_20/self_attention/query/kernel/v_col
I0512 23:42:16.346187 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_20/self_attention/query/kernel/v_row
I0512 23:42:16.346216 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_20/self_attention/value/kernel/m
I0512 23:42:16.346246 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_20/self_attention/value/kernel/v
I0512 23:42:16.346275 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_20/self_attention/value/kernel/v_col
I0512 23:42:16.346304 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_20/self_attention/value/kernel/v_row
I0512 23:42:16.346334 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_21/mlp/wi_0/kernel/m
I0512 23:42:16.346364 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_21/mlp/wi_0/kernel/v
I0512 23:42:16.346394 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_21/mlp/wi_0/kernel/v_col
I0512 23:42:16.346427 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_21/mlp/wi_0/kernel/v_row
I0512 23:42:16.346458 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_21/mlp/wi_1/kernel/m
I0512 23:42:16.346487 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_21/mlp/wi_1/kernel/v
I0512 23:42:16.346517 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_21/mlp/wi_1/kernel/v_col
I0512 23:42:16.346547 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_21/mlp/wi_1/kernel/v_row
I0512 23:42:16.346576 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_21/mlp/wo/kernel/m
I0512 23:42:16.346606 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_21/mlp/wo/kernel/v
I0512 23:42:16.346636 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_21/mlp/wo/kernel/v_col
I0512 23:42:16.346667 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_21/mlp/wo/kernel/v_row
I0512 23:42:16.346698 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_21/pre_mlp_layer_norm/scale/m
I0512 23:42:16.346729 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_21/pre_mlp_layer_norm/scale/v
I0512 23:42:16.346758 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_21/pre_mlp_layer_norm/scale/v_col
I0512 23:42:16.346788 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_21/pre_mlp_layer_norm/scale/v_row
I0512 23:42:16.346817 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_21/pre_self_attention_layer_norm/scale/m
I0512 23:42:16.346847 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_21/pre_self_attention_layer_norm/scale/v
I0512 23:42:16.346877 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_21/pre_self_attention_layer_norm/scale/v_col
I0512 23:42:16.346912 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_21/pre_self_attention_layer_norm/scale/v_row
I0512 23:42:16.346943 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_21/self_attention/key/kernel/m
I0512 23:42:16.346976 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_21/self_attention/key/kernel/v
I0512 23:42:16.347006 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_21/self_attention/key/kernel/v_col
I0512 23:42:16.347036 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_21/self_attention/key/kernel/v_row
I0512 23:42:16.347065 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_21/self_attention/out/kernel/m
I0512 23:42:16.347095 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_21/self_attention/out/kernel/v
I0512 23:42:16.347124 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_21/self_attention/out/kernel/v_col
I0512 23:42:16.347153 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_21/self_attention/out/kernel/v_row
I0512 23:42:16.347182 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_21/self_attention/query/kernel/m
I0512 23:42:16.347212 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_21/self_attention/query/kernel/v
I0512 23:42:16.347241 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_21/self_attention/query/kernel/v_col
I0512 23:42:16.347271 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_21/self_attention/query/kernel/v_row
I0512 23:42:16.347301 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_21/self_attention/value/kernel/m
I0512 23:42:16.347330 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_21/self_attention/value/kernel/v
I0512 23:42:16.347359 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_21/self_attention/value/kernel/v_col
I0512 23:42:16.347388 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_21/self_attention/value/kernel/v_row
I0512 23:42:16.347417 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_22/mlp/wi_0/kernel/m
I0512 23:42:16.347447 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_22/mlp/wi_0/kernel/v
I0512 23:42:16.347476 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_22/mlp/wi_0/kernel/v_col
I0512 23:42:16.347506 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_22/mlp/wi_0/kernel/v_row
I0512 23:42:16.347536 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_22/mlp/wi_1/kernel/m
I0512 23:42:16.347566 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_22/mlp/wi_1/kernel/v
I0512 23:42:16.347600 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_22/mlp/wi_1/kernel/v_col
I0512 23:42:16.347630 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_22/mlp/wi_1/kernel/v_row
I0512 23:42:16.347661 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_22/mlp/wo/kernel/m
I0512 23:42:16.347692 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_22/mlp/wo/kernel/v
I0512 23:42:16.347722 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_22/mlp/wo/kernel/v_col
I0512 23:42:16.347751 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_22/mlp/wo/kernel/v_row
I0512 23:42:16.347781 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_22/pre_mlp_layer_norm/scale/m
I0512 23:42:16.347811 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_22/pre_mlp_layer_norm/scale/v
I0512 23:42:16.347841 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_22/pre_mlp_layer_norm/scale/v_col
I0512 23:42:16.347870 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_22/pre_mlp_layer_norm/scale/v_row
I0512 23:42:16.347906 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_22/pre_self_attention_layer_norm/scale/m
I0512 23:42:16.347937 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_22/pre_self_attention_layer_norm/scale/v
I0512 23:42:16.347966 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_22/pre_self_attention_layer_norm/scale/v_col
I0512 23:42:16.347996 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_22/pre_self_attention_layer_norm/scale/v_row
I0512 23:42:16.348025 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_22/self_attention/key/kernel/m
I0512 23:42:16.348055 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_22/self_attention/key/kernel/v
I0512 23:42:16.348084 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_22/self_attention/key/kernel/v_col
I0512 23:42:16.348114 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_22/self_attention/key/kernel/v_row
I0512 23:42:16.348143 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_22/self_attention/out/kernel/m
I0512 23:42:16.348172 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_22/self_attention/out/kernel/v
I0512 23:42:16.348201 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_22/self_attention/out/kernel/v_col
I0512 23:42:16.348230 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_22/self_attention/out/kernel/v_row
I0512 23:42:16.348263 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_22/self_attention/query/kernel/m
I0512 23:42:16.348293 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_22/self_attention/query/kernel/v
I0512 23:42:16.348322 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_22/self_attention/query/kernel/v_col
I0512 23:42:16.348352 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_22/self_attention/query/kernel/v_row
I0512 23:42:16.348381 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_22/self_attention/value/kernel/m
I0512 23:42:16.348410 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_22/self_attention/value/kernel/v
I0512 23:42:16.348440 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_22/self_attention/value/kernel/v_col
I0512 23:42:16.348469 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_22/self_attention/value/kernel/v_row
I0512 23:42:16.348499 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/extra_mlp/wi_0/kernel/m
I0512 23:42:16.348528 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/extra_mlp/wi_0/kernel/v
I0512 23:42:16.348556 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/extra_mlp/wi_0/kernel/v_col
I0512 23:42:16.348587 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/extra_mlp/wi_0/kernel/v_row
I0512 23:42:16.348617 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/extra_mlp/wi_1/kernel/m
I0512 23:42:16.348646 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/extra_mlp/wi_1/kernel/v
I0512 23:42:16.348677 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/extra_mlp/wi_1/kernel/v_col
I0512 23:42:16.348707 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/extra_mlp/wi_1/kernel/v_row
I0512 23:42:16.348736 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/extra_mlp/wo/kernel/m
I0512 23:42:16.348766 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/extra_mlp/wo/kernel/v
I0512 23:42:16.348796 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/extra_mlp/wo/kernel/v_col
I0512 23:42:16.348825 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/extra_mlp/wo/kernel/v_row
I0512 23:42:16.348854 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/mlp/expert/wi_0/kernel/m
I0512 23:42:16.348888 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/mlp/expert/wi_0/kernel/v
I0512 23:42:16.348923 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/mlp/expert/wi_0/kernel/v_col
I0512 23:42:16.348953 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/mlp/expert/wi_0/kernel/v_row
I0512 23:42:16.348983 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/mlp/expert/wi_1/kernel/m
I0512 23:42:16.349017 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/mlp/expert/wi_1/kernel/v
I0512 23:42:16.349046 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/mlp/expert/wi_1/kernel/v_col
I0512 23:42:16.349076 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/mlp/expert/wi_1/kernel/v_row
I0512 23:42:16.349105 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/mlp/expert/wo/kernel/m
I0512 23:42:16.349135 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/mlp/expert/wo/kernel/v
I0512 23:42:16.349164 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/mlp/expert/wo/kernel/v_col
I0512 23:42:16.349195 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/mlp/expert/wo/kernel/v_row
I0512 23:42:16.349225 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/mlp/router/router_weights/w/kernel/m
I0512 23:42:16.349254 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/mlp/router/router_weights/w/kernel/v
I0512 23:42:16.349283 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/mlp/router/router_weights/w/kernel/v_col
I0512 23:42:16.349313 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/mlp/router/router_weights/w/kernel/v_row
I0512 23:42:16.349342 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/pre_extra_mlp_layer_norm/scale/m
I0512 23:42:16.349371 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/pre_extra_mlp_layer_norm/scale/v
I0512 23:42:16.349400 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/pre_extra_mlp_layer_norm/scale/v_col
I0512 23:42:16.349430 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/pre_extra_mlp_layer_norm/scale/v_row
I0512 23:42:16.349459 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/pre_mlp_layer_norm/scale/m
I0512 23:42:16.349488 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/pre_mlp_layer_norm/scale/v
I0512 23:42:16.349517 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/pre_mlp_layer_norm/scale/v_col
I0512 23:42:16.349550 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/pre_mlp_layer_norm/scale/v_row
I0512 23:42:16.349580 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/pre_self_attention_layer_norm/scale/m
I0512 23:42:16.349610 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/pre_self_attention_layer_norm/scale/v
I0512 23:42:16.349639 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/pre_self_attention_layer_norm/scale/v_col
I0512 23:42:16.349671 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/pre_self_attention_layer_norm/scale/v_row
I0512 23:42:16.349701 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/self_attention/key/kernel/m
I0512 23:42:16.349730 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/self_attention/key/kernel/v
I0512 23:42:16.349760 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/self_attention/key/kernel/v_col
I0512 23:42:16.349789 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/self_attention/key/kernel/v_row
I0512 23:42:16.349819 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/self_attention/out/kernel/m
I0512 23:42:16.349848 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/self_attention/out/kernel/v
I0512 23:42:16.349879 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/self_attention/out/kernel/v_col
I0512 23:42:16.349916 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/self_attention/out/kernel/v_row
I0512 23:42:16.349947 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/self_attention/query/kernel/m
I0512 23:42:16.349977 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/self_attention/query/kernel/v
I0512 23:42:16.350006 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/self_attention/query/kernel/v_col
I0512 23:42:16.350035 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/self_attention/query/kernel/v_row
I0512 23:42:16.350064 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/self_attention/value/kernel/m
I0512 23:42:16.350114 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/self_attention/value/kernel/v
I0512 23:42:16.350146 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/self_attention/value/kernel/v_col
I0512 23:42:16.350176 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/self_attention/value/kernel/v_row
I0512 23:42:16.350209 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_3/mlp/wi_0/kernel/m
I0512 23:42:16.350239 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_3/mlp/wi_0/kernel/v
I0512 23:42:16.350269 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_3/mlp/wi_0/kernel/v_col
I0512 23:42:16.350299 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_3/mlp/wi_0/kernel/v_row
I0512 23:42:16.350329 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_3/mlp/wi_1/kernel/m
I0512 23:42:16.350358 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_3/mlp/wi_1/kernel/v
I0512 23:42:16.350388 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_3/mlp/wi_1/kernel/v_col
I0512 23:42:16.350418 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_3/mlp/wi_1/kernel/v_row
I0512 23:42:16.350447 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_3/mlp/wo/kernel/m
I0512 23:42:16.350477 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_3/mlp/wo/kernel/v
I0512 23:42:16.350507 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_3/mlp/wo/kernel/v_col
I0512 23:42:16.350538 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_3/mlp/wo/kernel/v_row
I0512 23:42:16.350568 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_3/pre_mlp_layer_norm/scale/m
I0512 23:42:16.350597 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_3/pre_mlp_layer_norm/scale/v
I0512 23:42:16.350627 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_3/pre_mlp_layer_norm/scale/v_col
I0512 23:42:16.350657 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_3/pre_mlp_layer_norm/scale/v_row
I0512 23:42:16.350688 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_3/pre_self_attention_layer_norm/scale/m
I0512 23:42:16.350718 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_3/pre_self_attention_layer_norm/scale/v
I0512 23:42:16.350748 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_3/pre_self_attention_layer_norm/scale/v_col
I0512 23:42:16.350777 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_3/pre_self_attention_layer_norm/scale/v_row
I0512 23:42:16.350810 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_3/self_attention/key/kernel/m
I0512 23:42:16.350840 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_3/self_attention/key/kernel/v
I0512 23:42:16.350869 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_3/self_attention/key/kernel/v_col
I0512 23:42:16.350903 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_3/self_attention/key/kernel/v_row
I0512 23:42:16.350934 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_3/self_attention/out/kernel/m
I0512 23:42:16.350964 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_3/self_attention/out/kernel/v
I0512 23:42:16.350993 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_3/self_attention/out/kernel/v_col
I0512 23:42:16.351023 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_3/self_attention/out/kernel/v_row
I0512 23:42:16.351052 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_3/self_attention/query/kernel/m
I0512 23:42:16.351081 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_3/self_attention/query/kernel/v
I0512 23:42:16.351111 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_3/self_attention/query/kernel/v_col
I0512 23:42:16.351140 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_3/self_attention/query/kernel/v_row
I0512 23:42:16.351170 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_3/self_attention/value/kernel/m
I0512 23:42:16.351199 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_3/self_attention/value/kernel/v
I0512 23:42:16.351228 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_3/self_attention/value/kernel/v_col
I0512 23:42:16.351257 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_3/self_attention/value/kernel/v_row
I0512 23:42:16.351287 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_4/mlp/wi_0/kernel/m
I0512 23:42:16.351316 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_4/mlp/wi_0/kernel/v
I0512 23:42:16.351346 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_4/mlp/wi_0/kernel/v_col
I0512 23:42:16.351375 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_4/mlp/wi_0/kernel/v_row
I0512 23:42:16.351408 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_4/mlp/wi_1/kernel/m
I0512 23:42:16.351438 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_4/mlp/wi_1/kernel/v
I0512 23:42:16.351467 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_4/mlp/wi_1/kernel/v_col
I0512 23:42:16.351496 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_4/mlp/wi_1/kernel/v_row
I0512 23:42:16.351526 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_4/mlp/wo/kernel/m
I0512 23:42:16.351555 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_4/mlp/wo/kernel/v
I0512 23:42:16.351585 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_4/mlp/wo/kernel/v_col
I0512 23:42:16.351614 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_4/mlp/wo/kernel/v_row
I0512 23:42:16.351644 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_4/pre_mlp_layer_norm/scale/m
I0512 23:42:16.351676 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_4/pre_mlp_layer_norm/scale/v
I0512 23:42:16.351706 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_4/pre_mlp_layer_norm/scale/v_col
I0512 23:42:16.351735 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_4/pre_mlp_layer_norm/scale/v_row
I0512 23:42:16.351764 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_4/pre_self_attention_layer_norm/scale/m
I0512 23:42:16.351794 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_4/pre_self_attention_layer_norm/scale/v
I0512 23:42:16.351823 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_4/pre_self_attention_layer_norm/scale/v_col
I0512 23:42:16.351852 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_4/pre_self_attention_layer_norm/scale/v_row
I0512 23:42:16.351881 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_4/self_attention/key/kernel/m
I0512 23:42:16.351916 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_4/self_attention/key/kernel/v
I0512 23:42:16.351947 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_4/self_attention/key/kernel/v_col
I0512 23:42:16.351976 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_4/self_attention/key/kernel/v_row
I0512 23:42:16.352005 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_4/self_attention/out/kernel/m
I0512 23:42:16.352034 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_4/self_attention/out/kernel/v
I0512 23:42:16.352063 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_4/self_attention/out/kernel/v_col
I0512 23:42:16.352092 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_4/self_attention/out/kernel/v_row
I0512 23:42:16.352122 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_4/self_attention/query/kernel/m
I0512 23:42:16.352151 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_4/self_attention/query/kernel/v
I0512 23:42:16.352180 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_4/self_attention/query/kernel/v_col
I0512 23:42:16.352210 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_4/self_attention/query/kernel/v_row
I0512 23:42:16.352239 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_4/self_attention/value/kernel/m
I0512 23:42:16.352268 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_4/self_attention/value/kernel/v
I0512 23:42:16.352297 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_4/self_attention/value/kernel/v_col
I0512 23:42:16.352326 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_4/self_attention/value/kernel/v_row
I0512 23:42:16.352355 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/extra_mlp/wi_0/kernel/m
I0512 23:42:16.352385 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/extra_mlp/wi_0/kernel/v
I0512 23:42:16.352414 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/extra_mlp/wi_0/kernel/v_col
I0512 23:42:16.352443 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/extra_mlp/wi_0/kernel/v_row
I0512 23:42:16.352472 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/extra_mlp/wi_1/kernel/m
I0512 23:42:16.352502 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/extra_mlp/wi_1/kernel/v
I0512 23:42:16.352531 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/extra_mlp/wi_1/kernel/v_col
I0512 23:42:16.352560 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/extra_mlp/wi_1/kernel/v_row
I0512 23:42:16.352590 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/extra_mlp/wo/kernel/m
I0512 23:42:16.352619 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/extra_mlp/wo/kernel/v
I0512 23:42:16.352649 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/extra_mlp/wo/kernel/v_col
I0512 23:42:16.352681 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/extra_mlp/wo/kernel/v_row
I0512 23:42:16.352710 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/mlp/expert/wi_0/kernel/m
I0512 23:42:16.352740 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/mlp/expert/wi_0/kernel/v
I0512 23:42:16.352769 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/mlp/expert/wi_0/kernel/v_col
I0512 23:42:16.352798 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/mlp/expert/wi_0/kernel/v_row
I0512 23:42:16.352828 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/mlp/expert/wi_1/kernel/m
I0512 23:42:16.352857 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/mlp/expert/wi_1/kernel/v
I0512 23:42:16.352887 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/mlp/expert/wi_1/kernel/v_col
I0512 23:42:16.352922 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/mlp/expert/wi_1/kernel/v_row
I0512 23:42:16.352951 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/mlp/expert/wo/kernel/m
I0512 23:42:16.352981 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/mlp/expert/wo/kernel/v
I0512 23:42:16.353011 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/mlp/expert/wo/kernel/v_col
I0512 23:42:16.353040 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/mlp/expert/wo/kernel/v_row
I0512 23:42:16.353069 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/mlp/router/router_weights/w/kernel/m
I0512 23:42:16.353099 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/mlp/router/router_weights/w/kernel/v
I0512 23:42:16.353128 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/mlp/router/router_weights/w/kernel/v_col
I0512 23:42:16.353157 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/mlp/router/router_weights/w/kernel/v_row
I0512 23:42:16.353187 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/pre_extra_mlp_layer_norm/scale/m
I0512 23:42:16.353216 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/pre_extra_mlp_layer_norm/scale/v
I0512 23:42:16.353245 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/pre_extra_mlp_layer_norm/scale/v_col
I0512 23:42:16.353274 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/pre_extra_mlp_layer_norm/scale/v_row
I0512 23:42:16.353303 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/pre_mlp_layer_norm/scale/m
I0512 23:42:16.353332 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/pre_mlp_layer_norm/scale/v
I0512 23:42:16.353361 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/pre_mlp_layer_norm/scale/v_col
I0512 23:42:16.353390 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/pre_mlp_layer_norm/scale/v_row
I0512 23:42:16.353420 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/pre_self_attention_layer_norm/scale/m
I0512 23:42:16.353449 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/pre_self_attention_layer_norm/scale/v
I0512 23:42:16.353478 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/pre_self_attention_layer_norm/scale/v_col
I0512 23:42:16.353507 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/pre_self_attention_layer_norm/scale/v_row
I0512 23:42:16.353536 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/self_attention/key/kernel/m
I0512 23:42:16.353565 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/self_attention/key/kernel/v
I0512 23:42:16.353594 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/self_attention/key/kernel/v_col
I0512 23:42:16.353623 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/self_attention/key/kernel/v_row
I0512 23:42:16.353653 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/self_attention/out/kernel/m
I0512 23:42:16.353684 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/self_attention/out/kernel/v
I0512 23:42:16.353714 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/self_attention/out/kernel/v_col
I0512 23:42:16.353743 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/self_attention/out/kernel/v_row
I0512 23:42:16.353772 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/self_attention/query/kernel/m
I0512 23:42:16.353801 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/self_attention/query/kernel/v
I0512 23:42:16.353830 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/self_attention/query/kernel/v_col
I0512 23:42:16.353859 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/self_attention/query/kernel/v_row
I0512 23:42:16.353888 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/self_attention/value/kernel/m
I0512 23:42:16.353923 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/self_attention/value/kernel/v
I0512 23:42:16.353953 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/self_attention/value/kernel/v_col
I0512 23:42:16.353984 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/self_attention/value/kernel/v_row
I0512 23:42:16.354017 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_6/mlp/wi_0/kernel/m
I0512 23:42:16.354046 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_6/mlp/wi_0/kernel/v
I0512 23:42:16.354075 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_6/mlp/wi_0/kernel/v_col
I0512 23:42:16.354133 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_6/mlp/wi_0/kernel/v_row
I0512 23:42:16.354165 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_6/mlp/wi_1/kernel/m
I0512 23:42:16.354194 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_6/mlp/wi_1/kernel/v
I0512 23:42:16.354224 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_6/mlp/wi_1/kernel/v_col
I0512 23:42:16.354254 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_6/mlp/wi_1/kernel/v_row
I0512 23:42:16.354283 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_6/mlp/wo/kernel/m
I0512 23:42:16.354313 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_6/mlp/wo/kernel/v
I0512 23:42:16.354342 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_6/mlp/wo/kernel/v_col
I0512 23:42:16.354372 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_6/mlp/wo/kernel/v_row
I0512 23:42:16.354402 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_6/pre_mlp_layer_norm/scale/m
I0512 23:42:16.354431 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_6/pre_mlp_layer_norm/scale/v
I0512 23:42:16.354460 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_6/pre_mlp_layer_norm/scale/v_col
I0512 23:42:16.354490 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_6/pre_mlp_layer_norm/scale/v_row
I0512 23:42:16.354519 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_6/pre_self_attention_layer_norm/scale/m
I0512 23:42:16.354548 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_6/pre_self_attention_layer_norm/scale/v
I0512 23:42:16.354578 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_6/pre_self_attention_layer_norm/scale/v_col
I0512 23:42:16.354607 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_6/pre_self_attention_layer_norm/scale/v_row
I0512 23:42:16.354636 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_6/self_attention/key/kernel/m
I0512 23:42:16.354667 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_6/self_attention/key/kernel/v
I0512 23:42:16.354698 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_6/self_attention/key/kernel/v_col
I0512 23:42:16.354727 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_6/self_attention/key/kernel/v_row
I0512 23:42:16.354757 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_6/self_attention/out/kernel/m
I0512 23:42:16.354786 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_6/self_attention/out/kernel/v
I0512 23:42:16.354815 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_6/self_attention/out/kernel/v_col
I0512 23:42:16.354845 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_6/self_attention/out/kernel/v_row
I0512 23:42:16.354874 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_6/self_attention/query/kernel/m
I0512 23:42:16.354909 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_6/self_attention/query/kernel/v
I0512 23:42:16.354940 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_6/self_attention/query/kernel/v_col
I0512 23:42:16.354969 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_6/self_attention/query/kernel/v_row
I0512 23:42:16.354998 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_6/self_attention/value/kernel/m
I0512 23:42:16.355027 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_6/self_attention/value/kernel/v
I0512 23:42:16.355057 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_6/self_attention/value/kernel/v_col
I0512 23:42:16.355086 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_6/self_attention/value/kernel/v_row
I0512 23:42:16.355115 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_7/mlp/wi_0/kernel/m
I0512 23:42:16.355144 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_7/mlp/wi_0/kernel/v
I0512 23:42:16.355174 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_7/mlp/wi_0/kernel/v_col
I0512 23:42:16.355203 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_7/mlp/wi_0/kernel/v_row
I0512 23:42:16.355233 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_7/mlp/wi_1/kernel/m
I0512 23:42:16.355262 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_7/mlp/wi_1/kernel/v
I0512 23:42:16.355292 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_7/mlp/wi_1/kernel/v_col
I0512 23:42:16.355321 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_7/mlp/wi_1/kernel/v_row
I0512 23:42:16.355351 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_7/mlp/wo/kernel/m
I0512 23:42:16.355380 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_7/mlp/wo/kernel/v
I0512 23:42:16.355410 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_7/mlp/wo/kernel/v_col
I0512 23:42:16.355439 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_7/mlp/wo/kernel/v_row
I0512 23:42:16.355468 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_7/pre_mlp_layer_norm/scale/m
I0512 23:42:16.355498 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_7/pre_mlp_layer_norm/scale/v
I0512 23:42:16.355527 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_7/pre_mlp_layer_norm/scale/v_col
I0512 23:42:16.355557 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_7/pre_mlp_layer_norm/scale/v_row
I0512 23:42:16.355586 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_7/pre_self_attention_layer_norm/scale/m
I0512 23:42:16.355616 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_7/pre_self_attention_layer_norm/scale/v
I0512 23:42:16.355645 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_7/pre_self_attention_layer_norm/scale/v_col
I0512 23:42:16.355676 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_7/pre_self_attention_layer_norm/scale/v_row
I0512 23:42:16.355706 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_7/self_attention/key/kernel/m
I0512 23:42:16.355736 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_7/self_attention/key/kernel/v
I0512 23:42:16.355765 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_7/self_attention/key/kernel/v_col
I0512 23:42:16.355794 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_7/self_attention/key/kernel/v_row
I0512 23:42:16.355823 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_7/self_attention/out/kernel/m
I0512 23:42:16.355853 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_7/self_attention/out/kernel/v
I0512 23:42:16.355882 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_7/self_attention/out/kernel/v_col
I0512 23:42:16.355917 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_7/self_attention/out/kernel/v_row
I0512 23:42:16.355947 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_7/self_attention/query/kernel/m
I0512 23:42:16.355977 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_7/self_attention/query/kernel/v
I0512 23:42:16.356006 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_7/self_attention/query/kernel/v_col
I0512 23:42:16.356035 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_7/self_attention/query/kernel/v_row
I0512 23:42:16.356065 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_7/self_attention/value/kernel/m
I0512 23:42:16.356093 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_7/self_attention/value/kernel/v
I0512 23:42:16.356123 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_7/self_attention/value/kernel/v_col
I0512 23:42:16.356152 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_7/self_attention/value/kernel/v_row
I0512 23:42:16.356181 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_8/mlp/wi_0/kernel/m
I0512 23:42:16.356210 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_8/mlp/wi_0/kernel/v
I0512 23:42:16.356239 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_8/mlp/wi_0/kernel/v_col
I0512 23:42:16.356269 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_8/mlp/wi_0/kernel/v_row
I0512 23:42:16.356298 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_8/mlp/wi_1/kernel/m
I0512 23:42:16.356328 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_8/mlp/wi_1/kernel/v
I0512 23:42:16.356357 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_8/mlp/wi_1/kernel/v_col
I0512 23:42:16.356387 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_8/mlp/wi_1/kernel/v_row
I0512 23:42:16.356416 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_8/mlp/wo/kernel/m
I0512 23:42:16.356446 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_8/mlp/wo/kernel/v
I0512 23:42:16.356475 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_8/mlp/wo/kernel/v_col
I0512 23:42:16.356505 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_8/mlp/wo/kernel/v_row
I0512 23:42:16.356534 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_8/pre_mlp_layer_norm/scale/m
I0512 23:42:16.356564 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_8/pre_mlp_layer_norm/scale/v
I0512 23:42:16.356593 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_8/pre_mlp_layer_norm/scale/v_col
I0512 23:42:16.356623 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_8/pre_mlp_layer_norm/scale/v_row
I0512 23:42:16.356653 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_8/pre_self_attention_layer_norm/scale/m
I0512 23:42:16.356684 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_8/pre_self_attention_layer_norm/scale/v
I0512 23:42:16.356714 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_8/pre_self_attention_layer_norm/scale/v_col
I0512 23:42:16.356744 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_8/pre_self_attention_layer_norm/scale/v_row
I0512 23:42:16.356773 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_8/self_attention/key/kernel/m
I0512 23:42:16.356802 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_8/self_attention/key/kernel/v
I0512 23:42:16.356831 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_8/self_attention/key/kernel/v_col
I0512 23:42:16.356860 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_8/self_attention/key/kernel/v_row
I0512 23:42:16.356890 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_8/self_attention/out/kernel/m
I0512 23:42:16.356925 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_8/self_attention/out/kernel/v
I0512 23:42:16.356955 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_8/self_attention/out/kernel/v_col
I0512 23:42:16.356984 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_8/self_attention/out/kernel/v_row
I0512 23:42:16.357013 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_8/self_attention/query/kernel/m
I0512 23:42:16.357043 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_8/self_attention/query/kernel/v
I0512 23:42:16.357072 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_8/self_attention/query/kernel/v_col
I0512 23:42:16.357102 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_8/self_attention/query/kernel/v_row
I0512 23:42:16.357131 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_8/self_attention/value/kernel/m
I0512 23:42:16.357160 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_8/self_attention/value/kernel/v
I0512 23:42:16.357189 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_8/self_attention/value/kernel/v_col
I0512 23:42:16.357218 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_8/self_attention/value/kernel/v_row
I0512 23:42:16.357248 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_9/mlp/wi_0/kernel/m
I0512 23:42:16.357277 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_9/mlp/wi_0/kernel/v
I0512 23:42:16.357306 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_9/mlp/wi_0/kernel/v_col
I0512 23:42:16.357336 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_9/mlp/wi_0/kernel/v_row
I0512 23:42:16.357365 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_9/mlp/wi_1/kernel/m
I0512 23:42:16.357395 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_9/mlp/wi_1/kernel/v
I0512 23:42:16.357424 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_9/mlp/wi_1/kernel/v_col
I0512 23:42:16.357454 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_9/mlp/wi_1/kernel/v_row
I0512 23:42:16.357484 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_9/mlp/wo/kernel/m
I0512 23:42:16.357513 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_9/mlp/wo/kernel/v
I0512 23:42:16.357543 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_9/mlp/wo/kernel/v_col
I0512 23:42:16.357572 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_9/mlp/wo/kernel/v_row
I0512 23:42:16.357602 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_9/pre_mlp_layer_norm/scale/m
I0512 23:42:16.357631 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_9/pre_mlp_layer_norm/scale/v
I0512 23:42:16.357662 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_9/pre_mlp_layer_norm/scale/v_col
I0512 23:42:16.357693 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_9/pre_mlp_layer_norm/scale/v_row
I0512 23:42:16.357723 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_9/pre_self_attention_layer_norm/scale/m
I0512 23:42:16.357753 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_9/pre_self_attention_layer_norm/scale/v
I0512 23:42:16.357782 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_9/pre_self_attention_layer_norm/scale/v_col
I0512 23:42:16.357811 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_9/pre_self_attention_layer_norm/scale/v_row
I0512 23:42:16.357841 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_9/self_attention/key/kernel/m
I0512 23:42:16.357870 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_9/self_attention/key/kernel/v
I0512 23:42:16.357904 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_9/self_attention/key/kernel/v_col
I0512 23:42:16.357934 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_9/self_attention/key/kernel/v_row
I0512 23:42:16.357964 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_9/self_attention/out/kernel/m
I0512 23:42:16.357993 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_9/self_attention/out/kernel/v
I0512 23:42:16.358022 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_9/self_attention/out/kernel/v_col
I0512 23:42:16.358051 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_9/self_attention/out/kernel/v_row
I0512 23:42:16.358100 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_9/self_attention/query/kernel/m
I0512 23:42:16.358136 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_9/self_attention/query/kernel/v
I0512 23:42:16.358166 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_9/self_attention/query/kernel/v_col
I0512 23:42:16.358196 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_9/self_attention/query/kernel/v_row
I0512 23:42:16.358225 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_9/self_attention/value/kernel/m
I0512 23:42:16.358254 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_9/self_attention/value/kernel/v
I0512 23:42:16.358283 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_9/self_attention/value/kernel/v_col
I0512 23:42:16.358312 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_9/self_attention/value/kernel/v_row
I0512 23:42:16.358341 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/logits_dense/kernel/m
I0512 23:42:16.358371 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/logits_dense/kernel/v
I0512 23:42:16.358400 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/logits_dense/kernel/v_col
I0512 23:42:16.358430 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/logits_dense/kernel/v_row
I0512 23:42:16.358459 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/token_embedder/embedding/m
I0512 23:42:16.358489 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/token_embedder/embedding/v
I0512 23:42:16.358518 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/token_embedder/embedding/v_col
I0512 23:42:16.358548 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/param_states/token_embedder/embedding/v_row
I0512 23:42:16.358577 140031916288000 checkpoints.py:1100] Restoring key from ckpt: state/step
I0512 23:42:16.358607 140031916288000 checkpoints.py:1100] Restoring key from ckpt: target/decoder/decoder_norm/scale
I0512 23:42:16.358637 140031916288000 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_0/mlp/wi_0/kernel
I0512 23:42:16.358669 140031916288000 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_0/mlp/wi_1/kernel
I0512 23:42:16.358700 140031916288000 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_0/mlp/wo/kernel
I0512 23:42:16.358730 140031916288000 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_0/pre_mlp_layer_norm/scale
I0512 23:42:16.358759 140031916288000 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_0/pre_self_attention_layer_norm/scale
I0512 23:42:16.358789 140031916288000 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_0/self_attention/key/kernel
I0512 23:42:16.358819 140031916288000 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_0/self_attention/out/kernel
I0512 23:42:16.358848 140031916288000 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_0/self_attention/query/kernel
I0512 23:42:16.358878 140031916288000 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_0/self_attention/value/kernel
I0512 23:42:16.358913 140031916288000 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_1/mlp/wi_0/kernel
I0512 23:42:16.358944 140031916288000 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_1/mlp/wi_1/kernel
I0512 23:42:16.358973 140031916288000 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_1/mlp/wo/kernel
I0512 23:42:16.359003 140031916288000 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_1/pre_mlp_layer_norm/scale
I0512 23:42:16.359033 140031916288000 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_1/pre_self_attention_layer_norm/scale
I0512 23:42:16.359062 140031916288000 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_1/self_attention/key/kernel
I0512 23:42:16.359092 140031916288000 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_1/self_attention/out/kernel
I0512 23:42:16.359122 140031916288000 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_1/self_attention/query/kernel
I0512 23:42:16.359151 140031916288000 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_1/self_attention/value/kernel
I0512 23:42:16.359181 140031916288000 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_10/mlp/wi_0/kernel
I0512 23:42:16.359210 140031916288000 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_10/mlp/wi_1/kernel
I0512 23:42:16.359240 140031916288000 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_10/mlp/wo/kernel
I0512 23:42:16.359269 140031916288000 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_10/pre_mlp_layer_norm/scale
I0512 23:42:16.359299 140031916288000 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_10/pre_self_attention_layer_norm/scale
I0512 23:42:16.359329 140031916288000 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_10/self_attention/key/kernel
I0512 23:42:16.359359 140031916288000 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_10/self_attention/out/kernel
I0512 23:42:16.359388 140031916288000 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_10/self_attention/query/kernel
I0512 23:42:16.359418 140031916288000 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_10/self_attention/value/kernel
I0512 23:42:16.359447 140031916288000 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_11/extra_mlp/wi_0/kernel
I0512 23:42:16.359477 140031916288000 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_11/extra_mlp/wi_1/kernel
I0512 23:42:16.359506 140031916288000 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_11/extra_mlp/wo/kernel
I0512 23:42:16.359536 140031916288000 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_11/mlp/expert/wi_0/kernel
I0512 23:42:16.359565 140031916288000 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_11/mlp/expert/wi_1/kernel
I0512 23:42:16.359595 140031916288000 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_11/mlp/expert/wo/kernel
I0512 23:42:16.359625 140031916288000 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_11/mlp/router/router_weights/w/kernel
I0512 23:42:16.359656 140031916288000 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_11/pre_extra_mlp_layer_norm/scale
I0512 23:42:16.359687 140031916288000 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_11/pre_mlp_layer_norm/scale
I0512 23:42:16.359717 140031916288000 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_11/pre_self_attention_layer_norm/scale
I0512 23:42:16.359747 140031916288000 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_11/self_attention/key/kernel
I0512 23:42:16.359776 140031916288000 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_11/self_attention/out/kernel
I0512 23:42:16.359806 140031916288000 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_11/self_attention/query/kernel
I0512 23:42:16.359836 140031916288000 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_11/self_attention/value/kernel
I0512 23:42:16.359865 140031916288000 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_12/mlp/wi_0/kernel
I0512 23:42:16.359899 140031916288000 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_12/mlp/wi_1/kernel
I0512 23:42:16.359930 140031916288000 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_12/mlp/wo/kernel
I0512 23:42:16.359960 140031916288000 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_12/pre_mlp_layer_norm/scale
I0512 23:42:16.359990 140031916288000 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_12/pre_self_attention_layer_norm/scale
I0512 23:42:16.360019 140031916288000 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_12/self_attention/key/kernel
I0512 23:42:16.360049 140031916288000 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_12/self_attention/out/kernel
I0512 23:42:16.360078 140031916288000 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_12/self_attention/query/kernel
I0512 23:42:16.360108 140031916288000 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_12/self_attention/value/kernel
I0512 23:42:16.360138 140031916288000 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_13/mlp/wi_0/kernel
I0512 23:42:16.360167 140031916288000 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_13/mlp/wi_1/kernel
I0512 23:42:16.360197 140031916288000 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_13/mlp/wo/kernel
I0512 23:42:16.360227 140031916288000 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_13/pre_mlp_layer_norm/scale
I0512 23:42:16.360256 140031916288000 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_13/pre_self_attention_layer_norm/scale
I0512 23:42:16.360286 140031916288000 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_13/self_attention/key/kernel
I0512 23:42:16.360315 140031916288000 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_13/self_attention/out/kernel
I0512 23:42:16.360345 140031916288000 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_13/self_attention/query/kernel
I0512 23:42:16.360374 140031916288000 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_13/self_attention/value/kernel
I0512 23:42:16.360404 140031916288000 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_14/mlp/wi_0/kernel
I0512 23:42:16.360433 140031916288000 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_14/mlp/wi_1/kernel
I0512 23:42:16.360463 140031916288000 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_14/mlp/wo/kernel
I0512 23:42:16.360492 140031916288000 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_14/pre_mlp_layer_norm/scale
I0512 23:42:16.360522 140031916288000 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_14/pre_self_attention_layer_norm/scale
I0512 23:42:16.360551 140031916288000 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_14/self_attention/key/kernel
I0512 23:42:16.360581 140031916288000 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_14/self_attention/out/kernel
I0512 23:42:16.360610 140031916288000 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_14/self_attention/query/kernel
I0512 23:42:16.360640 140031916288000 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_14/self_attention/value/kernel
I0512 23:42:16.360672 140031916288000 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_15/mlp/wi_0/kernel
I0512 23:42:16.360702 140031916288000 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_15/mlp/wi_1/kernel
I0512 23:42:16.360732 140031916288000 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_15/mlp/wo/kernel
I0512 23:42:16.360762 140031916288000 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_15/pre_mlp_layer_norm/scale
I0512 23:42:16.360791 140031916288000 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_15/pre_self_attention_layer_norm/scale
I0512 23:42:16.360821 140031916288000 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_15/self_attention/key/kernel
I0512 23:42:16.360851 140031916288000 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_15/self_attention/out/kernel
I0512 23:42:16.360880 140031916288000 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_15/self_attention/query/kernel
I0512 23:42:16.360915 140031916288000 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_15/self_attention/value/kernel
I0512 23:42:16.360946 140031916288000 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_16/mlp/wi_0/kernel
I0512 23:42:16.360976 140031916288000 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_16/mlp/wi_1/kernel
I0512 23:42:16.361005 140031916288000 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_16/mlp/wo/kernel
I0512 23:42:16.361035 140031916288000 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_16/pre_mlp_layer_norm/scale
I0512 23:42:16.361065 140031916288000 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_16/pre_self_attention_layer_norm/scale
I0512 23:42:16.361094 140031916288000 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_16/self_attention/key/kernel
I0512 23:42:16.361124 140031916288000 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_16/self_attention/out/kernel
I0512 23:42:16.361154 140031916288000 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_16/self_attention/query/kernel
I0512 23:42:16.361183 140031916288000 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_16/self_attention/value/kernel
I0512 23:42:16.361213 140031916288000 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_17/extra_mlp/wi_0/kernel
I0512 23:42:16.361242 140031916288000 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_17/extra_mlp/wi_1/kernel
I0512 23:42:16.361272 140031916288000 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_17/extra_mlp/wo/kernel
I0512 23:42:16.361302 140031916288000 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_17/mlp/expert/wi_0/kernel
I0512 23:42:16.361331 140031916288000 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_17/mlp/expert/wi_1/kernel
I0512 23:42:16.361361 140031916288000 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_17/mlp/expert/wo/kernel
I0512 23:42:16.361390 140031916288000 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_17/mlp/router/router_weights/w/kernel
I0512 23:42:16.361420 140031916288000 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_17/pre_extra_mlp_layer_norm/scale
I0512 23:42:16.361449 140031916288000 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_17/pre_mlp_layer_norm/scale
I0512 23:42:16.361479 140031916288000 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_17/pre_self_attention_layer_norm/scale
I0512 23:42:16.361509 140031916288000 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_17/self_attention/key/kernel
I0512 23:42:16.361538 140031916288000 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_17/self_attention/out/kernel
I0512 23:42:16.361568 140031916288000 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_17/self_attention/query/kernel
I0512 23:42:16.361598 140031916288000 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_17/self_attention/value/kernel
I0512 23:42:16.361627 140031916288000 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_18/mlp/wi_0/kernel
I0512 23:42:16.361658 140031916288000 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_18/mlp/wi_1/kernel
I0512 23:42:16.361689 140031916288000 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_18/mlp/wo/kernel
I0512 23:42:16.361719 140031916288000 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_18/pre_mlp_layer_norm/scale
I0512 23:42:16.361749 140031916288000 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_18/pre_self_attention_layer_norm/scale
I0512 23:42:16.361778 140031916288000 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_18/self_attention/key/kernel
I0512 23:42:16.361808 140031916288000 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_18/self_attention/out/kernel
I0512 23:42:16.361838 140031916288000 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_18/self_attention/query/kernel
I0512 23:42:16.361867 140031916288000 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_18/self_attention/value/kernel
I0512 23:42:16.361901 140031916288000 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_19/mlp/wi_0/kernel
I0512 23:42:16.361933 140031916288000 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_19/mlp/wi_1/kernel
I0512 23:42:16.361963 140031916288000 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_19/mlp/wo/kernel
I0512 23:42:16.361993 140031916288000 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_19/pre_mlp_layer_norm/scale
I0512 23:42:16.362022 140031916288000 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_19/pre_self_attention_layer_norm/scale
I0512 23:42:16.362052 140031916288000 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_19/self_attention/key/kernel
I0512 23:42:16.362101 140031916288000 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_19/self_attention/out/kernel
I0512 23:42:16.362136 140031916288000 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_19/self_attention/query/kernel
I0512 23:42:16.362167 140031916288000 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_19/self_attention/value/kernel
I0512 23:42:16.362196 140031916288000 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_2/mlp/wi_0/kernel
I0512 23:42:16.362226 140031916288000 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_2/mlp/wi_1/kernel
I0512 23:42:16.362256 140031916288000 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_2/mlp/wo/kernel
I0512 23:42:16.362286 140031916288000 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_2/pre_mlp_layer_norm/scale
I0512 23:42:16.362316 140031916288000 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_2/pre_self_attention_layer_norm/scale
I0512 23:42:16.362345 140031916288000 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_2/self_attention/key/kernel
I0512 23:42:16.362375 140031916288000 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_2/self_attention/out/kernel
I0512 23:42:16.362405 140031916288000 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_2/self_attention/query/kernel
I0512 23:42:16.362434 140031916288000 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_2/self_attention/value/kernel
I0512 23:42:16.362463 140031916288000 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_20/mlp/wi_0/kernel
I0512 23:42:16.362493 140031916288000 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_20/mlp/wi_1/kernel
I0512 23:42:16.362522 140031916288000 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_20/mlp/wo/kernel
I0512 23:42:16.362552 140031916288000 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_20/pre_mlp_layer_norm/scale
I0512 23:42:16.362582 140031916288000 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_20/pre_self_attention_layer_norm/scale
I0512 23:42:16.362611 140031916288000 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_20/self_attention/key/kernel
I0512 23:42:16.362640 140031916288000 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_20/self_attention/out/kernel
I0512 23:42:16.362673 140031916288000 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_20/self_attention/query/kernel
I0512 23:42:16.362703 140031916288000 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_20/self_attention/value/kernel
I0512 23:42:16.362733 140031916288000 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_21/mlp/wi_0/kernel
I0512 23:42:16.362790 140031916288000 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_21/mlp/wi_1/kernel
I0512 23:42:16.362821 140031916288000 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_21/mlp/wo/kernel
I0512 23:42:16.362851 140031916288000 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_21/pre_mlp_layer_norm/scale
I0512 23:42:16.362881 140031916288000 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_21/pre_self_attention_layer_norm/scale
I0512 23:42:16.362916 140031916288000 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_21/self_attention/key/kernel
I0512 23:42:16.362946 140031916288000 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_21/self_attention/out/kernel
I0512 23:42:16.362977 140031916288000 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_21/self_attention/query/kernel
I0512 23:42:16.363006 140031916288000 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_21/self_attention/value/kernel
I0512 23:42:16.363036 140031916288000 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_22/mlp/wi_0/kernel
I0512 23:42:16.363066 140031916288000 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_22/mlp/wi_1/kernel
I0512 23:42:16.363095 140031916288000 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_22/mlp/wo/kernel
I0512 23:42:16.363125 140031916288000 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_22/pre_mlp_layer_norm/scale
I0512 23:42:16.363154 140031916288000 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_22/pre_self_attention_layer_norm/scale
I0512 23:42:16.363183 140031916288000 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_22/self_attention/key/kernel
I0512 23:42:16.363213 140031916288000 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_22/self_attention/out/kernel
I0512 23:42:16.363243 140031916288000 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_22/self_attention/query/kernel
I0512 23:42:16.363272 140031916288000 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_22/self_attention/value/kernel
I0512 23:42:16.363302 140031916288000 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_23/extra_mlp/wi_0/kernel
I0512 23:42:16.363331 140031916288000 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_23/extra_mlp/wi_1/kernel
I0512 23:42:16.363361 140031916288000 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_23/extra_mlp/wo/kernel
I0512 23:42:16.363390 140031916288000 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_23/mlp/expert/wi_0/kernel
I0512 23:42:16.363420 140031916288000 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_23/mlp/expert/wi_1/kernel
I0512 23:42:16.363450 140031916288000 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_23/mlp/expert/wo/kernel
I0512 23:42:16.363479 140031916288000 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_23/mlp/router/router_weights/w/kernel
I0512 23:42:16.363508 140031916288000 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_23/pre_extra_mlp_layer_norm/scale
I0512 23:42:16.363538 140031916288000 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_23/pre_mlp_layer_norm/scale
I0512 23:42:16.363568 140031916288000 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_23/pre_self_attention_layer_norm/scale
I0512 23:42:16.363598 140031916288000 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_23/self_attention/key/kernel
I0512 23:42:16.363627 140031916288000 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_23/self_attention/out/kernel
I0512 23:42:16.363658 140031916288000 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_23/self_attention/query/kernel
I0512 23:42:16.363690 140031916288000 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_23/self_attention/value/kernel
I0512 23:42:16.363720 140031916288000 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_3/mlp/wi_0/kernel
I0512 23:42:16.363750 140031916288000 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_3/mlp/wi_1/kernel
I0512 23:42:16.363805 140031916288000 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_3/mlp/wo/kernel
I0512 23:42:16.363838 140031916288000 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_3/pre_mlp_layer_norm/scale
I0512 23:42:16.363869 140031916288000 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_3/pre_self_attention_layer_norm/scale
I0512 23:42:16.363903 140031916288000 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_3/self_attention/key/kernel
I0512 23:42:16.363934 140031916288000 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_3/self_attention/out/kernel
I0512 23:42:16.363964 140031916288000 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_3/self_attention/query/kernel
I0512 23:42:16.363994 140031916288000 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_3/self_attention/value/kernel
I0512 23:42:16.364024 140031916288000 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_4/mlp/wi_0/kernel
I0512 23:42:16.364053 140031916288000 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_4/mlp/wi_1/kernel
I0512 23:42:16.364083 140031916288000 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_4/mlp/wo/kernel
I0512 23:42:16.364112 140031916288000 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_4/pre_mlp_layer_norm/scale
I0512 23:42:16.364142 140031916288000 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_4/pre_self_attention_layer_norm/scale
I0512 23:42:16.364171 140031916288000 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_4/self_attention/key/kernel
I0512 23:42:16.364201 140031916288000 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_4/self_attention/out/kernel
I0512 23:42:16.364230 140031916288000 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_4/self_attention/query/kernel
I0512 23:42:16.364260 140031916288000 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_4/self_attention/value/kernel
I0512 23:42:16.364289 140031916288000 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_5/extra_mlp/wi_0/kernel
I0512 23:42:16.364320 140031916288000 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_5/extra_mlp/wi_1/kernel
I0512 23:42:16.364349 140031916288000 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_5/extra_mlp/wo/kernel
I0512 23:42:16.364379 140031916288000 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_5/mlp/expert/wi_0/kernel
I0512 23:42:16.364408 140031916288000 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_5/mlp/expert/wi_1/kernel
I0512 23:42:16.364438 140031916288000 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_5/mlp/expert/wo/kernel
I0512 23:42:16.364467 140031916288000 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_5/mlp/router/router_weights/w/kernel
I0512 23:42:16.364497 140031916288000 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_5/pre_extra_mlp_layer_norm/scale
I0512 23:42:16.364526 140031916288000 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_5/pre_mlp_layer_norm/scale
I0512 23:42:16.364556 140031916288000 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_5/pre_self_attention_layer_norm/scale
I0512 23:42:16.364586 140031916288000 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_5/self_attention/key/kernel
I0512 23:42:16.364615 140031916288000 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_5/self_attention/out/kernel
I0512 23:42:16.364645 140031916288000 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_5/self_attention/query/kernel
I0512 23:42:16.364677 140031916288000 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_5/self_attention/value/kernel
I0512 23:42:16.364707 140031916288000 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_6/mlp/wi_0/kernel
I0512 23:42:16.364737 140031916288000 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_6/mlp/wi_1/kernel
I0512 23:42:16.364766 140031916288000 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_6/mlp/wo/kernel
I0512 23:42:16.364796 140031916288000 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_6/pre_mlp_layer_norm/scale
I0512 23:42:16.364825 140031916288000 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_6/pre_self_attention_layer_norm/scale
I0512 23:42:16.364855 140031916288000 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_6/self_attention/key/kernel
I0512 23:42:16.364884 140031916288000 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_6/self_attention/out/kernel
I0512 23:42:16.364919 140031916288000 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_6/self_attention/query/kernel
I0512 23:42:16.364949 140031916288000 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_6/self_attention/value/kernel
I0512 23:42:16.364979 140031916288000 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_7/mlp/wi_0/kernel
I0512 23:42:16.365009 140031916288000 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_7/mlp/wi_1/kernel
I0512 23:42:16.365038 140031916288000 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_7/mlp/wo/kernel
I0512 23:42:16.365067 140031916288000 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_7/pre_mlp_layer_norm/scale
I0512 23:42:16.365097 140031916288000 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_7/pre_self_attention_layer_norm/scale
I0512 23:42:16.365126 140031916288000 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_7/self_attention/key/kernel
I0512 23:42:16.365156 140031916288000 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_7/self_attention/out/kernel
I0512 23:42:16.365185 140031916288000 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_7/self_attention/query/kernel
I0512 23:42:16.365215 140031916288000 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_7/self_attention/value/kernel
I0512 23:42:16.365244 140031916288000 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_8/mlp/wi_0/kernel
I0512 23:42:16.365274 140031916288000 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_8/mlp/wi_1/kernel
I0512 23:42:16.365303 140031916288000 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_8/mlp/wo/kernel
I0512 23:42:16.365332 140031916288000 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_8/pre_mlp_layer_norm/scale
I0512 23:42:16.365362 140031916288000 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_8/pre_self_attention_layer_norm/scale
I0512 23:42:16.365391 140031916288000 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_8/self_attention/key/kernel
I0512 23:42:16.365421 140031916288000 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_8/self_attention/out/kernel
I0512 23:42:16.365451 140031916288000 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_8/self_attention/query/kernel
I0512 23:42:16.365480 140031916288000 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_8/self_attention/value/kernel
I0512 23:42:16.365510 140031916288000 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_9/mlp/wi_0/kernel
I0512 23:42:16.365539 140031916288000 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_9/mlp/wi_1/kernel
I0512 23:42:16.365569 140031916288000 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_9/mlp/wo/kernel
I0512 23:42:16.365598 140031916288000 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_9/pre_mlp_layer_norm/scale
I0512 23:42:16.365628 140031916288000 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_9/pre_self_attention_layer_norm/scale
I0512 23:42:16.365658 140031916288000 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_9/self_attention/key/kernel
I0512 23:42:16.365689 140031916288000 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_9/self_attention/out/kernel
I0512 23:42:16.365719 140031916288000 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_9/self_attention/query/kernel
I0512 23:42:16.365749 140031916288000 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_9/self_attention/value/kernel
I0512 23:42:16.365778 140031916288000 checkpoints.py:1100] Restoring key from ckpt: target/decoder/logits_dense/kernel
I0512 23:42:16.365808 140031916288000 checkpoints.py:1100] Restoring key from ckpt: target/token_embedder/embedding
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
I0000 00:00:1715557336.395338  453630 gcs_resource.cc:109] Using default AdmissionQueue with limit 32
I0512 23:42:16.381900 140529434355712 checkpoints.py:1054] Restoring from checkpoint: gs://rosinality-tpu-bucket/openmoe_8b/training/checkpoint_32500/checkpoint
I0000 00:00:1715557336.403854  455795 google_auth_provider.cc:180] Running on GCE, using service account 1081612546678-compute@developer.gserviceaccount.com
I0512 23:42:16.406538 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/decoder_norm/scale/m
I0512 23:42:16.406703 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/decoder_norm/scale/v
I0512 23:42:16.406745 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/decoder_norm/scale/v_col
I0512 23:42:16.406780 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/decoder_norm/scale/v_row
I0512 23:42:16.406817 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_0/mlp/wi_0/kernel/m
I0512 23:42:16.406852 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_0/mlp/wi_0/kernel/v
I0512 23:42:16.406885 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_0/mlp/wi_0/kernel/v_col
I0512 23:42:16.406920 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_0/mlp/wi_0/kernel/v_row
I0512 23:42:16.406954 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_0/mlp/wi_1/kernel/m
I0512 23:42:16.406987 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_0/mlp/wi_1/kernel/v
I0512 23:42:16.407021 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_0/mlp/wi_1/kernel/v_col
I0512 23:42:16.407055 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_0/mlp/wi_1/kernel/v_row
I0512 23:42:16.407089 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_0/mlp/wo/kernel/m
I0512 23:42:16.407122 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_0/mlp/wo/kernel/v
I0512 23:42:16.407156 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_0/mlp/wo/kernel/v_col
I0512 23:42:16.407193 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_0/mlp/wo/kernel/v_row
I0512 23:42:16.407228 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_0/pre_mlp_layer_norm/scale/m
I0512 23:42:16.407262 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_0/pre_mlp_layer_norm/scale/v
I0512 23:42:16.407295 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_0/pre_mlp_layer_norm/scale/v_col
I0512 23:42:16.407329 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_0/pre_mlp_layer_norm/scale/v_row
I0512 23:42:16.407362 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_0/pre_self_attention_layer_norm/scale/m
I0512 23:42:16.407396 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_0/pre_self_attention_layer_norm/scale/v
I0512 23:42:16.407439 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_0/pre_self_attention_layer_norm/scale/v_col
I0512 23:42:16.407475 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_0/pre_self_attention_layer_norm/scale/v_row
I0512 23:42:16.407509 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_0/self_attention/key/kernel/m
I0512 23:42:16.407543 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_0/self_attention/key/kernel/v
I0512 23:42:16.407576 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_0/self_attention/key/kernel/v_col
I0512 23:42:16.407610 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_0/self_attention/key/kernel/v_row
I0512 23:42:16.407644 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_0/self_attention/out/kernel/m
I0512 23:42:16.407677 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_0/self_attention/out/kernel/v
I0512 23:42:16.407710 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_0/self_attention/out/kernel/v_col
I0512 23:42:16.407744 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_0/self_attention/out/kernel/v_row
I0512 23:42:16.407778 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_0/self_attention/query/kernel/m
I0512 23:42:16.407811 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_0/self_attention/query/kernel/v
I0512 23:42:16.407844 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_0/self_attention/query/kernel/v_col
I0512 23:42:16.407878 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_0/self_attention/query/kernel/v_row
I0512 23:42:16.407911 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_0/self_attention/value/kernel/m
I0512 23:42:16.407944 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_0/self_attention/value/kernel/v
I0512 23:42:16.407978 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_0/self_attention/value/kernel/v_col
I0512 23:42:16.408011 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_0/self_attention/value/kernel/v_row
I0512 23:42:16.408045 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_1/mlp/wi_0/kernel/m
I0512 23:42:16.408079 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_1/mlp/wi_0/kernel/v
I0512 23:42:16.408113 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_1/mlp/wi_0/kernel/v_col
I0512 23:42:16.408146 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_1/mlp/wi_0/kernel/v_row
I0512 23:42:16.408180 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_1/mlp/wi_1/kernel/m
I0512 23:42:16.408216 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_1/mlp/wi_1/kernel/v
I0512 23:42:16.408250 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_1/mlp/wi_1/kernel/v_col
I0512 23:42:16.408284 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_1/mlp/wi_1/kernel/v_row
I0512 23:42:16.408317 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_1/mlp/wo/kernel/m
I0512 23:42:16.408351 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_1/mlp/wo/kernel/v
I0512 23:42:16.408384 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_1/mlp/wo/kernel/v_col
I0512 23:42:16.408418 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_1/mlp/wo/kernel/v_row
I0512 23:42:16.408458 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_1/pre_mlp_layer_norm/scale/m
I0512 23:42:16.408492 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_1/pre_mlp_layer_norm/scale/v
I0512 23:42:16.408526 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_1/pre_mlp_layer_norm/scale/v_col
I0512 23:42:16.408559 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_1/pre_mlp_layer_norm/scale/v_row
I0512 23:42:16.408592 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_1/pre_self_attention_layer_norm/scale/m
I0512 23:42:16.408626 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_1/pre_self_attention_layer_norm/scale/v
I0512 23:42:16.408659 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_1/pre_self_attention_layer_norm/scale/v_col
I0512 23:42:16.408693 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_1/pre_self_attention_layer_norm/scale/v_row
I0512 23:42:16.408726 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_1/self_attention/key/kernel/m
I0512 23:42:16.408760 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_1/self_attention/key/kernel/v
I0512 23:42:16.408794 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_1/self_attention/key/kernel/v_col
I0512 23:42:16.408827 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_1/self_attention/key/kernel/v_row
I0512 23:42:16.408860 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_1/self_attention/out/kernel/m
I0512 23:42:16.408894 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_1/self_attention/out/kernel/v
I0512 23:42:16.408927 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_1/self_attention/out/kernel/v_col
I0512 23:42:16.408981 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_1/self_attention/out/kernel/v_row
I0512 23:42:16.409019 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_1/self_attention/query/kernel/m
I0512 23:42:16.409052 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_1/self_attention/query/kernel/v
I0512 23:42:16.409086 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_1/self_attention/query/kernel/v_col
I0512 23:42:16.409119 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_1/self_attention/query/kernel/v_row
I0512 23:42:16.409153 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_1/self_attention/value/kernel/m
I0512 23:42:16.409188 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_1/self_attention/value/kernel/v
I0512 23:42:16.409223 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_1/self_attention/value/kernel/v_col
I0512 23:42:16.409256 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_1/self_attention/value/kernel/v_row
I0512 23:42:16.409290 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_10/mlp/wi_0/kernel/m
I0512 23:42:16.409323 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_10/mlp/wi_0/kernel/v
I0512 23:42:16.409357 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_10/mlp/wi_0/kernel/v_col
I0512 23:42:16.409391 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_10/mlp/wi_0/kernel/v_row
I0512 23:42:16.409429 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_10/mlp/wi_1/kernel/m
I0512 23:42:16.409465 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_10/mlp/wi_1/kernel/v
I0512 23:42:16.409498 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_10/mlp/wi_1/kernel/v_col
I0512 23:42:16.409532 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_10/mlp/wi_1/kernel/v_row
I0512 23:42:16.409565 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_10/mlp/wo/kernel/m
I0512 23:42:16.409598 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_10/mlp/wo/kernel/v
I0512 23:42:16.409632 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_10/mlp/wo/kernel/v_col
I0512 23:42:16.409665 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_10/mlp/wo/kernel/v_row
I0512 23:42:16.409698 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_10/pre_mlp_layer_norm/scale/m
I0512 23:42:16.409732 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_10/pre_mlp_layer_norm/scale/v
I0512 23:42:16.409765 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_10/pre_mlp_layer_norm/scale/v_col
I0512 23:42:16.409799 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_10/pre_mlp_layer_norm/scale/v_row
I0512 23:42:16.409832 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_10/pre_self_attention_layer_norm/scale/m
I0512 23:42:16.409866 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_10/pre_self_attention_layer_norm/scale/v
I0512 23:42:16.409899 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_10/pre_self_attention_layer_norm/scale/v_col
I0512 23:42:16.409933 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_10/pre_self_attention_layer_norm/scale/v_row
I0512 23:42:16.409966 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_10/self_attention/key/kernel/m
I0512 23:42:16.409999 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_10/self_attention/key/kernel/v
I0512 23:42:16.410033 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_10/self_attention/key/kernel/v_col
I0512 23:42:16.410066 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_10/self_attention/key/kernel/v_row
I0512 23:42:16.410099 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_10/self_attention/out/kernel/m
I0512 23:42:16.410132 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_10/self_attention/out/kernel/v
I0512 23:42:16.410166 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_10/self_attention/out/kernel/v_col
I0512 23:42:16.410202 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_10/self_attention/out/kernel/v_row
I0512 23:42:16.410236 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_10/self_attention/query/kernel/m
I0512 23:42:16.410269 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_10/self_attention/query/kernel/v
I0512 23:42:16.410303 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_10/self_attention/query/kernel/v_col
I0512 23:42:16.410336 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_10/self_attention/query/kernel/v_row
I0512 23:42:16.410369 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_10/self_attention/value/kernel/m
I0512 23:42:16.410403 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_10/self_attention/value/kernel/v
I0512 23:42:16.410443 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_10/self_attention/value/kernel/v_col
I0512 23:42:16.410477 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_10/self_attention/value/kernel/v_row
I0512 23:42:16.410510 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/extra_mlp/wi_0/kernel/m
I0512 23:42:16.410544 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/extra_mlp/wi_0/kernel/v
I0512 23:42:16.410577 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/extra_mlp/wi_0/kernel/v_col
I0512 23:42:16.410610 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/extra_mlp/wi_0/kernel/v_row
I0512 23:42:16.410644 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/extra_mlp/wi_1/kernel/m
I0512 23:42:16.410677 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/extra_mlp/wi_1/kernel/v
I0512 23:42:16.410711 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/extra_mlp/wi_1/kernel/v_col
I0512 23:42:16.410744 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/extra_mlp/wi_1/kernel/v_row
I0512 23:42:16.410777 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/extra_mlp/wo/kernel/m
I0512 23:42:16.410811 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/extra_mlp/wo/kernel/v
I0512 23:42:16.410844 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/extra_mlp/wo/kernel/v_col
I0512 23:42:16.410878 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/extra_mlp/wo/kernel/v_row
I0512 23:42:16.410912 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/mlp/expert/wi_0/kernel/m
I0512 23:42:16.410944 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/mlp/expert/wi_0/kernel/v
I0512 23:42:16.410978 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/mlp/expert/wi_0/kernel/v_col
I0512 23:42:16.411011 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/mlp/expert/wi_0/kernel/v_row
I0512 23:42:16.411044 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/mlp/expert/wi_1/kernel/m
I0512 23:42:16.411078 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/mlp/expert/wi_1/kernel/v
I0512 23:42:16.411111 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/mlp/expert/wi_1/kernel/v_col
I0512 23:42:16.411144 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/mlp/expert/wi_1/kernel/v_row
I0512 23:42:16.411179 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/mlp/expert/wo/kernel/m
I0512 23:42:16.411214 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/mlp/expert/wo/kernel/v
I0512 23:42:16.411247 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/mlp/expert/wo/kernel/v_col
I0512 23:42:16.411281 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/mlp/expert/wo/kernel/v_row
I0512 23:42:16.411314 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/mlp/router/router_weights/w/kernel/m
I0512 23:42:16.411348 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/mlp/router/router_weights/w/kernel/v
I0512 23:42:16.411381 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/mlp/router/router_weights/w/kernel/v_col
I0512 23:42:16.411415 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/mlp/router/router_weights/w/kernel/v_row
I0512 23:42:16.411455 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/pre_extra_mlp_layer_norm/scale/m
I0512 23:42:16.411489 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/pre_extra_mlp_layer_norm/scale/v
I0512 23:42:16.411523 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/pre_extra_mlp_layer_norm/scale/v_col
I0512 23:42:16.411556 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/pre_extra_mlp_layer_norm/scale/v_row
I0512 23:42:16.411590 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/pre_mlp_layer_norm/scale/m
I0512 23:42:16.411623 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/pre_mlp_layer_norm/scale/v
I0512 23:42:16.411656 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/pre_mlp_layer_norm/scale/v_col
I0512 23:42:16.411689 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/pre_mlp_layer_norm/scale/v_row
I0512 23:42:16.411720 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/pre_self_attention_layer_norm/scale/m
I0512 23:42:16.411752 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/pre_self_attention_layer_norm/scale/v
I0512 23:42:16.411783 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/pre_self_attention_layer_norm/scale/v_col
I0512 23:42:16.411814 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/pre_self_attention_layer_norm/scale/v_row
I0512 23:42:16.411846 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/self_attention/key/kernel/m
I0512 23:42:16.411877 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/self_attention/key/kernel/v
I0512 23:42:16.411908 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/self_attention/key/kernel/v_col
I0512 23:42:16.411939 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/self_attention/key/kernel/v_row
I0512 23:42:16.411971 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/self_attention/out/kernel/m
I0512 23:42:16.412002 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/self_attention/out/kernel/v
I0512 23:42:16.412033 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/self_attention/out/kernel/v_col
I0512 23:42:16.412064 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/self_attention/out/kernel/v_row
I0512 23:42:16.412095 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/self_attention/query/kernel/m
I0512 23:42:16.412126 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/self_attention/query/kernel/v
I0512 23:42:16.412158 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/self_attention/query/kernel/v_col
I0512 23:42:16.412190 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/self_attention/query/kernel/v_row
I0512 23:42:16.412223 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/self_attention/value/kernel/m
I0512 23:42:16.412254 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/self_attention/value/kernel/v
I0512 23:42:16.412286 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/self_attention/value/kernel/v_col
I0512 23:42:16.412317 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/self_attention/value/kernel/v_row
I0512 23:42:16.412348 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_12/mlp/wi_0/kernel/m
I0512 23:42:16.412379 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_12/mlp/wi_0/kernel/v
I0512 23:42:16.412410 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_12/mlp/wi_0/kernel/v_col
I0512 23:42:16.412447 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_12/mlp/wi_0/kernel/v_row
I0512 23:42:16.412479 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_12/mlp/wi_1/kernel/m
I0512 23:42:16.412510 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_12/mlp/wi_1/kernel/v
I0512 23:42:16.412541 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_12/mlp/wi_1/kernel/v_col
I0512 23:42:16.412572 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_12/mlp/wi_1/kernel/v_row
I0512 23:42:16.412603 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_12/mlp/wo/kernel/m
I0512 23:42:16.412635 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_12/mlp/wo/kernel/v
I0512 23:42:16.412666 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_12/mlp/wo/kernel/v_col
I0512 23:42:16.412698 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_12/mlp/wo/kernel/v_row
I0512 23:42:16.412729 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_12/pre_mlp_layer_norm/scale/m
I0512 23:42:16.412760 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_12/pre_mlp_layer_norm/scale/v
I0512 23:42:16.412791 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_12/pre_mlp_layer_norm/scale/v_col
I0512 23:42:16.412822 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_12/pre_mlp_layer_norm/scale/v_row
I0512 23:42:16.412853 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_12/pre_self_attention_layer_norm/scale/m
I0512 23:42:16.412884 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_12/pre_self_attention_layer_norm/scale/v
I0512 23:42:16.412915 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_12/pre_self_attention_layer_norm/scale/v_col
I0512 23:42:16.412946 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_12/pre_self_attention_layer_norm/scale/v_row
I0512 23:42:16.413001 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_12/self_attention/key/kernel/m
I0512 23:42:16.413034 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_12/self_attention/key/kernel/v
I0512 23:42:16.413066 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_12/self_attention/key/kernel/v_col
I0512 23:42:16.413097 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_12/self_attention/key/kernel/v_row
I0512 23:42:16.413128 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_12/self_attention/out/kernel/m
I0512 23:42:16.413159 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_12/self_attention/out/kernel/v
I0512 23:42:16.413192 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_12/self_attention/out/kernel/v_col
I0512 23:42:16.413224 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_12/self_attention/out/kernel/v_row
I0512 23:42:16.413255 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_12/self_attention/query/kernel/m
I0512 23:42:16.413286 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_12/self_attention/query/kernel/v
I0512 23:42:16.413317 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_12/self_attention/query/kernel/v_col
I0512 23:42:16.413348 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_12/self_attention/query/kernel/v_row
I0512 23:42:16.413379 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_12/self_attention/value/kernel/m
I0512 23:42:16.413410 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_12/self_attention/value/kernel/v
I0512 23:42:16.413447 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_12/self_attention/value/kernel/v_col
I0512 23:42:16.413481 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_12/self_attention/value/kernel/v_row
I0512 23:42:16.413513 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_13/mlp/wi_0/kernel/m
I0512 23:42:16.413544 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_13/mlp/wi_0/kernel/v
I0512 23:42:16.413575 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_13/mlp/wi_0/kernel/v_col
I0512 23:42:16.413606 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_13/mlp/wi_0/kernel/v_row
I0512 23:42:16.413637 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_13/mlp/wi_1/kernel/m
I0512 23:42:16.413668 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_13/mlp/wi_1/kernel/v
I0512 23:42:16.413698 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_13/mlp/wi_1/kernel/v_col
I0512 23:42:16.413729 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_13/mlp/wi_1/kernel/v_row
I0512 23:42:16.413760 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_13/mlp/wo/kernel/m
I0512 23:42:16.413790 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_13/mlp/wo/kernel/v
I0512 23:42:16.413821 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_13/mlp/wo/kernel/v_col
I0512 23:42:16.413852 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_13/mlp/wo/kernel/v_row
I0512 23:42:16.413882 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_13/pre_mlp_layer_norm/scale/m
I0512 23:42:16.413913 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_13/pre_mlp_layer_norm/scale/v
I0512 23:42:16.413944 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_13/pre_mlp_layer_norm/scale/v_col
I0512 23:42:16.413975 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_13/pre_mlp_layer_norm/scale/v_row
I0512 23:42:16.414005 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_13/pre_self_attention_layer_norm/scale/m
I0512 23:42:16.414036 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_13/pre_self_attention_layer_norm/scale/v
I0512 23:42:16.414067 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_13/pre_self_attention_layer_norm/scale/v_col
I0512 23:42:16.414098 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_13/pre_self_attention_layer_norm/scale/v_row
I0512 23:42:16.414129 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_13/self_attention/key/kernel/m
I0512 23:42:16.414159 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_13/self_attention/key/kernel/v
I0512 23:42:16.414192 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_13/self_attention/key/kernel/v_col
I0512 23:42:16.414224 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_13/self_attention/key/kernel/v_row
I0512 23:42:16.414255 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_13/self_attention/out/kernel/m
I0512 23:42:16.414286 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_13/self_attention/out/kernel/v
I0512 23:42:16.414316 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_13/self_attention/out/kernel/v_col
I0512 23:42:16.414347 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_13/self_attention/out/kernel/v_row
I0512 23:42:16.414378 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_13/self_attention/query/kernel/m
I0512 23:42:16.414408 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_13/self_attention/query/kernel/v
I0512 23:42:16.414445 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_13/self_attention/query/kernel/v_col
I0512 23:42:16.414477 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_13/self_attention/query/kernel/v_row
I0512 23:42:16.414508 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_13/self_attention/value/kernel/m
I0512 23:42:16.414539 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_13/self_attention/value/kernel/v
I0512 23:42:16.414570 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_13/self_attention/value/kernel/v_col
I0512 23:42:16.414601 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_13/self_attention/value/kernel/v_row
I0512 23:42:16.414631 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_14/mlp/wi_0/kernel/m
I0512 23:42:16.414662 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_14/mlp/wi_0/kernel/v
I0512 23:42:16.414693 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_14/mlp/wi_0/kernel/v_col
I0512 23:42:16.414724 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_14/mlp/wi_0/kernel/v_row
I0512 23:42:16.414755 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_14/mlp/wi_1/kernel/m
I0512 23:42:16.414786 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_14/mlp/wi_1/kernel/v
I0512 23:42:16.414816 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_14/mlp/wi_1/kernel/v_col
I0512 23:42:16.414847 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_14/mlp/wi_1/kernel/v_row
I0512 23:42:16.414877 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_14/mlp/wo/kernel/m
I0512 23:42:16.414908 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_14/mlp/wo/kernel/v
I0512 23:42:16.414938 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_14/mlp/wo/kernel/v_col
I0512 23:42:16.414969 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_14/mlp/wo/kernel/v_row
I0512 23:42:16.414999 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_14/pre_mlp_layer_norm/scale/m
I0512 23:42:16.415030 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_14/pre_mlp_layer_norm/scale/v
I0512 23:42:16.415061 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_14/pre_mlp_layer_norm/scale/v_col
I0512 23:42:16.415091 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_14/pre_mlp_layer_norm/scale/v_row
I0512 23:42:16.415122 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_14/pre_self_attention_layer_norm/scale/m
I0512 23:42:16.415153 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_14/pre_self_attention_layer_norm/scale/v
I0512 23:42:16.415185 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_14/pre_self_attention_layer_norm/scale/v_col
I0512 23:42:16.415218 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_14/pre_self_attention_layer_norm/scale/v_row
I0512 23:42:16.415249 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_14/self_attention/key/kernel/m
I0512 23:42:16.415280 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_14/self_attention/key/kernel/v
I0512 23:42:16.415310 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_14/self_attention/key/kernel/v_col
I0512 23:42:16.415341 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_14/self_attention/key/kernel/v_row
I0512 23:42:16.415372 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_14/self_attention/out/kernel/m
I0512 23:42:16.415402 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_14/self_attention/out/kernel/v
I0512 23:42:16.415438 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_14/self_attention/out/kernel/v_col
I0512 23:42:16.415470 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_14/self_attention/out/kernel/v_row
I0512 23:42:16.415501 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_14/self_attention/query/kernel/m
I0512 23:42:16.415532 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_14/self_attention/query/kernel/v
I0512 23:42:16.415563 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_14/self_attention/query/kernel/v_col
I0512 23:42:16.415593 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_14/self_attention/query/kernel/v_row
I0512 23:42:16.415624 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_14/self_attention/value/kernel/m
I0512 23:42:16.415654 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_14/self_attention/value/kernel/v
I0512 23:42:16.415685 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_14/self_attention/value/kernel/v_col
I0512 23:42:16.415716 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_14/self_attention/value/kernel/v_row
I0512 23:42:16.415747 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_15/mlp/wi_0/kernel/m
I0512 23:42:16.415778 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_15/mlp/wi_0/kernel/v
I0512 23:42:16.415809 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_15/mlp/wi_0/kernel/v_col
I0512 23:42:16.415839 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_15/mlp/wi_0/kernel/v_row
I0512 23:42:16.415870 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_15/mlp/wi_1/kernel/m
I0512 23:42:16.415901 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_15/mlp/wi_1/kernel/v
I0512 23:42:16.415932 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_15/mlp/wi_1/kernel/v_col
I0512 23:42:16.415962 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_15/mlp/wi_1/kernel/v_row
I0512 23:42:16.415993 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_15/mlp/wo/kernel/m
I0512 23:42:16.416023 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_15/mlp/wo/kernel/v
I0512 23:42:16.416054 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_15/mlp/wo/kernel/v_col
I0512 23:42:16.416085 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_15/mlp/wo/kernel/v_row
I0512 23:42:16.416115 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_15/pre_mlp_layer_norm/scale/m
I0512 23:42:16.416146 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_15/pre_mlp_layer_norm/scale/v
I0512 23:42:16.416178 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_15/pre_mlp_layer_norm/scale/v_col
I0512 23:42:16.416210 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_15/pre_mlp_layer_norm/scale/v_row
I0512 23:42:16.416241 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_15/pre_self_attention_layer_norm/scale/m
I0512 23:42:16.416272 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_15/pre_self_attention_layer_norm/scale/v
I0512 23:42:16.416303 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_15/pre_self_attention_layer_norm/scale/v_col
I0512 23:42:16.416334 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_15/pre_self_attention_layer_norm/scale/v_row
I0512 23:42:16.416364 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_15/self_attention/key/kernel/m
I0512 23:42:16.416395 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_15/self_attention/key/kernel/v
I0512 23:42:16.416430 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_15/self_attention/key/kernel/v_col
I0512 23:42:16.416463 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_15/self_attention/key/kernel/v_row
I0512 23:42:16.416494 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_15/self_attention/out/kernel/m
I0512 23:42:16.416525 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_15/self_attention/out/kernel/v
I0512 23:42:16.416556 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_15/self_attention/out/kernel/v_col
I0512 23:42:16.416586 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_15/self_attention/out/kernel/v_row
I0512 23:42:16.416617 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_15/self_attention/query/kernel/m
I0512 23:42:16.416648 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_15/self_attention/query/kernel/v
I0512 23:42:16.416679 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_15/self_attention/query/kernel/v_col
I0512 23:42:16.416710 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_15/self_attention/query/kernel/v_row
I0512 23:42:16.416741 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_15/self_attention/value/kernel/m
I0512 23:42:16.416771 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_15/self_attention/value/kernel/v
I0512 23:42:16.416802 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_15/self_attention/value/kernel/v_col
I0512 23:42:16.416833 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_15/self_attention/value/kernel/v_row
I0512 23:42:16.416869 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_16/mlp/wi_0/kernel/m
I0512 23:42:16.416906 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_16/mlp/wi_0/kernel/v
I0512 23:42:16.416939 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_16/mlp/wi_0/kernel/v_col
I0512 23:42:16.416991 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_16/mlp/wi_0/kernel/v_row
I0512 23:42:16.417032 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_16/mlp/wi_1/kernel/m
I0512 23:42:16.417065 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_16/mlp/wi_1/kernel/v
I0512 23:42:16.417096 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_16/mlp/wi_1/kernel/v_col
I0512 23:42:16.417125 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_16/mlp/wi_1/kernel/v_row
I0512 23:42:16.417156 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_16/mlp/wo/kernel/m
I0512 23:42:16.417187 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_16/mlp/wo/kernel/v
I0512 23:42:16.417219 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_16/mlp/wo/kernel/v_col
I0512 23:42:16.417250 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_16/mlp/wo/kernel/v_row
I0512 23:42:16.417280 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_16/pre_mlp_layer_norm/scale/m
I0512 23:42:16.417311 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_16/pre_mlp_layer_norm/scale/v
I0512 23:42:16.417341 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_16/pre_mlp_layer_norm/scale/v_col
I0512 23:42:16.417371 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_16/pre_mlp_layer_norm/scale/v_row
I0512 23:42:16.417401 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_16/pre_self_attention_layer_norm/scale/m
I0512 23:42:16.417436 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_16/pre_self_attention_layer_norm/scale/v
I0512 23:42:16.417469 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_16/pre_self_attention_layer_norm/scale/v_col
I0512 23:42:16.417501 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_16/pre_self_attention_layer_norm/scale/v_row
I0512 23:42:16.417532 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_16/self_attention/key/kernel/m
I0512 23:42:16.417562 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_16/self_attention/key/kernel/v
I0512 23:42:16.417592 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_16/self_attention/key/kernel/v_col
I0512 23:42:16.417622 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_16/self_attention/key/kernel/v_row
I0512 23:42:16.417652 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_16/self_attention/out/kernel/m
I0512 23:42:16.417681 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_16/self_attention/out/kernel/v
I0512 23:42:16.417711 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_16/self_attention/out/kernel/v_col
I0512 23:42:16.417741 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_16/self_attention/out/kernel/v_row
I0512 23:42:16.417771 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_16/self_attention/query/kernel/m
I0512 23:42:16.417800 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_16/self_attention/query/kernel/v
I0512 23:42:16.417830 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_16/self_attention/query/kernel/v_col
I0512 23:42:16.417860 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_16/self_attention/query/kernel/v_row
I0512 23:42:16.417890 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_16/self_attention/value/kernel/m
I0512 23:42:16.417920 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_16/self_attention/value/kernel/v
I0512 23:42:16.417949 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_16/self_attention/value/kernel/v_col
I0512 23:42:16.417979 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_16/self_attention/value/kernel/v_row
I0512 23:42:16.418008 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/extra_mlp/wi_0/kernel/m
I0512 23:42:16.418038 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/extra_mlp/wi_0/kernel/v
I0512 23:42:16.418068 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/extra_mlp/wi_0/kernel/v_col
I0512 23:42:16.418097 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/extra_mlp/wi_0/kernel/v_row
I0512 23:42:16.418127 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/extra_mlp/wi_1/kernel/m
I0512 23:42:16.418156 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/extra_mlp/wi_1/kernel/v
I0512 23:42:16.418187 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/extra_mlp/wi_1/kernel/v_col
I0512 23:42:16.418217 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/extra_mlp/wi_1/kernel/v_row
I0512 23:42:16.418247 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/extra_mlp/wo/kernel/m
I0512 23:42:16.418277 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/extra_mlp/wo/kernel/v
I0512 23:42:16.418308 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/extra_mlp/wo/kernel/v_col
I0512 23:42:16.418338 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/extra_mlp/wo/kernel/v_row
I0512 23:42:16.418367 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/mlp/expert/wi_0/kernel/m
I0512 23:42:16.418397 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/mlp/expert/wi_0/kernel/v
I0512 23:42:16.418432 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/mlp/expert/wi_0/kernel/v_col
I0512 23:42:16.418463 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/mlp/expert/wi_0/kernel/v_row
I0512 23:42:16.418493 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/mlp/expert/wi_1/kernel/m
I0512 23:42:16.418523 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/mlp/expert/wi_1/kernel/v
I0512 23:42:16.418553 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/mlp/expert/wi_1/kernel/v_col
I0512 23:42:16.418583 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/mlp/expert/wi_1/kernel/v_row
I0512 23:42:16.418612 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/mlp/expert/wo/kernel/m
I0512 23:42:16.418642 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/mlp/expert/wo/kernel/v
I0512 23:42:16.418672 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/mlp/expert/wo/kernel/v_col
I0512 23:42:16.418702 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/mlp/expert/wo/kernel/v_row
I0512 23:42:16.418732 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/mlp/router/router_weights/w/kernel/m
I0512 23:42:16.418761 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/mlp/router/router_weights/w/kernel/v
I0512 23:42:16.418792 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/mlp/router/router_weights/w/kernel/v_col
I0512 23:42:16.418821 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/mlp/router/router_weights/w/kernel/v_row
I0512 23:42:16.418851 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/pre_extra_mlp_layer_norm/scale/m
I0512 23:42:16.418881 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/pre_extra_mlp_layer_norm/scale/v
I0512 23:42:16.418910 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/pre_extra_mlp_layer_norm/scale/v_col
I0512 23:42:16.418940 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/pre_extra_mlp_layer_norm/scale/v_row
I0512 23:42:16.418969 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/pre_mlp_layer_norm/scale/m
I0512 23:42:16.418999 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/pre_mlp_layer_norm/scale/v
I0512 23:42:16.419028 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/pre_mlp_layer_norm/scale/v_col
I0512 23:42:16.419058 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/pre_mlp_layer_norm/scale/v_row
I0512 23:42:16.419088 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/pre_self_attention_layer_norm/scale/m
I0512 23:42:16.419117 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/pre_self_attention_layer_norm/scale/v
I0512 23:42:16.419147 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/pre_self_attention_layer_norm/scale/v_col
I0512 23:42:16.419177 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/pre_self_attention_layer_norm/scale/v_row
I0512 23:42:16.419209 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/self_attention/key/kernel/m
I0512 23:42:16.419240 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/self_attention/key/kernel/v
I0512 23:42:16.419269 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/self_attention/key/kernel/v_col
I0512 23:42:16.419299 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/self_attention/key/kernel/v_row
I0512 23:42:16.419329 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/self_attention/out/kernel/m
I0512 23:42:16.419359 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/self_attention/out/kernel/v
I0512 23:42:16.419389 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/self_attention/out/kernel/v_col
I0512 23:42:16.419418 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/self_attention/out/kernel/v_row
I0512 23:42:16.419455 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/self_attention/query/kernel/m
I0512 23:42:16.419485 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/self_attention/query/kernel/v
I0512 23:42:16.419515 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/self_attention/query/kernel/v_col
I0512 23:42:16.419545 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/self_attention/query/kernel/v_row
I0512 23:42:16.419574 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/self_attention/value/kernel/m
I0512 23:42:16.419604 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/self_attention/value/kernel/v
I0512 23:42:16.419634 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/self_attention/value/kernel/v_col
I0512 23:42:16.419663 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/self_attention/value/kernel/v_row
I0512 23:42:16.419693 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_18/mlp/wi_0/kernel/m
I0512 23:42:16.419723 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_18/mlp/wi_0/kernel/v
I0512 23:42:16.419753 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_18/mlp/wi_0/kernel/v_col
I0512 23:42:16.419783 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_18/mlp/wi_0/kernel/v_row
I0512 23:42:16.419813 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_18/mlp/wi_1/kernel/m
I0512 23:42:16.419842 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_18/mlp/wi_1/kernel/v
I0512 23:42:16.419872 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_18/mlp/wi_1/kernel/v_col
I0512 23:42:16.419902 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_18/mlp/wi_1/kernel/v_row
I0512 23:42:16.419932 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_18/mlp/wo/kernel/m
I0512 23:42:16.419961 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_18/mlp/wo/kernel/v
I0512 23:42:16.419991 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_18/mlp/wo/kernel/v_col
I0512 23:42:16.420021 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_18/mlp/wo/kernel/v_row
I0512 23:42:16.420051 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_18/pre_mlp_layer_norm/scale/m
I0512 23:42:16.420081 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_18/pre_mlp_layer_norm/scale/v
I0512 23:42:16.420110 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_18/pre_mlp_layer_norm/scale/v_col
I0512 23:42:16.420140 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_18/pre_mlp_layer_norm/scale/v_row
I0512 23:42:16.420170 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_18/pre_self_attention_layer_norm/scale/m
I0512 23:42:16.420202 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_18/pre_self_attention_layer_norm/scale/v
I0512 23:42:16.420232 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_18/pre_self_attention_layer_norm/scale/v_col
I0512 23:42:16.420262 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_18/pre_self_attention_layer_norm/scale/v_row
I0512 23:42:16.420292 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_18/self_attention/key/kernel/m
I0512 23:42:16.420322 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_18/self_attention/key/kernel/v
I0512 23:42:16.420351 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_18/self_attention/key/kernel/v_col
I0512 23:42:16.420381 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_18/self_attention/key/kernel/v_row
I0512 23:42:16.420411 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_18/self_attention/out/kernel/m
I0512 23:42:16.420448 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_18/self_attention/out/kernel/v
I0512 23:42:16.420478 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_18/self_attention/out/kernel/v_col
I0512 23:42:16.420508 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_18/self_attention/out/kernel/v_row
I0512 23:42:16.420538 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_18/self_attention/query/kernel/m
I0512 23:42:16.420568 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_18/self_attention/query/kernel/v
I0512 23:42:16.420598 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_18/self_attention/query/kernel/v_col
I0512 23:42:16.420628 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_18/self_attention/query/kernel/v_row
I0512 23:42:16.420658 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_18/self_attention/value/kernel/m
I0512 23:42:16.420688 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_18/self_attention/value/kernel/v
I0512 23:42:16.420717 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_18/self_attention/value/kernel/v_col
I0512 23:42:16.420747 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_18/self_attention/value/kernel/v_row
I0512 23:42:16.420776 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_19/mlp/wi_0/kernel/m
I0512 23:42:16.420807 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_19/mlp/wi_0/kernel/v
I0512 23:42:16.420837 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_19/mlp/wi_0/kernel/v_col
I0512 23:42:16.420866 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_19/mlp/wi_0/kernel/v_row
I0512 23:42:16.420896 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_19/mlp/wi_1/kernel/m
I0512 23:42:16.420926 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_19/mlp/wi_1/kernel/v
I0512 23:42:16.420973 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_19/mlp/wi_1/kernel/v_col
I0512 23:42:16.421010 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_19/mlp/wi_1/kernel/v_row
I0512 23:42:16.421041 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_19/mlp/wo/kernel/m
I0512 23:42:16.421071 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_19/mlp/wo/kernel/v
I0512 23:42:16.421100 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_19/mlp/wo/kernel/v_col
I0512 23:42:16.421130 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_19/mlp/wo/kernel/v_row
I0512 23:42:16.421160 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_19/pre_mlp_layer_norm/scale/m
I0512 23:42:16.421192 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_19/pre_mlp_layer_norm/scale/v
I0512 23:42:16.421223 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_19/pre_mlp_layer_norm/scale/v_col
I0512 23:42:16.421253 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_19/pre_mlp_layer_norm/scale/v_row
I0512 23:42:16.421283 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_19/pre_self_attention_layer_norm/scale/m
I0512 23:42:16.421313 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_19/pre_self_attention_layer_norm/scale/v
I0512 23:42:16.421342 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_19/pre_self_attention_layer_norm/scale/v_col
I0512 23:42:16.421372 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_19/pre_self_attention_layer_norm/scale/v_row
I0512 23:42:16.421402 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_19/self_attention/key/kernel/m
I0512 23:42:16.421437 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_19/self_attention/key/kernel/v
I0512 23:42:16.421468 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_19/self_attention/key/kernel/v_col
I0512 23:42:16.421498 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_19/self_attention/key/kernel/v_row
I0512 23:42:16.421528 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_19/self_attention/out/kernel/m
I0512 23:42:16.421558 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_19/self_attention/out/kernel/v
I0512 23:42:16.421587 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_19/self_attention/out/kernel/v_col
I0512 23:42:16.421617 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_19/self_attention/out/kernel/v_row
I0512 23:42:16.421646 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_19/self_attention/query/kernel/m
I0512 23:42:16.421676 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_19/self_attention/query/kernel/v
I0512 23:42:16.421705 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_19/self_attention/query/kernel/v_col
I0512 23:42:16.421735 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_19/self_attention/query/kernel/v_row
I0512 23:42:16.421764 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_19/self_attention/value/kernel/m
I0512 23:42:16.421794 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_19/self_attention/value/kernel/v
I0512 23:42:16.421823 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_19/self_attention/value/kernel/v_col
I0512 23:42:16.421853 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_19/self_attention/value/kernel/v_row
I0512 23:42:16.421883 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_2/mlp/wi_0/kernel/m
I0512 23:42:16.421912 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_2/mlp/wi_0/kernel/v
I0512 23:42:16.421942 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_2/mlp/wi_0/kernel/v_col
I0512 23:42:16.421972 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_2/mlp/wi_0/kernel/v_row
I0512 23:42:16.422002 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_2/mlp/wi_1/kernel/m
I0512 23:42:16.422032 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_2/mlp/wi_1/kernel/v
I0512 23:42:16.422061 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_2/mlp/wi_1/kernel/v_col
I0512 23:42:16.422091 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_2/mlp/wi_1/kernel/v_row
I0512 23:42:16.422121 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_2/mlp/wo/kernel/m
I0512 23:42:16.422151 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_2/mlp/wo/kernel/v
I0512 23:42:16.422182 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_2/mlp/wo/kernel/v_col
I0512 23:42:16.422213 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_2/mlp/wo/kernel/v_row
I0512 23:42:16.422244 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_2/pre_mlp_layer_norm/scale/m
I0512 23:42:16.422274 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_2/pre_mlp_layer_norm/scale/v
I0512 23:42:16.422303 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_2/pre_mlp_layer_norm/scale/v_col
I0512 23:42:16.422333 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_2/pre_mlp_layer_norm/scale/v_row
I0512 23:42:16.422363 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_2/pre_self_attention_layer_norm/scale/m
I0512 23:42:16.422393 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_2/pre_self_attention_layer_norm/scale/v
I0512 23:42:16.422423 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_2/pre_self_attention_layer_norm/scale/v_col
I0512 23:42:16.422460 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_2/pre_self_attention_layer_norm/scale/v_row
I0512 23:42:16.422490 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_2/self_attention/key/kernel/m
I0512 23:42:16.422520 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_2/self_attention/key/kernel/v
I0512 23:42:16.422549 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_2/self_attention/key/kernel/v_col
I0512 23:42:16.422579 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_2/self_attention/key/kernel/v_row
I0512 23:42:16.422609 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_2/self_attention/out/kernel/m
I0512 23:42:16.422638 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_2/self_attention/out/kernel/v
I0512 23:42:16.422668 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_2/self_attention/out/kernel/v_col
I0512 23:42:16.422698 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_2/self_attention/out/kernel/v_row
I0512 23:42:16.422727 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_2/self_attention/query/kernel/m
I0512 23:42:16.422757 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_2/self_attention/query/kernel/v
I0512 23:42:16.422787 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_2/self_attention/query/kernel/v_col
I0512 23:42:16.422816 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_2/self_attention/query/kernel/v_row
I0512 23:42:16.422846 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_2/self_attention/value/kernel/m
I0512 23:42:16.422875 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_2/self_attention/value/kernel/v
I0512 23:42:16.422905 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_2/self_attention/value/kernel/v_col
I0512 23:42:16.422935 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_2/self_attention/value/kernel/v_row
I0512 23:42:16.422965 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_20/mlp/wi_0/kernel/m
I0512 23:42:16.422994 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_20/mlp/wi_0/kernel/v
I0512 23:42:16.423024 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_20/mlp/wi_0/kernel/v_col
I0512 23:42:16.423054 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_20/mlp/wi_0/kernel/v_row
I0512 23:42:16.423084 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_20/mlp/wi_1/kernel/m
I0512 23:42:16.423114 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_20/mlp/wi_1/kernel/v
I0512 23:42:16.423144 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_20/mlp/wi_1/kernel/v_col
I0512 23:42:16.423174 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_20/mlp/wi_1/kernel/v_row
I0512 23:42:16.423206 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_20/mlp/wo/kernel/m
I0512 23:42:16.423237 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_20/mlp/wo/kernel/v
I0512 23:42:16.423267 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_20/mlp/wo/kernel/v_col
I0512 23:42:16.423296 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_20/mlp/wo/kernel/v_row
I0512 23:42:16.423326 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_20/pre_mlp_layer_norm/scale/m
I0512 23:42:16.423357 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_20/pre_mlp_layer_norm/scale/v
I0512 23:42:16.423386 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_20/pre_mlp_layer_norm/scale/v_col
I0512 23:42:16.423416 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_20/pre_mlp_layer_norm/scale/v_row
I0512 23:42:16.423452 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_20/pre_self_attention_layer_norm/scale/m
I0512 23:42:16.423483 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_20/pre_self_attention_layer_norm/scale/v
I0512 23:42:16.423513 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_20/pre_self_attention_layer_norm/scale/v_col
I0512 23:42:16.423543 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_20/pre_self_attention_layer_norm/scale/v_row
I0512 23:42:16.423573 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_20/self_attention/key/kernel/m
I0512 23:42:16.423634 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_20/self_attention/key/kernel/v
I0512 23:42:16.423666 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_20/self_attention/key/kernel/v_col
I0512 23:42:16.423696 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_20/self_attention/key/kernel/v_row
I0512 23:42:16.423726 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_20/self_attention/out/kernel/m
I0512 23:42:16.423756 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_20/self_attention/out/kernel/v
I0512 23:42:16.423785 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_20/self_attention/out/kernel/v_col
I0512 23:42:16.423815 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_20/self_attention/out/kernel/v_row
I0512 23:42:16.423844 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_20/self_attention/query/kernel/m
I0512 23:42:16.423874 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_20/self_attention/query/kernel/v
I0512 23:42:16.423903 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_20/self_attention/query/kernel/v_col
I0512 23:42:16.423933 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_20/self_attention/query/kernel/v_row
I0512 23:42:16.423963 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_20/self_attention/value/kernel/m
I0512 23:42:16.423993 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_20/self_attention/value/kernel/v
I0512 23:42:16.424023 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_20/self_attention/value/kernel/v_col
I0512 23:42:16.424052 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_20/self_attention/value/kernel/v_row
I0512 23:42:16.424082 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_21/mlp/wi_0/kernel/m
I0512 23:42:16.424112 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_21/mlp/wi_0/kernel/v
I0512 23:42:16.424142 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_21/mlp/wi_0/kernel/v_col
I0512 23:42:16.424172 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_21/mlp/wi_0/kernel/v_row
I0512 23:42:16.424204 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_21/mlp/wi_1/kernel/m
I0512 23:42:16.424235 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_21/mlp/wi_1/kernel/v
I0512 23:42:16.424264 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_21/mlp/wi_1/kernel/v_col
I0512 23:42:16.424294 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_21/mlp/wi_1/kernel/v_row
I0512 23:42:16.424324 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_21/mlp/wo/kernel/m
I0512 23:42:16.424354 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_21/mlp/wo/kernel/v
I0512 23:42:16.424384 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_21/mlp/wo/kernel/v_col
I0512 23:42:16.424414 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_21/mlp/wo/kernel/v_row
I0512 23:42:16.424451 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_21/pre_mlp_layer_norm/scale/m
I0512 23:42:16.424482 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_21/pre_mlp_layer_norm/scale/v
I0512 23:42:16.424512 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_21/pre_mlp_layer_norm/scale/v_col
I0512 23:42:16.424542 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_21/pre_mlp_layer_norm/scale/v_row
I0512 23:42:16.424572 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_21/pre_self_attention_layer_norm/scale/m
I0512 23:42:16.424602 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_21/pre_self_attention_layer_norm/scale/v
I0512 23:42:16.424631 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_21/pre_self_attention_layer_norm/scale/v_col
I0512 23:42:16.424661 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_21/pre_self_attention_layer_norm/scale/v_row
I0512 23:42:16.424690 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_21/self_attention/key/kernel/m
I0512 23:42:16.424720 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_21/self_attention/key/kernel/v
I0512 23:42:16.424750 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_21/self_attention/key/kernel/v_col
I0512 23:42:16.424780 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_21/self_attention/key/kernel/v_row
I0512 23:42:16.424809 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_21/self_attention/out/kernel/m
I0512 23:42:16.424839 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_21/self_attention/out/kernel/v
I0512 23:42:16.424869 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_21/self_attention/out/kernel/v_col
I0512 23:42:16.424899 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_21/self_attention/out/kernel/v_row
I0512 23:42:16.424928 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_21/self_attention/query/kernel/m
I0512 23:42:16.424976 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_21/self_attention/query/kernel/v
I0512 23:42:16.425012 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_21/self_attention/query/kernel/v_col
I0512 23:42:16.425042 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_21/self_attention/query/kernel/v_row
I0512 23:42:16.425073 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_21/self_attention/value/kernel/m
I0512 23:42:16.425102 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_21/self_attention/value/kernel/v
I0512 23:42:16.425132 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_21/self_attention/value/kernel/v_col
I0512 23:42:16.425161 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_21/self_attention/value/kernel/v_row
I0512 23:42:16.425193 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_22/mlp/wi_0/kernel/m
I0512 23:42:16.425223 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_22/mlp/wi_0/kernel/v
I0512 23:42:16.425254 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_22/mlp/wi_0/kernel/v_col
I0512 23:42:16.425284 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_22/mlp/wi_0/kernel/v_row
I0512 23:42:16.425314 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_22/mlp/wi_1/kernel/m
I0512 23:42:16.425343 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_22/mlp/wi_1/kernel/v
I0512 23:42:16.425373 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_22/mlp/wi_1/kernel/v_col
I0512 23:42:16.425403 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_22/mlp/wi_1/kernel/v_row
I0512 23:42:16.425439 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_22/mlp/wo/kernel/m
I0512 23:42:16.425471 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_22/mlp/wo/kernel/v
I0512 23:42:16.425501 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_22/mlp/wo/kernel/v_col
I0512 23:42:16.425530 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_22/mlp/wo/kernel/v_row
I0512 23:42:16.425560 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_22/pre_mlp_layer_norm/scale/m
I0512 23:42:16.425590 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_22/pre_mlp_layer_norm/scale/v
I0512 23:42:16.425620 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_22/pre_mlp_layer_norm/scale/v_col
I0512 23:42:16.425650 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_22/pre_mlp_layer_norm/scale/v_row
I0512 23:42:16.425680 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_22/pre_self_attention_layer_norm/scale/m
I0512 23:42:16.425710 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_22/pre_self_attention_layer_norm/scale/v
I0512 23:42:16.425740 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_22/pre_self_attention_layer_norm/scale/v_col
I0512 23:42:16.425769 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_22/pre_self_attention_layer_norm/scale/v_row
I0512 23:42:16.425799 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_22/self_attention/key/kernel/m
I0512 23:42:16.425828 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_22/self_attention/key/kernel/v
I0512 23:42:16.425858 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_22/self_attention/key/kernel/v_col
I0512 23:42:16.425888 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_22/self_attention/key/kernel/v_row
I0512 23:42:16.425917 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_22/self_attention/out/kernel/m
I0512 23:42:16.425947 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_22/self_attention/out/kernel/v
I0512 23:42:16.425977 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_22/self_attention/out/kernel/v_col
I0512 23:42:16.426007 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_22/self_attention/out/kernel/v_row
I0512 23:42:16.426036 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_22/self_attention/query/kernel/m
I0512 23:42:16.426065 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_22/self_attention/query/kernel/v
I0512 23:42:16.426095 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_22/self_attention/query/kernel/v_col
I0512 23:42:16.426124 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_22/self_attention/query/kernel/v_row
I0512 23:42:16.426153 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_22/self_attention/value/kernel/m
I0512 23:42:16.426184 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_22/self_attention/value/kernel/v
I0512 23:42:16.426215 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_22/self_attention/value/kernel/v_col
I0512 23:42:16.426245 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_22/self_attention/value/kernel/v_row
I0512 23:42:16.426275 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/extra_mlp/wi_0/kernel/m
I0512 23:42:16.426304 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/extra_mlp/wi_0/kernel/v
I0512 23:42:16.426333 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/extra_mlp/wi_0/kernel/v_col
I0512 23:42:16.426363 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/extra_mlp/wi_0/kernel/v_row
I0512 23:42:16.426393 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/extra_mlp/wi_1/kernel/m
I0512 23:42:16.426422 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/extra_mlp/wi_1/kernel/v
I0512 23:42:16.426458 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/extra_mlp/wi_1/kernel/v_col
I0512 23:42:16.426488 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/extra_mlp/wi_1/kernel/v_row
I0512 23:42:16.426517 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/extra_mlp/wo/kernel/m
I0512 23:42:16.426548 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/extra_mlp/wo/kernel/v
I0512 23:42:16.426578 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/extra_mlp/wo/kernel/v_col
I0512 23:42:16.426608 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/extra_mlp/wo/kernel/v_row
I0512 23:42:16.426637 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/mlp/expert/wi_0/kernel/m
I0512 23:42:16.426667 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/mlp/expert/wi_0/kernel/v
I0512 23:42:16.426697 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/mlp/expert/wi_0/kernel/v_col
I0512 23:42:16.426726 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/mlp/expert/wi_0/kernel/v_row
I0512 23:42:16.426756 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/mlp/expert/wi_1/kernel/m
I0512 23:42:16.426786 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/mlp/expert/wi_1/kernel/v
I0512 23:42:16.426815 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/mlp/expert/wi_1/kernel/v_col
I0512 23:42:16.426845 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/mlp/expert/wi_1/kernel/v_row
I0512 23:42:16.426874 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/mlp/expert/wo/kernel/m
I0512 23:42:16.426904 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/mlp/expert/wo/kernel/v
I0512 23:42:16.426934 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/mlp/expert/wo/kernel/v_col
I0512 23:42:16.426964 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/mlp/expert/wo/kernel/v_row
I0512 23:42:16.426993 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/mlp/router/router_weights/w/kernel/m
I0512 23:42:16.427023 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/mlp/router/router_weights/w/kernel/v
I0512 23:42:16.427053 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/mlp/router/router_weights/w/kernel/v_col
I0512 23:42:16.427083 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/mlp/router/router_weights/w/kernel/v_row
I0512 23:42:16.427113 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/pre_extra_mlp_layer_norm/scale/m
I0512 23:42:16.427142 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/pre_extra_mlp_layer_norm/scale/v
I0512 23:42:16.427172 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/pre_extra_mlp_layer_norm/scale/v_col
I0512 23:42:16.427204 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/pre_extra_mlp_layer_norm/scale/v_row
I0512 23:42:16.427234 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/pre_mlp_layer_norm/scale/m
I0512 23:42:16.427263 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/pre_mlp_layer_norm/scale/v
I0512 23:42:16.427293 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/pre_mlp_layer_norm/scale/v_col
I0512 23:42:16.427322 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/pre_mlp_layer_norm/scale/v_row
I0512 23:42:16.427352 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/pre_self_attention_layer_norm/scale/m
I0512 23:42:16.427381 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/pre_self_attention_layer_norm/scale/v
I0512 23:42:16.427411 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/pre_self_attention_layer_norm/scale/v_col
I0512 23:42:16.427446 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/pre_self_attention_layer_norm/scale/v_row
I0512 23:42:16.427477 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/self_attention/key/kernel/m
I0512 23:42:16.427507 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/self_attention/key/kernel/v
I0512 23:42:16.427536 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/self_attention/key/kernel/v_col
I0512 23:42:16.427566 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/self_attention/key/kernel/v_row
I0512 23:42:16.427596 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/self_attention/out/kernel/m
I0512 23:42:16.427625 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/self_attention/out/kernel/v
I0512 23:42:16.427655 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/self_attention/out/kernel/v_col
I0512 23:42:16.427684 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/self_attention/out/kernel/v_row
I0512 23:42:16.427714 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/self_attention/query/kernel/m
I0512 23:42:16.427743 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/self_attention/query/kernel/v
I0512 23:42:16.427773 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/self_attention/query/kernel/v_col
I0512 23:42:16.427803 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/self_attention/query/kernel/v_row
I0512 23:42:16.427832 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/self_attention/value/kernel/m
I0512 23:42:16.427861 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/self_attention/value/kernel/v
I0512 23:42:16.427891 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/self_attention/value/kernel/v_col
I0512 23:42:16.427921 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/self_attention/value/kernel/v_row
I0512 23:42:16.427950 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_3/mlp/wi_0/kernel/m
I0512 23:42:16.427980 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_3/mlp/wi_0/kernel/v
I0512 23:42:16.428010 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_3/mlp/wi_0/kernel/v_col
I0512 23:42:16.428040 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_3/mlp/wi_0/kernel/v_row
I0512 23:42:16.428069 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_3/mlp/wi_1/kernel/m
I0512 23:42:16.428099 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_3/mlp/wi_1/kernel/v
I0512 23:42:16.428129 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_3/mlp/wi_1/kernel/v_col
I0512 23:42:16.428159 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_3/mlp/wi_1/kernel/v_row
I0512 23:42:16.428190 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_3/mlp/wo/kernel/m
I0512 23:42:16.428221 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_3/mlp/wo/kernel/v
I0512 23:42:16.428252 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_3/mlp/wo/kernel/v_col
I0512 23:42:16.428282 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_3/mlp/wo/kernel/v_row
I0512 23:42:16.428312 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_3/pre_mlp_layer_norm/scale/m
I0512 23:42:16.428342 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_3/pre_mlp_layer_norm/scale/v
I0512 23:42:16.428372 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_3/pre_mlp_layer_norm/scale/v_col
I0512 23:42:16.428402 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_3/pre_mlp_layer_norm/scale/v_row
I0512 23:42:16.428436 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_3/pre_self_attention_layer_norm/scale/m
I0512 23:42:16.428468 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_3/pre_self_attention_layer_norm/scale/v
I0512 23:42:16.428498 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_3/pre_self_attention_layer_norm/scale/v_col
I0512 23:42:16.428528 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_3/pre_self_attention_layer_norm/scale/v_row
I0512 23:42:16.428558 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_3/self_attention/key/kernel/m
I0512 23:42:16.428588 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_3/self_attention/key/kernel/v
I0512 23:42:16.428618 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_3/self_attention/key/kernel/v_col
I0512 23:42:16.428648 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_3/self_attention/key/kernel/v_row
I0512 23:42:16.428678 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_3/self_attention/out/kernel/m
I0512 23:42:16.428708 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_3/self_attention/out/kernel/v
I0512 23:42:16.428738 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_3/self_attention/out/kernel/v_col
I0512 23:42:16.428767 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_3/self_attention/out/kernel/v_row
I0512 23:42:16.428797 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_3/self_attention/query/kernel/m
I0512 23:42:16.428827 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_3/self_attention/query/kernel/v
I0512 23:42:16.428857 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_3/self_attention/query/kernel/v_col
I0512 23:42:16.428887 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_3/self_attention/query/kernel/v_row
I0512 23:42:16.428917 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_3/self_attention/value/kernel/m
I0512 23:42:16.428946 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_3/self_attention/value/kernel/v
I0512 23:42:16.428998 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_3/self_attention/value/kernel/v_col
I0512 23:42:16.429030 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_3/self_attention/value/kernel/v_row
I0512 23:42:16.429061 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_4/mlp/wi_0/kernel/m
I0512 23:42:16.429091 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_4/mlp/wi_0/kernel/v
I0512 23:42:16.429121 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_4/mlp/wi_0/kernel/v_col
I0512 23:42:16.429152 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_4/mlp/wi_0/kernel/v_row
I0512 23:42:16.429183 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_4/mlp/wi_1/kernel/m
I0512 23:42:16.429214 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_4/mlp/wi_1/kernel/v
I0512 23:42:16.429244 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_4/mlp/wi_1/kernel/v_col
I0512 23:42:16.429274 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_4/mlp/wi_1/kernel/v_row
I0512 23:42:16.429304 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_4/mlp/wo/kernel/m
I0512 23:42:16.429334 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_4/mlp/wo/kernel/v
I0512 23:42:16.429364 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_4/mlp/wo/kernel/v_col
I0512 23:42:16.429394 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_4/mlp/wo/kernel/v_row
I0512 23:42:16.429424 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_4/pre_mlp_layer_norm/scale/m
I0512 23:42:16.429461 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_4/pre_mlp_layer_norm/scale/v
I0512 23:42:16.429491 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_4/pre_mlp_layer_norm/scale/v_col
I0512 23:42:16.429521 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_4/pre_mlp_layer_norm/scale/v_row
I0512 23:42:16.429551 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_4/pre_self_attention_layer_norm/scale/m
I0512 23:42:16.429581 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_4/pre_self_attention_layer_norm/scale/v
I0512 23:42:16.429611 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_4/pre_self_attention_layer_norm/scale/v_col
I0512 23:42:16.429641 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_4/pre_self_attention_layer_norm/scale/v_row
I0512 23:42:16.429671 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_4/self_attention/key/kernel/m
I0512 23:42:16.429701 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_4/self_attention/key/kernel/v
I0512 23:42:16.429730 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_4/self_attention/key/kernel/v_col
I0512 23:42:16.429760 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_4/self_attention/key/kernel/v_row
I0512 23:42:16.429790 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_4/self_attention/out/kernel/m
I0512 23:42:16.429820 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_4/self_attention/out/kernel/v
I0512 23:42:16.429850 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_4/self_attention/out/kernel/v_col
I0512 23:42:16.429880 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_4/self_attention/out/kernel/v_row
I0512 23:42:16.429909 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_4/self_attention/query/kernel/m
I0512 23:42:16.429939 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_4/self_attention/query/kernel/v
I0512 23:42:16.429970 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_4/self_attention/query/kernel/v_col
I0512 23:42:16.429999 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_4/self_attention/query/kernel/v_row
I0512 23:42:16.430029 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_4/self_attention/value/kernel/m
I0512 23:42:16.430058 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_4/self_attention/value/kernel/v
I0512 23:42:16.430088 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_4/self_attention/value/kernel/v_col
I0512 23:42:16.430118 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_4/self_attention/value/kernel/v_row
I0512 23:42:16.430148 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/extra_mlp/wi_0/kernel/m
I0512 23:42:16.430178 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/extra_mlp/wi_0/kernel/v
I0512 23:42:16.430211 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/extra_mlp/wi_0/kernel/v_col
I0512 23:42:16.430241 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/extra_mlp/wi_0/kernel/v_row
I0512 23:42:16.430270 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/extra_mlp/wi_1/kernel/m
I0512 23:42:16.430301 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/extra_mlp/wi_1/kernel/v
I0512 23:42:16.430331 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/extra_mlp/wi_1/kernel/v_col
I0512 23:42:16.430361 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/extra_mlp/wi_1/kernel/v_row
I0512 23:42:16.430391 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/extra_mlp/wo/kernel/m
I0512 23:42:16.430421 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/extra_mlp/wo/kernel/v
I0512 23:42:16.430458 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/extra_mlp/wo/kernel/v_col
I0512 23:42:16.430488 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/extra_mlp/wo/kernel/v_row
I0512 23:42:16.430518 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/mlp/expert/wi_0/kernel/m
I0512 23:42:16.430548 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/mlp/expert/wi_0/kernel/v
I0512 23:42:16.430577 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/mlp/expert/wi_0/kernel/v_col
I0512 23:42:16.430607 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/mlp/expert/wi_0/kernel/v_row
I0512 23:42:16.430637 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/mlp/expert/wi_1/kernel/m
I0512 23:42:16.430666 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/mlp/expert/wi_1/kernel/v
I0512 23:42:16.430696 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/mlp/expert/wi_1/kernel/v_col
I0512 23:42:16.430726 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/mlp/expert/wi_1/kernel/v_row
I0512 23:42:16.430755 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/mlp/expert/wo/kernel/m
I0512 23:42:16.430786 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/mlp/expert/wo/kernel/v
I0512 23:42:16.430816 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/mlp/expert/wo/kernel/v_col
I0512 23:42:16.430846 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/mlp/expert/wo/kernel/v_row
I0512 23:42:16.430876 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/mlp/router/router_weights/w/kernel/m
I0512 23:42:16.430905 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/mlp/router/router_weights/w/kernel/v
I0512 23:42:16.430935 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/mlp/router/router_weights/w/kernel/v_col
I0512 23:42:16.430964 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/mlp/router/router_weights/w/kernel/v_row
I0512 23:42:16.430994 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/pre_extra_mlp_layer_norm/scale/m
I0512 23:42:16.431024 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/pre_extra_mlp_layer_norm/scale/v
I0512 23:42:16.431054 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/pre_extra_mlp_layer_norm/scale/v_col
I0512 23:42:16.431083 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/pre_extra_mlp_layer_norm/scale/v_row
I0512 23:42:16.431113 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/pre_mlp_layer_norm/scale/m
I0512 23:42:16.431143 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/pre_mlp_layer_norm/scale/v
I0512 23:42:16.431172 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/pre_mlp_layer_norm/scale/v_col
I0512 23:42:16.431205 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/pre_mlp_layer_norm/scale/v_row
I0512 23:42:16.431235 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/pre_self_attention_layer_norm/scale/m
I0512 23:42:16.431266 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/pre_self_attention_layer_norm/scale/v
I0512 23:42:16.431295 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/pre_self_attention_layer_norm/scale/v_col
I0512 23:42:16.431325 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/pre_self_attention_layer_norm/scale/v_row
I0512 23:42:16.431355 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/self_attention/key/kernel/m
I0512 23:42:16.431385 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/self_attention/key/kernel/v
I0512 23:42:16.431414 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/self_attention/key/kernel/v_col
I0512 23:42:16.431450 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/self_attention/key/kernel/v_row
I0512 23:42:16.431480 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/self_attention/out/kernel/m
I0512 23:42:16.431510 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/self_attention/out/kernel/v
I0512 23:42:16.431540 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/self_attention/out/kernel/v_col
I0512 23:42:16.431569 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/self_attention/out/kernel/v_row
I0512 23:42:16.431599 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/self_attention/query/kernel/m
I0512 23:42:16.431628 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/self_attention/query/kernel/v
I0512 23:42:16.431658 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/self_attention/query/kernel/v_col
I0512 23:42:16.431687 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/self_attention/query/kernel/v_row
I0512 23:42:16.431717 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/self_attention/value/kernel/m
I0512 23:42:16.431747 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/self_attention/value/kernel/v
I0512 23:42:16.431777 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/self_attention/value/kernel/v_col
I0512 23:42:16.431806 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/self_attention/value/kernel/v_row
I0512 23:42:16.431836 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_6/mlp/wi_0/kernel/m
I0512 23:42:16.431866 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_6/mlp/wi_0/kernel/v
I0512 23:42:16.431896 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_6/mlp/wi_0/kernel/v_col
I0512 23:42:16.431926 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_6/mlp/wi_0/kernel/v_row
I0512 23:42:16.431956 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_6/mlp/wi_1/kernel/m
I0512 23:42:16.431985 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_6/mlp/wi_1/kernel/v
I0512 23:42:16.432015 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_6/mlp/wi_1/kernel/v_col
I0512 23:42:16.432045 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_6/mlp/wi_1/kernel/v_row
I0512 23:42:16.432075 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_6/mlp/wo/kernel/m
I0512 23:42:16.432105 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_6/mlp/wo/kernel/v
I0512 23:42:16.432134 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_6/mlp/wo/kernel/v_col
I0512 23:42:16.432164 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_6/mlp/wo/kernel/v_row
I0512 23:42:16.432196 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_6/pre_mlp_layer_norm/scale/m
I0512 23:42:16.432227 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_6/pre_mlp_layer_norm/scale/v
I0512 23:42:16.432257 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_6/pre_mlp_layer_norm/scale/v_col
I0512 23:42:16.432287 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_6/pre_mlp_layer_norm/scale/v_row
I0512 23:42:16.432317 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_6/pre_self_attention_layer_norm/scale/m
I0512 23:42:16.432348 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_6/pre_self_attention_layer_norm/scale/v
I0512 23:42:16.432377 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_6/pre_self_attention_layer_norm/scale/v_col
I0512 23:42:16.432407 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_6/pre_self_attention_layer_norm/scale/v_row
I0512 23:42:16.432443 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_6/self_attention/key/kernel/m
I0512 23:42:16.432474 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_6/self_attention/key/kernel/v
I0512 23:42:16.432504 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_6/self_attention/key/kernel/v_col
I0512 23:42:16.432534 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_6/self_attention/key/kernel/v_row
I0512 23:42:16.432564 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_6/self_attention/out/kernel/m
I0512 23:42:16.432594 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_6/self_attention/out/kernel/v
I0512 23:42:16.432624 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_6/self_attention/out/kernel/v_col
I0512 23:42:16.432653 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_6/self_attention/out/kernel/v_row
I0512 23:42:16.432683 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_6/self_attention/query/kernel/m
I0512 23:42:16.432713 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_6/self_attention/query/kernel/v
I0512 23:42:16.432743 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_6/self_attention/query/kernel/v_col
I0512 23:42:16.432773 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_6/self_attention/query/kernel/v_row
I0512 23:42:16.432803 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_6/self_attention/value/kernel/m
I0512 23:42:16.432832 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_6/self_attention/value/kernel/v
I0512 23:42:16.432862 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_6/self_attention/value/kernel/v_col
I0512 23:42:16.432892 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_6/self_attention/value/kernel/v_row
I0512 23:42:16.432922 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_7/mlp/wi_0/kernel/m
I0512 23:42:16.432952 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_7/mlp/wi_0/kernel/v
I0512 23:42:16.433005 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_7/mlp/wi_0/kernel/v_col
I0512 23:42:16.433037 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_7/mlp/wi_0/kernel/v_row
I0512 23:42:16.433067 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_7/mlp/wi_1/kernel/m
I0512 23:42:16.433097 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_7/mlp/wi_1/kernel/v
I0512 23:42:16.433127 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_7/mlp/wi_1/kernel/v_col
I0512 23:42:16.433157 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_7/mlp/wi_1/kernel/v_row
I0512 23:42:16.433188 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_7/mlp/wo/kernel/m
I0512 23:42:16.433220 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_7/mlp/wo/kernel/v
I0512 23:42:16.433250 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_7/mlp/wo/kernel/v_col
I0512 23:42:16.433280 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_7/mlp/wo/kernel/v_row
I0512 23:42:16.433310 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_7/pre_mlp_layer_norm/scale/m
I0512 23:42:16.433341 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_7/pre_mlp_layer_norm/scale/v
I0512 23:42:16.433370 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_7/pre_mlp_layer_norm/scale/v_col
I0512 23:42:16.433400 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_7/pre_mlp_layer_norm/scale/v_row
I0512 23:42:16.433435 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_7/pre_self_attention_layer_norm/scale/m
I0512 23:42:16.433467 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_7/pre_self_attention_layer_norm/scale/v
I0512 23:42:16.433497 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_7/pre_self_attention_layer_norm/scale/v_col
I0512 23:42:16.433527 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_7/pre_self_attention_layer_norm/scale/v_row
I0512 23:42:16.433557 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_7/self_attention/key/kernel/m
I0512 23:42:16.433587 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_7/self_attention/key/kernel/v
I0512 23:42:16.433617 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_7/self_attention/key/kernel/v_col
I0512 23:42:16.433647 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_7/self_attention/key/kernel/v_row
I0512 23:42:16.433677 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_7/self_attention/out/kernel/m
I0512 23:42:16.433707 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_7/self_attention/out/kernel/v
I0512 23:42:16.433737 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_7/self_attention/out/kernel/v_col
I0512 23:42:16.433767 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_7/self_attention/out/kernel/v_row
I0512 23:42:16.433797 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_7/self_attention/query/kernel/m
I0512 23:42:16.433827 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_7/self_attention/query/kernel/v
I0512 23:42:16.433857 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_7/self_attention/query/kernel/v_col
I0512 23:42:16.433887 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_7/self_attention/query/kernel/v_row
I0512 23:42:16.433916 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_7/self_attention/value/kernel/m
I0512 23:42:16.433946 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_7/self_attention/value/kernel/v
I0512 23:42:16.433976 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_7/self_attention/value/kernel/v_col
I0512 23:42:16.434005 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_7/self_attention/value/kernel/v_row
I0512 23:42:16.434035 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_8/mlp/wi_0/kernel/m
I0512 23:42:16.434065 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_8/mlp/wi_0/kernel/v
I0512 23:42:16.434096 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_8/mlp/wi_0/kernel/v_col
I0512 23:42:16.434126 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_8/mlp/wi_0/kernel/v_row
I0512 23:42:16.434156 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_8/mlp/wi_1/kernel/m
I0512 23:42:16.434187 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_8/mlp/wi_1/kernel/v
I0512 23:42:16.434218 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_8/mlp/wi_1/kernel/v_col
I0512 23:42:16.434248 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_8/mlp/wi_1/kernel/v_row
I0512 23:42:16.434278 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_8/mlp/wo/kernel/m
I0512 23:42:16.434308 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_8/mlp/wo/kernel/v
I0512 23:42:16.434338 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_8/mlp/wo/kernel/v_col
I0512 23:42:16.434368 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_8/mlp/wo/kernel/v_row
I0512 23:42:16.434398 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_8/pre_mlp_layer_norm/scale/m
I0512 23:42:16.434433 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_8/pre_mlp_layer_norm/scale/v
I0512 23:42:16.434465 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_8/pre_mlp_layer_norm/scale/v_col
I0512 23:42:16.434495 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_8/pre_mlp_layer_norm/scale/v_row
I0512 23:42:16.434525 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_8/pre_self_attention_layer_norm/scale/m
I0512 23:42:16.434555 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_8/pre_self_attention_layer_norm/scale/v
I0512 23:42:16.434585 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_8/pre_self_attention_layer_norm/scale/v_col
I0512 23:42:16.434615 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_8/pre_self_attention_layer_norm/scale/v_row
I0512 23:42:16.434645 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_8/self_attention/key/kernel/m
I0512 23:42:16.434675 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_8/self_attention/key/kernel/v
I0512 23:42:16.434704 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_8/self_attention/key/kernel/v_col
I0512 23:42:16.434734 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_8/self_attention/key/kernel/v_row
I0512 23:42:16.434764 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_8/self_attention/out/kernel/m
I0512 23:42:16.434794 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_8/self_attention/out/kernel/v
I0512 23:42:16.434824 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_8/self_attention/out/kernel/v_col
I0512 23:42:16.434853 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_8/self_attention/out/kernel/v_row
I0512 23:42:16.434883 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_8/self_attention/query/kernel/m
I0512 23:42:16.434913 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_8/self_attention/query/kernel/v
I0512 23:42:16.434942 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_8/self_attention/query/kernel/v_col
I0512 23:42:16.434972 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_8/self_attention/query/kernel/v_row
I0512 23:42:16.435002 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_8/self_attention/value/kernel/m
I0512 23:42:16.435032 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_8/self_attention/value/kernel/v
I0512 23:42:16.435061 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_8/self_attention/value/kernel/v_col
I0512 23:42:16.435091 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_8/self_attention/value/kernel/v_row
I0512 23:42:16.435121 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_9/mlp/wi_0/kernel/m
I0512 23:42:16.435151 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_9/mlp/wi_0/kernel/v
I0512 23:42:16.435182 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_9/mlp/wi_0/kernel/v_col
I0512 23:42:16.435214 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_9/mlp/wi_0/kernel/v_row
I0512 23:42:16.435244 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_9/mlp/wi_1/kernel/m
I0512 23:42:16.435275 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_9/mlp/wi_1/kernel/v
I0512 23:42:16.435304 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_9/mlp/wi_1/kernel/v_col
I0512 23:42:16.435334 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_9/mlp/wi_1/kernel/v_row
I0512 23:42:16.435364 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_9/mlp/wo/kernel/m
I0512 23:42:16.435394 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_9/mlp/wo/kernel/v
I0512 23:42:16.435429 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_9/mlp/wo/kernel/v_col
I0512 23:42:16.435460 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_9/mlp/wo/kernel/v_row
I0512 23:42:16.435491 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_9/pre_mlp_layer_norm/scale/m
I0512 23:42:16.435521 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_9/pre_mlp_layer_norm/scale/v
I0512 23:42:16.435550 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_9/pre_mlp_layer_norm/scale/v_col
I0512 23:42:16.435580 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_9/pre_mlp_layer_norm/scale/v_row
I0512 23:42:16.435611 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_9/pre_self_attention_layer_norm/scale/m
I0512 23:42:16.435641 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_9/pre_self_attention_layer_norm/scale/v
I0512 23:42:16.435671 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_9/pre_self_attention_layer_norm/scale/v_col
I0512 23:42:16.435701 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_9/pre_self_attention_layer_norm/scale/v_row
I0512 23:42:16.435731 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_9/self_attention/key/kernel/m
I0512 23:42:16.435761 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_9/self_attention/key/kernel/v
I0512 23:42:16.435791 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_9/self_attention/key/kernel/v_col
I0512 23:42:16.435821 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_9/self_attention/key/kernel/v_row
I0512 23:42:16.435851 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_9/self_attention/out/kernel/m
I0512 23:42:16.435881 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_9/self_attention/out/kernel/v
I0512 23:42:16.435911 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_9/self_attention/out/kernel/v_col
I0512 23:42:16.435941 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_9/self_attention/out/kernel/v_row
I0512 23:42:16.435970 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_9/self_attention/query/kernel/m
I0512 23:42:16.436000 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_9/self_attention/query/kernel/v
I0512 23:42:16.436029 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_9/self_attention/query/kernel/v_col
I0512 23:42:16.436059 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_9/self_attention/query/kernel/v_row
I0512 23:42:16.436089 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_9/self_attention/value/kernel/m
I0512 23:42:16.436119 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_9/self_attention/value/kernel/v
I0512 23:42:16.436149 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_9/self_attention/value/kernel/v_col
I0512 23:42:16.436179 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_9/self_attention/value/kernel/v_row
I0512 23:42:16.436211 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/logits_dense/kernel/m
I0512 23:42:16.436241 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/logits_dense/kernel/v
I0512 23:42:16.436271 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/logits_dense/kernel/v_col
I0512 23:42:16.436301 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/logits_dense/kernel/v_row
I0512 23:42:16.436331 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/token_embedder/embedding/m
I0512 23:42:16.436361 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/token_embedder/embedding/v
I0512 23:42:16.436391 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/token_embedder/embedding/v_col
I0512 23:42:16.436421 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/param_states/token_embedder/embedding/v_row
I0512 23:42:16.436457 140595318249472 checkpoints.py:1100] Restoring key from ckpt: state/step
I0512 23:42:16.436488 140595318249472 checkpoints.py:1100] Restoring key from ckpt: target/decoder/decoder_norm/scale
I0512 23:42:16.436519 140595318249472 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_0/mlp/wi_0/kernel
I0512 23:42:16.436549 140595318249472 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_0/mlp/wi_1/kernel
I0512 23:42:16.436579 140595318249472 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_0/mlp/wo/kernel
I0512 23:42:16.436610 140595318249472 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_0/pre_mlp_layer_norm/scale
I0512 23:42:16.436640 140595318249472 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_0/pre_self_attention_layer_norm/scale
I0512 23:42:16.436670 140595318249472 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_0/self_attention/key/kernel
I0512 23:42:16.436700 140595318249472 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_0/self_attention/out/kernel
I0512 23:42:16.436730 140595318249472 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_0/self_attention/query/kernel
I0512 23:42:16.436760 140595318249472 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_0/self_attention/value/kernel
I0512 23:42:16.436790 140595318249472 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_1/mlp/wi_0/kernel
I0512 23:42:16.436820 140595318249472 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_1/mlp/wi_1/kernel
I0512 23:42:16.436850 140595318249472 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_1/mlp/wo/kernel
I0512 23:42:16.436880 140595318249472 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_1/pre_mlp_layer_norm/scale
I0512 23:42:16.436910 140595318249472 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_1/pre_self_attention_layer_norm/scale
I0512 23:42:16.436940 140595318249472 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_1/self_attention/key/kernel
I0512 23:42:16.436997 140595318249472 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_1/self_attention/out/kernel
I0512 23:42:16.437030 140595318249472 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_1/self_attention/query/kernel
I0512 23:42:16.437060 140595318249472 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_1/self_attention/value/kernel
I0512 23:42:16.437091 140595318249472 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_10/mlp/wi_0/kernel
I0512 23:42:16.437121 140595318249472 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_10/mlp/wi_1/kernel
I0512 23:42:16.437151 140595318249472 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_10/mlp/wo/kernel
I0512 23:42:16.437182 140595318249472 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_10/pre_mlp_layer_norm/scale
I0512 23:42:16.437214 140595318249472 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_10/pre_self_attention_layer_norm/scale
I0512 23:42:16.437245 140595318249472 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_10/self_attention/key/kernel
I0512 23:42:16.437275 140595318249472 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_10/self_attention/out/kernel
I0512 23:42:16.437305 140595318249472 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_10/self_attention/query/kernel
I0512 23:42:16.437335 140595318249472 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_10/self_attention/value/kernel
I0512 23:42:16.437366 140595318249472 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_11/extra_mlp/wi_0/kernel
I0512 23:42:16.437396 140595318249472 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_11/extra_mlp/wi_1/kernel
I0512 23:42:16.437430 140595318249472 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_11/extra_mlp/wo/kernel
I0512 23:42:16.437462 140595318249472 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_11/mlp/expert/wi_0/kernel
I0512 23:42:16.437492 140595318249472 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_11/mlp/expert/wi_1/kernel
I0512 23:42:16.437522 140595318249472 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_11/mlp/expert/wo/kernel
I0512 23:42:16.437552 140595318249472 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_11/mlp/router/router_weights/w/kernel
I0512 23:42:16.437582 140595318249472 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_11/pre_extra_mlp_layer_norm/scale
I0512 23:42:16.437612 140595318249472 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_11/pre_mlp_layer_norm/scale
I0512 23:42:16.437642 140595318249472 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_11/pre_self_attention_layer_norm/scale
I0512 23:42:16.437672 140595318249472 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_11/self_attention/key/kernel
I0512 23:42:16.437702 140595318249472 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_11/self_attention/out/kernel
I0512 23:42:16.437732 140595318249472 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_11/self_attention/query/kernel
I0512 23:42:16.437762 140595318249472 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_11/self_attention/value/kernel
I0512 23:42:16.437792 140595318249472 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_12/mlp/wi_0/kernel
I0512 23:42:16.437822 140595318249472 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_12/mlp/wi_1/kernel
I0512 23:42:16.437852 140595318249472 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_12/mlp/wo/kernel
I0512 23:42:16.437882 140595318249472 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_12/pre_mlp_layer_norm/scale
I0512 23:42:16.437912 140595318249472 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_12/pre_self_attention_layer_norm/scale
I0512 23:42:16.437942 140595318249472 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_12/self_attention/key/kernel
I0512 23:42:16.437972 140595318249472 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_12/self_attention/out/kernel
I0512 23:42:16.438002 140595318249472 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_12/self_attention/query/kernel
I0512 23:42:16.438032 140595318249472 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_12/self_attention/value/kernel
I0512 23:42:16.438062 140595318249472 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_13/mlp/wi_0/kernel
I0512 23:42:16.438092 140595318249472 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_13/mlp/wi_1/kernel
I0512 23:42:16.438122 140595318249472 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_13/mlp/wo/kernel
I0512 23:42:16.438152 140595318249472 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_13/pre_mlp_layer_norm/scale
I0512 23:42:16.438183 140595318249472 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_13/pre_self_attention_layer_norm/scale
I0512 23:42:16.438215 140595318249472 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_13/self_attention/key/kernel
I0512 23:42:16.438245 140595318249472 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_13/self_attention/out/kernel
I0512 23:42:16.438276 140595318249472 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_13/self_attention/query/kernel
I0512 23:42:16.438305 140595318249472 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_13/self_attention/value/kernel
I0512 23:42:16.438335 140595318249472 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_14/mlp/wi_0/kernel
I0512 23:42:16.438365 140595318249472 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_14/mlp/wi_1/kernel
I0512 23:42:16.438395 140595318249472 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_14/mlp/wo/kernel
I0512 23:42:16.438432 140595318249472 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_14/pre_mlp_layer_norm/scale
I0512 23:42:16.438464 140595318249472 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_14/pre_self_attention_layer_norm/scale
I0512 23:42:16.438495 140595318249472 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_14/self_attention/key/kernel
I0512 23:42:16.438525 140595318249472 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_14/self_attention/out/kernel
I0512 23:42:16.438555 140595318249472 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_14/self_attention/query/kernel
I0512 23:42:16.438585 140595318249472 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_14/self_attention/value/kernel
I0512 23:42:16.438615 140595318249472 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_15/mlp/wi_0/kernel
I0512 23:42:16.438646 140595318249472 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_15/mlp/wi_1/kernel
I0512 23:42:16.438675 140595318249472 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_15/mlp/wo/kernel
I0512 23:42:16.438705 140595318249472 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_15/pre_mlp_layer_norm/scale
I0512 23:42:16.438735 140595318249472 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_15/pre_self_attention_layer_norm/scale
I0512 23:42:16.438766 140595318249472 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_15/self_attention/key/kernel
I0512 23:42:16.438796 140595318249472 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_15/self_attention/out/kernel
I0512 23:42:16.438826 140595318249472 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_15/self_attention/query/kernel
I0512 23:42:16.438856 140595318249472 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_15/self_attention/value/kernel
I0512 23:42:16.438886 140595318249472 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_16/mlp/wi_0/kernel
I0512 23:42:16.438915 140595318249472 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_16/mlp/wi_1/kernel
I0512 23:42:16.438945 140595318249472 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_16/mlp/wo/kernel
I0512 23:42:16.438975 140595318249472 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_16/pre_mlp_layer_norm/scale
I0512 23:42:16.439005 140595318249472 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_16/pre_self_attention_layer_norm/scale
I0512 23:42:16.439036 140595318249472 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_16/self_attention/key/kernel
I0512 23:42:16.439065 140595318249472 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_16/self_attention/out/kernel
I0512 23:42:16.439095 140595318249472 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_16/self_attention/query/kernel
I0512 23:42:16.439124 140595318249472 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_16/self_attention/value/kernel
I0512 23:42:16.439153 140595318249472 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_17/extra_mlp/wi_0/kernel
I0512 23:42:16.439184 140595318249472 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_17/extra_mlp/wi_1/kernel
I0512 23:42:16.439216 140595318249472 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_17/extra_mlp/wo/kernel
I0512 23:42:16.439245 140595318249472 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_17/mlp/expert/wi_0/kernel
I0512 23:42:16.439275 140595318249472 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_17/mlp/expert/wi_1/kernel
I0512 23:42:16.439304 140595318249472 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_17/mlp/expert/wo/kernel
I0512 23:42:16.439333 140595318249472 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_17/mlp/router/router_weights/w/kernel
I0512 23:42:16.439363 140595318249472 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_17/pre_extra_mlp_layer_norm/scale
I0512 23:42:16.439393 140595318249472 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_17/pre_mlp_layer_norm/scale
I0512 23:42:16.439422 140595318249472 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_17/pre_self_attention_layer_norm/scale
I0512 23:42:16.439458 140595318249472 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_17/self_attention/key/kernel
I0512 23:42:16.439487 140595318249472 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_17/self_attention/out/kernel
I0512 23:42:16.439517 140595318249472 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_17/self_attention/query/kernel
I0512 23:42:16.439546 140595318249472 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_17/self_attention/value/kernel
I0512 23:42:16.439576 140595318249472 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_18/mlp/wi_0/kernel
I0512 23:42:16.439605 140595318249472 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_18/mlp/wi_1/kernel
I0512 23:42:16.439635 140595318249472 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_18/mlp/wo/kernel
I0512 23:42:16.439664 140595318249472 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_18/pre_mlp_layer_norm/scale
I0512 23:42:16.439694 140595318249472 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_18/pre_self_attention_layer_norm/scale
I0512 23:42:16.439723 140595318249472 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_18/self_attention/key/kernel
I0512 23:42:16.439753 140595318249472 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_18/self_attention/out/kernel
I0512 23:42:16.439783 140595318249472 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_18/self_attention/query/kernel
I0512 23:42:16.439812 140595318249472 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_18/self_attention/value/kernel
I0512 23:42:16.439841 140595318249472 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_19/mlp/wi_0/kernel
I0512 23:42:16.439871 140595318249472 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_19/mlp/wi_1/kernel
I0512 23:42:16.439900 140595318249472 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_19/mlp/wo/kernel
I0512 23:42:16.439929 140595318249472 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_19/pre_mlp_layer_norm/scale
I0512 23:42:16.439959 140595318249472 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_19/pre_self_attention_layer_norm/scale
I0512 23:42:16.439988 140595318249472 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_19/self_attention/key/kernel
I0512 23:42:16.440018 140595318249472 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_19/self_attention/out/kernel
I0512 23:42:16.440047 140595318249472 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_19/self_attention/query/kernel
I0512 23:42:16.440077 140595318249472 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_19/self_attention/value/kernel
I0512 23:42:16.440106 140595318249472 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_2/mlp/wi_0/kernel
I0512 23:42:16.440135 140595318249472 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_2/mlp/wi_1/kernel
I0512 23:42:16.440165 140595318249472 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_2/mlp/wo/kernel
I0512 23:42:16.440196 140595318249472 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_2/pre_mlp_layer_norm/scale
I0512 23:42:16.440227 140595318249472 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_2/pre_self_attention_layer_norm/scale
I0512 23:42:16.440256 140595318249472 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_2/self_attention/key/kernel
I0512 23:42:16.440286 140595318249472 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_2/self_attention/out/kernel
I0512 23:42:16.440315 140595318249472 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_2/self_attention/query/kernel
I0512 23:42:16.440344 140595318249472 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_2/self_attention/value/kernel
I0512 23:42:16.440374 140595318249472 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_20/mlp/wi_0/kernel
I0512 23:42:16.440403 140595318249472 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_20/mlp/wi_1/kernel
I0512 23:42:16.440438 140595318249472 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_20/mlp/wo/kernel
I0512 23:42:16.440469 140595318249472 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_20/pre_mlp_layer_norm/scale
I0512 23:42:16.440499 140595318249472 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_20/pre_self_attention_layer_norm/scale
I0512 23:42:16.440528 140595318249472 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_20/self_attention/key/kernel
I0512 23:42:16.440558 140595318249472 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_20/self_attention/out/kernel
I0512 23:42:16.440587 140595318249472 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_20/self_attention/query/kernel
I0512 23:42:16.440617 140595318249472 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_20/self_attention/value/kernel
I0512 23:42:16.440646 140595318249472 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_21/mlp/wi_0/kernel
I0512 23:42:16.440676 140595318249472 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_21/mlp/wi_1/kernel
I0512 23:42:16.440705 140595318249472 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_21/mlp/wo/kernel
I0512 23:42:16.440734 140595318249472 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_21/pre_mlp_layer_norm/scale
I0512 23:42:16.440764 140595318249472 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_21/pre_self_attention_layer_norm/scale
I0512 23:42:16.440794 140595318249472 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_21/self_attention/key/kernel
I0512 23:42:16.440824 140595318249472 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_21/self_attention/out/kernel
I0512 23:42:16.440853 140595318249472 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_21/self_attention/query/kernel
I0512 23:42:16.440882 140595318249472 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_21/self_attention/value/kernel
I0512 23:42:16.440912 140595318249472 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_22/mlp/wi_0/kernel
I0512 23:42:16.440941 140595318249472 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_22/mlp/wi_1/kernel
I0512 23:42:16.440990 140595318249472 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_22/mlp/wo/kernel
I0512 23:42:16.441022 140595318249472 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_22/pre_mlp_layer_norm/scale
I0512 23:42:16.441052 140595318249472 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_22/pre_self_attention_layer_norm/scale
I0512 23:42:16.441082 140595318249472 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_22/self_attention/key/kernel
I0512 23:42:16.441112 140595318249472 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_22/self_attention/out/kernel
I0512 23:42:16.441142 140595318249472 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_22/self_attention/query/kernel
I0512 23:42:16.441171 140595318249472 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_22/self_attention/value/kernel
I0512 23:42:16.441203 140595318249472 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_23/extra_mlp/wi_0/kernel
I0512 23:42:16.441233 140595318249472 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_23/extra_mlp/wi_1/kernel
I0512 23:42:16.441262 140595318249472 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_23/extra_mlp/wo/kernel
I0512 23:42:16.441292 140595318249472 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_23/mlp/expert/wi_0/kernel
I0512 23:42:16.441321 140595318249472 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_23/mlp/expert/wi_1/kernel
I0512 23:42:16.441350 140595318249472 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_23/mlp/expert/wo/kernel
I0512 23:42:16.441380 140595318249472 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_23/mlp/router/router_weights/w/kernel
I0512 23:42:16.441410 140595318249472 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_23/pre_extra_mlp_layer_norm/scale
I0512 23:42:16.441445 140595318249472 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_23/pre_mlp_layer_norm/scale
I0512 23:42:16.441475 140595318249472 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_23/pre_self_attention_layer_norm/scale
I0512 23:42:16.441504 140595318249472 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_23/self_attention/key/kernel
I0512 23:42:16.441534 140595318249472 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_23/self_attention/out/kernel
I0512 23:42:16.441564 140595318249472 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_23/self_attention/query/kernel
I0512 23:42:16.441594 140595318249472 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_23/self_attention/value/kernel
I0512 23:42:16.441623 140595318249472 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_3/mlp/wi_0/kernel
I0512 23:42:16.441653 140595318249472 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_3/mlp/wi_1/kernel
I0512 23:42:16.441682 140595318249472 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_3/mlp/wo/kernel
I0512 23:42:16.441711 140595318249472 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_3/pre_mlp_layer_norm/scale
I0512 23:42:16.441741 140595318249472 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_3/pre_self_attention_layer_norm/scale
I0512 23:42:16.441771 140595318249472 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_3/self_attention/key/kernel
I0512 23:42:16.441800 140595318249472 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_3/self_attention/out/kernel
I0512 23:42:16.441829 140595318249472 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_3/self_attention/query/kernel
I0512 23:42:16.441858 140595318249472 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_3/self_attention/value/kernel
I0512 23:42:16.441888 140595318249472 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_4/mlp/wi_0/kernel
I0512 23:42:16.441917 140595318249472 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_4/mlp/wi_1/kernel
I0512 23:42:16.441946 140595318249472 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_4/mlp/wo/kernel
I0512 23:42:16.441976 140595318249472 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_4/pre_mlp_layer_norm/scale
I0512 23:42:16.442005 140595318249472 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_4/pre_self_attention_layer_norm/scale
I0512 23:42:16.442035 140595318249472 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_4/self_attention/key/kernel
I0512 23:42:16.442064 140595318249472 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_4/self_attention/out/kernel
I0512 23:42:16.442093 140595318249472 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_4/self_attention/query/kernel
I0512 23:42:16.442123 140595318249472 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_4/self_attention/value/kernel
I0512 23:42:16.442152 140595318249472 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_5/extra_mlp/wi_0/kernel
I0512 23:42:16.442182 140595318249472 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_5/extra_mlp/wi_1/kernel
I0512 23:42:16.442213 140595318249472 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_5/extra_mlp/wo/kernel
I0512 23:42:16.442243 140595318249472 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_5/mlp/expert/wi_0/kernel
I0512 23:42:16.442272 140595318249472 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_5/mlp/expert/wi_1/kernel
I0512 23:42:16.442302 140595318249472 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_5/mlp/expert/wo/kernel
I0512 23:42:16.442332 140595318249472 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_5/mlp/router/router_weights/w/kernel
I0512 23:42:16.442361 140595318249472 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_5/pre_extra_mlp_layer_norm/scale
I0512 23:42:16.442391 140595318249472 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_5/pre_mlp_layer_norm/scale
I0512 23:42:16.442420 140595318249472 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_5/pre_self_attention_layer_norm/scale
I0512 23:42:16.442455 140595318249472 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_5/self_attention/key/kernel
I0512 23:42:16.442486 140595318249472 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_5/self_attention/out/kernel
I0512 23:42:16.442515 140595318249472 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_5/self_attention/query/kernel
I0512 23:42:16.442545 140595318249472 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_5/self_attention/value/kernel
I0512 23:42:16.442574 140595318249472 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_6/mlp/wi_0/kernel
I0512 23:42:16.442604 140595318249472 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_6/mlp/wi_1/kernel
I0512 23:42:16.442633 140595318249472 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_6/mlp/wo/kernel
I0512 23:42:16.442663 140595318249472 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_6/pre_mlp_layer_norm/scale
I0512 23:42:16.442692 140595318249472 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_6/pre_self_attention_layer_norm/scale
I0512 23:42:16.442722 140595318249472 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_6/self_attention/key/kernel
I0512 23:42:16.442751 140595318249472 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_6/self_attention/out/kernel
I0512 23:42:16.442780 140595318249472 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_6/self_attention/query/kernel
I0512 23:42:16.442810 140595318249472 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_6/self_attention/value/kernel
I0512 23:42:16.442839 140595318249472 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_7/mlp/wi_0/kernel
I0512 23:42:16.442868 140595318249472 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_7/mlp/wi_1/kernel
I0512 23:42:16.442898 140595318249472 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_7/mlp/wo/kernel
I0512 23:42:16.442927 140595318249472 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_7/pre_mlp_layer_norm/scale
I0512 23:42:16.442957 140595318249472 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_7/pre_self_attention_layer_norm/scale
I0512 23:42:16.442986 140595318249472 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_7/self_attention/key/kernel
I0512 23:42:16.443016 140595318249472 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_7/self_attention/out/kernel
I0512 23:42:16.443045 140595318249472 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_7/self_attention/query/kernel
I0512 23:42:16.443075 140595318249472 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_7/self_attention/value/kernel
I0512 23:42:16.443104 140595318249472 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_8/mlp/wi_0/kernel
I0512 23:42:16.443133 140595318249472 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_8/mlp/wi_1/kernel
I0512 23:42:16.443162 140595318249472 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_8/mlp/wo/kernel
I0512 23:42:16.443194 140595318249472 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_8/pre_mlp_layer_norm/scale
I0512 23:42:16.443224 140595318249472 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_8/pre_self_attention_layer_norm/scale
I0512 23:42:16.443254 140595318249472 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_8/self_attention/key/kernel
I0512 23:42:16.443284 140595318249472 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_8/self_attention/out/kernel
I0512 23:42:16.443313 140595318249472 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_8/self_attention/query/kernel
I0512 23:42:16.443342 140595318249472 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_8/self_attention/value/kernel
I0512 23:42:16.443372 140595318249472 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_9/mlp/wi_0/kernel
I0512 23:42:16.443401 140595318249472 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_9/mlp/wi_1/kernel
I0512 23:42:16.443436 140595318249472 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_9/mlp/wo/kernel
I0512 23:42:16.443467 140595318249472 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_9/pre_mlp_layer_norm/scale
I0512 23:42:16.443497 140595318249472 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_9/pre_self_attention_layer_norm/scale
I0512 23:42:16.443527 140595318249472 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_9/self_attention/key/kernel
I0512 23:42:16.443557 140595318249472 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_9/self_attention/out/kernel
I0512 23:42:16.443614 140595318249472 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_9/self_attention/query/kernel
I0512 23:42:16.443646 140595318249472 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_9/self_attention/value/kernel
I0512 23:42:16.443676 140595318249472 checkpoints.py:1100] Restoring key from ckpt: target/decoder/logits_dense/kernel
I0512 23:42:16.443705 140595318249472 checkpoints.py:1100] Restoring key from ckpt: target/token_embedder/embedding
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
I0000 00:00:1715557336.473927  450135 gcs_resource.cc:109] Using default AdmissionQueue with limit 32
I0000 00:00:1715557336.478419  452297 google_auth_provider.cc:180] Running on GCE, using service account 1081612546678-compute@developer.gserviceaccount.com
I0512 23:42:16.528897 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/decoder_norm/scale/m
I0512 23:42:16.529066 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/decoder_norm/scale/v
I0512 23:42:16.529107 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/decoder_norm/scale/v_col
I0512 23:42:16.529142 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/decoder_norm/scale/v_row
I0512 23:42:16.529175 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_0/mlp/wi_0/kernel/m
I0512 23:42:16.529209 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_0/mlp/wi_0/kernel/v
I0512 23:42:16.529243 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_0/mlp/wi_0/kernel/v_col
I0512 23:42:16.529278 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_0/mlp/wi_0/kernel/v_row
I0512 23:42:16.529311 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_0/mlp/wi_1/kernel/m
I0512 23:42:16.529345 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_0/mlp/wi_1/kernel/v
I0512 23:42:16.529379 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_0/mlp/wi_1/kernel/v_col
I0512 23:42:16.529412 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_0/mlp/wi_1/kernel/v_row
I0512 23:42:16.529446 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_0/mlp/wo/kernel/m
I0512 23:42:16.529479 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_0/mlp/wo/kernel/v
I0512 23:42:16.529513 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_0/mlp/wo/kernel/v_col
I0512 23:42:16.529569 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_0/mlp/wo/kernel/v_row
I0512 23:42:16.529608 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_0/pre_mlp_layer_norm/scale/m
I0512 23:42:16.529643 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_0/pre_mlp_layer_norm/scale/v
I0512 23:42:16.529677 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_0/pre_mlp_layer_norm/scale/v_col
I0512 23:42:16.529710 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_0/pre_mlp_layer_norm/scale/v_row
I0512 23:42:16.529744 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_0/pre_self_attention_layer_norm/scale/m
I0512 23:42:16.529778 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_0/pre_self_attention_layer_norm/scale/v
I0512 23:42:16.529816 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_0/pre_self_attention_layer_norm/scale/v_col
I0512 23:42:16.529852 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_0/pre_self_attention_layer_norm/scale/v_row
I0512 23:42:16.529886 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_0/self_attention/key/kernel/m
I0512 23:42:16.529920 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_0/self_attention/key/kernel/v
I0512 23:42:16.529954 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_0/self_attention/key/kernel/v_col
I0512 23:42:16.529987 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_0/self_attention/key/kernel/v_row
I0512 23:42:16.530021 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_0/self_attention/out/kernel/m
I0512 23:42:16.530055 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_0/self_attention/out/kernel/v
I0512 23:42:16.530091 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_0/self_attention/out/kernel/v_col
I0512 23:42:16.530125 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_0/self_attention/out/kernel/v_row
I0512 23:42:16.530158 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_0/self_attention/query/kernel/m
I0512 23:42:16.530192 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_0/self_attention/query/kernel/v
I0512 23:42:16.530226 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_0/self_attention/query/kernel/v_col
I0512 23:42:16.530259 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_0/self_attention/query/kernel/v_row
I0512 23:42:16.530293 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_0/self_attention/value/kernel/m
I0512 23:42:16.530327 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_0/self_attention/value/kernel/v
I0512 23:42:16.530360 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_0/self_attention/value/kernel/v_col
I0512 23:42:16.530394 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_0/self_attention/value/kernel/v_row
I0512 23:42:16.530427 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_1/mlp/wi_0/kernel/m
I0512 23:42:16.530461 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_1/mlp/wi_0/kernel/v
I0512 23:42:16.530494 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_1/mlp/wi_0/kernel/v_col
I0512 23:42:16.530529 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_1/mlp/wi_0/kernel/v_row
I0512 23:42:16.530562 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_1/mlp/wi_1/kernel/m
I0512 23:42:16.530596 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_1/mlp/wi_1/kernel/v
I0512 23:42:16.530629 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_1/mlp/wi_1/kernel/v_col
I0512 23:42:16.530663 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_1/mlp/wi_1/kernel/v_row
I0512 23:42:16.530696 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_1/mlp/wo/kernel/m
I0512 23:42:16.530730 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_1/mlp/wo/kernel/v
I0512 23:42:16.530764 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_1/mlp/wo/kernel/v_col
I0512 23:42:16.530797 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_1/mlp/wo/kernel/v_row
I0512 23:42:16.530837 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_1/pre_mlp_layer_norm/scale/m
I0512 23:42:16.530872 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_1/pre_mlp_layer_norm/scale/v
I0512 23:42:16.530906 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_1/pre_mlp_layer_norm/scale/v_col
I0512 23:42:16.530939 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_1/pre_mlp_layer_norm/scale/v_row
I0512 23:42:16.530973 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_1/pre_self_attention_layer_norm/scale/m
I0512 23:42:16.531007 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_1/pre_self_attention_layer_norm/scale/v
I0512 23:42:16.531041 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_1/pre_self_attention_layer_norm/scale/v_col
I0512 23:42:16.531077 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_1/pre_self_attention_layer_norm/scale/v_row
I0512 23:42:16.531112 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_1/self_attention/key/kernel/m
I0512 23:42:16.531146 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_1/self_attention/key/kernel/v
I0512 23:42:16.531179 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_1/self_attention/key/kernel/v_col
I0512 23:42:16.531213 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_1/self_attention/key/kernel/v_row
I0512 23:42:16.531246 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_1/self_attention/out/kernel/m
I0512 23:42:16.531280 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_1/self_attention/out/kernel/v
I0512 23:42:16.531313 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_1/self_attention/out/kernel/v_col
I0512 23:42:16.531347 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_1/self_attention/out/kernel/v_row
I0512 23:42:16.531380 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_1/self_attention/query/kernel/m
I0512 23:42:16.531414 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_1/self_attention/query/kernel/v
I0512 23:42:16.531447 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_1/self_attention/query/kernel/v_col
I0512 23:42:16.531480 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_1/self_attention/query/kernel/v_row
I0512 23:42:16.531514 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_1/self_attention/value/kernel/m
I0512 23:42:16.531547 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_1/self_attention/value/kernel/v
I0512 23:42:16.531581 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_1/self_attention/value/kernel/v_col
I0512 23:42:16.531614 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_1/self_attention/value/kernel/v_row
I0512 23:42:16.531647 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_10/mlp/wi_0/kernel/m
I0512 23:42:16.531681 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_10/mlp/wi_0/kernel/v
I0512 23:42:16.531715 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_10/mlp/wi_0/kernel/v_col
I0512 23:42:16.531749 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_10/mlp/wi_0/kernel/v_row
I0512 23:42:16.531782 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_10/mlp/wi_1/kernel/m
I0512 23:42:16.531821 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_10/mlp/wi_1/kernel/v
I0512 23:42:16.531857 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_10/mlp/wi_1/kernel/v_col
I0512 23:42:16.531890 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_10/mlp/wi_1/kernel/v_row
I0512 23:42:16.531924 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_10/mlp/wo/kernel/m
I0512 23:42:16.531958 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_10/mlp/wo/kernel/v
I0512 23:42:16.531991 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_10/mlp/wo/kernel/v_col
I0512 23:42:16.532025 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_10/mlp/wo/kernel/v_row
I0512 23:42:16.532059 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_10/pre_mlp_layer_norm/scale/m
I0512 23:42:16.532095 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_10/pre_mlp_layer_norm/scale/v
I0512 23:42:16.532129 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_10/pre_mlp_layer_norm/scale/v_col
I0512 23:42:16.532163 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_10/pre_mlp_layer_norm/scale/v_row
I0512 23:42:16.532197 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_10/pre_self_attention_layer_norm/scale/m
I0512 23:42:16.532230 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_10/pre_self_attention_layer_norm/scale/v
I0512 23:42:16.532264 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_10/pre_self_attention_layer_norm/scale/v_col
I0512 23:42:16.532297 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_10/pre_self_attention_layer_norm/scale/v_row
I0512 23:42:16.532331 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_10/self_attention/key/kernel/m
I0512 23:42:16.532365 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_10/self_attention/key/kernel/v
I0512 23:42:16.532398 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_10/self_attention/key/kernel/v_col
I0512 23:42:16.532431 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_10/self_attention/key/kernel/v_row
I0512 23:42:16.532465 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_10/self_attention/out/kernel/m
I0512 23:42:16.532499 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_10/self_attention/out/kernel/v
I0512 23:42:16.532532 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_10/self_attention/out/kernel/v_col
I0512 23:42:16.532566 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_10/self_attention/out/kernel/v_row
I0512 23:42:16.532600 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_10/self_attention/query/kernel/m
I0512 23:42:16.532633 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_10/self_attention/query/kernel/v
I0512 23:42:16.532667 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_10/self_attention/query/kernel/v_col
I0512 23:42:16.532700 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_10/self_attention/query/kernel/v_row
I0512 23:42:16.532734 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_10/self_attention/value/kernel/m
I0512 23:42:16.532767 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_10/self_attention/value/kernel/v
I0512 23:42:16.532801 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_10/self_attention/value/kernel/v_col
I0512 23:42:16.532841 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_10/self_attention/value/kernel/v_row
I0512 23:42:16.532876 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/extra_mlp/wi_0/kernel/m
I0512 23:42:16.532909 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/extra_mlp/wi_0/kernel/v
I0512 23:42:16.532943 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/extra_mlp/wi_0/kernel/v_col
I0512 23:42:16.532977 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/extra_mlp/wi_0/kernel/v_row
I0512 23:42:16.533010 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/extra_mlp/wi_1/kernel/m
I0512 23:42:16.533044 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/extra_mlp/wi_1/kernel/v
I0512 23:42:16.533079 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/extra_mlp/wi_1/kernel/v_col
I0512 23:42:16.533114 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/extra_mlp/wi_1/kernel/v_row
I0512 23:42:16.533146 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/extra_mlp/wo/kernel/m
I0512 23:42:16.533178 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/extra_mlp/wo/kernel/v
I0512 23:42:16.533210 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/extra_mlp/wo/kernel/v_col
I0512 23:42:16.533242 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/extra_mlp/wo/kernel/v_row
I0512 23:42:16.533273 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/mlp/expert/wi_0/kernel/m
I0512 23:42:16.533305 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/mlp/expert/wi_0/kernel/v
I0512 23:42:16.533336 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/mlp/expert/wi_0/kernel/v_col
I0512 23:42:16.533368 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/mlp/expert/wi_0/kernel/v_row
I0512 23:42:16.533400 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/mlp/expert/wi_1/kernel/m
I0512 23:42:16.533431 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/mlp/expert/wi_1/kernel/v
I0512 23:42:16.533463 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/mlp/expert/wi_1/kernel/v_col
I0512 23:42:16.533495 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/mlp/expert/wi_1/kernel/v_row
I0512 23:42:16.533527 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/mlp/expert/wo/kernel/m
I0512 23:42:16.533589 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/mlp/expert/wo/kernel/v
I0512 23:42:16.533624 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/mlp/expert/wo/kernel/v_col
I0512 23:42:16.533656 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/mlp/expert/wo/kernel/v_row
I0512 23:42:16.533688 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/mlp/router/router_weights/w/kernel/m
I0512 23:42:16.533725 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/mlp/router/router_weights/w/kernel/v
I0512 23:42:16.533757 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/mlp/router/router_weights/w/kernel/v_col
I0512 23:42:16.533789 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/mlp/router/router_weights/w/kernel/v_row
I0512 23:42:16.534072 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/pre_extra_mlp_layer_norm/scale/m
I0512 23:42:16.534114 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/pre_extra_mlp_layer_norm/scale/v
I0512 23:42:16.534147 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/pre_extra_mlp_layer_norm/scale/v_col
I0512 23:42:16.534178 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/pre_extra_mlp_layer_norm/scale/v_row
I0512 23:42:16.534209 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/pre_mlp_layer_norm/scale/m
I0512 23:42:16.534241 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/pre_mlp_layer_norm/scale/v
I0512 23:42:16.534272 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/pre_mlp_layer_norm/scale/v_col
I0512 23:42:16.534303 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/pre_mlp_layer_norm/scale/v_row
I0512 23:42:16.534334 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/pre_self_attention_layer_norm/scale/m
I0512 23:42:16.534365 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/pre_self_attention_layer_norm/scale/v
I0512 23:42:16.534396 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/pre_self_attention_layer_norm/scale/v_col
I0512 23:42:16.534427 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/pre_self_attention_layer_norm/scale/v_row
I0512 23:42:16.534458 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/self_attention/key/kernel/m
I0512 23:42:16.534489 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/self_attention/key/kernel/v
I0512 23:42:16.534520 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/self_attention/key/kernel/v_col
I0512 23:42:16.534551 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/self_attention/key/kernel/v_row
I0512 23:42:16.534582 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/self_attention/out/kernel/m
I0512 23:42:16.534614 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/self_attention/out/kernel/v
I0512 23:42:16.534645 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/self_attention/out/kernel/v_col
I0512 23:42:16.534676 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/self_attention/out/kernel/v_row
I0512 23:42:16.534708 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/self_attention/query/kernel/m
I0512 23:42:16.534739 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/self_attention/query/kernel/v
I0512 23:42:16.534770 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/self_attention/query/kernel/v_col
I0512 23:42:16.534801 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/self_attention/query/kernel/v_row
I0512 23:42:16.535232 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/self_attention/value/kernel/m
I0512 23:42:16.535268 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/self_attention/value/kernel/v
I0512 23:42:16.535299 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/self_attention/value/kernel/v_col
I0512 23:42:16.535330 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/self_attention/value/kernel/v_row
I0512 23:42:16.535361 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_12/mlp/wi_0/kernel/m
I0512 23:42:16.535392 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_12/mlp/wi_0/kernel/v
I0512 23:42:16.535423 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_12/mlp/wi_0/kernel/v_col
I0512 23:42:16.535454 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_12/mlp/wi_0/kernel/v_row
I0512 23:42:16.535485 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_12/mlp/wi_1/kernel/m
I0512 23:42:16.535517 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_12/mlp/wi_1/kernel/v
I0512 23:42:16.535548 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_12/mlp/wi_1/kernel/v_col
I0512 23:42:16.535580 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_12/mlp/wi_1/kernel/v_row
I0512 23:42:16.535612 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_12/mlp/wo/kernel/m
I0512 23:42:16.535643 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_12/mlp/wo/kernel/v
I0512 23:42:16.535674 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_12/mlp/wo/kernel/v_col
I0512 23:42:16.535707 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_12/mlp/wo/kernel/v_row
I0512 23:42:16.535739 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_12/pre_mlp_layer_norm/scale/m
I0512 23:42:16.535770 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_12/pre_mlp_layer_norm/scale/v
I0512 23:42:16.535802 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_12/pre_mlp_layer_norm/scale/v_col
I0512 23:42:16.535843 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_12/pre_mlp_layer_norm/scale/v_row
I0512 23:42:16.535875 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_12/pre_self_attention_layer_norm/scale/m
I0512 23:42:16.535907 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_12/pre_self_attention_layer_norm/scale/v
I0512 23:42:16.535938 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_12/pre_self_attention_layer_norm/scale/v_col
I0512 23:42:16.535969 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_12/pre_self_attention_layer_norm/scale/v_row
I0512 23:42:16.536031 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_12/self_attention/key/kernel/m
I0512 23:42:16.536063 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_12/self_attention/key/kernel/v
I0512 23:42:16.536098 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_12/self_attention/key/kernel/v_col
I0512 23:42:16.536130 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_12/self_attention/key/kernel/v_row
I0512 23:42:16.536161 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_12/self_attention/out/kernel/m
I0512 23:42:16.536193 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_12/self_attention/out/kernel/v
I0512 23:42:16.536224 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_12/self_attention/out/kernel/v_col
I0512 23:42:16.536255 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_12/self_attention/out/kernel/v_row
I0512 23:42:16.536286 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_12/self_attention/query/kernel/m
I0512 23:42:16.536318 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_12/self_attention/query/kernel/v
I0512 23:42:16.536349 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_12/self_attention/query/kernel/v_col
I0512 23:42:16.536380 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_12/self_attention/query/kernel/v_row
I0512 23:42:16.536411 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_12/self_attention/value/kernel/m
I0512 23:42:16.536442 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_12/self_attention/value/kernel/v
I0512 23:42:16.536473 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_12/self_attention/value/kernel/v_col
I0512 23:42:16.536504 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_12/self_attention/value/kernel/v_row
I0512 23:42:16.536537 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_13/mlp/wi_0/kernel/m
I0512 23:42:16.536570 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_13/mlp/wi_0/kernel/v
I0512 23:42:16.536602 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_13/mlp/wi_0/kernel/v_col
I0512 23:42:16.536633 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_13/mlp/wi_0/kernel/v_row
I0512 23:42:16.536665 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_13/mlp/wi_1/kernel/m
I0512 23:42:16.536697 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_13/mlp/wi_1/kernel/v
I0512 23:42:16.536728 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_13/mlp/wi_1/kernel/v_col
I0512 23:42:16.536760 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_13/mlp/wi_1/kernel/v_row
I0512 23:42:16.536792 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_13/mlp/wo/kernel/m
I0512 23:42:16.536836 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_13/mlp/wo/kernel/v
I0512 23:42:16.536870 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_13/mlp/wo/kernel/v_col
I0512 23:42:16.536902 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_13/mlp/wo/kernel/v_row
I0512 23:42:16.536934 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_13/pre_mlp_layer_norm/scale/m
I0512 23:42:16.536965 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_13/pre_mlp_layer_norm/scale/v
I0512 23:42:16.536997 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_13/pre_mlp_layer_norm/scale/v_col
I0512 23:42:16.537032 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_13/pre_mlp_layer_norm/scale/v_row
I0512 23:42:16.537066 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_13/pre_self_attention_layer_norm/scale/m
I0512 23:42:16.537315 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_13/pre_self_attention_layer_norm/scale/v
I0512 23:42:16.537348 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_13/pre_self_attention_layer_norm/scale/v_col
I0512 23:42:16.537380 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_13/pre_self_attention_layer_norm/scale/v_row
I0512 23:42:16.537412 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_13/self_attention/key/kernel/m
I0512 23:42:16.537443 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_13/self_attention/key/kernel/v
I0512 23:42:16.537474 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_13/self_attention/key/kernel/v_col
I0512 23:42:16.537505 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_13/self_attention/key/kernel/v_row
I0512 23:42:16.537536 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_13/self_attention/out/kernel/m
I0512 23:42:16.537591 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_13/self_attention/out/kernel/v
I0512 23:42:16.537624 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_13/self_attention/out/kernel/v_col
I0512 23:42:16.537655 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_13/self_attention/out/kernel/v_row
I0512 23:42:16.537686 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_13/self_attention/query/kernel/m
I0512 23:42:16.537717 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_13/self_attention/query/kernel/v
I0512 23:42:16.537749 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_13/self_attention/query/kernel/v_col
I0512 23:42:16.537780 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_13/self_attention/query/kernel/v_row
I0512 23:42:16.537811 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_13/self_attention/value/kernel/m
I0512 23:42:16.537849 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_13/self_attention/value/kernel/v
I0512 23:42:16.537880 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_13/self_attention/value/kernel/v_col
I0512 23:42:16.537911 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_13/self_attention/value/kernel/v_row
I0512 23:42:16.537942 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_14/mlp/wi_0/kernel/m
I0512 23:42:16.537973 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_14/mlp/wi_0/kernel/v
I0512 23:42:16.538003 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_14/mlp/wi_0/kernel/v_col
I0512 23:42:16.538034 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_14/mlp/wi_0/kernel/v_row
I0512 23:42:16.538066 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_14/mlp/wi_1/kernel/m
I0512 23:42:16.538100 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_14/mlp/wi_1/kernel/v
I0512 23:42:16.538131 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_14/mlp/wi_1/kernel/v_col
I0512 23:42:16.538161 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_14/mlp/wi_1/kernel/v_row
I0512 23:42:16.538192 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_14/mlp/wo/kernel/m
I0512 23:42:16.538223 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_14/mlp/wo/kernel/v
I0512 23:42:16.538254 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_14/mlp/wo/kernel/v_col
I0512 23:42:16.538284 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_14/mlp/wo/kernel/v_row
I0512 23:42:16.538315 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_14/pre_mlp_layer_norm/scale/m
I0512 23:42:16.538346 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_14/pre_mlp_layer_norm/scale/v
I0512 23:42:16.538377 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_14/pre_mlp_layer_norm/scale/v_col
I0512 23:42:16.538408 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_14/pre_mlp_layer_norm/scale/v_row
I0512 23:42:16.538439 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_14/pre_self_attention_layer_norm/scale/m
I0512 23:42:16.538470 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_14/pre_self_attention_layer_norm/scale/v
I0512 23:42:16.538501 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_14/pre_self_attention_layer_norm/scale/v_col
I0512 23:42:16.538532 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_14/pre_self_attention_layer_norm/scale/v_row
I0512 23:42:16.538563 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_14/self_attention/key/kernel/m
I0512 23:42:16.538594 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_14/self_attention/key/kernel/v
I0512 23:42:16.538624 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_14/self_attention/key/kernel/v_col
I0512 23:42:16.538655 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_14/self_attention/key/kernel/v_row
I0512 23:42:16.538686 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_14/self_attention/out/kernel/m
I0512 23:42:16.538717 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_14/self_attention/out/kernel/v
I0512 23:42:16.538747 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_14/self_attention/out/kernel/v_col
I0512 23:42:16.538778 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_14/self_attention/out/kernel/v_row
I0512 23:42:16.538809 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_14/self_attention/query/kernel/m
I0512 23:42:16.538848 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_14/self_attention/query/kernel/v
I0512 23:42:16.538879 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_14/self_attention/query/kernel/v_col
I0512 23:42:16.538910 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_14/self_attention/query/kernel/v_row
I0512 23:42:16.538941 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_14/self_attention/value/kernel/m
I0512 23:42:16.538971 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_14/self_attention/value/kernel/v
I0512 23:42:16.539002 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_14/self_attention/value/kernel/v_col
I0512 23:42:16.539033 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_14/self_attention/value/kernel/v_row
I0512 23:42:16.539064 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_15/mlp/wi_0/kernel/m
I0512 23:42:16.539097 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_15/mlp/wi_0/kernel/v
I0512 23:42:16.539128 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_15/mlp/wi_0/kernel/v_col
I0512 23:42:16.539159 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_15/mlp/wi_0/kernel/v_row
I0512 23:42:16.539190 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_15/mlp/wi_1/kernel/m
I0512 23:42:16.539221 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_15/mlp/wi_1/kernel/v
I0512 23:42:16.539251 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_15/mlp/wi_1/kernel/v_col
I0512 23:42:16.539282 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_15/mlp/wi_1/kernel/v_row
I0512 23:42:16.539313 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_15/mlp/wo/kernel/m
I0512 23:42:16.539344 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_15/mlp/wo/kernel/v
I0512 23:42:16.539375 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_15/mlp/wo/kernel/v_col
I0512 23:42:16.539406 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_15/mlp/wo/kernel/v_row
I0512 23:42:16.539437 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_15/pre_mlp_layer_norm/scale/m
I0512 23:42:16.539468 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_15/pre_mlp_layer_norm/scale/v
I0512 23:42:16.539499 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_15/pre_mlp_layer_norm/scale/v_col
I0512 23:42:16.539529 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_15/pre_mlp_layer_norm/scale/v_row
I0512 23:42:16.539560 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_15/pre_self_attention_layer_norm/scale/m
I0512 23:42:16.539591 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_15/pre_self_attention_layer_norm/scale/v
I0512 23:42:16.539623 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_15/pre_self_attention_layer_norm/scale/v_col
I0512 23:42:16.539653 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_15/pre_self_attention_layer_norm/scale/v_row
I0512 23:42:16.539684 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_15/self_attention/key/kernel/m
I0512 23:42:16.539714 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_15/self_attention/key/kernel/v
I0512 23:42:16.539745 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_15/self_attention/key/kernel/v_col
I0512 23:42:16.539776 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_15/self_attention/key/kernel/v_row
I0512 23:42:16.539807 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_15/self_attention/out/kernel/m
I0512 23:42:16.539844 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_15/self_attention/out/kernel/v
I0512 23:42:16.539875 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_15/self_attention/out/kernel/v_col
I0512 23:42:16.539906 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_15/self_attention/out/kernel/v_row
I0512 23:42:16.539937 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_15/self_attention/query/kernel/m
I0512 23:42:16.539968 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_15/self_attention/query/kernel/v
I0512 23:42:16.539999 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_15/self_attention/query/kernel/v_col
I0512 23:42:16.540029 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_15/self_attention/query/kernel/v_row
I0512 23:42:16.540060 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_15/self_attention/value/kernel/m
I0512 23:42:16.540093 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_15/self_attention/value/kernel/v
I0512 23:42:16.540124 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_15/self_attention/value/kernel/v_col
I0512 23:42:16.540155 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_15/self_attention/value/kernel/v_row
I0512 23:42:16.540186 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_16/mlp/wi_0/kernel/m
I0512 23:42:16.540223 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_16/mlp/wi_0/kernel/v
I0512 23:42:16.540260 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_16/mlp/wi_0/kernel/v_col
I0512 23:42:16.540292 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_16/mlp/wi_0/kernel/v_row
I0512 23:42:16.540324 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_16/mlp/wi_1/kernel/m
I0512 23:42:16.540360 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_16/mlp/wi_1/kernel/v
I0512 23:42:16.540394 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_16/mlp/wi_1/kernel/v_col
I0512 23:42:16.540425 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_16/mlp/wi_1/kernel/v_row
I0512 23:42:16.540456 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_16/mlp/wo/kernel/m
I0512 23:42:16.540486 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_16/mlp/wo/kernel/v
I0512 23:42:16.540516 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_16/mlp/wo/kernel/v_col
I0512 23:42:16.540546 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_16/mlp/wo/kernel/v_row
I0512 23:42:16.540576 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_16/pre_mlp_layer_norm/scale/m
I0512 23:42:16.540607 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_16/pre_mlp_layer_norm/scale/v
I0512 23:42:16.540637 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_16/pre_mlp_layer_norm/scale/v_col
I0512 23:42:16.540667 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_16/pre_mlp_layer_norm/scale/v_row
I0512 23:42:16.540697 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_16/pre_self_attention_layer_norm/scale/m
I0512 23:42:16.540726 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_16/pre_self_attention_layer_norm/scale/v
I0512 23:42:16.540755 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_16/pre_self_attention_layer_norm/scale/v_col
I0512 23:42:16.540786 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_16/pre_self_attention_layer_norm/scale/v_row
I0512 23:42:16.540823 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_16/self_attention/key/kernel/m
I0512 23:42:16.540855 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_16/self_attention/key/kernel/v
I0512 23:42:16.540885 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_16/self_attention/key/kernel/v_col
I0512 23:42:16.540915 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_16/self_attention/key/kernel/v_row
I0512 23:42:16.540945 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_16/self_attention/out/kernel/m
I0512 23:42:16.540974 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_16/self_attention/out/kernel/v
I0512 23:42:16.541003 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_16/self_attention/out/kernel/v_col
I0512 23:42:16.541033 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_16/self_attention/out/kernel/v_row
I0512 23:42:16.541063 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_16/self_attention/query/kernel/m
I0512 23:42:16.541318 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_16/self_attention/query/kernel/v
I0512 23:42:16.541352 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_16/self_attention/query/kernel/v_col
I0512 23:42:16.541382 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_16/self_attention/query/kernel/v_row
I0512 23:42:16.541412 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_16/self_attention/value/kernel/m
I0512 23:42:16.541442 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_16/self_attention/value/kernel/v
I0512 23:42:16.541471 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_16/self_attention/value/kernel/v_col
I0512 23:42:16.541501 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_16/self_attention/value/kernel/v_row
I0512 23:42:16.541531 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/extra_mlp/wi_0/kernel/m
I0512 23:42:16.541586 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/extra_mlp/wi_0/kernel/v
I0512 23:42:16.541618 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/extra_mlp/wi_0/kernel/v_col
I0512 23:42:16.541648 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/extra_mlp/wi_0/kernel/v_row
I0512 23:42:16.541678 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/extra_mlp/wi_1/kernel/m
I0512 23:42:16.541708 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/extra_mlp/wi_1/kernel/v
I0512 23:42:16.541737 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/extra_mlp/wi_1/kernel/v_col
I0512 23:42:16.541767 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/extra_mlp/wi_1/kernel/v_row
I0512 23:42:16.541797 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/extra_mlp/wo/kernel/m
I0512 23:42:16.541836 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/extra_mlp/wo/kernel/v
I0512 23:42:16.541867 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/extra_mlp/wo/kernel/v_col
I0512 23:42:16.541898 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/extra_mlp/wo/kernel/v_row
I0512 23:42:16.541928 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/mlp/expert/wi_0/kernel/m
I0512 23:42:16.541957 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/mlp/expert/wi_0/kernel/v
I0512 23:42:16.541987 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/mlp/expert/wi_0/kernel/v_col
I0512 23:42:16.542016 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/mlp/expert/wi_0/kernel/v_row
I0512 23:42:16.542047 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/mlp/expert/wi_1/kernel/m
I0512 23:42:16.542078 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/mlp/expert/wi_1/kernel/v
I0512 23:42:16.542109 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/mlp/expert/wi_1/kernel/v_col
I0512 23:42:16.542139 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/mlp/expert/wi_1/kernel/v_row
I0512 23:42:16.542169 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/mlp/expert/wo/kernel/m
I0512 23:42:16.542199 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/mlp/expert/wo/kernel/v
I0512 23:42:16.542229 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/mlp/expert/wo/kernel/v_col
I0512 23:42:16.542258 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/mlp/expert/wo/kernel/v_row
I0512 23:42:16.542288 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/mlp/router/router_weights/w/kernel/m
I0512 23:42:16.542317 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/mlp/router/router_weights/w/kernel/v
I0512 23:42:16.542347 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/mlp/router/router_weights/w/kernel/v_col
I0512 23:42:16.542377 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/mlp/router/router_weights/w/kernel/v_row
I0512 23:42:16.542407 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/pre_extra_mlp_layer_norm/scale/m
I0512 23:42:16.542436 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/pre_extra_mlp_layer_norm/scale/v
I0512 23:42:16.542466 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/pre_extra_mlp_layer_norm/scale/v_col
I0512 23:42:16.542496 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/pre_extra_mlp_layer_norm/scale/v_row
I0512 23:42:16.542526 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/pre_mlp_layer_norm/scale/m
I0512 23:42:16.542555 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/pre_mlp_layer_norm/scale/v
I0512 23:42:16.542585 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/pre_mlp_layer_norm/scale/v_col
I0512 23:42:16.542614 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/pre_mlp_layer_norm/scale/v_row
I0512 23:42:16.542644 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/pre_self_attention_layer_norm/scale/m
I0512 23:42:16.542674 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/pre_self_attention_layer_norm/scale/v
I0512 23:42:16.542703 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/pre_self_attention_layer_norm/scale/v_col
I0512 23:42:16.542734 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/pre_self_attention_layer_norm/scale/v_row
I0512 23:42:16.542763 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/self_attention/key/kernel/m
I0512 23:42:16.542793 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/self_attention/key/kernel/v
I0512 23:42:16.542830 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/self_attention/key/kernel/v_col
I0512 23:42:16.542861 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/self_attention/key/kernel/v_row
I0512 23:42:16.542891 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/self_attention/out/kernel/m
I0512 23:42:16.542921 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/self_attention/out/kernel/v
I0512 23:42:16.542950 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/self_attention/out/kernel/v_col
I0512 23:42:16.542980 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/self_attention/out/kernel/v_row
I0512 23:42:16.543009 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/self_attention/query/kernel/m
I0512 23:42:16.543039 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/self_attention/query/kernel/v
I0512 23:42:16.543070 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/self_attention/query/kernel/v_col
I0512 23:42:16.543102 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/self_attention/query/kernel/v_row
I0512 23:42:16.543132 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/self_attention/value/kernel/m
I0512 23:42:16.543162 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/self_attention/value/kernel/v
I0512 23:42:16.543192 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/self_attention/value/kernel/v_col
I0512 23:42:16.543222 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/self_attention/value/kernel/v_row
I0512 23:42:16.543252 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_18/mlp/wi_0/kernel/m
I0512 23:42:16.543282 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_18/mlp/wi_0/kernel/v
I0512 23:42:16.543312 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_18/mlp/wi_0/kernel/v_col
I0512 23:42:16.543342 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_18/mlp/wi_0/kernel/v_row
I0512 23:42:16.543372 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_18/mlp/wi_1/kernel/m
I0512 23:42:16.543402 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_18/mlp/wi_1/kernel/v
I0512 23:42:16.543431 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_18/mlp/wi_1/kernel/v_col
I0512 23:42:16.543461 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_18/mlp/wi_1/kernel/v_row
I0512 23:42:16.543491 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_18/mlp/wo/kernel/m
I0512 23:42:16.543521 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_18/mlp/wo/kernel/v
I0512 23:42:16.543550 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_18/mlp/wo/kernel/v_col
I0512 23:42:16.543580 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_18/mlp/wo/kernel/v_row
I0512 23:42:16.543609 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_18/pre_mlp_layer_norm/scale/m
I0512 23:42:16.543639 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_18/pre_mlp_layer_norm/scale/v
I0512 23:42:16.543669 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_18/pre_mlp_layer_norm/scale/v_col
I0512 23:42:16.543699 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_18/pre_mlp_layer_norm/scale/v_row
I0512 23:42:16.543729 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_18/pre_self_attention_layer_norm/scale/m
I0512 23:42:16.543758 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_18/pre_self_attention_layer_norm/scale/v
I0512 23:42:16.543788 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_18/pre_self_attention_layer_norm/scale/v_col
I0512 23:42:16.543823 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_18/pre_self_attention_layer_norm/scale/v_row
I0512 23:42:16.543854 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_18/self_attention/key/kernel/m
I0512 23:42:16.543884 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_18/self_attention/key/kernel/v
I0512 23:42:16.543914 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_18/self_attention/key/kernel/v_col
I0512 23:42:16.543943 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_18/self_attention/key/kernel/v_row
I0512 23:42:16.543973 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_18/self_attention/out/kernel/m
I0512 23:42:16.544002 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_18/self_attention/out/kernel/v
I0512 23:42:16.544032 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_18/self_attention/out/kernel/v_col
I0512 23:42:16.544062 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_18/self_attention/out/kernel/v_row
I0512 23:42:16.544315 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_18/self_attention/query/kernel/m
I0512 23:42:16.544350 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_18/self_attention/query/kernel/v
I0512 23:42:16.544380 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_18/self_attention/query/kernel/v_col
I0512 23:42:16.544409 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_18/self_attention/query/kernel/v_row
I0512 23:42:16.544439 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_18/self_attention/value/kernel/m
I0512 23:42:16.544468 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_18/self_attention/value/kernel/v
I0512 23:42:16.544498 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_18/self_attention/value/kernel/v_col
I0512 23:42:16.544528 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_18/self_attention/value/kernel/v_row
I0512 23:42:16.544557 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_19/mlp/wi_0/kernel/m
I0512 23:42:16.544588 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_19/mlp/wi_0/kernel/v
I0512 23:42:16.544617 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_19/mlp/wi_0/kernel/v_col
I0512 23:42:16.544647 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_19/mlp/wi_0/kernel/v_row
I0512 23:42:16.544677 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_19/mlp/wi_1/kernel/m
I0512 23:42:16.544707 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_19/mlp/wi_1/kernel/v
I0512 23:42:16.544736 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_19/mlp/wi_1/kernel/v_col
I0512 23:42:16.544766 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_19/mlp/wi_1/kernel/v_row
I0512 23:42:16.544795 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_19/mlp/wo/kernel/m
I0512 23:42:16.544838 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_19/mlp/wo/kernel/v
I0512 23:42:16.544870 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_19/mlp/wo/kernel/v_col
I0512 23:42:16.544900 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_19/mlp/wo/kernel/v_row
I0512 23:42:16.544929 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_19/pre_mlp_layer_norm/scale/m
I0512 23:42:16.544959 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_19/pre_mlp_layer_norm/scale/v
I0512 23:42:16.544989 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_19/pre_mlp_layer_norm/scale/v_col
I0512 23:42:16.545018 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_19/pre_mlp_layer_norm/scale/v_row
I0512 23:42:16.545048 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_19/pre_self_attention_layer_norm/scale/m
I0512 23:42:16.545289 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_19/pre_self_attention_layer_norm/scale/v
I0512 23:42:16.545327 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_19/pre_self_attention_layer_norm/scale/v_col
I0512 23:42:16.545357 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_19/pre_self_attention_layer_norm/scale/v_row
I0512 23:42:16.545387 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_19/self_attention/key/kernel/m
I0512 23:42:16.545417 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_19/self_attention/key/kernel/v
I0512 23:42:16.545447 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_19/self_attention/key/kernel/v_col
I0512 23:42:16.545476 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_19/self_attention/key/kernel/v_row
I0512 23:42:16.545506 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_19/self_attention/out/kernel/m
I0512 23:42:16.545536 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_19/self_attention/out/kernel/v
I0512 23:42:16.545587 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_19/self_attention/out/kernel/v_col
I0512 23:42:16.545619 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_19/self_attention/out/kernel/v_row
I0512 23:42:16.545649 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_19/self_attention/query/kernel/m
I0512 23:42:16.545679 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_19/self_attention/query/kernel/v
I0512 23:42:16.545709 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_19/self_attention/query/kernel/v_col
I0512 23:42:16.545738 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_19/self_attention/query/kernel/v_row
I0512 23:42:16.545768 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_19/self_attention/value/kernel/m
I0512 23:42:16.545798 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_19/self_attention/value/kernel/v
I0512 23:42:16.545834 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_19/self_attention/value/kernel/v_col
I0512 23:42:16.545864 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_19/self_attention/value/kernel/v_row
I0512 23:42:16.545894 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_2/mlp/wi_0/kernel/m
I0512 23:42:16.545924 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_2/mlp/wi_0/kernel/v
I0512 23:42:16.545954 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_2/mlp/wi_0/kernel/v_col
I0512 23:42:16.545984 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_2/mlp/wi_0/kernel/v_row
I0512 23:42:16.546014 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_2/mlp/wi_1/kernel/m
I0512 23:42:16.546043 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_2/mlp/wi_1/kernel/v
I0512 23:42:16.546074 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_2/mlp/wi_1/kernel/v_col
I0512 23:42:16.546106 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_2/mlp/wi_1/kernel/v_row
I0512 23:42:16.546136 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_2/mlp/wo/kernel/m
I0512 23:42:16.546165 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_2/mlp/wo/kernel/v
I0512 23:42:16.546195 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_2/mlp/wo/kernel/v_col
I0512 23:42:16.546225 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_2/mlp/wo/kernel/v_row
I0512 23:42:16.546255 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_2/pre_mlp_layer_norm/scale/m
I0512 23:42:16.546284 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_2/pre_mlp_layer_norm/scale/v
I0512 23:42:16.546314 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_2/pre_mlp_layer_norm/scale/v_col
I0512 23:42:16.546344 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_2/pre_mlp_layer_norm/scale/v_row
I0512 23:42:16.546374 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_2/pre_self_attention_layer_norm/scale/m
I0512 23:42:16.546404 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_2/pre_self_attention_layer_norm/scale/v
I0512 23:42:16.546434 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_2/pre_self_attention_layer_norm/scale/v_col
I0512 23:42:16.546463 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_2/pre_self_attention_layer_norm/scale/v_row
I0512 23:42:16.546493 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_2/self_attention/key/kernel/m
I0512 23:42:16.546522 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_2/self_attention/key/kernel/v
I0512 23:42:16.546552 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_2/self_attention/key/kernel/v_col
I0512 23:42:16.546581 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_2/self_attention/key/kernel/v_row
I0512 23:42:16.546611 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_2/self_attention/out/kernel/m
I0512 23:42:16.546641 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_2/self_attention/out/kernel/v
I0512 23:42:16.546671 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_2/self_attention/out/kernel/v_col
I0512 23:42:16.546700 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_2/self_attention/out/kernel/v_row
I0512 23:42:16.546730 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_2/self_attention/query/kernel/m
I0512 23:42:16.546760 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_2/self_attention/query/kernel/v
I0512 23:42:16.546789 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_2/self_attention/query/kernel/v_col
I0512 23:42:16.546825 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_2/self_attention/query/kernel/v_row
I0512 23:42:16.546856 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_2/self_attention/value/kernel/m
I0512 23:42:16.546886 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_2/self_attention/value/kernel/v
I0512 23:42:16.546916 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_2/self_attention/value/kernel/v_col
I0512 23:42:16.546945 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_2/self_attention/value/kernel/v_row
I0512 23:42:16.546974 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_20/mlp/wi_0/kernel/m
I0512 23:42:16.547004 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_20/mlp/wi_0/kernel/v
I0512 23:42:16.547034 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_20/mlp/wi_0/kernel/v_col
I0512 23:42:16.547065 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_20/mlp/wi_0/kernel/v_row
I0512 23:42:16.547097 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_20/mlp/wi_1/kernel/m
I0512 23:42:16.547127 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_20/mlp/wi_1/kernel/v
I0512 23:42:16.547157 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_20/mlp/wi_1/kernel/v_col
I0512 23:42:16.547186 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_20/mlp/wi_1/kernel/v_row
I0512 23:42:16.547216 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_20/mlp/wo/kernel/m
I0512 23:42:16.547246 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_20/mlp/wo/kernel/v
I0512 23:42:16.547275 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_20/mlp/wo/kernel/v_col
I0512 23:42:16.547305 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_20/mlp/wo/kernel/v_row
I0512 23:42:16.547335 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_20/pre_mlp_layer_norm/scale/m
I0512 23:42:16.547364 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_20/pre_mlp_layer_norm/scale/v
I0512 23:42:16.547394 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_20/pre_mlp_layer_norm/scale/v_col
I0512 23:42:16.547424 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_20/pre_mlp_layer_norm/scale/v_row
I0512 23:42:16.547453 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_20/pre_self_attention_layer_norm/scale/m
I0512 23:42:16.547483 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_20/pre_self_attention_layer_norm/scale/v
I0512 23:42:16.547513 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_20/pre_self_attention_layer_norm/scale/v_col
I0512 23:42:16.547543 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_20/pre_self_attention_layer_norm/scale/v_row
I0512 23:42:16.547573 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_20/self_attention/key/kernel/m
I0512 23:42:16.547602 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_20/self_attention/key/kernel/v
I0512 23:42:16.547632 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_20/self_attention/key/kernel/v_col
I0512 23:42:16.547662 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_20/self_attention/key/kernel/v_row
I0512 23:42:16.547691 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_20/self_attention/out/kernel/m
I0512 23:42:16.547721 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_20/self_attention/out/kernel/v
I0512 23:42:16.547751 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_20/self_attention/out/kernel/v_col
I0512 23:42:16.547780 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_20/self_attention/out/kernel/v_row
I0512 23:42:16.547809 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_20/self_attention/query/kernel/m
I0512 23:42:16.547846 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_20/self_attention/query/kernel/v
I0512 23:42:16.547877 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_20/self_attention/query/kernel/v_col
I0512 23:42:16.547907 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_20/self_attention/query/kernel/v_row
I0512 23:42:16.547936 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_20/self_attention/value/kernel/m
I0512 23:42:16.547966 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_20/self_attention/value/kernel/v
I0512 23:42:16.547996 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_20/self_attention/value/kernel/v_col
I0512 23:42:16.548025 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_20/self_attention/value/kernel/v_row
I0512 23:42:16.548055 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_21/mlp/wi_0/kernel/m
I0512 23:42:16.548087 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_21/mlp/wi_0/kernel/v
I0512 23:42:16.548118 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_21/mlp/wi_0/kernel/v_col
I0512 23:42:16.548148 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_21/mlp/wi_0/kernel/v_row
I0512 23:42:16.548178 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_21/mlp/wi_1/kernel/m
I0512 23:42:16.548207 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_21/mlp/wi_1/kernel/v
I0512 23:42:16.548237 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_21/mlp/wi_1/kernel/v_col
I0512 23:42:16.548267 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_21/mlp/wi_1/kernel/v_row
I0512 23:42:16.548296 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_21/mlp/wo/kernel/m
I0512 23:42:16.548326 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_21/mlp/wo/kernel/v
I0512 23:42:16.548356 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_21/mlp/wo/kernel/v_col
I0512 23:42:16.548386 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_21/mlp/wo/kernel/v_row
I0512 23:42:16.548415 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_21/pre_mlp_layer_norm/scale/m
I0512 23:42:16.548445 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_21/pre_mlp_layer_norm/scale/v
I0512 23:42:16.548475 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_21/pre_mlp_layer_norm/scale/v_col
I0512 23:42:16.548505 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_21/pre_mlp_layer_norm/scale/v_row
I0512 23:42:16.548534 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_21/pre_self_attention_layer_norm/scale/m
I0512 23:42:16.548564 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_21/pre_self_attention_layer_norm/scale/v
I0512 23:42:16.548594 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_21/pre_self_attention_layer_norm/scale/v_col
I0512 23:42:16.548623 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_21/pre_self_attention_layer_norm/scale/v_row
I0512 23:42:16.548653 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_21/self_attention/key/kernel/m
I0512 23:42:16.548683 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_21/self_attention/key/kernel/v
I0512 23:42:16.548713 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_21/self_attention/key/kernel/v_col
I0512 23:42:16.548743 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_21/self_attention/key/kernel/v_row
I0512 23:42:16.548772 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_21/self_attention/out/kernel/m
I0512 23:42:16.548802 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_21/self_attention/out/kernel/v
I0512 23:42:16.548841 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_21/self_attention/out/kernel/v_col
I0512 23:42:16.548872 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_21/self_attention/out/kernel/v_row
I0512 23:42:16.548902 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_21/self_attention/query/kernel/m
I0512 23:42:16.548932 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_21/self_attention/query/kernel/v
I0512 23:42:16.548962 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_21/self_attention/query/kernel/v_col
I0512 23:42:16.548991 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_21/self_attention/query/kernel/v_row
I0512 23:42:16.549021 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_21/self_attention/value/kernel/m
I0512 23:42:16.549050 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_21/self_attention/value/kernel/v
I0512 23:42:16.549082 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_21/self_attention/value/kernel/v_col
I0512 23:42:16.549113 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_21/self_attention/value/kernel/v_row
I0512 23:42:16.549143 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_22/mlp/wi_0/kernel/m
I0512 23:42:16.549173 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_22/mlp/wi_0/kernel/v
I0512 23:42:16.549203 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_22/mlp/wi_0/kernel/v_col
I0512 23:42:16.549233 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_22/mlp/wi_0/kernel/v_row
I0512 23:42:16.549263 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_22/mlp/wi_1/kernel/m
I0512 23:42:16.549293 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_22/mlp/wi_1/kernel/v
I0512 23:42:16.549323 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_22/mlp/wi_1/kernel/v_col
I0512 23:42:16.549352 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_22/mlp/wi_1/kernel/v_row
I0512 23:42:16.549382 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_22/mlp/wo/kernel/m
I0512 23:42:16.549412 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_22/mlp/wo/kernel/v
I0512 23:42:16.549442 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_22/mlp/wo/kernel/v_col
I0512 23:42:16.549472 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_22/mlp/wo/kernel/v_row
I0512 23:42:16.549502 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_22/pre_mlp_layer_norm/scale/m
I0512 23:42:16.549532 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_22/pre_mlp_layer_norm/scale/v
I0512 23:42:16.549584 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_22/pre_mlp_layer_norm/scale/v_col
I0512 23:42:16.549617 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_22/pre_mlp_layer_norm/scale/v_row
I0512 23:42:16.549647 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_22/pre_self_attention_layer_norm/scale/m
I0512 23:42:16.549677 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_22/pre_self_attention_layer_norm/scale/v
I0512 23:42:16.549706 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_22/pre_self_attention_layer_norm/scale/v_col
I0512 23:42:16.549736 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_22/pre_self_attention_layer_norm/scale/v_row
I0512 23:42:16.549766 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_22/self_attention/key/kernel/m
I0512 23:42:16.549795 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_22/self_attention/key/kernel/v
I0512 23:42:16.549831 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_22/self_attention/key/kernel/v_col
I0512 23:42:16.549861 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_22/self_attention/key/kernel/v_row
I0512 23:42:16.549891 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_22/self_attention/out/kernel/m
I0512 23:42:16.549921 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_22/self_attention/out/kernel/v
I0512 23:42:16.549951 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_22/self_attention/out/kernel/v_col
I0512 23:42:16.549980 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_22/self_attention/out/kernel/v_row
I0512 23:42:16.550010 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_22/self_attention/query/kernel/m
I0512 23:42:16.550040 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_22/self_attention/query/kernel/v
I0512 23:42:16.550071 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_22/self_attention/query/kernel/v_col
I0512 23:42:16.550103 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_22/self_attention/query/kernel/v_row
I0512 23:42:16.550133 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_22/self_attention/value/kernel/m
I0512 23:42:16.550163 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_22/self_attention/value/kernel/v
I0512 23:42:16.550192 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_22/self_attention/value/kernel/v_col
I0512 23:42:16.550222 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_22/self_attention/value/kernel/v_row
I0512 23:42:16.550251 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/extra_mlp/wi_0/kernel/m
I0512 23:42:16.550281 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/extra_mlp/wi_0/kernel/v
I0512 23:42:16.550310 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/extra_mlp/wi_0/kernel/v_col
I0512 23:42:16.550340 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/extra_mlp/wi_0/kernel/v_row
I0512 23:42:16.550369 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/extra_mlp/wi_1/kernel/m
I0512 23:42:16.550399 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/extra_mlp/wi_1/kernel/v
I0512 23:42:16.550429 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/extra_mlp/wi_1/kernel/v_col
I0512 23:42:16.550458 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/extra_mlp/wi_1/kernel/v_row
I0512 23:42:16.550487 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/extra_mlp/wo/kernel/m
I0512 23:42:16.550518 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/extra_mlp/wo/kernel/v
I0512 23:42:16.550548 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/extra_mlp/wo/kernel/v_col
I0512 23:42:16.550578 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/extra_mlp/wo/kernel/v_row
I0512 23:42:16.550607 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/mlp/expert/wi_0/kernel/m
I0512 23:42:16.550637 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/mlp/expert/wi_0/kernel/v
I0512 23:42:16.550667 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/mlp/expert/wi_0/kernel/v_col
I0512 23:42:16.550697 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/mlp/expert/wi_0/kernel/v_row
I0512 23:42:16.550727 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/mlp/expert/wi_1/kernel/m
I0512 23:42:16.550756 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/mlp/expert/wi_1/kernel/v
I0512 23:42:16.550786 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/mlp/expert/wi_1/kernel/v_col
I0512 23:42:16.550821 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/mlp/expert/wi_1/kernel/v_row
I0512 23:42:16.550852 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/mlp/expert/wo/kernel/m
I0512 23:42:16.550883 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/mlp/expert/wo/kernel/v
I0512 23:42:16.550913 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/mlp/expert/wo/kernel/v_col
I0512 23:42:16.550942 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/mlp/expert/wo/kernel/v_row
I0512 23:42:16.550972 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/mlp/router/router_weights/w/kernel/m
I0512 23:42:16.551002 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/mlp/router/router_weights/w/kernel/v
I0512 23:42:16.551032 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/mlp/router/router_weights/w/kernel/v_col
I0512 23:42:16.551061 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/mlp/router/router_weights/w/kernel/v_row
I0512 23:42:16.551094 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/pre_extra_mlp_layer_norm/scale/m
I0512 23:42:16.551124 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/pre_extra_mlp_layer_norm/scale/v
I0512 23:42:16.551153 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/pre_extra_mlp_layer_norm/scale/v_col
I0512 23:42:16.551183 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/pre_extra_mlp_layer_norm/scale/v_row
I0512 23:42:16.551213 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/pre_mlp_layer_norm/scale/m
I0512 23:42:16.551243 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/pre_mlp_layer_norm/scale/v
I0512 23:42:16.551272 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/pre_mlp_layer_norm/scale/v_col
I0512 23:42:16.551302 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/pre_mlp_layer_norm/scale/v_row
I0512 23:42:16.551332 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/pre_self_attention_layer_norm/scale/m
I0512 23:42:16.551362 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/pre_self_attention_layer_norm/scale/v
I0512 23:42:16.551392 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/pre_self_attention_layer_norm/scale/v_col
I0512 23:42:16.551421 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/pre_self_attention_layer_norm/scale/v_row
I0512 23:42:16.551451 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/self_attention/key/kernel/m
I0512 23:42:16.551481 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/self_attention/key/kernel/v
I0512 23:42:16.551511 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/self_attention/key/kernel/v_col
I0512 23:42:16.551540 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/self_attention/key/kernel/v_row
I0512 23:42:16.551570 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/self_attention/out/kernel/m
I0512 23:42:16.551600 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/self_attention/out/kernel/v
I0512 23:42:16.551630 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/self_attention/out/kernel/v_col
I0512 23:42:16.551660 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/self_attention/out/kernel/v_row
I0512 23:42:16.551689 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/self_attention/query/kernel/m
I0512 23:42:16.551719 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/self_attention/query/kernel/v
I0512 23:42:16.551749 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/self_attention/query/kernel/v_col
I0512 23:42:16.551778 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/self_attention/query/kernel/v_row
I0512 23:42:16.551808 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/self_attention/value/kernel/m
I0512 23:42:16.551844 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/self_attention/value/kernel/v
I0512 23:42:16.551874 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/self_attention/value/kernel/v_col
I0512 23:42:16.551904 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/self_attention/value/kernel/v_row
I0512 23:42:16.551934 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_3/mlp/wi_0/kernel/m
I0512 23:42:16.551964 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_3/mlp/wi_0/kernel/v
I0512 23:42:16.551994 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_3/mlp/wi_0/kernel/v_col
I0512 23:42:16.552024 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_3/mlp/wi_0/kernel/v_row
I0512 23:42:16.552053 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_3/mlp/wi_1/kernel/m
I0512 23:42:16.552085 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_3/mlp/wi_1/kernel/v
I0512 23:42:16.552115 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_3/mlp/wi_1/kernel/v_col
I0512 23:42:16.552145 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_3/mlp/wi_1/kernel/v_row
I0512 23:42:16.552175 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_3/mlp/wo/kernel/m
I0512 23:42:16.552205 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_3/mlp/wo/kernel/v
I0512 23:42:16.552234 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_3/mlp/wo/kernel/v_col
I0512 23:42:16.552264 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_3/mlp/wo/kernel/v_row
I0512 23:42:16.552294 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_3/pre_mlp_layer_norm/scale/m
I0512 23:42:16.552324 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_3/pre_mlp_layer_norm/scale/v
I0512 23:42:16.552354 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_3/pre_mlp_layer_norm/scale/v_col
I0512 23:42:16.552384 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_3/pre_mlp_layer_norm/scale/v_row
I0512 23:42:16.552413 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_3/pre_self_attention_layer_norm/scale/m
I0512 23:42:16.552443 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_3/pre_self_attention_layer_norm/scale/v
I0512 23:42:16.552473 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_3/pre_self_attention_layer_norm/scale/v_col
I0512 23:42:16.552503 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_3/pre_self_attention_layer_norm/scale/v_row
I0512 23:42:16.552533 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_3/self_attention/key/kernel/m
I0512 23:42:16.552562 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_3/self_attention/key/kernel/v
I0512 23:42:16.552592 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_3/self_attention/key/kernel/v_col
I0512 23:42:16.552621 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_3/self_attention/key/kernel/v_row
I0512 23:42:16.552651 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_3/self_attention/out/kernel/m
I0512 23:42:16.552680 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_3/self_attention/out/kernel/v
I0512 23:42:16.552710 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_3/self_attention/out/kernel/v_col
I0512 23:42:16.552740 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_3/self_attention/out/kernel/v_row
I0512 23:42:16.552770 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_3/self_attention/query/kernel/m
I0512 23:42:16.552799 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_3/self_attention/query/kernel/v
I0512 23:42:16.552835 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_3/self_attention/query/kernel/v_col
I0512 23:42:16.552865 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_3/self_attention/query/kernel/v_row
I0512 23:42:16.552895 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_3/self_attention/value/kernel/m
I0512 23:42:16.552925 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_3/self_attention/value/kernel/v
I0512 23:42:16.552954 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_3/self_attention/value/kernel/v_col
I0512 23:42:16.552984 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_3/self_attention/value/kernel/v_row
I0512 23:42:16.553014 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_4/mlp/wi_0/kernel/m
I0512 23:42:16.553044 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_4/mlp/wi_0/kernel/v
I0512 23:42:16.553075 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_4/mlp/wi_0/kernel/v_col
I0512 23:42:16.553106 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_4/mlp/wi_0/kernel/v_row
I0512 23:42:16.553136 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_4/mlp/wi_1/kernel/m
I0512 23:42:16.553166 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_4/mlp/wi_1/kernel/v
I0512 23:42:16.553195 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_4/mlp/wi_1/kernel/v_col
I0512 23:42:16.553225 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_4/mlp/wi_1/kernel/v_row
I0512 23:42:16.553255 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_4/mlp/wo/kernel/m
I0512 23:42:16.553285 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_4/mlp/wo/kernel/v
I0512 23:42:16.553314 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_4/mlp/wo/kernel/v_col
I0512 23:42:16.553344 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_4/mlp/wo/kernel/v_row
I0512 23:42:16.553374 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_4/pre_mlp_layer_norm/scale/m
I0512 23:42:16.553404 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_4/pre_mlp_layer_norm/scale/v
I0512 23:42:16.553434 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_4/pre_mlp_layer_norm/scale/v_col
I0512 23:42:16.553464 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_4/pre_mlp_layer_norm/scale/v_row
I0512 23:42:16.553493 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_4/pre_self_attention_layer_norm/scale/m
I0512 23:42:16.553524 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_4/pre_self_attention_layer_norm/scale/v
I0512 23:42:16.553573 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_4/pre_self_attention_layer_norm/scale/v_col
I0512 23:42:16.553607 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_4/pre_self_attention_layer_norm/scale/v_row
I0512 23:42:16.553637 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_4/self_attention/key/kernel/m
I0512 23:42:16.553667 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_4/self_attention/key/kernel/v
I0512 23:42:16.553697 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_4/self_attention/key/kernel/v_col
I0512 23:42:16.553727 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_4/self_attention/key/kernel/v_row
I0512 23:42:16.553756 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_4/self_attention/out/kernel/m
I0512 23:42:16.553786 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_4/self_attention/out/kernel/v
I0512 23:42:16.553821 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_4/self_attention/out/kernel/v_col
I0512 23:42:16.553852 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_4/self_attention/out/kernel/v_row
I0512 23:42:16.553882 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_4/self_attention/query/kernel/m
I0512 23:42:16.553912 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_4/self_attention/query/kernel/v
I0512 23:42:16.553941 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_4/self_attention/query/kernel/v_col
I0512 23:42:16.553971 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_4/self_attention/query/kernel/v_row
I0512 23:42:16.554000 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_4/self_attention/value/kernel/m
I0512 23:42:16.554030 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_4/self_attention/value/kernel/v
I0512 23:42:16.554059 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_4/self_attention/value/kernel/v_col
I0512 23:42:16.554091 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_4/self_attention/value/kernel/v_row
I0512 23:42:16.554122 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/extra_mlp/wi_0/kernel/m
I0512 23:42:16.554152 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/extra_mlp/wi_0/kernel/v
I0512 23:42:16.554182 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/extra_mlp/wi_0/kernel/v_col
I0512 23:42:16.554211 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/extra_mlp/wi_0/kernel/v_row
I0512 23:42:16.554241 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/extra_mlp/wi_1/kernel/m
I0512 23:42:16.554271 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/extra_mlp/wi_1/kernel/v
I0512 23:42:16.554301 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/extra_mlp/wi_1/kernel/v_col
I0512 23:42:16.554330 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/extra_mlp/wi_1/kernel/v_row
I0512 23:42:16.554360 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/extra_mlp/wo/kernel/m
I0512 23:42:16.554390 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/extra_mlp/wo/kernel/v
I0512 23:42:16.554420 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/extra_mlp/wo/kernel/v_col
I0512 23:42:16.554450 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/extra_mlp/wo/kernel/v_row
I0512 23:42:16.554479 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/mlp/expert/wi_0/kernel/m
I0512 23:42:16.554509 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/mlp/expert/wi_0/kernel/v
I0512 23:42:16.554539 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/mlp/expert/wi_0/kernel/v_col
I0512 23:42:16.554569 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/mlp/expert/wi_0/kernel/v_row
I0512 23:42:16.554598 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/mlp/expert/wi_1/kernel/m
I0512 23:42:16.554628 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/mlp/expert/wi_1/kernel/v
I0512 23:42:16.554657 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/mlp/expert/wi_1/kernel/v_col
I0512 23:42:16.554687 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/mlp/expert/wi_1/kernel/v_row
I0512 23:42:16.554716 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/mlp/expert/wo/kernel/m
I0512 23:42:16.554747 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/mlp/expert/wo/kernel/v
I0512 23:42:16.554777 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/mlp/expert/wo/kernel/v_col
I0512 23:42:16.554807 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/mlp/expert/wo/kernel/v_row
I0512 23:42:16.554846 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/mlp/router/router_weights/w/kernel/m
I0512 23:42:16.554876 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/mlp/router/router_weights/w/kernel/v
I0512 23:42:16.554906 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/mlp/router/router_weights/w/kernel/v_col
I0512 23:42:16.554936 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/mlp/router/router_weights/w/kernel/v_row
I0512 23:42:16.554965 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/pre_extra_mlp_layer_norm/scale/m
I0512 23:42:16.554995 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/pre_extra_mlp_layer_norm/scale/v
I0512 23:42:16.555024 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/pre_extra_mlp_layer_norm/scale/v_col
I0512 23:42:16.555054 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/pre_extra_mlp_layer_norm/scale/v_row
I0512 23:42:16.555313 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/pre_mlp_layer_norm/scale/m
I0512 23:42:16.555347 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/pre_mlp_layer_norm/scale/v
I0512 23:42:16.555377 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/pre_mlp_layer_norm/scale/v_col
I0512 23:42:16.555408 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/pre_mlp_layer_norm/scale/v_row
I0512 23:42:16.555438 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/pre_self_attention_layer_norm/scale/m
I0512 23:42:16.555468 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/pre_self_attention_layer_norm/scale/v
I0512 23:42:16.555497 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/pre_self_attention_layer_norm/scale/v_col
I0512 23:42:16.555527 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/pre_self_attention_layer_norm/scale/v_row
I0512 23:42:16.555557 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/self_attention/key/kernel/m
I0512 23:42:16.555587 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/self_attention/key/kernel/v
I0512 23:42:16.555616 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/self_attention/key/kernel/v_col
I0512 23:42:16.555646 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/self_attention/key/kernel/v_row
I0512 23:42:16.555675 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/self_attention/out/kernel/m
I0512 23:42:16.555705 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/self_attention/out/kernel/v
I0512 23:42:16.555735 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/self_attention/out/kernel/v_col
I0512 23:42:16.555765 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/self_attention/out/kernel/v_row
I0512 23:42:16.555794 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/self_attention/query/kernel/m
I0512 23:42:16.555831 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/self_attention/query/kernel/v
I0512 23:42:16.555861 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/self_attention/query/kernel/v_col
I0512 23:42:16.555891 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/self_attention/query/kernel/v_row
I0512 23:42:16.555921 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/self_attention/value/kernel/m
I0512 23:42:16.555950 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/self_attention/value/kernel/v
I0512 23:42:16.556002 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/self_attention/value/kernel/v_col
I0512 23:42:16.556035 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/self_attention/value/kernel/v_row
I0512 23:42:16.556066 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_6/mlp/wi_0/kernel/m
I0512 23:42:16.556098 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_6/mlp/wi_0/kernel/v
I0512 23:42:16.556129 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_6/mlp/wi_0/kernel/v_col
I0512 23:42:16.556159 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_6/mlp/wi_0/kernel/v_row
I0512 23:42:16.556189 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_6/mlp/wi_1/kernel/m
I0512 23:42:16.556219 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_6/mlp/wi_1/kernel/v
I0512 23:42:16.556249 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_6/mlp/wi_1/kernel/v_col
I0512 23:42:16.556278 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_6/mlp/wi_1/kernel/v_row
I0512 23:42:16.556308 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_6/mlp/wo/kernel/m
I0512 23:42:16.556338 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_6/mlp/wo/kernel/v
I0512 23:42:16.556368 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_6/mlp/wo/kernel/v_col
I0512 23:42:16.556398 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_6/mlp/wo/kernel/v_row
I0512 23:42:16.556427 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_6/pre_mlp_layer_norm/scale/m
I0512 23:42:16.556457 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_6/pre_mlp_layer_norm/scale/v
I0512 23:42:16.556487 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_6/pre_mlp_layer_norm/scale/v_col
I0512 23:42:16.556517 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_6/pre_mlp_layer_norm/scale/v_row
I0512 23:42:16.556547 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_6/pre_self_attention_layer_norm/scale/m
I0512 23:42:16.556577 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_6/pre_self_attention_layer_norm/scale/v
I0512 23:42:16.556607 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_6/pre_self_attention_layer_norm/scale/v_col
I0512 23:42:16.556636 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_6/pre_self_attention_layer_norm/scale/v_row
I0512 23:42:16.556666 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_6/self_attention/key/kernel/m
I0512 23:42:16.556696 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_6/self_attention/key/kernel/v
I0512 23:42:16.556725 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_6/self_attention/key/kernel/v_col
I0512 23:42:16.556755 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_6/self_attention/key/kernel/v_row
I0512 23:42:16.556784 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_6/self_attention/out/kernel/m
I0512 23:42:16.557040 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_6/self_attention/out/kernel/v
I0512 23:42:16.557080 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_6/self_attention/out/kernel/v_col
I0512 23:42:16.557111 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_6/self_attention/out/kernel/v_row
I0512 23:42:16.557141 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_6/self_attention/query/kernel/m
I0512 23:42:16.557171 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_6/self_attention/query/kernel/v
I0512 23:42:16.557201 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_6/self_attention/query/kernel/v_col
I0512 23:42:16.557230 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_6/self_attention/query/kernel/v_row
I0512 23:42:16.557259 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_6/self_attention/value/kernel/m
I0512 23:42:16.557289 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_6/self_attention/value/kernel/v
I0512 23:42:16.557318 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_6/self_attention/value/kernel/v_col
I0512 23:42:16.557348 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_6/self_attention/value/kernel/v_row
I0512 23:42:16.557378 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_7/mlp/wi_0/kernel/m
I0512 23:42:16.557408 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_7/mlp/wi_0/kernel/v
I0512 23:42:16.557437 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_7/mlp/wi_0/kernel/v_col
I0512 23:42:16.557467 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_7/mlp/wi_0/kernel/v_row
I0512 23:42:16.557497 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_7/mlp/wi_1/kernel/m
I0512 23:42:16.557527 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_7/mlp/wi_1/kernel/v
I0512 23:42:16.557576 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_7/mlp/wi_1/kernel/v_col
I0512 23:42:16.557609 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_7/mlp/wi_1/kernel/v_row
I0512 23:42:16.557639 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_7/mlp/wo/kernel/m
I0512 23:42:16.557669 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_7/mlp/wo/kernel/v
I0512 23:42:16.557699 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_7/mlp/wo/kernel/v_col
I0512 23:42:16.557729 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_7/mlp/wo/kernel/v_row
I0512 23:42:16.557759 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_7/pre_mlp_layer_norm/scale/m
I0512 23:42:16.557789 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_7/pre_mlp_layer_norm/scale/v
I0512 23:42:16.557826 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_7/pre_mlp_layer_norm/scale/v_col
I0512 23:42:16.557857 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_7/pre_mlp_layer_norm/scale/v_row
I0512 23:42:16.557888 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_7/pre_self_attention_layer_norm/scale/m
I0512 23:42:16.557918 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_7/pre_self_attention_layer_norm/scale/v
I0512 23:42:16.557948 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_7/pre_self_attention_layer_norm/scale/v_col
I0512 23:42:16.557978 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_7/pre_self_attention_layer_norm/scale/v_row
I0512 23:42:16.558007 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_7/self_attention/key/kernel/m
I0512 23:42:16.558037 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_7/self_attention/key/kernel/v
I0512 23:42:16.558067 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_7/self_attention/key/kernel/v_col
I0512 23:42:16.558099 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_7/self_attention/key/kernel/v_row
I0512 23:42:16.558129 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_7/self_attention/out/kernel/m
I0512 23:42:16.558159 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_7/self_attention/out/kernel/v
I0512 23:42:16.558189 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_7/self_attention/out/kernel/v_col
I0512 23:42:16.558218 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_7/self_attention/out/kernel/v_row
I0512 23:42:16.558248 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_7/self_attention/query/kernel/m
I0512 23:42:16.558278 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_7/self_attention/query/kernel/v
I0512 23:42:16.558307 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_7/self_attention/query/kernel/v_col
I0512 23:42:16.558337 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_7/self_attention/query/kernel/v_row
I0512 23:42:16.558367 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_7/self_attention/value/kernel/m
I0512 23:42:16.558397 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_7/self_attention/value/kernel/v
I0512 23:42:16.558427 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_7/self_attention/value/kernel/v_col
I0512 23:42:16.558456 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_7/self_attention/value/kernel/v_row
I0512 23:42:16.558486 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_8/mlp/wi_0/kernel/m
I0512 23:42:16.558516 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_8/mlp/wi_0/kernel/v
I0512 23:42:16.558546 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_8/mlp/wi_0/kernel/v_col
I0512 23:42:16.558575 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_8/mlp/wi_0/kernel/v_row
I0512 23:42:16.558605 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_8/mlp/wi_1/kernel/m
I0512 23:42:16.558635 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_8/mlp/wi_1/kernel/v
I0512 23:42:16.558665 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_8/mlp/wi_1/kernel/v_col
I0512 23:42:16.558694 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_8/mlp/wi_1/kernel/v_row
I0512 23:42:16.558724 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_8/mlp/wo/kernel/m
I0512 23:42:16.558754 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_8/mlp/wo/kernel/v
I0512 23:42:16.558784 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_8/mlp/wo/kernel/v_col
I0512 23:42:16.558818 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_8/mlp/wo/kernel/v_row
I0512 23:42:16.558849 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_8/pre_mlp_layer_norm/scale/m
I0512 23:42:16.558879 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_8/pre_mlp_layer_norm/scale/v
I0512 23:42:16.558909 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_8/pre_mlp_layer_norm/scale/v_col
I0512 23:42:16.558940 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_8/pre_mlp_layer_norm/scale/v_row
I0512 23:42:16.558969 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_8/pre_self_attention_layer_norm/scale/m
I0512 23:42:16.559000 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_8/pre_self_attention_layer_norm/scale/v
I0512 23:42:16.559029 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_8/pre_self_attention_layer_norm/scale/v_col
I0512 23:42:16.559059 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_8/pre_self_attention_layer_norm/scale/v_row
I0512 23:42:16.559091 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_8/self_attention/key/kernel/m
I0512 23:42:16.559121 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_8/self_attention/key/kernel/v
I0512 23:42:16.559151 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_8/self_attention/key/kernel/v_col
I0512 23:42:16.559180 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_8/self_attention/key/kernel/v_row
I0512 23:42:16.559210 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_8/self_attention/out/kernel/m
I0512 23:42:16.559240 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_8/self_attention/out/kernel/v
I0512 23:42:16.559270 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_8/self_attention/out/kernel/v_col
I0512 23:42:16.559300 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_8/self_attention/out/kernel/v_row
I0512 23:42:16.559329 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_8/self_attention/query/kernel/m
I0512 23:42:16.559359 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_8/self_attention/query/kernel/v
I0512 23:42:16.559389 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_8/self_attention/query/kernel/v_col
I0512 23:42:16.559419 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_8/self_attention/query/kernel/v_row
I0512 23:42:16.559448 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_8/self_attention/value/kernel/m
I0512 23:42:16.559478 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_8/self_attention/value/kernel/v
I0512 23:42:16.559508 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_8/self_attention/value/kernel/v_col
I0512 23:42:16.559537 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_8/self_attention/value/kernel/v_row
I0512 23:42:16.559567 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_9/mlp/wi_0/kernel/m
I0512 23:42:16.559597 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_9/mlp/wi_0/kernel/v
I0512 23:42:16.559627 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_9/mlp/wi_0/kernel/v_col
I0512 23:42:16.559657 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_9/mlp/wi_0/kernel/v_row
I0512 23:42:16.559687 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_9/mlp/wi_1/kernel/m
I0512 23:42:16.559717 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_9/mlp/wi_1/kernel/v
I0512 23:42:16.559746 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_9/mlp/wi_1/kernel/v_col
I0512 23:42:16.559776 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_9/mlp/wi_1/kernel/v_row
I0512 23:42:16.559806 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_9/mlp/wo/kernel/m
I0512 23:42:16.559842 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_9/mlp/wo/kernel/v
I0512 23:42:16.559873 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_9/mlp/wo/kernel/v_col
I0512 23:42:16.559904 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_9/mlp/wo/kernel/v_row
I0512 23:42:16.559939 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_9/pre_mlp_layer_norm/scale/m
I0512 23:42:16.559969 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_9/pre_mlp_layer_norm/scale/v
I0512 23:42:16.559999 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_9/pre_mlp_layer_norm/scale/v_col
I0512 23:42:16.560029 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_9/pre_mlp_layer_norm/scale/v_row
I0512 23:42:16.560059 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_9/pre_self_attention_layer_norm/scale/m
I0512 23:42:16.560091 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_9/pre_self_attention_layer_norm/scale/v
I0512 23:42:16.560121 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_9/pre_self_attention_layer_norm/scale/v_col
I0512 23:42:16.560151 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_9/pre_self_attention_layer_norm/scale/v_row
I0512 23:42:16.560180 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_9/self_attention/key/kernel/m
I0512 23:42:16.560210 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_9/self_attention/key/kernel/v
I0512 23:42:16.560240 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_9/self_attention/key/kernel/v_col
I0512 23:42:16.560269 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_9/self_attention/key/kernel/v_row
I0512 23:42:16.560299 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_9/self_attention/out/kernel/m
I0512 23:42:16.560329 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_9/self_attention/out/kernel/v
I0512 23:42:16.560359 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_9/self_attention/out/kernel/v_col
I0512 23:42:16.560388 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_9/self_attention/out/kernel/v_row
I0512 23:42:16.560418 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_9/self_attention/query/kernel/m
I0512 23:42:16.560447 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_9/self_attention/query/kernel/v
I0512 23:42:16.560477 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_9/self_attention/query/kernel/v_col
I0512 23:42:16.560507 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_9/self_attention/query/kernel/v_row
I0512 23:42:16.560536 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_9/self_attention/value/kernel/m
I0512 23:42:16.560565 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_9/self_attention/value/kernel/v
I0512 23:42:16.560595 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_9/self_attention/value/kernel/v_col
I0512 23:42:16.560625 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_9/self_attention/value/kernel/v_row
I0512 23:42:16.560654 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/logits_dense/kernel/m
I0512 23:42:16.560684 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/logits_dense/kernel/v
I0512 23:42:16.560714 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/logits_dense/kernel/v_col
I0512 23:42:16.560744 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/logits_dense/kernel/v_row
I0512 23:42:16.560773 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/token_embedder/embedding/m
I0512 23:42:16.560803 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/token_embedder/embedding/v
I0512 23:42:16.560839 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/token_embedder/embedding/v_col
I0512 23:42:16.560869 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/token_embedder/embedding/v_row
I0512 23:42:16.560899 140529434355712 checkpoints.py:1100] Restoring key from ckpt: state/step
I0512 23:42:16.560930 140529434355712 checkpoints.py:1100] Restoring key from ckpt: target/decoder/decoder_norm/scale
I0512 23:42:16.560960 140529434355712 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_0/mlp/wi_0/kernel
I0512 23:42:16.560989 140529434355712 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_0/mlp/wi_1/kernel
I0512 23:42:16.561019 140529434355712 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_0/mlp/wo/kernel
I0512 23:42:16.561048 140529434355712 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_0/pre_mlp_layer_norm/scale
I0512 23:42:16.561080 140529434355712 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_0/pre_self_attention_layer_norm/scale
I0512 23:42:16.561111 140529434355712 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_0/self_attention/key/kernel
I0512 23:42:16.561141 140529434355712 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_0/self_attention/out/kernel
I0512 23:42:16.561171 140529434355712 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_0/self_attention/query/kernel
I0512 23:42:16.561200 140529434355712 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_0/self_attention/value/kernel
I0512 23:42:16.561229 140529434355712 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_1/mlp/wi_0/kernel
I0512 23:42:16.561259 140529434355712 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_1/mlp/wi_1/kernel
I0512 23:42:16.561289 140529434355712 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_1/mlp/wo/kernel
I0512 23:42:16.561318 140529434355712 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_1/pre_mlp_layer_norm/scale
I0512 23:42:16.561348 140529434355712 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_1/pre_self_attention_layer_norm/scale
I0512 23:42:16.561378 140529434355712 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_1/self_attention/key/kernel
I0512 23:42:16.561408 140529434355712 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_1/self_attention/out/kernel
I0512 23:42:16.561437 140529434355712 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_1/self_attention/query/kernel
I0512 23:42:16.561466 140529434355712 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_1/self_attention/value/kernel
I0512 23:42:16.561496 140529434355712 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_10/mlp/wi_0/kernel
I0512 23:42:16.561526 140529434355712 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_10/mlp/wi_1/kernel
I0512 23:42:16.561580 140529434355712 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_10/mlp/wo/kernel
I0512 23:42:16.561614 140529434355712 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_10/pre_mlp_layer_norm/scale
I0512 23:42:16.561644 140529434355712 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_10/pre_self_attention_layer_norm/scale
I0512 23:42:16.561674 140529434355712 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_10/self_attention/key/kernel
I0512 23:42:16.561704 140529434355712 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_10/self_attention/out/kernel
I0512 23:42:16.561734 140529434355712 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_10/self_attention/query/kernel
I0512 23:42:16.561763 140529434355712 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_10/self_attention/value/kernel
I0512 23:42:16.561793 140529434355712 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_11/extra_mlp/wi_0/kernel
I0512 23:42:16.561828 140529434355712 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_11/extra_mlp/wi_1/kernel
I0512 23:42:16.561858 140529434355712 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_11/extra_mlp/wo/kernel
I0512 23:42:16.561888 140529434355712 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_11/mlp/expert/wi_0/kernel
I0512 23:42:16.561918 140529434355712 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_11/mlp/expert/wi_1/kernel
I0512 23:42:16.561947 140529434355712 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_11/mlp/expert/wo/kernel
I0512 23:42:16.561977 140529434355712 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_11/mlp/router/router_weights/w/kernel
I0512 23:42:16.562007 140529434355712 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_11/pre_extra_mlp_layer_norm/scale
I0512 23:42:16.562036 140529434355712 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_11/pre_mlp_layer_norm/scale
I0512 23:42:16.562067 140529434355712 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_11/pre_self_attention_layer_norm/scale
I0512 23:42:16.562099 140529434355712 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_11/self_attention/key/kernel
I0512 23:42:16.562129 140529434355712 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_11/self_attention/out/kernel
I0512 23:42:16.562158 140529434355712 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_11/self_attention/query/kernel
I0512 23:42:16.562188 140529434355712 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_11/self_attention/value/kernel
I0512 23:42:16.562218 140529434355712 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_12/mlp/wi_0/kernel
I0512 23:42:16.562248 140529434355712 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_12/mlp/wi_1/kernel
I0512 23:42:16.562277 140529434355712 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_12/mlp/wo/kernel
I0512 23:42:16.562307 140529434355712 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_12/pre_mlp_layer_norm/scale
I0512 23:42:16.562337 140529434355712 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_12/pre_self_attention_layer_norm/scale
I0512 23:42:16.562366 140529434355712 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_12/self_attention/key/kernel
I0512 23:42:16.562396 140529434355712 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_12/self_attention/out/kernel
I0512 23:42:16.562426 140529434355712 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_12/self_attention/query/kernel
I0512 23:42:16.562456 140529434355712 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_12/self_attention/value/kernel
I0512 23:42:16.562485 140529434355712 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_13/mlp/wi_0/kernel
I0512 23:42:16.562515 140529434355712 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_13/mlp/wi_1/kernel
I0512 23:42:16.562545 140529434355712 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_13/mlp/wo/kernel
I0512 23:42:16.562575 140529434355712 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_13/pre_mlp_layer_norm/scale
I0512 23:42:16.562605 140529434355712 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_13/pre_self_attention_layer_norm/scale
I0512 23:42:16.562635 140529434355712 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_13/self_attention/key/kernel
I0512 23:42:16.562664 140529434355712 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_13/self_attention/out/kernel
I0512 23:42:16.562694 140529434355712 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_13/self_attention/query/kernel
I0512 23:42:16.562724 140529434355712 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_13/self_attention/value/kernel
I0512 23:42:16.562753 140529434355712 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_14/mlp/wi_0/kernel
I0512 23:42:16.562783 140529434355712 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_14/mlp/wi_1/kernel
I0512 23:42:16.562817 140529434355712 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_14/mlp/wo/kernel
I0512 23:42:16.562849 140529434355712 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_14/pre_mlp_layer_norm/scale
I0512 23:42:16.562879 140529434355712 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_14/pre_self_attention_layer_norm/scale
I0512 23:42:16.562909 140529434355712 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_14/self_attention/key/kernel
I0512 23:42:16.562939 140529434355712 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_14/self_attention/out/kernel
I0512 23:42:16.562969 140529434355712 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_14/self_attention/query/kernel
I0512 23:42:16.562999 140529434355712 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_14/self_attention/value/kernel
I0512 23:42:16.563029 140529434355712 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_15/mlp/wi_0/kernel
I0512 23:42:16.563058 140529434355712 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_15/mlp/wi_1/kernel
I0512 23:42:16.563090 140529434355712 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_15/mlp/wo/kernel
I0512 23:42:16.563120 140529434355712 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_15/pre_mlp_layer_norm/scale
I0512 23:42:16.563150 140529434355712 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_15/pre_self_attention_layer_norm/scale
I0512 23:42:16.563180 140529434355712 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_15/self_attention/key/kernel
I0512 23:42:16.563210 140529434355712 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_15/self_attention/out/kernel
I0512 23:42:16.563240 140529434355712 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_15/self_attention/query/kernel
I0512 23:42:16.563270 140529434355712 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_15/self_attention/value/kernel
I0512 23:42:16.563299 140529434355712 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_16/mlp/wi_0/kernel
I0512 23:42:16.563329 140529434355712 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_16/mlp/wi_1/kernel
I0512 23:42:16.563359 140529434355712 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_16/mlp/wo/kernel
I0512 23:42:16.563389 140529434355712 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_16/pre_mlp_layer_norm/scale
I0512 23:42:16.563419 140529434355712 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_16/pre_self_attention_layer_norm/scale
I0512 23:42:16.563449 140529434355712 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_16/self_attention/key/kernel
I0512 23:42:16.563478 140529434355712 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_16/self_attention/out/kernel
I0512 23:42:16.563508 140529434355712 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_16/self_attention/query/kernel
I0512 23:42:16.563538 140529434355712 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_16/self_attention/value/kernel
I0512 23:42:16.563568 140529434355712 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_17/extra_mlp/wi_0/kernel
I0512 23:42:16.563598 140529434355712 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_17/extra_mlp/wi_1/kernel
I0512 23:42:16.563627 140529434355712 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_17/extra_mlp/wo/kernel
I0512 23:42:16.563657 140529434355712 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_17/mlp/expert/wi_0/kernel
I0512 23:42:16.563686 140529434355712 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_17/mlp/expert/wi_1/kernel
I0512 23:42:16.563716 140529434355712 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_17/mlp/expert/wo/kernel
I0512 23:42:16.563745 140529434355712 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_17/mlp/router/router_weights/w/kernel
I0512 23:42:16.563775 140529434355712 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_17/pre_extra_mlp_layer_norm/scale
I0512 23:42:16.563805 140529434355712 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_17/pre_mlp_layer_norm/scale
I0512 23:42:16.563840 140529434355712 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_17/pre_self_attention_layer_norm/scale
I0512 23:42:16.563871 140529434355712 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_17/self_attention/key/kernel
I0512 23:42:16.563901 140529434355712 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_17/self_attention/out/kernel
I0512 23:42:16.563930 140529434355712 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_17/self_attention/query/kernel
I0512 23:42:16.563960 140529434355712 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_17/self_attention/value/kernel
I0512 23:42:16.563990 140529434355712 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_18/mlp/wi_0/kernel
I0512 23:42:16.564020 140529434355712 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_18/mlp/wi_1/kernel
I0512 23:42:16.564050 140529434355712 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_18/mlp/wo/kernel
I0512 23:42:16.564081 140529434355712 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_18/pre_mlp_layer_norm/scale
I0512 23:42:16.564112 140529434355712 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_18/pre_self_attention_layer_norm/scale
I0512 23:42:16.564142 140529434355712 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_18/self_attention/key/kernel
I0512 23:42:16.564172 140529434355712 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_18/self_attention/out/kernel
I0512 23:42:16.564202 140529434355712 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_18/self_attention/query/kernel
I0512 23:42:16.564231 140529434355712 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_18/self_attention/value/kernel
I0512 23:42:16.564261 140529434355712 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_19/mlp/wi_0/kernel
I0512 23:42:16.564291 140529434355712 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_19/mlp/wi_1/kernel
I0512 23:42:16.564321 140529434355712 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_19/mlp/wo/kernel
I0512 23:42:16.564350 140529434355712 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_19/pre_mlp_layer_norm/scale
I0512 23:42:16.564380 140529434355712 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_19/pre_self_attention_layer_norm/scale
I0512 23:42:16.564410 140529434355712 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_19/self_attention/key/kernel
I0512 23:42:16.564440 140529434355712 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_19/self_attention/out/kernel
I0512 23:42:16.564470 140529434355712 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_19/self_attention/query/kernel
I0512 23:42:16.564499 140529434355712 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_19/self_attention/value/kernel
I0512 23:42:16.564529 140529434355712 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_2/mlp/wi_0/kernel
I0512 23:42:16.564559 140529434355712 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_2/mlp/wi_1/kernel
I0512 23:42:16.564589 140529434355712 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_2/mlp/wo/kernel
I0512 23:42:16.564619 140529434355712 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_2/pre_mlp_layer_norm/scale
I0512 23:42:16.564648 140529434355712 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_2/pre_self_attention_layer_norm/scale
I0512 23:42:16.564678 140529434355712 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_2/self_attention/key/kernel
I0512 23:42:16.564708 140529434355712 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_2/self_attention/out/kernel
I0512 23:42:16.564738 140529434355712 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_2/self_attention/query/kernel
I0512 23:42:16.564768 140529434355712 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_2/self_attention/value/kernel
I0512 23:42:16.564797 140529434355712 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_20/mlp/wi_0/kernel
I0512 23:42:16.564833 140529434355712 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_20/mlp/wi_1/kernel
I0512 23:42:16.564864 140529434355712 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_20/mlp/wo/kernel
I0512 23:42:16.564894 140529434355712 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_20/pre_mlp_layer_norm/scale
I0512 23:42:16.564924 140529434355712 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_20/pre_self_attention_layer_norm/scale
I0512 23:42:16.564954 140529434355712 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_20/self_attention/key/kernel
I0512 23:42:16.564983 140529434355712 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_20/self_attention/out/kernel
I0512 23:42:16.565013 140529434355712 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_20/self_attention/query/kernel
I0512 23:42:16.565042 140529434355712 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_20/self_attention/value/kernel
I0512 23:42:16.565073 140529434355712 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_21/mlp/wi_0/kernel
I0512 23:42:16.565104 140529434355712 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_21/mlp/wi_1/kernel
I0512 23:42:16.565134 140529434355712 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_21/mlp/wo/kernel
I0512 23:42:16.565164 140529434355712 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_21/pre_mlp_layer_norm/scale
I0512 23:42:16.565194 140529434355712 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_21/pre_self_attention_layer_norm/scale
I0512 23:42:16.565224 140529434355712 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_21/self_attention/key/kernel
I0512 23:42:16.565253 140529434355712 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_21/self_attention/out/kernel
I0512 23:42:16.565283 140529434355712 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_21/self_attention/query/kernel
I0512 23:42:16.565313 140529434355712 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_21/self_attention/value/kernel
I0512 23:42:16.565343 140529434355712 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_22/mlp/wi_0/kernel
I0512 23:42:16.565373 140529434355712 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_22/mlp/wi_1/kernel
I0512 23:42:16.565402 140529434355712 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_22/mlp/wo/kernel
I0512 23:42:16.565432 140529434355712 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_22/pre_mlp_layer_norm/scale
I0512 23:42:16.565462 140529434355712 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_22/pre_self_attention_layer_norm/scale
I0512 23:42:16.565492 140529434355712 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_22/self_attention/key/kernel
I0512 23:42:16.565521 140529434355712 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_22/self_attention/out/kernel
I0512 23:42:16.565571 140529434355712 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_22/self_attention/query/kernel
I0512 23:42:16.565605 140529434355712 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_22/self_attention/value/kernel
I0512 23:42:16.565636 140529434355712 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_23/extra_mlp/wi_0/kernel
I0512 23:42:16.565665 140529434355712 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_23/extra_mlp/wi_1/kernel
I0512 23:42:16.565695 140529434355712 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_23/extra_mlp/wo/kernel
I0512 23:42:16.565725 140529434355712 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_23/mlp/expert/wi_0/kernel
I0512 23:42:16.565755 140529434355712 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_23/mlp/expert/wi_1/kernel
I0512 23:42:16.565785 140529434355712 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_23/mlp/expert/wo/kernel
I0512 23:42:16.565819 140529434355712 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_23/mlp/router/router_weights/w/kernel
I0512 23:42:16.565851 140529434355712 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_23/pre_extra_mlp_layer_norm/scale
I0512 23:42:16.565881 140529434355712 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_23/pre_mlp_layer_norm/scale
I0512 23:42:16.565911 140529434355712 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_23/pre_self_attention_layer_norm/scale
I0512 23:42:16.565941 140529434355712 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_23/self_attention/key/kernel
I0512 23:42:16.565971 140529434355712 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_23/self_attention/out/kernel
I0512 23:42:16.566000 140529434355712 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_23/self_attention/query/kernel
I0512 23:42:16.566030 140529434355712 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_23/self_attention/value/kernel
I0512 23:42:16.566060 140529434355712 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_3/mlp/wi_0/kernel
I0512 23:42:16.566092 140529434355712 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_3/mlp/wi_1/kernel
I0512 23:42:16.566122 140529434355712 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_3/mlp/wo/kernel
I0512 23:42:16.566152 140529434355712 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_3/pre_mlp_layer_norm/scale
I0512 23:42:16.566182 140529434355712 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_3/pre_self_attention_layer_norm/scale
I0512 23:42:16.566212 140529434355712 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_3/self_attention/key/kernel
I0512 23:42:16.566242 140529434355712 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_3/self_attention/out/kernel
I0512 23:42:16.566271 140529434355712 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_3/self_attention/query/kernel
I0512 23:42:16.566301 140529434355712 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_3/self_attention/value/kernel
I0512 23:42:16.566330 140529434355712 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_4/mlp/wi_0/kernel
I0512 23:42:16.566360 140529434355712 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_4/mlp/wi_1/kernel
I0512 23:42:16.566390 140529434355712 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_4/mlp/wo/kernel
I0512 23:42:16.566420 140529434355712 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_4/pre_mlp_layer_norm/scale
I0512 23:42:16.566449 140529434355712 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_4/pre_self_attention_layer_norm/scale
I0512 23:42:16.566479 140529434355712 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_4/self_attention/key/kernel
I0512 23:42:16.566509 140529434355712 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_4/self_attention/out/kernel
I0512 23:42:16.566539 140529434355712 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_4/self_attention/query/kernel
I0512 23:42:16.566568 140529434355712 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_4/self_attention/value/kernel
I0512 23:42:16.566598 140529434355712 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_5/extra_mlp/wi_0/kernel
I0512 23:42:16.566627 140529434355712 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_5/extra_mlp/wi_1/kernel
I0512 23:42:16.566657 140529434355712 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_5/extra_mlp/wo/kernel
I0512 23:42:16.566687 140529434355712 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_5/mlp/expert/wi_0/kernel
I0512 23:42:16.566717 140529434355712 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_5/mlp/expert/wi_1/kernel
I0512 23:42:16.566747 140529434355712 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_5/mlp/expert/wo/kernel
I0512 23:42:16.566776 140529434355712 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_5/mlp/router/router_weights/w/kernel
I0512 23:42:16.566806 140529434355712 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_5/pre_extra_mlp_layer_norm/scale
I0512 23:42:16.566841 140529434355712 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_5/pre_mlp_layer_norm/scale
I0512 23:42:16.566871 140529434355712 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_5/pre_self_attention_layer_norm/scale
I0512 23:42:16.566901 140529434355712 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_5/self_attention/key/kernel
I0512 23:42:16.566931 140529434355712 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_5/self_attention/out/kernel
I0512 23:42:16.566961 140529434355712 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_5/self_attention/query/kernel
I0512 23:42:16.566991 140529434355712 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_5/self_attention/value/kernel
I0512 23:42:16.567020 140529434355712 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_6/mlp/wi_0/kernel
I0512 23:42:16.567050 140529434355712 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_6/mlp/wi_1/kernel
I0512 23:42:16.567081 140529434355712 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_6/mlp/wo/kernel
I0512 23:42:16.567112 140529434355712 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_6/pre_mlp_layer_norm/scale
I0512 23:42:16.567142 140529434355712 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_6/pre_self_attention_layer_norm/scale
I0512 23:42:16.567172 140529434355712 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_6/self_attention/key/kernel
I0512 23:42:16.567201 140529434355712 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_6/self_attention/out/kernel
I0512 23:42:16.567231 140529434355712 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_6/self_attention/query/kernel
I0512 23:42:16.567261 140529434355712 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_6/self_attention/value/kernel
I0512 23:42:16.567290 140529434355712 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_7/mlp/wi_0/kernel
I0512 23:42:16.567320 140529434355712 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_7/mlp/wi_1/kernel
I0512 23:42:16.567350 140529434355712 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_7/mlp/wo/kernel
I0512 23:42:16.567379 140529434355712 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_7/pre_mlp_layer_norm/scale
I0512 23:42:16.567409 140529434355712 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_7/pre_self_attention_layer_norm/scale
I0512 23:42:16.567439 140529434355712 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_7/self_attention/key/kernel
I0512 23:42:16.567469 140529434355712 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_7/self_attention/out/kernel
I0512 23:42:16.567498 140529434355712 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_7/self_attention/query/kernel
I0512 23:42:16.567528 140529434355712 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_7/self_attention/value/kernel
I0512 23:42:16.567557 140529434355712 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_8/mlp/wi_0/kernel
I0512 23:42:16.567587 140529434355712 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_8/mlp/wi_1/kernel
I0512 23:42:16.567617 140529434355712 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_8/mlp/wo/kernel
I0512 23:42:16.567647 140529434355712 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_8/pre_mlp_layer_norm/scale
I0512 23:42:16.567677 140529434355712 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_8/pre_self_attention_layer_norm/scale
I0512 23:42:16.567706 140529434355712 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_8/self_attention/key/kernel
I0512 23:42:16.567736 140529434355712 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_8/self_attention/out/kernel
I0512 23:42:16.567765 140529434355712 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_8/self_attention/query/kernel
I0512 23:42:16.567795 140529434355712 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_8/self_attention/value/kernel
I0512 23:42:16.567830 140529434355712 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_9/mlp/wi_0/kernel
I0512 23:42:16.567861 140529434355712 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_9/mlp/wi_1/kernel
I0512 23:42:16.567890 140529434355712 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_9/mlp/wo/kernel
I0512 23:42:16.567920 140529434355712 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_9/pre_mlp_layer_norm/scale
I0512 23:42:16.567950 140529434355712 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_9/pre_self_attention_layer_norm/scale
I0512 23:42:16.567980 140529434355712 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_9/self_attention/key/kernel
I0512 23:42:16.568009 140529434355712 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_9/self_attention/out/kernel
I0512 23:42:16.568038 140529434355712 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_9/self_attention/query/kernel
I0512 23:42:16.568069 140529434355712 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_9/self_attention/value/kernel
I0512 23:42:16.568100 140529434355712 checkpoints.py:1100] Restoring key from ckpt: target/decoder/logits_dense/kernel
I0512 23:42:16.568130 140529434355712 checkpoints.py:1100] Restoring key from ckpt: target/token_embedder/embedding
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
I0000 00:00:1715557336.598017  489306 gcs_resource.cc:109] Using default AdmissionQueue with limit 32
I0000 00:00:1715557336.602623  491472 google_auth_provider.cc:180] Running on GCE, using service account 1081612546678-compute@developer.gserviceaccount.com
I0512 23:42:23.238043 140440593786880 train.py:421] Initialize/restore complete (11.41 seconds).
I0512 23:42:23.239366 140440593786880 train.py:573] Saving checkpoint before the training loop starts.
I0512 23:42:23.244389 139674595104768 train.py:421] Initialize/restore complete (11.49 seconds).
I0512 23:42:23.245690 139674595104768 train.py:573] Saving checkpoint before the training loop starts.
I0512 23:42:23.388530 140095893518336 train.py:421] Initialize/restore complete (11.48 seconds).
I0512 23:42:23.390193 140095893518336 train.py:573] Saving checkpoint before the training loop starts.
I0512 23:42:23.488474 140311617681408 train.py:421] Initialize/restore complete (11.63 seconds).
I0512 23:42:23.489813 140311617681408 train.py:573] Saving checkpoint before the training loop starts.
I0512 23:42:23.578390 140031916288000 train.py:421] Initialize/restore complete (11.58 seconds).
I0512 23:42:23.580181 140031916288000 train.py:573] Saving checkpoint before the training loop starts.
I0512 23:42:23.659791 140339465779200 train.py:421] Initialize/restore complete (11.93 seconds).
I0512 23:42:23.661432 140339465779200 train.py:573] Saving checkpoint before the training loop starts.
I0512 23:42:24.126745 140595318249472 train.py:421] Initialize/restore complete (12.20 seconds).
I0512 23:42:24.128694 140595318249472 train.py:573] Saving checkpoint before the training loop starts.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
I0512 23:42:24.795322 140529434355712 train.py:421] Initialize/restore complete (12.83 seconds).
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
I0512 23:42:24.974176 140529434355712 utils.py:1372] Variable decoder/decoder_norm/scale                                                       size 2048         shape (embed=2048)                             partition spec ('model',)
I0512 23:42:24.974699 140529434355712 utils.py:1372] Variable decoder/layers_0/mlp/wi_0/kernel                                                 size 33554432     shape (mlp_embed=2048, mlp=16384)              partition spec (None, 'model')
I0512 23:42:24.974769 140529434355712 utils.py:1372] Variable decoder/layers_0/mlp/wi_1/kernel                                                 size 16777216     shape (mlp_embed=2048, mlp=8192)               partition spec (None, 'model')
I0512 23:42:24.974820 140529434355712 utils.py:1372] Variable decoder/layers_0/mlp/wo/kernel                                                   size 16777216     shape (mlp=8192, mlp_embed=2048)               partition spec ('model', None)
I0512 23:42:24.974867 140529434355712 utils.py:1372] Variable decoder/layers_0/pre_mlp_layer_norm/scale                                        size 2048         shape (embed=2048)                             partition spec ('model',)
I0512 23:42:24.974913 140529434355712 utils.py:1372] Variable decoder/layers_0/pre_self_attention_layer_norm/scale                             size 2048         shape (embed=2048)                             partition spec ('model',)
I0512 23:42:24.974959 140529434355712 utils.py:1372] Variable decoder/layers_0/self_attention/key/kernel                                       size 6291456      shape (embed=2048, joined_kv=3072)             partition spec (None, 'model')
I0512 23:42:24.975013 140529434355712 utils.py:1372] Variable decoder/layers_0/self_attention/out/kernel                                       size 6291456      shape (joined_kv=3072, embed=2048)             partition spec ('model', None)
I0512 23:42:24.975060 140529434355712 utils.py:1372] Variable decoder/layers_0/self_attention/query/kernel                                     size 6291456      shape (embed=2048, joined_kv=3072)             partition spec (None, 'model')
I0512 23:42:24.975106 140529434355712 utils.py:1372] Variable decoder/layers_0/self_attention/value/kernel                                     size 6291456      shape (embed=2048, joined_kv=3072)             partition spec (None, 'model')
I0512 23:42:24.975151 140529434355712 utils.py:1372] Variable decoder/layers_1/mlp/wi_0/kernel                                                 size 33554432     shape (mlp_embed=2048, mlp=16384)              partition spec (None, 'model')
I0512 23:42:24.975197 140529434355712 utils.py:1372] Variable decoder/layers_1/mlp/wi_1/kernel                                                 size 16777216     shape (mlp_embed=2048, mlp=8192)               partition spec (None, 'model')
I0512 23:42:24.975242 140529434355712 utils.py:1372] Variable decoder/layers_1/mlp/wo/kernel                                                   size 16777216     shape (mlp=8192, mlp_embed=2048)               partition spec ('model', None)
I0512 23:42:24.975287 140529434355712 utils.py:1372] Variable decoder/layers_1/pre_mlp_layer_norm/scale                                        size 2048         shape (embed=2048)                             partition spec ('model',)
I0512 23:42:24.975332 140529434355712 utils.py:1372] Variable decoder/layers_1/pre_self_attention_layer_norm/scale                             size 2048         shape (embed=2048)                             partition spec ('model',)
I0512 23:42:24.975376 140529434355712 utils.py:1372] Variable decoder/layers_1/self_attention/key/kernel                                       size 6291456      shape (embed=2048, joined_kv=3072)             partition spec (None, 'model')
I0512 23:42:24.975421 140529434355712 utils.py:1372] Variable decoder/layers_1/self_attention/out/kernel                                       size 6291456      shape (joined_kv=3072, embed=2048)             partition spec ('model', None)
I0512 23:42:24.975467 140529434355712 utils.py:1372] Variable decoder/layers_1/self_attention/query/kernel                                     size 6291456      shape (embed=2048, joined_kv=3072)             partition spec (None, 'model')
I0512 23:42:24.975512 140529434355712 utils.py:1372] Variable decoder/layers_1/self_attention/value/kernel                                     size 6291456      shape (embed=2048, joined_kv=3072)             partition spec (None, 'model')
I0512 23:42:24.975557 140529434355712 utils.py:1372] Variable decoder/layers_10/mlp/wi_0/kernel                                                size 33554432     shape (mlp_embed=2048, mlp=16384)              partition spec (None, 'model')
I0512 23:42:24.975603 140529434355712 utils.py:1372] Variable decoder/layers_10/mlp/wi_1/kernel                                                size 16777216     shape (mlp_embed=2048, mlp=8192)               partition spec (None, 'model')
I0512 23:42:24.975648 140529434355712 utils.py:1372] Variable decoder/layers_10/mlp/wo/kernel                                                  size 16777216     shape (mlp=8192, mlp_embed=2048)               partition spec ('model', None)
I0512 23:42:24.975692 140529434355712 utils.py:1372] Variable decoder/layers_10/pre_mlp_layer_norm/scale                                       size 2048         shape (embed=2048)                             partition spec ('model',)
I0512 23:42:24.975738 140529434355712 utils.py:1372] Variable decoder/layers_10/pre_self_attention_layer_norm/scale                            size 2048         shape (embed=2048)                             partition spec ('model',)
I0512 23:42:24.975786 140529434355712 utils.py:1372] Variable decoder/layers_10/self_attention/key/kernel                                      size 6291456      shape (embed=2048, joined_kv=3072)             partition spec (None, 'model')
I0512 23:42:24.975831 140529434355712 utils.py:1372] Variable decoder/layers_10/self_attention/out/kernel                                      size 6291456      shape (joined_kv=3072, embed=2048)             partition spec ('model', None)
I0512 23:42:24.975877 140529434355712 utils.py:1372] Variable decoder/layers_10/self_attention/query/kernel                                    size 6291456      shape (embed=2048, joined_kv=3072)             partition spec (None, 'model')
I0512 23:42:24.975923 140529434355712 utils.py:1372] Variable decoder/layers_10/self_attention/value/kernel                                    size 6291456      shape (embed=2048, joined_kv=3072)             partition spec (None, 'model')
I0512 23:42:24.975968 140529434355712 utils.py:1372] Variable decoder/layers_11/extra_mlp/wi_0/kernel                                          size 33554432     shape (mlp_embed=2048, mlp=16384)              partition spec (None, 'model')
I0512 23:42:24.976047 140529434355712 utils.py:1372] Variable decoder/layers_11/extra_mlp/wi_1/kernel                                          size 16777216     shape (mlp_embed=2048, mlp=8192)               partition spec (None, 'model')
I0512 23:42:24.976094 140529434355712 utils.py:1372] Variable decoder/layers_11/extra_mlp/wo/kernel                                            size 16777216     shape (mlp=8192, mlp_embed=2048)               partition spec ('model', None)
I0512 23:42:24.976140 140529434355712 utils.py:1372] Variable decoder/layers_11/mlp/expert/wi_0/kernel                                         size 268435456    shape (expert=8, mlp_embed=2048, expert_mlp=16384) partition spec ('expert', None, 'model')
I0512 23:42:24.976186 140529434355712 utils.py:1372] Variable decoder/layers_11/mlp/expert/wi_1/kernel                                         size 134217728    shape (expert=8, mlp_embed=2048, expert_mlp=8192) partition spec ('expert', None, 'model')
I0512 23:42:24.976233 140529434355712 utils.py:1372] Variable decoder/layers_11/mlp/expert/wo/kernel                                           size 134217728    shape (expert=8, expert_mlp=8192, mlp_embed=2048) partition spec ('expert', 'model', None)
I0512 23:42:24.976279 140529434355712 utils.py:1372] Variable decoder/layers_11/mlp/router/router_weights/w/kernel                             size 16384        shape (embed=2048, unmodeled=8)                partition spec ('model', None)
I0512 23:42:24.976325 140529434355712 utils.py:1372] Variable decoder/layers_11/pre_extra_mlp_layer_norm/scale                                 size 2048         shape (embed=2048)                             partition spec ('model',)
I0512 23:42:24.976371 140529434355712 utils.py:1372] Variable decoder/layers_11/pre_mlp_layer_norm/scale                                       size 2048         shape (embed=2048)                             partition spec ('model',)
I0512 23:42:24.976416 140529434355712 utils.py:1372] Variable decoder/layers_11/pre_self_attention_layer_norm/scale                            size 2048         shape (embed=2048)                             partition spec ('model',)
I0512 23:42:24.976461 140529434355712 utils.py:1372] Variable decoder/layers_11/self_attention/key/kernel                                      size 6291456      shape (embed=2048, joined_kv=3072)             partition spec (None, 'model')
I0512 23:42:24.976506 140529434355712 utils.py:1372] Variable decoder/layers_11/self_attention/out/kernel                                      size 6291456      shape (joined_kv=3072, embed=2048)             partition spec ('model', None)
I0512 23:42:24.976552 140529434355712 utils.py:1372] Variable decoder/layers_11/self_attention/query/kernel                                    size 6291456      shape (embed=2048, joined_kv=3072)             partition spec (None, 'model')
I0512 23:42:24.976597 140529434355712 utils.py:1372] Variable decoder/layers_11/self_attention/value/kernel                                    size 6291456      shape (embed=2048, joined_kv=3072)             partition spec (None, 'model')
I0512 23:42:24.976641 140529434355712 utils.py:1372] Variable decoder/layers_12/mlp/wi_0/kernel                                                size 33554432     shape (mlp_embed=2048, mlp=16384)              partition spec (None, 'model')
I0512 23:42:24.976687 140529434355712 utils.py:1372] Variable decoder/layers_12/mlp/wi_1/kernel                                                size 16777216     shape (mlp_embed=2048, mlp=8192)               partition spec (None, 'model')
I0512 23:42:24.976733 140529434355712 utils.py:1372] Variable decoder/layers_12/mlp/wo/kernel                                                  size 16777216     shape (mlp=8192, mlp_embed=2048)               partition spec ('model', None)
I0512 23:42:24.976827 140529434355712 utils.py:1372] Variable decoder/layers_12/pre_mlp_layer_norm/scale                                       size 2048         shape (embed=2048)                             partition spec ('model',)
I0512 23:42:24.976877 140529434355712 utils.py:1372] Variable decoder/layers_12/pre_self_attention_layer_norm/scale                            size 2048         shape (embed=2048)                             partition spec ('model',)
I0512 23:42:24.976922 140529434355712 utils.py:1372] Variable decoder/layers_12/self_attention/key/kernel                                      size 6291456      shape (embed=2048, joined_kv=3072)             partition spec (None, 'model')
I0512 23:42:24.976968 140529434355712 utils.py:1372] Variable decoder/layers_12/self_attention/out/kernel                                      size 6291456      shape (joined_kv=3072, embed=2048)             partition spec ('model', None)
I0512 23:42:24.977021 140529434355712 utils.py:1372] Variable decoder/layers_12/self_attention/query/kernel                                    size 6291456      shape (embed=2048, joined_kv=3072)             partition spec (None, 'model')
I0512 23:42:24.977067 140529434355712 utils.py:1372] Variable decoder/layers_12/self_attention/value/kernel                                    size 6291456      shape (embed=2048, joined_kv=3072)             partition spec (None, 'model')
I0512 23:42:24.977112 140529434355712 utils.py:1372] Variable decoder/layers_13/mlp/wi_0/kernel                                                size 33554432     shape (mlp_embed=2048, mlp=16384)              partition spec (None, 'model')
I0512 23:42:24.977158 140529434355712 utils.py:1372] Variable decoder/layers_13/mlp/wi_1/kernel                                                size 16777216     shape (mlp_embed=2048, mlp=8192)               partition spec (None, 'model')
I0512 23:42:24.977203 140529434355712 utils.py:1372] Variable decoder/layers_13/mlp/wo/kernel                                                  size 16777216     shape (mlp=8192, mlp_embed=2048)               partition spec ('model', None)
I0512 23:42:24.977248 140529434355712 utils.py:1372] Variable decoder/layers_13/pre_mlp_layer_norm/scale                                       size 2048         shape (embed=2048)                             partition spec ('model',)
I0512 23:42:24.977293 140529434355712 utils.py:1372] Variable decoder/layers_13/pre_self_attention_layer_norm/scale                            size 2048         shape (embed=2048)                             partition spec ('model',)
I0512 23:42:24.977338 140529434355712 utils.py:1372] Variable decoder/layers_13/self_attention/key/kernel                                      size 6291456      shape (embed=2048, joined_kv=3072)             partition spec (None, 'model')
I0512 23:42:24.977383 140529434355712 utils.py:1372] Variable decoder/layers_13/self_attention/out/kernel                                      size 6291456      shape (joined_kv=3072, embed=2048)             partition spec ('model', None)
I0512 23:42:24.977428 140529434355712 utils.py:1372] Variable decoder/layers_13/self_attention/query/kernel                                    size 6291456      shape (embed=2048, joined_kv=3072)             partition spec (None, 'model')
I0512 23:42:24.977473 140529434355712 utils.py:1372] Variable decoder/layers_13/self_attention/value/kernel                                    size 6291456      shape (embed=2048, joined_kv=3072)             partition spec (None, 'model')
I0512 23:42:24.977519 140529434355712 utils.py:1372] Variable decoder/layers_14/mlp/wi_0/kernel                                                size 33554432     shape (mlp_embed=2048, mlp=16384)              partition spec (None, 'model')
I0512 23:42:24.977590 140529434355712 utils.py:1372] Variable decoder/layers_14/mlp/wi_1/kernel                                                size 16777216     shape (mlp_embed=2048, mlp=8192)               partition spec (None, 'model')
I0512 23:42:24.977638 140529434355712 utils.py:1372] Variable decoder/layers_14/mlp/wo/kernel                                                  size 16777216     shape (mlp=8192, mlp_embed=2048)               partition spec ('model', None)
I0512 23:42:24.977684 140529434355712 utils.py:1372] Variable decoder/layers_14/pre_mlp_layer_norm/scale                                       size 2048         shape (embed=2048)                             partition spec ('model',)
I0512 23:42:24.977730 140529434355712 utils.py:1372] Variable decoder/layers_14/pre_self_attention_layer_norm/scale                            size 2048         shape (embed=2048)                             partition spec ('model',)
I0512 23:42:24.977778 140529434355712 utils.py:1372] Variable decoder/layers_14/self_attention/key/kernel                                      size 6291456      shape (embed=2048, joined_kv=3072)             partition spec (None, 'model')
I0512 23:42:24.977823 140529434355712 utils.py:1372] Variable decoder/layers_14/self_attention/out/kernel                                      size 6291456      shape (joined_kv=3072, embed=2048)             partition spec ('model', None)
I0512 23:42:24.977868 140529434355712 utils.py:1372] Variable decoder/layers_14/self_attention/query/kernel                                    size 6291456      shape (embed=2048, joined_kv=3072)             partition spec (None, 'model')
I0512 23:42:24.977913 140529434355712 utils.py:1372] Variable decoder/layers_14/self_attention/value/kernel                                    size 6291456      shape (embed=2048, joined_kv=3072)             partition spec (None, 'model')
I0512 23:42:24.977957 140529434355712 utils.py:1372] Variable decoder/layers_15/mlp/wi_0/kernel                                                size 33554432     shape (mlp_embed=2048, mlp=16384)              partition spec (None, 'model')
I0512 23:42:24.978010 140529434355712 utils.py:1372] Variable decoder/layers_15/mlp/wi_1/kernel                                                size 16777216     shape (mlp_embed=2048, mlp=8192)               partition spec (None, 'model')
I0512 23:42:24.978055 140529434355712 utils.py:1372] Variable decoder/layers_15/mlp/wo/kernel                                                  size 16777216     shape (mlp=8192, mlp_embed=2048)               partition spec ('model', None)
I0512 23:42:24.978100 140529434355712 utils.py:1372] Variable decoder/layers_15/pre_mlp_layer_norm/scale                                       size 2048         shape (embed=2048)                             partition spec ('model',)
I0512 23:42:24.978145 140529434355712 utils.py:1372] Variable decoder/layers_15/pre_self_attention_layer_norm/scale                            size 2048         shape (embed=2048)                             partition spec ('model',)
I0512 23:42:24.978190 140529434355712 utils.py:1372] Variable decoder/layers_15/self_attention/key/kernel                                      size 6291456      shape (embed=2048, joined_kv=3072)             partition spec (None, 'model')
I0512 23:42:24.978235 140529434355712 utils.py:1372] Variable decoder/layers_15/self_attention/out/kernel                                      size 6291456      shape (joined_kv=3072, embed=2048)             partition spec ('model', None)
I0512 23:42:24.978281 140529434355712 utils.py:1372] Variable decoder/layers_15/self_attention/query/kernel                                    size 6291456      shape (embed=2048, joined_kv=3072)             partition spec (None, 'model')
I0512 23:42:24.978325 140529434355712 utils.py:1372] Variable decoder/layers_15/self_attention/value/kernel                                    size 6291456      shape (embed=2048, joined_kv=3072)             partition spec (None, 'model')
I0512 23:42:24.978370 140529434355712 utils.py:1372] Variable decoder/layers_16/mlp/wi_0/kernel                                                size 33554432     shape (mlp_embed=2048, mlp=16384)              partition spec (None, 'model')
I0512 23:42:24.978416 140529434355712 utils.py:1372] Variable decoder/layers_16/mlp/wi_1/kernel                                                size 16777216     shape (mlp_embed=2048, mlp=8192)               partition spec (None, 'model')
I0512 23:42:24.978461 140529434355712 utils.py:1372] Variable decoder/layers_16/mlp/wo/kernel                                                  size 16777216     shape (mlp=8192, mlp_embed=2048)               partition spec ('model', None)
I0512 23:42:24.978505 140529434355712 utils.py:1372] Variable decoder/layers_16/pre_mlp_layer_norm/scale                                       size 2048         shape (embed=2048)                             partition spec ('model',)
I0512 23:42:24.978550 140529434355712 utils.py:1372] Variable decoder/layers_16/pre_self_attention_layer_norm/scale                            size 2048         shape (embed=2048)                             partition spec ('model',)
I0512 23:42:24.978595 140529434355712 utils.py:1372] Variable decoder/layers_16/self_attention/key/kernel                                      size 6291456      shape (embed=2048, joined_kv=3072)             partition spec (None, 'model')
I0512 23:42:24.978640 140529434355712 utils.py:1372] Variable decoder/layers_16/self_attention/out/kernel                                      size 6291456      shape (joined_kv=3072, embed=2048)             partition spec ('model', None)
I0512 23:42:24.978684 140529434355712 utils.py:1372] Variable decoder/layers_16/self_attention/query/kernel                                    size 6291456      shape (embed=2048, joined_kv=3072)             partition spec (None, 'model')
I0512 23:42:24.978729 140529434355712 utils.py:1372] Variable decoder/layers_16/self_attention/value/kernel                                    size 6291456      shape (embed=2048, joined_kv=3072)             partition spec (None, 'model')
I0512 23:42:24.978776 140529434355712 utils.py:1372] Variable decoder/layers_17/extra_mlp/wi_0/kernel                                          size 33554432     shape (mlp_embed=2048, mlp=16384)              partition spec (None, 'model')
I0512 23:42:24.978823 140529434355712 utils.py:1372] Variable decoder/layers_17/extra_mlp/wi_1/kernel                                          size 16777216     shape (mlp_embed=2048, mlp=8192)               partition spec (None, 'model')
I0512 23:42:24.978868 140529434355712 utils.py:1372] Variable decoder/layers_17/extra_mlp/wo/kernel                                            size 16777216     shape (mlp=8192, mlp_embed=2048)               partition spec ('model', None)
I0512 23:42:24.978930 140529434355712 utils.py:1372] Variable decoder/layers_17/mlp/expert/wi_0/kernel                                         size 268435456    shape (expert=8, mlp_embed=2048, expert_mlp=16384) partition spec ('expert', None, 'model')
I0512 23:42:24.978980 140529434355712 utils.py:1372] Variable decoder/layers_17/mlp/expert/wi_1/kernel                                         size 134217728    shape (expert=8, mlp_embed=2048, expert_mlp=8192) partition spec ('expert', None, 'model')
I0512 23:42:24.979034 140529434355712 utils.py:1372] Variable decoder/layers_17/mlp/expert/wo/kernel                                           size 134217728    shape (expert=8, expert_mlp=8192, mlp_embed=2048) partition spec ('expert', 'model', None)
I0512 23:42:24.979080 140529434355712 utils.py:1372] Variable decoder/layers_17/mlp/router/router_weights/w/kernel                             size 16384        shape (embed=2048, unmodeled=8)                partition spec ('model', None)
I0512 23:42:24.979125 140529434355712 utils.py:1372] Variable decoder/layers_17/pre_extra_mlp_layer_norm/scale                                 size 2048         shape (embed=2048)                             partition spec ('model',)
I0512 23:42:24.979171 140529434355712 utils.py:1372] Variable decoder/layers_17/pre_mlp_layer_norm/scale                                       size 2048         shape (embed=2048)                             partition spec ('model',)
I0512 23:42:24.979215 140529434355712 utils.py:1372] Variable decoder/layers_17/pre_self_attention_layer_norm/scale                            size 2048         shape (embed=2048)                             partition spec ('model',)
I0512 23:42:24.979260 140529434355712 utils.py:1372] Variable decoder/layers_17/self_attention/key/kernel                                      size 6291456      shape (embed=2048, joined_kv=3072)             partition spec (None, 'model')
I0512 23:42:24.979305 140529434355712 utils.py:1372] Variable decoder/layers_17/self_attention/out/kernel                                      size 6291456      shape (joined_kv=3072, embed=2048)             partition spec ('model', None)
I0512 23:42:24.979351 140529434355712 utils.py:1372] Variable decoder/layers_17/self_attention/query/kernel                                    size 6291456      shape (embed=2048, joined_kv=3072)             partition spec (None, 'model')
I0512 23:42:24.979396 140529434355712 utils.py:1372] Variable decoder/layers_17/self_attention/value/kernel                                    size 6291456      shape (embed=2048, joined_kv=3072)             partition spec (None, 'model')
I0512 23:42:24.979441 140529434355712 utils.py:1372] Variable decoder/layers_18/mlp/wi_0/kernel                                                size 33554432     shape (mlp_embed=2048, mlp=16384)              partition spec (None, 'model')
I0512 23:42:24.979487 140529434355712 utils.py:1372] Variable decoder/layers_18/mlp/wi_1/kernel                                                size 16777216     shape (mlp_embed=2048, mlp=8192)               partition spec (None, 'model')
I0512 23:42:24.979532 140529434355712 utils.py:1372] Variable decoder/layers_18/mlp/wo/kernel                                                  size 16777216     shape (mlp=8192, mlp_embed=2048)               partition spec ('model', None)
I0512 23:42:24.979576 140529434355712 utils.py:1372] Variable decoder/layers_18/pre_mlp_layer_norm/scale                                       size 2048         shape (embed=2048)                             partition spec ('model',)
I0512 23:42:24.979621 140529434355712 utils.py:1372] Variable decoder/layers_18/pre_self_attention_layer_norm/scale                            size 2048         shape (embed=2048)                             partition spec ('model',)
I0512 23:42:24.979666 140529434355712 utils.py:1372] Variable decoder/layers_18/self_attention/key/kernel                                      size 6291456      shape (embed=2048, joined_kv=3072)             partition spec (None, 'model')
I0512 23:42:24.979711 140529434355712 utils.py:1372] Variable decoder/layers_18/self_attention/out/kernel                                      size 6291456      shape (joined_kv=3072, embed=2048)             partition spec ('model', None)
I0512 23:42:24.979758 140529434355712 utils.py:1372] Variable decoder/layers_18/self_attention/query/kernel                                    size 6291456      shape (embed=2048, joined_kv=3072)             partition spec (None, 'model')
I0512 23:42:24.979804 140529434355712 utils.py:1372] Variable decoder/layers_18/self_attention/value/kernel                                    size 6291456      shape (embed=2048, joined_kv=3072)             partition spec (None, 'model')
I0512 23:42:24.979849 140529434355712 utils.py:1372] Variable decoder/layers_19/mlp/wi_0/kernel                                                size 33554432     shape (mlp_embed=2048, mlp=16384)              partition spec (None, 'model')
I0512 23:42:24.979895 140529434355712 utils.py:1372] Variable decoder/layers_19/mlp/wi_1/kernel                                                size 16777216     shape (mlp_embed=2048, mlp=8192)               partition spec (None, 'model')
I0512 23:42:24.979940 140529434355712 utils.py:1372] Variable decoder/layers_19/mlp/wo/kernel                                                  size 16777216     shape (mlp=8192, mlp_embed=2048)               partition spec ('model', None)
I0512 23:42:24.979992 140529434355712 utils.py:1372] Variable decoder/layers_19/pre_mlp_layer_norm/scale                                       size 2048         shape (embed=2048)                             partition spec ('model',)
I0512 23:42:24.980039 140529434355712 utils.py:1372] Variable decoder/layers_19/pre_self_attention_layer_norm/scale                            size 2048         shape (embed=2048)                             partition spec ('model',)
I0512 23:42:24.980084 140529434355712 utils.py:1372] Variable decoder/layers_19/self_attention/key/kernel                                      size 6291456      shape (embed=2048, joined_kv=3072)             partition spec (None, 'model')
I0512 23:42:24.980129 140529434355712 utils.py:1372] Variable decoder/layers_19/self_attention/out/kernel                                      size 6291456      shape (joined_kv=3072, embed=2048)             partition spec ('model', None)
I0512 23:42:24.980174 140529434355712 utils.py:1372] Variable decoder/layers_19/self_attention/query/kernel                                    size 6291456      shape (embed=2048, joined_kv=3072)             partition spec (None, 'model')
I0512 23:42:24.980219 140529434355712 utils.py:1372] Variable decoder/layers_19/self_attention/value/kernel                                    size 6291456      shape (embed=2048, joined_kv=3072)             partition spec (None, 'model')
I0512 23:42:24.980265 140529434355712 utils.py:1372] Variable decoder/layers_2/mlp/wi_0/kernel                                                 size 33554432     shape (mlp_embed=2048, mlp=16384)              partition spec (None, 'model')
I0512 23:42:24.980311 140529434355712 utils.py:1372] Variable decoder/layers_2/mlp/wi_1/kernel                                                 size 16777216     shape (mlp_embed=2048, mlp=8192)               partition spec (None, 'model')
I0512 23:42:24.980355 140529434355712 utils.py:1372] Variable decoder/layers_2/mlp/wo/kernel                                                   size 16777216     shape (mlp=8192, mlp_embed=2048)               partition spec ('model', None)
I0512 23:42:24.980400 140529434355712 utils.py:1372] Variable decoder/layers_2/pre_mlp_layer_norm/scale                                        size 2048         shape (embed=2048)                             partition spec ('model',)
I0512 23:42:24.980445 140529434355712 utils.py:1372] Variable decoder/layers_2/pre_self_attention_layer_norm/scale                             size 2048         shape (embed=2048)                             partition spec ('model',)
I0512 23:42:24.980490 140529434355712 utils.py:1372] Variable decoder/layers_2/self_attention/key/kernel                                       size 6291456      shape (embed=2048, joined_kv=3072)             partition spec (None, 'model')
I0512 23:42:24.980535 140529434355712 utils.py:1372] Variable decoder/layers_2/self_attention/out/kernel                                       size 6291456      shape (joined_kv=3072, embed=2048)             partition spec ('model', None)
I0512 23:42:24.980580 140529434355712 utils.py:1372] Variable decoder/layers_2/self_attention/query/kernel                                     size 6291456      shape (embed=2048, joined_kv=3072)             partition spec (None, 'model')
I0512 23:42:24.980624 140529434355712 utils.py:1372] Variable decoder/layers_2/self_attention/value/kernel                                     size 6291456      shape (embed=2048, joined_kv=3072)             partition spec (None, 'model')
I0512 23:42:24.980669 140529434355712 utils.py:1372] Variable decoder/layers_20/mlp/wi_0/kernel                                                size 33554432     shape (mlp_embed=2048, mlp=16384)              partition spec (None, 'model')
I0512 23:42:24.980715 140529434355712 utils.py:1372] Variable decoder/layers_20/mlp/wi_1/kernel                                                size 16777216     shape (mlp_embed=2048, mlp=8192)               partition spec (None, 'model')
I0512 23:42:24.980762 140529434355712 utils.py:1372] Variable decoder/layers_20/mlp/wo/kernel                                                  size 16777216     shape (mlp=8192, mlp_embed=2048)               partition spec ('model', None)
I0512 23:42:24.980807 140529434355712 utils.py:1372] Variable decoder/layers_20/pre_mlp_layer_norm/scale                                       size 2048         shape (embed=2048)                             partition spec ('model',)
I0512 23:42:24.980852 140529434355712 utils.py:1372] Variable decoder/layers_20/pre_self_attention_layer_norm/scale                            size 2048         shape (embed=2048)                             partition spec ('model',)
I0512 23:42:24.980897 140529434355712 utils.py:1372] Variable decoder/layers_20/self_attention/key/kernel                                      size 6291456      shape (embed=2048, joined_kv=3072)             partition spec (None, 'model')
I0512 23:42:24.980942 140529434355712 utils.py:1372] Variable decoder/layers_20/self_attention/out/kernel                                      size 6291456      shape (joined_kv=3072, embed=2048)             partition spec ('model', None)
I0512 23:42:24.981009 140529434355712 utils.py:1372] Variable decoder/layers_20/self_attention/query/kernel                                    size 6291456      shape (embed=2048, joined_kv=3072)             partition spec (None, 'model')
I0512 23:42:24.981058 140529434355712 utils.py:1372] Variable decoder/layers_20/self_attention/value/kernel                                    size 6291456      shape (embed=2048, joined_kv=3072)             partition spec (None, 'model')
I0512 23:42:24.981103 140529434355712 utils.py:1372] Variable decoder/layers_21/mlp/wi_0/kernel                                                size 33554432     shape (mlp_embed=2048, mlp=16384)              partition spec (None, 'model')
I0512 23:42:24.981149 140529434355712 utils.py:1372] Variable decoder/layers_21/mlp/wi_1/kernel                                                size 16777216     shape (mlp_embed=2048, mlp=8192)               partition spec (None, 'model')
I0512 23:42:24.981194 140529434355712 utils.py:1372] Variable decoder/layers_21/mlp/wo/kernel                                                  size 16777216     shape (mlp=8192, mlp_embed=2048)               partition spec ('model', None)
I0512 23:42:24.981238 140529434355712 utils.py:1372] Variable decoder/layers_21/pre_mlp_layer_norm/scale                                       size 2048         shape (embed=2048)                             partition spec ('model',)
I0512 23:42:24.981283 140529434355712 utils.py:1372] Variable decoder/layers_21/pre_self_attention_layer_norm/scale                            size 2048         shape (embed=2048)                             partition spec ('model',)
I0512 23:42:24.981328 140529434355712 utils.py:1372] Variable decoder/layers_21/self_attention/key/kernel                                      size 6291456      shape (embed=2048, joined_kv=3072)             partition spec (None, 'model')
I0512 23:42:24.981373 140529434355712 utils.py:1372] Variable decoder/layers_21/self_attention/out/kernel                                      size 6291456      shape (joined_kv=3072, embed=2048)             partition spec ('model', None)
I0512 23:42:24.981419 140529434355712 utils.py:1372] Variable decoder/layers_21/self_attention/query/kernel                                    size 6291456      shape (embed=2048, joined_kv=3072)             partition spec (None, 'model')
I0512 23:42:24.981464 140529434355712 utils.py:1372] Variable decoder/layers_21/self_attention/value/kernel                                    size 6291456      shape (embed=2048, joined_kv=3072)             partition spec (None, 'model')
I0512 23:42:24.981509 140529434355712 utils.py:1372] Variable decoder/layers_22/mlp/wi_0/kernel                                                size 33554432     shape (mlp_embed=2048, mlp=16384)              partition spec (None, 'model')
I0512 23:42:24.981591 140529434355712 utils.py:1372] Variable decoder/layers_22/mlp/wi_1/kernel                                                size 16777216     shape (mlp_embed=2048, mlp=8192)               partition spec (None, 'model')
I0512 23:42:24.981642 140529434355712 utils.py:1372] Variable decoder/layers_22/mlp/wo/kernel                                                  size 16777216     shape (mlp=8192, mlp_embed=2048)               partition spec ('model', None)
I0512 23:42:24.981687 140529434355712 utils.py:1372] Variable decoder/layers_22/pre_mlp_layer_norm/scale                                       size 2048         shape (embed=2048)                             partition spec ('model',)
I0512 23:42:24.981733 140529434355712 utils.py:1372] Variable decoder/layers_22/pre_self_attention_layer_norm/scale                            size 2048         shape (embed=2048)                             partition spec ('model',)
I0512 23:42:24.981781 140529434355712 utils.py:1372] Variable decoder/layers_22/self_attention/key/kernel                                      size 6291456      shape (embed=2048, joined_kv=3072)             partition spec (None, 'model')
I0512 23:42:24.981827 140529434355712 utils.py:1372] Variable decoder/layers_22/self_attention/out/kernel                                      size 6291456      shape (joined_kv=3072, embed=2048)             partition spec ('model', None)
I0512 23:42:24.981873 140529434355712 utils.py:1372] Variable decoder/layers_22/self_attention/query/kernel                                    size 6291456      shape (embed=2048, joined_kv=3072)             partition spec (None, 'model')
I0512 23:42:24.981918 140529434355712 utils.py:1372] Variable decoder/layers_22/self_attention/value/kernel                                    size 6291456      shape (embed=2048, joined_kv=3072)             partition spec (None, 'model')
I0512 23:42:24.981963 140529434355712 utils.py:1372] Variable decoder/layers_23/extra_mlp/wi_0/kernel                                          size 33554432     shape (mlp_embed=2048, mlp=16384)              partition spec (None, 'model')
I0512 23:42:24.982017 140529434355712 utils.py:1372] Variable decoder/layers_23/extra_mlp/wi_1/kernel                                          size 16777216     shape (mlp_embed=2048, mlp=8192)               partition spec (None, 'model')
I0512 23:42:24.982072 140529434355712 utils.py:1372] Variable decoder/layers_23/extra_mlp/wo/kernel                                            size 16777216     shape (mlp=8192, mlp_embed=2048)               partition spec ('model', None)
I0512 23:42:24.982120 140529434355712 utils.py:1372] Variable decoder/layers_23/mlp/expert/wi_0/kernel                                         size 268435456    shape (expert=8, mlp_embed=2048, expert_mlp=16384) partition spec ('expert', None, 'model')
I0512 23:42:24.982168 140529434355712 utils.py:1372] Variable decoder/layers_23/mlp/expert/wi_1/kernel                                         size 134217728    shape (expert=8, mlp_embed=2048, expert_mlp=8192) partition spec ('expert', None, 'model')
I0512 23:42:24.982215 140529434355712 utils.py:1372] Variable decoder/layers_23/mlp/expert/wo/kernel                                           size 134217728    shape (expert=8, expert_mlp=8192, mlp_embed=2048) partition spec ('expert', 'model', None)
I0512 23:42:24.982262 140529434355712 utils.py:1372] Variable decoder/layers_23/mlp/router/router_weights/w/kernel                             size 16384        shape (embed=2048, unmodeled=8)                partition spec ('model', None)
I0512 23:42:24.982307 140529434355712 utils.py:1372] Variable decoder/layers_23/pre_extra_mlp_layer_norm/scale                                 size 2048         shape (embed=2048)                             partition spec ('model',)
I0512 23:42:24.982353 140529434355712 utils.py:1372] Variable decoder/layers_23/pre_mlp_layer_norm/scale                                       size 2048         shape (embed=2048)                             partition spec ('model',)
I0512 23:42:24.982398 140529434355712 utils.py:1372] Variable decoder/layers_23/pre_self_attention_layer_norm/scale                            size 2048         shape (embed=2048)                             partition spec ('model',)
I0512 23:42:24.982443 140529434355712 utils.py:1372] Variable decoder/layers_23/self_attention/key/kernel                                      size 6291456      shape (embed=2048, joined_kv=3072)             partition spec (None, 'model')
I0512 23:42:24.982488 140529434355712 utils.py:1372] Variable decoder/layers_23/self_attention/out/kernel                                      size 6291456      shape (joined_kv=3072, embed=2048)             partition spec ('model', None)
I0512 23:42:24.982533 140529434355712 utils.py:1372] Variable decoder/layers_23/self_attention/query/kernel                                    size 6291456      shape (embed=2048, joined_kv=3072)             partition spec (None, 'model')
I0512 23:42:24.982579 140529434355712 utils.py:1372] Variable decoder/layers_23/self_attention/value/kernel                                    size 6291456      shape (embed=2048, joined_kv=3072)             partition spec (None, 'model')
I0512 23:42:24.982624 140529434355712 utils.py:1372] Variable decoder/layers_3/mlp/wi_0/kernel                                                 size 33554432     shape (mlp_embed=2048, mlp=16384)              partition spec (None, 'model')
I0512 23:42:24.982669 140529434355712 utils.py:1372] Variable decoder/layers_3/mlp/wi_1/kernel                                                 size 16777216     shape (mlp_embed=2048, mlp=8192)               partition spec (None, 'model')
I0512 23:42:24.982714 140529434355712 utils.py:1372] Variable decoder/layers_3/mlp/wo/kernel                                                   size 16777216     shape (mlp=8192, mlp_embed=2048)               partition spec ('model', None)
I0512 23:42:24.982764 140529434355712 utils.py:1372] Variable decoder/layers_3/pre_mlp_layer_norm/scale                                        size 2048         shape (embed=2048)                             partition spec ('model',)
I0512 23:42:24.982810 140529434355712 utils.py:1372] Variable decoder/layers_3/pre_self_attention_layer_norm/scale                             size 2048         shape (embed=2048)                             partition spec ('model',)
I0512 23:42:24.982856 140529434355712 utils.py:1372] Variable decoder/layers_3/self_attention/key/kernel                                       size 6291456      shape (embed=2048, joined_kv=3072)             partition spec (None, 'model')
I0512 23:42:24.982901 140529434355712 utils.py:1372] Variable decoder/layers_3/self_attention/out/kernel                                       size 6291456      shape (joined_kv=3072, embed=2048)             partition spec ('model', None)
I0512 23:42:24.982947 140529434355712 utils.py:1372] Variable decoder/layers_3/self_attention/query/kernel                                     size 6291456      shape (embed=2048, joined_kv=3072)             partition spec (None, 'model')
I0512 23:42:24.983236 140529434355712 utils.py:1372] Variable decoder/layers_3/self_attention/value/kernel                                     size 6291456      shape (embed=2048, joined_kv=3072)             partition spec (None, 'model')
I0512 23:42:24.983291 140529434355712 utils.py:1372] Variable decoder/layers_4/mlp/wi_0/kernel                                                 size 33554432     shape (mlp_embed=2048, mlp=16384)              partition spec (None, 'model')
I0512 23:42:24.983339 140529434355712 utils.py:1372] Variable decoder/layers_4/mlp/wi_1/kernel                                                 size 16777216     shape (mlp_embed=2048, mlp=8192)               partition spec (None, 'model')
I0512 23:42:24.983403 140529434355712 utils.py:1372] Variable decoder/layers_4/mlp/wo/kernel                                                   size 16777216     shape (mlp=8192, mlp_embed=2048)               partition spec ('model', None)
I0512 23:42:24.983450 140529434355712 utils.py:1372] Variable decoder/layers_4/pre_mlp_layer_norm/scale                                        size 2048         shape (embed=2048)                             partition spec ('model',)
I0512 23:42:24.983496 140529434355712 utils.py:1372] Variable decoder/layers_4/pre_self_attention_layer_norm/scale                             size 2048         shape (embed=2048)                             partition spec ('model',)
I0512 23:42:24.983541 140529434355712 utils.py:1372] Variable decoder/layers_4/self_attention/key/kernel                                       size 6291456      shape (embed=2048, joined_kv=3072)             partition spec (None, 'model')
I0512 23:42:24.983587 140529434355712 utils.py:1372] Variable decoder/layers_4/self_attention/out/kernel                                       size 6291456      shape (joined_kv=3072, embed=2048)             partition spec ('model', None)
I0512 23:42:24.983631 140529434355712 utils.py:1372] Variable decoder/layers_4/self_attention/query/kernel                                     size 6291456      shape (embed=2048, joined_kv=3072)             partition spec (None, 'model')
I0512 23:42:24.983676 140529434355712 utils.py:1372] Variable decoder/layers_4/self_attention/value/kernel                                     size 6291456      shape (embed=2048, joined_kv=3072)             partition spec (None, 'model')
I0512 23:42:24.983721 140529434355712 utils.py:1372] Variable decoder/layers_5/extra_mlp/wi_0/kernel                                           size 33554432     shape (mlp_embed=2048, mlp=16384)              partition spec (None, 'model')
I0512 23:42:24.983769 140529434355712 utils.py:1372] Variable decoder/layers_5/extra_mlp/wi_1/kernel                                           size 16777216     shape (mlp_embed=2048, mlp=8192)               partition spec (None, 'model')
I0512 23:42:24.983815 140529434355712 utils.py:1372] Variable decoder/layers_5/extra_mlp/wo/kernel                                             size 16777216     shape (mlp=8192, mlp_embed=2048)               partition spec ('model', None)
I0512 23:42:24.983861 140529434355712 utils.py:1372] Variable decoder/layers_5/mlp/expert/wi_0/kernel                                          size 268435456    shape (expert=8, mlp_embed=2048, expert_mlp=16384) partition spec ('expert', None, 'model')
I0512 23:42:24.983908 140529434355712 utils.py:1372] Variable decoder/layers_5/mlp/expert/wi_1/kernel                                          size 134217728    shape (expert=8, mlp_embed=2048, expert_mlp=8192) partition spec ('expert', None, 'model')
I0512 23:42:24.983954 140529434355712 utils.py:1372] Variable decoder/layers_5/mlp/expert/wo/kernel                                            size 134217728    shape (expert=8, expert_mlp=8192, mlp_embed=2048) partition spec ('expert', 'model', None)
I0512 23:42:24.984010 140529434355712 utils.py:1372] Variable decoder/layers_5/mlp/router/router_weights/w/kernel                              size 16384        shape (embed=2048, unmodeled=8)                partition spec ('model', None)
I0512 23:42:24.984056 140529434355712 utils.py:1372] Variable decoder/layers_5/pre_extra_mlp_layer_norm/scale                                  size 2048         shape (embed=2048)                             partition spec ('model',)
I0512 23:42:24.984102 140529434355712 utils.py:1372] Variable decoder/layers_5/pre_mlp_layer_norm/scale                                        size 2048         shape (embed=2048)                             partition spec ('model',)
I0512 23:42:24.984147 140529434355712 utils.py:1372] Variable decoder/layers_5/pre_self_attention_layer_norm/scale                             size 2048         shape (embed=2048)                             partition spec ('model',)
I0512 23:42:24.984192 140529434355712 utils.py:1372] Variable decoder/layers_5/self_attention/key/kernel                                       size 6291456      shape (embed=2048, joined_kv=3072)             partition spec (None, 'model')
I0512 23:42:24.984237 140529434355712 utils.py:1372] Variable decoder/layers_5/self_attention/out/kernel                                       size 6291456      shape (joined_kv=3072, embed=2048)             partition spec ('model', None)
I0512 23:42:24.984283 140529434355712 utils.py:1372] Variable decoder/layers_5/self_attention/query/kernel                                     size 6291456      shape (embed=2048, joined_kv=3072)             partition spec (None, 'model')
I0512 23:42:24.984328 140529434355712 utils.py:1372] Variable decoder/layers_5/self_attention/value/kernel                                     size 6291456      shape (embed=2048, joined_kv=3072)             partition spec (None, 'model')
I0512 23:42:24.984372 140529434355712 utils.py:1372] Variable decoder/layers_6/mlp/wi_0/kernel                                                 size 33554432     shape (mlp_embed=2048, mlp=16384)              partition spec (None, 'model')
I0512 23:42:24.984418 140529434355712 utils.py:1372] Variable decoder/layers_6/mlp/wi_1/kernel                                                 size 16777216     shape (mlp_embed=2048, mlp=8192)               partition spec (None, 'model')
I0512 23:42:24.984463 140529434355712 utils.py:1372] Variable decoder/layers_6/mlp/wo/kernel                                                   size 16777216     shape (mlp=8192, mlp_embed=2048)               partition spec ('model', None)
I0512 23:42:24.984508 140529434355712 utils.py:1372] Variable decoder/layers_6/pre_mlp_layer_norm/scale                                        size 2048         shape (embed=2048)                             partition spec ('model',)
I0512 23:42:24.984553 140529434355712 utils.py:1372] Variable decoder/layers_6/pre_self_attention_layer_norm/scale                             size 2048         shape (embed=2048)                             partition spec ('model',)
I0512 23:42:24.984598 140529434355712 utils.py:1372] Variable decoder/layers_6/self_attention/key/kernel                                       size 6291456      shape (embed=2048, joined_kv=3072)             partition spec (None, 'model')
I0512 23:42:24.984643 140529434355712 utils.py:1372] Variable decoder/layers_6/self_attention/out/kernel                                       size 6291456      shape (joined_kv=3072, embed=2048)             partition spec ('model', None)
I0512 23:42:24.984688 140529434355712 utils.py:1372] Variable decoder/layers_6/self_attention/query/kernel                                     size 6291456      shape (embed=2048, joined_kv=3072)             partition spec (None, 'model')
I0512 23:42:24.984733 140529434355712 utils.py:1372] Variable decoder/layers_6/self_attention/value/kernel                                     size 6291456      shape (embed=2048, joined_kv=3072)             partition spec (None, 'model')
I0512 23:42:24.984780 140529434355712 utils.py:1372] Variable decoder/layers_7/mlp/wi_0/kernel                                                 size 33554432     shape (mlp_embed=2048, mlp=16384)              partition spec (None, 'model')
I0512 23:42:24.984826 140529434355712 utils.py:1372] Variable decoder/layers_7/mlp/wi_1/kernel                                                 size 16777216     shape (mlp_embed=2048, mlp=8192)               partition spec (None, 'model')
I0512 23:42:24.984871 140529434355712 utils.py:1372] Variable decoder/layers_7/mlp/wo/kernel                                                   size 16777216     shape (mlp=8192, mlp_embed=2048)               partition spec ('model', None)
I0512 23:42:24.984916 140529434355712 utils.py:1372] Variable decoder/layers_7/pre_mlp_layer_norm/scale                                        size 2048         shape (embed=2048)                             partition spec ('model',)
I0512 23:42:24.984961 140529434355712 utils.py:1372] Variable decoder/layers_7/pre_self_attention_layer_norm/scale                             size 2048         shape (embed=2048)                             partition spec ('model',)
I0512 23:42:24.985013 140529434355712 utils.py:1372] Variable decoder/layers_7/self_attention/key/kernel                                       size 6291456      shape (embed=2048, joined_kv=3072)             partition spec (None, 'model')
I0512 23:42:24.985058 140529434355712 utils.py:1372] Variable decoder/layers_7/self_attention/out/kernel                                       size 6291456      shape (joined_kv=3072, embed=2048)             partition spec ('model', None)
I0512 23:42:24.985103 140529434355712 utils.py:1372] Variable decoder/layers_7/self_attention/query/kernel                                     size 6291456      shape (embed=2048, joined_kv=3072)             partition spec (None, 'model')
I0512 23:42:24.985148 140529434355712 utils.py:1372] Variable decoder/layers_7/self_attention/value/kernel                                     size 6291456      shape (embed=2048, joined_kv=3072)             partition spec (None, 'model')
I0512 23:42:24.985193 140529434355712 utils.py:1372] Variable decoder/layers_8/mlp/wi_0/kernel                                                 size 33554432     shape (mlp_embed=2048, mlp=16384)              partition spec (None, 'model')
I0512 23:42:24.985239 140529434355712 utils.py:1372] Variable decoder/layers_8/mlp/wi_1/kernel                                                 size 16777216     shape (mlp_embed=2048, mlp=8192)               partition spec (None, 'model')
I0512 23:42:24.985284 140529434355712 utils.py:1372] Variable decoder/layers_8/mlp/wo/kernel                                                   size 16777216     shape (mlp=8192, mlp_embed=2048)               partition spec ('model', None)
I0512 23:42:24.985328 140529434355712 utils.py:1372] Variable decoder/layers_8/pre_mlp_layer_norm/scale                                        size 2048         shape (embed=2048)                             partition spec ('model',)
I0512 23:42:24.985373 140529434355712 utils.py:1372] Variable decoder/layers_8/pre_self_attention_layer_norm/scale                             size 2048         shape (embed=2048)                             partition spec ('model',)
I0512 23:42:24.985418 140529434355712 utils.py:1372] Variable decoder/layers_8/self_attention/key/kernel                                       size 6291456      shape (embed=2048, joined_kv=3072)             partition spec (None, 'model')
I0512 23:42:24.985478 140529434355712 utils.py:1372] Variable decoder/layers_8/self_attention/out/kernel                                       size 6291456      shape (joined_kv=3072, embed=2048)             partition spec ('model', None)
I0512 23:42:24.985526 140529434355712 utils.py:1372] Variable decoder/layers_8/self_attention/query/kernel                                     size 6291456      shape (embed=2048, joined_kv=3072)             partition spec (None, 'model')
I0512 23:42:24.985596 140529434355712 utils.py:1372] Variable decoder/layers_8/self_attention/value/kernel                                     size 6291456      shape (embed=2048, joined_kv=3072)             partition spec (None, 'model')
I0512 23:42:24.985643 140529434355712 utils.py:1372] Variable decoder/layers_9/mlp/wi_0/kernel                                                 size 33554432     shape (mlp_embed=2048, mlp=16384)              partition spec (None, 'model')
I0512 23:42:24.985689 140529434355712 utils.py:1372] Variable decoder/layers_9/mlp/wi_1/kernel                                                 size 16777216     shape (mlp_embed=2048, mlp=8192)               partition spec (None, 'model')
I0512 23:42:24.985735 140529434355712 utils.py:1372] Variable decoder/layers_9/mlp/wo/kernel                                                   size 16777216     shape (mlp=8192, mlp_embed=2048)               partition spec ('model', None)
I0512 23:42:24.985782 140529434355712 utils.py:1372] Variable decoder/layers_9/pre_mlp_layer_norm/scale                                        size 2048         shape (embed=2048)                             partition spec ('model',)
I0512 23:42:24.985827 140529434355712 utils.py:1372] Variable decoder/layers_9/pre_self_attention_layer_norm/scale                             size 2048         shape (embed=2048)                             partition spec ('model',)
I0512 23:42:24.985870 140529434355712 utils.py:1372] Variable decoder/layers_9/self_attention/key/kernel                                       size 6291456      shape (embed=2048, joined_kv=3072)             partition spec (None, 'model')
I0512 23:42:24.985913 140529434355712 utils.py:1372] Variable decoder/layers_9/self_attention/out/kernel                                       size 6291456      shape (joined_kv=3072, embed=2048)             partition spec ('model', None)
I0512 23:42:24.985955 140529434355712 utils.py:1372] Variable decoder/layers_9/self_attention/query/kernel                                     size 6291456      shape (embed=2048, joined_kv=3072)             partition spec (None, 'model')
I0512 23:42:24.986004 140529434355712 utils.py:1372] Variable decoder/layers_9/self_attention/value/kernel                                     size 6291456      shape (embed=2048, joined_kv=3072)             partition spec (None, 'model')
I0512 23:42:24.986049 140529434355712 utils.py:1372] Variable decoder/logits_dense/kernel                                                      size 525074432    shape (embed=2048, vocab=256384)               partition spec (None, 'model')
I0512 23:42:24.986092 140529434355712 utils.py:1372] Variable token_embedder/embedding                                                         size 525074432    shape (vocab=256384, embed=2048)               partition spec ('model', None)
I0512 23:42:24.986196 140529434355712 utils.py:1372] Total number of parameters: 5412399104
I0512 23:42:24.986263 140529434355712 utils.py:1372] 
I0512 23:42:24.989946 140529434355712 utils.py:1372] Variable param_states/decoder/decoder_norm/scale/m                                        size 1            shape (1,)                                     partition spec None
I0512 23:42:24.990021 140529434355712 utils.py:1372] Variable param_states/decoder/decoder_norm/scale/v                                        size 2048         shape (embed=2048)                             partition spec ('model',)
I0512 23:42:24.990069 140529434355712 utils.py:1372] Variable param_states/decoder/decoder_norm/scale/v_col                                    size 1            shape (1,)                                     partition spec None
I0512 23:42:24.990110 140529434355712 utils.py:1372] Variable param_states/decoder/decoder_norm/scale/v_row                                    size 1            shape (1,)                                     partition spec None
I0512 23:42:24.990150 140529434355712 utils.py:1372] Variable param_states/decoder/layers_0/mlp/wi_0/kernel/m                                  size 1            shape (1,)                                     partition spec None
I0512 23:42:24.990189 140529434355712 utils.py:1372] Variable param_states/decoder/layers_0/mlp/wi_0/kernel/v                                  size 1            shape (1,)                                     partition spec None
I0512 23:42:24.990229 140529434355712 utils.py:1372] Variable param_states/decoder/layers_0/mlp/wi_0/kernel/v_col                              size 16384        shape (16384,)                                 partition spec None
I0512 23:42:24.990268 140529434355712 utils.py:1372] Variable param_states/decoder/layers_0/mlp/wi_0/kernel/v_row                              size 2048         shape (2048,)                                  partition spec None
I0512 23:42:24.990307 140529434355712 utils.py:1372] Variable param_states/decoder/layers_0/mlp/wi_1/kernel/m                                  size 1            shape (1,)                                     partition spec None
I0512 23:42:24.990346 140529434355712 utils.py:1372] Variable param_states/decoder/layers_0/mlp/wi_1/kernel/v                                  size 1            shape (1,)                                     partition spec None
I0512 23:42:24.990385 140529434355712 utils.py:1372] Variable param_states/decoder/layers_0/mlp/wi_1/kernel/v_col                              size 8192         shape (8192,)                                  partition spec None
I0512 23:42:24.990424 140529434355712 utils.py:1372] Variable param_states/decoder/layers_0/mlp/wi_1/kernel/v_row                              size 2048         shape (2048,)                                  partition spec None
I0512 23:42:24.990463 140529434355712 utils.py:1372] Variable param_states/decoder/layers_0/mlp/wo/kernel/m                                    size 1            shape (1,)                                     partition spec None
I0512 23:42:24.990501 140529434355712 utils.py:1372] Variable param_states/decoder/layers_0/mlp/wo/kernel/v                                    size 1            shape (1,)                                     partition spec None
I0512 23:42:24.990540 140529434355712 utils.py:1372] Variable param_states/decoder/layers_0/mlp/wo/kernel/v_col                                size 8192         shape (8192,)                                  partition spec None
I0512 23:42:24.990579 140529434355712 utils.py:1372] Variable param_states/decoder/layers_0/mlp/wo/kernel/v_row                                size 2048         shape (2048,)                                  partition spec None
I0512 23:42:24.990617 140529434355712 utils.py:1372] Variable param_states/decoder/layers_0/pre_mlp_layer_norm/scale/m                         size 1            shape (1,)                                     partition spec None
I0512 23:42:24.990658 140529434355712 utils.py:1372] Variable param_states/decoder/layers_0/pre_mlp_layer_norm/scale/v                         size 2048         shape (embed=2048)                             partition spec ('model',)
I0512 23:42:24.990699 140529434355712 utils.py:1372] Variable param_states/decoder/layers_0/pre_mlp_layer_norm/scale/v_col                     size 1            shape (1,)                                     partition spec None
I0512 23:42:24.990739 140529434355712 utils.py:1372] Variable param_states/decoder/layers_0/pre_mlp_layer_norm/scale/v_row                     size 1            shape (1,)                                     partition spec None
I0512 23:42:24.990780 140529434355712 utils.py:1372] Variable param_states/decoder/layers_0/pre_self_attention_layer_norm/scale/m              size 1            shape (1,)                                     partition spec None
I0512 23:42:24.990821 140529434355712 utils.py:1372] Variable param_states/decoder/layers_0/pre_self_attention_layer_norm/scale/v              size 2048         shape (embed=2048)                             partition spec ('model',)
I0512 23:42:24.990861 140529434355712 utils.py:1372] Variable param_states/decoder/layers_0/pre_self_attention_layer_norm/scale/v_col          size 1            shape (1,)                                     partition spec None
I0512 23:42:24.990900 140529434355712 utils.py:1372] Variable param_states/decoder/layers_0/pre_self_attention_layer_norm/scale/v_row          size 1            shape (1,)                                     partition spec None
I0512 23:42:24.990938 140529434355712 utils.py:1372] Variable param_states/decoder/layers_0/self_attention/key/kernel/m                        size 1            shape (1,)                                     partition spec None
I0512 23:42:24.990977 140529434355712 utils.py:1372] Variable param_states/decoder/layers_0/self_attention/key/kernel/v                        size 1            shape (1,)                                     partition spec None
I0512 23:42:24.991024 140529434355712 utils.py:1372] Variable param_states/decoder/layers_0/self_attention/key/kernel/v_col                    size 3072         shape (3072,)                                  partition spec None
I0512 23:42:24.991063 140529434355712 utils.py:1372] Variable param_states/decoder/layers_0/self_attention/key/kernel/v_row                    size 2048         shape (2048,)                                  partition spec None
I0512 23:42:24.991101 140529434355712 utils.py:1372] Variable param_states/decoder/layers_0/self_attention/out/kernel/m                        size 1            shape (1,)                                     partition spec None
I0512 23:42:24.991139 140529434355712 utils.py:1372] Variable param_states/decoder/layers_0/self_attention/out/kernel/v                        size 1            shape (1,)                                     partition spec None
I0512 23:42:24.991178 140529434355712 utils.py:1372] Variable param_states/decoder/layers_0/self_attention/out/kernel/v_col                    size 3072         shape (3072,)                                  partition spec None
I0512 23:42:24.991216 140529434355712 utils.py:1372] Variable param_states/decoder/layers_0/self_attention/out/kernel/v_row                    size 2048         shape (2048,)                                  partition spec None
I0512 23:42:24.991273 140529434355712 utils.py:1372] Variable param_states/decoder/layers_0/self_attention/query/kernel/m                      size 1            shape (1,)                                     partition spec None
I0512 23:42:24.991317 140529434355712 utils.py:1372] Variable param_states/decoder/layers_0/self_attention/query/kernel/v                      size 1            shape (1,)                                     partition spec None
I0512 23:42:24.991357 140529434355712 utils.py:1372] Variable param_states/decoder/layers_0/self_attention/query/kernel/v_col                  size 3072         shape (3072,)                                  partition spec None
I0512 23:42:24.991396 140529434355712 utils.py:1372] Variable param_states/decoder/layers_0/self_attention/query/kernel/v_row                  size 2048         shape (2048,)                                  partition spec None
I0512 23:42:24.991434 140529434355712 utils.py:1372] Variable param_states/decoder/layers_0/self_attention/value/kernel/m                      size 1            shape (1,)                                     partition spec None
I0512 23:42:24.991471 140529434355712 utils.py:1372] Variable param_states/decoder/layers_0/self_attention/value/kernel/v                      size 1            shape (1,)                                     partition spec None
I0512 23:42:24.991511 140529434355712 utils.py:1372] Variable param_states/decoder/layers_0/self_attention/value/kernel/v_col                  size 3072         shape (3072,)                                  partition spec None
I0512 23:42:24.991549 140529434355712 utils.py:1372] Variable param_states/decoder/layers_0/self_attention/value/kernel/v_row                  size 2048         shape (2048,)                                  partition spec None
I0512 23:42:24.991587 140529434355712 utils.py:1372] Variable param_states/decoder/layers_1/mlp/wi_0/kernel/m                                  size 1            shape (1,)                                     partition spec None
I0512 23:42:24.991625 140529434355712 utils.py:1372] Variable param_states/decoder/layers_1/mlp/wi_0/kernel/v                                  size 1            shape (1,)                                     partition spec None
I0512 23:42:24.991665 140529434355712 utils.py:1372] Variable param_states/decoder/layers_1/mlp/wi_0/kernel/v_col                              size 16384        shape (16384,)                                 partition spec None
I0512 23:42:24.991703 140529434355712 utils.py:1372] Variable param_states/decoder/layers_1/mlp/wi_0/kernel/v_row                              size 2048         shape (2048,)                                  partition spec None
I0512 23:42:24.991742 140529434355712 utils.py:1372] Variable param_states/decoder/layers_1/mlp/wi_1/kernel/m                                  size 1            shape (1,)                                     partition spec None
I0512 23:42:24.991783 140529434355712 utils.py:1372] Variable param_states/decoder/layers_1/mlp/wi_1/kernel/v                                  size 1            shape (1,)                                     partition spec None
I0512 23:42:24.991822 140529434355712 utils.py:1372] Variable param_states/decoder/layers_1/mlp/wi_1/kernel/v_col                              size 8192         shape (8192,)                                  partition spec None
I0512 23:42:24.991861 140529434355712 utils.py:1372] Variable param_states/decoder/layers_1/mlp/wi_1/kernel/v_row                              size 2048         shape (2048,)                                  partition spec None
I0512 23:42:24.991898 140529434355712 utils.py:1372] Variable param_states/decoder/layers_1/mlp/wo/kernel/m                                    size 1            shape (1,)                                     partition spec None
I0512 23:42:24.991937 140529434355712 utils.py:1372] Variable param_states/decoder/layers_1/mlp/wo/kernel/v                                    size 1            shape (1,)                                     partition spec None
I0512 23:42:24.991977 140529434355712 utils.py:1372] Variable param_states/decoder/layers_1/mlp/wo/kernel/v_col                                size 8192         shape (8192,)                                  partition spec None
I0512 23:42:24.992025 140529434355712 utils.py:1372] Variable param_states/decoder/layers_1/mlp/wo/kernel/v_row                                size 2048         shape (2048,)                                  partition spec None
I0512 23:42:24.992063 140529434355712 utils.py:1372] Variable param_states/decoder/layers_1/pre_mlp_layer_norm/scale/m                         size 1            shape (1,)                                     partition spec None
I0512 23:42:24.992105 140529434355712 utils.py:1372] Variable param_states/decoder/layers_1/pre_mlp_layer_norm/scale/v                         size 2048         shape (embed=2048)                             partition spec ('model',)
I0512 23:42:24.992146 140529434355712 utils.py:1372] Variable param_states/decoder/layers_1/pre_mlp_layer_norm/scale/v_col                     size 1            shape (1,)                                     partition spec None
I0512 23:42:24.992185 140529434355712 utils.py:1372] Variable param_states/decoder/layers_1/pre_mlp_layer_norm/scale/v_row                     size 1            shape (1,)                                     partition spec None
I0512 23:42:24.992223 140529434355712 utils.py:1372] Variable param_states/decoder/layers_1/pre_self_attention_layer_norm/scale/m              size 1            shape (1,)                                     partition spec None
I0512 23:42:24.992264 140529434355712 utils.py:1372] Variable param_states/decoder/layers_1/pre_self_attention_layer_norm/scale/v              size 2048         shape (embed=2048)                             partition spec ('model',)
I0512 23:42:24.992304 140529434355712 utils.py:1372] Variable param_states/decoder/layers_1/pre_self_attention_layer_norm/scale/v_col          size 1            shape (1,)                                     partition spec None
I0512 23:42:24.992342 140529434355712 utils.py:1372] Variable param_states/decoder/layers_1/pre_self_attention_layer_norm/scale/v_row          size 1            shape (1,)                                     partition spec None
I0512 23:42:24.992380 140529434355712 utils.py:1372] Variable param_states/decoder/layers_1/self_attention/key/kernel/m                        size 1            shape (1,)                                     partition spec None
I0512 23:42:24.992418 140529434355712 utils.py:1372] Variable param_states/decoder/layers_1/self_attention/key/kernel/v                        size 1            shape (1,)                                     partition spec None
I0512 23:42:24.992456 140529434355712 utils.py:1372] Variable param_states/decoder/layers_1/self_attention/key/kernel/v_col                    size 3072         shape (3072,)                                  partition spec None
I0512 23:42:24.992494 140529434355712 utils.py:1372] Variable param_states/decoder/layers_1/self_attention/key/kernel/v_row                    size 2048         shape (2048,)                                  partition spec None
I0512 23:42:24.992532 140529434355712 utils.py:1372] Variable param_states/decoder/layers_1/self_attention/out/kernel/m                        size 1            shape (1,)                                     partition spec None
I0512 23:42:24.992570 140529434355712 utils.py:1372] Variable param_states/decoder/layers_1/self_attention/out/kernel/v                        size 1            shape (1,)                                     partition spec None
I0512 23:42:24.992609 140529434355712 utils.py:1372] Variable param_states/decoder/layers_1/self_attention/out/kernel/v_col                    size 3072         shape (3072,)                                  partition spec None
I0512 23:42:24.992646 140529434355712 utils.py:1372] Variable param_states/decoder/layers_1/self_attention/out/kernel/v_row                    size 2048         shape (2048,)                                  partition spec None
I0512 23:42:24.992685 140529434355712 utils.py:1372] Variable param_states/decoder/layers_1/self_attention/query/kernel/m                      size 1            shape (1,)                                     partition spec None
I0512 23:42:24.992723 140529434355712 utils.py:1372] Variable param_states/decoder/layers_1/self_attention/query/kernel/v                      size 1            shape (1,)                                     partition spec None
I0512 23:42:24.992764 140529434355712 utils.py:1372] Variable param_states/decoder/layers_1/self_attention/query/kernel/v_col                  size 3072         shape (3072,)                                  partition spec None
I0512 23:42:24.992803 140529434355712 utils.py:1372] Variable param_states/decoder/layers_1/self_attention/query/kernel/v_row                  size 2048         shape (2048,)                                  partition spec None
I0512 23:42:24.992842 140529434355712 utils.py:1372] Variable param_states/decoder/layers_1/self_attention/value/kernel/m                      size 1            shape (1,)                                     partition spec None
I0512 23:42:24.992880 140529434355712 utils.py:1372] Variable param_states/decoder/layers_1/self_attention/value/kernel/v                      size 1            shape (1,)                                     partition spec None
I0512 23:42:24.992918 140529434355712 utils.py:1372] Variable param_states/decoder/layers_1/self_attention/value/kernel/v_col                  size 3072         shape (3072,)                                  partition spec None
I0512 23:42:24.992957 140529434355712 utils.py:1372] Variable param_states/decoder/layers_1/self_attention/value/kernel/v_row                  size 2048         shape (2048,)                                  partition spec None
I0512 23:42:24.993001 140529434355712 utils.py:1372] Variable param_states/decoder/layers_10/mlp/wi_0/kernel/m                                 size 1            shape (1,)                                     partition spec None
I0512 23:42:24.993041 140529434355712 utils.py:1372] Variable param_states/decoder/layers_10/mlp/wi_0/kernel/v                                 size 1            shape (1,)                                     partition spec None
I0512 23:42:24.993081 140529434355712 utils.py:1372] Variable param_states/decoder/layers_10/mlp/wi_0/kernel/v_col                             size 16384        shape (16384,)                                 partition spec None
I0512 23:42:24.993134 140529434355712 utils.py:1372] Variable param_states/decoder/layers_10/mlp/wi_0/kernel/v_row                             size 2048         shape (2048,)                                  partition spec None
I0512 23:42:24.993176 140529434355712 utils.py:1372] Variable param_states/decoder/layers_10/mlp/wi_1/kernel/m                                 size 1            shape (1,)                                     partition spec None
I0512 23:42:24.993216 140529434355712 utils.py:1372] Variable param_states/decoder/layers_10/mlp/wi_1/kernel/v                                 size 1            shape (1,)                                     partition spec None
I0512 23:42:24.993255 140529434355712 utils.py:1372] Variable param_states/decoder/layers_10/mlp/wi_1/kernel/v_col                             size 8192         shape (8192,)                                  partition spec None
I0512 23:42:24.993294 140529434355712 utils.py:1372] Variable param_states/decoder/layers_10/mlp/wi_1/kernel/v_row                             size 2048         shape (2048,)                                  partition spec None
I0512 23:42:24.993331 140529434355712 utils.py:1372] Variable param_states/decoder/layers_10/mlp/wo/kernel/m                                   size 1            shape (1,)                                     partition spec None
I0512 23:42:24.993370 140529434355712 utils.py:1372] Variable param_states/decoder/layers_10/mlp/wo/kernel/v                                   size 1            shape (1,)                                     partition spec None
I0512 23:42:24.993409 140529434355712 utils.py:1372] Variable param_states/decoder/layers_10/mlp/wo/kernel/v_col                               size 8192         shape (8192,)                                  partition spec None
I0512 23:42:24.993447 140529434355712 utils.py:1372] Variable param_states/decoder/layers_10/mlp/wo/kernel/v_row                               size 2048         shape (2048,)                                  partition spec None
I0512 23:42:24.993486 140529434355712 utils.py:1372] Variable param_states/decoder/layers_10/pre_mlp_layer_norm/scale/m                        size 1            shape (1,)                                     partition spec None
I0512 23:42:24.993527 140529434355712 utils.py:1372] Variable param_states/decoder/layers_10/pre_mlp_layer_norm/scale/v                        size 2048         shape (embed=2048)                             partition spec ('model',)
I0512 23:42:24.993594 140529434355712 utils.py:1372] Variable param_states/decoder/layers_10/pre_mlp_layer_norm/scale/v_col                    size 1            shape (1,)                                     partition spec None
I0512 23:42:24.993635 140529434355712 utils.py:1372] Variable param_states/decoder/layers_10/pre_mlp_layer_norm/scale/v_row                    size 1            shape (1,)                                     partition spec None
I0512 23:42:24.993674 140529434355712 utils.py:1372] Variable param_states/decoder/layers_10/pre_self_attention_layer_norm/scale/m             size 1            shape (1,)                                     partition spec None
I0512 23:42:24.993715 140529434355712 utils.py:1372] Variable param_states/decoder/layers_10/pre_self_attention_layer_norm/scale/v             size 2048         shape (embed=2048)                             partition spec ('model',)
I0512 23:42:24.993757 140529434355712 utils.py:1372] Variable param_states/decoder/layers_10/pre_self_attention_layer_norm/scale/v_col         size 1            shape (1,)                                     partition spec None
I0512 23:42:24.993800 140529434355712 utils.py:1372] Variable param_states/decoder/layers_10/pre_self_attention_layer_norm/scale/v_row         size 1            shape (1,)                                     partition spec None
I0512 23:42:24.993839 140529434355712 utils.py:1372] Variable param_states/decoder/layers_10/self_attention/key/kernel/m                       size 1            shape (1,)                                     partition spec None
I0512 23:42:24.993878 140529434355712 utils.py:1372] Variable param_states/decoder/layers_10/self_attention/key/kernel/v                       size 1            shape (1,)                                     partition spec None
I0512 23:42:24.993917 140529434355712 utils.py:1372] Variable param_states/decoder/layers_10/self_attention/key/kernel/v_col                   size 3072         shape (3072,)                                  partition spec None
I0512 23:42:24.993955 140529434355712 utils.py:1372] Variable param_states/decoder/layers_10/self_attention/key/kernel/v_row                   size 2048         shape (2048,)                                  partition spec None
I0512 23:42:24.993999 140529434355712 utils.py:1372] Variable param_states/decoder/layers_10/self_attention/out/kernel/m                       size 1            shape (1,)                                     partition spec None
I0512 23:42:24.994040 140529434355712 utils.py:1372] Variable param_states/decoder/layers_10/self_attention/out/kernel/v                       size 1            shape (1,)                                     partition spec None
I0512 23:42:24.994080 140529434355712 utils.py:1372] Variable param_states/decoder/layers_10/self_attention/out/kernel/v_col                   size 3072         shape (3072,)                                  partition spec None
I0512 23:42:24.994118 140529434355712 utils.py:1372] Variable param_states/decoder/layers_10/self_attention/out/kernel/v_row                   size 2048         shape (2048,)                                  partition spec None
I0512 23:42:24.994156 140529434355712 utils.py:1372] Variable param_states/decoder/layers_10/self_attention/query/kernel/m                     size 1            shape (1,)                                     partition spec None
I0512 23:42:24.994195 140529434355712 utils.py:1372] Variable param_states/decoder/layers_10/self_attention/query/kernel/v                     size 1            shape (1,)                                     partition spec None
I0512 23:42:24.994233 140529434355712 utils.py:1372] Variable param_states/decoder/layers_10/self_attention/query/kernel/v_col                 size 3072         shape (3072,)                                  partition spec None
I0512 23:42:24.994272 140529434355712 utils.py:1372] Variable param_states/decoder/layers_10/self_attention/query/kernel/v_row                 size 2048         shape (2048,)                                  partition spec None
I0512 23:42:24.994310 140529434355712 utils.py:1372] Variable param_states/decoder/layers_10/self_attention/value/kernel/m                     size 1            shape (1,)                                     partition spec None
I0512 23:42:24.994349 140529434355712 utils.py:1372] Variable param_states/decoder/layers_10/self_attention/value/kernel/v                     size 1            shape (1,)                                     partition spec None
I0512 23:42:24.994387 140529434355712 utils.py:1372] Variable param_states/decoder/layers_10/self_attention/value/kernel/v_col                 size 3072         shape (3072,)                                  partition spec None
I0512 23:42:24.994426 140529434355712 utils.py:1372] Variable param_states/decoder/layers_10/self_attention/value/kernel/v_row                 size 2048         shape (2048,)                                  partition spec None
I0512 23:42:24.994464 140529434355712 utils.py:1372] Variable param_states/decoder/layers_11/extra_mlp/wi_0/kernel/m                           size 1            shape (1,)                                     partition spec None
I0512 23:42:24.994503 140529434355712 utils.py:1372] Variable param_states/decoder/layers_11/extra_mlp/wi_0/kernel/v                           size 1            shape (1,)                                     partition spec None
I0512 23:42:24.994542 140529434355712 utils.py:1372] Variable param_states/decoder/layers_11/extra_mlp/wi_0/kernel/v_col                       size 16384        shape (16384,)                                 partition spec None
I0512 23:42:24.994581 140529434355712 utils.py:1372] Variable param_states/decoder/layers_11/extra_mlp/wi_0/kernel/v_row                       size 2048         shape (2048,)                                  partition spec None
I0512 23:42:24.994619 140529434355712 utils.py:1372] Variable param_states/decoder/layers_11/extra_mlp/wi_1/kernel/m                           size 1            shape (1,)                                     partition spec None
I0512 23:42:24.994658 140529434355712 utils.py:1372] Variable param_states/decoder/layers_11/extra_mlp/wi_1/kernel/v                           size 1            shape (1,)                                     partition spec None
I0512 23:42:24.994696 140529434355712 utils.py:1372] Variable param_states/decoder/layers_11/extra_mlp/wi_1/kernel/v_col                       size 8192         shape (8192,)                                  partition spec None
I0512 23:42:24.994735 140529434355712 utils.py:1372] Variable param_states/decoder/layers_11/extra_mlp/wi_1/kernel/v_row                       size 2048         shape (2048,)                                  partition spec None
I0512 23:42:24.994776 140529434355712 utils.py:1372] Variable param_states/decoder/layers_11/extra_mlp/wo/kernel/m                             size 1            shape (1,)                                     partition spec None
I0512 23:42:24.994815 140529434355712 utils.py:1372] Variable param_states/decoder/layers_11/extra_mlp/wo/kernel/v                             size 1            shape (1,)                                     partition spec None
I0512 23:42:24.994854 140529434355712 utils.py:1372] Variable param_states/decoder/layers_11/extra_mlp/wo/kernel/v_col                         size 8192         shape (8192,)                                  partition spec None
I0512 23:42:24.994893 140529434355712 utils.py:1372] Variable param_states/decoder/layers_11/extra_mlp/wo/kernel/v_row                         size 2048         shape (2048,)                                  partition spec None
I0512 23:42:24.994931 140529434355712 utils.py:1372] Variable param_states/decoder/layers_11/mlp/expert/wi_0/kernel/m                          size 1            shape (1,)                                     partition spec None
I0512 23:42:24.994969 140529434355712 utils.py:1372] Variable param_states/decoder/layers_11/mlp/expert/wi_0/kernel/v                          size 1            shape (1,)                                     partition spec None
I0512 23:42:24.995033 140529434355712 utils.py:1372] Variable param_states/decoder/layers_11/mlp/expert/wi_0/kernel/v_col                      size 131072       shape (8, 16384)                               partition spec ('expert',)
I0512 23:42:24.995078 140529434355712 utils.py:1372] Variable param_states/decoder/layers_11/mlp/expert/wi_0/kernel/v_row                      size 16384        shape (8, 2048)                                partition spec ('expert',)
I0512 23:42:24.995118 140529434355712 utils.py:1372] Variable param_states/decoder/layers_11/mlp/expert/wi_1/kernel/m                          size 1            shape (1,)                                     partition spec None
I0512 23:42:24.995156 140529434355712 utils.py:1372] Variable param_states/decoder/layers_11/mlp/expert/wi_1/kernel/v                          size 1            shape (1,)                                     partition spec None
I0512 23:42:24.995196 140529434355712 utils.py:1372] Variable param_states/decoder/layers_11/mlp/expert/wi_1/kernel/v_col                      size 65536        shape (8, 8192)                                partition spec ('expert',)
I0512 23:42:24.995235 140529434355712 utils.py:1372] Variable param_states/decoder/layers_11/mlp/expert/wi_1/kernel/v_row                      size 16384        shape (8, 2048)                                partition spec ('expert',)
I0512 23:42:24.995274 140529434355712 utils.py:1372] Variable param_states/decoder/layers_11/mlp/expert/wo/kernel/m                            size 1            shape (1,)                                     partition spec None
I0512 23:42:24.995312 140529434355712 utils.py:1372] Variable param_states/decoder/layers_11/mlp/expert/wo/kernel/v                            size 1            shape (1,)                                     partition spec None
I0512 23:42:24.995352 140529434355712 utils.py:1372] Variable param_states/decoder/layers_11/mlp/expert/wo/kernel/v_col                        size 65536        shape (8, 8192)                                partition spec ('expert',)
I0512 23:42:24.995391 140529434355712 utils.py:1372] Variable param_states/decoder/layers_11/mlp/expert/wo/kernel/v_row                        size 16384        shape (8, 2048)                                partition spec ('expert',)
I0512 23:42:24.995430 140529434355712 utils.py:1372] Variable param_states/decoder/layers_11/mlp/router/router_weights/w/kernel/m              size 1            shape (1,)                                     partition spec None
I0512 23:42:24.995473 140529434355712 utils.py:1372] Variable param_states/decoder/layers_11/mlp/router/router_weights/w/kernel/v              size 16384        shape (embed=2048, unmodeled=8)                partition spec ('model', None)
I0512 23:42:24.995514 140529434355712 utils.py:1372] Variable param_states/decoder/layers_11/mlp/router/router_weights/w/kernel/v_col          size 1            shape (1,)                                     partition spec None
I0512 23:42:24.995553 140529434355712 utils.py:1372] Variable param_states/decoder/layers_11/mlp/router/router_weights/w/kernel/v_row          size 1            shape (1,)                                     partition spec None
I0512 23:42:24.995591 140529434355712 utils.py:1372] Variable param_states/decoder/layers_11/pre_extra_mlp_layer_norm/scale/m                  size 1            shape (1,)                                     partition spec None
I0512 23:42:24.995632 140529434355712 utils.py:1372] Variable param_states/decoder/layers_11/pre_extra_mlp_layer_norm/scale/v                  size 2048         shape (embed=2048)                             partition spec ('model',)
I0512 23:42:24.995672 140529434355712 utils.py:1372] Variable param_states/decoder/layers_11/pre_extra_mlp_layer_norm/scale/v_col              size 1            shape (1,)                                     partition spec None
I0512 23:42:24.995711 140529434355712 utils.py:1372] Variable param_states/decoder/layers_11/pre_extra_mlp_layer_norm/scale/v_row              size 1            shape (1,)                                     partition spec None
I0512 23:42:24.995751 140529434355712 utils.py:1372] Variable param_states/decoder/layers_11/pre_mlp_layer_norm/scale/m                        size 1            shape (1,)                                     partition spec None
I0512 23:42:24.995792 140529434355712 utils.py:1372] Variable param_states/decoder/layers_11/pre_mlp_layer_norm/scale/v                        size 2048         shape (embed=2048)                             partition spec ('model',)
I0512 23:42:24.995832 140529434355712 utils.py:1372] Variable param_states/decoder/layers_11/pre_mlp_layer_norm/scale/v_col                    size 1            shape (1,)                                     partition spec None
I0512 23:42:24.995871 140529434355712 utils.py:1372] Variable param_states/decoder/layers_11/pre_mlp_layer_norm/scale/v_row                    size 1            shape (1,)                                     partition spec None
I0512 23:42:24.995908 140529434355712 utils.py:1372] Variable param_states/decoder/layers_11/pre_self_attention_layer_norm/scale/m             size 1            shape (1,)                                     partition spec None
I0512 23:42:24.995949 140529434355712 utils.py:1372] Variable param_states/decoder/layers_11/pre_self_attention_layer_norm/scale/v             size 2048         shape (embed=2048)                             partition spec ('model',)
I0512 23:42:24.996016 140529434355712 utils.py:1372] Variable param_states/decoder/layers_11/pre_self_attention_layer_norm/scale/v_col         size 1            shape (1,)                                     partition spec None
I0512 23:42:24.996058 140529434355712 utils.py:1372] Variable param_states/decoder/layers_11/pre_self_attention_layer_norm/scale/v_row         size 1            shape (1,)                                     partition spec None
I0512 23:42:24.996096 140529434355712 utils.py:1372] Variable param_states/decoder/layers_11/self_attention/key/kernel/m                       size 1            shape (1,)                                     partition spec None
I0512 23:42:24.996135 140529434355712 utils.py:1372] Variable param_states/decoder/layers_11/self_attention/key/kernel/v                       size 1            shape (1,)                                     partition spec None
I0512 23:42:24.996174 140529434355712 utils.py:1372] Variable param_states/decoder/layers_11/self_attention/key/kernel/v_col                   size 3072         shape (3072,)                                  partition spec None
I0512 23:42:24.996212 140529434355712 utils.py:1372] Variable param_states/decoder/layers_11/self_attention/key/kernel/v_row                   size 2048         shape (2048,)                                  partition spec None
I0512 23:42:24.996251 140529434355712 utils.py:1372] Variable param_states/decoder/layers_11/self_attention/out/kernel/m                       size 1            shape (1,)                                     partition spec None
I0512 23:42:24.996289 140529434355712 utils.py:1372] Variable param_states/decoder/layers_11/self_attention/out/kernel/v                       size 1            shape (1,)                                     partition spec None
I0512 23:42:24.996328 140529434355712 utils.py:1372] Variable param_states/decoder/layers_11/self_attention/out/kernel/v_col                   size 3072         shape (3072,)                                  partition spec None
I0512 23:42:24.996366 140529434355712 utils.py:1372] Variable param_states/decoder/layers_11/self_attention/out/kernel/v_row                   size 2048         shape (2048,)                                  partition spec None
I0512 23:42:24.996404 140529434355712 utils.py:1372] Variable param_states/decoder/layers_11/self_attention/query/kernel/m                     size 1            shape (1,)                                     partition spec None
I0512 23:42:24.996443 140529434355712 utils.py:1372] Variable param_states/decoder/layers_11/self_attention/query/kernel/v                     size 1            shape (1,)                                     partition spec None
I0512 23:42:24.996481 140529434355712 utils.py:1372] Variable param_states/decoder/layers_11/self_attention/query/kernel/v_col                 size 3072         shape (3072,)                                  partition spec None
I0512 23:42:24.996520 140529434355712 utils.py:1372] Variable param_states/decoder/layers_11/self_attention/query/kernel/v_row                 size 2048         shape (2048,)                                  partition spec None
I0512 23:42:24.996558 140529434355712 utils.py:1372] Variable param_states/decoder/layers_11/self_attention/value/kernel/m                     size 1            shape (1,)                                     partition spec None
I0512 23:42:24.996596 140529434355712 utils.py:1372] Variable param_states/decoder/layers_11/self_attention/value/kernel/v                     size 1            shape (1,)                                     partition spec None
I0512 23:42:24.996635 140529434355712 utils.py:1372] Variable param_states/decoder/layers_11/self_attention/value/kernel/v_col                 size 3072         shape (3072,)                                  partition spec None
I0512 23:42:24.996673 140529434355712 utils.py:1372] Variable param_states/decoder/layers_11/self_attention/value/kernel/v_row                 size 2048         shape (2048,)                                  partition spec None
I0512 23:42:24.996711 140529434355712 utils.py:1372] Variable param_states/decoder/layers_12/mlp/wi_0/kernel/m                                 size 1            shape (1,)                                     partition spec None
I0512 23:42:24.996753 140529434355712 utils.py:1372] Variable param_states/decoder/layers_12/mlp/wi_0/kernel/v                                 size 1            shape (1,)                                     partition spec None
I0512 23:42:24.996794 140529434355712 utils.py:1372] Variable param_states/decoder/layers_12/mlp/wi_0/kernel/v_col                             size 16384        shape (16384,)                                 partition spec None
I0512 23:42:24.996833 140529434355712 utils.py:1372] Variable param_states/decoder/layers_12/mlp/wi_0/kernel/v_row                             size 2048         shape (2048,)                                  partition spec None
I0512 23:42:24.996871 140529434355712 utils.py:1372] Variable param_states/decoder/layers_12/mlp/wi_1/kernel/m                                 size 1            shape (1,)                                     partition spec None
I0512 23:42:24.996926 140529434355712 utils.py:1372] Variable param_states/decoder/layers_12/mlp/wi_1/kernel/v                                 size 1            shape (1,)                                     partition spec None
I0512 23:42:24.996969 140529434355712 utils.py:1372] Variable param_states/decoder/layers_12/mlp/wi_1/kernel/v_col                             size 8192         shape (8192,)                                  partition spec None
I0512 23:42:24.997018 140529434355712 utils.py:1372] Variable param_states/decoder/layers_12/mlp/wi_1/kernel/v_row                             size 2048         shape (2048,)                                  partition spec None
I0512 23:42:24.997059 140529434355712 utils.py:1372] Variable param_states/decoder/layers_12/mlp/wo/kernel/m                                   size 1            shape (1,)                                     partition spec None
I0512 23:42:24.997097 140529434355712 utils.py:1372] Variable param_states/decoder/layers_12/mlp/wo/kernel/v                                   size 1            shape (1,)                                     partition spec None
I0512 23:42:24.997137 140529434355712 utils.py:1372] Variable param_states/decoder/layers_12/mlp/wo/kernel/v_col                               size 8192         shape (8192,)                                  partition spec None
I0512 23:42:24.997176 140529434355712 utils.py:1372] Variable param_states/decoder/layers_12/mlp/wo/kernel/v_row                               size 2048         shape (2048,)                                  partition spec None
I0512 23:42:24.997214 140529434355712 utils.py:1372] Variable param_states/decoder/layers_12/pre_mlp_layer_norm/scale/m                        size 1            shape (1,)                                     partition spec None
I0512 23:42:24.997255 140529434355712 utils.py:1372] Variable param_states/decoder/layers_12/pre_mlp_layer_norm/scale/v                        size 2048         shape (embed=2048)                             partition spec ('model',)
I0512 23:42:24.997298 140529434355712 utils.py:1372] Variable param_states/decoder/layers_12/pre_mlp_layer_norm/scale/v_col                    size 1            shape (1,)                                     partition spec None
I0512 23:42:24.997337 140529434355712 utils.py:1372] Variable param_states/decoder/layers_12/pre_mlp_layer_norm/scale/v_row                    size 1            shape (1,)                                     partition spec None
I0512 23:42:24.997376 140529434355712 utils.py:1372] Variable param_states/decoder/layers_12/pre_self_attention_layer_norm/scale/m             size 1            shape (1,)                                     partition spec None
I0512 23:42:24.997416 140529434355712 utils.py:1372] Variable param_states/decoder/layers_12/pre_self_attention_layer_norm/scale/v             size 2048         shape (embed=2048)                             partition spec ('model',)
I0512 23:42:24.997457 140529434355712 utils.py:1372] Variable param_states/decoder/layers_12/pre_self_attention_layer_norm/scale/v_col         size 1            shape (1,)                                     partition spec None
I0512 23:42:24.997496 140529434355712 utils.py:1372] Variable param_states/decoder/layers_12/pre_self_attention_layer_norm/scale/v_row         size 1            shape (1,)                                     partition spec None
I0512 23:42:24.997534 140529434355712 utils.py:1372] Variable param_states/decoder/layers_12/self_attention/key/kernel/m                       size 1            shape (1,)                                     partition spec None
I0512 23:42:24.997605 140529434355712 utils.py:1372] Variable param_states/decoder/layers_12/self_attention/key/kernel/v                       size 1            shape (1,)                                     partition spec None
I0512 23:42:24.997646 140529434355712 utils.py:1372] Variable param_states/decoder/layers_12/self_attention/key/kernel/v_col                   size 3072         shape (3072,)                                  partition spec None
I0512 23:42:24.997684 140529434355712 utils.py:1372] Variable param_states/decoder/layers_12/self_attention/key/kernel/v_row                   size 2048         shape (2048,)                                  partition spec None
I0512 23:42:24.997722 140529434355712 utils.py:1372] Variable param_states/decoder/layers_12/self_attention/out/kernel/m                       size 1            shape (1,)                                     partition spec None
I0512 23:42:24.997762 140529434355712 utils.py:1372] Variable param_states/decoder/layers_12/self_attention/out/kernel/v                       size 1            shape (1,)                                     partition spec None
I0512 23:42:24.997802 140529434355712 utils.py:1372] Variable param_states/decoder/layers_12/self_attention/out/kernel/v_col                   size 3072         shape (3072,)                                  partition spec None
I0512 23:42:24.997840 140529434355712 utils.py:1372] Variable param_states/decoder/layers_12/self_attention/out/kernel/v_row                   size 2048         shape (2048,)                                  partition spec None
I0512 23:42:24.997879 140529434355712 utils.py:1372] Variable param_states/decoder/layers_12/self_attention/query/kernel/m                     size 1            shape (1,)                                     partition spec None
I0512 23:42:24.997917 140529434355712 utils.py:1372] Variable param_states/decoder/layers_12/self_attention/query/kernel/v                     size 1            shape (1,)                                     partition spec None
I0512 23:42:24.997956 140529434355712 utils.py:1372] Variable param_states/decoder/layers_12/self_attention/query/kernel/v_col                 size 3072         shape (3072,)                                  partition spec None
I0512 23:42:24.998003 140529434355712 utils.py:1372] Variable param_states/decoder/layers_12/self_attention/query/kernel/v_row                 size 2048         shape (2048,)                                  partition spec None
I0512 23:42:24.998044 140529434355712 utils.py:1372] Variable param_states/decoder/layers_12/self_attention/value/kernel/m                     size 1            shape (1,)                                     partition spec None
I0512 23:42:24.998082 140529434355712 utils.py:1372] Variable param_states/decoder/layers_12/self_attention/value/kernel/v                     size 1            shape (1,)                                     partition spec None
I0512 23:42:24.998121 140529434355712 utils.py:1372] Variable param_states/decoder/layers_12/self_attention/value/kernel/v_col                 size 3072         shape (3072,)                                  partition spec None
I0512 23:42:24.998160 140529434355712 utils.py:1372] Variable param_states/decoder/layers_12/self_attention/value/kernel/v_row                 size 2048         shape (2048,)                                  partition spec None
I0512 23:42:24.998198 140529434355712 utils.py:1372] Variable param_states/decoder/layers_13/mlp/wi_0/kernel/m                                 size 1            shape (1,)                                     partition spec None
I0512 23:42:24.998237 140529434355712 utils.py:1372] Variable param_states/decoder/layers_13/mlp/wi_0/kernel/v                                 size 1            shape (1,)                                     partition spec None
I0512 23:42:24.998276 140529434355712 utils.py:1372] Variable param_states/decoder/layers_13/mlp/wi_0/kernel/v_col                             size 16384        shape (16384,)                                 partition spec None
I0512 23:42:24.998316 140529434355712 utils.py:1372] Variable param_states/decoder/layers_13/mlp/wi_0/kernel/v_row                             size 2048         shape (2048,)                                  partition spec None
I0512 23:42:24.998353 140529434355712 utils.py:1372] Variable param_states/decoder/layers_13/mlp/wi_1/kernel/m                                 size 1            shape (1,)                                     partition spec None
I0512 23:42:24.998392 140529434355712 utils.py:1372] Variable param_states/decoder/layers_13/mlp/wi_1/kernel/v                                 size 1            shape (1,)                                     partition spec None
I0512 23:42:24.998431 140529434355712 utils.py:1372] Variable param_states/decoder/layers_13/mlp/wi_1/kernel/v_col                             size 8192         shape (8192,)                                  partition spec None
I0512 23:42:24.998470 140529434355712 utils.py:1372] Variable param_states/decoder/layers_13/mlp/wi_1/kernel/v_row                             size 2048         shape (2048,)                                  partition spec None
I0512 23:42:24.998508 140529434355712 utils.py:1372] Variable param_states/decoder/layers_13/mlp/wo/kernel/m                                   size 1            shape (1,)                                     partition spec None
I0512 23:42:24.998546 140529434355712 utils.py:1372] Variable param_states/decoder/layers_13/mlp/wo/kernel/v                                   size 1            shape (1,)                                     partition spec None
I0512 23:42:24.998585 140529434355712 utils.py:1372] Variable param_states/decoder/layers_13/mlp/wo/kernel/v_col                               size 8192         shape (8192,)                                  partition spec None
I0512 23:42:24.998623 140529434355712 utils.py:1372] Variable param_states/decoder/layers_13/mlp/wo/kernel/v_row                               size 2048         shape (2048,)                                  partition spec None
I0512 23:42:24.998661 140529434355712 utils.py:1372] Variable param_states/decoder/layers_13/pre_mlp_layer_norm/scale/m                        size 1            shape (1,)                                     partition spec None
I0512 23:42:24.998703 140529434355712 utils.py:1372] Variable param_states/decoder/layers_13/pre_mlp_layer_norm/scale/v                        size 2048         shape (embed=2048)                             partition spec ('model',)
I0512 23:42:24.998746 140529434355712 utils.py:1372] Variable param_states/decoder/layers_13/pre_mlp_layer_norm/scale/v_col                    size 1            shape (1,)                                     partition spec None
I0512 23:42:24.998787 140529434355712 utils.py:1372] Variable param_states/decoder/layers_13/pre_mlp_layer_norm/scale/v_row                    size 1            shape (1,)                                     partition spec None
I0512 23:42:24.998853 140529434355712 utils.py:1372] Variable param_states/decoder/layers_13/pre_self_attention_layer_norm/scale/m             size 1            shape (1,)                                     partition spec None
I0512 23:42:24.998899 140529434355712 utils.py:1372] Variable param_states/decoder/layers_13/pre_self_attention_layer_norm/scale/v             size 2048         shape (embed=2048)                             partition spec ('model',)
I0512 23:42:24.998940 140529434355712 utils.py:1372] Variable param_states/decoder/layers_13/pre_self_attention_layer_norm/scale/v_col         size 1            shape (1,)                                     partition spec None
I0512 23:42:24.998979 140529434355712 utils.py:1372] Variable param_states/decoder/layers_13/pre_self_attention_layer_norm/scale/v_row         size 1            shape (1,)                                     partition spec None
I0512 23:42:24.999026 140529434355712 utils.py:1372] Variable param_states/decoder/layers_13/self_attention/key/kernel/m                       size 1            shape (1,)                                     partition spec None
I0512 23:42:24.999066 140529434355712 utils.py:1372] Variable param_states/decoder/layers_13/self_attention/key/kernel/v                       size 1            shape (1,)                                     partition spec None
I0512 23:42:24.999104 140529434355712 utils.py:1372] Variable param_states/decoder/layers_13/self_attention/key/kernel/v_col                   size 3072         shape (3072,)                                  partition spec None
I0512 23:42:24.999144 140529434355712 utils.py:1372] Variable param_states/decoder/layers_13/self_attention/key/kernel/v_row                   size 2048         shape (2048,)                                  partition spec None
I0512 23:42:24.999183 140529434355712 utils.py:1372] Variable param_states/decoder/layers_13/self_attention/out/kernel/m                       size 1            shape (1,)                                     partition spec None
I0512 23:42:24.999221 140529434355712 utils.py:1372] Variable param_states/decoder/layers_13/self_attention/out/kernel/v                       size 1            shape (1,)                                     partition spec None
I0512 23:42:24.999260 140529434355712 utils.py:1372] Variable param_states/decoder/layers_13/self_attention/out/kernel/v_col                   size 3072         shape (3072,)                                  partition spec None
I0512 23:42:24.999299 140529434355712 utils.py:1372] Variable param_states/decoder/layers_13/self_attention/out/kernel/v_row                   size 2048         shape (2048,)                                  partition spec None
I0512 23:42:24.999337 140529434355712 utils.py:1372] Variable param_states/decoder/layers_13/self_attention/query/kernel/m                     size 1            shape (1,)                                     partition spec None
I0512 23:42:24.999375 140529434355712 utils.py:1372] Variable param_states/decoder/layers_13/self_attention/query/kernel/v                     size 1            shape (1,)                                     partition spec None
I0512 23:42:24.999414 140529434355712 utils.py:1372] Variable param_states/decoder/layers_13/self_attention/query/kernel/v_col                 size 3072         shape (3072,)                                  partition spec None
I0512 23:42:24.999453 140529434355712 utils.py:1372] Variable param_states/decoder/layers_13/self_attention/query/kernel/v_row                 size 2048         shape (2048,)                                  partition spec None
I0512 23:42:24.999490 140529434355712 utils.py:1372] Variable param_states/decoder/layers_13/self_attention/value/kernel/m                     size 1            shape (1,)                                     partition spec None
I0512 23:42:24.999529 140529434355712 utils.py:1372] Variable param_states/decoder/layers_13/self_attention/value/kernel/v                     size 1            shape (1,)                                     partition spec None
I0512 23:42:24.999567 140529434355712 utils.py:1372] Variable param_states/decoder/layers_13/self_attention/value/kernel/v_col                 size 3072         shape (3072,)                                  partition spec None
I0512 23:42:24.999606 140529434355712 utils.py:1372] Variable param_states/decoder/layers_13/self_attention/value/kernel/v_row                 size 2048         shape (2048,)                                  partition spec None
I0512 23:42:24.999644 140529434355712 utils.py:1372] Variable param_states/decoder/layers_14/mlp/wi_0/kernel/m                                 size 1            shape (1,)                                     partition spec None
I0512 23:42:24.999683 140529434355712 utils.py:1372] Variable param_states/decoder/layers_14/mlp/wi_0/kernel/v                                 size 1            shape (1,)                                     partition spec None
I0512 23:42:24.999723 140529434355712 utils.py:1372] Variable param_states/decoder/layers_14/mlp/wi_0/kernel/v_col                             size 16384        shape (16384,)                                 partition spec None
I0512 23:42:24.999764 140529434355712 utils.py:1372] Variable param_states/decoder/layers_14/mlp/wi_0/kernel/v_row                             size 2048         shape (2048,)                                  partition spec None
I0512 23:42:24.999802 140529434355712 utils.py:1372] Variable param_states/decoder/layers_14/mlp/wi_1/kernel/m                                 size 1            shape (1,)                                     partition spec None
I0512 23:42:24.999841 140529434355712 utils.py:1372] Variable param_states/decoder/layers_14/mlp/wi_1/kernel/v                                 size 1            shape (1,)                                     partition spec None
I0512 23:42:24.999879 140529434355712 utils.py:1372] Variable param_states/decoder/layers_14/mlp/wi_1/kernel/v_col                             size 8192         shape (8192,)                                  partition spec None
I0512 23:42:24.999917 140529434355712 utils.py:1372] Variable param_states/decoder/layers_14/mlp/wi_1/kernel/v_row                             size 2048         shape (2048,)                                  partition spec None
I0512 23:42:24.999955 140529434355712 utils.py:1372] Variable param_states/decoder/layers_14/mlp/wo/kernel/m                                   size 1            shape (1,)                                     partition spec None
I0512 23:42:25.000000 140529434355712 utils.py:1372] Variable param_states/decoder/layers_14/mlp/wo/kernel/v                                   size 1            shape (1,)                                     partition spec None
I0512 23:42:25.000041 140529434355712 utils.py:1372] Variable param_states/decoder/layers_14/mlp/wo/kernel/v_col                               size 8192         shape (8192,)                                  partition spec None
I0512 23:42:25.000081 140529434355712 utils.py:1372] Variable param_states/decoder/layers_14/mlp/wo/kernel/v_row                               size 2048         shape (2048,)                                  partition spec None
I0512 23:42:25.000118 140529434355712 utils.py:1372] Variable param_states/decoder/layers_14/pre_mlp_layer_norm/scale/m                        size 1            shape (1,)                                     partition spec None
I0512 23:42:25.000160 140529434355712 utils.py:1372] Variable param_states/decoder/layers_14/pre_mlp_layer_norm/scale/v                        size 2048         shape (embed=2048)                             partition spec ('model',)
I0512 23:42:25.000201 140529434355712 utils.py:1372] Variable param_states/decoder/layers_14/pre_mlp_layer_norm/scale/v_col                    size 1            shape (1,)                                     partition spec None
I0512 23:42:25.000239 140529434355712 utils.py:1372] Variable param_states/decoder/layers_14/pre_mlp_layer_norm/scale/v_row                    size 1            shape (1,)                                     partition spec None
I0512 23:42:25.000277 140529434355712 utils.py:1372] Variable param_states/decoder/layers_14/pre_self_attention_layer_norm/scale/m             size 1            shape (1,)                                     partition spec None
I0512 23:42:25.000318 140529434355712 utils.py:1372] Variable param_states/decoder/layers_14/pre_self_attention_layer_norm/scale/v             size 2048         shape (embed=2048)                             partition spec ('model',)
I0512 23:42:25.000358 140529434355712 utils.py:1372] Variable param_states/decoder/layers_14/pre_self_attention_layer_norm/scale/v_col         size 1            shape (1,)                                     partition spec None
I0512 23:42:25.000396 140529434355712 utils.py:1372] Variable param_states/decoder/layers_14/pre_self_attention_layer_norm/scale/v_row         size 1            shape (1,)                                     partition spec None
I0512 23:42:25.000434 140529434355712 utils.py:1372] Variable param_states/decoder/layers_14/self_attention/key/kernel/m                       size 1            shape (1,)                                     partition spec None
I0512 23:42:25.000473 140529434355712 utils.py:1372] Variable param_states/decoder/layers_14/self_attention/key/kernel/v                       size 1            shape (1,)                                     partition spec None
I0512 23:42:25.000512 140529434355712 utils.py:1372] Variable param_states/decoder/layers_14/self_attention/key/kernel/v_col                   size 3072         shape (3072,)                                  partition spec None
I0512 23:42:25.000550 140529434355712 utils.py:1372] Variable param_states/decoder/layers_14/self_attention/key/kernel/v_row                   size 2048         shape (2048,)                                  partition spec None
I0512 23:42:25.000588 140529434355712 utils.py:1372] Variable param_states/decoder/layers_14/self_attention/out/kernel/m                       size 1            shape (1,)                                     partition spec None
I0512 23:42:25.000627 140529434355712 utils.py:1372] Variable param_states/decoder/layers_14/self_attention/out/kernel/v                       size 1            shape (1,)                                     partition spec None
I0512 23:42:25.000666 140529434355712 utils.py:1372] Variable param_states/decoder/layers_14/self_attention/out/kernel/v_col                   size 3072         shape (3072,)                                  partition spec None
I0512 23:42:25.000722 140529434355712 utils.py:1372] Variable param_states/decoder/layers_14/self_attention/out/kernel/v_row                   size 2048         shape (2048,)                                  partition spec None
I0512 23:42:25.000767 140529434355712 utils.py:1372] Variable param_states/decoder/layers_14/self_attention/query/kernel/m                     size 1            shape (1,)                                     partition spec None
I0512 23:42:25.000807 140529434355712 utils.py:1372] Variable param_states/decoder/layers_14/self_attention/query/kernel/v                     size 1            shape (1,)                                     partition spec None
I0512 23:42:25.000846 140529434355712 utils.py:1372] Variable param_states/decoder/layers_14/self_attention/query/kernel/v_col                 size 3072         shape (3072,)                                  partition spec None
I0512 23:42:25.000889 140529434355712 utils.py:1372] Variable param_states/decoder/layers_14/self_attention/query/kernel/v_row                 size 2048         shape (2048,)                                  partition spec None
I0512 23:42:25.000927 140529434355712 utils.py:1372] Variable param_states/decoder/layers_14/self_attention/value/kernel/m                     size 1            shape (1,)                                     partition spec None
I0512 23:42:25.000965 140529434355712 utils.py:1372] Variable param_states/decoder/layers_14/self_attention/value/kernel/v                     size 1            shape (1,)                                     partition spec None
I0512 23:42:25.001011 140529434355712 utils.py:1372] Variable param_states/decoder/layers_14/self_attention/value/kernel/v_col                 size 3072         shape (3072,)                                  partition spec None
I0512 23:42:25.001054 140529434355712 utils.py:1372] Variable param_states/decoder/layers_14/self_attention/value/kernel/v_row                 size 2048         shape (2048,)                                  partition spec None
I0512 23:42:25.001094 140529434355712 utils.py:1372] Variable param_states/decoder/layers_15/mlp/wi_0/kernel/m                                 size 1            shape (1,)                                     partition spec None
I0512 23:42:25.001135 140529434355712 utils.py:1372] Variable param_states/decoder/layers_15/mlp/wi_0/kernel/v                                 size 1            shape (1,)                                     partition spec None
I0512 23:42:25.001176 140529434355712 utils.py:1372] Variable param_states/decoder/layers_15/mlp/wi_0/kernel/v_col                             size 16384        shape (16384,)                                 partition spec None
I0512 23:42:25.001215 140529434355712 utils.py:1372] Variable param_states/decoder/layers_15/mlp/wi_0/kernel/v_row                             size 2048         shape (2048,)                                  partition spec None
I0512 23:42:25.001255 140529434355712 utils.py:1372] Variable param_states/decoder/layers_15/mlp/wi_1/kernel/m                                 size 1            shape (1,)                                     partition spec None
I0512 23:42:25.001294 140529434355712 utils.py:1372] Variable param_states/decoder/layers_15/mlp/wi_1/kernel/v                                 size 1            shape (1,)                                     partition spec None
I0512 23:42:25.001335 140529434355712 utils.py:1372] Variable param_states/decoder/layers_15/mlp/wi_1/kernel/v_col                             size 8192         shape (8192,)                                  partition spec None
I0512 23:42:25.001375 140529434355712 utils.py:1372] Variable param_states/decoder/layers_15/mlp/wi_1/kernel/v_row                             size 2048         shape (2048,)                                  partition spec None
I0512 23:42:25.001415 140529434355712 utils.py:1372] Variable param_states/decoder/layers_15/mlp/wo/kernel/m                                   size 1            shape (1,)                                     partition spec None
I0512 23:42:25.001455 140529434355712 utils.py:1372] Variable param_states/decoder/layers_15/mlp/wo/kernel/v                                   size 1            shape (1,)                                     partition spec None
I0512 23:42:25.001495 140529434355712 utils.py:1372] Variable param_states/decoder/layers_15/mlp/wo/kernel/v_col                               size 8192         shape (8192,)                                  partition spec None
I0512 23:42:25.001535 140529434355712 utils.py:1372] Variable param_states/decoder/layers_15/mlp/wo/kernel/v_row                               size 2048         shape (2048,)                                  partition spec None
I0512 23:42:25.001608 140529434355712 utils.py:1372] Variable param_states/decoder/layers_15/pre_mlp_layer_norm/scale/m                        size 1            shape (1,)                                     partition spec None
I0512 23:42:25.001653 140529434355712 utils.py:1372] Variable param_states/decoder/layers_15/pre_mlp_layer_norm/scale/v                        size 2048         shape (embed=2048)                             partition spec ('model',)
I0512 23:42:25.001697 140529434355712 utils.py:1372] Variable param_states/decoder/layers_15/pre_mlp_layer_norm/scale/v_col                    size 1            shape (1,)                                     partition spec None
I0512 23:42:25.001737 140529434355712 utils.py:1372] Variable param_states/decoder/layers_15/pre_mlp_layer_norm/scale/v_row                    size 1            shape (1,)                                     partition spec None
I0512 23:42:25.001779 140529434355712 utils.py:1372] Variable param_states/decoder/layers_15/pre_self_attention_layer_norm/scale/m             size 1            shape (1,)                                     partition spec None
I0512 23:42:25.001822 140529434355712 utils.py:1372] Variable param_states/decoder/layers_15/pre_self_attention_layer_norm/scale/v             size 2048         shape (embed=2048)                             partition spec ('model',)
I0512 23:42:25.001864 140529434355712 utils.py:1372] Variable param_states/decoder/layers_15/pre_self_attention_layer_norm/scale/v_col         size 1            shape (1,)                                     partition spec None
I0512 23:42:25.001903 140529434355712 utils.py:1372] Variable param_states/decoder/layers_15/pre_self_attention_layer_norm/scale/v_row         size 1            shape (1,)                                     partition spec None
I0512 23:42:25.001943 140529434355712 utils.py:1372] Variable param_states/decoder/layers_15/self_attention/key/kernel/m                       size 1            shape (1,)                                     partition spec None
I0512 23:42:25.001989 140529434355712 utils.py:1372] Variable param_states/decoder/layers_15/self_attention/key/kernel/v                       size 1            shape (1,)                                     partition spec None
I0512 23:42:25.002031 140529434355712 utils.py:1372] Variable param_states/decoder/layers_15/self_attention/key/kernel/v_col                   size 3072         shape (3072,)                                  partition spec None
I0512 23:42:25.002072 140529434355712 utils.py:1372] Variable param_states/decoder/layers_15/self_attention/key/kernel/v_row                   size 2048         shape (2048,)                                  partition spec None
I0512 23:42:25.002111 140529434355712 utils.py:1372] Variable param_states/decoder/layers_15/self_attention/out/kernel/m                       size 1            shape (1,)                                     partition spec None
I0512 23:42:25.002151 140529434355712 utils.py:1372] Variable param_states/decoder/layers_15/self_attention/out/kernel/v                       size 1            shape (1,)                                     partition spec None
I0512 23:42:25.002191 140529434355712 utils.py:1372] Variable param_states/decoder/layers_15/self_attention/out/kernel/v_col                   size 3072         shape (3072,)                                  partition spec None
I0512 23:42:25.002230 140529434355712 utils.py:1372] Variable param_states/decoder/layers_15/self_attention/out/kernel/v_row                   size 2048         shape (2048,)                                  partition spec None
I0512 23:42:25.002269 140529434355712 utils.py:1372] Variable param_states/decoder/layers_15/self_attention/query/kernel/m                     size 1            shape (1,)                                     partition spec None
I0512 23:42:25.002309 140529434355712 utils.py:1372] Variable param_states/decoder/layers_15/self_attention/query/kernel/v                     size 1            shape (1,)                                     partition spec None
I0512 23:42:25.002349 140529434355712 utils.py:1372] Variable param_states/decoder/layers_15/self_attention/query/kernel/v_col                 size 3072         shape (3072,)                                  partition spec None
I0512 23:42:25.002388 140529434355712 utils.py:1372] Variable param_states/decoder/layers_15/self_attention/query/kernel/v_row                 size 2048         shape (2048,)                                  partition spec None
I0512 23:42:25.002428 140529434355712 utils.py:1372] Variable param_states/decoder/layers_15/self_attention/value/kernel/m                     size 1            shape (1,)                                     partition spec None
I0512 23:42:25.002468 140529434355712 utils.py:1372] Variable param_states/decoder/layers_15/self_attention/value/kernel/v                     size 1            shape (1,)                                     partition spec None
I0512 23:42:25.002507 140529434355712 utils.py:1372] Variable param_states/decoder/layers_15/self_attention/value/kernel/v_col                 size 3072         shape (3072,)                                  partition spec None
I0512 23:42:25.002547 140529434355712 utils.py:1372] Variable param_states/decoder/layers_15/self_attention/value/kernel/v_row                 size 2048         shape (2048,)                                  partition spec None
I0512 23:42:25.002586 140529434355712 utils.py:1372] Variable param_states/decoder/layers_16/mlp/wi_0/kernel/m                                 size 1            shape (1,)                                     partition spec None
I0512 23:42:25.002626 140529434355712 utils.py:1372] Variable param_states/decoder/layers_16/mlp/wi_0/kernel/v                                 size 1            shape (1,)                                     partition spec None
I0512 23:42:25.002684 140529434355712 utils.py:1372] Variable param_states/decoder/layers_16/mlp/wi_0/kernel/v_col                             size 16384        shape (16384,)                                 partition spec None
I0512 23:42:25.002728 140529434355712 utils.py:1372] Variable param_states/decoder/layers_16/mlp/wi_0/kernel/v_row                             size 2048         shape (2048,)                                  partition spec None
I0512 23:42:25.002771 140529434355712 utils.py:1372] Variable param_states/decoder/layers_16/mlp/wi_1/kernel/m                                 size 1            shape (1,)                                     partition spec None
I0512 23:42:25.002812 140529434355712 utils.py:1372] Variable param_states/decoder/layers_16/mlp/wi_1/kernel/v                                 size 1            shape (1,)                                     partition spec None
I0512 23:42:25.002852 140529434355712 utils.py:1372] Variable param_states/decoder/layers_16/mlp/wi_1/kernel/v_col                             size 8192         shape (8192,)                                  partition spec None
I0512 23:42:25.002892 140529434355712 utils.py:1372] Variable param_states/decoder/layers_16/mlp/wi_1/kernel/v_row                             size 2048         shape (2048,)                                  partition spec None
I0512 23:42:25.002932 140529434355712 utils.py:1372] Variable param_states/decoder/layers_16/mlp/wo/kernel/m                                   size 1            shape (1,)                                     partition spec None
I0512 23:42:25.002972 140529434355712 utils.py:1372] Variable param_states/decoder/layers_16/mlp/wo/kernel/v                                   size 1            shape (1,)                                     partition spec None
I0512 23:42:25.003020 140529434355712 utils.py:1372] Variable param_states/decoder/layers_16/mlp/wo/kernel/v_col                               size 8192         shape (8192,)                                  partition spec None
I0512 23:42:25.003061 140529434355712 utils.py:1372] Variable param_states/decoder/layers_16/mlp/wo/kernel/v_row                               size 2048         shape (2048,)                                  partition spec None
I0512 23:42:25.003101 140529434355712 utils.py:1372] Variable param_states/decoder/layers_16/pre_mlp_layer_norm/scale/m                        size 1            shape (1,)                                     partition spec None
I0512 23:42:25.003144 140529434355712 utils.py:1372] Variable param_states/decoder/layers_16/pre_mlp_layer_norm/scale/v                        size 2048         shape (embed=2048)                             partition spec ('model',)
I0512 23:42:25.003185 140529434355712 utils.py:1372] Variable param_states/decoder/layers_16/pre_mlp_layer_norm/scale/v_col                    size 1            shape (1,)                                     partition spec None
I0512 23:42:25.003225 140529434355712 utils.py:1372] Variable param_states/decoder/layers_16/pre_mlp_layer_norm/scale/v_row                    size 1            shape (1,)                                     partition spec None
I0512 23:42:25.003264 140529434355712 utils.py:1372] Variable param_states/decoder/layers_16/pre_self_attention_layer_norm/scale/m             size 1            shape (1,)                                     partition spec None
I0512 23:42:25.003306 140529434355712 utils.py:1372] Variable param_states/decoder/layers_16/pre_self_attention_layer_norm/scale/v             size 2048         shape (embed=2048)                             partition spec ('model',)
I0512 23:42:25.003348 140529434355712 utils.py:1372] Variable param_states/decoder/layers_16/pre_self_attention_layer_norm/scale/v_col         size 1            shape (1,)                                     partition spec None
I0512 23:42:25.003388 140529434355712 utils.py:1372] Variable param_states/decoder/layers_16/pre_self_attention_layer_norm/scale/v_row         size 1            shape (1,)                                     partition spec None
I0512 23:42:25.003427 140529434355712 utils.py:1372] Variable param_states/decoder/layers_16/self_attention/key/kernel/m                       size 1            shape (1,)                                     partition spec None
I0512 23:42:25.003467 140529434355712 utils.py:1372] Variable param_states/decoder/layers_16/self_attention/key/kernel/v                       size 1            shape (1,)                                     partition spec None
I0512 23:42:25.003507 140529434355712 utils.py:1372] Variable param_states/decoder/layers_16/self_attention/key/kernel/v_col                   size 3072         shape (3072,)                                  partition spec None
I0512 23:42:25.003547 140529434355712 utils.py:1372] Variable param_states/decoder/layers_16/self_attention/key/kernel/v_row                   size 2048         shape (2048,)                                  partition spec None
I0512 23:42:25.003585 140529434355712 utils.py:1372] Variable param_states/decoder/layers_16/self_attention/out/kernel/m                       size 1            shape (1,)                                     partition spec None
I0512 23:42:25.003625 140529434355712 utils.py:1372] Variable param_states/decoder/layers_16/self_attention/out/kernel/v                       size 1            shape (1,)                                     partition spec None
I0512 23:42:25.003664 140529434355712 utils.py:1372] Variable param_states/decoder/layers_16/self_attention/out/kernel/v_col                   size 3072         shape (3072,)                                  partition spec None
I0512 23:42:25.003703 140529434355712 utils.py:1372] Variable param_states/decoder/layers_16/self_attention/out/kernel/v_row                   size 2048         shape (2048,)                                  partition spec None
I0512 23:42:25.003744 140529434355712 utils.py:1372] Variable param_states/decoder/layers_16/self_attention/query/kernel/m                     size 1            shape (1,)                                     partition spec None
I0512 23:42:25.003786 140529434355712 utils.py:1372] Variable param_states/decoder/layers_16/self_attention/query/kernel/v                     size 1            shape (1,)                                     partition spec None
I0512 23:42:25.003826 140529434355712 utils.py:1372] Variable param_states/decoder/layers_16/self_attention/query/kernel/v_col                 size 3072         shape (3072,)                                  partition spec None
I0512 23:42:25.003865 140529434355712 utils.py:1372] Variable param_states/decoder/layers_16/self_attention/query/kernel/v_row                 size 2048         shape (2048,)                                  partition spec None
I0512 23:42:25.003904 140529434355712 utils.py:1372] Variable param_states/decoder/layers_16/self_attention/value/kernel/m                     size 1            shape (1,)                                     partition spec None
I0512 23:42:25.003943 140529434355712 utils.py:1372] Variable param_states/decoder/layers_16/self_attention/value/kernel/v                     size 1            shape (1,)                                     partition spec None
I0512 23:42:25.003988 140529434355712 utils.py:1372] Variable param_states/decoder/layers_16/self_attention/value/kernel/v_col                 size 3072         shape (3072,)                                  partition spec None
I0512 23:42:25.004031 140529434355712 utils.py:1372] Variable param_states/decoder/layers_16/self_attention/value/kernel/v_row                 size 2048         shape (2048,)                                  partition spec None
I0512 23:42:25.004071 140529434355712 utils.py:1372] Variable param_states/decoder/layers_17/extra_mlp/wi_0/kernel/m                           size 1            shape (1,)                                     partition spec None
I0512 23:42:25.004112 140529434355712 utils.py:1372] Variable param_states/decoder/layers_17/extra_mlp/wi_0/kernel/v                           size 1            shape (1,)                                     partition spec None
I0512 23:42:25.004153 140529434355712 utils.py:1372] Variable param_states/decoder/layers_17/extra_mlp/wi_0/kernel/v_col                       size 16384        shape (16384,)                                 partition spec None
I0512 23:42:25.004193 140529434355712 utils.py:1372] Variable param_states/decoder/layers_17/extra_mlp/wi_0/kernel/v_row                       size 2048         shape (2048,)                                  partition spec None
I0512 23:42:25.004231 140529434355712 utils.py:1372] Variable param_states/decoder/layers_17/extra_mlp/wi_1/kernel/m                           size 1            shape (1,)                                     partition spec None
I0512 23:42:25.004270 140529434355712 utils.py:1372] Variable param_states/decoder/layers_17/extra_mlp/wi_1/kernel/v                           size 1            shape (1,)                                     partition spec None
I0512 23:42:25.004310 140529434355712 utils.py:1372] Variable param_states/decoder/layers_17/extra_mlp/wi_1/kernel/v_col                       size 8192         shape (8192,)                                  partition spec None
I0512 23:42:25.004349 140529434355712 utils.py:1372] Variable param_states/decoder/layers_17/extra_mlp/wi_1/kernel/v_row                       size 2048         shape (2048,)                                  partition spec None
I0512 23:42:25.004390 140529434355712 utils.py:1372] Variable param_states/decoder/layers_17/extra_mlp/wo/kernel/m                             size 1            shape (1,)                                     partition spec None
I0512 23:42:25.004428 140529434355712 utils.py:1372] Variable param_states/decoder/layers_17/extra_mlp/wo/kernel/v                             size 1            shape (1,)                                     partition spec None
I0512 23:42:25.004468 140529434355712 utils.py:1372] Variable param_states/decoder/layers_17/extra_mlp/wo/kernel/v_col                         size 8192         shape (8192,)                                  partition spec None
I0512 23:42:25.004507 140529434355712 utils.py:1372] Variable param_states/decoder/layers_17/extra_mlp/wo/kernel/v_row                         size 2048         shape (2048,)                                  partition spec None
I0512 23:42:25.004546 140529434355712 utils.py:1372] Variable param_states/decoder/layers_17/mlp/expert/wi_0/kernel/m                          size 1            shape (1,)                                     partition spec None
I0512 23:42:25.004604 140529434355712 utils.py:1372] Variable param_states/decoder/layers_17/mlp/expert/wi_0/kernel/v                          size 1            shape (1,)                                     partition spec None
I0512 23:42:25.004649 140529434355712 utils.py:1372] Variable param_states/decoder/layers_17/mlp/expert/wi_0/kernel/v_col                      size 131072       shape (8, 16384)                               partition spec ('expert',)
I0512 23:42:25.004692 140529434355712 utils.py:1372] Variable param_states/decoder/layers_17/mlp/expert/wi_0/kernel/v_row                      size 16384        shape (8, 2048)                                partition spec ('expert',)
I0512 23:42:25.004732 140529434355712 utils.py:1372] Variable param_states/decoder/layers_17/mlp/expert/wi_1/kernel/m                          size 1            shape (1,)                                     partition spec None
I0512 23:42:25.004776 140529434355712 utils.py:1372] Variable param_states/decoder/layers_17/mlp/expert/wi_1/kernel/v                          size 1            shape (1,)                                     partition spec None
I0512 23:42:25.004817 140529434355712 utils.py:1372] Variable param_states/decoder/layers_17/mlp/expert/wi_1/kernel/v_col                      size 65536        shape (8, 8192)                                partition spec ('expert',)
I0512 23:42:25.004857 140529434355712 utils.py:1372] Variable param_states/decoder/layers_17/mlp/expert/wi_1/kernel/v_row                      size 16384        shape (8, 2048)                                partition spec ('expert',)
I0512 23:42:25.004897 140529434355712 utils.py:1372] Variable param_states/decoder/layers_17/mlp/expert/wo/kernel/m                            size 1            shape (1,)                                     partition spec None
I0512 23:42:25.004936 140529434355712 utils.py:1372] Variable param_states/decoder/layers_17/mlp/expert/wo/kernel/v                            size 1            shape (1,)                                     partition spec None
I0512 23:42:25.004976 140529434355712 utils.py:1372] Variable param_states/decoder/layers_17/mlp/expert/wo/kernel/v_col                        size 65536        shape (8, 8192)                                partition spec ('expert',)
I0512 23:42:25.005024 140529434355712 utils.py:1372] Variable param_states/decoder/layers_17/mlp/expert/wo/kernel/v_row                        size 16384        shape (8, 2048)                                partition spec ('expert',)
I0512 23:42:25.005066 140529434355712 utils.py:1372] Variable param_states/decoder/layers_17/mlp/router/router_weights/w/kernel/m              size 1            shape (1,)                                     partition spec None
I0512 23:42:25.005109 140529434355712 utils.py:1372] Variable param_states/decoder/layers_17/mlp/router/router_weights/w/kernel/v              size 16384        shape (embed=2048, unmodeled=8)                partition spec ('model', None)
I0512 23:42:25.005151 140529434355712 utils.py:1372] Variable param_states/decoder/layers_17/mlp/router/router_weights/w/kernel/v_col          size 1            shape (1,)                                     partition spec None
I0512 23:42:25.005191 140529434355712 utils.py:1372] Variable param_states/decoder/layers_17/mlp/router/router_weights/w/kernel/v_row          size 1            shape (1,)                                     partition spec None
I0512 23:42:25.005230 140529434355712 utils.py:1372] Variable param_states/decoder/layers_17/pre_extra_mlp_layer_norm/scale/m                  size 1            shape (1,)                                     partition spec None
I0512 23:42:25.005273 140529434355712 utils.py:1372] Variable param_states/decoder/layers_17/pre_extra_mlp_layer_norm/scale/v                  size 2048         shape (embed=2048)                             partition spec ('model',)
I0512 23:42:25.005314 140529434355712 utils.py:1372] Variable param_states/decoder/layers_17/pre_extra_mlp_layer_norm/scale/v_col              size 1            shape (1,)                                     partition spec None
I0512 23:42:25.005353 140529434355712 utils.py:1372] Variable param_states/decoder/layers_17/pre_extra_mlp_layer_norm/scale/v_row              size 1            shape (1,)                                     partition spec None
I0512 23:42:25.005393 140529434355712 utils.py:1372] Variable param_states/decoder/layers_17/pre_mlp_layer_norm/scale/m                        size 1            shape (1,)                                     partition spec None
I0512 23:42:25.005435 140529434355712 utils.py:1372] Variable param_states/decoder/layers_17/pre_mlp_layer_norm/scale/v                        size 2048         shape (embed=2048)                             partition spec ('model',)
I0512 23:42:25.005476 140529434355712 utils.py:1372] Variable param_states/decoder/layers_17/pre_mlp_layer_norm/scale/v_col                    size 1            shape (1,)                                     partition spec None
I0512 23:42:25.005517 140529434355712 utils.py:1372] Variable param_states/decoder/layers_17/pre_mlp_layer_norm/scale/v_row                    size 1            shape (1,)                                     partition spec None
I0512 23:42:25.005579 140529434355712 utils.py:1372] Variable param_states/decoder/layers_17/pre_self_attention_layer_norm/scale/m             size 1            shape (1,)                                     partition spec None
I0512 23:42:25.005627 140529434355712 utils.py:1372] Variable param_states/decoder/layers_17/pre_self_attention_layer_norm/scale/v             size 2048         shape (embed=2048)                             partition spec ('model',)
I0512 23:42:25.005670 140529434355712 utils.py:1372] Variable param_states/decoder/layers_17/pre_self_attention_layer_norm/scale/v_col         size 1            shape (1,)                                     partition spec None
I0512 23:42:25.005710 140529434355712 utils.py:1372] Variable param_states/decoder/layers_17/pre_self_attention_layer_norm/scale/v_row         size 1            shape (1,)                                     partition spec None
I0512 23:42:25.005751 140529434355712 utils.py:1372] Variable param_states/decoder/layers_17/self_attention/key/kernel/m                       size 1            shape (1,)                                     partition spec None
I0512 23:42:25.005793 140529434355712 utils.py:1372] Variable param_states/decoder/layers_17/self_attention/key/kernel/v                       size 1            shape (1,)                                     partition spec None
I0512 23:42:25.005834 140529434355712 utils.py:1372] Variable param_states/decoder/layers_17/self_attention/key/kernel/v_col                   size 3072         shape (3072,)                                  partition spec None
I0512 23:42:25.005873 140529434355712 utils.py:1372] Variable param_states/decoder/layers_17/self_attention/key/kernel/v_row                   size 2048         shape (2048,)                                  partition spec None
I0512 23:42:25.005913 140529434355712 utils.py:1372] Variable param_states/decoder/layers_17/self_attention/out/kernel/m                       size 1            shape (1,)                                     partition spec None
I0512 23:42:25.005952 140529434355712 utils.py:1372] Variable param_states/decoder/layers_17/self_attention/out/kernel/v                       size 1            shape (1,)                                     partition spec None
I0512 23:42:25.005998 140529434355712 utils.py:1372] Variable param_states/decoder/layers_17/self_attention/out/kernel/v_col                   size 3072         shape (3072,)                                  partition spec None
I0512 23:42:25.006040 140529434355712 utils.py:1372] Variable param_states/decoder/layers_17/self_attention/out/kernel/v_row                   size 2048         shape (2048,)                                  partition spec None
I0512 23:42:25.006080 140529434355712 utils.py:1372] Variable param_states/decoder/layers_17/self_attention/query/kernel/m                     size 1            shape (1,)                                     partition spec None
I0512 23:42:25.006120 140529434355712 utils.py:1372] Variable param_states/decoder/layers_17/self_attention/query/kernel/v                     size 1            shape (1,)                                     partition spec None
I0512 23:42:25.006159 140529434355712 utils.py:1372] Variable param_states/decoder/layers_17/self_attention/query/kernel/v_col                 size 3072         shape (3072,)                                  partition spec None
I0512 23:42:25.006197 140529434355712 utils.py:1372] Variable param_states/decoder/layers_17/self_attention/query/kernel/v_row                 size 2048         shape (2048,)                                  partition spec None
I0512 23:42:25.006235 140529434355712 utils.py:1372] Variable param_states/decoder/layers_17/self_attention/value/kernel/m                     size 1            shape (1,)                                     partition spec None
I0512 23:42:25.006273 140529434355712 utils.py:1372] Variable param_states/decoder/layers_17/self_attention/value/kernel/v                     size 1            shape (1,)                                     partition spec None
I0512 23:42:25.006311 140529434355712 utils.py:1372] Variable param_states/decoder/layers_17/self_attention/value/kernel/v_col                 size 3072         shape (3072,)                                  partition spec None
I0512 23:42:25.006349 140529434355712 utils.py:1372] Variable param_states/decoder/layers_17/self_attention/value/kernel/v_row                 size 2048         shape (2048,)                                  partition spec None
I0512 23:42:25.006387 140529434355712 utils.py:1372] Variable param_states/decoder/layers_18/mlp/wi_0/kernel/m                                 size 1            shape (1,)                                     partition spec None
I0512 23:42:25.006426 140529434355712 utils.py:1372] Variable param_states/decoder/layers_18/mlp/wi_0/kernel/v                                 size 1            shape (1,)                                     partition spec None
I0512 23:42:25.006464 140529434355712 utils.py:1372] Variable param_states/decoder/layers_18/mlp/wi_0/kernel/v_col                             size 16384        shape (16384,)                                 partition spec None
I0512 23:42:25.006502 140529434355712 utils.py:1372] Variable param_states/decoder/layers_18/mlp/wi_0/kernel/v_row                             size 2048         shape (2048,)                                  partition spec None
I0512 23:42:25.006571 140529434355712 utils.py:1372] Variable param_states/decoder/layers_18/mlp/wi_1/kernel/m                                 size 1            shape (1,)                                     partition spec None
I0512 23:42:25.006614 140529434355712 utils.py:1372] Variable param_states/decoder/layers_18/mlp/wi_1/kernel/v                                 size 1            shape (1,)                                     partition spec None
I0512 23:42:25.006653 140529434355712 utils.py:1372] Variable param_states/decoder/layers_18/mlp/wi_1/kernel/v_col                             size 8192         shape (8192,)                                  partition spec None
I0512 23:42:25.006690 140529434355712 utils.py:1372] Variable param_states/decoder/layers_18/mlp/wi_1/kernel/v_row                             size 2048         shape (2048,)                                  partition spec None
I0512 23:42:25.006728 140529434355712 utils.py:1372] Variable param_states/decoder/layers_18/mlp/wo/kernel/m                                   size 1            shape (1,)                                     partition spec None
I0512 23:42:25.006770 140529434355712 utils.py:1372] Variable param_states/decoder/layers_18/mlp/wo/kernel/v                                   size 1            shape (1,)                                     partition spec None
I0512 23:42:25.006809 140529434355712 utils.py:1372] Variable param_states/decoder/layers_18/mlp/wo/kernel/v_col                               size 8192         shape (8192,)                                  partition spec None
I0512 23:42:25.006847 140529434355712 utils.py:1372] Variable param_states/decoder/layers_18/mlp/wo/kernel/v_row                               size 2048         shape (2048,)                                  partition spec None
I0512 23:42:25.006885 140529434355712 utils.py:1372] Variable param_states/decoder/layers_18/pre_mlp_layer_norm/scale/m                        size 1            shape (1,)                                     partition spec None
I0512 23:42:25.006926 140529434355712 utils.py:1372] Variable param_states/decoder/layers_18/pre_mlp_layer_norm/scale/v                        size 2048         shape (embed=2048)                             partition spec ('model',)
I0512 23:42:25.006967 140529434355712 utils.py:1372] Variable param_states/decoder/layers_18/pre_mlp_layer_norm/scale/v_col                    size 1            shape (1,)                                     partition spec None
I0512 23:42:25.007013 140529434355712 utils.py:1372] Variable param_states/decoder/layers_18/pre_mlp_layer_norm/scale/v_row                    size 1            shape (1,)                                     partition spec None
I0512 23:42:25.007051 140529434355712 utils.py:1372] Variable param_states/decoder/layers_18/pre_self_attention_layer_norm/scale/m             size 1            shape (1,)                                     partition spec None
I0512 23:42:25.007091 140529434355712 utils.py:1372] Variable param_states/decoder/layers_18/pre_self_attention_layer_norm/scale/v             size 2048         shape (embed=2048)                             partition spec ('model',)
I0512 23:42:25.007132 140529434355712 utils.py:1372] Variable param_states/decoder/layers_18/pre_self_attention_layer_norm/scale/v_col         size 1            shape (1,)                                     partition spec None
I0512 23:42:25.007169 140529434355712 utils.py:1372] Variable param_states/decoder/layers_18/pre_self_attention_layer_norm/scale/v_row         size 1            shape (1,)                                     partition spec None
I0512 23:42:25.007208 140529434355712 utils.py:1372] Variable param_states/decoder/layers_18/self_attention/key/kernel/m                       size 1            shape (1,)                                     partition spec None
I0512 23:42:25.007247 140529434355712 utils.py:1372] Variable param_states/decoder/layers_18/self_attention/key/kernel/v                       size 1            shape (1,)                                     partition spec None
I0512 23:42:25.007599 140529434355712 utils.py:1372] Variable param_states/decoder/layers_18/self_attention/key/kernel/v_col                   size 3072         shape (3072,)                                  partition spec None
I0512 23:42:25.007826 140529434355712 utils.py:1372] Variable param_states/decoder/layers_18/self_attention/key/kernel/v_row                   size 2048         shape (2048,)                                  partition spec None
I0512 23:42:25.007874 140529434355712 utils.py:1372] Variable param_states/decoder/layers_18/self_attention/out/kernel/m                       size 1            shape (1,)                                     partition spec None
I0512 23:42:25.007922 140529434355712 utils.py:1372] Variable param_states/decoder/layers_18/self_attention/out/kernel/v                       size 1            shape (1,)                                     partition spec None
I0512 23:42:25.007964 140529434355712 utils.py:1372] Variable param_states/decoder/layers_18/self_attention/out/kernel/v_col                   size 3072         shape (3072,)                                  partition spec None
I0512 23:42:25.008003 140529434355712 utils.py:1372] Variable param_states/decoder/layers_18/self_attention/out/kernel/v_row                   size 2048         shape (2048,)                                  partition spec None
I0512 23:42:25.008043 140529434355712 utils.py:1372] Variable param_states/decoder/layers_18/self_attention/query/kernel/m                     size 1            shape (1,)                                     partition spec None
I0512 23:42:25.008092 140529434355712 utils.py:1372] Variable param_states/decoder/layers_18/self_attention/query/kernel/v                     size 1            shape (1,)                                     partition spec None
I0512 23:42:25.008136 140529434355712 utils.py:1372] Variable param_states/decoder/layers_18/self_attention/query/kernel/v_col                 size 3072         shape (3072,)                                  partition spec None
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
I0512 23:42:25.008176 140529434355712 utils.py:1372] Variable param_states/decoder/layers_18/self_attention/query/kernel/v_row                 size 2048         shape (2048,)                                  partition spec None
I0512 23:42:25.008215 140529434355712 utils.py:1372] Variable param_states/decoder/layers_18/self_attention/value/kernel/m                     size 1            shape (1,)                                     partition spec None
I0512 23:42:25.008254 140529434355712 utils.py:1372] Variable param_states/decoder/layers_18/self_attention/value/kernel/v                     size 1            shape (1,)                                     partition spec None
I0512 23:42:25.008294 140529434355712 utils.py:1372] Variable param_states/decoder/layers_18/self_attention/value/kernel/v_col                 size 3072         shape (3072,)                                  partition spec None
I0512 23:42:25.008333 140529434355712 utils.py:1372] Variable param_states/decoder/layers_18/self_attention/value/kernel/v_row                 size 2048         shape (2048,)                                  partition spec None
I0512 23:42:25.008372 140529434355712 utils.py:1372] Variable param_states/decoder/layers_19/mlp/wi_0/kernel/m                                 size 1            shape (1,)                                     partition spec None
I0512 23:42:25.008418 140529434355712 utils.py:1372] Variable param_states/decoder/layers_19/mlp/wi_0/kernel/v                                 size 1            shape (1,)                                     partition spec None
I0512 23:42:25.008458 140529434355712 utils.py:1372] Variable param_states/decoder/layers_19/mlp/wi_0/kernel/v_col                             size 16384        shape (16384,)                                 partition spec None
I0512 23:42:25.008498 140529434355712 utils.py:1372] Variable param_states/decoder/layers_19/mlp/wi_0/kernel/v_row                             size 2048         shape (2048,)                                  partition spec None
I0512 23:42:25.008536 140529434355712 utils.py:1372] Variable param_states/decoder/layers_19/mlp/wi_1/kernel/m                                 size 1            shape (1,)                                     partition spec None
I0512 23:42:25.008577 140529434355712 utils.py:1372] Variable param_states/decoder/layers_19/mlp/wi_1/kernel/v                                 size 1            shape (1,)                                     partition spec None
I0512 23:42:25.008617 140529434355712 utils.py:1372] Variable param_states/decoder/layers_19/mlp/wi_1/kernel/v_col                             size 8192         shape (8192,)                                  partition spec None
I0512 23:42:25.008655 140529434355712 utils.py:1372] Variable param_states/decoder/layers_19/mlp/wi_1/kernel/v_row                             size 2048         shape (2048,)                                  partition spec None
I0512 23:42:25.008694 140529434355712 utils.py:1372] Variable param_states/decoder/layers_19/mlp/wo/kernel/m                                   size 1            shape (1,)                                     partition spec None
I0512 23:42:25.008734 140529434355712 utils.py:1372] Variable param_states/decoder/layers_19/mlp/wo/kernel/v                                   size 1            shape (1,)                                     partition spec None
I0512 23:42:25.008774 140529434355712 utils.py:1372] Variable param_states/decoder/layers_19/mlp/wo/kernel/v_col                               size 8192         shape (8192,)                                  partition spec None
I0512 23:42:25.008812 140529434355712 utils.py:1372] Variable param_states/decoder/layers_19/mlp/wo/kernel/v_row                               size 2048         shape (2048,)                                  partition spec None
I0512 23:42:25.008853 140529434355712 utils.py:1372] Variable param_states/decoder/layers_19/pre_mlp_layer_norm/scale/m                        size 1            shape (1,)                                     partition spec None
I0512 23:42:25.008913 140529434355712 utils.py:1372] Variable param_states/decoder/layers_19/pre_mlp_layer_norm/scale/v                        size 2048         shape (embed=2048)                             partition spec ('model',)
I0512 23:42:25.008960 140529434355712 utils.py:1372] Variable param_states/decoder/layers_19/pre_mlp_layer_norm/scale/v_col                    size 1            shape (1,)                                     partition spec None
I0512 23:42:25.009087 140529434355712 utils.py:1372] Variable param_states/decoder/layers_19/pre_mlp_layer_norm/scale/v_row                    size 1            shape (1,)                                     partition spec None
I0512 23:42:25.009134 140529434355712 utils.py:1372] Variable param_states/decoder/layers_19/pre_self_attention_layer_norm/scale/m             size 1            shape (1,)                                     partition spec None
I0512 23:42:25.009179 140529434355712 utils.py:1372] Variable param_states/decoder/layers_19/pre_self_attention_layer_norm/scale/v             size 2048         shape (embed=2048)                             partition spec ('model',)
I0512 23:42:25.009222 140529434355712 utils.py:1372] Variable param_states/decoder/layers_19/pre_self_attention_layer_norm/scale/v_col         size 1            shape (1,)                                     partition spec None
I0512 23:42:25.009261 140529434355712 utils.py:1372] Variable param_states/decoder/layers_19/pre_self_attention_layer_norm/scale/v_row         size 1            shape (1,)                                     partition spec None
I0512 23:42:25.009299 140529434355712 utils.py:1372] Variable param_states/decoder/layers_19/self_attention/key/kernel/m                       size 1            shape (1,)                                     partition spec None
I0512 23:42:25.009340 140529434355712 utils.py:1372] Variable param_states/decoder/layers_19/self_attention/key/kernel/v                       size 1            shape (1,)                                     partition spec None
I0512 23:42:25.009380 140529434355712 utils.py:1372] Variable param_states/decoder/layers_19/self_attention/key/kernel/v_col                   size 3072         shape (3072,)                                  partition spec None
I0512 23:42:25.009419 140529434355712 utils.py:1372] Variable param_states/decoder/layers_19/self_attention/key/kernel/v_row                   size 2048         shape (2048,)                                  partition spec None
I0512 23:42:25.009459 140529434355712 utils.py:1372] Variable param_states/decoder/layers_19/self_attention/out/kernel/m                       size 1            shape (1,)                                     partition spec None
I0512 23:42:25.009498 140529434355712 utils.py:1372] Variable param_states/decoder/layers_19/self_attention/out/kernel/v                       size 1            shape (1,)                                     partition spec None
I0512 23:42:25.009537 140529434355712 utils.py:1372] Variable param_states/decoder/layers_19/self_attention/out/kernel/v_col                   size 3072         shape (3072,)                                  partition spec None
I0512 23:42:25.009615 140529434355712 utils.py:1372] Variable param_states/decoder/layers_19/self_attention/out/kernel/v_row                   size 2048         shape (2048,)                                  partition spec None
I0512 23:42:25.009656 140529434355712 utils.py:1372] Variable param_states/decoder/layers_19/self_attention/query/kernel/m                     size 1            shape (1,)                                     partition spec None
I0512 23:42:25.009697 140529434355712 utils.py:1372] Variable param_states/decoder/layers_19/self_attention/query/kernel/v                     size 1            shape (1,)                                     partition spec None
I0512 23:42:25.009738 140529434355712 utils.py:1372] Variable param_states/decoder/layers_19/self_attention/query/kernel/v_col                 size 3072         shape (3072,)                                  partition spec None
I0512 23:42:25.009777 140529434355712 utils.py:1372] Variable param_states/decoder/layers_19/self_attention/query/kernel/v_row                 size 2048         shape (2048,)                                  partition spec None
I0512 23:42:25.009816 140529434355712 utils.py:1372] Variable param_states/decoder/layers_19/self_attention/value/kernel/m                     size 1            shape (1,)                                     partition spec None
I0512 23:42:25.009859 140529434355712 utils.py:1372] Variable param_states/decoder/layers_19/self_attention/value/kernel/v                     size 1            shape (1,)                                     partition spec None
I0512 23:42:25.009900 140529434355712 utils.py:1372] Variable param_states/decoder/layers_19/self_attention/value/kernel/v_col                 size 3072         shape (3072,)                                  partition spec None
I0512 23:42:25.009939 140529434355712 utils.py:1372] Variable param_states/decoder/layers_19/self_attention/value/kernel/v_row                 size 2048         shape (2048,)                                  partition spec None
I0512 23:42:25.009977 140529434355712 utils.py:1372] Variable param_states/decoder/layers_2/mlp/wi_0/kernel/m                                  size 1            shape (1,)                                     partition spec None
I0512 23:42:25.010021 140529434355712 utils.py:1372] Variable param_states/decoder/layers_2/mlp/wi_0/kernel/v                                  size 1            shape (1,)                                     partition spec None
I0512 23:42:25.010062 140529434355712 utils.py:1372] Variable param_states/decoder/layers_2/mlp/wi_0/kernel/v_col                              size 16384        shape (16384,)                                 partition spec None
I0512 23:42:25.010109 140529434355712 utils.py:1372] Variable param_states/decoder/layers_2/mlp/wi_0/kernel/v_row                              size 2048         shape (2048,)                                  partition spec None
I0512 23:42:25.010149 140529434355712 utils.py:1372] Variable param_states/decoder/layers_2/mlp/wi_1/kernel/m                                  size 1            shape (1,)                                     partition spec None
I0512 23:42:25.010190 140529434355712 utils.py:1372] Variable param_states/decoder/layers_2/mlp/wi_1/kernel/v                                  size 1            shape (1,)                                     partition spec None
I0512 23:42:25.010230 140529434355712 utils.py:1372] Variable param_states/decoder/layers_2/mlp/wi_1/kernel/v_col                              size 8192         shape (8192,)                                  partition spec None
I0512 23:42:25.010268 140529434355712 utils.py:1372] Variable param_states/decoder/layers_2/mlp/wi_1/kernel/v_row                              size 2048         shape (2048,)                                  partition spec None
I0512 23:42:25.010307 140529434355712 utils.py:1372] Variable param_states/decoder/layers_2/mlp/wo/kernel/m                                    size 1            shape (1,)                                     partition spec None
I0512 23:42:25.010348 140529434355712 utils.py:1372] Variable param_states/decoder/layers_2/mlp/wo/kernel/v                                    size 1            shape (1,)                                     partition spec None
I0512 23:42:25.010387 140529434355712 utils.py:1372] Variable param_states/decoder/layers_2/mlp/wo/kernel/v_col                                size 8192         shape (8192,)                                  partition spec None
I0512 23:42:25.010426 140529434355712 utils.py:1372] Variable param_states/decoder/layers_2/mlp/wo/kernel/v_row                                size 2048         shape (2048,)                                  partition spec None
I0512 23:42:25.010464 140529434355712 utils.py:1372] Variable param_states/decoder/layers_2/pre_mlp_layer_norm/scale/m                         size 1            shape (1,)                                     partition spec None
I0512 23:42:25.010509 140529434355712 utils.py:1372] Variable param_states/decoder/layers_2/pre_mlp_layer_norm/scale/v                         size 2048         shape (embed=2048)                             partition spec ('model',)
I0512 23:42:25.010551 140529434355712 utils.py:1372] Variable param_states/decoder/layers_2/pre_mlp_layer_norm/scale/v_col                     size 1            shape (1,)                                     partition spec None
I0512 23:42:25.010590 140529434355712 utils.py:1372] Variable param_states/decoder/layers_2/pre_mlp_layer_norm/scale/v_row                     size 1            shape (1,)                                     partition spec None
I0512 23:42:25.010630 140529434355712 utils.py:1372] Variable param_states/decoder/layers_2/pre_self_attention_layer_norm/scale/m              size 1            shape (1,)                                     partition spec None
I0512 23:42:25.010673 140529434355712 utils.py:1372] Variable param_states/decoder/layers_2/pre_self_attention_layer_norm/scale/v              size 2048         shape (embed=2048)                             partition spec ('model',)
I0512 23:42:25.010715 140529434355712 utils.py:1372] Variable param_states/decoder/layers_2/pre_self_attention_layer_norm/scale/v_col          size 1            shape (1,)                                     partition spec None
I0512 23:42:25.010755 140529434355712 utils.py:1372] Variable param_states/decoder/layers_2/pre_self_attention_layer_norm/scale/v_row          size 1            shape (1,)                                     partition spec None
I0512 23:42:25.010795 140529434355712 utils.py:1372] Variable param_states/decoder/layers_2/self_attention/key/kernel/m                        size 1            shape (1,)                                     partition spec None
I0512 23:42:25.010837 140529434355712 utils.py:1372] Variable param_states/decoder/layers_2/self_attention/key/kernel/v                        size 1            shape (1,)                                     partition spec None
I0512 23:42:25.010880 140529434355712 utils.py:1372] Variable param_states/decoder/layers_2/self_attention/key/kernel/v_col                    size 3072         shape (3072,)                                  partition spec None
I0512 23:42:25.010919 140529434355712 utils.py:1372] Variable param_states/decoder/layers_2/self_attention/key/kernel/v_row                    size 2048         shape (2048,)                                  partition spec None
I0512 23:42:25.010958 140529434355712 utils.py:1372] Variable param_states/decoder/layers_2/self_attention/out/kernel/m                        size 1            shape (1,)                                     partition spec None
I0512 23:42:25.010999 140529434355712 utils.py:1372] Variable param_states/decoder/layers_2/self_attention/out/kernel/v                        size 1            shape (1,)                                     partition spec None
I0512 23:42:25.011057 140529434355712 utils.py:1372] Variable param_states/decoder/layers_2/self_attention/out/kernel/v_col                    size 3072         shape (3072,)                                  partition spec None
I0512 23:42:25.011109 140529434355712 utils.py:1372] Variable param_states/decoder/layers_2/self_attention/out/kernel/v_row                    size 2048         shape (2048,)                                  partition spec None
I0512 23:42:25.011150 140529434355712 utils.py:1372] Variable param_states/decoder/layers_2/self_attention/query/kernel/m                      size 1            shape (1,)                                     partition spec None
I0512 23:42:25.011191 140529434355712 utils.py:1372] Variable param_states/decoder/layers_2/self_attention/query/kernel/v                      size 1            shape (1,)                                     partition spec None
I0512 23:42:25.011231 140529434355712 utils.py:1372] Variable param_states/decoder/layers_2/self_attention/query/kernel/v_col                  size 3072         shape (3072,)                                  partition spec None
I0512 23:42:25.011270 140529434355712 utils.py:1372] Variable param_states/decoder/layers_2/self_attention/query/kernel/v_row                  size 2048         shape (2048,)                                  partition spec None
I0512 23:42:25.011309 140529434355712 utils.py:1372] Variable param_states/decoder/layers_2/self_attention/value/kernel/m                      size 1            shape (1,)                                     partition spec None
I0512 23:42:25.011350 140529434355712 utils.py:1372] Variable param_states/decoder/layers_2/self_attention/value/kernel/v                      size 1            shape (1,)                                     partition spec None
I0512 23:42:25.011390 140529434355712 utils.py:1372] Variable param_states/decoder/layers_2/self_attention/value/kernel/v_col                  size 3072         shape (3072,)                                  partition spec None
I0512 23:42:25.011429 140529434355712 utils.py:1372] Variable param_states/decoder/layers_2/self_attention/value/kernel/v_row                  size 2048         shape (2048,)                                  partition spec None
I0512 23:42:25.011468 140529434355712 utils.py:1372] Variable param_states/decoder/layers_20/mlp/wi_0/kernel/m                                 size 1            shape (1,)                                     partition spec None
I0512 23:42:25.011512 140529434355712 utils.py:1372] Variable param_states/decoder/layers_20/mlp/wi_0/kernel/v                                 size 1            shape (1,)                                     partition spec None
I0512 23:42:25.011553 140529434355712 utils.py:1372] Variable param_states/decoder/layers_20/mlp/wi_0/kernel/v_col                             size 16384        shape (16384,)                                 partition spec None
I0512 23:42:25.011593 140529434355712 utils.py:1372] Variable param_states/decoder/layers_20/mlp/wi_0/kernel/v_row                             size 2048         shape (2048,)                                  partition spec None
I0512 23:42:25.011632 140529434355712 utils.py:1372] Variable param_states/decoder/layers_20/mlp/wi_1/kernel/m                                 size 1            shape (1,)                                     partition spec None
I0512 23:42:25.011672 140529434355712 utils.py:1372] Variable param_states/decoder/layers_20/mlp/wi_1/kernel/v                                 size 1            shape (1,)                                     partition spec None
I0512 23:42:25.011713 140529434355712 utils.py:1372] Variable param_states/decoder/layers_20/mlp/wi_1/kernel/v_col                             size 8192         shape (8192,)                                  partition spec None
I0512 23:42:25.011751 140529434355712 utils.py:1372] Variable param_states/decoder/layers_20/mlp/wi_1/kernel/v_row                             size 2048         shape (2048,)                                  partition spec None
I0512 23:42:25.011790 140529434355712 utils.py:1372] Variable param_states/decoder/layers_20/mlp/wo/kernel/m                                   size 1            shape (1,)                                     partition spec None
I0512 23:42:25.011832 140529434355712 utils.py:1372] Variable param_states/decoder/layers_20/mlp/wo/kernel/v                                   size 1            shape (1,)                                     partition spec None
I0512 23:42:25.011875 140529434355712 utils.py:1372] Variable param_states/decoder/layers_20/mlp/wo/kernel/v_col                               size 8192         shape (8192,)                                  partition spec None
I0512 23:42:25.011915 140529434355712 utils.py:1372] Variable param_states/decoder/layers_20/mlp/wo/kernel/v_row                               size 2048         shape (2048,)                                  partition spec None
I0512 23:42:25.011954 140529434355712 utils.py:1372] Variable param_states/decoder/layers_20/pre_mlp_layer_norm/scale/m                        size 1            shape (1,)                                     partition spec None
I0512 23:42:25.011998 140529434355712 utils.py:1372] Variable param_states/decoder/layers_20/pre_mlp_layer_norm/scale/v                        size 2048         shape (embed=2048)                             partition spec ('model',)
I0512 23:42:25.012040 140529434355712 utils.py:1372] Variable param_states/decoder/layers_20/pre_mlp_layer_norm/scale/v_col                    size 1            shape (1,)                                     partition spec None
I0512 23:42:25.012086 140529434355712 utils.py:1372] Variable param_states/decoder/layers_20/pre_mlp_layer_norm/scale/v_row                    size 1            shape (1,)                                     partition spec None
I0512 23:42:25.012127 140529434355712 utils.py:1372] Variable param_states/decoder/layers_20/pre_self_attention_layer_norm/scale/m             size 1            shape (1,)                                     partition spec None
I0512 23:42:25.012170 140529434355712 utils.py:1372] Variable param_states/decoder/layers_20/pre_self_attention_layer_norm/scale/v             size 2048         shape (embed=2048)                             partition spec ('model',)
I0512 23:42:25.012211 140529434355712 utils.py:1372] Variable param_states/decoder/layers_20/pre_self_attention_layer_norm/scale/v_col         size 1            shape (1,)                                     partition spec None
I0512 23:42:25.012250 140529434355712 utils.py:1372] Variable param_states/decoder/layers_20/pre_self_attention_layer_norm/scale/v_row         size 1            shape (1,)                                     partition spec None
I0512 23:42:25.012289 140529434355712 utils.py:1372] Variable param_states/decoder/layers_20/self_attention/key/kernel/m                       size 1            shape (1,)                                     partition spec None
I0512 23:42:25.012329 140529434355712 utils.py:1372] Variable param_states/decoder/layers_20/self_attention/key/kernel/v                       size 1            shape (1,)                                     partition spec None
I0512 23:42:25.012369 140529434355712 utils.py:1372] Variable param_states/decoder/layers_20/self_attention/key/kernel/v_col                   size 3072         shape (3072,)                                  partition spec None
I0512 23:42:25.012409 140529434355712 utils.py:1372] Variable param_states/decoder/layers_20/self_attention/key/kernel/v_row                   size 2048         shape (2048,)                                  partition spec None
I0512 23:42:25.012448 140529434355712 utils.py:1372] Variable param_states/decoder/layers_20/self_attention/out/kernel/m                       size 1            shape (1,)                                     partition spec None
I0512 23:42:25.012489 140529434355712 utils.py:1372] Variable param_states/decoder/layers_20/self_attention/out/kernel/v                       size 1            shape (1,)                                     partition spec None
I0512 23:42:25.012529 140529434355712 utils.py:1372] Variable param_states/decoder/layers_20/self_attention/out/kernel/v_col                   size 3072         shape (3072,)                                  partition spec None
I0512 23:42:25.012567 140529434355712 utils.py:1372] Variable param_states/decoder/layers_20/self_attention/out/kernel/v_row                   size 2048         shape (2048,)                                  partition spec None
I0512 23:42:25.012606 140529434355712 utils.py:1372] Variable param_states/decoder/layers_20/self_attention/query/kernel/m                     size 1            shape (1,)                                     partition spec None
I0512 23:42:25.012646 140529434355712 utils.py:1372] Variable param_states/decoder/layers_20/self_attention/query/kernel/v                     size 1            shape (1,)                                     partition spec None
I0512 23:42:25.012686 140529434355712 utils.py:1372] Variable param_states/decoder/layers_20/self_attention/query/kernel/v_col                 size 3072         shape (3072,)                                  partition spec None
I0512 23:42:25.012725 140529434355712 utils.py:1372] Variable param_states/decoder/layers_20/self_attention/query/kernel/v_row                 size 2048         shape (2048,)                                  partition spec None
I0512 23:42:25.012764 140529434355712 utils.py:1372] Variable param_states/decoder/layers_20/self_attention/value/kernel/m                     size 1            shape (1,)                                     partition spec None
I0512 23:42:25.012806 140529434355712 utils.py:1372] Variable param_states/decoder/layers_20/self_attention/value/kernel/v                     size 1            shape (1,)                                     partition spec None
I0512 23:42:25.012849 140529434355712 utils.py:1372] Variable param_states/decoder/layers_20/self_attention/value/kernel/v_col                 size 3072         shape (3072,)                                  partition spec None
I0512 23:42:25.012890 140529434355712 utils.py:1372] Variable param_states/decoder/layers_20/self_attention/value/kernel/v_row                 size 2048         shape (2048,)                                  partition spec None
I0512 23:42:25.012928 140529434355712 utils.py:1372] Variable param_states/decoder/layers_21/mlp/wi_0/kernel/m                                 size 1            shape (1,)                                     partition spec None
I0512 23:42:25.012987 140529434355712 utils.py:1372] Variable param_states/decoder/layers_21/mlp/wi_0/kernel/v                                 size 1            shape (1,)                                     partition spec None
I0512 23:42:25.013031 140529434355712 utils.py:1372] Variable param_states/decoder/layers_21/mlp/wi_0/kernel/v_col                             size 16384        shape (16384,)                                 partition spec None
I0512 23:42:25.013088 140529434355712 utils.py:1372] Variable param_states/decoder/layers_21/mlp/wi_0/kernel/v_row                             size 2048         shape (2048,)                                  partition spec None
I0512 23:42:25.013135 140529434355712 utils.py:1372] Variable param_states/decoder/layers_21/mlp/wi_1/kernel/m                                 size 1            shape (1,)                                     partition spec None
I0512 23:42:25.013175 140529434355712 utils.py:1372] Variable param_states/decoder/layers_21/mlp/wi_1/kernel/v                                 size 1            shape (1,)                                     partition spec None
I0512 23:42:25.013213 140529434355712 utils.py:1372] Variable param_states/decoder/layers_21/mlp/wi_1/kernel/v_col                             size 8192         shape (8192,)                                  partition spec None
I0512 23:42:25.013251 140529434355712 utils.py:1372] Variable param_states/decoder/layers_21/mlp/wi_1/kernel/v_row                             size 2048         shape (2048,)                                  partition spec None
I0512 23:42:25.013289 140529434355712 utils.py:1372] Variable param_states/decoder/layers_21/mlp/wo/kernel/m                                   size 1            shape (1,)                                     partition spec None
I0512 23:42:25.013328 140529434355712 utils.py:1372] Variable param_states/decoder/layers_21/mlp/wo/kernel/v                                   size 1            shape (1,)                                     partition spec None
I0512 23:42:25.013367 140529434355712 utils.py:1372] Variable param_states/decoder/layers_21/mlp/wo/kernel/v_col                               size 8192         shape (8192,)                                  partition spec None
I0512 23:42:25.013406 140529434355712 utils.py:1372] Variable param_states/decoder/layers_21/mlp/wo/kernel/v_row                               size 2048         shape (2048,)                                  partition spec None
I0512 23:42:25.013444 140529434355712 utils.py:1372] Variable param_states/decoder/layers_21/pre_mlp_layer_norm/scale/m                        size 1            shape (1,)                                     partition spec None
I0512 23:42:25.013488 140529434355712 utils.py:1372] Variable param_states/decoder/layers_21/pre_mlp_layer_norm/scale/v                        size 2048         shape (embed=2048)                             partition spec ('model',)
I0512 23:42:25.013530 140529434355712 utils.py:1372] Variable param_states/decoder/layers_21/pre_mlp_layer_norm/scale/v_col                    size 1            shape (1,)                                     partition spec None
I0512 23:42:25.013598 140529434355712 utils.py:1372] Variable param_states/decoder/layers_21/pre_mlp_layer_norm/scale/v_row                    size 1            shape (1,)                                     partition spec None
I0512 23:42:25.013639 140529434355712 utils.py:1372] Variable param_states/decoder/layers_21/pre_self_attention_layer_norm/scale/m             size 1            shape (1,)                                     partition spec None
I0512 23:42:25.013682 140529434355712 utils.py:1372] Variable param_states/decoder/layers_21/pre_self_attention_layer_norm/scale/v             size 2048         shape (embed=2048)                             partition spec ('model',)
I0512 23:42:25.013724 140529434355712 utils.py:1372] Variable param_states/decoder/layers_21/pre_self_attention_layer_norm/scale/v_col         size 1            shape (1,)                                     partition spec None
I0512 23:42:25.013764 140529434355712 utils.py:1372] Variable param_states/decoder/layers_21/pre_self_attention_layer_norm/scale/v_row         size 1            shape (1,)                                     partition spec None
I0512 23:42:25.013802 140529434355712 utils.py:1372] Variable param_states/decoder/layers_21/self_attention/key/kernel/m                       size 1            shape (1,)                                     partition spec None
I0512 23:42:25.013844 140529434355712 utils.py:1372] Variable param_states/decoder/layers_21/self_attention/key/kernel/v                       size 1            shape (1,)                                     partition spec None
I0512 23:42:25.013885 140529434355712 utils.py:1372] Variable param_states/decoder/layers_21/self_attention/key/kernel/v_col                   size 3072         shape (3072,)                                  partition spec None
I0512 23:42:25.013925 140529434355712 utils.py:1372] Variable param_states/decoder/layers_21/self_attention/key/kernel/v_row                   size 2048         shape (2048,)                                  partition spec None
I0512 23:42:25.013963 140529434355712 utils.py:1372] Variable param_states/decoder/layers_21/self_attention/out/kernel/m                       size 1            shape (1,)                                     partition spec None
I0512 23:42:25.014003 140529434355712 utils.py:1372] Variable param_states/decoder/layers_21/self_attention/out/kernel/v                       size 1            shape (1,)                                     partition spec None
I0512 23:42:25.014043 140529434355712 utils.py:1372] Variable param_states/decoder/layers_21/self_attention/out/kernel/v_col                   size 3072         shape (3072,)                                  partition spec None
I0512 23:42:25.014091 140529434355712 utils.py:1372] Variable param_states/decoder/layers_21/self_attention/out/kernel/v_row                   size 2048         shape (2048,)                                  partition spec None
I0512 23:42:25.014131 140529434355712 utils.py:1372] Variable param_states/decoder/layers_21/self_attention/query/kernel/m                     size 1            shape (1,)                                     partition spec None
I0512 23:42:25.014171 140529434355712 utils.py:1372] Variable param_states/decoder/layers_21/self_attention/query/kernel/v                     size 1            shape (1,)                                     partition spec None
I0512 23:42:25.014211 140529434355712 utils.py:1372] Variable param_states/decoder/layers_21/self_attention/query/kernel/v_col                 size 3072         shape (3072,)                                  partition spec None
I0512 23:42:25.014249 140529434355712 utils.py:1372] Variable param_states/decoder/layers_21/self_attention/query/kernel/v_row                 size 2048         shape (2048,)                                  partition spec None
I0512 23:42:25.014286 140529434355712 utils.py:1372] Variable param_states/decoder/layers_21/self_attention/value/kernel/m                     size 1            shape (1,)                                     partition spec None
I0512 23:42:25.014326 140529434355712 utils.py:1372] Variable param_states/decoder/layers_21/self_attention/value/kernel/v                     size 1            shape (1,)                                     partition spec None
I0512 23:42:25.014365 140529434355712 utils.py:1372] Variable param_states/decoder/layers_21/self_attention/value/kernel/v_col                 size 3072         shape (3072,)                                  partition spec None
I0512 23:42:25.014403 140529434355712 utils.py:1372] Variable param_states/decoder/layers_21/self_attention/value/kernel/v_row                 size 2048         shape (2048,)                                  partition spec None
I0512 23:42:25.014441 140529434355712 utils.py:1372] Variable param_states/decoder/layers_22/mlp/wi_0/kernel/m                                 size 1            shape (1,)                                     partition spec None
I0512 23:42:25.014483 140529434355712 utils.py:1372] Variable param_states/decoder/layers_22/mlp/wi_0/kernel/v                                 size 1            shape (1,)                                     partition spec None
I0512 23:42:25.014523 140529434355712 utils.py:1372] Variable param_states/decoder/layers_22/mlp/wi_0/kernel/v_col                             size 16384        shape (16384,)                                 partition spec None
I0512 23:42:25.014562 140529434355712 utils.py:1372] Variable param_states/decoder/layers_22/mlp/wi_0/kernel/v_row                             size 2048         shape (2048,)                                  partition spec None
I0512 23:42:25.014600 140529434355712 utils.py:1372] Variable param_states/decoder/layers_22/mlp/wi_1/kernel/m                                 size 1            shape (1,)                                     partition spec None
I0512 23:42:25.014639 140529434355712 utils.py:1372] Variable param_states/decoder/layers_22/mlp/wi_1/kernel/v                                 size 1            shape (1,)                                     partition spec None
I0512 23:42:25.014678 140529434355712 utils.py:1372] Variable param_states/decoder/layers_22/mlp/wi_1/kernel/v_col                             size 8192         shape (8192,)                                  partition spec None
I0512 23:42:25.014716 140529434355712 utils.py:1372] Variable param_states/decoder/layers_22/mlp/wi_1/kernel/v_row                             size 2048         shape (2048,)                                  partition spec None
I0512 23:42:25.014755 140529434355712 utils.py:1372] Variable param_states/decoder/layers_22/mlp/wo/kernel/m                                   size 1            shape (1,)                                     partition spec None
I0512 23:42:25.014794 140529434355712 utils.py:1372] Variable param_states/decoder/layers_22/mlp/wo/kernel/v                                   size 1            shape (1,)                                     partition spec None
I0512 23:42:25.014834 140529434355712 utils.py:1372] Variable param_states/decoder/layers_22/mlp/wo/kernel/v_col                               size 8192         shape (8192,)                                  partition spec None
I0512 23:42:25.014874 140529434355712 utils.py:1372] Variable param_states/decoder/layers_22/mlp/wo/kernel/v_row                               size 2048         shape (2048,)                                  partition spec None
I0512 23:42:25.014932 140529434355712 utils.py:1372] Variable param_states/decoder/layers_22/pre_mlp_layer_norm/scale/m                        size 1            shape (1,)                                     partition spec None
I0512 23:42:25.014981 140529434355712 utils.py:1372] Variable param_states/decoder/layers_22/pre_mlp_layer_norm/scale/v                        size 2048         shape (embed=2048)                             partition spec ('model',)
I0512 23:42:25.015023 140529434355712 utils.py:1372] Variable param_states/decoder/layers_22/pre_mlp_layer_norm/scale/v_col                    size 1            shape (1,)                                     partition spec None
I0512 23:42:25.015062 140529434355712 utils.py:1372] Variable param_states/decoder/layers_22/pre_mlp_layer_norm/scale/v_row                    size 1            shape (1,)                                     partition spec None
I0512 23:42:25.015113 140529434355712 utils.py:1372] Variable param_states/decoder/layers_22/pre_self_attention_layer_norm/scale/m             size 1            shape (1,)                                     partition spec None
I0512 23:42:25.015159 140529434355712 utils.py:1372] Variable param_states/decoder/layers_22/pre_self_attention_layer_norm/scale/v             size 2048         shape (embed=2048)                             partition spec ('model',)
I0512 23:42:25.015200 140529434355712 utils.py:1372] Variable param_states/decoder/layers_22/pre_self_attention_layer_norm/scale/v_col         size 1            shape (1,)                                     partition spec None
I0512 23:42:25.015239 140529434355712 utils.py:1372] Variable param_states/decoder/layers_22/pre_self_attention_layer_norm/scale/v_row         size 1            shape (1,)                                     partition spec None
I0512 23:42:25.015277 140529434355712 utils.py:1372] Variable param_states/decoder/layers_22/self_attention/key/kernel/m                       size 1            shape (1,)                                     partition spec None
I0512 23:42:25.015318 140529434355712 utils.py:1372] Variable param_states/decoder/layers_22/self_attention/key/kernel/v                       size 1            shape (1,)                                     partition spec None
I0512 23:42:25.015358 140529434355712 utils.py:1372] Variable param_states/decoder/layers_22/self_attention/key/kernel/v_col                   size 3072         shape (3072,)                                  partition spec None
I0512 23:42:25.015397 140529434355712 utils.py:1372] Variable param_states/decoder/layers_22/self_attention/key/kernel/v_row                   size 2048         shape (2048,)                                  partition spec None
I0512 23:42:25.015437 140529434355712 utils.py:1372] Variable param_states/decoder/layers_22/self_attention/out/kernel/m                       size 1            shape (1,)                                     partition spec None
I0512 23:42:25.015477 140529434355712 utils.py:1372] Variable param_states/decoder/layers_22/self_attention/out/kernel/v                       size 1            shape (1,)                                     partition spec None
I0512 23:42:25.015517 140529434355712 utils.py:1372] Variable param_states/decoder/layers_22/self_attention/out/kernel/v_col                   size 3072         shape (3072,)                                  partition spec None
I0512 23:42:25.015556 140529434355712 utils.py:1372] Variable param_states/decoder/layers_22/self_attention/out/kernel/v_row                   size 2048         shape (2048,)                                  partition spec None
I0512 23:42:25.015595 140529434355712 utils.py:1372] Variable param_states/decoder/layers_22/self_attention/query/kernel/m                     size 1            shape (1,)                                     partition spec None
I0512 23:42:25.015635 140529434355712 utils.py:1372] Variable param_states/decoder/layers_22/self_attention/query/kernel/v                     size 1            shape (1,)                                     partition spec None
I0512 23:42:25.015677 140529434355712 utils.py:1372] Variable param_states/decoder/layers_22/self_attention/query/kernel/v_col                 size 3072         shape (3072,)                                  partition spec None
I0512 23:42:25.015716 140529434355712 utils.py:1372] Variable param_states/decoder/layers_22/self_attention/query/kernel/v_row                 size 2048         shape (2048,)                                  partition spec None
I0512 23:42:25.015754 140529434355712 utils.py:1372] Variable param_states/decoder/layers_22/self_attention/value/kernel/m                     size 1            shape (1,)                                     partition spec None
I0512 23:42:25.015794 140529434355712 utils.py:1372] Variable param_states/decoder/layers_22/self_attention/value/kernel/v                     size 1            shape (1,)                                     partition spec None
I0512 23:42:25.015836 140529434355712 utils.py:1372] Variable param_states/decoder/layers_22/self_attention/value/kernel/v_col                 size 3072         shape (3072,)                                  partition spec None
I0512 23:42:25.015878 140529434355712 utils.py:1372] Variable param_states/decoder/layers_22/self_attention/value/kernel/v_row                 size 2048         shape (2048,)                                  partition spec None
I0512 23:42:25.015918 140529434355712 utils.py:1372] Variable param_states/decoder/layers_23/extra_mlp/wi_0/kernel/m                           size 1            shape (1,)                                     partition spec None
I0512 23:42:25.015961 140529434355712 utils.py:1372] Variable param_states/decoder/layers_23/extra_mlp/wi_0/kernel/v                           size 1            shape (1,)                                     partition spec None
I0512 23:42:25.016048 140529434355712 utils.py:1372] Variable param_states/decoder/layers_23/extra_mlp/wi_0/kernel/v_col                       size 16384        shape (16384,)                                 partition spec None
I0512 23:42:25.016095 140529434355712 utils.py:1372] Variable param_states/decoder/layers_23/extra_mlp/wi_0/kernel/v_row                       size 2048         shape (2048,)                                  partition spec None
I0512 23:42:25.016136 140529434355712 utils.py:1372] Variable param_states/decoder/layers_23/extra_mlp/wi_1/kernel/m                           size 1            shape (1,)                                     partition spec None
I0512 23:42:25.016177 140529434355712 utils.py:1372] Variable param_states/decoder/layers_23/extra_mlp/wi_1/kernel/v                           size 1            shape (1,)                                     partition spec None
I0512 23:42:25.016217 140529434355712 utils.py:1372] Variable param_states/decoder/layers_23/extra_mlp/wi_1/kernel/v_col                       size 8192         shape (8192,)                                  partition spec None
I0512 23:42:25.016257 140529434355712 utils.py:1372] Variable param_states/decoder/layers_23/extra_mlp/wi_1/kernel/v_row                       size 2048         shape (2048,)                                  partition spec None
I0512 23:42:25.016295 140529434355712 utils.py:1372] Variable param_states/decoder/layers_23/extra_mlp/wo/kernel/m                             size 1            shape (1,)                                     partition spec None
I0512 23:42:25.016336 140529434355712 utils.py:1372] Variable param_states/decoder/layers_23/extra_mlp/wo/kernel/v                             size 1            shape (1,)                                     partition spec None
I0512 23:42:25.016376 140529434355712 utils.py:1372] Variable param_states/decoder/layers_23/extra_mlp/wo/kernel/v_col                         size 8192         shape (8192,)                                  partition spec None
I0512 23:42:25.016414 140529434355712 utils.py:1372] Variable param_states/decoder/layers_23/extra_mlp/wo/kernel/v_row                         size 2048         shape (2048,)                                  partition spec None
I0512 23:42:25.016453 140529434355712 utils.py:1372] Variable param_states/decoder/layers_23/mlp/expert/wi_0/kernel/m                          size 1            shape (1,)                                     partition spec None
I0512 23:42:25.016494 140529434355712 utils.py:1372] Variable param_states/decoder/layers_23/mlp/expert/wi_0/kernel/v                          size 1            shape (1,)                                     partition spec None
I0512 23:42:25.016535 140529434355712 utils.py:1372] Variable param_states/decoder/layers_23/mlp/expert/wi_0/kernel/v_col                      size 131072       shape (8, 16384)                               partition spec ('expert',)
I0512 23:42:25.016576 140529434355712 utils.py:1372] Variable param_states/decoder/layers_23/mlp/expert/wi_0/kernel/v_row                      size 16384        shape (8, 2048)                                partition spec ('expert',)
I0512 23:42:25.016616 140529434355712 utils.py:1372] Variable param_states/decoder/layers_23/mlp/expert/wi_1/kernel/m                          size 1            shape (1,)                                     partition spec None
I0512 23:42:25.016657 140529434355712 utils.py:1372] Variable param_states/decoder/layers_23/mlp/expert/wi_1/kernel/v                          size 1            shape (1,)                                     partition spec None
I0512 23:42:25.016696 140529434355712 utils.py:1372] Variable param_states/decoder/layers_23/mlp/expert/wi_1/kernel/v_col                      size 65536        shape (8, 8192)                                partition spec ('expert',)
I0512 23:42:25.016736 140529434355712 utils.py:1372] Variable param_states/decoder/layers_23/mlp/expert/wi_1/kernel/v_row                      size 16384        shape (8, 2048)                                partition spec ('expert',)
I0512 23:42:25.016776 140529434355712 utils.py:1372] Variable param_states/decoder/layers_23/mlp/expert/wo/kernel/m                            size 1            shape (1,)                                     partition spec None
I0512 23:42:25.016815 140529434355712 utils.py:1372] Variable param_states/decoder/layers_23/mlp/expert/wo/kernel/v                            size 1            shape (1,)                                     partition spec None
I0512 23:42:25.016858 140529434355712 utils.py:1372] Variable param_states/decoder/layers_23/mlp/expert/wo/kernel/v_col                        size 65536        shape (8, 8192)                                partition spec ('expert',)
I0512 23:42:25.016917 140529434355712 utils.py:1372] Variable param_states/decoder/layers_23/mlp/expert/wo/kernel/v_row                        size 16384        shape (8, 2048)                                partition spec ('expert',)
I0512 23:42:25.016961 140529434355712 utils.py:1372] Variable param_states/decoder/layers_23/mlp/router/router_weights/w/kernel/m              size 1            shape (1,)                                     partition spec None
I0512 23:42:25.017009 140529434355712 utils.py:1372] Variable param_states/decoder/layers_23/mlp/router/router_weights/w/kernel/v              size 16384        shape (embed=2048, unmodeled=8)                partition spec ('model', None)
I0512 23:42:25.017052 140529434355712 utils.py:1372] Variable param_states/decoder/layers_23/mlp/router/router_weights/w/kernel/v_col          size 1            shape (1,)                                     partition spec None
I0512 23:42:25.017099 140529434355712 utils.py:1372] Variable param_states/decoder/layers_23/mlp/router/router_weights/w/kernel/v_row          size 1            shape (1,)                                     partition spec None
I0512 23:42:25.017139 140529434355712 utils.py:1372] Variable param_states/decoder/layers_23/pre_extra_mlp_layer_norm/scale/m                  size 1            shape (1,)                                     partition spec None
I0512 23:42:25.017185 140529434355712 utils.py:1372] Variable param_states/decoder/layers_23/pre_extra_mlp_layer_norm/scale/v                  size 2048         shape (embed=2048)                             partition spec ('model',)
I0512 23:42:25.017228 140529434355712 utils.py:1372] Variable param_states/decoder/layers_23/pre_extra_mlp_layer_norm/scale/v_col              size 1            shape (1,)                                     partition spec None
I0512 23:42:25.017267 140529434355712 utils.py:1372] Variable param_states/decoder/layers_23/pre_extra_mlp_layer_norm/scale/v_row              size 1            shape (1,)                                     partition spec None
I0512 23:42:25.017306 140529434355712 utils.py:1372] Variable param_states/decoder/layers_23/pre_mlp_layer_norm/scale/m                        size 1            shape (1,)                                     partition spec None
I0512 23:42:25.017349 140529434355712 utils.py:1372] Variable param_states/decoder/layers_23/pre_mlp_layer_norm/scale/v                        size 2048         shape (embed=2048)                             partition spec ('model',)
I0512 23:42:25.017390 140529434355712 utils.py:1372] Variable param_states/decoder/layers_23/pre_mlp_layer_norm/scale/v_col                    size 1            shape (1,)                                     partition spec None
I0512 23:42:25.017429 140529434355712 utils.py:1372] Variable param_states/decoder/layers_23/pre_mlp_layer_norm/scale/v_row                    size 1            shape (1,)                                     partition spec None
I0512 23:42:25.017468 140529434355712 utils.py:1372] Variable param_states/decoder/layers_23/pre_self_attention_layer_norm/scale/m             size 1            shape (1,)                                     partition spec None
I0512 23:42:25.017510 140529434355712 utils.py:1372] Variable param_states/decoder/layers_23/pre_self_attention_layer_norm/scale/v             size 2048         shape (embed=2048)                             partition spec ('model',)
I0512 23:42:25.017577 140529434355712 utils.py:1372] Variable param_states/decoder/layers_23/pre_self_attention_layer_norm/scale/v_col         size 1            shape (1,)                                     partition spec None
I0512 23:42:25.017623 140529434355712 utils.py:1372] Variable param_states/decoder/layers_23/pre_self_attention_layer_norm/scale/v_row         size 1            shape (1,)                                     partition spec None
I0512 23:42:25.017663 140529434355712 utils.py:1372] Variable param_states/decoder/layers_23/self_attention/key/kernel/m                       size 1            shape (1,)                                     partition spec None
I0512 23:42:25.017704 140529434355712 utils.py:1372] Variable param_states/decoder/layers_23/self_attention/key/kernel/v                       size 1            shape (1,)                                     partition spec None
I0512 23:42:25.017744 140529434355712 utils.py:1372] Variable param_states/decoder/layers_23/self_attention/key/kernel/v_col                   size 3072         shape (3072,)                                  partition spec None
I0512 23:42:25.017784 140529434355712 utils.py:1372] Variable param_states/decoder/layers_23/self_attention/key/kernel/v_row                   size 2048         shape (2048,)                                  partition spec None
I0512 23:42:25.017823 140529434355712 utils.py:1372] Variable param_states/decoder/layers_23/self_attention/out/kernel/m                       size 1            shape (1,)                                     partition spec None
I0512 23:42:25.017866 140529434355712 utils.py:1372] Variable param_states/decoder/layers_23/self_attention/out/kernel/v                       size 1            shape (1,)                                     partition spec None
I0512 23:42:25.017907 140529434355712 utils.py:1372] Variable param_states/decoder/layers_23/self_attention/out/kernel/v_col                   size 3072         shape (3072,)                                  partition spec None
I0512 23:42:25.017946 140529434355712 utils.py:1372] Variable param_states/decoder/layers_23/self_attention/out/kernel/v_row                   size 2048         shape (2048,)                                  partition spec None
I0512 23:42:25.017985 140529434355712 utils.py:1372] Variable param_states/decoder/layers_23/self_attention/query/kernel/m                     size 1            shape (1,)                                     partition spec None
I0512 23:42:25.018025 140529434355712 utils.py:1372] Variable param_states/decoder/layers_23/self_attention/query/kernel/v                     size 1            shape (1,)                                     partition spec None
I0512 23:42:25.018066 140529434355712 utils.py:1372] Variable param_states/decoder/layers_23/self_attention/query/kernel/v_col                 size 3072         shape (3072,)                                  partition spec None
I0512 23:42:25.018116 140529434355712 utils.py:1372] Variable param_states/decoder/layers_23/self_attention/query/kernel/v_row                 size 2048         shape (2048,)                                  partition spec None
I0512 23:42:25.018156 140529434355712 utils.py:1372] Variable param_states/decoder/layers_23/self_attention/value/kernel/m                     size 1            shape (1,)                                     partition spec None
I0512 23:42:25.018196 140529434355712 utils.py:1372] Variable param_states/decoder/layers_23/self_attention/value/kernel/v                     size 1            shape (1,)                                     partition spec None
I0512 23:42:25.018236 140529434355712 utils.py:1372] Variable param_states/decoder/layers_23/self_attention/value/kernel/v_col                 size 3072         shape (3072,)                                  partition spec None
I0512 23:42:25.018276 140529434355712 utils.py:1372] Variable param_states/decoder/layers_23/self_attention/value/kernel/v_row                 size 2048         shape (2048,)                                  partition spec None
I0512 23:42:25.018315 140529434355712 utils.py:1372] Variable param_states/decoder/layers_3/mlp/wi_0/kernel/m                                  size 1            shape (1,)                                     partition spec None
I0512 23:42:25.018360 140529434355712 utils.py:1372] Variable param_states/decoder/layers_3/mlp/wi_0/kernel/v                                  size 1            shape (1,)                                     partition spec None
I0512 23:42:25.018401 140529434355712 utils.py:1372] Variable param_states/decoder/layers_3/mlp/wi_0/kernel/v_col                              size 16384        shape (16384,)                                 partition spec None
I0512 23:42:25.018440 140529434355712 utils.py:1372] Variable param_states/decoder/layers_3/mlp/wi_0/kernel/v_row                              size 2048         shape (2048,)                                  partition spec None
I0512 23:42:25.018479 140529434355712 utils.py:1372] Variable param_states/decoder/layers_3/mlp/wi_1/kernel/m                                  size 1            shape (1,)                                     partition spec None
I0512 23:42:25.018519 140529434355712 utils.py:1372] Variable param_states/decoder/layers_3/mlp/wi_1/kernel/v                                  size 1            shape (1,)                                     partition spec None
I0512 23:42:25.018558 140529434355712 utils.py:1372] Variable param_states/decoder/layers_3/mlp/wi_1/kernel/v_col                              size 8192         shape (8192,)                                  partition spec None
I0512 23:42:25.018597 140529434355712 utils.py:1372] Variable param_states/decoder/layers_3/mlp/wi_1/kernel/v_row                              size 2048         shape (2048,)                                  partition spec None
I0512 23:42:25.018635 140529434355712 utils.py:1372] Variable param_states/decoder/layers_3/mlp/wo/kernel/m                                    size 1            shape (1,)                                     partition spec None
I0512 23:42:25.018676 140529434355712 utils.py:1372] Variable param_states/decoder/layers_3/mlp/wo/kernel/v                                    size 1            shape (1,)                                     partition spec None
I0512 23:42:25.018716 140529434355712 utils.py:1372] Variable param_states/decoder/layers_3/mlp/wo/kernel/v_col                                size 8192         shape (8192,)                                  partition spec None
I0512 23:42:25.018754 140529434355712 utils.py:1372] Variable param_states/decoder/layers_3/mlp/wo/kernel/v_row                                size 2048         shape (2048,)                                  partition spec None
I0512 23:42:25.018794 140529434355712 utils.py:1372] Variable param_states/decoder/layers_3/pre_mlp_layer_norm/scale/m                         size 1            shape (1,)                                     partition spec None
I0512 23:42:25.018840 140529434355712 utils.py:1372] Variable param_states/decoder/layers_3/pre_mlp_layer_norm/scale/v                         size 2048         shape (embed=2048)                             partition spec ('model',)
I0512 23:42:25.018906 140529434355712 utils.py:1372] Variable param_states/decoder/layers_3/pre_mlp_layer_norm/scale/v_col                     size 1            shape (1,)                                     partition spec None
I0512 23:42:25.018949 140529434355712 utils.py:1372] Variable param_states/decoder/layers_3/pre_mlp_layer_norm/scale/v_row                     size 1            shape (1,)                                     partition spec None
I0512 23:42:25.018989 140529434355712 utils.py:1372] Variable param_states/decoder/layers_3/pre_self_attention_layer_norm/scale/m              size 1            shape (1,)                                     partition spec None
I0512 23:42:25.019032 140529434355712 utils.py:1372] Variable param_states/decoder/layers_3/pre_self_attention_layer_norm/scale/v              size 2048         shape (embed=2048)                             partition spec ('model',)
I0512 23:42:25.019082 140529434355712 utils.py:1372] Variable param_states/decoder/layers_3/pre_self_attention_layer_norm/scale/v_col          size 1            shape (1,)                                     partition spec None
I0512 23:42:25.019124 140529434355712 utils.py:1372] Variable param_states/decoder/layers_3/pre_self_attention_layer_norm/scale/v_row          size 1            shape (1,)                                     partition spec None
I0512 23:42:25.019162 140529434355712 utils.py:1372] Variable param_states/decoder/layers_3/self_attention/key/kernel/m                        size 1            shape (1,)                                     partition spec None
I0512 23:42:25.019203 140529434355712 utils.py:1372] Variable param_states/decoder/layers_3/self_attention/key/kernel/v                        size 1            shape (1,)                                     partition spec None
I0512 23:42:25.019243 140529434355712 utils.py:1372] Variable param_states/decoder/layers_3/self_attention/key/kernel/v_col                    size 3072         shape (3072,)                                  partition spec None
I0512 23:42:25.019281 140529434355712 utils.py:1372] Variable param_states/decoder/layers_3/self_attention/key/kernel/v_row                    size 2048         shape (2048,)                                  partition spec None
I0512 23:42:25.019320 140529434355712 utils.py:1372] Variable param_states/decoder/layers_3/self_attention/out/kernel/m                        size 1            shape (1,)                                     partition spec None
I0512 23:42:25.019360 140529434355712 utils.py:1372] Variable param_states/decoder/layers_3/self_attention/out/kernel/v                        size 1            shape (1,)                                     partition spec None
I0512 23:42:25.019401 140529434355712 utils.py:1372] Variable param_states/decoder/layers_3/self_attention/out/kernel/v_col                    size 3072         shape (3072,)                                  partition spec None
I0512 23:42:25.019440 140529434355712 utils.py:1372] Variable param_states/decoder/layers_3/self_attention/out/kernel/v_row                    size 2048         shape (2048,)                                  partition spec None
I0512 23:42:25.019479 140529434355712 utils.py:1372] Variable param_states/decoder/layers_3/self_attention/query/kernel/m                      size 1            shape (1,)                                     partition spec None
I0512 23:42:25.019520 140529434355712 utils.py:1372] Variable param_states/decoder/layers_3/self_attention/query/kernel/v                      size 1            shape (1,)                                     partition spec None
I0512 23:42:25.019559 140529434355712 utils.py:1372] Variable param_states/decoder/layers_3/self_attention/query/kernel/v_col                  size 3072         shape (3072,)                                  partition spec None
I0512 23:42:25.019598 140529434355712 utils.py:1372] Variable param_states/decoder/layers_3/self_attention/query/kernel/v_row                  size 2048         shape (2048,)                                  partition spec None
I0512 23:42:25.019637 140529434355712 utils.py:1372] Variable param_states/decoder/layers_3/self_attention/value/kernel/m                      size 1            shape (1,)                                     partition spec None
I0512 23:42:25.019678 140529434355712 utils.py:1372] Variable param_states/decoder/layers_3/self_attention/value/kernel/v                      size 1            shape (1,)                                     partition spec None
I0512 23:42:25.019718 140529434355712 utils.py:1372] Variable param_states/decoder/layers_3/self_attention/value/kernel/v_col                  size 3072         shape (3072,)                                  partition spec None
I0512 23:42:25.019757 140529434355712 utils.py:1372] Variable param_states/decoder/layers_3/self_attention/value/kernel/v_row                  size 2048         shape (2048,)                                  partition spec None
I0512 23:42:25.019796 140529434355712 utils.py:1372] Variable param_states/decoder/layers_4/mlp/wi_0/kernel/m                                  size 1            shape (1,)                                     partition spec None
I0512 23:42:25.019842 140529434355712 utils.py:1372] Variable param_states/decoder/layers_4/mlp/wi_0/kernel/v                                  size 1            shape (1,)                                     partition spec None
I0512 23:42:25.019886 140529434355712 utils.py:1372] Variable param_states/decoder/layers_4/mlp/wi_0/kernel/v_col                              size 16384        shape (16384,)                                 partition spec None
I0512 23:42:25.019925 140529434355712 utils.py:1372] Variable param_states/decoder/layers_4/mlp/wi_0/kernel/v_row                              size 2048         shape (2048,)                                  partition spec None
I0512 23:42:25.019964 140529434355712 utils.py:1372] Variable param_states/decoder/layers_4/mlp/wi_1/kernel/m                                  size 1            shape (1,)                                     partition spec None
I0512 23:42:25.020004 140529434355712 utils.py:1372] Variable param_states/decoder/layers_4/mlp/wi_1/kernel/v                                  size 1            shape (1,)                                     partition spec None
I0512 23:42:25.020044 140529434355712 utils.py:1372] Variable param_states/decoder/layers_4/mlp/wi_1/kernel/v_col                              size 8192         shape (8192,)                                  partition spec None
I0512 23:42:25.020089 140529434355712 utils.py:1372] Variable param_states/decoder/layers_4/mlp/wi_1/kernel/v_row                              size 2048         shape (2048,)                                  partition spec None
I0512 23:42:25.020130 140529434355712 utils.py:1372] Variable param_states/decoder/layers_4/mlp/wo/kernel/m                                    size 1            shape (1,)                                     partition spec None
I0512 23:42:25.020170 140529434355712 utils.py:1372] Variable param_states/decoder/layers_4/mlp/wo/kernel/v                                    size 1            shape (1,)                                     partition spec None
I0512 23:42:25.020210 140529434355712 utils.py:1372] Variable param_states/decoder/layers_4/mlp/wo/kernel/v_col                                size 8192         shape (8192,)                                  partition spec None
I0512 23:42:25.020250 140529434355712 utils.py:1372] Variable param_states/decoder/layers_4/mlp/wo/kernel/v_row                                size 2048         shape (2048,)                                  partition spec None
I0512 23:42:25.020288 140529434355712 utils.py:1372] Variable param_states/decoder/layers_4/pre_mlp_layer_norm/scale/m                         size 1            shape (1,)                                     partition spec None
I0512 23:42:25.020332 140529434355712 utils.py:1372] Variable param_states/decoder/layers_4/pre_mlp_layer_norm/scale/v                         size 2048         shape (embed=2048)                             partition spec ('model',)
I0512 23:42:25.020375 140529434355712 utils.py:1372] Variable param_states/decoder/layers_4/pre_mlp_layer_norm/scale/v_col                     size 1            shape (1,)                                     partition spec None
I0512 23:42:25.020414 140529434355712 utils.py:1372] Variable param_states/decoder/layers_4/pre_mlp_layer_norm/scale/v_row                     size 1            shape (1,)                                     partition spec None
I0512 23:42:25.020453 140529434355712 utils.py:1372] Variable param_states/decoder/layers_4/pre_self_attention_layer_norm/scale/m              size 1            shape (1,)                                     partition spec None
I0512 23:42:25.020494 140529434355712 utils.py:1372] Variable param_states/decoder/layers_4/pre_self_attention_layer_norm/scale/v              size 2048         shape (embed=2048)                             partition spec ('model',)
I0512 23:42:25.020536 140529434355712 utils.py:1372] Variable param_states/decoder/layers_4/pre_self_attention_layer_norm/scale/v_col          size 1            shape (1,)                                     partition spec None
I0512 23:42:25.020575 140529434355712 utils.py:1372] Variable param_states/decoder/layers_4/pre_self_attention_layer_norm/scale/v_row          size 1            shape (1,)                                     partition spec None
I0512 23:42:25.020614 140529434355712 utils.py:1372] Variable param_states/decoder/layers_4/self_attention/key/kernel/m                        size 1            shape (1,)                                     partition spec None
I0512 23:42:25.020654 140529434355712 utils.py:1372] Variable param_states/decoder/layers_4/self_attention/key/kernel/v                        size 1            shape (1,)                                     partition spec None
I0512 23:42:25.020694 140529434355712 utils.py:1372] Variable param_states/decoder/layers_4/self_attention/key/kernel/v_col                    size 3072         shape (3072,)                                  partition spec None
I0512 23:42:25.020734 140529434355712 utils.py:1372] Variable param_states/decoder/layers_4/self_attention/key/kernel/v_row                    size 2048         shape (2048,)                                  partition spec None
I0512 23:42:25.020773 140529434355712 utils.py:1372] Variable param_states/decoder/layers_4/self_attention/out/kernel/m                        size 1            shape (1,)                                     partition spec None
I0512 23:42:25.020829 140529434355712 utils.py:1372] Variable param_states/decoder/layers_4/self_attention/out/kernel/v                        size 1            shape (1,)                                     partition spec None
I0512 23:42:25.020875 140529434355712 utils.py:1372] Variable param_states/decoder/layers_4/self_attention/out/kernel/v_col                    size 3072         shape (3072,)                                  partition spec None
I0512 23:42:25.020915 140529434355712 utils.py:1372] Variable param_states/decoder/layers_4/self_attention/out/kernel/v_row                    size 2048         shape (2048,)                                  partition spec None
I0512 23:42:25.020954 140529434355712 utils.py:1372] Variable param_states/decoder/layers_4/self_attention/query/kernel/m                      size 1            shape (1,)                                     partition spec None
I0512 23:42:25.020995 140529434355712 utils.py:1372] Variable param_states/decoder/layers_4/self_attention/query/kernel/v                      size 1            shape (1,)                                     partition spec None
I0512 23:42:25.021035 140529434355712 utils.py:1372] Variable param_states/decoder/layers_4/self_attention/query/kernel/v_col                  size 3072         shape (3072,)                                  partition spec None
I0512 23:42:25.021083 140529434355712 utils.py:1372] Variable param_states/decoder/layers_4/self_attention/query/kernel/v_row                  size 2048         shape (2048,)                                  partition spec None
I0512 23:42:25.021125 140529434355712 utils.py:1372] Variable param_states/decoder/layers_4/self_attention/value/kernel/m                      size 1            shape (1,)                                     partition spec None
I0512 23:42:25.021166 140529434355712 utils.py:1372] Variable param_states/decoder/layers_4/self_attention/value/kernel/v                      size 1            shape (1,)                                     partition spec None
I0512 23:42:25.021207 140529434355712 utils.py:1372] Variable param_states/decoder/layers_4/self_attention/value/kernel/v_col                  size 3072         shape (3072,)                                  partition spec None
I0512 23:42:25.021246 140529434355712 utils.py:1372] Variable param_states/decoder/layers_4/self_attention/value/kernel/v_row                  size 2048         shape (2048,)                                  partition spec None
I0512 23:42:25.021285 140529434355712 utils.py:1372] Variable param_states/decoder/layers_5/extra_mlp/wi_0/kernel/m                            size 1            shape (1,)                                     partition spec None
I0512 23:42:25.021328 140529434355712 utils.py:1372] Variable param_states/decoder/layers_5/extra_mlp/wi_0/kernel/v                            size 1            shape (1,)                                     partition spec None
I0512 23:42:25.021370 140529434355712 utils.py:1372] Variable param_states/decoder/layers_5/extra_mlp/wi_0/kernel/v_col                        size 16384        shape (16384,)                                 partition spec None
I0512 23:42:25.021409 140529434355712 utils.py:1372] Variable param_states/decoder/layers_5/extra_mlp/wi_0/kernel/v_row                        size 2048         shape (2048,)                                  partition spec None
I0512 23:42:25.021448 140529434355712 utils.py:1372] Variable param_states/decoder/layers_5/extra_mlp/wi_1/kernel/m                            size 1            shape (1,)                                     partition spec None
I0512 23:42:25.021489 140529434355712 utils.py:1372] Variable param_states/decoder/layers_5/extra_mlp/wi_1/kernel/v                            size 1            shape (1,)                                     partition spec None
I0512 23:42:25.021529 140529434355712 utils.py:1372] Variable param_states/decoder/layers_5/extra_mlp/wi_1/kernel/v_col                        size 8192         shape (8192,)                                  partition spec None
I0512 23:42:25.021594 140529434355712 utils.py:1372] Variable param_states/decoder/layers_5/extra_mlp/wi_1/kernel/v_row                        size 2048         shape (2048,)                                  partition spec None
I0512 23:42:25.021636 140529434355712 utils.py:1372] Variable param_states/decoder/layers_5/extra_mlp/wo/kernel/m                              size 1            shape (1,)                                     partition spec None
I0512 23:42:25.021677 140529434355712 utils.py:1372] Variable param_states/decoder/layers_5/extra_mlp/wo/kernel/v                              size 1            shape (1,)                                     partition spec None
I0512 23:42:25.021717 140529434355712 utils.py:1372] Variable param_states/decoder/layers_5/extra_mlp/wo/kernel/v_col                          size 8192         shape (8192,)                                  partition spec None
I0512 23:42:25.021756 140529434355712 utils.py:1372] Variable param_states/decoder/layers_5/extra_mlp/wo/kernel/v_row                          size 2048         shape (2048,)                                  partition spec None
I0512 23:42:25.021795 140529434355712 utils.py:1372] Variable param_states/decoder/layers_5/mlp/expert/wi_0/kernel/m                           size 1            shape (1,)                                     partition spec None
I0512 23:42:25.021838 140529434355712 utils.py:1372] Variable param_states/decoder/layers_5/mlp/expert/wi_0/kernel/v                           size 1            shape (1,)                                     partition spec None
I0512 23:42:25.021881 140529434355712 utils.py:1372] Variable param_states/decoder/layers_5/mlp/expert/wi_0/kernel/v_col                       size 131072       shape (8, 16384)                               partition spec ('expert',)
I0512 23:42:25.021922 140529434355712 utils.py:1372] Variable param_states/decoder/layers_5/mlp/expert/wi_0/kernel/v_row                       size 16384        shape (8, 2048)                                partition spec ('expert',)
I0512 23:42:25.021962 140529434355712 utils.py:1372] Variable param_states/decoder/layers_5/mlp/expert/wi_1/kernel/m                           size 1            shape (1,)                                     partition spec None
I0512 23:42:25.022002 140529434355712 utils.py:1372] Variable param_states/decoder/layers_5/mlp/expert/wi_1/kernel/v                           size 1            shape (1,)                                     partition spec None
I0512 23:42:25.022042 140529434355712 utils.py:1372] Variable param_states/decoder/layers_5/mlp/expert/wi_1/kernel/v_col                       size 65536        shape (8, 8192)                                partition spec ('expert',)
I0512 23:42:25.022090 140529434355712 utils.py:1372] Variable param_states/decoder/layers_5/mlp/expert/wi_1/kernel/v_row                       size 16384        shape (8, 2048)                                partition spec ('expert',)
I0512 23:42:25.022131 140529434355712 utils.py:1372] Variable param_states/decoder/layers_5/mlp/expert/wo/kernel/m                             size 1            shape (1,)                                     partition spec None
I0512 23:42:25.022171 140529434355712 utils.py:1372] Variable param_states/decoder/layers_5/mlp/expert/wo/kernel/v                             size 1            shape (1,)                                     partition spec None
I0512 23:42:25.022213 140529434355712 utils.py:1372] Variable param_states/decoder/layers_5/mlp/expert/wo/kernel/v_col                         size 65536        shape (8, 8192)                                partition spec ('expert',)
I0512 23:42:25.022253 140529434355712 utils.py:1372] Variable param_states/decoder/layers_5/mlp/expert/wo/kernel/v_row                         size 16384        shape (8, 2048)                                partition spec ('expert',)
I0512 23:42:25.022293 140529434355712 utils.py:1372] Variable param_states/decoder/layers_5/mlp/router/router_weights/w/kernel/m               size 1            shape (1,)                                     partition spec None
I0512 23:42:25.022338 140529434355712 utils.py:1372] Variable param_states/decoder/layers_5/mlp/router/router_weights/w/kernel/v               size 16384        shape (embed=2048, unmodeled=8)                partition spec ('model', None)
I0512 23:42:25.022382 140529434355712 utils.py:1372] Variable param_states/decoder/layers_5/mlp/router/router_weights/w/kernel/v_col           size 1            shape (1,)                                     partition spec None
I0512 23:42:25.022421 140529434355712 utils.py:1372] Variable param_states/decoder/layers_5/mlp/router/router_weights/w/kernel/v_row           size 1            shape (1,)                                     partition spec None
I0512 23:42:25.022460 140529434355712 utils.py:1372] Variable param_states/decoder/layers_5/pre_extra_mlp_layer_norm/scale/m                   size 1            shape (1,)                                     partition spec None
I0512 23:42:25.022503 140529434355712 utils.py:1372] Variable param_states/decoder/layers_5/pre_extra_mlp_layer_norm/scale/v                   size 2048         shape (embed=2048)                             partition spec ('model',)
I0512 23:42:25.022546 140529434355712 utils.py:1372] Variable param_states/decoder/layers_5/pre_extra_mlp_layer_norm/scale/v_col               size 1            shape (1,)                                     partition spec None
I0512 23:42:25.022585 140529434355712 utils.py:1372] Variable param_states/decoder/layers_5/pre_extra_mlp_layer_norm/scale/v_row               size 1            shape (1,)                                     partition spec None
I0512 23:42:25.022624 140529434355712 utils.py:1372] Variable param_states/decoder/layers_5/pre_mlp_layer_norm/scale/m                         size 1            shape (1,)                                     partition spec None
I0512 23:42:25.022666 140529434355712 utils.py:1372] Variable param_states/decoder/layers_5/pre_mlp_layer_norm/scale/v                         size 2048         shape (embed=2048)                             partition spec ('model',)
I0512 23:42:25.022707 140529434355712 utils.py:1372] Variable param_states/decoder/layers_5/pre_mlp_layer_norm/scale/v_col                     size 1            shape (1,)                                     partition spec None
I0512 23:42:25.022747 140529434355712 utils.py:1372] Variable param_states/decoder/layers_5/pre_mlp_layer_norm/scale/v_row                     size 1            shape (1,)                                     partition spec None
I0512 23:42:25.022805 140529434355712 utils.py:1372] Variable param_states/decoder/layers_5/pre_self_attention_layer_norm/scale/m              size 1            shape (1,)                                     partition spec None
I0512 23:42:25.022854 140529434355712 utils.py:1372] Variable param_states/decoder/layers_5/pre_self_attention_layer_norm/scale/v              size 2048         shape (embed=2048)                             partition spec ('model',)
I0512 23:42:25.022898 140529434355712 utils.py:1372] Variable param_states/decoder/layers_5/pre_self_attention_layer_norm/scale/v_col          size 1            shape (1,)                                     partition spec None
I0512 23:42:25.022938 140529434355712 utils.py:1372] Variable param_states/decoder/layers_5/pre_self_attention_layer_norm/scale/v_row          size 1            shape (1,)                                     partition spec None
I0512 23:42:25.022977 140529434355712 utils.py:1372] Variable param_states/decoder/layers_5/self_attention/key/kernel/m                        size 1            shape (1,)                                     partition spec None
I0512 23:42:25.023017 140529434355712 utils.py:1372] Variable param_states/decoder/layers_5/self_attention/key/kernel/v                        size 1            shape (1,)                                     partition spec None
I0512 23:42:25.023058 140529434355712 utils.py:1372] Variable param_states/decoder/layers_5/self_attention/key/kernel/v_col                    size 3072         shape (3072,)                                  partition spec None
I0512 23:42:25.023109 140529434355712 utils.py:1372] Variable param_states/decoder/layers_5/self_attention/key/kernel/v_row                    size 2048         shape (2048,)                                  partition spec None
I0512 23:42:25.023150 140529434355712 utils.py:1372] Variable param_states/decoder/layers_5/self_attention/out/kernel/m                        size 1            shape (1,)                                     partition spec None
I0512 23:42:25.023193 140529434355712 utils.py:1372] Variable param_states/decoder/layers_5/self_attention/out/kernel/v                        size 1            shape (1,)                                     partition spec None
I0512 23:42:25.023235 140529434355712 utils.py:1372] Variable param_states/decoder/layers_5/self_attention/out/kernel/v_col                    size 3072         shape (3072,)                                  partition spec None
I0512 23:42:25.023276 140529434355712 utils.py:1372] Variable param_states/decoder/layers_5/self_attention/out/kernel/v_row                    size 2048         shape (2048,)                                  partition spec None
I0512 23:42:25.023317 140529434355712 utils.py:1372] Variable param_states/decoder/layers_5/self_attention/query/kernel/m                      size 1            shape (1,)                                     partition spec None
I0512 23:42:25.023359 140529434355712 utils.py:1372] Variable param_states/decoder/layers_5/self_attention/query/kernel/v                      size 1            shape (1,)                                     partition spec None
I0512 23:42:25.023401 140529434355712 utils.py:1372] Variable param_states/decoder/layers_5/self_attention/query/kernel/v_col                  size 3072         shape (3072,)                                  partition spec None
I0512 23:42:25.023442 140529434355712 utils.py:1372] Variable param_states/decoder/layers_5/self_attention/query/kernel/v_row                  size 2048         shape (2048,)                                  partition spec None
I0512 23:42:25.023483 140529434355712 utils.py:1372] Variable param_states/decoder/layers_5/self_attention/value/kernel/m                      size 1            shape (1,)                                     partition spec None
I0512 23:42:25.023525 140529434355712 utils.py:1372] Variable param_states/decoder/layers_5/self_attention/value/kernel/v                      size 1            shape (1,)                                     partition spec None
I0512 23:42:25.023567 140529434355712 utils.py:1372] Variable param_states/decoder/layers_5/self_attention/value/kernel/v_col                  size 3072         shape (3072,)                                  partition spec None
I0512 23:42:25.023607 140529434355712 utils.py:1372] Variable param_states/decoder/layers_5/self_attention/value/kernel/v_row                  size 2048         shape (2048,)                                  partition spec None
I0512 23:42:25.023648 140529434355712 utils.py:1372] Variable param_states/decoder/layers_6/mlp/wi_0/kernel/m                                  size 1            shape (1,)                                     partition spec None
I0512 23:42:25.023692 140529434355712 utils.py:1372] Variable param_states/decoder/layers_6/mlp/wi_0/kernel/v                                  size 1            shape (1,)                                     partition spec None
I0512 23:42:25.023732 140529434355712 utils.py:1372] Variable param_states/decoder/layers_6/mlp/wi_0/kernel/v_col                              size 16384        shape (16384,)                                 partition spec None
I0512 23:42:25.023771 140529434355712 utils.py:1372] Variable param_states/decoder/layers_6/mlp/wi_0/kernel/v_row                              size 2048         shape (2048,)                                  partition spec None
I0512 23:42:25.023810 140529434355712 utils.py:1372] Variable param_states/decoder/layers_6/mlp/wi_1/kernel/m                                  size 1            shape (1,)                                     partition spec None
I0512 23:42:25.023853 140529434355712 utils.py:1372] Variable param_states/decoder/layers_6/mlp/wi_1/kernel/v                                  size 1            shape (1,)                                     partition spec None
I0512 23:42:25.023894 140529434355712 utils.py:1372] Variable param_states/decoder/layers_6/mlp/wi_1/kernel/v_col                              size 8192         shape (8192,)                                  partition spec None
I0512 23:42:25.023932 140529434355712 utils.py:1372] Variable param_states/decoder/layers_6/mlp/wi_1/kernel/v_row                              size 2048         shape (2048,)                                  partition spec None
I0512 23:42:25.023971 140529434355712 utils.py:1372] Variable param_states/decoder/layers_6/mlp/wo/kernel/m                                    size 1            shape (1,)                                     partition spec None
I0512 23:42:25.024010 140529434355712 utils.py:1372] Variable param_states/decoder/layers_6/mlp/wo/kernel/v                                    size 1            shape (1,)                                     partition spec None
I0512 23:42:25.024051 140529434355712 utils.py:1372] Variable param_states/decoder/layers_6/mlp/wo/kernel/v_col                                size 8192         shape (8192,)                                  partition spec None
I0512 23:42:25.024100 140529434355712 utils.py:1372] Variable param_states/decoder/layers_6/mlp/wo/kernel/v_row                                size 2048         shape (2048,)                                  partition spec None
I0512 23:42:25.024140 140529434355712 utils.py:1372] Variable param_states/decoder/layers_6/pre_mlp_layer_norm/scale/m                         size 1            shape (1,)                                     partition spec None
I0512 23:42:25.024185 140529434355712 utils.py:1372] Variable param_states/decoder/layers_6/pre_mlp_layer_norm/scale/v                         size 2048         shape (embed=2048)                             partition spec ('model',)
I0512 23:42:25.024226 140529434355712 utils.py:1372] Variable param_states/decoder/layers_6/pre_mlp_layer_norm/scale/v_col                     size 1            shape (1,)                                     partition spec None
I0512 23:42:25.024266 140529434355712 utils.py:1372] Variable param_states/decoder/layers_6/pre_mlp_layer_norm/scale/v_row                     size 1            shape (1,)                                     partition spec None
I0512 23:42:25.024305 140529434355712 utils.py:1372] Variable param_states/decoder/layers_6/pre_self_attention_layer_norm/scale/m              size 1            shape (1,)                                     partition spec None
I0512 23:42:25.024347 140529434355712 utils.py:1372] Variable param_states/decoder/layers_6/pre_self_attention_layer_norm/scale/v              size 2048         shape (embed=2048)                             partition spec ('model',)
I0512 23:42:25.024388 140529434355712 utils.py:1372] Variable param_states/decoder/layers_6/pre_self_attention_layer_norm/scale/v_col          size 1            shape (1,)                                     partition spec None
I0512 23:42:25.024428 140529434355712 utils.py:1372] Variable param_states/decoder/layers_6/pre_self_attention_layer_norm/scale/v_row          size 1            shape (1,)                                     partition spec None
I0512 23:42:25.024466 140529434355712 utils.py:1372] Variable param_states/decoder/layers_6/self_attention/key/kernel/m                        size 1            shape (1,)                                     partition spec None
I0512 23:42:25.024506 140529434355712 utils.py:1372] Variable param_states/decoder/layers_6/self_attention/key/kernel/v                        size 1            shape (1,)                                     partition spec None
I0512 23:42:25.024545 140529434355712 utils.py:1372] Variable param_states/decoder/layers_6/self_attention/key/kernel/v_col                    size 3072         shape (3072,)                                  partition spec None
I0512 23:42:25.024584 140529434355712 utils.py:1372] Variable param_states/decoder/layers_6/self_attention/key/kernel/v_row                    size 2048         shape (2048,)                                  partition spec None
I0512 23:42:25.024622 140529434355712 utils.py:1372] Variable param_states/decoder/layers_6/self_attention/out/kernel/m                        size 1            shape (1,)                                     partition spec None
I0512 23:42:25.024662 140529434355712 utils.py:1372] Variable param_states/decoder/layers_6/self_attention/out/kernel/v                        size 1            shape (1,)                                     partition spec None
I0512 23:42:25.024702 140529434355712 utils.py:1372] Variable param_states/decoder/layers_6/self_attention/out/kernel/v_col                    size 3072         shape (3072,)                                  partition spec None
I0512 23:42:25.024755 140529434355712 utils.py:1372] Variable param_states/decoder/layers_6/self_attention/out/kernel/v_row                    size 2048         shape (2048,)                                  partition spec None
I0512 23:42:25.024798 140529434355712 utils.py:1372] Variable param_states/decoder/layers_6/self_attention/query/kernel/m                      size 1            shape (1,)                                     partition spec None
I0512 23:42:25.024843 140529434355712 utils.py:1372] Variable param_states/decoder/layers_6/self_attention/query/kernel/v                      size 1            shape (1,)                                     partition spec None
I0512 23:42:25.024886 140529434355712 utils.py:1372] Variable param_states/decoder/layers_6/self_attention/query/kernel/v_col                  size 3072         shape (3072,)                                  partition spec None
I0512 23:42:25.024927 140529434355712 utils.py:1372] Variable param_states/decoder/layers_6/self_attention/query/kernel/v_row                  size 2048         shape (2048,)                                  partition spec None
I0512 23:42:25.024968 140529434355712 utils.py:1372] Variable param_states/decoder/layers_6/self_attention/value/kernel/m                      size 1            shape (1,)                                     partition spec None
I0512 23:42:25.025010 140529434355712 utils.py:1372] Variable param_states/decoder/layers_6/self_attention/value/kernel/v                      size 1            shape (1,)                                     partition spec None
I0512 23:42:25.025051 140529434355712 utils.py:1372] Variable param_states/decoder/layers_6/self_attention/value/kernel/v_col                  size 3072         shape (3072,)                                  partition spec None
I0512 23:42:25.025100 140529434355712 utils.py:1372] Variable param_states/decoder/layers_6/self_attention/value/kernel/v_row                  size 2048         shape (2048,)                                  partition spec None
I0512 23:42:25.025142 140529434355712 utils.py:1372] Variable param_states/decoder/layers_7/mlp/wi_0/kernel/m                                  size 1            shape (1,)                                     partition spec None
I0512 23:42:25.025186 140529434355712 utils.py:1372] Variable param_states/decoder/layers_7/mlp/wi_0/kernel/v                                  size 1            shape (1,)                                     partition spec None
I0512 23:42:25.025228 140529434355712 utils.py:1372] Variable param_states/decoder/layers_7/mlp/wi_0/kernel/v_col                              size 16384        shape (16384,)                                 partition spec None
I0512 23:42:25.025269 140529434355712 utils.py:1372] Variable param_states/decoder/layers_7/mlp/wi_0/kernel/v_row                              size 2048         shape (2048,)                                  partition spec None
I0512 23:42:25.025310 140529434355712 utils.py:1372] Variable param_states/decoder/layers_7/mlp/wi_1/kernel/m                                  size 1            shape (1,)                                     partition spec None
I0512 23:42:25.025353 140529434355712 utils.py:1372] Variable param_states/decoder/layers_7/mlp/wi_1/kernel/v                                  size 1            shape (1,)                                     partition spec None
I0512 23:42:25.025393 140529434355712 utils.py:1372] Variable param_states/decoder/layers_7/mlp/wi_1/kernel/v_col                              size 8192         shape (8192,)                                  partition spec None
I0512 23:42:25.025432 140529434355712 utils.py:1372] Variable param_states/decoder/layers_7/mlp/wi_1/kernel/v_row                              size 2048         shape (2048,)                                  partition spec None
I0512 23:42:25.025470 140529434355712 utils.py:1372] Variable param_states/decoder/layers_7/mlp/wo/kernel/m                                    size 1            shape (1,)                                     partition spec None
I0512 23:42:25.025510 140529434355712 utils.py:1372] Variable param_states/decoder/layers_7/mlp/wo/kernel/v                                    size 1            shape (1,)                                     partition spec None
I0512 23:42:25.025570 140529434355712 utils.py:1372] Variable param_states/decoder/layers_7/mlp/wo/kernel/v_col                                size 8192         shape (8192,)                                  partition spec None
I0512 23:42:25.025615 140529434355712 utils.py:1372] Variable param_states/decoder/layers_7/mlp/wo/kernel/v_row                                size 2048         shape (2048,)                                  partition spec None
I0512 23:42:25.025655 140529434355712 utils.py:1372] Variable param_states/decoder/layers_7/pre_mlp_layer_norm/scale/m                         size 1            shape (1,)                                     partition spec None
I0512 23:42:25.025700 140529434355712 utils.py:1372] Variable param_states/decoder/layers_7/pre_mlp_layer_norm/scale/v                         size 2048         shape (embed=2048)                             partition spec ('model',)
I0512 23:42:25.025741 140529434355712 utils.py:1372] Variable param_states/decoder/layers_7/pre_mlp_layer_norm/scale/v_col                     size 1            shape (1,)                                     partition spec None
I0512 23:42:25.025780 140529434355712 utils.py:1372] Variable param_states/decoder/layers_7/pre_mlp_layer_norm/scale/v_row                     size 1            shape (1,)                                     partition spec None
I0512 23:42:25.025819 140529434355712 utils.py:1372] Variable param_states/decoder/layers_7/pre_self_attention_layer_norm/scale/m              size 1            shape (1,)                                     partition spec None
I0512 23:42:25.025864 140529434355712 utils.py:1372] Variable param_states/decoder/layers_7/pre_self_attention_layer_norm/scale/v              size 2048         shape (embed=2048)                             partition spec ('model',)
I0512 23:42:25.025906 140529434355712 utils.py:1372] Variable param_states/decoder/layers_7/pre_self_attention_layer_norm/scale/v_col          size 1            shape (1,)                                     partition spec None
I0512 23:42:25.025946 140529434355712 utils.py:1372] Variable param_states/decoder/layers_7/pre_self_attention_layer_norm/scale/v_row          size 1            shape (1,)                                     partition spec None
I0512 23:42:25.025984 140529434355712 utils.py:1372] Variable param_states/decoder/layers_7/self_attention/key/kernel/m                        size 1            shape (1,)                                     partition spec None
I0512 23:42:25.026025 140529434355712 utils.py:1372] Variable param_states/decoder/layers_7/self_attention/key/kernel/v                        size 1            shape (1,)                                     partition spec None
I0512 23:42:25.026066 140529434355712 utils.py:1372] Variable param_states/decoder/layers_7/self_attention/key/kernel/v_col                    size 3072         shape (3072,)                                  partition spec None
I0512 23:42:25.026112 140529434355712 utils.py:1372] Variable param_states/decoder/layers_7/self_attention/key/kernel/v_row                    size 2048         shape (2048,)                                  partition spec None
I0512 23:42:25.026151 140529434355712 utils.py:1372] Variable param_states/decoder/layers_7/self_attention/out/kernel/m                        size 1            shape (1,)                                     partition spec None
I0512 23:42:25.026191 140529434355712 utils.py:1372] Variable param_states/decoder/layers_7/self_attention/out/kernel/v                        size 1            shape (1,)                                     partition spec None
I0512 23:42:25.026231 140529434355712 utils.py:1372] Variable param_states/decoder/layers_7/self_attention/out/kernel/v_col                    size 3072         shape (3072,)                                  partition spec None
I0512 23:42:25.026270 140529434355712 utils.py:1372] Variable param_states/decoder/layers_7/self_attention/out/kernel/v_row                    size 2048         shape (2048,)                                  partition spec None
I0512 23:42:25.026308 140529434355712 utils.py:1372] Variable param_states/decoder/layers_7/self_attention/query/kernel/m                      size 1            shape (1,)                                     partition spec None
I0512 23:42:25.026348 140529434355712 utils.py:1372] Variable param_states/decoder/layers_7/self_attention/query/kernel/v                      size 1            shape (1,)                                     partition spec None
I0512 23:42:25.026387 140529434355712 utils.py:1372] Variable param_states/decoder/layers_7/self_attention/query/kernel/v_col                  size 3072         shape (3072,)                                  partition spec None
I0512 23:42:25.026426 140529434355712 utils.py:1372] Variable param_states/decoder/layers_7/self_attention/query/kernel/v_row                  size 2048         shape (2048,)                                  partition spec None
I0512 23:42:25.026465 140529434355712 utils.py:1372] Variable param_states/decoder/layers_7/self_attention/value/kernel/m                      size 1            shape (1,)                                     partition spec None
I0512 23:42:25.026505 140529434355712 utils.py:1372] Variable param_states/decoder/layers_7/self_attention/value/kernel/v                      size 1            shape (1,)                                     partition spec None
I0512 23:42:25.026544 140529434355712 utils.py:1372] Variable param_states/decoder/layers_7/self_attention/value/kernel/v_col                  size 3072         shape (3072,)                                  partition spec None
I0512 23:42:25.026583 140529434355712 utils.py:1372] Variable param_states/decoder/layers_7/self_attention/value/kernel/v_row                  size 2048         shape (2048,)                                  partition spec None
I0512 23:42:25.026622 140529434355712 utils.py:1372] Variable param_states/decoder/layers_8/mlp/wi_0/kernel/m                                  size 1            shape (1,)                                     partition spec None
I0512 23:42:25.026665 140529434355712 utils.py:1372] Variable param_states/decoder/layers_8/mlp/wi_0/kernel/v                                  size 1            shape (1,)                                     partition spec None
I0512 23:42:25.026735 140529434355712 utils.py:1372] Variable param_states/decoder/layers_8/mlp/wi_0/kernel/v_col                              size 16384        shape (16384,)                                 partition spec None
I0512 23:42:25.026780 140529434355712 utils.py:1372] Variable param_states/decoder/layers_8/mlp/wi_0/kernel/v_row                              size 2048         shape (2048,)                                  partition spec None
I0512 23:42:25.026820 140529434355712 utils.py:1372] Variable param_states/decoder/layers_8/mlp/wi_1/kernel/m                                  size 1            shape (1,)                                     partition spec None
I0512 23:42:25.026862 140529434355712 utils.py:1372] Variable param_states/decoder/layers_8/mlp/wi_1/kernel/v                                  size 1            shape (1,)                                     partition spec None
I0512 23:42:25.026903 140529434355712 utils.py:1372] Variable param_states/decoder/layers_8/mlp/wi_1/kernel/v_col                              size 8192         shape (8192,)                                  partition spec None
I0512 23:42:25.026942 140529434355712 utils.py:1372] Variable param_states/decoder/layers_8/mlp/wi_1/kernel/v_row                              size 2048         shape (2048,)                                  partition spec None
I0512 23:42:25.026981 140529434355712 utils.py:1372] Variable param_states/decoder/layers_8/mlp/wo/kernel/m                                    size 1            shape (1,)                                     partition spec None
I0512 23:42:25.027021 140529434355712 utils.py:1372] Variable param_states/decoder/layers_8/mlp/wo/kernel/v                                    size 1            shape (1,)                                     partition spec None
I0512 23:42:25.027060 140529434355712 utils.py:1372] Variable param_states/decoder/layers_8/mlp/wo/kernel/v_col                                size 8192         shape (8192,)                                  partition spec None
I0512 23:42:25.027109 140529434355712 utils.py:1372] Variable param_states/decoder/layers_8/mlp/wo/kernel/v_row                                size 2048         shape (2048,)                                  partition spec None
I0512 23:42:25.027148 140529434355712 utils.py:1372] Variable param_states/decoder/layers_8/pre_mlp_layer_norm/scale/m                         size 1            shape (1,)                                     partition spec None
I0512 23:42:25.027192 140529434355712 utils.py:1372] Variable param_states/decoder/layers_8/pre_mlp_layer_norm/scale/v                         size 2048         shape (embed=2048)                             partition spec ('model',)
I0512 23:42:25.027235 140529434355712 utils.py:1372] Variable param_states/decoder/layers_8/pre_mlp_layer_norm/scale/v_col                     size 1            shape (1,)                                     partition spec None
I0512 23:42:25.027274 140529434355712 utils.py:1372] Variable param_states/decoder/layers_8/pre_mlp_layer_norm/scale/v_row                     size 1            shape (1,)                                     partition spec None
I0512 23:42:25.027313 140529434355712 utils.py:1372] Variable param_states/decoder/layers_8/pre_self_attention_layer_norm/scale/m              size 1            shape (1,)                                     partition spec None
I0512 23:42:25.027355 140529434355712 utils.py:1372] Variable param_states/decoder/layers_8/pre_self_attention_layer_norm/scale/v              size 2048         shape (embed=2048)                             partition spec ('model',)
I0512 23:42:25.027396 140529434355712 utils.py:1372] Variable param_states/decoder/layers_8/pre_self_attention_layer_norm/scale/v_col          size 1            shape (1,)                                     partition spec None
I0512 23:42:25.027435 140529434355712 utils.py:1372] Variable param_states/decoder/layers_8/pre_self_attention_layer_norm/scale/v_row          size 1            shape (1,)                                     partition spec None
I0512 23:42:25.027474 140529434355712 utils.py:1372] Variable param_states/decoder/layers_8/self_attention/key/kernel/m                        size 1            shape (1,)                                     partition spec None
I0512 23:42:25.027515 140529434355712 utils.py:1372] Variable param_states/decoder/layers_8/self_attention/key/kernel/v                        size 1            shape (1,)                                     partition spec None
I0512 23:42:25.027554 140529434355712 utils.py:1372] Variable param_states/decoder/layers_8/self_attention/key/kernel/v_col                    size 3072         shape (3072,)                                  partition spec None
I0512 23:42:25.027594 140529434355712 utils.py:1372] Variable param_states/decoder/layers_8/self_attention/key/kernel/v_row                    size 2048         shape (2048,)                                  partition spec None
I0512 23:42:25.027632 140529434355712 utils.py:1372] Variable param_states/decoder/layers_8/self_attention/out/kernel/m                        size 1            shape (1,)                                     partition spec None
I0512 23:42:25.027673 140529434355712 utils.py:1372] Variable param_states/decoder/layers_8/self_attention/out/kernel/v                        size 1            shape (1,)                                     partition spec None
I0512 23:42:25.027713 140529434355712 utils.py:1372] Variable param_states/decoder/layers_8/self_attention/out/kernel/v_col                    size 3072         shape (3072,)                                  partition spec None
I0512 23:42:25.027752 140529434355712 utils.py:1372] Variable param_states/decoder/layers_8/self_attention/out/kernel/v_row                    size 2048         shape (2048,)                                  partition spec None
I0512 23:42:25.027790 140529434355712 utils.py:1372] Variable param_states/decoder/layers_8/self_attention/query/kernel/m                      size 1            shape (1,)                                     partition spec None
I0512 23:42:25.027830 140529434355712 utils.py:1372] Variable param_states/decoder/layers_8/self_attention/query/kernel/v                      size 1            shape (1,)                                     partition spec None
I0512 23:42:25.027872 140529434355712 utils.py:1372] Variable param_states/decoder/layers_8/self_attention/query/kernel/v_col                  size 3072         shape (3072,)                                  partition spec None
I0512 23:42:25.027910 140529434355712 utils.py:1372] Variable param_states/decoder/layers_8/self_attention/query/kernel/v_row                  size 2048         shape (2048,)                                  partition spec None
I0512 23:42:25.027948 140529434355712 utils.py:1372] Variable param_states/decoder/layers_8/self_attention/value/kernel/m                      size 1            shape (1,)                                     partition spec None
I0512 23:42:25.027989 140529434355712 utils.py:1372] Variable param_states/decoder/layers_8/self_attention/value/kernel/v                      size 1            shape (1,)                                     partition spec None
I0512 23:42:25.028028 140529434355712 utils.py:1372] Variable param_states/decoder/layers_8/self_attention/value/kernel/v_col                  size 3072         shape (3072,)                                  partition spec None
I0512 23:42:25.028066 140529434355712 utils.py:1372] Variable param_states/decoder/layers_8/self_attention/value/kernel/v_row                  size 2048         shape (2048,)                                  partition spec None
I0512 23:42:25.028113 140529434355712 utils.py:1372] Variable param_states/decoder/layers_9/mlp/wi_0/kernel/m                                  size 1            shape (1,)                                     partition spec None
I0512 23:42:25.028158 140529434355712 utils.py:1372] Variable param_states/decoder/layers_9/mlp/wi_0/kernel/v                                  size 1            shape (1,)                                     partition spec None
I0512 23:42:25.028197 140529434355712 utils.py:1372] Variable param_states/decoder/layers_9/mlp/wi_0/kernel/v_col                              size 16384        shape (16384,)                                 partition spec None
I0512 23:42:25.028236 140529434355712 utils.py:1372] Variable param_states/decoder/layers_9/mlp/wi_0/kernel/v_row                              size 2048         shape (2048,)                                  partition spec None
I0512 23:42:25.028274 140529434355712 utils.py:1372] Variable param_states/decoder/layers_9/mlp/wi_1/kernel/m                                  size 1            shape (1,)                                     partition spec None
I0512 23:42:25.028314 140529434355712 utils.py:1372] Variable param_states/decoder/layers_9/mlp/wi_1/kernel/v                                  size 1            shape (1,)                                     partition spec None
I0512 23:42:25.028353 140529434355712 utils.py:1372] Variable param_states/decoder/layers_9/mlp/wi_1/kernel/v_col                              size 8192         shape (8192,)                                  partition spec None
I0512 23:42:25.028389 140529434355712 utils.py:1372] Variable param_states/decoder/layers_9/mlp/wi_1/kernel/v_row                              size 2048         shape (2048,)                                  partition spec None
I0512 23:42:25.028429 140529434355712 utils.py:1372] Variable param_states/decoder/layers_9/mlp/wo/kernel/m                                    size 1            shape (1,)                                     partition spec None
I0512 23:42:25.028467 140529434355712 utils.py:1372] Variable param_states/decoder/layers_9/mlp/wo/kernel/v                                    size 1            shape (1,)                                     partition spec None
I0512 23:42:25.028506 140529434355712 utils.py:1372] Variable param_states/decoder/layers_9/mlp/wo/kernel/v_col                                size 8192         shape (8192,)                                  partition spec None
I0512 23:42:25.028545 140529434355712 utils.py:1372] Variable param_states/decoder/layers_9/mlp/wo/kernel/v_row                                size 2048         shape (2048,)                                  partition spec None
I0512 23:42:25.028583 140529434355712 utils.py:1372] Variable param_states/decoder/layers_9/pre_mlp_layer_norm/scale/m                         size 1            shape (1,)                                     partition spec None
I0512 23:42:25.028643 140529434355712 utils.py:1372] Variable param_states/decoder/layers_9/pre_mlp_layer_norm/scale/v                         size 2048         shape (embed=2048)                             partition spec ('model',)
I0512 23:42:25.028688 140529434355712 utils.py:1372] Variable param_states/decoder/layers_9/pre_mlp_layer_norm/scale/v_col                     size 1            shape (1,)                                     partition spec None
I0512 23:42:25.028728 140529434355712 utils.py:1372] Variable param_states/decoder/layers_9/pre_mlp_layer_norm/scale/v_row                     size 1            shape (1,)                                     partition spec None
I0512 23:42:25.028766 140529434355712 utils.py:1372] Variable param_states/decoder/layers_9/pre_self_attention_layer_norm/scale/m              size 1            shape (1,)                                     partition spec None
I0512 23:42:25.028808 140529434355712 utils.py:1372] Variable param_states/decoder/layers_9/pre_self_attention_layer_norm/scale/v              size 2048         shape (embed=2048)                             partition spec ('model',)
I0512 23:42:25.028851 140529434355712 utils.py:1372] Variable param_states/decoder/layers_9/pre_self_attention_layer_norm/scale/v_col          size 1            shape (1,)                                     partition spec None
I0512 23:42:25.028891 140529434355712 utils.py:1372] Variable param_states/decoder/layers_9/pre_self_attention_layer_norm/scale/v_row          size 1            shape (1,)                                     partition spec None
I0512 23:42:25.028928 140529434355712 utils.py:1372] Variable param_states/decoder/layers_9/self_attention/key/kernel/m                        size 1            shape (1,)                                     partition spec None
I0512 23:42:25.028968 140529434355712 utils.py:1372] Variable param_states/decoder/layers_9/self_attention/key/kernel/v                        size 1            shape (1,)                                     partition spec None
I0512 23:42:25.029008 140529434355712 utils.py:1372] Variable param_states/decoder/layers_9/self_attention/key/kernel/v_col                    size 3072         shape (3072,)                                  partition spec None
I0512 23:42:25.029046 140529434355712 utils.py:1372] Variable param_states/decoder/layers_9/self_attention/key/kernel/v_row                    size 2048         shape (2048,)                                  partition spec None
I0512 23:42:25.029097 140529434355712 utils.py:1372] Variable param_states/decoder/layers_9/self_attention/out/kernel/m                        size 1            shape (1,)                                     partition spec None
I0512 23:42:25.029139 140529434355712 utils.py:1372] Variable param_states/decoder/layers_9/self_attention/out/kernel/v                        size 1            shape (1,)                                     partition spec None
I0512 23:42:25.029179 140529434355712 utils.py:1372] Variable param_states/decoder/layers_9/self_attention/out/kernel/v_col                    size 3072         shape (3072,)                                  partition spec None
I0512 23:42:25.029218 140529434355712 utils.py:1372] Variable param_states/decoder/layers_9/self_attention/out/kernel/v_row                    size 2048         shape (2048,)                                  partition spec None
I0512 23:42:25.029257 140529434355712 utils.py:1372] Variable param_states/decoder/layers_9/self_attention/query/kernel/m                      size 1            shape (1,)                                     partition spec None
I0512 23:42:25.029297 140529434355712 utils.py:1372] Variable param_states/decoder/layers_9/self_attention/query/kernel/v                      size 1            shape (1,)                                     partition spec None
I0512 23:42:25.029336 140529434355712 utils.py:1372] Variable param_states/decoder/layers_9/self_attention/query/kernel/v_col                  size 3072         shape (3072,)                                  partition spec None
I0512 23:42:25.029375 140529434355712 utils.py:1372] Variable param_states/decoder/layers_9/self_attention/query/kernel/v_row                  size 2048         shape (2048,)                                  partition spec None
I0512 23:42:25.029415 140529434355712 utils.py:1372] Variable param_states/decoder/layers_9/self_attention/value/kernel/m                      size 1            shape (1,)                                     partition spec None
I0512 23:42:25.029455 140529434355712 utils.py:1372] Variable param_states/decoder/layers_9/self_attention/value/kernel/v                      size 1            shape (1,)                                     partition spec None
I0512 23:42:25.029494 140529434355712 utils.py:1372] Variable param_states/decoder/layers_9/self_attention/value/kernel/v_col                  size 3072         shape (3072,)                                  partition spec None
I0512 23:42:25.029533 140529434355712 utils.py:1372] Variable param_states/decoder/layers_9/self_attention/value/kernel/v_row                  size 2048         shape (2048,)                                  partition spec None
I0512 23:42:25.029600 140529434355712 utils.py:1372] Variable param_states/decoder/logits_dense/kernel/m                                       size 1            shape (1,)                                     partition spec None
I0512 23:42:25.029647 140529434355712 utils.py:1372] Variable param_states/decoder/logits_dense/kernel/v                                       size 1            shape (1,)                                     partition spec None
I0512 23:42:25.029687 140529434355712 utils.py:1372] Variable param_states/decoder/logits_dense/kernel/v_col                                   size 256384       shape (256384,)                                partition spec None
I0512 23:42:25.029726 140529434355712 utils.py:1372] Variable param_states/decoder/logits_dense/kernel/v_row                                   size 2048         shape (2048,)                                  partition spec None
I0512 23:42:25.029765 140529434355712 utils.py:1372] Variable param_states/token_embedder/embedding/m                                          size 1            shape (1,)                                     partition spec None
I0512 23:42:25.029816 140529434355712 utils.py:1372] Variable param_states/token_embedder/embedding/v                                          size 1            shape (1,)                                     partition spec None
I0512 23:42:25.029859 140529434355712 utils.py:1372] Variable param_states/token_embedder/embedding/v_col                                      size 256384       shape (256384,)                                partition spec None
I0512 23:42:25.029900 140529434355712 utils.py:1372] Variable param_states/token_embedder/embedding/v_row                                      size 2048         shape (2048,)                                  partition spec None
I0512 23:42:25.029938 140529434355712 utils.py:1372] Variable step                                                                             size 1            shape ()                                       partition spec None
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
I0512 23:42:25.506689 140439994910272 logging_writer.py:64] [32500] collection=train Got texts: {'config': "    from __gin__ import dynamic_registration\n    import __main__ as train_script\n    from flax import linen\n    import flaxformer\n    from flaxformer.architectures.moe import moe_architecture\n    from flaxformer.architectures.moe import moe_enums\n    from flaxformer.architectures.moe import moe_layers\n    from flaxformer.architectures.moe import routing\n    from flaxformer.architectures.t5 import t5_architecture\n    from flaxformer.components.attention import dense_attention\n    from flaxformer.components.attention import memory_efficient_attention\n    from flaxformer.components import dense\n    from flaxformer.components import embedding\n    from flaxformer.components import layer_norm\n    from gin import config\n    import seqio\n    import t5.data.mixtures\n    from t5x import adafactor\n    from t5x.contrib.moe import adafactor_utils\n    from t5x.contrib.moe import models\n    from t5x.contrib.moe import partitioning\n    from t5x.contrib.moe import trainer as moe_trainer\n    from t5x import gin_utils\n    from t5x import partitioning as partitioning2\n    from t5x import trainer\n    from t5x import utils\n    \n#### Macros:\n\n    ACTIVATION_DTYPE = 'bfloat16'\n    ACTIVATION_PARTITIONING_DIMS = 1\n    ARCHITECTURE = @t5_architecture.DecoderOnly()\n    AUX_LOSS_FACTOR = 0.01\n    BATCH_SIZE = 384\n    BIAS_INIT = @bias_init/linen.initializers.normal()\n    DECODER_SPARSE_LAYOUT = %flaxformer.architectures.moe.moe_enums.LayerLayout.MIXED\n    DROPOUT_FACTORY = @dropout_factory/linen.Dropout\n    DROPOUT_RATE = 0.0\n    EMBED_DIM = 2048\n    EVAL_EXPERT_CAPACITY_FACTOR = 2.0\n    EXPERT_DROPOUT_RATE = %DROPOUT_RATE\n    EXPERT_MLP_DIM = %MLP_DIM\n    GROUP_SIZE = 4096\n    HEAD_DIM = 128\n    JITTER_NOISE = 0.0\n    LABEL_SMOOTHING = 0.0\n    LOSS_NORMALIZING_FACTOR = 'NUM_REAL_TARGET_TOKENS'\n    MIXTURE_OR_TASK_MODULE = None\n    MIXTURE_OR_TASK_NAME = 'mix_full_lm_test'\n    MLP_DIM = 8192\n    MODEL = @models.MoeDecoderOnlyModel()\n    MODEL_DIR = 'gs://rosinality-tpu-bucket/openmoe_8b/training'\n    MODEL_PARALLEL_SUBMESH = None\n    MOE_TRUNCATED_DTYPE = 'bfloat16'\n    NUM_DECODER_LAYERS = 24\n    NUM_DECODER_SPARSE_LAYERS = 4\n    NUM_EMBEDDINGS = 256384\n    NUM_EXPERT_PARTITIONS = 8\n    NUM_EXPERTS = 8\n    NUM_HEADS = 24\n    NUM_MODEL_PARTITIONS = 4\n    NUM_SELECTED_EXPERTS = 2\n    OPTIMIZER = @adafactor.Adafactor()\n    RANDOM_SEED = None\n    ROUTER_Z_LOSS_FACTOR = 0.0001\n    SCALE = 0.1\n    SHUFFLE_TRAIN_EXAMPLES = True\n    TASK_FEATURE_LENGTHS = {'targets': 2048}\n    TRAIN_EXPERT_CAPACITY_FACTOR = 1.25\n    TRAIN_STEPS = 500000\n    USE_CACHED_TASKS = False\n    USE_HARDWARE_RNG = False\n    VOCABULARY = @seqio.SentencePieceVocabulary()\n    Z_LOSS = 0.0001\n    \n#### Parameters for adafactor.Adafactor:\n\n    adafactor.Adafactor.decay_rate = 0.8\n    adafactor.Adafactor.logical_factor_rules = @adafactor_utils.logical_factor_rules()\n    adafactor.Adafactor.step_offset = 0\n    \n#### Parameters for utils.CheckpointConfig:\n\n    utils.CheckpointConfig.restore = @utils.RestoreCheckpointConfig()\n    utils.CheckpointConfig.save = @utils.SaveCheckpointConfig()\n    \n#### Parameters for partitioning.compute_num_model_partitions:\n\n    partitioning.compute_num_model_partitions.model_parallel_submesh = \\\n        %MODEL_PARALLEL_SUBMESH\n    partitioning.compute_num_model_partitions.num_model_partitions = \\\n        %NUM_MODEL_PARTITIONS\n    \n#### Parameters for utils.create_learning_rate_scheduler:\n\n    utils.create_learning_rate_scheduler.base_learning_rate = 1.0\n    utils.create_learning_rate_scheduler.factors = 'constant * rsqrt_decay'\n    utils.create_learning_rate_scheduler.warmup_steps = 10000\n    \n#### Parameters for train/utils.DatasetConfig:\n\n    train/utils.DatasetConfig.batch_size = %BATCH_SIZE\n    train/utils.DatasetConfig.mixture_or_task_name = %MIXTURE_OR_TASK_NAME\n    train/utils.DatasetConfig.module = %MIXTURE_OR_TASK_MODULE\n    train/utils.DatasetConfig.pack = True\n    train/utils.DatasetConfig.seed = None\n    train/utils.DatasetConfig.shuffle = %SHUFFLE_TRAIN_EXAMPLES\n    train/utils.DatasetConfig.split = 'train'\n    train/utils.DatasetConfig.task_feature_lengths = %TASK_FEATURE_LENGTHS\n    train/utils.DatasetConfig.use_cached = %USE_CACHED_TASKS\n    \n#### Parameters for train_eval/utils.DatasetConfig:\n\n    train_eval/utils.DatasetConfig.batch_size = 128\n    train_eval/utils.DatasetConfig.mixture_or_task_name = %MIXTURE_OR_TASK_NAME\n    train_eval/utils.DatasetConfig.module = %MIXTURE_OR_TASK_MODULE\n    train_eval/utils.DatasetConfig.pack = True\n    train_eval/utils.DatasetConfig.seed = 42\n    train_eval/utils.DatasetConfig.shuffle = False\n    train_eval/utils.DatasetConfig.split = 'validation'\n    train_eval/utils.DatasetConfig.task_feature_lengths = %TASK_FEATURE_LENGTHS\n    train_eval/utils.DatasetConfig.use_cached = %USE_CACHED_TASKS\n    \n#### Parameters for t5_architecture.DecoderLayer:\n\n    t5_architecture.DecoderLayer.activation_partitioning_dims = \\\n        %ACTIVATION_PARTITIONING_DIMS\n    t5_architecture.DecoderLayer.dropout_factory = %DROPOUT_FACTORY\n    t5_architecture.DecoderLayer.encoder_decoder_attention = None\n    t5_architecture.DecoderLayer.layer_norm_factory = @layer_norm.T5LayerNorm\n    t5_architecture.DecoderLayer.mlp = @dense.MlpBlock()\n    t5_architecture.DecoderLayer.scanned = False\n    t5_architecture.DecoderLayer.self_attention = \\\n        @dense_attention.MultiHeadDotProductAttention()\n    \n#### Parameters for t5_architecture.DecoderOnly:\n\n    t5_architecture.DecoderOnly.decoder_factory = @moe_architecture.SparseDecoder\n    t5_architecture.DecoderOnly.dtype = %ACTIVATION_DTYPE\n    t5_architecture.DecoderOnly.shared_token_embedder_factory = @embedding.Embed\n    \n#### Parameters for output_logits/dense.DenseGeneral:\n\n    output_logits/dense.DenseGeneral.bias_init = %BIAS_INIT\n    output_logits/dense.DenseGeneral.dtype = 'float32'\n    output_logits/dense.DenseGeneral.features = %NUM_EMBEDDINGS\n    output_logits/dense.DenseGeneral.kernel_axis_names = ['embed', 'vocab']\n    output_logits/dense.DenseGeneral.kernel_init = \\\n        @output_logits_kernel_init/linen.initializers.variance_scaling()\n    output_logits/dense.DenseGeneral.use_bias = False\n    \n#### Parameters for dropout_factory/linen.Dropout:\n\n    dropout_factory/linen.Dropout.broadcast_dims = (-2,)\n    dropout_factory/linen.Dropout.rate = %DROPOUT_RATE\n    \n#### Parameters for embedding.Embed:\n\n    embedding.Embed.attend_dtype = 'float32'\n    embedding.Embed.cast_input_dtype = 'int32'\n    embedding.Embed.dtype = %ACTIVATION_DTYPE\n    embedding.Embed.embedding_init = @token_embedder_init/linen.initializers.normal()\n    embedding.Embed.features = %EMBED_DIM\n    embedding.Embed.name = 'token_embedder'\n    embedding.Embed.num_embeddings = %NUM_EMBEDDINGS\n    embedding.Embed.one_hot = True\n    \n#### Parameters for dense.MlpBlock:\n\n    dense.MlpBlock.activations = ('swiglu', 'linear')\n    dense.MlpBlock.bias_init = %BIAS_INIT\n    dense.MlpBlock.dtype = %ACTIVATION_DTYPE\n    dense.MlpBlock.final_dropout_rate = 0\n    dense.MlpBlock.input_axis_name = 'mlp_embed'\n    dense.MlpBlock.intermediate_dim = %MLP_DIM\n    dense.MlpBlock.intermediate_dropout_rate = %DROPOUT_RATE\n    dense.MlpBlock.kernel_init = @mlp_kernel_init/linen.initializers.variance_scaling()\n    dense.MlpBlock.output_axis_name = 'mlp_embed'\n    dense.MlpBlock.use_bias = False\n    \n#### Parameters for expert/dense.MlpBlock:\n\n    expert/dense.MlpBlock.activation_partitioning_dims = 1\n    expert/dense.MlpBlock.activations = ('swiglu', 'linear')\n    expert/dense.MlpBlock.bias_init = %BIAS_INIT\n    expert/dense.MlpBlock.data_sharding_constraints = ('expert_replicas', 'mlp')\n    expert/dense.MlpBlock.dtype = %MOE_TRUNCATED_DTYPE\n    expert/dense.MlpBlock.final_dropout_rate = 0.0\n    expert/dense.MlpBlock.input_axis_name = 'mlp_embed'\n    expert/dense.MlpBlock.intermediate_axis_name = 'expert_mlp'\n    expert/dense.MlpBlock.intermediate_dim = %EXPERT_MLP_DIM\n    expert/dense.MlpBlock.intermediate_dropout_rate = %EXPERT_DROPOUT_RATE\n    expert/dense.MlpBlock.kernel_init = \\\n        @expert_kernel_init/linen.initializers.variance_scaling()\n    expert/dense.MlpBlock.output_axis_name = 'mlp_embed'\n    expert/dense.MlpBlock.use_bias = False\n    \n#### Parameters for models.MoeDecoderOnlyModel:\n\n    models.MoeDecoderOnlyModel.aux_loss_factor = %AUX_LOSS_FACTOR\n    models.MoeDecoderOnlyModel.label_smoothing = %LABEL_SMOOTHING\n    models.MoeDecoderOnlyModel.loss_normalizing_factor = %LOSS_NORMALIZING_FACTOR\n    models.MoeDecoderOnlyModel.module = %ARCHITECTURE\n    models.MoeDecoderOnlyModel.optimizer_def = %OPTIMIZER\n    models.MoeDecoderOnlyModel.router_z_loss_factor = %ROUTER_Z_LOSS_FACTOR\n    models.MoeDecoderOnlyModel.vocabulary = %VOCABULARY\n    models.MoeDecoderOnlyModel.z_loss = %Z_LOSS\n    \n#### Parameters for moe_layers.MoeLayer:\n\n    moe_layers.MoeLayer.dtype = %MOE_TRUNCATED_DTYPE\n    moe_layers.MoeLayer.eval_capacity_factor = %EVAL_EXPERT_CAPACITY_FACTOR\n    moe_layers.MoeLayer.expert = @expert/dense.MlpBlock()\n    moe_layers.MoeLayer.max_group_size = %GROUP_SIZE\n    moe_layers.MoeLayer.num_expert_partitions = %NUM_EXPERT_PARTITIONS\n    moe_layers.MoeLayer.num_experts = %NUM_EXPERTS\n    moe_layers.MoeLayer.num_model_partitions = \\\n        @partitioning.compute_num_model_partitions()\n    moe_layers.MoeLayer.train_capacity_factor = %TRAIN_EXPERT_CAPACITY_FACTOR\n    \n#### Parameters for sparse_decoder/moe_layers.MoeLayer:\n\n    sparse_decoder/moe_layers.MoeLayer.router = \\\n        @sparse_decoder/routing.TokensChooseMaskedRouter()\n    \n#### Parameters for partitioning.MoePjitPartitioner:\n\n    partitioning.MoePjitPartitioner.model_parallel_submesh = %MODEL_PARALLEL_SUBMESH\n    partitioning.MoePjitPartitioner.num_expert_partitions = %NUM_EXPERT_PARTITIONS\n    partitioning.MoePjitPartitioner.num_partitions = %NUM_MODEL_PARTITIONS\n    \n#### Parameters for moe_trainer.MoeTrainer:\n\n    moe_trainer.MoeTrainer.learning_rate_fn = @utils.create_learning_rate_scheduler()\n    moe_trainer.MoeTrainer.num_expert_partitions = %NUM_EXPERT_PARTITIONS\n    moe_trainer.MoeTrainer.num_microbatches = 8\n    \n#### Parameters for dense_attention.MultiHeadDotProductAttention:\n\n    dense_attention.MultiHeadDotProductAttention.bias_init = %BIAS_INIT\n    dense_attention.MultiHeadDotProductAttention.broadcast_dropout = True\n    dense_attention.MultiHeadDotProductAttention.dropout_rate = %DROPOUT_RATE\n    dense_attention.MultiHeadDotProductAttention.dtype = %ACTIVATION_DTYPE\n    dense_attention.MultiHeadDotProductAttention.head_dim = %HEAD_DIM\n    dense_attention.MultiHeadDotProductAttention.kernel_init = \\\n        @attention_kernel_init/linen.initializers.variance_scaling()\n    dense_attention.MultiHeadDotProductAttention.num_heads = %NUM_HEADS\n    dense_attention.MultiHeadDotProductAttention.use_bias = False\n    dense_attention.MultiHeadDotProductAttention.use_rotary_embedding = True\n    \n#### Parameters for bias_init/linen.initializers.normal:\n\n    bias_init/linen.initializers.normal.stddev = 1e-06\n    \n#### Parameters for router_init/linen.initializers.normal:\n\n    router_init/linen.initializers.normal.stddev = 0.02\n    \n#### Parameters for token_embedder_init/linen.initializers.normal:\n\n    token_embedder_init/linen.initializers.normal.stddev = 1.0\n    \n#### Parameters for partitioning2.PjitPartitioner:\n\n    partitioning2.PjitPartitioner.logical_axis_rules = \\\n        @partitioning2.standard_logical_axis_rules()\n    partitioning2.PjitPartitioner.model_parallel_submesh = None\n    partitioning2.PjitPartitioner.num_partitions = 1\n    \n#### Parameters for utils.RestoreCheckpointConfig:\n\n    utils.RestoreCheckpointConfig.path = []\n    \n#### Parameters for routing.RouterWeights:\n\n    routing.RouterWeights.bias_init = %BIAS_INIT\n    routing.RouterWeights.dtype = 'float32'\n    routing.RouterWeights.kernel_init = @router_init/linen.initializers.normal()\n    routing.RouterWeights.use_bias = False\n    \n#### Parameters for utils.SaveCheckpointConfig:\n\n    utils.SaveCheckpointConfig.dtype = 'float32'\n    utils.SaveCheckpointConfig.keep = 20\n    utils.SaveCheckpointConfig.period = 2500\n    utils.SaveCheckpointConfig.save_dataset = False\n    \n#### Parameters for seqio.SentencePieceVocabulary:\n\n    seqio.SentencePieceVocabulary.extra_ids = 300\n    seqio.SentencePieceVocabulary.sentencepiece_model_file = \\\n        'gs://rosinality-tpu-bucket/sentencepiece.model'\n    \n#### Parameters for moe_architecture.SparseDecoder:\n\n    moe_architecture.SparseDecoder.dropout_factory = %DROPOUT_FACTORY\n    moe_architecture.SparseDecoder.dtype = %ACTIVATION_DTYPE\n    moe_architecture.SparseDecoder.layer_factory = @t5_architecture.DecoderLayer\n    moe_architecture.SparseDecoder.layer_norm_factory = @layer_norm.T5LayerNorm\n    moe_architecture.SparseDecoder.num_layers = %NUM_DECODER_LAYERS\n    moe_architecture.SparseDecoder.num_sparse_layers = %NUM_DECODER_SPARSE_LAYERS\n    moe_architecture.SparseDecoder.output_logits_factory = \\\n        @output_logits/dense.DenseGeneral\n    moe_architecture.SparseDecoder.sparse_layer_factory = \\\n        @moe_architecture.SparseDecoderLayer\n    moe_architecture.SparseDecoder.sparse_layout = %DECODER_SPARSE_LAYOUT\n    \n#### Parameters for moe_architecture.SparseDecoderLayer:\n\n    moe_architecture.SparseDecoderLayer.activation_partitioning_dims = \\\n        %ACTIVATION_PARTITIONING_DIMS\n    moe_architecture.SparseDecoderLayer.dropout_factory = %DROPOUT_FACTORY\n    moe_architecture.SparseDecoderLayer.encoder_decoder_attention = None\n    moe_architecture.SparseDecoderLayer.extra_mlp = @dense.MlpBlock()\n    moe_architecture.SparseDecoderLayer.layer_norm_factory = @layer_norm.T5LayerNorm\n    moe_architecture.SparseDecoderLayer.mlp = @sparse_decoder/moe_layers.MoeLayer()\n    moe_architecture.SparseDecoderLayer.scanned = False\n    moe_architecture.SparseDecoderLayer.self_attention = \\\n        @dense_attention.MultiHeadDotProductAttention()\n    \n#### Parameters for layer_norm.T5LayerNorm:\n\n    layer_norm.T5LayerNorm.dtype = %ACTIVATION_DTYPE\n    \n#### Parameters for routing.TokensChooseMaskedRouter:\n\n    routing.TokensChooseMaskedRouter.dtype = 'float32'\n    routing.TokensChooseMaskedRouter.ignore_padding_tokens = False\n    routing.TokensChooseMaskedRouter.jitter_noise = %JITTER_NOISE\n    routing.TokensChooseMaskedRouter.num_selected_experts = %NUM_SELECTED_EXPERTS\n    routing.TokensChooseMaskedRouter.router_weights = @routing.RouterWeights()\n    \n#### Parameters for sparse_decoder/routing.TokensChooseMaskedRouter:\n\n    sparse_decoder/routing.TokensChooseMaskedRouter.batch_prioritized_routing = False\n    \n#### Parameters for sparse_encoder/routing.TokensChooseMaskedRouter:\n\n    sparse_encoder/routing.TokensChooseMaskedRouter.batch_prioritized_routing = False\n    \n#### Parameters for train_script.train:\n\n    train_script.train.checkpoint_cfg = @utils.CheckpointConfig()\n    train_script.train.eval_period = 2500\n    train_script.train.eval_steps = 20\n    train_script.train.infer_eval_dataset_cfg = None\n    train_script.train.model = %MODEL\n    train_script.train.model_dir = %MODEL_DIR\n    train_script.train.partitioner = @partitioning.MoePjitPartitioner()\n    train_script.train.random_seed = %RANDOM_SEED\n    train_script.train.stats_period = 10\n    train_script.train.summarize_config_fn = @gin_utils.summarize_gin_config\n    train_script.train.total_steps = %TRAIN_STEPS\n    train_script.train.train_dataset_cfg = @train/utils.DatasetConfig()\n    train_script.train.train_eval_dataset_cfg = None\n    train_script.train.trainer_cls = @moe_trainer.MoeTrainer\n    train_script.train.use_hardware_rng = %USE_HARDWARE_RNG\n    \n#### Parameters for trainer.Trainer:\n\n    trainer.Trainer.learning_rate_fn = @utils.create_learning_rate_scheduler()\n    trainer.Trainer.num_microbatches = None\n    \n#### Parameters for attention_kernel_init/linen.initializers.variance_scaling:\n\n    attention_kernel_init/linen.initializers.variance_scaling.distribution = 'normal'\n    attention_kernel_init/linen.initializers.variance_scaling.mode = 'fan_in'\n    attention_kernel_init/linen.initializers.variance_scaling.scale = %SCALE\n    \n#### Parameters for expert_kernel_init/linen.initializers.variance_scaling:\n\n    expert_kernel_init/linen.initializers.variance_scaling.distribution = 'normal'\n    expert_kernel_init/linen.initializers.variance_scaling.mode = 'fan_in'\n    expert_kernel_init/linen.initializers.variance_scaling.scale = %SCALE\n    \n#### Parameters for mlp_kernel_init/linen.initializers.variance_scaling:\n\n    mlp_kernel_init/linen.initializers.variance_scaling.distribution = \\\n        'truncated_normal'\n    mlp_kernel_init/linen.initializers.variance_scaling.mode = 'fan_in'\n    mlp_kernel_init/linen.initializers.variance_scaling.scale = %SCALE\n    \n#### Parameters for output_logits_kernel_init/linen.initializers.variance_scaling:\n\n    output_logits_kernel_init/linen.initializers.variance_scaling.distribution = \\\n        'truncated_normal'\n    output_logits_kernel_init/linen.initializers.variance_scaling.mode = 'fan_in'\n    output_logits_kernel_init/linen.initializers.variance_scaling.scale = %SCALE"}.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
I0512 23:42:25.663351 140439986517568 logging_writer.py:48] [32500] collection=train timing/init_or_restore_seconds=12.8324
I0512 23:42:25.664971 140529434355712 train.py:573] Saving checkpoint before the training loop starts.
I0512 23:42:25.733252 140095893518336 checkpoints.py:781] Skipping save checkpoint for step 32500 (directory gs://rosinality-tpu-bucket/openmoe_8b/training/checkpoint_32500 already exists)
I0512 23:42:25.733485 140095893518336 train.py:587] Starting training loop.
I0512 23:42:25.733531 140095893518336 train.py:614] Starting main loop over steps 32500-500000
I0512 23:42:25.733572 140095893518336 train.py:621] Training with artificial "epochs" of 2500 steps.
I0512 23:42:25.733621 140095893518336 train.py:625] Compiling train loop.
I0512 23:42:25.734595 140339465779200 checkpoints.py:781] Skipping save checkpoint for step 32500 (directory gs://rosinality-tpu-bucket/openmoe_8b/training/checkpoint_32500 already exists)
I0512 23:42:25.734824 140339465779200 train.py:587] Starting training loop.
I0512 23:42:25.734869 140339465779200 train.py:614] Starting main loop over steps 32500-500000
I0512 23:42:25.734912 140339465779200 train.py:621] Training with artificial "epochs" of 2500 steps.
I0512 23:42:25.734961 140339465779200 train.py:625] Compiling train loop.
I0512 23:42:25.745921 139674595104768 checkpoints.py:781] Skipping save checkpoint for step 32500 (directory gs://rosinality-tpu-bucket/openmoe_8b/training/checkpoint_32500 already exists)
I0512 23:42:25.746194 139674595104768 train.py:587] Starting training loop.
I0512 23:42:25.746252 139674595104768 train.py:614] Starting main loop over steps 32500-500000
I0512 23:42:25.745575 140595318249472 checkpoints.py:781] Skipping save checkpoint for step 32500 (directory gs://rosinality-tpu-bucket/openmoe_8b/training/checkpoint_32500 already exists)
I0512 23:42:25.745787 140595318249472 train.py:587] Starting training loop.
I0512 23:42:25.745831 140595318249472 train.py:614] Starting main loop over steps 32500-500000
I0512 23:42:25.745872 140595318249472 train.py:621] Training with artificial "epochs" of 2500 steps.
I0512 23:42:25.745918 140595318249472 train.py:625] Compiling train loop.
I0512 23:42:25.744326 140031916288000 checkpoints.py:781] Skipping save checkpoint for step 32500 (directory gs://rosinality-tpu-bucket/openmoe_8b/training/checkpoint_32500 already exists)
I0512 23:42:25.744930 140031916288000 train.py:587] Starting training loop.
I0512 23:42:25.745021 140031916288000 train.py:614] Starting main loop over steps 32500-500000
I0512 23:42:25.745095 140031916288000 train.py:621] Training with artificial "epochs" of 2500 steps.
I0512 23:42:25.746301 139674595104768 train.py:621] Training with artificial "epochs" of 2500 steps.
I0512 23:42:25.746350 139674595104768 train.py:625] Compiling train loop.
I0512 23:42:25.745182 140031916288000 train.py:625] Compiling train loop.
I0512 23:42:25.753790 140440593786880 checkpoints.py:781] Skipping save checkpoint for step 32500 (directory gs://rosinality-tpu-bucket/openmoe_8b/training/checkpoint_32500 already exists)
I0512 23:42:25.754135 140440593786880 train.py:587] Starting training loop.
I0512 23:42:25.754287 140440593786880 train.py:614] Starting main loop over steps 32500-500000
I0512 23:42:25.754362 140440593786880 train.py:621] Training with artificial "epochs" of 2500 steps.
I0512 23:42:25.754431 140440593786880 train.py:625] Compiling train loop.
I0512 23:42:25.736808 140529434355712 checkpoints.py:781] Skipping save checkpoint for step 32500 (directory gs://rosinality-tpu-bucket/openmoe_8b/training/checkpoint_32500 already exists)
I0512 23:42:25.737114 140529434355712 train.py:587] Starting training loop.
I0512 23:42:25.737162 140529434355712 train.py:614] Starting main loop over steps 32500-500000
I0512 23:42:25.737209 140529434355712 train.py:621] Training with artificial "epochs" of 2500 steps.
I0512 23:42:25.737256 140529434355712 train.py:625] Compiling train loop.
I0512 23:42:25.766044 140311617681408 checkpoints.py:781] Skipping save checkpoint for step 32500 (directory gs://rosinality-tpu-bucket/openmoe_8b/training/checkpoint_32500 already exists)
I0512 23:42:25.766286 140311617681408 train.py:587] Starting training loop.
I0512 23:42:25.766336 140311617681408 train.py:614] Starting main loop over steps 32500-500000
I0512 23:42:25.766384 140311617681408 train.py:621] Training with artificial "epochs" of 2500 steps.
I0512 23:42:25.766463 140311617681408 train.py:625] Compiling train loop.
I0512 23:42:25.842589 140095893518336 trainer.py:704] using microbatches: 8 microbatches, 48 size
I0512 23:42:25.847118 140595318249472 trainer.py:704] using microbatches: 8 microbatches, 48 size
I0512 23:42:25.835264 140529434355712 trainer.py:704] using microbatches: 8 microbatches, 48 size
I0512 23:42:25.863977 140339465779200 trainer.py:704] using microbatches: 8 microbatches, 48 size
I0512 23:42:25.867190 140031916288000 trainer.py:704] using microbatches: 8 microbatches, 48 size
I0512 23:42:25.885491 139674595104768 trainer.py:704] using microbatches: 8 microbatches, 48 size
I0512 23:42:25.898862 140440593786880 trainer.py:704] using microbatches: 8 microbatches, 48 size
I0512 23:42:25.927650 140311617681408 trainer.py:704] using microbatches: 8 microbatches, 48 size
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
I0512 23:42:26.494721 140095893518336 moe_layers.py:777] Selected group_size=4096 and num_groups=24 for input num_tokens=98304, max_group_size=4096, num_experts=8 and num_expert_replicas=1
I0512 23:42:26.481803 140529434355712 moe_layers.py:777] Selected group_size=4096 and num_groups=24 for input num_tokens=98304, max_group_size=4096, num_experts=8 and num_expert_replicas=1
I0512 23:42:26.589935 140339465779200 moe_layers.py:777] Selected group_size=4096 and num_groups=24 for input num_tokens=98304, max_group_size=4096, num_experts=8 and num_expert_replicas=1
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
I0512 23:42:26.602840 140440593786880 moe_layers.py:777] Selected group_size=4096 and num_groups=24 for input num_tokens=98304, max_group_size=4096, num_experts=8 and num_expert_replicas=1
I0512 23:42:26.610454 139674595104768 moe_layers.py:777] Selected group_size=4096 and num_groups=24 for input num_tokens=98304, max_group_size=4096, num_experts=8 and num_expert_replicas=1
I0512 23:42:26.626656 140595318249472 moe_layers.py:777] Selected group_size=4096 and num_groups=24 for input num_tokens=98304, max_group_size=4096, num_experts=8 and num_expert_replicas=1
I0512 23:42:26.632666 140031916288000 moe_layers.py:777] Selected group_size=4096 and num_groups=24 for input num_tokens=98304, max_group_size=4096, num_experts=8 and num_expert_replicas=1
I0512 23:42:26.634494 140311617681408 moe_layers.py:777] Selected group_size=4096 and num_groups=24 for input num_tokens=98304, max_group_size=4096, num_experts=8 and num_expert_replicas=1
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
I0512 23:42:27.037618 140095893518336 moe_layers.py:777] Selected group_size=4096 and num_groups=24 for input num_tokens=98304, max_group_size=4096, num_experts=8 and num_expert_replicas=1
I0512 23:42:27.032761 140529434355712 moe_layers.py:777] Selected group_size=4096 and num_groups=24 for input num_tokens=98304, max_group_size=4096, num_experts=8 and num_expert_replicas=1
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
I0512 23:42:27.141118 140339465779200 moe_layers.py:777] Selected group_size=4096 and num_groups=24 for input num_tokens=98304, max_group_size=4096, num_experts=8 and num_expert_replicas=1
I0512 23:42:27.170165 139674595104768 moe_layers.py:777] Selected group_size=4096 and num_groups=24 for input num_tokens=98304, max_group_size=4096, num_experts=8 and num_expert_replicas=1
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
I0512 23:42:27.185308 140031916288000 moe_layers.py:777] Selected group_size=4096 and num_groups=24 for input num_tokens=98304, max_group_size=4096, num_experts=8 and num_expert_replicas=1
I0512 23:42:27.189052 140440593786880 moe_layers.py:777] Selected group_size=4096 and num_groups=24 for input num_tokens=98304, max_group_size=4096, num_experts=8 and num_expert_replicas=1
I0512 23:42:27.208420 140595318249472 moe_layers.py:777] Selected group_size=4096 and num_groups=24 for input num_tokens=98304, max_group_size=4096, num_experts=8 and num_expert_replicas=1
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
I0512 23:42:27.216971 140311617681408 moe_layers.py:777] Selected group_size=4096 and num_groups=24 for input num_tokens=98304, max_group_size=4096, num_experts=8 and num_expert_replicas=1
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
I0512 23:42:27.512253 140095893518336 moe_layers.py:777] Selected group_size=4096 and num_groups=24 for input num_tokens=98304, max_group_size=4096, num_experts=8 and num_expert_replicas=1
I0512 23:42:27.493293 140529434355712 moe_layers.py:777] Selected group_size=4096 and num_groups=24 for input num_tokens=98304, max_group_size=4096, num_experts=8 and num_expert_replicas=1
I0512 23:42:27.622722 140339465779200 moe_layers.py:777] Selected group_size=4096 and num_groups=24 for input num_tokens=98304, max_group_size=4096, num_experts=8 and num_expert_replicas=1
I0512 23:42:27.649253 140031916288000 moe_layers.py:777] Selected group_size=4096 and num_groups=24 for input num_tokens=98304, max_group_size=4096, num_experts=8 and num_expert_replicas=1
I0512 23:42:27.657176 139674595104768 moe_layers.py:777] Selected group_size=4096 and num_groups=24 for input num_tokens=98304, max_group_size=4096, num_experts=8 and num_expert_replicas=1
I0512 23:42:27.668643 140440593786880 moe_layers.py:777] Selected group_size=4096 and num_groups=24 for input num_tokens=98304, max_group_size=4096, num_experts=8 and num_expert_replicas=1
I0512 23:42:27.668540 140595318249472 moe_layers.py:777] Selected group_size=4096 and num_groups=24 for input num_tokens=98304, max_group_size=4096, num_experts=8 and num_expert_replicas=1
I0512 23:42:27.696467 140311617681408 moe_layers.py:777] Selected group_size=4096 and num_groups=24 for input num_tokens=98304, max_group_size=4096, num_experts=8 and num_expert_replicas=1
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
I0512 23:42:28.291591 140095893518336 moe_layers.py:777] Selected group_size=4096 and num_groups=24 for input num_tokens=98304, max_group_size=4096, num_experts=8 and num_expert_replicas=1
I0512 23:42:28.280615 140529434355712 moe_layers.py:777] Selected group_size=4096 and num_groups=24 for input num_tokens=98304, max_group_size=4096, num_experts=8 and num_expert_replicas=1
I0512 23:42:28.394944 140339465779200 moe_layers.py:777] Selected group_size=4096 and num_groups=24 for input num_tokens=98304, max_group_size=4096, num_experts=8 and num_expert_replicas=1
I0512 23:42:28.443201 140440593786880 moe_layers.py:777] Selected group_size=4096 and num_groups=24 for input num_tokens=98304, max_group_size=4096, num_experts=8 and num_expert_replicas=1
I0512 23:42:28.450031 140031916288000 moe_layers.py:777] Selected group_size=4096 and num_groups=24 for input num_tokens=98304, max_group_size=4096, num_experts=8 and num_expert_replicas=1
I0512 23:42:28.463859 139674595104768 moe_layers.py:777] Selected group_size=4096 and num_groups=24 for input num_tokens=98304, max_group_size=4096, num_experts=8 and num_expert_replicas=1
I0512 23:42:28.465813 140311617681408 moe_layers.py:777] Selected group_size=4096 and num_groups=24 for input num_tokens=98304, max_group_size=4096, num_experts=8 and num_expert_replicas=1
I0512 23:42:28.487862 140595318249472 moe_layers.py:777] Selected group_size=4096 and num_groups=24 for input num_tokens=98304, max_group_size=4096, num_experts=8 and num_expert_replicas=1
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
I0512 23:42:29.592026 140095893518336 moe_layers.py:777] Selected group_size=4096 and num_groups=24 for input num_tokens=98304, max_group_size=4096, num_experts=8 and num_expert_replicas=1
I0512 23:42:29.591079 140529434355712 moe_layers.py:777] Selected group_size=4096 and num_groups=24 for input num_tokens=98304, max_group_size=4096, num_experts=8 and num_expert_replicas=1
I0512 23:42:29.715007 140339465779200 moe_layers.py:777] Selected group_size=4096 and num_groups=24 for input num_tokens=98304, max_group_size=4096, num_experts=8 and num_expert_replicas=1
I0512 23:42:29.789888 140440593786880 moe_layers.py:777] Selected group_size=4096 and num_groups=24 for input num_tokens=98304, max_group_size=4096, num_experts=8 and num_expert_replicas=1
I0512 23:42:29.791862 140031916288000 moe_layers.py:777] Selected group_size=4096 and num_groups=24 for input num_tokens=98304, max_group_size=4096, num_experts=8 and num_expert_replicas=1
I0512 23:42:29.817905 139674595104768 moe_layers.py:777] Selected group_size=4096 and num_groups=24 for input num_tokens=98304, max_group_size=4096, num_experts=8 and num_expert_replicas=1
I0512 23:42:29.819954 140595318249472 moe_layers.py:777] Selected group_size=4096 and num_groups=24 for input num_tokens=98304, max_group_size=4096, num_experts=8 and num_expert_replicas=1
I0512 23:42:29.910419 140311617681408 moe_layers.py:777] Selected group_size=4096 and num_groups=24 for input num_tokens=98304, max_group_size=4096, num_experts=8 and num_expert_replicas=1
I0512 23:42:30.051257 140095893518336 moe_layers.py:777] Selected group_size=4096 and num_groups=24 for input num_tokens=98304, max_group_size=4096, num_experts=8 and num_expert_replicas=1
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
I0512 23:42:30.201397 140339465779200 moe_layers.py:777] Selected group_size=4096 and num_groups=24 for input num_tokens=98304, max_group_size=4096, num_experts=8 and num_expert_replicas=1
I0512 23:42:30.261974 140440593786880 moe_layers.py:777] Selected group_size=4096 and num_groups=24 for input num_tokens=98304, max_group_size=4096, num_experts=8 and num_expert_replicas=1
I0512 23:42:30.262829 140031916288000 moe_layers.py:777] Selected group_size=4096 and num_groups=24 for input num_tokens=98304, max_group_size=4096, num_experts=8 and num_expert_replicas=1
I0512 23:42:30.301217 139674595104768 moe_layers.py:777] Selected group_size=4096 and num_groups=24 for input num_tokens=98304, max_group_size=4096, num_experts=8 and num_expert_replicas=1
I0512 23:42:30.308199 140595318249472 moe_layers.py:777] Selected group_size=4096 and num_groups=24 for input num_tokens=98304, max_group_size=4096, num_experts=8 and num_expert_replicas=1
I0512 23:42:30.341294 140529434355712 moe_layers.py:777] Selected group_size=4096 and num_groups=24 for input num_tokens=98304, max_group_size=4096, num_experts=8 and num_expert_replicas=1
I0512 23:42:30.387256 140311617681408 moe_layers.py:777] Selected group_size=4096 and num_groups=24 for input num_tokens=98304, max_group_size=4096, num_experts=8 and num_expert_replicas=1
I0512 23:42:30.681430 140339465779200 moe_layers.py:777] Selected group_size=4096 and num_groups=24 for input num_tokens=98304, max_group_size=4096, num_experts=8 and num_expert_replicas=1
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
I0512 23:42:30.824928 140529434355712 moe_layers.py:777] Selected group_size=4096 and num_groups=24 for input num_tokens=98304, max_group_size=4096, num_experts=8 and num_expert_replicas=1
I0512 23:42:30.846109 140095893518336 moe_layers.py:777] Selected group_size=4096 and num_groups=24 for input num_tokens=98304, max_group_size=4096, num_experts=8 and num_expert_replicas=1
I0512 23:42:31.012869 140031916288000 moe_layers.py:777] Selected group_size=4096 and num_groups=24 for input num_tokens=98304, max_group_size=4096, num_experts=8 and num_expert_replicas=1
I0512 23:42:31.132621 140595318249472 moe_layers.py:777] Selected group_size=4096 and num_groups=24 for input num_tokens=98304, max_group_size=4096, num_experts=8 and num_expert_replicas=1
I0512 23:42:31.164245 140311617681408 moe_layers.py:777] Selected group_size=4096 and num_groups=24 for input num_tokens=98304, max_group_size=4096, num_experts=8 and num_expert_replicas=1
I0512 23:42:31.194270 139674595104768 moe_layers.py:777] Selected group_size=4096 and num_groups=24 for input num_tokens=98304, max_group_size=4096, num_experts=8 and num_expert_replicas=1
I0512 23:42:31.208893 140440593786880 moe_layers.py:777] Selected group_size=4096 and num_groups=24 for input num_tokens=98304, max_group_size=4096, num_experts=8 and num_expert_replicas=1
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
I0512 23:42:31.311020 140095893518336 moe_layers.py:777] Selected group_size=4096 and num_groups=24 for input num_tokens=98304, max_group_size=4096, num_experts=8 and num_expert_replicas=1
I0512 23:42:31.300195 140529434355712 moe_layers.py:777] Selected group_size=4096 and num_groups=24 for input num_tokens=98304, max_group_size=4096, num_experts=8 and num_expert_replicas=1
I0512 23:42:31.430646 140339465779200 moe_layers.py:777] Selected group_size=4096 and num_groups=24 for input num_tokens=98304, max_group_size=4096, num_experts=8 and num_expert_replicas=1
I0512 23:42:31.518476 140031916288000 moe_layers.py:777] Selected group_size=4096 and num_groups=24 for input num_tokens=98304, max_group_size=4096, num_experts=8 and num_expert_replicas=1
I0512 23:42:31.605781 140595318249472 moe_layers.py:777] Selected group_size=4096 and num_groups=24 for input num_tokens=98304, max_group_size=4096, num_experts=8 and num_expert_replicas=1
I0512 23:42:31.664127 139674595104768 moe_layers.py:777] Selected group_size=4096 and num_groups=24 for input num_tokens=98304, max_group_size=4096, num_experts=8 and num_expert_replicas=1
I0512 23:42:31.659519 140311617681408 moe_layers.py:777] Selected group_size=4096 and num_groups=24 for input num_tokens=98304, max_group_size=4096, num_experts=8 and num_expert_replicas=1
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
I0512 23:42:31.725830 140440593786880 moe_layers.py:777] Selected group_size=4096 and num_groups=24 for input num_tokens=98304, max_group_size=4096, num_experts=8 and num_expert_replicas=1
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
W0512 23:42:32.386428 140339465779200 adafactor.py:358] Since rank of parameter decoder/decoder_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:32.407806 140339465779200 adafactor.py:358] Since rank of parameter decoder/layers_0/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:32.437185 140339465779200 adafactor.py:358] Since rank of parameter decoder/layers_0/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:32.464019 140339465779200 adafactor.py:358] Since rank of parameter decoder/layers_0/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
W0512 23:42:32.525432 140095893518336 adafactor.py:358] Since rank of parameter decoder/decoder_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:32.524616 140529434355712 adafactor.py:358] Since rank of parameter decoder/decoder_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:32.546861 140095893518336 adafactor.py:358] Since rank of parameter decoder/layers_0/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:32.544664 140529434355712 adafactor.py:358] Since rank of parameter decoder/layers_0/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:32.575759 140095893518336 adafactor.py:358] Since rank of parameter decoder/layers_0/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
W0512 23:42:32.570772 140529434355712 adafactor.py:358] Since rank of parameter decoder/layers_0/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:32.602041 140095893518336 adafactor.py:358] Since rank of parameter decoder/layers_0/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:32.595188 140529434355712 adafactor.py:358] Since rank of parameter decoder/layers_0/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:32.625456 140095893518336 adafactor.py:358] Since rank of parameter decoder/layers_0/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:32.635875 140095893518336 adafactor.py:358] Since rank of parameter decoder/layers_0/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:32.616846 140529434355712 adafactor.py:358] Since rank of parameter decoder/layers_0/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:32.646028 140095893518336 adafactor.py:358] Since rank of parameter decoder/layers_0/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:32.626970 140529434355712 adafactor.py:358] Since rank of parameter decoder/layers_0/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:32.637051 140529434355712 adafactor.py:358] Since rank of parameter decoder/layers_0/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:32.669558 140095893518336 adafactor.py:358] Since rank of parameter decoder/layers_0/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:32.660201 140529434355712 adafactor.py:358] Since rank of parameter decoder/layers_0/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:32.692188 140095893518336 adafactor.py:358] Since rank of parameter decoder/layers_0/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:32.682416 140529434355712 adafactor.py:358] Since rank of parameter decoder/layers_0/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:32.706592 140095893518336 adafactor.py:358] Since rank of parameter decoder/layers_0/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:32.696171 140529434355712 adafactor.py:358] Since rank of parameter decoder/layers_0/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:32.721673 140095893518336 adafactor.py:358] Since rank of parameter decoder/layers_1/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:32.709735 140529434355712 adafactor.py:358] Since rank of parameter decoder/layers_1/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:32.736757 140095893518336 adafactor.py:358] Since rank of parameter decoder/layers_1/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:32.723363 140529434355712 adafactor.py:358] Since rank of parameter decoder/layers_1/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:32.751551 140095893518336 adafactor.py:358] Since rank of parameter decoder/layers_1/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:32.736997 140529434355712 adafactor.py:358] Since rank of parameter decoder/layers_1/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:32.766386 140095893518336 adafactor.py:358] Since rank of parameter decoder/layers_1/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:32.750513 140529434355712 adafactor.py:358] Since rank of parameter decoder/layers_1/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:32.776606 140095893518336 adafactor.py:358] Since rank of parameter decoder/layers_1/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:32.774138 140031916288000 adafactor.py:358] Since rank of parameter decoder/decoder_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:32.760489 140529434355712 adafactor.py:358] Since rank of parameter decoder/layers_1/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:32.787952 140095893518336 adafactor.py:358] Since rank of parameter decoder/layers_1/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:32.770470 140529434355712 adafactor.py:358] Since rank of parameter decoder/layers_1/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:32.793532 140031916288000 adafactor.py:358] Since rank of parameter decoder/layers_0/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:32.803938 140095893518336 adafactor.py:358] Since rank of parameter decoder/layers_1/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:32.783955 140529434355712 adafactor.py:358] Since rank of parameter decoder/layers_1/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:32.798038 140529434355712 adafactor.py:358] Since rank of parameter decoder/layers_1/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:32.820591 140095893518336 adafactor.py:358] Since rank of parameter decoder/layers_1/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:32.819493 140031916288000 adafactor.py:358] Since rank of parameter decoder/layers_0/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:32.811556 140529434355712 adafactor.py:358] Since rank of parameter decoder/layers_1/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:32.835004 140595318249472 adafactor.py:358] Since rank of parameter decoder/decoder_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:32.837893 140095893518336 adafactor.py:358] Since rank of parameter decoder/layers_1/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:32.825112 140529434355712 adafactor.py:358] Since rank of parameter decoder/layers_10/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:32.845901 140031916288000 adafactor.py:358] Since rank of parameter decoder/layers_0/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:32.853071 140095893518336 adafactor.py:358] Since rank of parameter decoder/layers_10/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:32.838742 140529434355712 adafactor.py:358] Since rank of parameter decoder/layers_10/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:32.857249 140595318249472 adafactor.py:358] Since rank of parameter decoder/layers_0/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:32.867894 140095893518336 adafactor.py:358] Since rank of parameter decoder/layers_10/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:32.868105 140031916288000 adafactor.py:358] Since rank of parameter decoder/layers_0/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:32.852246 140529434355712 adafactor.py:358] Since rank of parameter decoder/layers_10/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:32.878365 140031916288000 adafactor.py:358] Since rank of parameter decoder/layers_0/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:32.881799 140095893518336 adafactor.py:358] Since rank of parameter decoder/layers_10/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:32.866137 140529434355712 adafactor.py:358] Since rank of parameter decoder/layers_10/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:32.887767 140595318249472 adafactor.py:358] Since rank of parameter decoder/layers_0/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:32.888488 140031916288000 adafactor.py:358] Since rank of parameter decoder/layers_0/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:32.895988 140095893518336 adafactor.py:358] Since rank of parameter decoder/layers_10/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:32.876157 140529434355712 adafactor.py:358] Since rank of parameter decoder/layers_10/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:32.906142 140095893518336 adafactor.py:358] Since rank of parameter decoder/layers_10/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:32.886161 140529434355712 adafactor.py:358] Since rank of parameter decoder/layers_10/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:32.911878 140031916288000 adafactor.py:358] Since rank of parameter decoder/layers_0/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:32.916276 140095893518336 adafactor.py:358] Since rank of parameter decoder/layers_10/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:32.914222 140595318249472 adafactor.py:358] Since rank of parameter decoder/layers_0/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:32.899761 140529434355712 adafactor.py:358] Since rank of parameter decoder/layers_10/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:32.903736 140311617681408 adafactor.py:358] Since rank of parameter decoder/decoder_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:32.930075 140095893518336 adafactor.py:358] Since rank of parameter decoder/layers_10/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:32.913284 140529434355712 adafactor.py:358] Since rank of parameter decoder/layers_10/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:32.934360 140031916288000 adafactor.py:358] Since rank of parameter decoder/layers_0/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:32.936554 140595318249472 adafactor.py:358] Since rank of parameter decoder/layers_0/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:32.944583 140095893518336 adafactor.py:358] Since rank of parameter decoder/layers_10/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:32.923526 140311617681408 adafactor.py:358] Since rank of parameter decoder/layers_0/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:32.927365 140529434355712 adafactor.py:358] Since rank of parameter decoder/layers_10/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:32.947776 140595318249472 adafactor.py:358] Since rank of parameter decoder/layers_0/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:32.948173 140031916288000 adafactor.py:358] Since rank of parameter decoder/layers_0/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:32.959500 140095893518336 adafactor.py:358] Since rank of parameter decoder/layers_10/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:32.958229 140595318249472 adafactor.py:358] Since rank of parameter decoder/layers_0/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:32.940965 140529434355712 adafactor.py:358] Since rank of parameter decoder/layers_11/extra_mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:32.961878 140031916288000 adafactor.py:358] Since rank of parameter decoder/layers_1/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:32.962808 140339465779200 adafactor.py:358] Since rank of parameter decoder/layers_0/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:32.949473 140311617681408 adafactor.py:358] Since rank of parameter decoder/layers_0/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:32.974176 140095893518336 adafactor.py:358] Since rank of parameter decoder/layers_11/extra_mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:32.954512 140529434355712 adafactor.py:358] Since rank of parameter decoder/layers_11/extra_mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:32.975760 140031916288000 adafactor.py:358] Since rank of parameter decoder/layers_1/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:32.976405 140339465779200 adafactor.py:358] Since rank of parameter decoder/layers_0/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:32.983381 140595318249472 adafactor.py:358] Since rank of parameter decoder/layers_0/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:32.968029 140529434355712 adafactor.py:358] Since rank of parameter decoder/layers_11/extra_mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:32.988897 140095893518336 adafactor.py:358] Since rank of parameter decoder/layers_11/extra_mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:32.989603 140031916288000 adafactor.py:358] Since rank of parameter decoder/layers_1/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:32.989538 140339465779200 adafactor.py:358] Since rank of parameter decoder/layers_0/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:32.976368 140311617681408 adafactor.py:358] Since rank of parameter decoder/layers_0/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:32.997256 140440593786880 adafactor.py:358] Since rank of parameter decoder/decoder_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:33.003203 140095893518336 adafactor.py:358] Since rank of parameter decoder/layers_11/extra_mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:33.007680 140595318249472 adafactor.py:358] Since rank of parameter decoder/layers_0/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:33.008201 140031916288000 adafactor.py:358] Since rank of parameter decoder/layers_1/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:32.998513 140311617681408 adafactor.py:358] Since rank of parameter decoder/layers_0/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:33.018670 140031916288000 adafactor.py:358] Since rank of parameter decoder/layers_1/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:33.020286 140440593786880 adafactor.py:358] Since rank of parameter decoder/layers_0/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:33.019225 140339465779200 adafactor.py:358] Since rank of parameter decoder/layers_0/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:33.021737 140595318249472 adafactor.py:358] Since rank of parameter decoder/layers_0/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:33.008716 140311617681408 adafactor.py:358] Since rank of parameter decoder/layers_0/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:33.028970 140031916288000 adafactor.py:358] Since rank of parameter decoder/layers_1/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:33.037421 140595318249472 adafactor.py:358] Since rank of parameter decoder/layers_1/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:33.018997 140311617681408 adafactor.py:358] Since rank of parameter decoder/layers_0/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:33.042959 140031916288000 adafactor.py:358] Since rank of parameter decoder/layers_1/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:33.044549 140339465779200 adafactor.py:358] Since rank of parameter decoder/layers_0/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:33.047803 140440593786880 adafactor.py:358] Since rank of parameter decoder/layers_0/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:33.051594 140595318249472 adafactor.py:358] Since rank of parameter decoder/layers_1/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:33.056904 140031916288000 adafactor.py:358] Since rank of parameter decoder/layers_1/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:33.059561 140339465779200 adafactor.py:358] Since rank of parameter decoder/layers_0/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:33.042794 140311617681408 adafactor.py:358] Since rank of parameter decoder/layers_0/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:33.066530 140595318249472 adafactor.py:358] Since rank of parameter decoder/layers_1/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:33.071621 140031916288000 adafactor.py:358] Since rank of parameter decoder/layers_1/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:33.072901 140440593786880 adafactor.py:358] Since rank of parameter decoder/layers_0/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:33.055709 140529434355712 adafactor.py:358] Since rank of parameter decoder/layers_11/mlp/router/router_weights/w/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>, <FactorDim.NONE: None>) is ignored.
W0512 23:42:33.075466 140339465779200 adafactor.py:358] Since rank of parameter decoder/layers_1/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:33.080686 140595318249472 adafactor.py:358] Since rank of parameter decoder/layers_1/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
W0512 23:42:33.065240 140311617681408 adafactor.py:358] Since rank of parameter decoder/layers_0/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:33.085586 140031916288000 adafactor.py:358] Since rank of parameter decoder/layers_10/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:33.070923 140529434355712 adafactor.py:358] Since rank of parameter decoder/layers_11/pre_extra_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:33.091035 140595318249472 adafactor.py:358] Since rank of parameter decoder/layers_1/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:33.090897 140339465779200 adafactor.py:358] Since rank of parameter decoder/layers_1/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:33.095263 140440593786880 adafactor.py:358] Since rank of parameter decoder/layers_0/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:33.080928 140529434355712 adafactor.py:358] Since rank of parameter decoder/layers_11/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:33.079482 140311617681408 adafactor.py:358] Since rank of parameter decoder/layers_0/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:33.102310 140095893518336 adafactor.py:358] Since rank of parameter decoder/layers_11/mlp/router/router_weights/w/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>, <FactorDim.NONE: None>) is ignored.
W0512 23:42:33.101283 140595318249472 adafactor.py:358] Since rank of parameter decoder/layers_1/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:33.101486 140031916288000 adafactor.py:358] Since rank of parameter decoder/layers_10/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:33.105788 140440593786880 adafactor.py:358] Since rank of parameter decoder/layers_0/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:33.105571 140339465779200 adafactor.py:358] Since rank of parameter decoder/layers_1/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:33.108702 139674595104768 adafactor.py:358] Since rank of parameter decoder/decoder_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:33.090905 140529434355712 adafactor.py:358] Since rank of parameter decoder/layers_11/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:33.093738 140311617681408 adafactor.py:358] Since rank of parameter decoder/layers_1/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:33.115633 140595318249472 adafactor.py:358] Since rank of parameter decoder/layers_1/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:33.116068 140440593786880 adafactor.py:358] Since rank of parameter decoder/layers_0/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:33.116729 140031916288000 adafactor.py:358] Since rank of parameter decoder/layers_10/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:33.120463 140095893518336 adafactor.py:358] Since rank of parameter decoder/layers_11/pre_extra_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:33.100958 140529434355712 adafactor.py:358] Since rank of parameter decoder/layers_11/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:33.121677 140339465779200 adafactor.py:358] Since rank of parameter decoder/layers_1/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:33.107615 140311617681408 adafactor.py:358] Since rank of parameter decoder/layers_1/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:33.132967 140095893518336 adafactor.py:358] Since rank of parameter decoder/layers_11/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:33.131461 140595318249472 adafactor.py:358] Since rank of parameter decoder/layers_1/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:33.114524 140529434355712 adafactor.py:358] Since rank of parameter decoder/layers_11/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:33.132607 140031916288000 adafactor.py:358] Since rank of parameter decoder/layers_10/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:33.133576 140339465779200 adafactor.py:358] Since rank of parameter decoder/layers_1/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:33.135099 139674595104768 adafactor.py:358] Since rank of parameter decoder/layers_0/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:33.139960 140440593786880 adafactor.py:358] Since rank of parameter decoder/layers_0/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:33.121421 140311617681408 adafactor.py:358] Since rank of parameter decoder/layers_1/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:33.144569 140095893518336 adafactor.py:358] Since rank of parameter decoder/layers_11/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:33.143711 140339465779200 adafactor.py:358] Since rank of parameter decoder/layers_1/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:33.144520 140031916288000 adafactor.py:358] Since rank of parameter decoder/layers_10/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:33.128066 140529434355712 adafactor.py:358] Since rank of parameter decoder/layers_11/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:33.147314 140595318249472 adafactor.py:358] Since rank of parameter decoder/layers_1/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:33.135232 140311617681408 adafactor.py:358] Since rank of parameter decoder/layers_1/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:33.157826 140095893518336 adafactor.py:358] Since rank of parameter decoder/layers_11/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:33.155674 140031916288000 adafactor.py:358] Since rank of parameter decoder/layers_10/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:33.157660 140339465779200 adafactor.py:358] Since rank of parameter decoder/layers_1/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:33.142319 140529434355712 adafactor.py:358] Since rank of parameter decoder/layers_11/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:33.161371 140595318249472 adafactor.py:358] Since rank of parameter decoder/layers_10/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:33.163353 140440593786880 adafactor.py:358] Since rank of parameter decoder/layers_0/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:33.145392 140311617681408 adafactor.py:358] Since rank of parameter decoder/layers_1/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:33.172853 140095893518336 adafactor.py:358] Since rank of parameter decoder/layers_11/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:33.170788 140031916288000 adafactor.py:358] Since rank of parameter decoder/layers_10/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:33.171641 140339465779200 adafactor.py:358] Since rank of parameter decoder/layers_1/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:33.174584 139674595104768 adafactor.py:358] Since rank of parameter decoder/layers_0/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:33.155829 140529434355712 adafactor.py:358] Since rank of parameter decoder/layers_12/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
W0512 23:42:33.155590 140311617681408 adafactor.py:358] Since rank of parameter decoder/layers_1/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:33.176220 140595318249472 adafactor.py:358] Since rank of parameter decoder/layers_10/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:33.178165 140440593786880 adafactor.py:358] Since rank of parameter decoder/layers_0/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:33.187310 140095893518336 adafactor.py:358] Since rank of parameter decoder/layers_11/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
W0512 23:42:33.185968 140031916288000 adafactor.py:358] Since rank of parameter decoder/layers_10/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:33.185416 140339465779200 adafactor.py:358] Since rank of parameter decoder/layers_1/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:33.169405 140529434355712 adafactor.py:358] Since rank of parameter decoder/layers_12/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:33.169378 140311617681408 adafactor.py:358] Since rank of parameter decoder/layers_1/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:33.190436 140595318249472 adafactor.py:358] Since rank of parameter decoder/layers_10/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:33.193906 140440593786880 adafactor.py:358] Since rank of parameter decoder/layers_1/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:33.199497 139674595104768 adafactor.py:358] Since rank of parameter decoder/layers_0/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:33.202662 140095893518336 adafactor.py:358] Since rank of parameter decoder/layers_11/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:33.199836 140339465779200 adafactor.py:358] Since rank of parameter decoder/layers_10/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:33.183228 140529434355712 adafactor.py:358] Since rank of parameter decoder/layers_12/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:33.200797 140031916288000 adafactor.py:358] Since rank of parameter decoder/layers_10/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:33.204339 140595318249472 adafactor.py:358] Since rank of parameter decoder/layers_10/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:33.208132 140440593786880 adafactor.py:358] Since rank of parameter decoder/layers_1/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:33.191278 140311617681408 adafactor.py:358] Since rank of parameter decoder/layers_1/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:33.216688 140095893518336 adafactor.py:358] Since rank of parameter decoder/layers_12/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:33.214679 140595318249472 adafactor.py:358] Since rank of parameter decoder/layers_10/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:33.213592 140339465779200 adafactor.py:358] Since rank of parameter decoder/layers_10/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:33.196841 140529434355712 adafactor.py:358] Since rank of parameter decoder/layers_12/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:33.217439 140031916288000 adafactor.py:358] Since rank of parameter decoder/layers_11/extra_mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:33.220314 139674595104768 adafactor.py:358] Since rank of parameter decoder/layers_0/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:33.222382 140440593786880 adafactor.py:358] Since rank of parameter decoder/layers_1/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:33.224851 140595318249472 adafactor.py:358] Since rank of parameter decoder/layers_10/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:33.207114 140529434355712 adafactor.py:358] Since rank of parameter decoder/layers_12/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:33.206379 140311617681408 adafactor.py:358] Since rank of parameter decoder/layers_1/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:33.227255 140339465779200 adafactor.py:358] Since rank of parameter decoder/layers_10/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:33.230046 139674595104768 adafactor.py:358] Since rank of parameter decoder/layers_0/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:33.231353 140095893518336 adafactor.py:358] Since rank of parameter decoder/layers_12/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:33.232972 140031916288000 adafactor.py:358] Since rank of parameter decoder/layers_11/extra_mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:33.217424 140529434355712 adafactor.py:358] Since rank of parameter decoder/layers_12/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:33.236579 140440593786880 adafactor.py:358] Since rank of parameter decoder/layers_1/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:33.239667 139674595104768 adafactor.py:358] Since rank of parameter decoder/layers_0/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:33.220618 140311617681408 adafactor.py:358] Since rank of parameter decoder/layers_10/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:33.240558 140595318249472 adafactor.py:358] Since rank of parameter decoder/layers_10/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:33.241274 140339465779200 adafactor.py:358] Since rank of parameter decoder/layers_10/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:33.245426 140095893518336 adafactor.py:358] Since rank of parameter decoder/layers_12/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:33.247915 140440593786880 adafactor.py:358] Since rank of parameter decoder/layers_1/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:33.247888 140031916288000 adafactor.py:358] Since rank of parameter decoder/layers_11/extra_mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:33.231623 140529434355712 adafactor.py:358] Since rank of parameter decoder/layers_12/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:33.251298 140339465779200 adafactor.py:358] Since rank of parameter decoder/layers_10/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:33.235065 140311617681408 adafactor.py:358] Since rank of parameter decoder/layers_10/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:33.259337 140095893518336 adafactor.py:358] Since rank of parameter decoder/layers_12/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:33.257616 140595318249472 adafactor.py:358] Since rank of parameter decoder/layers_10/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:33.258804 140440593786880 adafactor.py:358] Since rank of parameter decoder/layers_1/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:33.261875 139674595104768 adafactor.py:358] Since rank of parameter decoder/layers_0/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:33.261411 140339465779200 adafactor.py:358] Since rank of parameter decoder/layers_10/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:33.245679 140529434355712 adafactor.py:358] Since rank of parameter decoder/layers_12/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:33.270479 140095893518336 adafactor.py:358] Since rank of parameter decoder/layers_12/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:33.248954 140311617681408 adafactor.py:358] Since rank of parameter decoder/layers_10/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:33.272866 140595318249472 adafactor.py:358] Since rank of parameter decoder/layers_10/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:33.273921 140440593786880 adafactor.py:358] Since rank of parameter decoder/layers_1/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:33.275407 140339465779200 adafactor.py:358] Since rank of parameter decoder/layers_10/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:33.259321 140529434355712 adafactor.py:358] Since rank of parameter decoder/layers_12/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:33.281287 140095893518336 adafactor.py:358] Since rank of parameter decoder/layers_12/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:33.282953 139674595104768 adafactor.py:358] Since rank of parameter decoder/layers_0/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:33.263300 140311617681408 adafactor.py:358] Since rank of parameter decoder/layers_10/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:33.290366 140440593786880 adafactor.py:358] Since rank of parameter decoder/layers_1/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:33.289366 140339465779200 adafactor.py:358] Since rank of parameter decoder/layers_10/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:33.290891 140595318249472 adafactor.py:358] Since rank of parameter decoder/layers_11/extra_mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:33.273797 140529434355712 adafactor.py:358] Since rank of parameter decoder/layers_13/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:33.273901 140311617681408 adafactor.py:358] Since rank of parameter decoder/layers_10/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:33.295934 140095893518336 adafactor.py:358] Since rank of parameter decoder/layers_12/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:33.295821 139674595104768 adafactor.py:358] Since rank of parameter decoder/layers_0/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:33.302905 140339465779200 adafactor.py:358] Since rank of parameter decoder/layers_10/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:33.284668 140311617681408 adafactor.py:358] Since rank of parameter decoder/layers_10/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:33.287813 140529434355712 adafactor.py:358] Since rank of parameter decoder/layers_13/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:33.306879 140595318249472 adafactor.py:358] Since rank of parameter decoder/layers_11/extra_mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:33.307417 140440593786880 adafactor.py:358] Since rank of parameter decoder/layers_1/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:33.308661 139674595104768 adafactor.py:358] Since rank of parameter decoder/layers_1/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:33.310030 140095893518336 adafactor.py:358] Since rank of parameter decoder/layers_12/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:33.317195 140339465779200 adafactor.py:358] Since rank of parameter decoder/layers_11/extra_mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:33.299459 140311617681408 adafactor.py:358] Since rank of parameter decoder/layers_10/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:33.301835 140529434355712 adafactor.py:358] Since rank of parameter decoder/layers_13/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:33.321581 139674595104768 adafactor.py:358] Since rank of parameter decoder/layers_1/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:33.320978 140595318249472 adafactor.py:358] Since rank of parameter decoder/layers_11/extra_mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:33.324516 140095893518336 adafactor.py:358] Since rank of parameter decoder/layers_12/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:33.322749 140440593786880 adafactor.py:358] Since rank of parameter decoder/layers_10/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:33.331105 140339465779200 adafactor.py:358] Since rank of parameter decoder/layers_11/extra_mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:33.334540 139674595104768 adafactor.py:358] Since rank of parameter decoder/layers_1/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:33.313832 140311617681408 adafactor.py:358] Since rank of parameter decoder/layers_10/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:33.315698 140529434355712 adafactor.py:358] Since rank of parameter decoder/layers_13/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:33.337305 140440593786880 adafactor.py:358] Since rank of parameter decoder/layers_10/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:33.342128 140095893518336 adafactor.py:358] Since rank of parameter decoder/layers_13/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:33.341860 140031916288000 adafactor.py:358] Since rank of parameter decoder/layers_11/mlp/router/router_weights/w/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>, <FactorDim.NONE: None>) is ignored.
W0512 23:42:33.325706 140529434355712 adafactor.py:358] Since rank of parameter decoder/layers_13/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:33.344712 140339465779200 adafactor.py:358] Since rank of parameter decoder/layers_11/extra_mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:33.347508 139674595104768 adafactor.py:358] Since rank of parameter decoder/layers_1/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:33.327532 140311617681408 adafactor.py:358] Since rank of parameter decoder/layers_10/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:33.351416 140440593786880 adafactor.py:358] Since rank of parameter decoder/layers_10/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:33.335670 140529434355712 adafactor.py:358] Since rank of parameter decoder/layers_13/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:33.357844 140095893518336 adafactor.py:358] Since rank of parameter decoder/layers_13/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:33.357048 139674595104768 adafactor.py:358] Since rank of parameter decoder/layers_1/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:33.358449 140031916288000 adafactor.py:358] Since rank of parameter decoder/layers_11/pre_extra_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:33.341960 140311617681408 adafactor.py:358] Since rank of parameter decoder/layers_11/extra_mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:33.366513 139674595104768 adafactor.py:358] Since rank of parameter decoder/layers_1/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:33.365769 140440593786880 adafactor.py:358] Since rank of parameter decoder/layers_10/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:33.368838 140031916288000 adafactor.py:358] Since rank of parameter decoder/layers_11/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:33.351628 140529434355712 adafactor.py:358] Since rank of parameter decoder/layers_13/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:33.373408 140095893518336 adafactor.py:358] Since rank of parameter decoder/layers_13/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:33.355721 140311617681408 adafactor.py:358] Since rank of parameter decoder/layers_11/extra_mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:33.376223 140440593786880 adafactor.py:358] Since rank of parameter decoder/layers_10/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:33.379273 139674595104768 adafactor.py:358] Since rank of parameter decoder/layers_1/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:33.379059 140031916288000 adafactor.py:358] Since rank of parameter decoder/layers_11/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:33.366027 140529434355712 adafactor.py:358] Since rank of parameter decoder/layers_13/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:33.386651 140440593786880 adafactor.py:358] Since rank of parameter decoder/layers_10/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:33.389701 140095893518336 adafactor.py:358] Since rank of parameter decoder/layers_13/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:33.369440 140311617681408 adafactor.py:358] Since rank of parameter decoder/layers_11/extra_mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:33.389450 140031916288000 adafactor.py:358] Since rank of parameter decoder/layers_11/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:33.392207 139674595104768 adafactor.py:358] Since rank of parameter decoder/layers_1/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:33.380045 140529434355712 adafactor.py:358] Since rank of parameter decoder/layers_13/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:33.400923 140095893518336 adafactor.py:358] Since rank of parameter decoder/layers_13/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:33.401034 140440593786880 adafactor.py:358] Since rank of parameter decoder/layers_10/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:33.403563 140031916288000 adafactor.py:358] Since rank of parameter decoder/layers_11/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:33.405567 139674595104768 adafactor.py:358] Since rank of parameter decoder/layers_1/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:33.411329 140095893518336 adafactor.py:358] Since rank of parameter decoder/layers_13/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:33.393863 140529434355712 adafactor.py:358] Since rank of parameter decoder/layers_14/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:33.415269 140440593786880 adafactor.py:358] Since rank of parameter decoder/layers_10/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:33.418359 139674595104768 adafactor.py:358] Since rank of parameter decoder/layers_10/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:33.418689 140595318249472 adafactor.py:358] Since rank of parameter decoder/layers_11/mlp/router/router_weights/w/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>, <FactorDim.NONE: None>) is ignored.
W0512 23:42:33.418587 140031916288000 adafactor.py:358] Since rank of parameter decoder/layers_11/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:33.425433 140095893518336 adafactor.py:358] Since rank of parameter decoder/layers_13/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:33.408479 140529434355712 adafactor.py:358] Since rank of parameter decoder/layers_14/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:33.429378 140440593786880 adafactor.py:358] Since rank of parameter decoder/layers_10/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:33.431227 139674595104768 adafactor.py:358] Since rank of parameter decoder/layers_10/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:33.432800 140339465779200 adafactor.py:358] Since rank of parameter decoder/layers_11/mlp/router/router_weights/w/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>, <FactorDim.NONE: None>) is ignored.
W0512 23:42:33.434508 140031916288000 adafactor.py:358] Since rank of parameter decoder/layers_11/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:33.436110 140595318249472 adafactor.py:358] Since rank of parameter decoder/layers_11/pre_extra_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:33.440362 140095893518336 adafactor.py:358] Since rank of parameter decoder/layers_13/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:33.422264 140529434355712 adafactor.py:358] Since rank of parameter decoder/layers_14/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:33.444027 139674595104768 adafactor.py:358] Since rank of parameter decoder/layers_10/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:33.444372 140440593786880 adafactor.py:358] Since rank of parameter decoder/layers_11/extra_mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:33.447599 140595318249472 adafactor.py:358] Since rank of parameter decoder/layers_11/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:33.447996 140339465779200 adafactor.py:358] Since rank of parameter decoder/layers_11/pre_extra_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:33.449734 140031916288000 adafactor.py:358] Since rank of parameter decoder/layers_12/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:33.454689 140095893518336 adafactor.py:358] Since rank of parameter decoder/layers_13/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:33.436158 140529434355712 adafactor.py:358] Since rank of parameter decoder/layers_14/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:33.456796 139674595104768 adafactor.py:358] Since rank of parameter decoder/layers_10/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:33.458085 140595318249472 adafactor.py:358] Since rank of parameter decoder/layers_11/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:33.458640 140440593786880 adafactor.py:358] Since rank of parameter decoder/layers_11/extra_mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:33.458030 140339465779200 adafactor.py:358] Since rank of parameter decoder/layers_11/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:33.446243 140529434355712 adafactor.py:358] Since rank of parameter decoder/layers_14/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:33.464514 140031916288000 adafactor.py:358] Since rank of parameter decoder/layers_12/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:33.468396 140095893518336 adafactor.py:358] Since rank of parameter decoder/layers_14/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:33.468846 139674595104768 adafactor.py:358] Since rank of parameter decoder/layers_10/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:33.468559 140595318249472 adafactor.py:358] Since rank of parameter decoder/layers_11/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:33.468131 140339465779200 adafactor.py:358] Since rank of parameter decoder/layers_11/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:33.473259 140440593786880 adafactor.py:358] Since rank of parameter decoder/layers_11/extra_mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:33.456241 140529434355712 adafactor.py:358] Since rank of parameter decoder/layers_14/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:33.478498 139674595104768 adafactor.py:358] Since rank of parameter decoder/layers_10/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:33.458508 140311617681408 adafactor.py:358] Since rank of parameter decoder/layers_11/mlp/router/router_weights/w/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>, <FactorDim.NONE: None>) is ignored.
W0512 23:42:33.478553 140031916288000 adafactor.py:358] Since rank of parameter decoder/layers_12/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:33.478479 140339465779200 adafactor.py:358] Since rank of parameter decoder/layers_11/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:33.483331 140095893518336 adafactor.py:358] Since rank of parameter decoder/layers_14/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:33.482682 140595318249472 adafactor.py:358] Since rank of parameter decoder/layers_11/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:33.491561 139674595104768 adafactor.py:358] Since rank of parameter decoder/layers_10/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:33.472173 140529434355712 adafactor.py:358] Since rank of parameter decoder/layers_14/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:33.492552 140031916288000 adafactor.py:358] Since rank of parameter decoder/layers_12/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:33.474010 140311617681408 adafactor.py:358] Since rank of parameter decoder/layers_11/pre_extra_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:33.492542 140339465779200 adafactor.py:358] Since rank of parameter decoder/layers_11/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:33.497291 140095893518336 adafactor.py:358] Since rank of parameter decoder/layers_14/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:33.496638 140595318249472 adafactor.py:358] Since rank of parameter decoder/layers_11/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:33.504390 139674595104768 adafactor.py:358] Since rank of parameter decoder/layers_10/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:33.502788 140031916288000 adafactor.py:358] Since rank of parameter decoder/layers_12/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:33.485831 140529434355712 adafactor.py:358] Since rank of parameter decoder/layers_14/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:33.484178 140311617681408 adafactor.py:358] Since rank of parameter decoder/layers_11/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:33.506630 140339465779200 adafactor.py:358] Since rank of parameter decoder/layers_11/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:33.511234 140095893518336 adafactor.py:358] Since rank of parameter decoder/layers_14/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:33.511780 140595318249472 adafactor.py:358] Since rank of parameter decoder/layers_11/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:33.512935 140031916288000 adafactor.py:358] Since rank of parameter decoder/layers_12/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:33.494331 140311617681408 adafactor.py:358] Since rank of parameter decoder/layers_11/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:33.517163 139674595104768 adafactor.py:358] Since rank of parameter decoder/layers_10/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:33.499721 140529434355712 adafactor.py:358] Since rank of parameter decoder/layers_14/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:33.521610 140095893518336 adafactor.py:358] Since rank of parameter decoder/layers_14/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:33.521184 140339465779200 adafactor.py:358] Since rank of parameter decoder/layers_11/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:33.504424 140311617681408 adafactor.py:358] Since rank of parameter decoder/layers_11/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:33.525728 140595318249472 adafactor.py:358] Since rank of parameter decoder/layers_12/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:33.527047 140031916288000 adafactor.py:358] Since rank of parameter decoder/layers_12/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:33.530668 139674595104768 adafactor.py:358] Since rank of parameter decoder/layers_11/extra_mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:33.532130 140095893518336 adafactor.py:358] Since rank of parameter decoder/layers_14/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:33.513347 140529434355712 adafactor.py:358] Since rank of parameter decoder/layers_15/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:33.535028 140339465779200 adafactor.py:358] Since rank of parameter decoder/layers_12/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:33.518202 140311617681408 adafactor.py:358] Since rank of parameter decoder/layers_11/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:33.539762 140595318249472 adafactor.py:358] Since rank of parameter decoder/layers_12/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:33.541251 140031916288000 adafactor.py:358] Since rank of parameter decoder/layers_12/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:33.543584 139674595104768 adafactor.py:358] Since rank of parameter decoder/layers_11/extra_mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:33.527106 140529434355712 adafactor.py:358] Since rank of parameter decoder/layers_15/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:33.547421 140095893518336 adafactor.py:358] Since rank of parameter decoder/layers_14/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:33.548798 140339465779200 adafactor.py:358] Since rank of parameter decoder/layers_12/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:33.532195 140311617681408 adafactor.py:358] Since rank of parameter decoder/layers_11/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:33.554185 140595318249472 adafactor.py:358] Since rank of parameter decoder/layers_12/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:33.556492 139674595104768 adafactor.py:358] Since rank of parameter decoder/layers_11/extra_mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:33.555213 140031916288000 adafactor.py:358] Since rank of parameter decoder/layers_12/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:33.561644 140095893518336 adafactor.py:358] Since rank of parameter decoder/layers_14/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:33.541633 140529434355712 adafactor.py:358] Since rank of parameter decoder/layers_15/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:33.562467 140339465779200 adafactor.py:358] Since rank of parameter decoder/layers_12/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:33.546990 140311617681408 adafactor.py:358] Since rank of parameter decoder/layers_11/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:33.568315 140595318249472 adafactor.py:358] Since rank of parameter decoder/layers_12/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:33.570122 140440593786880 adafactor.py:358] Since rank of parameter decoder/layers_11/mlp/router/router_weights/w/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>, <FactorDim.NONE: None>) is ignored.
W0512 23:42:33.569939 140031916288000 adafactor.py:358] Since rank of parameter decoder/layers_13/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:33.555327 140529434355712 adafactor.py:358] Since rank of parameter decoder/layers_15/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:33.575934 140095893518336 adafactor.py:358] Since rank of parameter decoder/layers_14/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:33.576100 140339465779200 adafactor.py:358] Since rank of parameter decoder/layers_12/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:33.578706 140595318249472 adafactor.py:358] Since rank of parameter decoder/layers_12/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:33.561020 140311617681408 adafactor.py:358] Since rank of parameter decoder/layers_12/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:33.565341 140529434355712 adafactor.py:358] Since rank of parameter decoder/layers_15/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:33.583908 140031916288000 adafactor.py:358] Since rank of parameter decoder/layers_13/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:33.585956 140440593786880 adafactor.py:358] Since rank of parameter decoder/layers_11/pre_extra_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:33.586389 140339465779200 adafactor.py:358] Since rank of parameter decoder/layers_12/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:33.590862 140095893518336 adafactor.py:358] Since rank of parameter decoder/layers_15/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:33.589687 140595318249472 adafactor.py:358] Since rank of parameter decoder/layers_12/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:33.575294 140529434355712 adafactor.py:358] Since rank of parameter decoder/layers_15/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:33.574854 140311617681408 adafactor.py:358] Since rank of parameter decoder/layers_12/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:33.596550 140440593786880 adafactor.py:358] Since rank of parameter decoder/layers_11/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:33.596718 140339465779200 adafactor.py:358] Since rank of parameter decoder/layers_12/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:33.597773 140031916288000 adafactor.py:358] Since rank of parameter decoder/layers_13/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:33.605928 140095893518336 adafactor.py:358] Since rank of parameter decoder/layers_15/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:33.588794 140529434355712 adafactor.py:358] Since rank of parameter decoder/layers_15/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:33.606993 140440593786880 adafactor.py:358] Since rank of parameter decoder/layers_11/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:33.608242 140595318249472 adafactor.py:358] Since rank of parameter decoder/layers_12/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:33.588714 140311617681408 adafactor.py:358] Since rank of parameter decoder/layers_12/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:33.610625 140339465779200 adafactor.py:358] Since rank of parameter decoder/layers_12/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:33.612829 140031916288000 adafactor.py:358] Since rank of parameter decoder/layers_13/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:33.621918 140095893518336 adafactor.py:358] Since rank of parameter decoder/layers_15/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:33.602363 140529434355712 adafactor.py:358] Since rank of parameter decoder/layers_15/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:33.620689 140440593786880 adafactor.py:358] Since rank of parameter decoder/layers_11/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:33.602607 140311617681408 adafactor.py:358] Since rank of parameter decoder/layers_12/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:33.624604 140595318249472 adafactor.py:358] Since rank of parameter decoder/layers_12/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:33.624216 140339465779200 adafactor.py:358] Since rank of parameter decoder/layers_12/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:33.624582 140031916288000 adafactor.py:358] Since rank of parameter decoder/layers_13/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:33.612786 140311617681408 adafactor.py:358] Since rank of parameter decoder/layers_12/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:33.615884 140529434355712 adafactor.py:358] Since rank of parameter decoder/layers_15/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:33.637265 140095893518336 adafactor.py:358] Since rank of parameter decoder/layers_15/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:33.635616 140031916288000 adafactor.py:358] Since rank of parameter decoder/layers_13/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:33.637445 140440593786880 adafactor.py:358] Since rank of parameter decoder/layers_11/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:33.637756 140339465779200 adafactor.py:358] Since rank of parameter decoder/layers_12/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:33.641987 139674595104768 adafactor.py:358] Since rank of parameter decoder/layers_11/mlp/router/router_weights/w/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>, <FactorDim.NONE: None>) is ignored.
W0512 23:42:33.640850 140595318249472 adafactor.py:358] Since rank of parameter decoder/layers_12/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:33.622970 140311617681408 adafactor.py:358] Since rank of parameter decoder/layers_12/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:33.648907 140095893518336 adafactor.py:358] Since rank of parameter decoder/layers_15/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:33.629446 140529434355712 adafactor.py:358] Since rank of parameter decoder/layers_16/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:33.650186 140031916288000 adafactor.py:358] Since rank of parameter decoder/layers_13/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:33.652013 140440593786880 adafactor.py:358] Since rank of parameter decoder/layers_11/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:33.651859 140339465779200 adafactor.py:358] Since rank of parameter decoder/layers_13/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:33.656653 139674595104768 adafactor.py:358] Since rank of parameter decoder/layers_11/pre_extra_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:33.637105 140311617681408 adafactor.py:358] Since rank of parameter decoder/layers_12/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:33.657256 140595318249472 adafactor.py:358] Since rank of parameter decoder/layers_13/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:33.642998 140529434355712 adafactor.py:358] Since rank of parameter decoder/layers_16/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:33.664314 140095893518336 adafactor.py:358] Since rank of parameter decoder/layers_15/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:33.666180 139674595104768 adafactor.py:358] Since rank of parameter decoder/layers_11/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:33.665504 140339465779200 adafactor.py:358] Since rank of parameter decoder/layers_13/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:33.666692 140031916288000 adafactor.py:358] Since rank of parameter decoder/layers_13/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:33.667863 140440593786880 adafactor.py:358] Since rank of parameter decoder/layers_11/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:33.650858 140311617681408 adafactor.py:358] Since rank of parameter decoder/layers_12/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:33.672878 140595318249472 adafactor.py:358] Since rank of parameter decoder/layers_13/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:33.675607 139674595104768 adafactor.py:358] Since rank of parameter decoder/layers_11/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:33.656536 140529434355712 adafactor.py:358] Since rank of parameter decoder/layers_16/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:33.679358 140095893518336 adafactor.py:358] Since rank of parameter decoder/layers_15/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:33.679063 140339465779200 adafactor.py:358] Since rank of parameter decoder/layers_13/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:33.681985 140031916288000 adafactor.py:358] Since rank of parameter decoder/layers_13/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:33.685002 139674595104768 adafactor.py:358] Since rank of parameter decoder/layers_11/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:33.683842 140440593786880 adafactor.py:358] Since rank of parameter decoder/layers_12/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
W0512 23:42:33.664858 140311617681408 adafactor.py:358] Since rank of parameter decoder/layers_12/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:33.687699 140595318249472 adafactor.py:358] Since rank of parameter decoder/layers_13/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:33.670565 140529434355712 adafactor.py:358] Since rank of parameter decoder/layers_16/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:33.693317 140095893518336 adafactor.py:358] Since rank of parameter decoder/layers_15/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:33.692780 140339465779200 adafactor.py:358] Since rank of parameter decoder/layers_13/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:33.697881 139674595104768 adafactor.py:358] Since rank of parameter decoder/layers_11/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:33.698635 140440593786880 adafactor.py:358] Since rank of parameter decoder/layers_12/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:33.680530 140529434355712 adafactor.py:358] Since rank of parameter decoder/layers_16/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:33.698047 140031916288000 adafactor.py:358] Since rank of parameter decoder/layers_14/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:33.679282 140311617681408 adafactor.py:358] Since rank of parameter decoder/layers_13/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:33.703721 140595318249472 adafactor.py:358] Since rank of parameter decoder/layers_13/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:33.702954 140339465779200 adafactor.py:358] Since rank of parameter decoder/layers_13/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:33.707168 140095893518336 adafactor.py:358] Since rank of parameter decoder/layers_15/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:33.690445 140529434355712 adafactor.py:358] Since rank of parameter decoder/layers_16/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:33.710749 139674595104768 adafactor.py:358] Since rank of parameter decoder/layers_11/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:33.692992 140311617681408 adafactor.py:358] Since rank of parameter decoder/layers_13/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:33.713693 140440593786880 adafactor.py:358] Since rank of parameter decoder/layers_12/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:33.714274 140595318249472 adafactor.py:358] Since rank of parameter decoder/layers_13/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:33.713259 140339465779200 adafactor.py:358] Since rank of parameter decoder/layers_13/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:33.716467 140031916288000 adafactor.py:358] Since rank of parameter decoder/layers_14/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:33.721381 140095893518336 adafactor.py:358] Since rank of parameter decoder/layers_16/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:33.703978 140529434355712 adafactor.py:358] Since rank of parameter decoder/layers_16/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:33.724259 139674595104768 adafactor.py:358] Since rank of parameter decoder/layers_11/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:33.725773 140595318249472 adafactor.py:358] Since rank of parameter decoder/layers_13/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:33.706784 140311617681408 adafactor.py:358] Since rank of parameter decoder/layers_13/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:33.726954 140339465779200 adafactor.py:358] Since rank of parameter decoder/layers_13/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:33.735562 140095893518336 adafactor.py:358] Since rank of parameter decoder/layers_16/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:33.732945 140031916288000 adafactor.py:358] Since rank of parameter decoder/layers_14/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:33.717570 140529434355712 adafactor.py:358] Since rank of parameter decoder/layers_16/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:33.737047 139674595104768 adafactor.py:358] Since rank of parameter decoder/layers_12/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:33.720498 140311617681408 adafactor.py:358] Since rank of parameter decoder/layers_13/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:33.741674 140595318249472 adafactor.py:358] Since rank of parameter decoder/layers_13/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:33.740503 140339465779200 adafactor.py:358] Since rank of parameter decoder/layers_13/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:33.749997 139674595104768 adafactor.py:358] Since rank of parameter decoder/layers_12/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:33.750203 140095893518336 adafactor.py:358] Since rank of parameter decoder/layers_16/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:33.748134 140031916288000 adafactor.py:358] Since rank of parameter decoder/layers_14/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:33.731256 140529434355712 adafactor.py:358] Since rank of parameter decoder/layers_16/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:33.730629 140311617681408 adafactor.py:358] Since rank of parameter decoder/layers_13/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:33.752460 140440593786880 adafactor.py:358] Since rank of parameter decoder/layers_12/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:33.754024 140339465779200 adafactor.py:358] Since rank of parameter decoder/layers_13/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:33.756767 140595318249472 adafactor.py:358] Since rank of parameter decoder/layers_13/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
W0512 23:42:33.758553 140031916288000 adafactor.py:358] Since rank of parameter decoder/layers_14/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:33.740759 140311617681408 adafactor.py:358] Since rank of parameter decoder/layers_13/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:33.762963 139674595104768 adafactor.py:358] Since rank of parameter decoder/layers_12/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:33.764234 140095893518336 adafactor.py:358] Since rank of parameter decoder/layers_16/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:33.764395 140440593786880 adafactor.py:358] Since rank of parameter decoder/layers_12/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:33.746896 140529434355712 adafactor.py:358] Since rank of parameter decoder/layers_17/extra_mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:33.768144 140339465779200 adafactor.py:358] Since rank of parameter decoder/layers_14/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:33.770772 140595318249472 adafactor.py:358] Since rank of parameter decoder/layers_13/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:33.770338 140031916288000 adafactor.py:358] Since rank of parameter decoder/layers_14/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
W0512 23:42:33.775468 140095893518336 adafactor.py:358] Since rank of parameter decoder/layers_16/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:33.775853 139674595104768 adafactor.py:358] Since rank of parameter decoder/layers_12/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:33.755429 140311617681408 adafactor.py:358] Since rank of parameter decoder/layers_13/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:33.777022 140440593786880 adafactor.py:358] Since rank of parameter decoder/layers_12/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:33.762761 140529434355712 adafactor.py:358] Since rank of parameter decoder/layers_17/extra_mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:33.781725 140339465779200 adafactor.py:358] Since rank of parameter decoder/layers_14/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:33.785981 140095893518336 adafactor.py:358] Since rank of parameter decoder/layers_16/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:33.785254 139674595104768 adafactor.py:358] Since rank of parameter decoder/layers_12/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:33.784619 140595318249472 adafactor.py:358] Since rank of parameter decoder/layers_14/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:33.786336 140031916288000 adafactor.py:358] Since rank of parameter decoder/layers_14/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:33.770835 140311617681408 adafactor.py:358] Since rank of parameter decoder/layers_13/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:33.791881 140440593786880 adafactor.py:358] Since rank of parameter decoder/layers_12/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:33.794636 139674595104768 adafactor.py:358] Since rank of parameter decoder/layers_12/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:33.777425 140529434355712 adafactor.py:358] Since rank of parameter decoder/layers_17/extra_mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:33.795452 140339465779200 adafactor.py:358] Since rank of parameter decoder/layers_14/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:33.800510 140095893518336 adafactor.py:358] Since rank of parameter decoder/layers_16/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:33.799526 140595318249472 adafactor.py:358] Since rank of parameter decoder/layers_14/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
W0512 23:42:33.802043 140031916288000 adafactor.py:358] Since rank of parameter decoder/layers_14/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:33.786084 140311617681408 adafactor.py:358] Since rank of parameter decoder/layers_13/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:33.806456 140440593786880 adafactor.py:358] Since rank of parameter decoder/layers_12/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:33.807757 139674595104768 adafactor.py:358] Since rank of parameter decoder/layers_12/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:33.809433 140339465779200 adafactor.py:358] Since rank of parameter decoder/layers_14/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:33.814874 140095893518336 adafactor.py:358] Since rank of parameter decoder/layers_16/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:33.814722 140595318249472 adafactor.py:358] Since rank of parameter decoder/layers_14/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:33.817561 140031916288000 adafactor.py:358] Since rank of parameter decoder/layers_14/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:33.819411 140339465779200 adafactor.py:358] Since rank of parameter decoder/layers_14/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:33.800989 140311617681408 adafactor.py:358] Since rank of parameter decoder/layers_14/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:33.821238 140440593786880 adafactor.py:358] Since rank of parameter decoder/layers_12/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:33.826136 139674595104768 adafactor.py:358] Since rank of parameter decoder/layers_12/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:33.828775 140095893518336 adafactor.py:358] Since rank of parameter decoder/layers_16/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:33.830352 140595318249472 adafactor.py:358] Since rank of parameter decoder/layers_14/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:33.829519 140339465779200 adafactor.py:358] Since rank of parameter decoder/layers_14/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:33.832224 140031916288000 adafactor.py:358] Since rank of parameter decoder/layers_15/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:33.836743 140440593786880 adafactor.py:358] Since rank of parameter decoder/layers_13/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:33.817632 140311617681408 adafactor.py:358] Since rank of parameter decoder/layers_14/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:33.839226 139674595104768 adafactor.py:358] Since rank of parameter decoder/layers_12/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:33.842841 140095893518336 adafactor.py:358] Since rank of parameter decoder/layers_17/extra_mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:33.842535 140595318249472 adafactor.py:358] Since rank of parameter decoder/layers_14/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:33.843049 140339465779200 adafactor.py:358] Since rank of parameter decoder/layers_14/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:33.846340 140031916288000 adafactor.py:358] Since rank of parameter decoder/layers_15/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:33.851063 140440593786880 adafactor.py:358] Since rank of parameter decoder/layers_13/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:33.853008 139674595104768 adafactor.py:358] Since rank of parameter decoder/layers_13/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:33.832866 140311617681408 adafactor.py:358] Since rank of parameter decoder/layers_14/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:33.854177 140595318249472 adafactor.py:358] Since rank of parameter decoder/layers_14/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:33.857669 140095893518336 adafactor.py:358] Since rank of parameter decoder/layers_17/extra_mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:33.837801 140529434355712 adafactor.py:358] Since rank of parameter decoder/layers_17/mlp/router/router_weights/w/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>, <FactorDim.NONE: None>) is ignored.
W0512 23:42:33.856657 140339465779200 adafactor.py:358] Since rank of parameter decoder/layers_14/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:33.860289 140031916288000 adafactor.py:358] Since rank of parameter decoder/layers_15/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:33.865908 139674595104768 adafactor.py:358] Since rank of parameter decoder/layers_13/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:33.865348 140440593786880 adafactor.py:358] Since rank of parameter decoder/layers_13/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:33.847422 140311617681408 adafactor.py:358] Since rank of parameter decoder/layers_14/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:33.849038 140529434355712 adafactor.py:358] Since rank of parameter decoder/layers_17/pre_extra_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:33.871854 140095893518336 adafactor.py:358] Since rank of parameter decoder/layers_17/extra_mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:33.869626 140595318249472 adafactor.py:358] Since rank of parameter decoder/layers_14/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:33.870503 140339465779200 adafactor.py:358] Since rank of parameter decoder/layers_14/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:33.875220 140031916288000 adafactor.py:358] Since rank of parameter decoder/layers_15/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:33.878844 139674595104768 adafactor.py:358] Since rank of parameter decoder/layers_13/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:33.860021 140529434355712 adafactor.py:358] Since rank of parameter decoder/layers_17/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:33.859207 140311617681408 adafactor.py:358] Since rank of parameter decoder/layers_14/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:33.879599 140440593786880 adafactor.py:358] Since rank of parameter decoder/layers_13/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:33.884026 140339465779200 adafactor.py:358] Since rank of parameter decoder/layers_15/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:33.885834 140595318249472 adafactor.py:358] Since rank of parameter decoder/layers_14/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:33.885575 140031916288000 adafactor.py:358] Since rank of parameter decoder/layers_15/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:33.871587 140529434355712 adafactor.py:358] Since rank of parameter decoder/layers_17/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:33.891780 139674595104768 adafactor.py:358] Since rank of parameter decoder/layers_13/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:33.870541 140311617681408 adafactor.py:358] Since rank of parameter decoder/layers_14/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:33.893064 140440593786880 adafactor.py:358] Since rank of parameter decoder/layers_13/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:33.896189 140031916288000 adafactor.py:358] Since rank of parameter decoder/layers_15/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:33.898227 140339465779200 adafactor.py:358] Since rank of parameter decoder/layers_15/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:33.901274 139674595104768 adafactor.py:358] Since rank of parameter decoder/layers_13/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:33.882196 140529434355712 adafactor.py:358] Since rank of parameter decoder/layers_17/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:33.902074 140595318249472 adafactor.py:358] Since rank of parameter decoder/layers_14/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:33.903598 140440593786880 adafactor.py:358] Since rank of parameter decoder/layers_13/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:33.886150 140311617681408 adafactor.py:358] Since rank of parameter decoder/layers_14/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:33.910739 139674595104768 adafactor.py:358] Since rank of parameter decoder/layers_13/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:33.911181 140031916288000 adafactor.py:358] Since rank of parameter decoder/layers_15/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:33.911967 140339465779200 adafactor.py:358] Since rank of parameter decoder/layers_15/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:33.897053 140529434355712 adafactor.py:358] Since rank of parameter decoder/layers_17/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:33.916622 140595318249472 adafactor.py:358] Since rank of parameter decoder/layers_15/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:33.917778 140440593786880 adafactor.py:358] Since rank of parameter decoder/layers_13/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:33.900788 140311617681408 adafactor.py:358] Since rank of parameter decoder/layers_14/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:33.923605 139674595104768 adafactor.py:358] Since rank of parameter decoder/layers_13/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:33.925340 140031916288000 adafactor.py:358] Since rank of parameter decoder/layers_15/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:33.925575 140339465779200 adafactor.py:358] Since rank of parameter decoder/layers_15/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:33.929771 140095893518336 adafactor.py:358] Since rank of parameter decoder/layers_17/mlp/router/router_weights/w/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>, <FactorDim.NONE: None>) is ignored.
W0512 23:42:33.911093 140529434355712 adafactor.py:358] Since rank of parameter decoder/layers_17/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:33.931257 140595318249472 adafactor.py:358] Since rank of parameter decoder/layers_15/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:33.931981 140440593786880 adafactor.py:358] Since rank of parameter decoder/layers_13/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:33.915185 140311617681408 adafactor.py:358] Since rank of parameter decoder/layers_14/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:33.936459 139674595104768 adafactor.py:358] Since rank of parameter decoder/layers_13/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:33.935674 140339465779200 adafactor.py:358] Since rank of parameter decoder/layers_15/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:33.940264 140095893518336 adafactor.py:358] Since rank of parameter decoder/layers_17/pre_extra_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:33.939123 140031916288000 adafactor.py:358] Since rank of parameter decoder/layers_15/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:33.927996 140529434355712 adafactor.py:358] Since rank of parameter decoder/layers_17/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:33.946313 140440593786880 adafactor.py:358] Since rank of parameter decoder/layers_13/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:33.945767 140339465779200 adafactor.py:358] Since rank of parameter decoder/layers_15/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:33.947485 140595318249472 adafactor.py:358] Since rank of parameter decoder/layers_15/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:33.949244 139674595104768 adafactor.py:358] Since rank of parameter decoder/layers_13/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:33.950586 140095893518336 adafactor.py:358] Since rank of parameter decoder/layers_17/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:33.930263 140311617681408 adafactor.py:358] Since rank of parameter decoder/layers_15/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:33.953053 140031916288000 adafactor.py:358] Since rank of parameter decoder/layers_16/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:33.959366 140339465779200 adafactor.py:358] Since rank of parameter decoder/layers_15/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:33.942496 140529434355712 adafactor.py:358] Since rank of parameter decoder/layers_18/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:33.961998 139674595104768 adafactor.py:358] Since rank of parameter decoder/layers_14/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:33.960715 140440593786880 adafactor.py:358] Since rank of parameter decoder/layers_14/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:33.963118 140095893518336 adafactor.py:358] Since rank of parameter decoder/layers_17/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:33.944595 140311617681408 adafactor.py:358] Since rank of parameter decoder/layers_15/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:33.965522 140595318249472 adafactor.py:358] Since rank of parameter decoder/layers_15/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:33.969029 140031916288000 adafactor.py:358] Since rank of parameter decoder/layers_16/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:33.973370 140095893518336 adafactor.py:358] Since rank of parameter decoder/layers_17/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:33.975470 139674595104768 adafactor.py:358] Since rank of parameter decoder/layers_14/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:33.973030 140339465779200 adafactor.py:358] Since rank of parameter decoder/layers_15/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:33.956312 140529434355712 adafactor.py:358] Since rank of parameter decoder/layers_18/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:33.975509 140440593786880 adafactor.py:358] Since rank of parameter decoder/layers_14/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:33.978010 140595318249472 adafactor.py:358] Since rank of parameter decoder/layers_15/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:33.959575 140311617681408 adafactor.py:358] Since rank of parameter decoder/layers_15/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:33.984411 140031916288000 adafactor.py:358] Since rank of parameter decoder/layers_16/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:33.988100 140095893518336 adafactor.py:358] Since rank of parameter decoder/layers_17/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:33.988374 139674595104768 adafactor.py:358] Since rank of parameter decoder/layers_14/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:33.986715 140339465779200 adafactor.py:358] Since rank of parameter decoder/layers_15/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:33.970094 140529434355712 adafactor.py:358] Since rank of parameter decoder/layers_18/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:33.988721 140595318249472 adafactor.py:358] Since rank of parameter decoder/layers_15/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:33.989630 140440593786880 adafactor.py:358] Since rank of parameter decoder/layers_14/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:33.974585 140311617681408 adafactor.py:358] Since rank of parameter decoder/layers_15/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:34.001214 139674595104768 adafactor.py:358] Since rank of parameter decoder/layers_14/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:34.002155 140095893518336 adafactor.py:358] Since rank of parameter decoder/layers_17/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:33.999984 140031916288000 adafactor.py:358] Since rank of parameter decoder/layers_16/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:34.000302 140339465779200 adafactor.py:358] Since rank of parameter decoder/layers_16/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:33.983784 140529434355712 adafactor.py:358] Since rank of parameter decoder/layers_18/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:34.003766 140440593786880 adafactor.py:358] Since rank of parameter decoder/layers_14/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:33.984915 140311617681408 adafactor.py:358] Since rank of parameter decoder/layers_15/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:34.005631 140595318249472 adafactor.py:358] Since rank of parameter decoder/layers_15/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:34.010626 139674595104768 adafactor.py:358] Since rank of parameter decoder/layers_14/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:33.993860 140529434355712 adafactor.py:358] Since rank of parameter decoder/layers_18/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:34.011275 140031916288000 adafactor.py:358] Since rank of parameter decoder/layers_16/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:34.014303 140440593786880 adafactor.py:358] Since rank of parameter decoder/layers_14/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:34.013942 140339465779200 adafactor.py:358] Since rank of parameter decoder/layers_16/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:33.995615 140311617681408 adafactor.py:358] Since rank of parameter decoder/layers_15/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:34.019620 140095893518336 adafactor.py:358] Since rank of parameter decoder/layers_17/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:34.020094 139674595104768 adafactor.py:358] Since rank of parameter decoder/layers_14/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:34.021690 140595318249472 adafactor.py:358] Since rank of parameter decoder/layers_15/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:34.003851 140529434355712 adafactor.py:358] Since rank of parameter decoder/layers_18/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:34.021761 140031916288000 adafactor.py:358] Since rank of parameter decoder/layers_16/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:34.024812 140440593786880 adafactor.py:358] Since rank of parameter decoder/layers_14/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:34.027636 140339465779200 adafactor.py:358] Since rank of parameter decoder/layers_16/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:34.009685 140311617681408 adafactor.py:358] Since rank of parameter decoder/layers_15/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:34.033606 140095893518336 adafactor.py:358] Since rank of parameter decoder/layers_18/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:34.032902 139674595104768 adafactor.py:358] Since rank of parameter decoder/layers_14/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:34.017434 140529434355712 adafactor.py:358] Since rank of parameter decoder/layers_18/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:34.036323 140031916288000 adafactor.py:358] Since rank of parameter decoder/layers_16/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:34.037858 140595318249472 adafactor.py:358] Since rank of parameter decoder/layers_15/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:34.039025 140440593786880 adafactor.py:358] Since rank of parameter decoder/layers_14/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:34.041835 140339465779200 adafactor.py:358] Since rank of parameter decoder/layers_16/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:34.024194 140311617681408 adafactor.py:358] Since rank of parameter decoder/layers_15/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:34.045719 139674595104768 adafactor.py:358] Since rank of parameter decoder/layers_14/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:34.048392 140095893518336 adafactor.py:358] Since rank of parameter decoder/layers_18/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:34.031413 140529434355712 adafactor.py:358] Since rank of parameter decoder/layers_18/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:34.050325 140031916288000 adafactor.py:358] Since rank of parameter decoder/layers_16/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:34.052022 140339465779200 adafactor.py:358] Since rank of parameter decoder/layers_16/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:34.053394 140440593786880 adafactor.py:358] Since rank of parameter decoder/layers_14/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:34.054570 140595318249472 adafactor.py:358] Since rank of parameter decoder/layers_16/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:34.058526 139674595104768 adafactor.py:358] Since rank of parameter decoder/layers_14/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:34.038182 140311617681408 adafactor.py:358] Since rank of parameter decoder/layers_15/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:34.062871 140095893518336 adafactor.py:358] Since rank of parameter decoder/layers_18/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:34.045078 140529434355712 adafactor.py:358] Since rank of parameter decoder/layers_18/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:34.062055 140339465779200 adafactor.py:358] Since rank of parameter decoder/layers_16/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:34.064443 140031916288000 adafactor.py:358] Since rank of parameter decoder/layers_16/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:34.067817 140440593786880 adafactor.py:358] Since rank of parameter decoder/layers_14/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:34.071361 139674595104768 adafactor.py:358] Since rank of parameter decoder/layers_15/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:34.070337 140595318249472 adafactor.py:358] Since rank of parameter decoder/layers_16/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:34.052133 140311617681408 adafactor.py:358] Since rank of parameter decoder/layers_16/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:34.077236 140095893518336 adafactor.py:358] Since rank of parameter decoder/layers_18/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:34.058638 140529434355712 adafactor.py:358] Since rank of parameter decoder/layers_19/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:34.075703 140339465779200 adafactor.py:358] Since rank of parameter decoder/layers_16/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:34.078473 140031916288000 adafactor.py:358] Since rank of parameter decoder/layers_17/extra_mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:34.081978 140440593786880 adafactor.py:358] Since rank of parameter decoder/layers_15/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:34.084188 139674595104768 adafactor.py:358] Since rank of parameter decoder/layers_15/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:34.084676 140595318249472 adafactor.py:358] Since rank of parameter decoder/layers_16/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:34.066077 140311617681408 adafactor.py:358] Since rank of parameter decoder/layers_16/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:34.088056 140095893518336 adafactor.py:358] Since rank of parameter decoder/layers_18/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:34.072437 140529434355712 adafactor.py:358] Since rank of parameter decoder/layers_19/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:34.090218 140339465779200 adafactor.py:358] Since rank of parameter decoder/layers_16/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:34.092502 140031916288000 adafactor.py:358] Since rank of parameter decoder/layers_17/extra_mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:34.096916 139674595104768 adafactor.py:358] Since rank of parameter decoder/layers_15/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:34.098785 140095893518336 adafactor.py:358] Since rank of parameter decoder/layers_18/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:34.097695 140440593786880 adafactor.py:358] Since rank of parameter decoder/layers_15/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:34.099422 140595318249472 adafactor.py:358] Since rank of parameter decoder/layers_16/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:34.080006 140311617681408 adafactor.py:358] Since rank of parameter decoder/layers_16/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:34.087252 140529434355712 adafactor.py:358] Since rank of parameter decoder/layers_19/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:34.104981 140339465779200 adafactor.py:358] Since rank of parameter decoder/layers_16/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:34.110263 139674595104768 adafactor.py:358] Since rank of parameter decoder/layers_15/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:34.111084 140595318249472 adafactor.py:358] Since rank of parameter decoder/layers_16/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:34.113765 140095893518336 adafactor.py:358] Since rank of parameter decoder/layers_18/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:34.112354 140440593786880 adafactor.py:358] Since rank of parameter decoder/layers_15/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:34.093894 140311617681408 adafactor.py:358] Since rank of parameter decoder/layers_16/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:34.114392 140031916288000 adafactor.py:358] Since rank of parameter decoder/layers_17/extra_mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:34.119742 139674595104768 adafactor.py:358] Since rank of parameter decoder/layers_15/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:34.101002 140529434355712 adafactor.py:358] Since rank of parameter decoder/layers_19/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:34.119390 140339465779200 adafactor.py:358] Since rank of parameter decoder/layers_17/extra_mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:34.121506 140595318249472 adafactor.py:358] Since rank of parameter decoder/layers_16/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:34.104490 140311617681408 adafactor.py:358] Since rank of parameter decoder/layers_16/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:34.128539 140095893518336 adafactor.py:358] Since rank of parameter decoder/layers_18/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:34.127282 140440593786880 adafactor.py:358] Since rank of parameter decoder/layers_15/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:34.129172 139674595104768 adafactor.py:358] Since rank of parameter decoder/layers_15/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:34.111158 140529434355712 adafactor.py:358] Since rank of parameter decoder/layers_19/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:34.133102 140339465779200 adafactor.py:358] Since rank of parameter decoder/layers_17/extra_mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:34.114934 140311617681408 adafactor.py:358] Since rank of parameter decoder/layers_16/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:34.136077 140595318249472 adafactor.py:358] Since rank of parameter decoder/layers_16/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:34.137654 140440593786880 adafactor.py:358] Since rank of parameter decoder/layers_15/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:34.121184 140529434355712 adafactor.py:358] Since rank of parameter decoder/layers_19/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:34.141897 139674595104768 adafactor.py:358] Since rank of parameter decoder/layers_15/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:34.143046 140095893518336 adafactor.py:358] Since rank of parameter decoder/layers_18/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:34.146768 140339465779200 adafactor.py:358] Since rank of parameter decoder/layers_17/extra_mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:34.148144 140440593786880 adafactor.py:358] Since rank of parameter decoder/layers_15/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:34.129240 140311617681408 adafactor.py:358] Since rank of parameter decoder/layers_16/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:34.151124 140595318249472 adafactor.py:358] Since rank of parameter decoder/layers_16/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:34.154612 139674595104768 adafactor.py:358] Since rank of parameter decoder/layers_15/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:34.157930 140095893518336 adafactor.py:358] Since rank of parameter decoder/layers_19/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:34.139502 140529434355712 adafactor.py:358] Since rank of parameter decoder/layers_19/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:34.162651 140440593786880 adafactor.py:358] Since rank of parameter decoder/layers_15/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:34.143395 140311617681408 adafactor.py:358] Since rank of parameter decoder/layers_16/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:34.167347 139674595104768 adafactor.py:358] Since rank of parameter decoder/layers_15/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:34.166120 140595318249472 adafactor.py:358] Since rank of parameter decoder/layers_16/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:34.172304 140095893518336 adafactor.py:358] Since rank of parameter decoder/layers_19/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:34.155355 140529434355712 adafactor.py:358] Since rank of parameter decoder/layers_19/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:34.157355 140311617681408 adafactor.py:358] Since rank of parameter decoder/layers_16/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:34.177746 140440593786880 adafactor.py:358] Since rank of parameter decoder/layers_15/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:34.176984 140031916288000 adafactor.py:358] Since rank of parameter decoder/layers_17/mlp/router/router_weights/w/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>, <FactorDim.NONE: None>) is ignored.
W0512 23:42:34.180076 139674595104768 adafactor.py:358] Since rank of parameter decoder/layers_16/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:34.180540 140595318249472 adafactor.py:358] Since rank of parameter decoder/layers_17/extra_mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:34.186593 140095893518336 adafactor.py:358] Since rank of parameter decoder/layers_19/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:34.187940 140031916288000 adafactor.py:358] Since rank of parameter decoder/layers_17/pre_extra_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:34.170593 140529434355712 adafactor.py:358] Since rank of parameter decoder/layers_19/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:34.171286 140311617681408 adafactor.py:358] Since rank of parameter decoder/layers_17/extra_mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:34.192809 139674595104768 adafactor.py:358] Since rank of parameter decoder/layers_16/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:34.192398 140440593786880 adafactor.py:358] Since rank of parameter decoder/layers_15/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:34.195269 140595318249472 adafactor.py:358] Since rank of parameter decoder/layers_17/extra_mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:34.200389 140095893518336 adafactor.py:358] Since rank of parameter decoder/layers_19/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:34.200029 140031916288000 adafactor.py:358] Since rank of parameter decoder/layers_17/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:34.184878 140529434355712 adafactor.py:358] Since rank of parameter decoder/layers_2/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:34.201984 140339465779200 adafactor.py:358] Since rank of parameter decoder/layers_17/mlp/router/router_weights/w/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>, <FactorDim.NONE: None>) is ignored.
W0512 23:42:34.205630 139674595104768 adafactor.py:358] Since rank of parameter decoder/layers_16/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:34.185267 140311617681408 adafactor.py:358] Since rank of parameter decoder/layers_17/extra_mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:34.206730 140440593786880 adafactor.py:358] Since rank of parameter decoder/layers_16/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:34.210476 140095893518336 adafactor.py:358] Since rank of parameter decoder/layers_19/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:34.209888 140595318249472 adafactor.py:358] Since rank of parameter decoder/layers_17/extra_mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:34.212392 140031916288000 adafactor.py:358] Since rank of parameter decoder/layers_17/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:34.212141 140339465779200 adafactor.py:358] Since rank of parameter decoder/layers_17/pre_extra_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:34.218387 139674595104768 adafactor.py:358] Since rank of parameter decoder/layers_16/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:34.199636 140529434355712 adafactor.py:358] Since rank of parameter decoder/layers_2/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:34.220587 140095893518336 adafactor.py:358] Since rank of parameter decoder/layers_19/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:34.199321 140311617681408 adafactor.py:358] Since rank of parameter decoder/layers_17/extra_mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:34.222014 140440593786880 adafactor.py:358] Since rank of parameter decoder/layers_16/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:34.222195 140339465779200 adafactor.py:358] Since rank of parameter decoder/layers_17/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:34.223292 140031916288000 adafactor.py:358] Since rank of parameter decoder/layers_17/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:34.228238 139674595104768 adafactor.py:358] Since rank of parameter decoder/layers_16/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:34.234251 140095893518336 adafactor.py:358] Since rank of parameter decoder/layers_19/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:34.214484 140529434355712 adafactor.py:358] Since rank of parameter decoder/layers_2/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:34.232314 140339465779200 adafactor.py:358] Since rank of parameter decoder/layers_17/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:34.237607 139674595104768 adafactor.py:358] Since rank of parameter decoder/layers_16/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:34.236608 140440593786880 adafactor.py:358] Since rank of parameter decoder/layers_16/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:34.238683 140031916288000 adafactor.py:358] Since rank of parameter decoder/layers_17/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:34.242308 140339465779200 adafactor.py:358] Since rank of parameter decoder/layers_17/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:34.247978 140095893518336 adafactor.py:358] Since rank of parameter decoder/layers_19/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:34.230334 140529434355712 adafactor.py:358] Since rank of parameter decoder/layers_2/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:34.250349 139674595104768 adafactor.py:358] Since rank of parameter decoder/layers_16/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:34.250672 140440593786880 adafactor.py:358] Since rank of parameter decoder/layers_16/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:34.252716 140031916288000 adafactor.py:358] Since rank of parameter decoder/layers_17/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:34.255932 140339465779200 adafactor.py:358] Since rank of parameter decoder/layers_17/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:34.261791 140095893518336 adafactor.py:358] Since rank of parameter decoder/layers_19/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:34.242031 140529434355712 adafactor.py:358] Since rank of parameter decoder/layers_2/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:34.261744 140440593786880 adafactor.py:358] Since rank of parameter decoder/layers_16/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:34.263190 139674595104768 adafactor.py:358] Since rank of parameter decoder/layers_16/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:34.252677 140529434355712 adafactor.py:358] Since rank of parameter decoder/layers_2/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:34.270096 140031916288000 adafactor.py:358] Since rank of parameter decoder/layers_17/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:34.269554 140339465779200 adafactor.py:358] Since rank of parameter decoder/layers_17/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:34.272509 140595318249472 adafactor.py:358] Since rank of parameter decoder/layers_17/mlp/router/router_weights/w/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>, <FactorDim.NONE: None>) is ignored.
W0512 23:42:34.272837 140440593786880 adafactor.py:358] Since rank of parameter decoder/layers_16/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:34.275721 140095893518336 adafactor.py:358] Since rank of parameter decoder/layers_2/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:34.275986 139674595104768 adafactor.py:358] Since rank of parameter decoder/layers_16/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:34.256256 140311617681408 adafactor.py:358] Since rank of parameter decoder/layers_17/mlp/router/router_weights/w/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>, <FactorDim.NONE: None>) is ignored.
W0512 23:42:34.283515 140595318249472 adafactor.py:358] Since rank of parameter decoder/layers_17/pre_extra_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:34.284344 140031916288000 adafactor.py:358] Since rank of parameter decoder/layers_18/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:34.268084 140529434355712 adafactor.py:358] Since rank of parameter decoder/layers_2/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:34.266683 140311617681408 adafactor.py:358] Since rank of parameter decoder/layers_17/pre_extra_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:34.285508 140339465779200 adafactor.py:358] Since rank of parameter decoder/layers_17/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:34.286836 140440593786880 adafactor.py:358] Since rank of parameter decoder/layers_16/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:34.289374 140095893518336 adafactor.py:358] Since rank of parameter decoder/layers_2/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:34.288806 139674595104768 adafactor.py:358] Since rank of parameter decoder/layers_17/extra_mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:34.294388 140595318249472 adafactor.py:358] Since rank of parameter decoder/layers_17/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:34.277127 140311617681408 adafactor.py:358] Since rank of parameter decoder/layers_17/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:34.301597 139674595104768 adafactor.py:358] Since rank of parameter decoder/layers_17/extra_mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:34.299495 140339465779200 adafactor.py:358] Since rank of parameter decoder/layers_18/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:34.303098 140095893518336 adafactor.py:358] Since rank of parameter decoder/layers_2/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:34.301120 140440593786880 adafactor.py:358] Since rank of parameter decoder/layers_16/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:34.283356 140529434355712 adafactor.py:358] Since rank of parameter decoder/layers_2/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:34.300920 140031916288000 adafactor.py:358] Since rank of parameter decoder/layers_18/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:34.305056 140595318249472 adafactor.py:358] Since rank of parameter decoder/layers_17/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:34.287553 140311617681408 adafactor.py:358] Since rank of parameter decoder/layers_17/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:34.314484 139674595104768 adafactor.py:358] Since rank of parameter decoder/layers_17/extra_mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:34.313149 140339465779200 adafactor.py:358] Since rank of parameter decoder/layers_18/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:34.317272 140095893518336 adafactor.py:358] Since rank of parameter decoder/layers_2/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:34.315252 140440593786880 adafactor.py:358] Since rank of parameter decoder/layers_16/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:34.298031 140529434355712 adafactor.py:358] Since rank of parameter decoder/layers_2/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:34.315757 140595318249472 adafactor.py:358] Since rank of parameter decoder/layers_17/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:34.297929 140311617681408 adafactor.py:358] Since rank of parameter decoder/layers_17/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:34.317788 140031916288000 adafactor.py:358] Since rank of parameter decoder/layers_18/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:34.327376 140095893518336 adafactor.py:358] Since rank of parameter decoder/layers_2/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
W0512 23:42:34.326819 140339465779200 adafactor.py:358] Since rank of parameter decoder/layers_18/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
W0512 23:42:34.329406 140440593786880 adafactor.py:358] Since rank of parameter decoder/layers_17/extra_mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:34.311625 140529434355712 adafactor.py:358] Since rank of parameter decoder/layers_20/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:34.312143 140311617681408 adafactor.py:358] Since rank of parameter decoder/layers_17/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:34.330605 140595318249472 adafactor.py:358] Since rank of parameter decoder/layers_17/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:34.333010 140031916288000 adafactor.py:358] Since rank of parameter decoder/layers_18/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:34.337407 140095893518336 adafactor.py:358] Since rank of parameter decoder/layers_2/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:34.341692 140339465779200 adafactor.py:358] Since rank of parameter decoder/layers_18/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:34.343620 140440593786880 adafactor.py:358] Since rank of parameter decoder/layers_17/extra_mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:34.325508 140529434355712 adafactor.py:358] Since rank of parameter decoder/layers_20/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:34.344320 140031916288000 adafactor.py:358] Since rank of parameter decoder/layers_18/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:34.345767 140595318249472 adafactor.py:358] Since rank of parameter decoder/layers_17/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:34.326298 140311617681408 adafactor.py:358] Since rank of parameter decoder/layers_17/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:34.351207 140095893518336 adafactor.py:358] Since rank of parameter decoder/layers_2/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:34.353071 140339465779200 adafactor.py:358] Since rank of parameter decoder/layers_18/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:34.355335 140031916288000 adafactor.py:358] Since rank of parameter decoder/layers_18/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:34.357887 140440593786880 adafactor.py:358] Since rank of parameter decoder/layers_17/extra_mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:34.339690 140529434355712 adafactor.py:358] Since rank of parameter decoder/layers_20/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:34.364886 140095893518336 adafactor.py:358] Since rank of parameter decoder/layers_2/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:34.343439 140311617681408 adafactor.py:358] Since rank of parameter decoder/layers_17/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:34.364825 140595318249472 adafactor.py:358] Since rank of parameter decoder/layers_17/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:34.365913 140339465779200 adafactor.py:358] Since rank of parameter decoder/layers_18/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:34.353484 140529434355712 adafactor.py:358] Since rank of parameter decoder/layers_20/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:34.371590 140031916288000 adafactor.py:358] Since rank of parameter decoder/layers_18/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:34.375900 139674595104768 adafactor.py:358] Since rank of parameter decoder/layers_17/mlp/router/router_weights/w/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>, <FactorDim.NONE: None>) is ignored.
W0512 23:42:34.357447 140311617681408 adafactor.py:358] Since rank of parameter decoder/layers_18/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:34.382034 140095893518336 adafactor.py:358] Since rank of parameter decoder/layers_2/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:34.380064 140595318249472 adafactor.py:358] Since rank of parameter decoder/layers_18/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:34.364647 140529434355712 adafactor.py:358] Since rank of parameter decoder/layers_20/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:34.382021 140339465779200 adafactor.py:358] Since rank of parameter decoder/layers_18/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:34.386456 139674595104768 adafactor.py:358] Since rank of parameter decoder/layers_17/pre_extra_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:34.387450 140031916288000 adafactor.py:358] Since rank of parameter decoder/layers_18/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:34.372262 140311617681408 adafactor.py:358] Since rank of parameter decoder/layers_18/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:34.374989 140529434355712 adafactor.py:358] Since rank of parameter decoder/layers_20/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:34.396378 140095893518336 adafactor.py:358] Since rank of parameter decoder/layers_20/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:34.396673 140595318249472 adafactor.py:358] Since rank of parameter decoder/layers_18/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:34.397151 140339465779200 adafactor.py:358] Since rank of parameter decoder/layers_18/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:34.400310 139674595104768 adafactor.py:358] Since rank of parameter decoder/layers_17/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:34.402319 140031916288000 adafactor.py:358] Since rank of parameter decoder/layers_18/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:34.386435 140311617681408 adafactor.py:358] Since rank of parameter decoder/layers_18/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:34.389168 140529434355712 adafactor.py:358] Since rank of parameter decoder/layers_20/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:34.410234 140095893518336 adafactor.py:358] Since rank of parameter decoder/layers_20/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:34.410858 139674595104768 adafactor.py:358] Since rank of parameter decoder/layers_17/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:34.411635 140595318249472 adafactor.py:358] Since rank of parameter decoder/layers_18/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:34.413422 140339465779200 adafactor.py:358] Since rank of parameter decoder/layers_18/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:34.416051 140440593786880 adafactor.py:358] Since rank of parameter decoder/layers_17/mlp/router/router_weights/w/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>, <FactorDim.NONE: None>) is ignored.
W0512 23:42:34.416805 140031916288000 adafactor.py:358] Since rank of parameter decoder/layers_19/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:34.400568 140311617681408 adafactor.py:358] Since rank of parameter decoder/layers_18/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:34.424480 140095893518336 adafactor.py:358] Since rank of parameter decoder/layers_20/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:34.404449 140529434355712 adafactor.py:358] Since rank of parameter decoder/layers_20/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:34.424517 139674595104768 adafactor.py:358] Since rank of parameter decoder/layers_17/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:34.426649 140440593786880 adafactor.py:358] Since rank of parameter decoder/layers_17/pre_extra_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:34.427167 140595318249472 adafactor.py:358] Since rank of parameter decoder/layers_18/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:34.428121 140339465779200 adafactor.py:358] Since rank of parameter decoder/layers_19/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:34.410919 140311617681408 adafactor.py:358] Since rank of parameter decoder/layers_18/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:34.431504 140031916288000 adafactor.py:358] Since rank of parameter decoder/layers_19/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:34.438310 140095893518336 adafactor.py:358] Since rank of parameter decoder/layers_20/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:34.418127 140529434355712 adafactor.py:358] Since rank of parameter decoder/layers_20/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:34.437067 140440593786880 adafactor.py:358] Since rank of parameter decoder/layers_17/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:34.438712 139674595104768 adafactor.py:358] Since rank of parameter decoder/layers_17/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:34.438136 140595318249472 adafactor.py:358] Since rank of parameter decoder/layers_18/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:34.421165 140311617681408 adafactor.py:358] Since rank of parameter decoder/layers_18/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:34.443415 140339465779200 adafactor.py:358] Since rank of parameter decoder/layers_19/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:34.448436 140095893518336 adafactor.py:358] Since rank of parameter decoder/layers_20/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:34.447293 140440593786880 adafactor.py:358] Since rank of parameter decoder/layers_17/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:34.447457 140031916288000 adafactor.py:358] Since rank of parameter decoder/layers_19/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:34.431778 140529434355712 adafactor.py:358] Since rank of parameter decoder/layers_21/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:34.451158 140595318249472 adafactor.py:358] Since rank of parameter decoder/layers_18/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:34.455699 139674595104768 adafactor.py:358] Since rank of parameter decoder/layers_17/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:34.435135 140311617681408 adafactor.py:358] Since rank of parameter decoder/layers_18/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:34.459174 140095893518336 adafactor.py:358] Since rank of parameter decoder/layers_20/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:34.457532 140440593786880 adafactor.py:358] Since rank of parameter decoder/layers_17/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:34.458378 140339465779200 adafactor.py:358] Since rank of parameter decoder/layers_19/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:34.461684 140031916288000 adafactor.py:358] Since rank of parameter decoder/layers_19/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:34.445744 140529434355712 adafactor.py:358] Since rank of parameter decoder/layers_21/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:34.467204 140595318249472 adafactor.py:358] Since rank of parameter decoder/layers_18/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:34.449138 140311617681408 adafactor.py:358] Since rank of parameter decoder/layers_18/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:34.472799 140095893518336 adafactor.py:358] Since rank of parameter decoder/layers_20/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:34.471993 140440593786880 adafactor.py:358] Since rank of parameter decoder/layers_17/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:34.473551 139674595104768 adafactor.py:358] Since rank of parameter decoder/layers_17/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:34.472845 140031916288000 adafactor.py:358] Since rank of parameter decoder/layers_19/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:34.473319 140339465779200 adafactor.py:358] Since rank of parameter decoder/layers_19/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:34.459547 140529434355712 adafactor.py:358] Since rank of parameter decoder/layers_21/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:34.482738 140595318249472 adafactor.py:358] Since rank of parameter decoder/layers_18/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:34.463127 140311617681408 adafactor.py:358] Since rank of parameter decoder/layers_18/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:34.484335 140339465779200 adafactor.py:358] Since rank of parameter decoder/layers_19/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:34.484918 140031916288000 adafactor.py:358] Since rank of parameter decoder/layers_19/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:34.487573 139674595104768 adafactor.py:358] Since rank of parameter decoder/layers_18/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:34.486485 140440593786880 adafactor.py:358] Since rank of parameter decoder/layers_17/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:34.490991 140095893518336 adafactor.py:358] Since rank of parameter decoder/layers_20/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:34.473275 140529434355712 adafactor.py:358] Since rank of parameter decoder/layers_21/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:34.494900 140339465779200 adafactor.py:358] Since rank of parameter decoder/layers_19/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:34.477036 140311617681408 adafactor.py:358] Since rank of parameter decoder/layers_19/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:34.498031 140595318249472 adafactor.py:358] Since rank of parameter decoder/layers_18/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:34.499580 140031916288000 adafactor.py:358] Since rank of parameter decoder/layers_19/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:34.483497 140529434355712 adafactor.py:358] Since rank of parameter decoder/layers_21/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:34.503456 139674595104768 adafactor.py:358] Since rank of parameter decoder/layers_18/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:34.505192 140095893518336 adafactor.py:358] Since rank of parameter decoder/layers_20/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:34.503722 140440593786880 adafactor.py:358] Since rank of parameter decoder/layers_17/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:34.509855 140339465779200 adafactor.py:358] Since rank of parameter decoder/layers_19/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:34.491634 140311617681408 adafactor.py:358] Since rank of parameter decoder/layers_19/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:34.512621 140595318249472 adafactor.py:358] Since rank of parameter decoder/layers_19/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:34.495051 140529434355712 adafactor.py:358] Since rank of parameter decoder/layers_21/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:34.514912 140031916288000 adafactor.py:358] Since rank of parameter decoder/layers_19/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:34.517660 139674595104768 adafactor.py:358] Since rank of parameter decoder/layers_18/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:34.518922 140095893518336 adafactor.py:358] Since rank of parameter decoder/layers_21/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:34.518606 140440593786880 adafactor.py:358] Since rank of parameter decoder/layers_18/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:34.526763 140595318249472 adafactor.py:358] Since rank of parameter decoder/layers_19/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:34.525454 140339465779200 adafactor.py:358] Since rank of parameter decoder/layers_19/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:34.506921 140311617681408 adafactor.py:358] Since rank of parameter decoder/layers_19/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:34.509961 140529434355712 adafactor.py:358] Since rank of parameter decoder/layers_21/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:34.530667 139674595104768 adafactor.py:358] Since rank of parameter decoder/layers_18/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:34.532705 140095893518336 adafactor.py:358] Since rank of parameter decoder/layers_21/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:34.530612 140031916288000 adafactor.py:358] Since rank of parameter decoder/layers_19/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:34.534042 140440593786880 adafactor.py:358] Since rank of parameter decoder/layers_18/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:34.540560 139674595104768 adafactor.py:358] Since rank of parameter decoder/layers_18/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:34.521598 140311617681408 adafactor.py:358] Since rank of parameter decoder/layers_19/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:34.541570 140595318249472 adafactor.py:358] Since rank of parameter decoder/layers_19/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:34.540361 140339465779200 adafactor.py:358] Since rank of parameter decoder/layers_19/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:34.524805 140529434355712 adafactor.py:358] Since rank of parameter decoder/layers_21/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:34.546638 140095893518336 adafactor.py:358] Since rank of parameter decoder/layers_21/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:34.548274 140440593786880 adafactor.py:358] Since rank of parameter decoder/layers_18/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:34.550135 139674595104768 adafactor.py:358] Since rank of parameter decoder/layers_18/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:34.548624 140031916288000 adafactor.py:358] Since rank of parameter decoder/layers_2/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:34.532923 140311617681408 adafactor.py:358] Since rank of parameter decoder/layers_19/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:34.555552 140595318249472 adafactor.py:358] Since rank of parameter decoder/layers_19/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:34.554726 140339465779200 adafactor.py:358] Since rank of parameter decoder/layers_2/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:34.538756 140529434355712 adafactor.py:358] Since rank of parameter decoder/layers_21/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:34.560392 140095893518336 adafactor.py:358] Since rank of parameter decoder/layers_21/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:34.562227 140440593786880 adafactor.py:358] Since rank of parameter decoder/layers_18/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:34.563478 139674595104768 adafactor.py:358] Since rank of parameter decoder/layers_18/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:34.543320 140311617681408 adafactor.py:358] Since rank of parameter decoder/layers_19/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:34.565893 140595318249472 adafactor.py:358] Since rank of parameter decoder/layers_19/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:34.565890 140031916288000 adafactor.py:358] Since rank of parameter decoder/layers_2/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:34.570548 140095893518336 adafactor.py:358] Since rank of parameter decoder/layers_21/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:34.569115 140339465779200 adafactor.py:358] Since rank of parameter decoder/layers_2/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:34.552688 140529434355712 adafactor.py:358] Since rank of parameter decoder/layers_22/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:34.572475 140440593786880 adafactor.py:358] Since rank of parameter decoder/layers_18/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:34.577423 139674595104768 adafactor.py:358] Since rank of parameter decoder/layers_18/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:34.576287 140595318249472 adafactor.py:358] Since rank of parameter decoder/layers_19/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:34.557508 140311617681408 adafactor.py:358] Since rank of parameter decoder/layers_19/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:34.580707 140095893518336 adafactor.py:358] Since rank of parameter decoder/layers_21/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:34.582697 140440593786880 adafactor.py:358] Since rank of parameter decoder/layers_18/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:34.583525 140031916288000 adafactor.py:358] Since rank of parameter decoder/layers_2/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:34.566761 140529434355712 adafactor.py:358] Since rank of parameter decoder/layers_22/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:34.583976 140339465779200 adafactor.py:358] Since rank of parameter decoder/layers_2/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:34.590705 139674595104768 adafactor.py:358] Since rank of parameter decoder/layers_18/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:34.590981 140595318249472 adafactor.py:358] Since rank of parameter decoder/layers_19/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:34.571624 140311617681408 adafactor.py:358] Since rank of parameter decoder/layers_19/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:34.595031 140095893518336 adafactor.py:358] Since rank of parameter decoder/layers_21/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:34.596618 140440593786880 adafactor.py:358] Since rank of parameter decoder/layers_18/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:34.580622 140529434355712 adafactor.py:358] Since rank of parameter decoder/layers_22/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:34.598057 140339465779200 adafactor.py:358] Since rank of parameter decoder/layers_2/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:34.603887 139674595104768 adafactor.py:358] Since rank of parameter decoder/layers_19/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:34.601752 140031916288000 adafactor.py:358] Since rank of parameter decoder/layers_2/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:34.605269 140595318249472 adafactor.py:358] Since rank of parameter decoder/layers_19/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:34.585690 140311617681408 adafactor.py:358] Since rank of parameter decoder/layers_19/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:34.608906 140095893518336 adafactor.py:358] Since rank of parameter decoder/layers_21/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:34.609069 140339465779200 adafactor.py:358] Since rank of parameter decoder/layers_2/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:34.610578 140440593786880 adafactor.py:358] Since rank of parameter decoder/layers_18/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:34.594751 140529434355712 adafactor.py:358] Since rank of parameter decoder/layers_22/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:34.617082 139674595104768 adafactor.py:358] Since rank of parameter decoder/layers_19/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:34.615519 140031916288000 adafactor.py:358] Since rank of parameter decoder/layers_2/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:34.619595 140595318249472 adafactor.py:358] Since rank of parameter decoder/layers_19/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:34.599671 140311617681408 adafactor.py:358] Since rank of parameter decoder/layers_2/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:34.622747 140095893518336 adafactor.py:358] Since rank of parameter decoder/layers_21/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:34.620226 140339465779200 adafactor.py:358] Since rank of parameter decoder/layers_2/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:34.604849 140529434355712 adafactor.py:358] Since rank of parameter decoder/layers_22/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:34.624543 140440593786880 adafactor.py:358] Since rank of parameter decoder/layers_18/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:34.630642 139674595104768 adafactor.py:358] Since rank of parameter decoder/layers_19/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:34.628438 140031916288000 adafactor.py:358] Since rank of parameter decoder/layers_2/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:34.613826 140311617681408 adafactor.py:358] Since rank of parameter decoder/layers_2/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:34.634107 140595318249472 adafactor.py:358] Since rank of parameter decoder/layers_2/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:34.636611 140095893518336 adafactor.py:358] Since rank of parameter decoder/layers_22/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:34.616926 140529434355712 adafactor.py:358] Since rank of parameter decoder/layers_22/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:34.634727 140339465779200 adafactor.py:358] Since rank of parameter decoder/layers_2/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:34.638358 140440593786880 adafactor.py:358] Since rank of parameter decoder/layers_19/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:34.644491 139674595104768 adafactor.py:358] Since rank of parameter decoder/layers_19/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:34.646615 140031916288000 adafactor.py:358] Since rank of parameter decoder/layers_2/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:34.627866 140311617681408 adafactor.py:358] Since rank of parameter decoder/layers_2/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:34.650308 140095893518336 adafactor.py:358] Since rank of parameter decoder/layers_22/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:34.648506 140595318249472 adafactor.py:358] Since rank of parameter decoder/layers_2/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:34.649240 140339465779200 adafactor.py:358] Since rank of parameter decoder/layers_2/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:34.632565 140529434355712 adafactor.py:358] Since rank of parameter decoder/layers_22/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:34.652225 140440593786880 adafactor.py:358] Since rank of parameter decoder/layers_19/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:34.654095 139674595104768 adafactor.py:358] Since rank of parameter decoder/layers_19/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:34.664018 140095893518336 adafactor.py:358] Since rank of parameter decoder/layers_22/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:34.663642 139674595104768 adafactor.py:358] Since rank of parameter decoder/layers_19/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:34.642744 140311617681408 adafactor.py:358] Since rank of parameter decoder/layers_2/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:34.662766 140595318249472 adafactor.py:358] Since rank of parameter decoder/layers_2/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:34.663364 140339465779200 adafactor.py:358] Since rank of parameter decoder/layers_2/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:34.664500 140031916288000 adafactor.py:358] Since rank of parameter decoder/layers_2/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:34.666610 140440593786880 adafactor.py:358] Since rank of parameter decoder/layers_19/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:34.648726 140529434355712 adafactor.py:358] Since rank of parameter decoder/layers_22/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:34.653038 140311617681408 adafactor.py:358] Since rank of parameter decoder/layers_2/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:34.676578 139674595104768 adafactor.py:358] Since rank of parameter decoder/layers_19/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:34.677884 140095893518336 adafactor.py:358] Since rank of parameter decoder/layers_22/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:34.677689 140595318249472 adafactor.py:358] Since rank of parameter decoder/layers_2/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:34.677232 140339465779200 adafactor.py:358] Since rank of parameter decoder/layers_20/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:34.680480 140440593786880 adafactor.py:358] Since rank of parameter decoder/layers_19/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:34.681723 140031916288000 adafactor.py:358] Since rank of parameter decoder/layers_2/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:34.664694 140529434355712 adafactor.py:358] Since rank of parameter decoder/layers_22/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:34.664710 140311617681408 adafactor.py:358] Since rank of parameter decoder/layers_2/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:34.687941 140095893518336 adafactor.py:358] Since rank of parameter decoder/layers_22/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:34.688210 140595318249472 adafactor.py:358] Since rank of parameter decoder/layers_2/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:34.689538 139674595104768 adafactor.py:358] Since rank of parameter decoder/layers_19/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:34.690683 140440593786880 adafactor.py:358] Since rank of parameter decoder/layers_19/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:34.694447 140339465779200 adafactor.py:358] Since rank of parameter decoder/layers_20/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:34.679648 140529434355712 adafactor.py:358] Since rank of parameter decoder/layers_23/extra_mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:34.698266 140595318249472 adafactor.py:358] Since rank of parameter decoder/layers_2/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:34.701366 140095893518336 adafactor.py:358] Since rank of parameter decoder/layers_22/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:34.699022 140031916288000 adafactor.py:358] Since rank of parameter decoder/layers_20/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:34.680263 140311617681408 adafactor.py:358] Since rank of parameter decoder/layers_2/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:34.700919 140440593786880 adafactor.py:358] Since rank of parameter decoder/layers_19/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:34.702509 139674595104768 adafactor.py:358] Since rank of parameter decoder/layers_19/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:34.710596 140339465779200 adafactor.py:358] Since rank of parameter decoder/layers_20/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:34.712342 140595318249472 adafactor.py:358] Since rank of parameter decoder/layers_2/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:34.715340 140095893518336 adafactor.py:358] Since rank of parameter decoder/layers_22/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:34.695488 140529434355712 adafactor.py:358] Since rank of parameter decoder/layers_23/extra_mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:34.715543 139674595104768 adafactor.py:358] Since rank of parameter decoder/layers_2/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:34.714686 140440593786880 adafactor.py:358] Since rank of parameter decoder/layers_19/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:34.697475 140311617681408 adafactor.py:358] Since rank of parameter decoder/layers_2/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:34.717461 140031916288000 adafactor.py:358] Since rank of parameter decoder/layers_20/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:34.726793 140595318249472 adafactor.py:358] Since rank of parameter decoder/layers_2/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:34.728495 139674595104768 adafactor.py:358] Since rank of parameter decoder/layers_2/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:34.729798 140095893518336 adafactor.py:358] Since rank of parameter decoder/layers_22/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:34.728476 140440593786880 adafactor.py:358] Since rank of parameter decoder/layers_19/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:34.710460 140529434355712 adafactor.py:358] Since rank of parameter decoder/layers_23/extra_mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:34.728049 140339465779200 adafactor.py:358] Since rank of parameter decoder/layers_20/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:34.712533 140311617681408 adafactor.py:358] Since rank of parameter decoder/layers_2/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:34.736901 140031916288000 adafactor.py:358] Since rank of parameter decoder/layers_20/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:34.741400 139674595104768 adafactor.py:358] Since rank of parameter decoder/layers_2/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:34.741049 140595318249472 adafactor.py:358] Since rank of parameter decoder/layers_2/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:34.740044 140339465779200 adafactor.py:358] Since rank of parameter decoder/layers_20/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:34.743550 140095893518336 adafactor.py:358] Since rank of parameter decoder/layers_22/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:34.742413 140440593786880 adafactor.py:358] Since rank of parameter decoder/layers_19/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:34.727586 140311617681408 adafactor.py:358] Since rank of parameter decoder/layers_20/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:34.750717 140339465779200 adafactor.py:358] Since rank of parameter decoder/layers_20/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:34.754875 139674595104768 adafactor.py:358] Since rank of parameter decoder/layers_2/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:34.757261 140095893518336 adafactor.py:358] Since rank of parameter decoder/layers_23/extra_mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:34.755469 140595318249472 adafactor.py:358] Since rank of parameter decoder/layers_20/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:34.756145 140440593786880 adafactor.py:358] Since rank of parameter decoder/layers_2/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:34.755627 140031916288000 adafactor.py:358] Since rank of parameter decoder/layers_20/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:34.742170 140311617681408 adafactor.py:358] Since rank of parameter decoder/layers_20/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:34.764632 139674595104768 adafactor.py:358] Since rank of parameter decoder/layers_2/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:34.765100 140339465779200 adafactor.py:358] Since rank of parameter decoder/layers_20/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:34.771112 140095893518336 adafactor.py:358] Since rank of parameter decoder/layers_23/extra_mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:34.769605 140595318249472 adafactor.py:358] Since rank of parameter decoder/layers_20/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:34.769998 140440593786880 adafactor.py:358] Since rank of parameter decoder/layers_2/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:34.771331 140031916288000 adafactor.py:358] Since rank of parameter decoder/layers_20/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:34.774118 139674595104768 adafactor.py:358] Since rank of parameter decoder/layers_2/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:34.757916 140311617681408 adafactor.py:358] Since rank of parameter decoder/layers_20/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:34.779197 140339465779200 adafactor.py:358] Since rank of parameter decoder/layers_20/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:34.784974 140095893518336 adafactor.py:358] Since rank of parameter decoder/layers_23/extra_mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:34.783793 140440593786880 adafactor.py:358] Since rank of parameter decoder/layers_2/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:34.783941 140595318249472 adafactor.py:358] Since rank of parameter decoder/layers_20/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:34.787658 139674595104768 adafactor.py:358] Since rank of parameter decoder/layers_2/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:34.786288 140031916288000 adafactor.py:358] Since rank of parameter decoder/layers_20/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:34.769207 140529434355712 adafactor.py:358] Since rank of parameter decoder/layers_23/mlp/router/router_weights/w/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>, <FactorDim.NONE: None>) is ignored.
W0512 23:42:34.774273 140311617681408 adafactor.py:358] Since rank of parameter decoder/layers_20/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:34.793422 140339465779200 adafactor.py:358] Since rank of parameter decoder/layers_20/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:34.798105 140440593786880 adafactor.py:358] Since rank of parameter decoder/layers_2/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:34.798183 140595318249472 adafactor.py:358] Since rank of parameter decoder/layers_20/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:34.780467 140529434355712 adafactor.py:358] Since rank of parameter decoder/layers_23/pre_extra_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:34.801916 139674595104768 adafactor.py:358] Since rank of parameter decoder/layers_2/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:34.785484 140311617681408 adafactor.py:358] Since rank of parameter decoder/layers_20/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:34.805278 140031916288000 adafactor.py:358] Since rank of parameter decoder/layers_20/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:34.808273 140440593786880 adafactor.py:358] Since rank of parameter decoder/layers_2/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:34.807263 140339465779200 adafactor.py:358] Since rank of parameter decoder/layers_21/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:34.808690 140595318249472 adafactor.py:358] Since rank of parameter decoder/layers_20/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:34.791573 140529434355712 adafactor.py:358] Since rank of parameter decoder/layers_23/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:34.816450 139674595104768 adafactor.py:358] Since rank of parameter decoder/layers_2/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:34.818377 140440593786880 adafactor.py:358] Since rank of parameter decoder/layers_2/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:34.819324 140595318249472 adafactor.py:358] Since rank of parameter decoder/layers_20/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:34.799812 140311617681408 adafactor.py:358] Since rank of parameter decoder/layers_20/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:34.802668 140529434355712 adafactor.py:358] Since rank of parameter decoder/layers_23/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:34.821380 140339465779200 adafactor.py:358] Since rank of parameter decoder/layers_21/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:34.823643 140031916288000 adafactor.py:358] Since rank of parameter decoder/layers_20/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:34.829443 139674595104768 adafactor.py:358] Since rank of parameter decoder/layers_20/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:34.813874 140529434355712 adafactor.py:358] Since rank of parameter decoder/layers_23/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:34.832262 140440593786880 adafactor.py:358] Since rank of parameter decoder/layers_2/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:34.833443 140595318249472 adafactor.py:358] Since rank of parameter decoder/layers_20/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:34.814256 140311617681408 adafactor.py:358] Since rank of parameter decoder/layers_20/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:34.836853 140339465779200 adafactor.py:358] Since rank of parameter decoder/layers_21/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:34.840289 140095893518336 adafactor.py:358] Since rank of parameter decoder/layers_23/mlp/router/router_weights/w/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>, <FactorDim.NONE: None>) is ignored.
W0512 23:42:34.843886 139674595104768 adafactor.py:358] Since rank of parameter decoder/layers_20/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:34.842144 140031916288000 adafactor.py:358] Since rank of parameter decoder/layers_20/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:34.846144 140440593786880 adafactor.py:358] Since rank of parameter decoder/layers_2/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:34.828648 140529434355712 adafactor.py:358] Since rank of parameter decoder/layers_23/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:34.847502 140595318249472 adafactor.py:358] Since rank of parameter decoder/layers_20/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:34.850562 140095893518336 adafactor.py:358] Since rank of parameter decoder/layers_23/pre_extra_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:34.828484 140311617681408 adafactor.py:358] Since rank of parameter decoder/layers_20/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:34.851013 140339465779200 adafactor.py:358] Since rank of parameter decoder/layers_21/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:34.858341 139674595104768 adafactor.py:358] Since rank of parameter decoder/layers_20/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:34.861711 140095893518336 adafactor.py:358] Since rank of parameter decoder/layers_23/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:34.859907 140440593786880 adafactor.py:358] Since rank of parameter decoder/layers_2/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:34.861321 140595318249472 adafactor.py:358] Since rank of parameter decoder/layers_20/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:34.861168 140031916288000 adafactor.py:358] Since rank of parameter decoder/layers_21/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:34.861397 140339465779200 adafactor.py:358] Since rank of parameter decoder/layers_21/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:34.844487 140529434355712 adafactor.py:358] Since rank of parameter decoder/layers_23/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:34.842771 140311617681408 adafactor.py:358] Since rank of parameter decoder/layers_20/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:34.871800 140095893518336 adafactor.py:358] Since rank of parameter decoder/layers_23/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:34.872245 139674595104768 adafactor.py:358] Since rank of parameter decoder/layers_20/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:34.872310 140339465779200 adafactor.py:358] Since rank of parameter decoder/layers_21/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:34.873773 140440593786880 adafactor.py:358] Since rank of parameter decoder/layers_20/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:34.875230 140595318249472 adafactor.py:358] Since rank of parameter decoder/layers_21/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:34.857278 140311617681408 adafactor.py:358] Since rank of parameter decoder/layers_21/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:34.859227 140529434355712 adafactor.py:358] Since rank of parameter decoder/layers_23/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:34.881844 140095893518336 adafactor.py:358] Since rank of parameter decoder/layers_23/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:34.881743 139674595104768 adafactor.py:358] Since rank of parameter decoder/layers_20/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:34.880136 140031916288000 adafactor.py:358] Since rank of parameter decoder/layers_21/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:34.886147 140339465779200 adafactor.py:358] Since rank of parameter decoder/layers_21/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:34.887777 140440593786880 adafactor.py:358] Since rank of parameter decoder/layers_20/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:34.889356 140595318249472 adafactor.py:358] Since rank of parameter decoder/layers_21/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:34.871764 140311617681408 adafactor.py:358] Since rank of parameter decoder/layers_21/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:34.893812 139674595104768 adafactor.py:358] Since rank of parameter decoder/layers_20/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:34.895546 140095893518336 adafactor.py:358] Since rank of parameter decoder/layers_23/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:34.876872 140529434355712 adafactor.py:358] Since rank of parameter decoder/layers_3/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:34.899823 140031916288000 adafactor.py:358] Since rank of parameter decoder/layers_21/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:34.900363 140339465779200 adafactor.py:358] Since rank of parameter decoder/layers_21/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:34.901838 140440593786880 adafactor.py:358] Since rank of parameter decoder/layers_20/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:34.903781 140595318249472 adafactor.py:358] Since rank of parameter decoder/layers_21/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
W0512 23:42:34.886253 140311617681408 adafactor.py:358] Since rank of parameter decoder/layers_21/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:34.908348 139674595104768 adafactor.py:358] Since rank of parameter decoder/layers_20/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:34.911756 140095893518336 adafactor.py:358] Since rank of parameter decoder/layers_23/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:34.891549 140529434355712 adafactor.py:358] Since rank of parameter decoder/layers_3/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:34.914451 140339465779200 adafactor.py:358] Since rank of parameter decoder/layers_21/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:34.915623 140440593786880 adafactor.py:358] Since rank of parameter decoder/layers_20/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:34.917734 140595318249472 adafactor.py:358] Since rank of parameter decoder/layers_21/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:34.918853 140031916288000 adafactor.py:358] Since rank of parameter decoder/layers_21/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:34.900483 140311617681408 adafactor.py:358] Since rank of parameter decoder/layers_21/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:34.923935 139674595104768 adafactor.py:358] Since rank of parameter decoder/layers_20/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:34.925534 140095893518336 adafactor.py:358] Since rank of parameter decoder/layers_23/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:34.906264 140529434355712 adafactor.py:358] Since rank of parameter decoder/layers_3/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:34.925847 140440593786880 adafactor.py:358] Since rank of parameter decoder/layers_20/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:34.927942 140595318249472 adafactor.py:358] Since rank of parameter decoder/layers_21/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:34.928310 140339465779200 adafactor.py:358] Since rank of parameter decoder/layers_22/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:34.929957 140031916288000 adafactor.py:358] Since rank of parameter decoder/layers_21/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:34.911278 140311617681408 adafactor.py:358] Since rank of parameter decoder/layers_21/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:34.936743 140440593786880 adafactor.py:358] Since rank of parameter decoder/layers_20/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:34.939415 140095893518336 adafactor.py:358] Since rank of parameter decoder/layers_3/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:34.938970 139674595104768 adafactor.py:358] Since rank of parameter decoder/layers_20/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:34.938046 140595318249472 adafactor.py:358] Since rank of parameter decoder/layers_21/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:34.922535 140529434355712 adafactor.py:358] Since rank of parameter decoder/layers_3/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:34.921695 140311617681408 adafactor.py:358] Since rank of parameter decoder/layers_21/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:34.941136 140031916288000 adafactor.py:358] Since rank of parameter decoder/layers_21/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:34.942464 140339465779200 adafactor.py:358] Since rank of parameter decoder/layers_22/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:34.950669 140440593786880 adafactor.py:358] Since rank of parameter decoder/layers_20/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:34.953181 140095893518336 adafactor.py:358] Since rank of parameter decoder/layers_3/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:34.952943 139674595104768 adafactor.py:358] Since rank of parameter decoder/layers_21/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:34.952300 140595318249472 adafactor.py:358] Since rank of parameter decoder/layers_21/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:34.934655 140529434355712 adafactor.py:358] Since rank of parameter decoder/layers_3/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:34.936948 140311617681408 adafactor.py:358] Since rank of parameter decoder/layers_21/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:34.956876 140031916288000 adafactor.py:358] Since rank of parameter decoder/layers_21/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:34.956775 140339465779200 adafactor.py:358] Since rank of parameter decoder/layers_22/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:34.946154 140529434355712 adafactor.py:358] Since rank of parameter decoder/layers_3/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:34.964680 140440593786880 adafactor.py:358] Since rank of parameter decoder/layers_20/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:34.967019 140095893518336 adafactor.py:358] Since rank of parameter decoder/layers_3/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:34.966934 139674595104768 adafactor.py:358] Since rank of parameter decoder/layers_21/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:34.966202 140595318249472 adafactor.py:358] Since rank of parameter decoder/layers_21/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:34.951138 140311617681408 adafactor.py:358] Since rank of parameter decoder/layers_21/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:34.970673 140339465779200 adafactor.py:358] Since rank of parameter decoder/layers_22/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:34.971605 140031916288000 adafactor.py:358] Since rank of parameter decoder/layers_21/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:34.980751 140095893518336 adafactor.py:358] Since rank of parameter decoder/layers_3/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:34.978552 140440593786880 adafactor.py:358] Since rank of parameter decoder/layers_20/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:34.980996 139674595104768 adafactor.py:358] Since rank of parameter decoder/layers_21/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:34.980497 140595318249472 adafactor.py:358] Since rank of parameter decoder/layers_21/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:34.962609 140529434355712 adafactor.py:358] Since rank of parameter decoder/layers_3/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:34.981077 140339465779200 adafactor.py:358] Since rank of parameter decoder/layers_22/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:34.965546 140311617681408 adafactor.py:358] Since rank of parameter decoder/layers_21/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:34.986231 140031916288000 adafactor.py:358] Since rank of parameter decoder/layers_21/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:34.990897 140095893518336 adafactor.py:358] Since rank of parameter decoder/layers_3/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:34.992363 140440593786880 adafactor.py:358] Since rank of parameter decoder/layers_21/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:34.991370 140339465779200 adafactor.py:358] Since rank of parameter decoder/layers_22/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:34.994999 139674595104768 adafactor.py:358] Since rank of parameter decoder/layers_21/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:34.994310 140595318249472 adafactor.py:358] Since rank of parameter decoder/layers_22/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:34.979939 140529434355712 adafactor.py:358] Since rank of parameter decoder/layers_3/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:35.001423 140095893518336 adafactor.py:358] Since rank of parameter decoder/layers_3/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:34.980111 140311617681408 adafactor.py:358] Since rank of parameter decoder/layers_22/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:35.000336 140031916288000 adafactor.py:358] Since rank of parameter decoder/layers_22/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:35.005331 139674595104768 adafactor.py:358] Since rank of parameter decoder/layers_21/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:35.006203 140440593786880 adafactor.py:358] Since rank of parameter decoder/layers_21/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:35.005969 140339465779200 adafactor.py:358] Since rank of parameter decoder/layers_22/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:35.008149 140595318249472 adafactor.py:358] Since rank of parameter decoder/layers_22/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:35.015195 140095893518336 adafactor.py:358] Since rank of parameter decoder/layers_3/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:34.994785 140529434355712 adafactor.py:358] Since rank of parameter decoder/layers_3/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:35.015782 139674595104768 adafactor.py:358] Since rank of parameter decoder/layers_21/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:34.995184 140311617681408 adafactor.py:358] Since rank of parameter decoder/layers_22/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:35.014437 140031916288000 adafactor.py:358] Since rank of parameter decoder/layers_22/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:35.020150 140440593786880 adafactor.py:358] Since rank of parameter decoder/layers_21/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:35.020107 140339465779200 adafactor.py:358] Since rank of parameter decoder/layers_22/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:35.021936 140595318249472 adafactor.py:358] Since rank of parameter decoder/layers_22/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:35.028815 140095893518336 adafactor.py:358] Since rank of parameter decoder/layers_3/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:35.009103 140529434355712 adafactor.py:358] Since rank of parameter decoder/layers_4/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:35.030346 139674595104768 adafactor.py:358] Since rank of parameter decoder/layers_21/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:35.010123 140311617681408 adafactor.py:358] Since rank of parameter decoder/layers_22/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:35.029932 140031916288000 adafactor.py:358] Since rank of parameter decoder/layers_22/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:35.034030 140440593786880 adafactor.py:358] Since rank of parameter decoder/layers_21/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:35.033876 140339465779200 adafactor.py:358] Since rank of parameter decoder/layers_22/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:35.035763 140595318249472 adafactor.py:358] Since rank of parameter decoder/layers_22/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:35.042524 140095893518336 adafactor.py:358] Since rank of parameter decoder/layers_3/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:35.023213 140529434355712 adafactor.py:358] Since rank of parameter decoder/layers_4/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:35.044384 139674595104768 adafactor.py:358] Since rank of parameter decoder/layers_21/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:35.024416 140311617681408 adafactor.py:358] Since rank of parameter decoder/layers_22/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:35.044327 140440593786880 adafactor.py:358] Since rank of parameter decoder/layers_21/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:35.045906 140595318249472 adafactor.py:358] Since rank of parameter decoder/layers_22/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:35.045068 140031916288000 adafactor.py:358] Since rank of parameter decoder/layers_22/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:35.048281 140339465779200 adafactor.py:358] Since rank of parameter decoder/layers_23/extra_mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:35.056255 140095893518336 adafactor.py:358] Since rank of parameter decoder/layers_4/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:35.054687 140440593786880 adafactor.py:358] Since rank of parameter decoder/layers_21/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:35.035458 140311617681408 adafactor.py:358] Since rank of parameter decoder/layers_22/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:35.037679 140529434355712 adafactor.py:358] Since rank of parameter decoder/layers_4/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:35.056214 140595318249472 adafactor.py:358] Since rank of parameter decoder/layers_22/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:35.055662 140031916288000 adafactor.py:358] Since rank of parameter decoder/layers_22/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:35.059314 139674595104768 adafactor.py:358] Since rank of parameter decoder/layers_21/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:35.062398 140339465779200 adafactor.py:358] Since rank of parameter decoder/layers_23/extra_mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:35.045897 140311617681408 adafactor.py:358] Since rank of parameter decoder/layers_22/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:35.065987 140031916288000 adafactor.py:358] Since rank of parameter decoder/layers_22/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:35.069949 140095893518336 adafactor.py:358] Since rank of parameter decoder/layers_4/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:35.069512 140440593786880 adafactor.py:358] Since rank of parameter decoder/layers_21/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:35.070145 140595318249472 adafactor.py:358] Since rank of parameter decoder/layers_22/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:35.052737 140529434355712 adafactor.py:358] Since rank of parameter decoder/layers_4/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:35.073360 139674595104768 adafactor.py:358] Since rank of parameter decoder/layers_22/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:35.076541 140339465779200 adafactor.py:358] Since rank of parameter decoder/layers_23/extra_mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:35.060229 140311617681408 adafactor.py:358] Since rank of parameter decoder/layers_22/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:35.080190 140031916288000 adafactor.py:358] Since rank of parameter decoder/layers_22/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:35.063031 140529434355712 adafactor.py:358] Since rank of parameter decoder/layers_4/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:35.083607 140095893518336 adafactor.py:358] Since rank of parameter decoder/layers_4/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:35.083524 140440593786880 adafactor.py:358] Since rank of parameter decoder/layers_21/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:35.084573 140595318249472 adafactor.py:358] Since rank of parameter decoder/layers_22/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:35.073722 140529434355712 adafactor.py:358] Since rank of parameter decoder/layers_4/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:35.097249 140095893518336 adafactor.py:358] Since rank of parameter decoder/layers_4/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:35.075486 140311617681408 adafactor.py:358] Since rank of parameter decoder/layers_22/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:35.095157 140031916288000 adafactor.py:358] Since rank of parameter decoder/layers_22/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:35.097503 140440593786880 adafactor.py:358] Since rank of parameter decoder/layers_21/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:35.098333 140595318249472 adafactor.py:358] Since rank of parameter decoder/layers_22/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:35.102039 139674595104768 adafactor.py:358] Since rank of parameter decoder/layers_22/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:35.107305 140095893518336 adafactor.py:358] Since rank of parameter decoder/layers_4/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:35.087488 140529434355712 adafactor.py:358] Since rank of parameter decoder/layers_4/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:35.089692 140311617681408 adafactor.py:358] Since rank of parameter decoder/layers_22/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:35.109423 140031916288000 adafactor.py:358] Since rank of parameter decoder/layers_22/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:35.111563 140440593786880 adafactor.py:358] Since rank of parameter decoder/layers_22/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:35.112107 140595318249472 adafactor.py:358] Since rank of parameter decoder/layers_23/extra_mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:35.116370 139674595104768 adafactor.py:358] Since rank of parameter decoder/layers_22/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:35.119993 140095893518336 adafactor.py:358] Since rank of parameter decoder/layers_4/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:35.101375 140529434355712 adafactor.py:358] Since rank of parameter decoder/layers_4/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:35.103800 140311617681408 adafactor.py:358] Since rank of parameter decoder/layers_23/extra_mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:35.123449 140031916288000 adafactor.py:358] Since rank of parameter decoder/layers_23/extra_mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:35.125531 140440593786880 adafactor.py:358] Since rank of parameter decoder/layers_22/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:35.125904 140595318249472 adafactor.py:358] Since rank of parameter decoder/layers_23/extra_mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:35.130341 139674595104768 adafactor.py:358] Since rank of parameter decoder/layers_22/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:35.134612 140095893518336 adafactor.py:358] Since rank of parameter decoder/layers_4/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:35.133618 140339465779200 adafactor.py:358] Since rank of parameter decoder/layers_23/mlp/router/router_weights/w/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>, <FactorDim.NONE: None>) is ignored.
W0512 23:42:35.116726 140529434355712 adafactor.py:358] Since rank of parameter decoder/layers_4/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:35.118098 140311617681408 adafactor.py:358] Since rank of parameter decoder/layers_23/extra_mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:35.138255 140031916288000 adafactor.py:358] Since rank of parameter decoder/layers_23/extra_mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:35.140529 139674595104768 adafactor.py:358] Since rank of parameter decoder/layers_22/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:35.139436 140440593786880 adafactor.py:358] Since rank of parameter decoder/layers_22/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:35.139597 140595318249472 adafactor.py:358] Since rank of parameter decoder/layers_23/extra_mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:35.144909 140339465779200 adafactor.py:358] Since rank of parameter decoder/layers_23/pre_extra_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:35.148364 140095893518336 adafactor.py:358] Since rank of parameter decoder/layers_4/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:35.150667 139674595104768 adafactor.py:358] Since rank of parameter decoder/layers_22/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:35.133277 140529434355712 adafactor.py:358] Since rank of parameter decoder/layers_5/extra_mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:35.132302 140311617681408 adafactor.py:358] Since rank of parameter decoder/layers_23/extra_mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:35.152370 140031916288000 adafactor.py:358] Since rank of parameter decoder/layers_23/extra_mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:35.153578 140440593786880 adafactor.py:358] Since rank of parameter decoder/layers_22/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:35.155065 140339465779200 adafactor.py:358] Since rank of parameter decoder/layers_23/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:35.162117 140095893518336 adafactor.py:358] Since rank of parameter decoder/layers_4/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:35.164751 139674595104768 adafactor.py:358] Since rank of parameter decoder/layers_22/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:35.164117 140440593786880 adafactor.py:358] Since rank of parameter decoder/layers_22/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:35.165431 140339465779200 adafactor.py:358] Since rank of parameter decoder/layers_23/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:35.150176 140529434355712 adafactor.py:358] Since rank of parameter decoder/layers_5/extra_mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:35.175853 140095893518336 adafactor.py:358] Since rank of parameter decoder/layers_5/extra_mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:35.174360 140440593786880 adafactor.py:358] Since rank of parameter decoder/layers_22/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:35.175616 140339465779200 adafactor.py:358] Since rank of parameter decoder/layers_23/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:35.179519 139674595104768 adafactor.py:358] Since rank of parameter decoder/layers_22/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:35.164090 140529434355712 adafactor.py:358] Since rank of parameter decoder/layers_5/extra_mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:35.189559 140095893518336 adafactor.py:358] Since rank of parameter decoder/layers_5/extra_mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:35.188329 140440593786880 adafactor.py:358] Since rank of parameter decoder/layers_22/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:35.189751 140339465779200 adafactor.py:358] Since rank of parameter decoder/layers_23/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:35.193308 139674595104768 adafactor.py:358] Since rank of parameter decoder/layers_22/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:35.195106 140595318249472 adafactor.py:358] Since rank of parameter decoder/layers_23/mlp/router/router_weights/w/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>, <FactorDim.NONE: None>) is ignored.
W0512 23:42:35.203330 140095893518336 adafactor.py:358] Since rank of parameter decoder/layers_5/extra_mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:35.202777 140440593786880 adafactor.py:358] Since rank of parameter decoder/layers_22/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
W0512 23:42:35.203935 140339465779200 adafactor.py:358] Since rank of parameter decoder/layers_23/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:35.205417 140595318249472 adafactor.py:358] Since rank of parameter decoder/layers_23/pre_extra_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:35.207080 139674595104768 adafactor.py:358] Since rank of parameter decoder/layers_23/extra_mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
W0512 23:42:35.190461 140311617681408 adafactor.py:358] Since rank of parameter decoder/layers_23/mlp/router/router_weights/w/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>, <FactorDim.NONE: None>) is ignored.
W0512 23:42:35.210348 140031916288000 adafactor.py:358] Since rank of parameter decoder/layers_23/mlp/router/router_weights/w/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>, <FactorDim.NONE: None>) is ignored.
W0512 23:42:35.216003 140595318249472 adafactor.py:358] Since rank of parameter decoder/layers_23/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:35.216727 140440593786880 adafactor.py:358] Since rank of parameter decoder/layers_22/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:35.218000 140339465779200 adafactor.py:358] Since rank of parameter decoder/layers_23/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:35.221403 139674595104768 adafactor.py:358] Since rank of parameter decoder/layers_23/extra_mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:35.201270 140311617681408 adafactor.py:358] Since rank of parameter decoder/layers_23/pre_extra_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:35.221030 140031916288000 adafactor.py:358] Since rank of parameter decoder/layers_23/pre_extra_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:35.226155 140595318249472 adafactor.py:358] Since rank of parameter decoder/layers_23/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:35.230685 140440593786880 adafactor.py:358] Since rank of parameter decoder/layers_23/extra_mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:35.212545 140311617681408 adafactor.py:358] Since rank of parameter decoder/layers_23/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:35.232193 140031916288000 adafactor.py:358] Since rank of parameter decoder/layers_23/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:35.231988 140339465779200 adafactor.py:358] Since rank of parameter decoder/layers_3/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:35.235428 139674595104768 adafactor.py:358] Since rank of parameter decoder/layers_23/extra_mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:35.236305 140595318249472 adafactor.py:358] Since rank of parameter decoder/layers_23/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:35.222993 140311617681408 adafactor.py:358] Since rank of parameter decoder/layers_23/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:35.242417 140031916288000 adafactor.py:358] Since rank of parameter decoder/layers_23/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:35.226079 140529434355712 adafactor.py:358] Since rank of parameter decoder/layers_5/mlp/router/router_weights/w/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>, <FactorDim.NONE: None>) is ignored.
W0512 23:42:35.244660 140440593786880 adafactor.py:358] Since rank of parameter decoder/layers_23/extra_mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:35.246139 140339465779200 adafactor.py:358] Since rank of parameter decoder/layers_3/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:35.250098 140595318249472 adafactor.py:358] Since rank of parameter decoder/layers_23/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:35.252813 140031916288000 adafactor.py:358] Since rank of parameter decoder/layers_23/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:35.234261 140311617681408 adafactor.py:358] Since rank of parameter decoder/layers_23/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:35.258492 140095893518336 adafactor.py:358] Since rank of parameter decoder/layers_5/mlp/router/router_weights/w/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>, <FactorDim.NONE: None>) is ignored.
W0512 23:42:35.238518 140529434355712 adafactor.py:358] Since rank of parameter decoder/layers_5/pre_extra_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:35.258576 140440593786880 adafactor.py:358] Since rank of parameter decoder/layers_23/extra_mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:35.261691 140339465779200 adafactor.py:358] Since rank of parameter decoder/layers_3/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:35.263856 140595318249472 adafactor.py:358] Since rank of parameter decoder/layers_23/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:35.269264 140095893518336 adafactor.py:358] Since rank of parameter decoder/layers_5/pre_extra_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:35.249505 140529434355712 adafactor.py:358] Since rank of parameter decoder/layers_5/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:35.267134 140031916288000 adafactor.py:358] Since rank of parameter decoder/layers_23/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:35.248909 140311617681408 adafactor.py:358] Since rank of parameter decoder/layers_23/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:35.279356 140095893518336 adafactor.py:358] Since rank of parameter decoder/layers_5/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:35.276255 140339465779200 adafactor.py:358] Since rank of parameter decoder/layers_3/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:35.259828 140529434355712 adafactor.py:358] Since rank of parameter decoder/layers_5/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:35.277983 140595318249472 adafactor.py:358] Since rank of parameter decoder/layers_23/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:35.281216 140031916288000 adafactor.py:358] Since rank of parameter decoder/layers_23/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:35.263921 140311617681408 adafactor.py:358] Since rank of parameter decoder/layers_23/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:35.289421 140095893518336 adafactor.py:358] Since rank of parameter decoder/layers_5/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:35.286601 140339465779200 adafactor.py:358] Since rank of parameter decoder/layers_3/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:35.269994 140529434355712 adafactor.py:358] Since rank of parameter decoder/layers_5/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:35.291435 139674595104768 adafactor.py:358] Since rank of parameter decoder/layers_23/mlp/router/router_weights/w/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>, <FactorDim.NONE: None>) is ignored.
W0512 23:42:35.291854 140595318249472 adafactor.py:358] Since rank of parameter decoder/layers_3/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:35.295176 140031916288000 adafactor.py:358] Since rank of parameter decoder/layers_23/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:35.299527 140095893518336 adafactor.py:358] Since rank of parameter decoder/layers_5/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:35.296805 140339465779200 adafactor.py:358] Since rank of parameter decoder/layers_3/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:35.278501 140311617681408 adafactor.py:358] Since rank of parameter decoder/layers_23/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:35.302402 139674595104768 adafactor.py:358] Since rank of parameter decoder/layers_23/pre_extra_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:35.285234 140529434355712 adafactor.py:358] Since rank of parameter decoder/layers_5/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:35.305727 140595318249472 adafactor.py:358] Since rank of parameter decoder/layers_3/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:35.309242 140031916288000 adafactor.py:358] Since rank of parameter decoder/layers_3/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:35.313262 140095893518336 adafactor.py:358] Since rank of parameter decoder/layers_5/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:35.313309 139674595104768 adafactor.py:358] Since rank of parameter decoder/layers_23/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:35.314380 140339465779200 adafactor.py:358] Since rank of parameter decoder/layers_3/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:35.316229 140440593786880 adafactor.py:358] Since rank of parameter decoder/layers_23/mlp/router/router_weights/w/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>, <FactorDim.NONE: None>) is ignored.
W0512 23:42:35.297587 140311617681408 adafactor.py:358] Since rank of parameter decoder/layers_3/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:35.300821 140529434355712 adafactor.py:358] Since rank of parameter decoder/layers_5/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:35.319491 140595318249472 adafactor.py:358] Since rank of parameter decoder/layers_3/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:35.323479 139674595104768 adafactor.py:358] Since rank of parameter decoder/layers_23/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:35.323212 140031916288000 adafactor.py:358] Since rank of parameter decoder/layers_3/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:35.326970 140440593786880 adafactor.py:358] Since rank of parameter decoder/layers_23/pre_extra_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:35.329700 140095893518336 adafactor.py:358] Since rank of parameter decoder/layers_5/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:35.329255 140339465779200 adafactor.py:358] Since rank of parameter decoder/layers_3/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:35.333530 139674595104768 adafactor.py:358] Since rank of parameter decoder/layers_23/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:35.333328 140595318249472 adafactor.py:358] Since rank of parameter decoder/layers_3/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:35.315176 140529434355712 adafactor.py:358] Since rank of parameter decoder/layers_5/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:35.315693 140311617681408 adafactor.py:358] Since rank of parameter decoder/layers_3/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:35.337988 140440593786880 adafactor.py:358] Since rank of parameter decoder/layers_23/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:35.337306 140031916288000 adafactor.py:358] Since rank of parameter decoder/layers_3/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:35.343490 140095893518336 adafactor.py:358] Since rank of parameter decoder/layers_5/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:35.343477 140595318249472 adafactor.py:358] Since rank of parameter decoder/layers_3/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:35.344081 140339465779200 adafactor.py:358] Since rank of parameter decoder/layers_3/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:35.347426 139674595104768 adafactor.py:358] Since rank of parameter decoder/layers_23/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:35.328875 140529434355712 adafactor.py:358] Since rank of parameter decoder/layers_6/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:35.348558 140440593786880 adafactor.py:358] Since rank of parameter decoder/layers_23/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:35.351360 140031916288000 adafactor.py:358] Since rank of parameter decoder/layers_3/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:35.332176 140311617681408 adafactor.py:358] Since rank of parameter decoder/layers_3/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:35.354104 140595318249472 adafactor.py:358] Since rank of parameter decoder/layers_3/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:35.357224 140095893518336 adafactor.py:358] Since rank of parameter decoder/layers_6/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:35.358852 140440593786880 adafactor.py:358] Since rank of parameter decoder/layers_23/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:35.361213 139674595104768 adafactor.py:358] Since rank of parameter decoder/layers_23/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:35.342697 140529434355712 adafactor.py:358] Since rank of parameter decoder/layers_6/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:35.360996 140339465779200 adafactor.py:358] Since rank of parameter decoder/layers_4/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:35.361658 140031916288000 adafactor.py:358] Since rank of parameter decoder/layers_3/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:35.347524 140311617681408 adafactor.py:358] Since rank of parameter decoder/layers_3/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:35.367949 140595318249472 adafactor.py:358] Since rank of parameter decoder/layers_3/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:35.371094 140095893518336 adafactor.py:358] Since rank of parameter decoder/layers_6/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:35.372804 140440593786880 adafactor.py:358] Since rank of parameter decoder/layers_23/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:35.372259 140031916288000 adafactor.py:358] Since rank of parameter decoder/layers_3/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:35.374977 139674595104768 adafactor.py:358] Since rank of parameter decoder/layers_23/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:35.357048 140529434355712 adafactor.py:358] Since rank of parameter decoder/layers_6/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:35.375114 140339465779200 adafactor.py:358] Since rank of parameter decoder/layers_4/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:35.359149 140311617681408 adafactor.py:358] Since rank of parameter decoder/layers_3/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:35.381891 140595318249472 adafactor.py:358] Since rank of parameter decoder/layers_3/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:35.384989 140095893518336 adafactor.py:358] Since rank of parameter decoder/layers_6/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:35.387060 140440593786880 adafactor.py:358] Since rank of parameter decoder/layers_23/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:35.386298 140031916288000 adafactor.py:358] Since rank of parameter decoder/layers_3/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:35.388765 139674595104768 adafactor.py:358] Since rank of parameter decoder/layers_3/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:35.370819 140529434355712 adafactor.py:358] Since rank of parameter decoder/layers_6/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:35.389245 140339465779200 adafactor.py:358] Since rank of parameter decoder/layers_4/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:35.372760 140311617681408 adafactor.py:358] Since rank of parameter decoder/layers_3/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:35.396001 140595318249472 adafactor.py:358] Since rank of parameter decoder/layers_3/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:35.399347 140095893518336 adafactor.py:358] Since rank of parameter decoder/layers_6/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:35.380890 140529434355712 adafactor.py:358] Since rank of parameter decoder/layers_6/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:35.401041 140440593786880 adafactor.py:358] Since rank of parameter decoder/layers_23/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:35.400275 140031916288000 adafactor.py:358] Since rank of parameter decoder/layers_3/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:35.402597 139674595104768 adafactor.py:358] Since rank of parameter decoder/layers_3/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:35.404094 140339465779200 adafactor.py:358] Since rank of parameter decoder/layers_4/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:35.409506 140095893518336 adafactor.py:358] Since rank of parameter decoder/layers_6/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:35.387808 140311617681408 adafactor.py:358] Since rank of parameter decoder/layers_3/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:35.391021 140529434355712 adafactor.py:358] Since rank of parameter decoder/layers_6/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:35.409891 140595318249472 adafactor.py:358] Since rank of parameter decoder/layers_4/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:35.414999 140440593786880 adafactor.py:358] Since rank of parameter decoder/layers_3/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:35.414258 140031916288000 adafactor.py:358] Since rank of parameter decoder/layers_3/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:35.416515 139674595104768 adafactor.py:358] Since rank of parameter decoder/layers_3/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:35.414457 140339465779200 adafactor.py:358] Since rank of parameter decoder/layers_4/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:35.419666 140095893518336 adafactor.py:358] Since rank of parameter decoder/layers_6/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:35.402198 140311617681408 adafactor.py:358] Since rank of parameter decoder/layers_3/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:35.404739 140529434355712 adafactor.py:358] Since rank of parameter decoder/layers_6/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:35.423712 140595318249472 adafactor.py:358] Since rank of parameter decoder/layers_4/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:35.426026 140339465779200 adafactor.py:358] Since rank of parameter decoder/layers_4/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:35.428175 140031916288000 adafactor.py:358] Since rank of parameter decoder/layers_4/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:35.429020 140440593786880 adafactor.py:358] Since rank of parameter decoder/layers_3/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:35.430497 139674595104768 adafactor.py:358] Since rank of parameter decoder/layers_3/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:35.433443 140095893518336 adafactor.py:358] Since rank of parameter decoder/layers_6/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:35.416365 140311617681408 adafactor.py:358] Since rank of parameter decoder/layers_3/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:35.418425 140529434355712 adafactor.py:358] Since rank of parameter decoder/layers_6/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:35.437429 140595318249472 adafactor.py:358] Since rank of parameter decoder/layers_4/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:35.441308 139674595104768 adafactor.py:358] Since rank of parameter decoder/layers_3/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:35.443014 140440593786880 adafactor.py:358] Since rank of parameter decoder/layers_3/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:35.442182 140031916288000 adafactor.py:358] Since rank of parameter decoder/layers_4/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:35.442351 140339465779200 adafactor.py:358] Since rank of parameter decoder/layers_4/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:35.447390 140095893518336 adafactor.py:358] Since rank of parameter decoder/layers_6/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:35.432062 140529434355712 adafactor.py:358] Since rank of parameter decoder/layers_6/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:35.430432 140311617681408 adafactor.py:358] Since rank of parameter decoder/layers_4/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:35.451209 140595318249472 adafactor.py:358] Since rank of parameter decoder/layers_4/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:35.452671 139674595104768 adafactor.py:358] Since rank of parameter decoder/layers_3/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:35.456118 140031916288000 adafactor.py:358] Since rank of parameter decoder/layers_4/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:35.456961 140440593786880 adafactor.py:358] Since rank of parameter decoder/layers_3/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:35.461154 140095893518336 adafactor.py:358] Since rank of parameter decoder/layers_6/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:35.461326 140595318249472 adafactor.py:358] Since rank of parameter decoder/layers_4/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:35.462280 140339465779200 adafactor.py:358] Since rank of parameter decoder/layers_4/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:35.445755 140529434355712 adafactor.py:358] Since rank of parameter decoder/layers_7/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:35.444796 140311617681408 adafactor.py:358] Since rank of parameter decoder/layers_4/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:35.466940 139674595104768 adafactor.py:358] Since rank of parameter decoder/layers_3/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:35.467487 140440593786880 adafactor.py:358] Since rank of parameter decoder/layers_3/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:35.470168 140031916288000 adafactor.py:358] Since rank of parameter decoder/layers_4/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:35.471456 140595318249472 adafactor.py:358] Since rank of parameter decoder/layers_4/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:35.474889 140095893518336 adafactor.py:358] Since rank of parameter decoder/layers_7/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:35.459422 140529434355712 adafactor.py:358] Since rank of parameter decoder/layers_7/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:35.478085 140440593786880 adafactor.py:358] Since rank of parameter decoder/layers_3/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:35.459177 140311617681408 adafactor.py:358] Since rank of parameter decoder/layers_4/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:35.480662 139674595104768 adafactor.py:358] Since rank of parameter decoder/layers_3/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:35.480485 140031916288000 adafactor.py:358] Since rank of parameter decoder/layers_4/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:35.481895 140339465779200 adafactor.py:358] Since rank of parameter decoder/layers_4/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:35.485692 140595318249472 adafactor.py:358] Since rank of parameter decoder/layers_4/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:35.488593 140095893518336 adafactor.py:358] Since rank of parameter decoder/layers_7/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:35.473095 140529434355712 adafactor.py:358] Since rank of parameter decoder/layers_7/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:35.490763 140031916288000 adafactor.py:358] Since rank of parameter decoder/layers_4/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:35.492870 140440593786880 adafactor.py:358] Since rank of parameter decoder/layers_3/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:35.494359 139674595104768 adafactor.py:358] Since rank of parameter decoder/layers_3/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:35.473718 140311617681408 adafactor.py:358] Since rank of parameter decoder/layers_4/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:35.497514 140339465779200 adafactor.py:358] Since rank of parameter decoder/layers_5/extra_mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:35.499468 140595318249472 adafactor.py:358] Since rank of parameter decoder/layers_4/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:35.502373 140095893518336 adafactor.py:358] Since rank of parameter decoder/layers_7/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:35.484413 140311617681408 adafactor.py:358] Since rank of parameter decoder/layers_4/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:35.487539 140529434355712 adafactor.py:358] Since rank of parameter decoder/layers_7/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:35.505078 140031916288000 adafactor.py:358] Since rank of parameter decoder/layers_4/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:35.508086 139674595104768 adafactor.py:358] Since rank of parameter decoder/layers_4/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:35.507212 140440593786880 adafactor.py:358] Since rank of parameter decoder/layers_3/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:35.513266 140595318249472 adafactor.py:358] Since rank of parameter decoder/layers_4/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:35.516186 140095893518336 adafactor.py:358] Since rank of parameter decoder/layers_7/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:35.494881 140311617681408 adafactor.py:358] Since rank of parameter decoder/layers_4/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:35.497624 140529434355712 adafactor.py:358] Since rank of parameter decoder/layers_7/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:35.514786 140339465779200 adafactor.py:358] Since rank of parameter decoder/layers_5/extra_mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:35.519065 140031916288000 adafactor.py:358] Since rank of parameter decoder/layers_4/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:35.521820 139674595104768 adafactor.py:358] Since rank of parameter decoder/layers_4/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:35.521086 140440593786880 adafactor.py:358] Since rank of parameter decoder/layers_3/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:35.507565 140529434355712 adafactor.py:358] Since rank of parameter decoder/layers_7/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:35.529049 140095893518336 adafactor.py:358] Since rank of parameter decoder/layers_7/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:35.527185 140595318249472 adafactor.py:358] Since rank of parameter decoder/layers_5/extra_mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
W0512 23:42:35.509926 140311617681408 adafactor.py:358] Since rank of parameter decoder/layers_4/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:35.529913 140339465779200 adafactor.py:358] Since rank of parameter decoder/layers_5/extra_mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:35.533033 140031916288000 adafactor.py:358] Since rank of parameter decoder/layers_4/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:35.535574 139674595104768 adafactor.py:358] Since rank of parameter decoder/layers_4/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:35.534969 140440593786880 adafactor.py:358] Since rank of parameter decoder/layers_4/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:35.540552 140095893518336 adafactor.py:358] Since rank of parameter decoder/layers_7/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:35.521245 140529434355712 adafactor.py:358] Since rank of parameter decoder/layers_7/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:35.541138 140595318249472 adafactor.py:358] Since rank of parameter decoder/layers_5/extra_mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:35.524146 140311617681408 adafactor.py:358] Since rank of parameter decoder/layers_4/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:35.546947 140031916288000 adafactor.py:358] Since rank of parameter decoder/layers_5/extra_mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:35.549396 139674595104768 adafactor.py:358] Since rank of parameter decoder/layers_4/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:35.549972 140440593786880 adafactor.py:358] Since rank of parameter decoder/layers_4/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:35.534822 140529434355712 adafactor.py:358] Since rank of parameter decoder/layers_7/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:35.556465 140095893518336 adafactor.py:358] Since rank of parameter decoder/layers_7/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:35.538232 140311617681408 adafactor.py:358] Since rank of parameter decoder/layers_4/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:35.559640 139674595104768 adafactor.py:358] Since rank of parameter decoder/layers_4/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:35.560142 140595318249472 adafactor.py:358] Since rank of parameter decoder/layers_5/extra_mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:35.560853 140031916288000 adafactor.py:358] Since rank of parameter decoder/layers_5/extra_mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:35.565016 140440593786880 adafactor.py:358] Since rank of parameter decoder/layers_4/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:35.548475 140529434355712 adafactor.py:358] Since rank of parameter decoder/layers_7/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:35.569798 139674595104768 adafactor.py:358] Since rank of parameter decoder/layers_4/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:35.572095 140095893518336 adafactor.py:358] Since rank of parameter decoder/layers_7/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:35.552299 140311617681408 adafactor.py:358] Since rank of parameter decoder/layers_5/extra_mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:35.574662 140031916288000 adafactor.py:358] Since rank of parameter decoder/layers_5/extra_mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:35.579829 140440593786880 adafactor.py:358] Since rank of parameter decoder/layers_4/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:35.562117 140529434355712 adafactor.py:358] Since rank of parameter decoder/layers_8/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:35.584456 139674595104768 adafactor.py:358] Since rank of parameter decoder/layers_4/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:35.587204 140095893518336 adafactor.py:358] Since rank of parameter decoder/layers_7/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
W0512 23:42:35.566470 140311617681408 adafactor.py:358] Since rank of parameter decoder/layers_5/extra_mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:35.590196 140440593786880 adafactor.py:358] Since rank of parameter decoder/layers_4/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:35.575683 140529434355712 adafactor.py:358] Since rank of parameter decoder/layers_8/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:35.593145 140339465779200 adafactor.py:358] Since rank of parameter decoder/layers_5/mlp/router/router_weights/w/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>, <FactorDim.NONE: None>) is ignored.
W0512 23:42:35.598428 139674595104768 adafactor.py:358] Since rank of parameter decoder/layers_4/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:35.580595 140311617681408 adafactor.py:358] Since rank of parameter decoder/layers_5/extra_mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:35.600762 140440593786880 adafactor.py:358] Since rank of parameter decoder/layers_4/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:35.603559 140095893518336 adafactor.py:358] Since rank of parameter decoder/layers_8/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:35.604912 140339465779200 adafactor.py:358] Since rank of parameter decoder/layers_5/pre_extra_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:35.589299 140529434355712 adafactor.py:358] Since rank of parameter decoder/layers_8/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:35.612846 139674595104768 adafactor.py:358] Since rank of parameter decoder/layers_4/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:35.615126 140440593786880 adafactor.py:358] Since rank of parameter decoder/layers_4/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:35.618161 140095893518336 adafactor.py:358] Since rank of parameter decoder/layers_8/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:35.616215 140595318249472 adafactor.py:358] Since rank of parameter decoder/layers_5/mlp/router/router_weights/w/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>, <FactorDim.NONE: None>) is ignored.
W0512 23:42:35.616536 140339465779200 adafactor.py:358] Since rank of parameter decoder/layers_5/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:35.602900 140529434355712 adafactor.py:358] Since rank of parameter decoder/layers_8/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:35.627096 139674595104768 adafactor.py:358] Since rank of parameter decoder/layers_5/extra_mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:35.627362 140595318249472 adafactor.py:358] Since rank of parameter decoder/layers_5/pre_extra_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:35.629060 140440593786880 adafactor.py:358] Since rank of parameter decoder/layers_4/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:35.628878 140339465779200 adafactor.py:358] Since rank of parameter decoder/layers_5/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:35.632588 140095893518336 adafactor.py:358] Since rank of parameter decoder/layers_8/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:35.613002 140529434355712 adafactor.py:358] Since rank of parameter decoder/layers_8/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:35.630733 140031916288000 adafactor.py:358] Since rank of parameter decoder/layers_5/mlp/router/router_weights/w/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>, <FactorDim.NONE: None>) is ignored.
W0512 23:42:35.637651 140595318249472 adafactor.py:358] Since rank of parameter decoder/layers_5/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:35.640903 139674595104768 adafactor.py:358] Since rank of parameter decoder/layers_5/extra_mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:35.639937 140339465779200 adafactor.py:358] Since rank of parameter decoder/layers_5/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:35.623533 140529434355712 adafactor.py:358] Since rank of parameter decoder/layers_8/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:35.641575 140031916288000 adafactor.py:358] Since rank of parameter decoder/layers_5/pre_extra_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:35.643047 140440593786880 adafactor.py:358] Since rank of parameter decoder/layers_4/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:35.646701 140095893518336 adafactor.py:358] Since rank of parameter decoder/layers_8/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:35.647815 140595318249472 adafactor.py:358] Since rank of parameter decoder/layers_5/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:35.651915 140031916288000 adafactor.py:358] Since rank of parameter decoder/layers_5/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:35.655008 139674595104768 adafactor.py:358] Since rank of parameter decoder/layers_5/extra_mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:35.637135 140529434355712 adafactor.py:358] Since rank of parameter decoder/layers_8/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:35.657946 140095893518336 adafactor.py:358] Since rank of parameter decoder/layers_8/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:35.654735 140339465779200 adafactor.py:358] Since rank of parameter decoder/layers_5/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:35.656971 140440593786880 adafactor.py:358] Since rank of parameter decoder/layers_5/extra_mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:35.658053 140595318249472 adafactor.py:358] Since rank of parameter decoder/layers_5/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:35.641346 140311617681408 adafactor.py:358] Since rank of parameter decoder/layers_5/mlp/router/router_weights/w/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>, <FactorDim.NONE: None>) is ignored.
W0512 23:42:35.662276 140031916288000 adafactor.py:358] Since rank of parameter decoder/layers_5/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:35.670631 140095893518336 adafactor.py:358] Since rank of parameter decoder/layers_8/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:35.650759 140529434355712 adafactor.py:358] Since rank of parameter decoder/layers_8/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:35.670868 140440593786880 adafactor.py:358] Since rank of parameter decoder/layers_5/extra_mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:35.671942 140595318249472 adafactor.py:358] Since rank of parameter decoder/layers_5/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:35.672516 140031916288000 adafactor.py:358] Since rank of parameter decoder/layers_5/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:35.672735 140339465779200 adafactor.py:358] Since rank of parameter decoder/layers_5/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:35.654893 140311617681408 adafactor.py:358] Since rank of parameter decoder/layers_5/pre_extra_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:35.664367 140529434355712 adafactor.py:358] Since rank of parameter decoder/layers_8/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:35.684788 140440593786880 adafactor.py:358] Since rank of parameter decoder/layers_5/extra_mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:35.687216 140095893518336 adafactor.py:358] Since rank of parameter decoder/layers_8/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:35.685936 140595318249472 adafactor.py:358] Since rank of parameter decoder/layers_5/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:35.666047 140311617681408 adafactor.py:358] Since rank of parameter decoder/layers_5/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:35.686437 140031916288000 adafactor.py:358] Since rank of parameter decoder/layers_5/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:35.690947 140339465779200 adafactor.py:358] Since rank of parameter decoder/layers_5/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:35.677957 140529434355712 adafactor.py:358] Since rank of parameter decoder/layers_9/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:35.677605 140311617681408 adafactor.py:358] Since rank of parameter decoder/layers_5/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:35.699576 140595318249472 adafactor.py:358] Since rank of parameter decoder/layers_5/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:35.702173 140095893518336 adafactor.py:358] Since rank of parameter decoder/layers_8/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:35.700710 140031916288000 adafactor.py:358] Since rank of parameter decoder/layers_5/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:35.688137 140311617681408 adafactor.py:358] Since rank of parameter decoder/layers_5/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:35.711359 139674595104768 adafactor.py:358] Since rank of parameter decoder/layers_5/mlp/router/router_weights/w/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>, <FactorDim.NONE: None>) is ignored.
W0512 23:42:35.708941 140339465779200 adafactor.py:358] Since rank of parameter decoder/layers_6/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:35.713288 140595318249472 adafactor.py:358] Since rank of parameter decoder/layers_6/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:35.695693 140529434355712 adafactor.py:358] Since rank of parameter decoder/layers_9/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:35.717177 140095893518336 adafactor.py:358] Since rank of parameter decoder/layers_8/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:35.714534 140031916288000 adafactor.py:358] Since rank of parameter decoder/layers_5/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
W0512 23:42:35.723163 139674595104768 adafactor.py:358] Since rank of parameter decoder/layers_5/pre_extra_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:35.703489 140311617681408 adafactor.py:358] Since rank of parameter decoder/layers_5/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:35.709571 140529434355712 adafactor.py:358] Since rank of parameter decoder/layers_9/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:35.726505 140339465779200 adafactor.py:358] Since rank of parameter decoder/layers_6/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:35.728553 140031916288000 adafactor.py:358] Since rank of parameter decoder/layers_6/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:35.729402 140595318249472 adafactor.py:358] Since rank of parameter decoder/layers_6/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:35.732267 140095893518336 adafactor.py:358] Since rank of parameter decoder/layers_9/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:35.733968 139674595104768 adafactor.py:358] Since rank of parameter decoder/layers_5/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:35.719862 140311617681408 adafactor.py:358] Since rank of parameter decoder/layers_5/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:35.741237 140440593786880 adafactor.py:358] Since rank of parameter decoder/layers_5/mlp/router/router_weights/w/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>, <FactorDim.NONE: None>) is ignored.
W0512 23:42:35.723262 140529434355712 adafactor.py:358] Since rank of parameter decoder/layers_9/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:35.744647 139674595104768 adafactor.py:358] Since rank of parameter decoder/layers_5/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:35.742988 140031916288000 adafactor.py:358] Since rank of parameter decoder/layers_6/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:35.747030 140095893518336 adafactor.py:358] Since rank of parameter decoder/layers_9/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:35.745157 140339465779200 adafactor.py:358] Since rank of parameter decoder/layers_6/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:35.748362 140595318249472 adafactor.py:358] Since rank of parameter decoder/layers_6/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:35.733277 140529434355712 adafactor.py:358] Since rank of parameter decoder/layers_9/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:35.752132 140440593786880 adafactor.py:358] Since rank of parameter decoder/layers_5/pre_extra_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:35.755386 139674595104768 adafactor.py:358] Since rank of parameter decoder/layers_5/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:35.736675 140311617681408 adafactor.py:358] Since rank of parameter decoder/layers_5/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:35.757962 140031916288000 adafactor.py:358] Since rank of parameter decoder/layers_6/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:35.761898 140095893518336 adafactor.py:358] Since rank of parameter decoder/layers_9/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:35.743341 140529434355712 adafactor.py:358] Since rank of parameter decoder/layers_9/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:35.762367 140440593786880 adafactor.py:358] Since rank of parameter decoder/layers_5/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:35.762166 140339465779200 adafactor.py:358] Since rank of parameter decoder/layers_6/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:35.766944 140595318249472 adafactor.py:358] Since rank of parameter decoder/layers_6/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:35.770298 139674595104768 adafactor.py:358] Since rank of parameter decoder/layers_5/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:35.751007 140311617681408 adafactor.py:358] Since rank of parameter decoder/layers_6/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:35.772529 140440593786880 adafactor.py:358] Since rank of parameter decoder/layers_5/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:35.776427 140095893518336 adafactor.py:358] Since rank of parameter decoder/layers_9/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:35.773510 140031916288000 adafactor.py:358] Since rank of parameter decoder/layers_6/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:35.757472 140529434355712 adafactor.py:358] Since rank of parameter decoder/layers_9/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:35.774704 140339465779200 adafactor.py:358] Since rank of parameter decoder/layers_6/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:35.782599 140595318249472 adafactor.py:358] Since rank of parameter decoder/layers_6/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:35.782724 140440593786880 adafactor.py:358] Since rank of parameter decoder/layers_5/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:35.784376 139674595104768 adafactor.py:358] Since rank of parameter decoder/layers_5/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:35.765240 140311617681408 adafactor.py:358] Since rank of parameter decoder/layers_6/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:35.787358 140095893518336 adafactor.py:358] Since rank of parameter decoder/layers_9/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:35.784622 140031916288000 adafactor.py:358] Since rank of parameter decoder/layers_6/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:35.771108 140529434355712 adafactor.py:358] Since rank of parameter decoder/layers_9/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:35.787596 140339465779200 adafactor.py:358] Since rank of parameter decoder/layers_6/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:35.795901 140595318249472 adafactor.py:358] Since rank of parameter decoder/layers_6/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:35.798149 140095893518336 adafactor.py:358] Since rank of parameter decoder/layers_9/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:35.795719 140031916288000 adafactor.py:358] Since rank of parameter decoder/layers_6/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:35.796546 140440593786880 adafactor.py:358] Since rank of parameter decoder/layers_5/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:35.798039 139674595104768 adafactor.py:358] Since rank of parameter decoder/layers_5/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:35.779573 140311617681408 adafactor.py:358] Since rank of parameter decoder/layers_6/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:35.785612 140529434355712 adafactor.py:358] Since rank of parameter decoder/layers_9/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:35.812734 140095893518336 adafactor.py:358] Since rank of parameter decoder/layers_9/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:35.810712 140440593786880 adafactor.py:358] Since rank of parameter decoder/layers_5/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:35.812123 139674595104768 adafactor.py:358] Since rank of parameter decoder/layers_6/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:35.810652 140339465779200 adafactor.py:358] Since rank of parameter decoder/layers_6/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:35.813565 140595318249472 adafactor.py:358] Since rank of parameter decoder/layers_6/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:35.794599 140311617681408 adafactor.py:358] Since rank of parameter decoder/layers_6/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:35.814405 140031916288000 adafactor.py:358] Since rank of parameter decoder/layers_6/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:35.804410 140529434355712 adafactor.py:358] Since rank of parameter decoder/logits_dense/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:35.824589 140440593786880 adafactor.py:358] Since rank of parameter decoder/layers_5/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:35.826060 139674595104768 adafactor.py:358] Since rank of parameter decoder/layers_6/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:35.805463 140311617681408 adafactor.py:358] Since rank of parameter decoder/layers_6/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:35.828216 140095893518336 adafactor.py:358] Since rank of parameter decoder/layers_9/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:35.829453 140031916288000 adafactor.py:358] Since rank of parameter decoder/layers_6/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:35.830104 140595318249472 adafactor.py:358] Since rank of parameter decoder/layers_6/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:35.831816 140339465779200 adafactor.py:358] Since rank of parameter decoder/layers_6/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:35.816163 140311617681408 adafactor.py:358] Since rank of parameter decoder/layers_6/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:35.838454 140440593786880 adafactor.py:358] Since rank of parameter decoder/layers_6/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:35.841755 139674595104768 adafactor.py:358] Since rank of parameter decoder/layers_6/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:35.843542 140095893518336 adafactor.py:358] Since rank of parameter decoder/layers_9/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:35.843759 140031916288000 adafactor.py:358] Since rank of parameter decoder/layers_6/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:35.844482 140595318249472 adafactor.py:358] Since rank of parameter decoder/layers_6/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:35.828177 140529434355712 adafactor.py:358] Since rank of parameter token_embedder/embedding 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:35.852331 140440593786880 adafactor.py:358] Since rank of parameter decoder/layers_6/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:35.851983 140339465779200 adafactor.py:358] Since rank of parameter decoder/layers_6/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:35.858397 140095893518336 adafactor.py:358] Since rank of parameter decoder/logits_dense/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:35.858537 139674595104768 adafactor.py:358] Since rank of parameter decoder/layers_6/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:35.837982 140311617681408 adafactor.py:358] Since rank of parameter decoder/layers_6/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:35.857503 140031916288000 adafactor.py:358] Since rank of parameter decoder/layers_7/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:35.862601 140595318249472 adafactor.py:358] Since rank of parameter decoder/layers_7/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:35.866291 140440593786880 adafactor.py:358] Since rank of parameter decoder/layers_6/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:35.869668 139674595104768 adafactor.py:358] Since rank of parameter decoder/layers_6/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:35.871401 140031916288000 adafactor.py:358] Since rank of parameter decoder/layers_7/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:35.852790 140311617681408 adafactor.py:358] Since rank of parameter decoder/layers_6/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:35.873515 140339465779200 adafactor.py:358] Since rank of parameter decoder/layers_7/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:35.879990 139674595104768 adafactor.py:358] Since rank of parameter decoder/layers_6/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:35.880695 140440593786880 adafactor.py:358] Since rank of parameter decoder/layers_6/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:35.881602 140595318249472 adafactor.py:358] Since rank of parameter decoder/layers_7/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:35.884226 140095893518336 adafactor.py:358] Since rank of parameter token_embedder/embedding 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:35.885208 140031916288000 adafactor.py:358] Since rank of parameter decoder/layers_7/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:35.868404 140311617681408 adafactor.py:358] Since rank of parameter decoder/layers_6/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:35.890887 140440593786880 adafactor.py:358] Since rank of parameter decoder/layers_6/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:35.895985 139674595104768 adafactor.py:358] Since rank of parameter decoder/layers_6/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:35.894175 140339465779200 adafactor.py:358] Since rank of parameter decoder/layers_7/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:35.895990 140595318249472 adafactor.py:358] Since rank of parameter decoder/layers_7/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:35.899087 140031916288000 adafactor.py:358] Since rank of parameter decoder/layers_7/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:35.901209 140440593786880 adafactor.py:358] Since rank of parameter decoder/layers_6/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:35.882989 140311617681408 adafactor.py:358] Since rank of parameter decoder/layers_7/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
W0512 23:42:35.910214 139674595104768 adafactor.py:358] Since rank of parameter decoder/layers_6/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:35.909773 140031916288000 adafactor.py:358] Since rank of parameter decoder/layers_7/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:35.910975 140595318249472 adafactor.py:358] Since rank of parameter decoder/layers_7/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:35.914512 140339465779200 adafactor.py:358] Since rank of parameter decoder/layers_7/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:35.916083 140440593786880 adafactor.py:358] Since rank of parameter decoder/layers_6/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:35.897408 140311617681408 adafactor.py:358] Since rank of parameter decoder/layers_7/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:35.919884 140031916288000 adafactor.py:358] Since rank of parameter decoder/layers_7/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:35.924185 139674595104768 adafactor.py:358] Since rank of parameter decoder/layers_6/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:35.923660 140595318249472 adafactor.py:358] Since rank of parameter decoder/layers_7/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:35.912369 140311617681408 adafactor.py:358] Since rank of parameter decoder/layers_7/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:35.933334 140440593786880 adafactor.py:358] Since rank of parameter decoder/layers_6/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:35.933648 140031916288000 adafactor.py:358] Since rank of parameter decoder/layers_7/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:35.935455 140595318249472 adafactor.py:358] Since rank of parameter decoder/layers_7/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:35.939013 139674595104768 adafactor.py:358] Since rank of parameter decoder/layers_7/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:35.936656 140339465779200 adafactor.py:358] Since rank of parameter decoder/layers_7/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:35.947523 140031916288000 adafactor.py:358] Since rank of parameter decoder/layers_7/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:35.950354 140440593786880 adafactor.py:358] Since rank of parameter decoder/layers_6/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:35.950883 140595318249472 adafactor.py:358] Since rank of parameter decoder/layers_7/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:35.931859 140311617681408 adafactor.py:358] Since rank of parameter decoder/layers_7/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:35.953565 139674595104768 adafactor.py:358] Since rank of parameter decoder/layers_7/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:35.952102 140339465779200 adafactor.py:358] Since rank of parameter decoder/layers_7/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:35.961363 140031916288000 adafactor.py:358] Since rank of parameter decoder/layers_7/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:35.966010 140595318249472 adafactor.py:358] Since rank of parameter decoder/layers_7/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:35.966360 140440593786880 adafactor.py:358] Since rank of parameter decoder/layers_7/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:35.967926 139674595104768 adafactor.py:358] Since rank of parameter decoder/layers_7/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:35.947069 140311617681408 adafactor.py:358] Since rank of parameter decoder/layers_7/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:35.968618 140339465779200 adafactor.py:358] Since rank of parameter decoder/layers_7/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:35.957987 140311617681408 adafactor.py:358] Since rank of parameter decoder/layers_7/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:35.980223 140595318249472 adafactor.py:358] Since rank of parameter decoder/layers_7/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:35.979616 140031916288000 adafactor.py:358] Since rank of parameter decoder/layers_8/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:35.981898 139674595104768 adafactor.py:358] Since rank of parameter decoder/layers_7/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:35.982796 140440593786880 adafactor.py:358] Since rank of parameter decoder/layers_7/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:35.985670 140339465779200 adafactor.py:358] Since rank of parameter decoder/layers_7/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:35.992993 139674595104768 adafactor.py:358] Since rank of parameter decoder/layers_7/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:35.974055 140311617681408 adafactor.py:358] Since rank of parameter decoder/layers_7/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:35.993582 140031916288000 adafactor.py:358] Since rank of parameter decoder/layers_8/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:35.994358 140595318249472 adafactor.py:358] Since rank of parameter decoder/layers_8/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:36.000097 140339465779200 adafactor.py:358] Since rank of parameter decoder/layers_7/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:36.001646 140440593786880 adafactor.py:358] Since rank of parameter decoder/layers_7/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:36.003107 139674595104768 adafactor.py:358] Since rank of parameter decoder/layers_7/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:36.007488 140031916288000 adafactor.py:358] Since rank of parameter decoder/layers_8/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:36.008528 140595318249472 adafactor.py:358] Since rank of parameter decoder/layers_8/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:35.989123 140311617681408 adafactor.py:358] Since rank of parameter decoder/layers_7/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:36.014133 140339465779200 adafactor.py:358] Since rank of parameter decoder/layers_7/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:36.016998 139674595104768 adafactor.py:358] Since rank of parameter decoder/layers_7/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:36.018081 140440593786880 adafactor.py:358] Since rank of parameter decoder/layers_7/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:36.021324 140031916288000 adafactor.py:358] Since rank of parameter decoder/layers_8/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:36.022658 140595318249472 adafactor.py:358] Since rank of parameter decoder/layers_8/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:36.003653 140311617681408 adafactor.py:358] Since rank of parameter decoder/layers_7/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:36.027987 140339465779200 adafactor.py:358] Since rank of parameter decoder/layers_8/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:36.030908 139674595104768 adafactor.py:358] Since rank of parameter decoder/layers_7/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:36.030821 140440593786880 adafactor.py:358] Since rank of parameter decoder/layers_7/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:36.031642 140031916288000 adafactor.py:358] Since rank of parameter decoder/layers_8/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
W0512 23:42:36.017855 140311617681408 adafactor.py:358] Since rank of parameter decoder/layers_8/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:36.038754 140595318249472 adafactor.py:358] Since rank of parameter decoder/layers_8/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:36.041818 140031916288000 adafactor.py:358] Since rank of parameter decoder/layers_8/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:36.042050 140339465779200 adafactor.py:358] Since rank of parameter decoder/layers_8/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:36.046235 139674595104768 adafactor.py:358] Since rank of parameter decoder/layers_7/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:36.045377 140440593786880 adafactor.py:358] Since rank of parameter decoder/layers_7/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:36.049412 140595318249472 adafactor.py:358] Since rank of parameter decoder/layers_8/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:36.034426 140311617681408 adafactor.py:358] Since rank of parameter decoder/layers_8/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:36.056252 140031916288000 adafactor.py:358] Since rank of parameter decoder/layers_8/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:36.056054 140339465779200 adafactor.py:358] Since rank of parameter decoder/layers_8/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:36.059981 139674595104768 adafactor.py:358] Since rank of parameter decoder/layers_8/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:36.059712 140595318249472 adafactor.py:358] Since rank of parameter decoder/layers_8/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:36.059988 140440593786880 adafactor.py:358] Since rank of parameter decoder/layers_7/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:36.070174 140031916288000 adafactor.py:358] Since rank of parameter decoder/layers_8/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:36.070414 140339465779200 adafactor.py:358] Since rank of parameter decoder/layers_8/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:36.052472 140311617681408 adafactor.py:358] Since rank of parameter decoder/layers_8/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:36.073807 139674595104768 adafactor.py:358] Since rank of parameter decoder/layers_8/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:36.074123 140440593786880 adafactor.py:358] Since rank of parameter decoder/layers_7/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:36.074394 140595318249472 adafactor.py:358] Since rank of parameter decoder/layers_8/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:36.081535 140339465779200 adafactor.py:358] Since rank of parameter decoder/layers_8/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:36.083994 140031916288000 adafactor.py:358] Since rank of parameter decoder/layers_8/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:36.087647 139674595104768 adafactor.py:358] Since rank of parameter decoder/layers_8/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:36.067198 140311617681408 adafactor.py:358] Since rank of parameter decoder/layers_8/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:36.088112 140440593786880 adafactor.py:358] Since rank of parameter decoder/layers_7/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:36.089485 140595318249472 adafactor.py:358] Since rank of parameter decoder/layers_8/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:36.091947 140339465779200 adafactor.py:358] Since rank of parameter decoder/layers_8/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:36.097782 140031916288000 adafactor.py:358] Since rank of parameter decoder/layers_9/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:36.078903 140311617681408 adafactor.py:358] Since rank of parameter decoder/layers_8/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:36.101349 139674595104768 adafactor.py:358] Since rank of parameter decoder/layers_8/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:36.102129 140440593786880 adafactor.py:358] Since rank of parameter decoder/layers_8/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:36.104441 140595318249472 adafactor.py:358] Since rank of parameter decoder/layers_8/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:36.106227 140339465779200 adafactor.py:358] Since rank of parameter decoder/layers_8/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:36.090129 140311617681408 adafactor.py:358] Since rank of parameter decoder/layers_8/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:36.111566 139674595104768 adafactor.py:358] Since rank of parameter decoder/layers_8/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:36.111632 140031916288000 adafactor.py:358] Since rank of parameter decoder/layers_9/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:36.116541 140440593786880 adafactor.py:358] Since rank of parameter decoder/layers_8/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:36.121690 139674595104768 adafactor.py:358] Since rank of parameter decoder/layers_8/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:36.121885 140595318249472 adafactor.py:358] Since rank of parameter decoder/layers_9/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:36.120819 140339465779200 adafactor.py:358] Since rank of parameter decoder/layers_8/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:36.106378 140311617681408 adafactor.py:358] Since rank of parameter decoder/layers_8/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:36.125642 140031916288000 adafactor.py:358] Since rank of parameter decoder/layers_9/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:36.131319 140440593786880 adafactor.py:358] Since rank of parameter decoder/layers_8/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:36.135864 139674595104768 adafactor.py:358] Since rank of parameter decoder/layers_8/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:36.135775 140339465779200 adafactor.py:358] Since rank of parameter decoder/layers_8/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:36.137559 140595318249472 adafactor.py:358] Since rank of parameter decoder/layers_9/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:36.139348 140031916288000 adafactor.py:358] Since rank of parameter decoder/layers_9/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:36.121751 140311617681408 adafactor.py:358] Since rank of parameter decoder/layers_8/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:36.146476 140440593786880 adafactor.py:358] Since rank of parameter decoder/layers_8/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:36.149632 139674595104768 adafactor.py:358] Since rank of parameter decoder/layers_8/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:36.149444 140031916288000 adafactor.py:358] Since rank of parameter decoder/layers_9/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:36.151641 140595318249472 adafactor.py:358] Since rank of parameter decoder/layers_9/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:36.150351 140339465779200 adafactor.py:358] Since rank of parameter decoder/layers_9/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:36.156849 140440593786880 adafactor.py:358] Since rank of parameter decoder/layers_8/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:36.137926 140311617681408 adafactor.py:358] Since rank of parameter decoder/layers_8/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:36.159571 140031916288000 adafactor.py:358] Since rank of parameter decoder/layers_9/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:36.163456 139674595104768 adafactor.py:358] Since rank of parameter decoder/layers_8/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:36.165863 140595318249472 adafactor.py:358] Since rank of parameter decoder/layers_9/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:36.165520 140339465779200 adafactor.py:358] Since rank of parameter decoder/layers_9/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:36.167162 140440593786880 adafactor.py:358] Since rank of parameter decoder/layers_8/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
W0512 23:42:36.153180 140311617681408 adafactor.py:358] Since rank of parameter decoder/layers_9/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:36.177139 139674595104768 adafactor.py:358] Since rank of parameter decoder/layers_9/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:36.174804 140031916288000 adafactor.py:358] Since rank of parameter decoder/layers_9/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:36.176161 140595318249472 adafactor.py:358] Since rank of parameter decoder/layers_9/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:36.181924 140440593786880 adafactor.py:358] Since rank of parameter decoder/layers_8/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:36.180747 140339465779200 adafactor.py:358] Since rank of parameter decoder/layers_9/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:36.186495 140595318249472 adafactor.py:358] Since rank of parameter decoder/layers_9/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:36.167991 140311617681408 adafactor.py:358] Since rank of parameter decoder/layers_9/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:36.191041 139674595104768 adafactor.py:358] Since rank of parameter decoder/layers_9/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:36.191808 140031916288000 adafactor.py:358] Since rank of parameter decoder/layers_9/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:36.196067 140440593786880 adafactor.py:358] Since rank of parameter decoder/layers_8/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:36.194793 140339465779200 adafactor.py:358] Since rank of parameter decoder/layers_9/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:36.200879 140595318249472 adafactor.py:358] Since rank of parameter decoder/layers_9/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:36.205272 139674595104768 adafactor.py:358] Since rank of parameter decoder/layers_9/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:36.185438 140311617681408 adafactor.py:358] Since rank of parameter decoder/layers_9/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:36.205100 140339465779200 adafactor.py:358] Since rank of parameter decoder/layers_9/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:36.210355 140440593786880 adafactor.py:358] Since rank of parameter decoder/layers_8/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:36.210240 140031916288000 adafactor.py:358] Since rank of parameter decoder/layers_9/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:36.216625 140595318249472 adafactor.py:358] Since rank of parameter decoder/layers_9/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:36.215270 140339465779200 adafactor.py:358] Since rank of parameter decoder/layers_9/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:36.221558 139674595104768 adafactor.py:358] Since rank of parameter decoder/layers_9/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:36.200599 140311617681408 adafactor.py:358] Since rank of parameter decoder/layers_9/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:36.228485 140440593786880 adafactor.py:358] Since rank of parameter decoder/layers_9/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:36.228626 140031916288000 adafactor.py:358] Since rank of parameter decoder/logits_dense/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:36.229663 140339465779200 adafactor.py:358] Since rank of parameter decoder/layers_9/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:36.235045 139674595104768 adafactor.py:358] Since rank of parameter decoder/layers_9/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:36.234178 140595318249472 adafactor.py:358] Since rank of parameter decoder/layers_9/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:36.217192 140311617681408 adafactor.py:358] Since rank of parameter decoder/layers_9/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:36.243149 140440593786880 adafactor.py:358] Since rank of parameter decoder/layers_9/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:36.245887 139674595104768 adafactor.py:358] Since rank of parameter decoder/layers_9/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:36.243573 140339465779200 adafactor.py:358] Since rank of parameter decoder/layers_9/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:36.248502 140595318249472 adafactor.py:358] Since rank of parameter decoder/logits_dense/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:36.236901 140311617681408 adafactor.py:358] Since rank of parameter decoder/layers_9/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:36.257669 140440593786880 adafactor.py:358] Since rank of parameter decoder/layers_9/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:36.257460 140031916288000 adafactor.py:358] Since rank of parameter token_embedder/embedding 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:36.257648 140339465779200 adafactor.py:358] Since rank of parameter decoder/layers_9/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:36.260931 139674595104768 adafactor.py:358] Since rank of parameter decoder/layers_9/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:36.271870 140440593786880 adafactor.py:358] Since rank of parameter decoder/layers_9/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:36.271337 140339465779200 adafactor.py:358] Since rank of parameter decoder/logits_dense/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:36.272947 140595318249472 adafactor.py:358] Since rank of parameter token_embedder/embedding 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:36.254869 140311617681408 adafactor.py:358] Since rank of parameter decoder/layers_9/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:36.278125 139674595104768 adafactor.py:358] Since rank of parameter decoder/layers_9/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:36.282721 140440593786880 adafactor.py:358] Since rank of parameter decoder/layers_9/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:42:36.293285 139674595104768 adafactor.py:358] Since rank of parameter decoder/layers_9/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:36.293291 140440593786880 adafactor.py:358] Since rank of parameter decoder/layers_9/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:36.274241 140311617681408 adafactor.py:358] Since rank of parameter decoder/layers_9/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:36.296025 140339465779200 adafactor.py:358] Since rank of parameter token_embedder/embedding 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:36.307726 139674595104768 adafactor.py:358] Since rank of parameter decoder/logits_dense/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:36.307686 140440593786880 adafactor.py:358] Since rank of parameter decoder/layers_9/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:36.295953 140311617681408 adafactor.py:358] Since rank of parameter decoder/layers_9/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:36.325325 140440593786880 adafactor.py:358] Since rank of parameter decoder/layers_9/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:36.332477 139674595104768 adafactor.py:358] Since rank of parameter token_embedder/embedding 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:36.317217 140311617681408 adafactor.py:358] Since rank of parameter decoder/logits_dense/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:36.339516 140440593786880 adafactor.py:358] Since rank of parameter decoder/layers_9/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:36.353605 140440593786880 adafactor.py:358] Since rank of parameter decoder/logits_dense/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:42:36.378060 140440593786880 adafactor.py:358] Since rank of parameter token_embedder/embedding 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:42:36.357086 140311617681408 adafactor.py:358] Since rank of parameter token_embedder/embedding 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
I0512 23:46:55.708817 140095893518336 train.py:675] Epoch 13 of 200
I0512 23:46:55.709185 140095893518336 train.py:681] BEGIN Train loop.
I0512 23:46:55.709235 140095893518336 train.py:686] Training for 2500 steps.
I0512 23:46:55.714636 140095893518336 trainer.py:518] Training: step 32500
I0512 23:46:56.759634 140440593786880 train.py:675] Epoch 13 of 200
I0512 23:46:56.760139 140440593786880 train.py:681] BEGIN Train loop.
I0512 23:46:56.760190 140440593786880 train.py:686] Training for 2500 steps.
I0512 23:46:56.766393 140440593786880 trainer.py:518] Training: step 32500
I0512 23:47:05.638316 140595318249472 train.py:675] Epoch 13 of 200
I0512 23:47:05.638848 140595318249472 train.py:681] BEGIN Train loop.
I0512 23:47:05.638900 140595318249472 train.py:686] Training for 2500 steps.
I0512 23:47:05.647504 140595318249472 trainer.py:518] Training: step 32500
I0512 23:47:08.025720 140529434355712 train.py:675] Epoch 13 of 200
I0512 23:47:08.026447 140439986517568 logging_writer.py:48] [32500] collection=train timing/compilation_seconds=282.28
I0512 23:47:08.028817 140529434355712 train.py:681] BEGIN Train loop.
I0512 23:47:08.029446 140439986517568 logging_writer.py:48] [32500] collection=train timing/train_iter_warmup=1.64509e-05
I0512 23:47:08.029615 140529434355712 train.py:686] Training for 2500 steps.
I0512 23:47:08.035233 140529434355712 trainer.py:518] Training: step 32500
I0512 23:47:09.686017 140311617681408 train.py:675] Epoch 13 of 200
I0512 23:47:09.686578 140311617681408 train.py:681] BEGIN Train loop.
I0512 23:47:09.686637 140311617681408 train.py:686] Training for 2500 steps.
I0512 23:47:09.695293 140311617681408 trainer.py:518] Training: step 32500
I0512 23:47:15.416078 140339465779200 train.py:675] Epoch 13 of 200
I0512 23:47:15.416563 140339465779200 train.py:681] BEGIN Train loop.
I0512 23:47:15.416616 140339465779200 train.py:686] Training for 2500 steps.
I0512 23:47:15.422410 140339465779200 trainer.py:518] Training: step 32500
I0512 23:47:22.151899 140031916288000 train.py:675] Epoch 13 of 200
I0512 23:47:22.152340 140031916288000 train.py:681] BEGIN Train loop.
I0512 23:47:22.152388 140031916288000 train.py:686] Training for 2500 steps.
I0512 23:47:22.158719 140031916288000 trainer.py:518] Training: step 32500
I0512 23:47:32.792262 139674595104768 train.py:675] Epoch 13 of 200
I0512 23:47:32.792666 139674595104768 train.py:681] BEGIN Train loop.
I0512 23:47:32.792718 139674595104768 train.py:686] Training for 2500 steps.
I0512 23:47:32.799104 139674595104768 trainer.py:518] Training: step 32500
I0512 23:47:47.636199 140095893518336 trainer.py:518] Training: step 32513
I0512 23:47:47.636452 139674595104768 trainer.py:518] Training: step 32513
I0512 23:47:47.636937 140440593786880 trainer.py:518] Training: step 32513
I0512 23:47:47.637706 140339465779200 trainer.py:518] Training: step 32513
I0512 23:47:47.639255 140595318249472 trainer.py:518] Training: step 32513
I0512 23:47:47.643237 140031916288000 trainer.py:518] Training: step 32513
I0512 23:47:47.636336 140529434355712 trainer.py:518] Training: step 32513
I0512 23:47:47.638244 140311617681408 trainer.py:518] Training: step 32513
I0512 23:48:05.556056 140440593786880 trainer.py:518] Training: step 32515
I0512 23:48:05.581665 140095893518336 trainer.py:518] Training: step 32515
I0512 23:48:05.592504 139674595104768 trainer.py:518] Training: step 32515
I0512 23:48:05.575088 140529434355712 trainer.py:518] Training: step 32515
I0512 23:48:05.628809 140339465779200 trainer.py:518] Training: step 32515
I0512 23:48:05.639209 140595318249472 trainer.py:518] Training: step 32515
I0512 23:48:05.809955 140311617681408 trainer.py:518] Training: step 32515
I0512 23:48:05.840109 140031916288000 trainer.py:518] Training: step 32515
I0512 23:48:23.458187 139674595104768 trainer.py:518] Training: step 32517
I0512 23:48:23.459307 140339465779200 trainer.py:518] Training: step 32517
I0512 23:48:23.476080 140095893518336 trainer.py:518] Training: step 32517
I0512 23:48:23.473191 140311617681408 trainer.py:518] Training: step 32517
I0512 23:48:23.513788 140529434355712 trainer.py:518] Training: step 32517
I0512 23:48:23.600832 140440593786880 trainer.py:518] Training: step 32517
I0512 23:48:23.656460 140595318249472 trainer.py:518] Training: step 32517
I0512 23:48:23.763374 140031916288000 trainer.py:518] Training: step 32517
I0512 23:48:41.357219 140440593786880 trainer.py:518] Training: step 32519
I0512 23:48:41.366550 140339465779200 trainer.py:518] Training: step 32519
I0512 23:48:41.365994 140529434355712 trainer.py:518] Training: step 32519
I0512 23:48:41.375965 140311617681408 trainer.py:518] Training: step 32519
I0512 23:48:41.405381 140595318249472 trainer.py:518] Training: step 32519
I0512 23:48:41.410148 140095893518336 trainer.py:518] Training: step 32519
I0512 23:48:41.686342 140031916288000 trainer.py:518] Training: step 32519
I0512 23:48:41.998053 139674595104768 trainer.py:518] Training: step 32519
I0512 23:49:08.148382 140095893518336 trainer.py:518] Training: step 32520
I0512 23:49:08.149474 140595318249472 trainer.py:518] Training: step 32520
I0512 23:49:08.149123 140031916288000 trainer.py:518] Training: step 32520
I0512 23:49:08.149161 140339465779200 trainer.py:518] Training: step 32520
I0512 23:49:08.150514 140529434355712 trainer.py:518] Training: step 32520
I0512 23:49:08.190119 140311617681408 trainer.py:518] Training: step 32520
I0512 23:49:08.247942 139674595104768 trainer.py:518] Training: step 32520
I0512 23:49:08.311727 140440593786880 trainer.py:518] Training: step 32520
I0512 23:49:26.039305 140095893518336 trainer.py:518] Training: step 32523
I0512 23:49:26.040462 139674595104768 trainer.py:518] Training: step 32523
I0512 23:49:26.039566 140339465779200 trainer.py:518] Training: step 32523
I0512 23:49:26.043206 140595318249472 trainer.py:518] Training: step 32523
I0512 23:49:26.043542 140440593786880 trainer.py:518] Training: step 32522
I0512 23:49:26.049654 140031916288000 trainer.py:518] Training: step 32523
I0512 23:49:26.043059 140529434355712 trainer.py:518] Training: step 32523
I0512 23:49:26.046865 140311617681408 trainer.py:518] Training: step 32523
I0512 23:49:43.928130 140031916288000 trainer.py:518] Training: step 32524
I0512 23:49:43.934668 139674595104768 trainer.py:518] Training: step 32524
I0512 23:49:43.933129 140339465779200 trainer.py:518] Training: step 32524
I0512 23:49:43.935868 140440593786880 trainer.py:518] Training: step 32523
I0512 23:49:43.938063 140529434355712 trainer.py:518] Training: step 32524
I0512 23:49:43.957230 140595318249472 trainer.py:518] Training: step 32524
I0512 23:49:43.999497 140095893518336 trainer.py:518] Training: step 32524
I0512 23:49:43.977370 140311617681408 trainer.py:518] Training: step 32524
I0512 23:50:01.821981 139674595104768 trainer.py:518] Training: step 32525
I0512 23:50:01.822143 140595318249472 trainer.py:518] Training: step 32525
I0512 23:50:01.822941 140031916288000 trainer.py:518] Training: step 32525
I0512 23:50:01.823554 140339465779200 trainer.py:518] Training: step 32525
I0512 23:50:01.829937 140095893518336 trainer.py:518] Training: step 32525
I0512 23:50:01.835429 140440593786880 trainer.py:518] Training: step 32524
I0512 23:50:01.819938 140529434355712 trainer.py:518] Training: step 32525
I0512 23:50:01.820190 140311617681408 trainer.py:518] Training: step 32525
I0512 23:50:19.711080 139674595104768 trainer.py:518] Training: step 32526
I0512 23:50:19.711085 140440593786880 trainer.py:518] Training: step 32525
I0512 23:50:19.711190 140031916288000 trainer.py:518] Training: step 32526
I0512 23:50:19.712163 140595318249472 trainer.py:518] Training: step 32526
I0512 23:50:19.724134 140095893518336 trainer.py:518] Training: step 32526
I0512 23:50:19.717746 140529434355712 trainer.py:518] Training: step 32526
I0512 23:50:19.718958 140311617681408 trainer.py:518] Training: step 32526
I0512 23:50:19.818804 140339465779200 trainer.py:518] Training: step 32526
I0512 23:50:37.599793 140095893518336 trainer.py:518] Training: step 32527
I0512 23:50:37.601622 139674595104768 trainer.py:518] Training: step 32527
I0512 23:50:37.602147 140595318249472 trainer.py:518] Training: step 32527
I0512 23:50:37.608227 140440593786880 trainer.py:518] Training: step 32526
I0512 23:50:37.611202 140339465779200 trainer.py:518] Training: step 32527
I0512 23:50:37.613391 140031916288000 trainer.py:518] Training: step 32527
I0512 23:50:37.604394 140529434355712 trainer.py:518] Training: step 32527
I0512 23:50:37.606214 140311617681408 trainer.py:518] Training: step 32527
I0512 23:50:55.494105 140095893518336 trainer.py:518] Training: step 32529
I0512 23:50:55.493561 140595318249472 trainer.py:518] Training: step 32529
I0512 23:50:55.497129 140031916288000 trainer.py:518] Training: step 32529
I0512 23:50:55.501358 139674595104768 trainer.py:518] Training: step 32529
I0512 23:50:55.503674 140339465779200 trainer.py:518] Training: step 32529
I0512 23:50:55.491995 140529434355712 trainer.py:518] Training: step 32529
I0512 23:50:55.493160 140311617681408 trainer.py:518] Training: step 32529
I0512 23:51:04.437002 140440593786880 trainer.py:518] Training: step 32529
I0512 23:51:31.306310 140095893518336 trainer.py:518] Training: step 32530
I0512 23:51:40.229622 140031916288000 trainer.py:518] Training: step 32530
I0512 23:51:40.252982 140339465779200 trainer.py:518] Training: step 32530
I0512 23:51:40.293560 140311617681408 trainer.py:518] Training: step 32530
I0512 23:51:49.165293 139674595104768 trainer.py:518] Training: step 32530
I0512 23:51:49.169086 140595318249472 trainer.py:518] Training: step 32530
I0512 23:51:49.164650 140529434355712 trainer.py:518] Training: step 32530
I0512 23:51:58.111243 140095893518336 trainer.py:518] Training: step 32532
I0512 23:51:58.111361 140339465779200 trainer.py:518] Training: step 32532
I0512 23:51:58.112902 140440593786880 trainer.py:518] Training: step 32530
I0512 23:51:58.113850 140311617681408 trainer.py:518] Training: step 32532
I0512 23:51:58.134796 140031916288000 trainer.py:518] Training: step 32532
I0512 23:52:07.053475 140439986517568 logging_writer.py:48] [32510] collection=train accuracy=0.573532, cross_ent_loss=16.347862243652344, cross_ent_loss_per_all_target_tokens=2.07874e-05, experts/auxiliary_loss=0.1607515662908554, experts/expert_usage=7.978900909423828, experts/fraction_tokens_left_behind=0.27227693796157837, experts/router_confidence=3.4338600635528564, experts/router_z_loss=0.0002985403116326779, learning_rate=0.00554662, learning_rate/current=0.00554623, loss=16.517118453979492, loss_per_all_target_tokens=2.10026e-05, loss_per_nonpadding_target_token=2.11123e-05, non_padding_fraction/loss_weights=0.994803, timing/seconds=120.07012176513672, timing/seqs=3840, timing/seqs_per_second=31.981311798095703, timing/seqs_per_second_per_core=0.9994159936904907, timing/steps_per_second=0.08328466862440109, timing/target_tokens_per_second=65497.7265625, timing/target_tokens_per_second_per_core=2046.803955078125, timing/uptime=286.058, z_loss=0.00820355024188757, z_loss_per_all_target_tokens=1.04314e-08
I0512 23:52:07.096266 140529434355712 trainer.py:518] Training: step 32532
I0512 23:52:16.020788 140339465779200 trainer.py:518] Training: step 32533
I0512 23:52:16.025060 140595318249472 trainer.py:518] Training: step 32532
I0512 23:52:16.026712 139674595104768 trainer.py:518] Training: step 32532
I0512 23:52:16.030157 140095893518336 trainer.py:518] Training: step 32533
I0512 23:52:16.028066 140031916288000 trainer.py:518] Training: step 32533
I0512 23:52:16.024173 140311617681408 trainer.py:518] Training: step 32533
I0512 23:52:16.191262 140440593786880 trainer.py:518] Training: step 32532
I0512 23:52:24.975485 140529434355712 trainer.py:518] Training: step 32533
I0512 23:52:24.980154 140439986517568 logging_writer.py:48] [32520] collection=train accuracy=0.57801, cross_ent_loss=16.1187801361084, cross_ent_loss_per_all_target_tokens=2.04961e-05, experts/auxiliary_loss=0.16071061789989471, experts/expert_usage=7.968201637268066, experts/fraction_tokens_left_behind=0.2610667943954468, experts/router_confidence=3.4319303035736084, experts/router_z_loss=0.00030070674256421626, learning_rate=0.00554577, learning_rate/current=0.00554538, loss=16.287527084350586, loss_per_all_target_tokens=2.07107e-05, loss_per_nonpadding_target_token=2.08045e-05, non_padding_fraction/loss_weights=0.995488, timing/seconds=89.43501281738281, timing/seqs=3840, timing/seqs_per_second=42.93620300292969, timing/seqs_per_second_per_core=1.3417563438415527, timing/steps_per_second=0.1118130311369896, timing/target_tokens_per_second=87933.34375, timing/target_tokens_per_second_per_core=2747.9169921875, timing/uptime=385.709, z_loss=0.007738082204014063, z_loss_per_all_target_tokens=9.83948e-09
I0512 23:52:26.071137 140595318249472 trainer.py:518] Training: step 32539
I0512 23:52:26.076160 139674595104768 trainer.py:518] Training: step 32538
I0512 23:52:33.923476 140311617681408 trainer.py:518] Training: step 32534
I0512 23:52:33.927606 140439986517568 logging_writer.py:48] [32530] collection=train accuracy=0.56931, cross_ent_loss=16.585712432861328, cross_ent_loss_per_all_target_tokens=2.10898e-05, experts/auxiliary_loss=0.16068804264068604, experts/expert_usage=7.959143161773682, experts/fraction_tokens_left_behind=0.25840693712234497, experts/router_confidence=3.4705567359924316, experts/router_z_loss=0.0003102955233771354, learning_rate=0.00554491, learning_rate/current=0.00554453, loss=16.754568099975586, loss_per_all_target_tokens=2.13045e-05, loss_per_nonpadding_target_token=2.14252e-05, non_padding_fraction/loss_weights=0.994368, timing/seconds=89.44866943359375, timing/seqs=3840, timing/seqs_per_second=42.929649353027344, timing/seqs_per_second_per_core=1.3415515422821045, timing/steps_per_second=0.11179596185684204, timing/target_tokens_per_second=87919.921875, timing/target_tokens_per_second_per_core=2747.49755859375, timing/uptime=528.303, z_loss=0.007856997661292553, z_loss_per_all_target_tokens=9.99069e-09
I0512 23:52:33.957573 140095893518336 trainer.py:518] Training: step 32535
I0512 23:52:34.087408 140440593786880 trainer.py:518] Training: step 32545
I0512 23:52:42.868343 140031916288000 trainer.py:518] Training: step 32535
I0512 23:52:42.897473 140339465779200 trainer.py:518] Training: step 32535
I0512 23:52:42.896694 140529434355712 trainer.py:518] Training: step 32546
I0512 23:52:43.006665 140595318249472 trainer.py:518] Training: step 32546
I0512 23:52:43.242505 139674595104768 trainer.py:518] Training: step 32546
I0512 23:52:51.817133 140095893518336 trainer.py:518] Training: step 32540
I0512 23:52:52.002999 140440593786880 trainer.py:518] Training: step 32547
I0512 23:52:52.163067 140311617681408 trainer.py:518] Training: step 32547
I0512 23:53:00.796726 140529434355712 trainer.py:518] Training: step 32548
I0512 23:53:00.816513 140339465779200 trainer.py:518] Training: step 32548
I0512 23:53:00.889554 139674595104768 trainer.py:518] Training: step 32548
I0512 23:53:00.969807 140595318249472 trainer.py:518] Training: step 32548
I0512 23:53:01.068681 140031916288000 trainer.py:518] Training: step 32548
I0512 23:53:09.751236 140095893518336 trainer.py:518] Training: step 32549
I0512 23:53:09.757578 140440593786880 trainer.py:518] Training: step 32549
I0512 23:53:09.865040 140311617681408 trainer.py:518] Training: step 32549
I0512 23:53:36.539876 140095893518336 trainer.py:518] Training: step 32550
I0512 23:53:36.539440 139674595104768 trainer.py:518] Training: step 32550
I0512 23:53:36.546827 140440593786880 trainer.py:518] Training: step 32550
I0512 23:53:36.544306 140529434355712 trainer.py:518] Training: step 32550
I0512 23:53:36.548982 140311617681408 trainer.py:518] Training: step 32550
I0512 23:53:36.573075 140595318249472 trainer.py:518] Training: step 32550
I0512 23:53:36.723042 140031916288000 trainer.py:518] Training: step 32550
I0512 23:53:36.751609 140339465779200 trainer.py:518] Training: step 32550
I0512 23:54:03.374982 139674595104768 trainer.py:518] Training: step 32552
I0512 23:54:03.376240 140440593786880 trainer.py:518] Training: step 32552
I0512 23:54:03.375731 140339465779200 trainer.py:518] Training: step 32552
I0512 23:54:03.377929 140595318249472 trainer.py:518] Training: step 32552
I0512 23:54:03.378820 140031916288000 trainer.py:518] Training: step 32552
I0512 23:54:03.383046 140095893518336 trainer.py:518] Training: step 32552
I0512 23:54:03.378444 140529434355712 trainer.py:518] Training: step 32552
I0512 23:54:03.378845 140311617681408 trainer.py:518] Training: step 32552
I0512 23:54:21.267063 140595318249472 trainer.py:518] Training: step 32553
I0512 23:54:21.270592 139674595104768 trainer.py:518] Training: step 32553
I0512 23:54:21.274497 140339465779200 trainer.py:518] Training: step 32553
I0512 23:54:21.278625 140440593786880 trainer.py:518] Training: step 32553
I0512 23:54:21.282780 140095893518336 trainer.py:518] Training: step 32553
I0512 23:54:21.265498 140529434355712 trainer.py:518] Training: step 32553
I0512 23:54:21.283864 140031916288000 trainer.py:518] Training: step 32553
I0512 23:54:21.277528 140311617681408 trainer.py:518] Training: step 32553
I0512 23:54:39.157605 139674595104768 trainer.py:518] Training: step 32554
I0512 23:54:39.158648 140339465779200 trainer.py:518] Training: step 32554
I0512 23:54:39.164278 140440593786880 trainer.py:518] Training: step 32554
I0512 23:54:39.165966 140595318249472 trainer.py:518] Training: step 32554
I0512 23:54:39.177198 140095893518336 trainer.py:518] Training: step 32554
I0512 23:54:39.157316 140529434355712 trainer.py:518] Training: step 32554
I0512 23:54:39.160207 140311617681408 trainer.py:518] Training: step 32554
I0512 23:54:39.186907 140031916288000 trainer.py:518] Training: step 32554
I0512 23:54:57.046911 139674595104768 trainer.py:518] Training: step 32555
I0512 23:54:57.047359 140440593786880 trainer.py:518] Training: step 32555
I0512 23:54:57.047796 140031916288000 trainer.py:518] Training: step 32555
I0512 23:54:57.048357 140339465779200 trainer.py:518] Training: step 32555
I0512 23:54:57.050600 140595318249472 trainer.py:518] Training: step 32555
I0512 23:54:57.063451 140095893518336 trainer.py:518] Training: step 32555
I0512 23:54:57.048995 140529434355712 trainer.py:518] Training: step 32555
I0512 23:54:57.052718 140311617681408 trainer.py:518] Training: step 32555
I0512 23:55:14.938011 139674595104768 trainer.py:518] Training: step 32556
I0512 23:55:14.940181 140095893518336 trainer.py:518] Training: step 32556
I0512 23:55:14.938290 140031916288000 trainer.py:518] Training: step 32556
I0512 23:55:14.940141 140595318249472 trainer.py:518] Training: step 32556
I0512 23:55:14.939756 140339465779200 trainer.py:518] Training: step 32556
I0512 23:55:14.954010 140440593786880 trainer.py:518] Training: step 32556
I0512 23:55:14.940860 140529434355712 trainer.py:518] Training: step 32556
I0512 23:55:14.943201 140311617681408 trainer.py:518] Training: step 32556
I0512 23:55:32.829259 140095893518336 trainer.py:518] Training: step 32557
I0512 23:55:32.829052 139674595104768 trainer.py:518] Training: step 32557
I0512 23:55:32.829015 140440593786880 trainer.py:518] Training: step 32557
I0512 23:55:32.829121 140339465779200 trainer.py:518] Training: step 32557
I0512 23:55:32.831271 140031916288000 trainer.py:518] Training: step 32557
I0512 23:55:32.844653 140595318249472 trainer.py:518] Training: step 32557
I0512 23:55:32.830146 140529434355712 trainer.py:518] Training: step 32557
I0512 23:55:32.829506 140311617681408 trainer.py:518] Training: step 32557
I0512 23:55:50.721291 140095893518336 trainer.py:518] Training: step 32558
I0512 23:55:50.720999 140595318249472 trainer.py:518] Training: step 32558
I0512 23:55:50.721313 140339465779200 trainer.py:518] Training: step 32558
I0512 23:55:50.726056 139674595104768 trainer.py:518] Training: step 32558
I0512 23:55:50.728831 140031916288000 trainer.py:518] Training: step 32558
I0512 23:55:50.737265 140440593786880 trainer.py:518] Training: step 32558
I0512 23:55:50.721527 140529434355712 trainer.py:518] Training: step 32558
I0512 23:55:50.739059 140311617681408 trainer.py:518] Training: step 32558
I0512 23:55:59.668456 140439986517568 logging_writer.py:48] [32540] collection=train accuracy=0.572353, cross_ent_loss=16.418315887451172, cross_ent_loss_per_all_target_tokens=2.0877e-05, experts/auxiliary_loss=0.16070373356342316, experts/expert_usage=7.9644012451171875, experts/fraction_tokens_left_behind=0.2561076879501343, experts/router_confidence=3.4525973796844482, experts/router_z_loss=0.00030822798726148903, learning_rate=0.00554406, learning_rate/current=0.00554368, loss=16.586950302124023, loss_per_all_target_tokens=2.10914e-05, loss_per_nonpadding_target_token=2.12144e-05, non_padding_fraction/loss_weights=0.994202, timing/seconds=89.48037719726562, timing/seqs=3840, timing/seqs_per_second=42.9144401550293, timing/seqs_per_second_per_core=1.3410762548446655, timing/steps_per_second=0.1117563545703888, timing/target_tokens_per_second=87888.7734375, timing/target_tokens_per_second_per_core=2746.524169921875, timing/uptime=608.849, z_loss=0.007620789110660553, z_loss_per_all_target_tokens=9.69033e-09
I0512 23:56:08.611801 139674595104768 trainer.py:518] Training: step 32559
I0512 23:56:08.611829 140595318249472 trainer.py:518] Training: step 32559
I0512 23:56:08.611847 140440593786880 trainer.py:518] Training: step 32559
I0512 23:56:08.612995 140339465779200 trainer.py:518] Training: step 32559
I0512 23:56:08.619530 140031916288000 trainer.py:518] Training: step 32559
I0512 23:56:08.632101 140095893518336 trainer.py:518] Training: step 32559
I0512 23:56:08.612559 140529434355712 trainer.py:518] Training: step 32559
I0512 23:56:08.611974 140311617681408 trainer.py:518] Training: step 32559
I0512 23:56:26.502697 140440593786880 trainer.py:518] Training: step 32565
I0512 23:56:26.506026 140095893518336 trainer.py:518] Training: step 32565
I0512 23:56:26.503905 140339465779200 trainer.py:518] Training: step 32564
I0512 23:56:26.503916 140529434355712 trainer.py:518] Training: step 32565
I0512 23:56:26.505183 140439986517568 logging_writer.py:48] [32550] collection=train accuracy=0.571445, cross_ent_loss=16.49104881286621, cross_ent_loss_per_all_target_tokens=2.09695e-05, experts/auxiliary_loss=0.16069236397743225, experts/expert_usage=7.97119665145874, experts/fraction_tokens_left_behind=0.23532091081142426, experts/router_confidence=3.4656550884246826, experts/router_z_loss=0.00030761995003558695, learning_rate=0.00554321, learning_rate/current=0.00554283, loss=16.659902572631836, loss_per_all_target_tokens=2.11842e-05, loss_per_nonpadding_target_token=2.12931e-05, non_padding_fraction/loss_weights=0.994885, timing/seconds=89.44841766357422, timing/seqs=3840, timing/seqs_per_second=42.929771423339844, timing/seqs_per_second_per_core=1.3415553569793701, timing/steps_per_second=0.11179627478122711, timing/target_tokens_per_second=87920.171875, timing/target_tokens_per_second_per_core=2747.50537109375, timing/uptime=653.677, z_loss=0.007851704023778439, z_loss_per_all_target_tokens=9.98396e-09
I0512 23:56:26.503083 140311617681408 trainer.py:518] Training: step 32564
I0512 23:56:26.531805 139674595104768 trainer.py:518] Training: step 32564
I0512 23:56:26.533377 140031916288000 trainer.py:518] Training: step 32565
I0512 23:56:26.680388 140595318249472 trainer.py:518] Training: step 32568
I0512 23:57:02.284738 140095893518336 trainer.py:518] Training: step 32572
I0512 23:57:02.285870 139674595104768 trainer.py:518] Training: step 32572
I0512 23:57:02.284817 140595318249472 trainer.py:518] Training: step 32572
I0512 23:57:02.285096 140031916288000 trainer.py:518] Training: step 32572
I0512 23:57:02.288463 140339465779200 trainer.py:518] Training: step 32572
I0512 23:57:02.290347 140440593786880 trainer.py:518] Training: step 32572
I0512 23:57:02.284644 140529434355712 trainer.py:518] Training: step 32572
I0512 23:57:02.284757 140311617681408 trainer.py:518] Training: step 32572
I0512 23:57:20.175295 140095893518336 trainer.py:518] Training: step 32573
I0512 23:57:20.174959 139674595104768 trainer.py:518] Training: step 32573
I0512 23:57:20.175529 140031916288000 trainer.py:518] Training: step 32573
I0512 23:57:20.178033 140440593786880 trainer.py:518] Training: step 32573
I0512 23:57:20.176568 140339465779200 trainer.py:518] Training: step 32573
I0512 23:57:20.181198 140595318249472 trainer.py:518] Training: step 32573
I0512 23:57:20.174866 140529434355712 trainer.py:518] Training: step 32573
I0512 23:57:20.175364 140311617681408 trainer.py:518] Training: step 32573
I0512 23:57:38.067368 140095893518336 trainer.py:518] Training: step 32574
I0512 23:57:38.066795 139674595104768 trainer.py:518] Training: step 32574
I0512 23:57:38.067084 140031916288000 trainer.py:518] Training: step 32574
I0512 23:57:38.067476 140339465779200 trainer.py:518] Training: step 32574
I0512 23:57:38.070304 140440593786880 trainer.py:518] Training: step 32574
I0512 23:57:38.071737 140595318249472 trainer.py:518] Training: step 32574
I0512 23:57:38.066778 140529434355712 trainer.py:518] Training: step 32574
I0512 23:57:38.067245 140311617681408 trainer.py:518] Training: step 32574
I0512 23:57:55.958228 139674595104768 trainer.py:518] Training: step 32575
I0512 23:57:55.959063 140095893518336 trainer.py:518] Training: step 32575
I0512 23:57:55.957965 140440593786880 trainer.py:518] Training: step 32575
I0512 23:57:55.958397 140031916288000 trainer.py:518] Training: step 32575
I0512 23:57:55.958549 140339465779200 trainer.py:518] Training: step 32575
I0512 23:57:55.966957 140595318249472 trainer.py:518] Training: step 32575
I0512 23:57:55.958126 140529434355712 trainer.py:518] Training: step 32575
I0512 23:57:55.958206 140311617681408 trainer.py:518] Training: step 32575
I0512 23:58:13.850931 140095893518336 trainer.py:518] Training: step 32576
I0512 23:58:13.850390 139674595104768 trainer.py:518] Training: step 32576
I0512 23:58:13.850153 140440593786880 trainer.py:518] Training: step 32576
I0512 23:58:13.850628 140595318249472 trainer.py:518] Training: step 32576
I0512 23:58:13.850903 140031916288000 trainer.py:518] Training: step 32576
I0512 23:58:13.850730 140339465779200 trainer.py:518] Training: step 32576
I0512 23:58:13.850440 140529434355712 trainer.py:518] Training: step 32576
I0512 23:58:13.851583 140311617681408 trainer.py:518] Training: step 32576
I0512 23:58:31.741122 140095893518336 trainer.py:518] Training: step 32577
I0512 23:58:31.741054 139674595104768 trainer.py:518] Training: step 32577
I0512 23:58:31.741032 140440593786880 trainer.py:518] Training: step 32577
I0512 23:58:31.741038 140339465779200 trainer.py:518] Training: step 32577
I0512 23:58:31.741322 140031916288000 trainer.py:518] Training: step 32577
I0512 23:58:31.743629 140595318249472 trainer.py:518] Training: step 32577
I0512 23:58:31.741974 140529434355712 trainer.py:518] Training: step 32577
I0512 23:58:31.741158 140311617681408 trainer.py:518] Training: step 32577
I0512 23:58:49.633126 140339465779200 trainer.py:518] Training: step 32578
I0512 23:58:49.632614 140440593786880 trainer.py:518] Training: step 32578
I0512 23:58:49.633239 140595318249472 trainer.py:518] Training: step 32578
I0512 23:58:49.633984 140095893518336 trainer.py:518] Training: step 32578
I0512 23:58:49.633011 140031916288000 trainer.py:518] Training: step 32578
I0512 23:58:49.633592 140529434355712 trainer.py:518] Training: step 32578
I0512 23:58:49.632608 139674595104768 trainer.py:518] Training: step 32578
I0512 23:58:49.633047 140311617681408 trainer.py:518] Training: step 32578
I0512 23:58:58.580349 140439986517568 logging_writer.py:48] [32560] collection=train accuracy=0.572129, cross_ent_loss=16.427200317382812, cross_ent_loss_per_all_target_tokens=2.08883e-05, experts/auxiliary_loss=0.16069410741329193, experts/expert_usage=7.971740245819092, experts/fraction_tokens_left_behind=0.25073254108428955, experts/router_confidence=3.458867311477661, experts/router_z_loss=0.0003135128354188055, learning_rate=0.00554236, learning_rate/current=0.00554197, loss=16.595895767211914, loss_per_all_target_tokens=2.11028e-05, loss_per_nonpadding_target_token=2.12294e-05, non_padding_fraction/loss_weights=0.994036, timing/seconds=89.45101928710938, timing/seqs=3840, timing/seqs_per_second=42.928524017333984, timing/seqs_per_second_per_core=1.341516375541687, timing/steps_per_second=0.11179302632808685, timing/target_tokens_per_second=87917.6171875, timing/target_tokens_per_second_per_core=2747.425537109375, timing/uptime=823.616, z_loss=0.007687182165682316, z_loss_per_all_target_tokens=9.77476e-09
I0512 23:59:07.525739 140095893518336 trainer.py:518] Training: step 32579
I0512 23:59:07.525216 139674595104768 trainer.py:518] Training: step 32579
I0512 23:59:07.525643 140440593786880 trainer.py:518] Training: step 32579
I0512 23:59:07.527015 140529434355712 trainer.py:518] Training: step 32579
I0512 23:59:07.527652 140031916288000 trainer.py:518] Training: step 32579
I0512 23:59:07.531426 140339465779200 trainer.py:518] Training: step 32579
I0512 23:59:07.525374 140311617681408 trainer.py:518] Training: step 32579
I0512 23:59:07.526533 140595318249472 trainer.py:518] Training: step 32579
I0512 23:59:25.415004 140095893518336 trainer.py:518] Training: step 32584
I0512 23:59:25.415444 140339465779200 trainer.py:518] Training: step 32586
I0512 23:59:25.414775 139674595104768 trainer.py:518] Training: step 32586
I0512 23:59:25.414492 140440593786880 trainer.py:518] Training: step 32584
I0512 23:59:25.415732 140529434355712 trainer.py:518] Training: step 32583
I0512 23:59:25.417451 140031916288000 trainer.py:518] Training: step 32584
I0512 23:59:25.419183 140439986517568 logging_writer.py:48] [32570] collection=train accuracy=0.56777, cross_ent_loss=16.59585952758789, cross_ent_loss_per_all_target_tokens=2.11027e-05, experts/auxiliary_loss=0.16073104739189148, experts/expert_usage=7.9675493240356445, experts/fraction_tokens_left_behind=0.26071736216545105, experts/router_confidence=3.464411497116089, experts/router_z_loss=0.0003135779988951981, learning_rate=0.00554151, learning_rate/current=0.00554112, loss=16.76445770263672, loss_per_all_target_tokens=2.13171e-05, loss_per_nonpadding_target_token=2.14429e-05, non_padding_fraction/loss_weights=0.994132, timing/seconds=89.4490966796875, timing/seqs=3840, timing/seqs_per_second=42.929447174072266, timing/seqs_per_second_per_core=1.3415452241897583, timing/steps_per_second=0.11179543286561966, timing/target_tokens_per_second=87919.5078125, timing/target_tokens_per_second_per_core=2747.484619140625, timing/uptime=841.596, z_loss=0.007551883812993765, z_loss_per_all_target_tokens=9.60272e-09
I0512 23:59:25.442851 140311617681408 trainer.py:518] Training: step 32588
I0512 23:59:25.442978 140595318249472 trainer.py:518] Training: step 32585
I0513 00:00:01.197020 140095893518336 trainer.py:518] Training: step 32592
I0513 00:00:01.197008 140339465779200 trainer.py:518] Training: step 32592
I0513 00:00:01.196415 139674595104768 trainer.py:518] Training: step 32592
I0513 00:00:01.196233 140440593786880 trainer.py:518] Training: step 32592
I0513 00:00:01.197091 140529434355712 trainer.py:518] Training: step 32592
I0513 00:00:01.197378 140031916288000 trainer.py:518] Training: step 32592
I0513 00:00:01.196817 140311617681408 trainer.py:518] Training: step 32592
I0513 00:00:01.197028 140595318249472 trainer.py:518] Training: step 32592
I0513 00:00:19.087185 140095893518336 trainer.py:518] Training: step 32593
I0513 00:00:19.086767 139674595104768 trainer.py:518] Training: step 32593
I0513 00:00:19.090073 140339465779200 trainer.py:518] Training: step 32593
I0513 00:00:19.088671 140031916288000 trainer.py:518] Training: step 32593
I0513 00:00:19.086789 140529434355712 trainer.py:518] Training: step 32593
I0513 00:00:19.086896 140311617681408 trainer.py:518] Training: step 32593
I0513 00:00:19.087251 140595318249472 trainer.py:518] Training: step 32593
I0513 00:00:28.030904 140440593786880 trainer.py:518] Training: step 32593
I0513 00:00:36.977120 140339465779200 trainer.py:518] Training: step 32594
I0513 00:00:36.976416 139674595104768 trainer.py:518] Training: step 32594
I0513 00:00:36.977884 140031916288000 trainer.py:518] Training: step 32594
I0513 00:00:36.976570 140529434355712 trainer.py:518] Training: step 32594
I0513 00:00:36.976440 140311617681408 trainer.py:518] Training: step 32594
I0513 00:00:36.976878 140595318249472 trainer.py:518] Training: step 32594
I0513 00:00:45.921848 140095893518336 trainer.py:518] Training: step 32594
I0513 00:00:45.921505 140440593786880 trainer.py:518] Training: step 32594
I0513 00:00:54.868096 140339465779200 trainer.py:518] Training: step 32595
I0513 00:00:54.867384 139674595104768 trainer.py:518] Training: step 32595
I0513 00:00:54.867677 140031916288000 trainer.py:518] Training: step 32595
I0513 00:00:54.867982 140529434355712 trainer.py:518] Training: step 32595
I0513 00:00:54.867918 140311617681408 trainer.py:518] Training: step 32595
I0513 00:00:54.868320 140595318249472 trainer.py:518] Training: step 32595
I0513 00:01:03.816358 140095893518336 trainer.py:518] Training: step 32595
I0513 00:01:03.815917 140440593786880 trainer.py:518] Training: step 32595
I0513 00:01:12.758979 140339465779200 trainer.py:518] Training: step 32596
I0513 00:01:12.758618 139674595104768 trainer.py:518] Training: step 32596
I0513 00:01:12.759330 140031916288000 trainer.py:518] Training: step 32596
I0513 00:01:12.759696 140529434355712 trainer.py:518] Training: step 32596
I0513 00:01:12.758902 140311617681408 trainer.py:518] Training: step 32596
I0513 00:01:12.759299 140595318249472 trainer.py:518] Training: step 32596
I0513 00:01:21.704988 140095893518336 trainer.py:518] Training: step 32596
I0513 00:01:21.704685 140440593786880 trainer.py:518] Training: step 32596
I0513 00:01:30.648589 139674595104768 trainer.py:518] Training: step 32597
I0513 00:01:30.650277 140339465779200 trainer.py:518] Training: step 32597
I0513 00:01:30.648921 140031916288000 trainer.py:518] Training: step 32597
I0513 00:01:30.649501 140529434355712 trainer.py:518] Training: step 32597
I0513 00:01:30.648844 140311617681408 trainer.py:518] Training: step 32597
I0513 00:01:30.649284 140595318249472 trainer.py:518] Training: step 32597
I0513 00:01:48.539726 140095893518336 trainer.py:518] Training: step 32598
I0513 00:01:48.539954 140339465779200 trainer.py:518] Training: step 32598
I0513 00:01:48.539670 139674595104768 trainer.py:518] Training: step 32598
I0513 00:01:48.539114 140440593786880 trainer.py:518] Training: step 32598
I0513 00:01:48.539265 140031916288000 trainer.py:518] Training: step 32598
I0513 00:01:48.540227 140529434355712 trainer.py:518] Training: step 32598
I0513 00:01:48.539705 140311617681408 trainer.py:518] Training: step 32598
I0513 00:01:48.539559 140595318249472 trainer.py:518] Training: step 32598
I0513 00:01:57.485824 140439986517568 logging_writer.py:48] [32580] collection=train accuracy=0.577743, cross_ent_loss=16.158639907836914, cross_ent_loss_per_all_target_tokens=2.05468e-05, experts/auxiliary_loss=0.1607501357793808, experts/expert_usage=7.960374355316162, experts/fraction_tokens_left_behind=0.2467680275440216, experts/router_confidence=3.4558186531066895, experts/router_z_loss=0.0002974395174533129, learning_rate=0.00554066, learning_rate/current=0.00554027, loss=16.32745361328125, loss_per_all_target_tokens=2.07614e-05, loss_per_nonpadding_target_token=2.08475e-05, non_padding_fraction/loss_weights=0.995873, timing/seconds=89.45394134521484, timing/seqs=3840, timing/seqs_per_second=42.927120208740234, timing/seqs_per_second_per_core=1.3414725065231323, timing/steps_per_second=0.11178937554359436, timing/target_tokens_per_second=87914.7421875, timing/target_tokens_per_second_per_core=2747.335693359375, timing/uptime=1002.43, z_loss=0.007766913156956434, z_loss_per_all_target_tokens=9.87614e-09
I0513 00:02:06.430927 140095893518336 trainer.py:518] Training: step 32599
I0513 00:02:06.430576 139674595104768 trainer.py:518] Training: step 32599
I0513 00:02:06.430752 140440593786880 trainer.py:518] Training: step 32599
I0513 00:02:06.430939 140031916288000 trainer.py:518] Training: step 32599
I0513 00:02:06.436782 140339465779200 trainer.py:518] Training: step 32599
I0513 00:02:06.432407 140529434355712 trainer.py:518] Training: step 32599
I0513 00:02:06.431524 140311617681408 trainer.py:518] Training: step 32599
I0513 00:02:06.431741 140595318249472 trainer.py:518] Training: step 32599
I0513 00:02:24.320888 139674595104768 trainer.py:518] Training: step 32606
I0513 00:02:24.321151 140031916288000 trainer.py:518] Training: step 32606
I0513 00:02:24.322173 140439986517568 logging_writer.py:48] [32590] collection=train accuracy=0.578754, cross_ent_loss=16.17268943786621, cross_ent_loss_per_all_target_tokens=2.05646e-05, experts/auxiliary_loss=0.16072283685207367, experts/expert_usage=7.9681715965271, experts/fraction_tokens_left_behind=0.2520363926887512, experts/router_confidence=3.467278003692627, experts/router_z_loss=0.00031532763387076557, learning_rate=0.00553981, learning_rate/current=0.00553942, loss=16.3426513671875, loss_per_all_target_tokens=2.07808e-05, loss_per_nonpadding_target_token=2.08968e-05, non_padding_fraction/loss_weights=0.994445, timing/seconds=89.44557189941406, timing/seqs=3840, timing/seqs_per_second=42.93113708496094, timing/seqs_per_second_per_core=1.3415980339050293, timing/steps_per_second=0.11179983615875244, timing/target_tokens_per_second=87922.96875, timing/target_tokens_per_second_per_core=2747.5927734375, timing/uptime=1020.9, z_loss=0.008925918489694595, z_loss_per_all_target_tokens=1.13499e-08
I0513 00:02:24.355793 140339465779200 trainer.py:518] Training: step 32605
I0513 00:02:24.349869 140529434355712 trainer.py:518] Training: step 32604
I0513 00:02:24.348197 140311617681408 trainer.py:518] Training: step 32608
I0513 00:02:24.348489 140595318249472 trainer.py:518] Training: step 32605
I0513 00:02:33.266222 140440593786880 trainer.py:518] Training: step 32610
I0513 00:02:33.283627 140095893518336 trainer.py:518] Training: step 32610
I0513 00:03:00.103631 140095893518336 trainer.py:518] Training: step 32612
I0513 00:03:00.103637 140339465779200 trainer.py:518] Training: step 32612
I0513 00:03:00.103776 139674595104768 trainer.py:518] Training: step 32612
I0513 00:03:00.103008 140440593786880 trainer.py:518] Training: step 32612
I0513 00:03:00.104016 140031916288000 trainer.py:518] Training: step 32612
I0513 00:03:00.103866 140529434355712 trainer.py:518] Training: step 32612
I0513 00:03:00.103552 140311617681408 trainer.py:518] Training: step 32612
I0513 00:03:00.104232 140595318249472 trainer.py:518] Training: step 32612
I0513 00:03:17.993190 140095893518336 trainer.py:518] Training: step 32613
I0513 00:03:17.993417 140339465779200 trainer.py:518] Training: step 32613
I0513 00:03:17.993549 139674595104768 trainer.py:518] Training: step 32613
I0513 00:03:17.992902 140440593786880 trainer.py:518] Training: step 32613
I0513 00:03:17.994102 140031916288000 trainer.py:518] Training: step 32613
I0513 00:03:17.993283 140529434355712 trainer.py:518] Training: step 32613
I0513 00:03:17.993751 140595318249472 trainer.py:518] Training: step 32613
I0513 00:03:26.937857 140311617681408 trainer.py:518] Training: step 32613
I0513 00:03:35.883677 140339465779200 trainer.py:518] Training: step 32614
I0513 00:03:35.883963 140095893518336 trainer.py:518] Training: step 32614
I0513 00:03:35.883879 139674595104768 trainer.py:518] Training: step 32614
I0513 00:03:35.883351 140440593786880 trainer.py:518] Training: step 32614
I0513 00:03:35.884300 140031916288000 trainer.py:518] Training: step 32614
I0513 00:03:35.883682 140529434355712 trainer.py:518] Training: step 32614
I0513 00:03:35.884245 140595318249472 trainer.py:518] Training: step 32614
I0513 00:03:44.829572 140311617681408 trainer.py:518] Training: step 32614
I0513 00:03:53.774484 140339465779200 trainer.py:518] Training: step 32615
I0513 00:03:53.774966 140095893518336 trainer.py:518] Training: step 32615
I0513 00:03:53.774403 139674595104768 trainer.py:518] Training: step 32615
I0513 00:03:53.774137 140440593786880 trainer.py:518] Training: step 32615
I0513 00:03:53.774347 140031916288000 trainer.py:518] Training: step 32615
I0513 00:03:53.774583 140529434355712 trainer.py:518] Training: step 32615
I0513 00:03:53.774639 140595318249472 trainer.py:518] Training: step 32615
I0513 00:04:02.723437 140311617681408 trainer.py:518] Training: step 32615
I0513 00:04:11.666411 140339465779200 trainer.py:518] Training: step 32616
I0513 00:04:11.666457 140095893518336 trainer.py:518] Training: step 32616
I0513 00:04:11.666208 139674595104768 trainer.py:518] Training: step 32616
I0513 00:04:11.665746 140440593786880 trainer.py:518] Training: step 32616
I0513 00:04:11.666115 140031916288000 trainer.py:518] Training: step 32616
I0513 00:04:11.666307 140529434355712 trainer.py:518] Training: step 32616
I0513 00:04:11.666661 140595318249472 trainer.py:518] Training: step 32616
I0513 00:04:20.612466 140311617681408 trainer.py:518] Training: step 32616
I0513 00:04:29.558299 140095893518336 trainer.py:518] Training: step 32617
I0513 00:04:29.559769 140339465779200 trainer.py:518] Training: step 32617
I0513 00:04:29.557583 140031916288000 trainer.py:518] Training: step 32617
I0513 00:04:29.557763 140440593786880 trainer.py:518] Training: step 32617
I0513 00:04:29.559115 139674595104768 trainer.py:518] Training: step 32617
I0513 00:04:29.558540 140529434355712 trainer.py:518] Training: step 32617
I0513 00:04:29.558385 140595318249472 trainer.py:518] Training: step 32617
I0513 00:04:47.449437 140095893518336 trainer.py:518] Training: step 32618
I0513 00:04:47.449220 139674595104768 trainer.py:518] Training: step 32618
I0513 00:04:47.448842 140440593786880 trainer.py:518] Training: step 32618
I0513 00:04:47.448911 140031916288000 trainer.py:518] Training: step 32618
I0513 00:04:47.451335 140339465779200 trainer.py:518] Training: step 32618
I0513 00:04:47.449916 140529434355712 trainer.py:518] Training: step 32618
I0513 00:04:47.449954 140311617681408 trainer.py:518] Training: step 32618
I0513 00:04:47.449218 140595318249472 trainer.py:518] Training: step 32618
I0513 00:04:56.396670 140439986517568 logging_writer.py:48] [32600] collection=train accuracy=0.576046, cross_ent_loss=16.25953483581543, cross_ent_loss_per_all_target_tokens=2.06751e-05, experts/auxiliary_loss=0.1607319414615631, experts/expert_usage=7.970929145812988, experts/fraction_tokens_left_behind=0.25650259852409363, experts/router_confidence=3.4619698524475098, experts/router_z_loss=0.000315842597046867, learning_rate=0.00553895, learning_rate/current=0.00553857, loss=16.429140090942383, loss_per_all_target_tokens=2.08907e-05, loss_per_nonpadding_target_token=2.10097e-05, non_padding_fraction/loss_weights=0.994336, timing/seconds=89.4510726928711, timing/seqs=3840, timing/seqs_per_second=42.928497314453125, timing/seqs_per_second_per_core=1.3415155410766602, timing/steps_per_second=0.11179296672344208, timing/target_tokens_per_second=87917.5625, timing/target_tokens_per_second_per_core=2747.423828125, timing/uptime=1181.33, z_loss=0.008556409738957882, z_loss_per_all_target_tokens=1.088e-08
I0513 00:05:05.341372 140095893518336 trainer.py:518] Training: step 32619
I0513 00:05:05.341770 140339465779200 trainer.py:518] Training: step 32619
I0513 00:05:05.341564 139674595104768 trainer.py:518] Training: step 32619
I0513 00:05:05.340695 140440593786880 trainer.py:518] Training: step 32619
I0513 00:05:05.341483 140031916288000 trainer.py:518] Training: step 32619
I0513 00:05:05.342941 140529434355712 trainer.py:518] Training: step 32619
I0513 00:05:05.341171 140311617681408 trainer.py:518] Training: step 32619
I0513 00:05:05.341922 140595318249472 trainer.py:518] Training: step 32619
I0513 00:05:23.232254 140339465779200 trainer.py:518] Training: step 32624
I0513 00:05:23.231755 139674595104768 trainer.py:518] Training: step 32625
I0513 00:05:23.232632 140031916288000 trainer.py:518] Training: step 32626
I0513 00:05:23.232425 140529434355712 trainer.py:518] Training: step 32623
I0513 00:05:23.231877 140595318249472 trainer.py:518] Training: step 32624
I0513 00:05:23.236383 140439986517568 logging_writer.py:48] [32610] collection=train accuracy=0.574791, cross_ent_loss=16.29664421081543, cross_ent_loss_per_all_target_tokens=2.07223e-05, experts/auxiliary_loss=0.16072587668895721, experts/expert_usage=7.9620208740234375, experts/fraction_tokens_left_behind=0.25747108459472656, experts/router_confidence=3.459935426712036, experts/router_z_loss=0.0003107683442067355, learning_rate=0.00553811, learning_rate/current=0.00553772, loss=16.465789794921875, loss_per_all_target_tokens=2.09373e-05, loss_per_nonpadding_target_token=2.10389e-05, non_padding_fraction/loss_weights=0.995171, timing/seconds=89.44715118408203, timing/seqs=3840, timing/seqs_per_second=42.93037796020508, timing/seqs_per_second_per_core=1.3415743112564087, timing/steps_per_second=0.11179786175489426, timing/target_tokens_per_second=87921.4140625, timing/target_tokens_per_second_per_core=2747.544189453125, timing/uptime=1199.41, z_loss=0.008110794238746166, z_loss_per_all_target_tokens=1.03134e-08
I0513 00:05:23.258384 140440593786880 trainer.py:518] Training: step 32626
I0513 00:05:23.266824 140095893518336 trainer.py:518] Training: step 32627
I0513 00:05:32.178451 140311617681408 trainer.py:518] Training: step 32630
I0513 00:05:59.014439 140095893518336 trainer.py:518] Training: step 32632
I0513 00:05:59.013114 139674595104768 trainer.py:518] Training: step 32632
I0513 00:05:59.015548 140339465779200 trainer.py:518] Training: step 32632
I0513 00:05:59.013570 140440593786880 trainer.py:518] Training: step 32632
I0513 00:05:59.013662 140031916288000 trainer.py:518] Training: step 32632
I0513 00:05:59.013700 140529434355712 trainer.py:518] Training: step 32632
I0513 00:05:59.014034 140311617681408 trainer.py:518] Training: step 32632
I0513 00:05:59.014010 140595318249472 trainer.py:518] Training: step 32632
I0513 00:06:16.904945 140095893518336 trainer.py:518] Training: step 32633
I0513 00:06:16.905122 140339465779200 trainer.py:518] Training: step 32633
I0513 00:06:16.904696 139674595104768 trainer.py:518] Training: step 32633
I0513 00:06:16.904625 140031916288000 trainer.py:518] Training: step 32633
I0513 00:06:16.904745 140440593786880 trainer.py:518] Training: step 32633
I0513 00:06:16.905073 140529434355712 trainer.py:518] Training: step 32633
I0513 00:06:16.904965 140311617681408 trainer.py:518] Training: step 32633
I0513 00:06:16.904919 140595318249472 trainer.py:518] Training: step 32633
I0513 00:06:34.796497 140095893518336 trainer.py:518] Training: step 32634
I0513 00:06:34.796319 140339465779200 trainer.py:518] Training: step 32634
I0513 00:06:34.795876 139674595104768 trainer.py:518] Training: step 32634
I0513 00:06:34.796084 140440593786880 trainer.py:518] Training: step 32634
I0513 00:06:34.796287 140031916288000 trainer.py:518] Training: step 32634
I0513 00:06:34.796122 140529434355712 trainer.py:518] Training: step 32634
I0513 00:06:34.796335 140311617681408 trainer.py:518] Training: step 32634
I0513 00:06:34.796906 140595318249472 trainer.py:518] Training: step 32634
I0513 00:06:52.688818 140095893518336 trainer.py:518] Training: step 32635
I0513 00:06:52.688826 140339465779200 trainer.py:518] Training: step 32635
I0513 00:06:52.688213 139674595104768 trainer.py:518] Training: step 32635
I0513 00:06:52.688478 140031916288000 trainer.py:518] Training: step 32635
I0513 00:06:52.688653 140440593786880 trainer.py:518] Training: step 32635
I0513 00:06:52.689578 140529434355712 trainer.py:518] Training: step 32635
I0513 00:06:52.688811 140311617681408 trainer.py:518] Training: step 32635
I0513 00:06:52.689363 140595318249472 trainer.py:518] Training: step 32635
I0513 00:07:10.579660 140339465779200 trainer.py:518] Training: step 32636
I0513 00:07:10.580057 140095893518336 trainer.py:518] Training: step 32636
I0513 00:07:10.579113 139674595104768 trainer.py:518] Training: step 32636
I0513 00:07:10.579064 140440593786880 trainer.py:518] Training: step 32636
I0513 00:07:10.579536 140031916288000 trainer.py:518] Training: step 32636
I0513 00:07:10.580089 140529434355712 trainer.py:518] Training: step 32636
I0513 00:07:10.579825 140311617681408 trainer.py:518] Training: step 32636
I0513 00:07:10.580207 140595318249472 trainer.py:518] Training: step 32636
I0513 00:07:28.470623 140095893518336 trainer.py:518] Training: step 32637
I0513 00:07:28.470411 139674595104768 trainer.py:518] Training: step 32637
I0513 00:07:28.471917 140339465779200 trainer.py:518] Training: step 32637
I0513 00:07:28.469795 140440593786880 trainer.py:518] Training: step 32637
I0513 00:07:28.470152 140031916288000 trainer.py:518] Training: step 32637
I0513 00:07:28.471172 140529434355712 trainer.py:518] Training: step 32637
I0513 00:07:28.470470 140311617681408 trainer.py:518] Training: step 32637
I0513 00:07:28.470541 140595318249472 trainer.py:518] Training: step 32637
I0513 00:07:46.360286 140339465779200 trainer.py:518] Training: step 32638
I0513 00:07:46.361152 140095893518336 trainer.py:518] Training: step 32638
I0513 00:07:46.360209 139674595104768 trainer.py:518] Training: step 32638
I0513 00:07:46.360182 140440593786880 trainer.py:518] Training: step 32638
I0513 00:07:46.360377 140031916288000 trainer.py:518] Training: step 32638
I0513 00:07:46.361465 140529434355712 trainer.py:518] Training: step 32638
I0513 00:07:46.360730 140311617681408 trainer.py:518] Training: step 32638
I0513 00:07:46.360686 140595318249472 trainer.py:518] Training: step 32638
I0513 00:07:55.307339 140439986517568 logging_writer.py:48] [32620] collection=train accuracy=0.56934, cross_ent_loss=16.59552001953125, cross_ent_loss_per_all_target_tokens=2.11023e-05, experts/auxiliary_loss=0.16070492565631866, experts/expert_usage=7.966957092285156, experts/fraction_tokens_left_behind=0.2636432349681854, experts/router_confidence=3.4813029766082764, experts/router_z_loss=0.0003157231258228421, learning_rate=0.00553726, learning_rate/current=0.00553687, loss=16.764551162719727, loss_per_all_target_tokens=2.13172e-05, loss_per_nonpadding_target_token=2.14395e-05, non_padding_fraction/loss_weights=0.994299, timing/seconds=89.45625305175781, timing/seqs=3840, timing/seqs_per_second=42.92601013183594, timing/seqs_per_second_per_core=1.341437816619873, timing/steps_per_second=0.11178648471832275, timing/target_tokens_per_second=87912.46875, timing/target_tokens_per_second_per_core=2747.2646484375, timing/uptime=1360.25, z_loss=0.008009160868823528, z_loss_per_all_target_tokens=1.01842e-08
I0513 00:08:04.253106 140095893518336 trainer.py:518] Training: step 32639
I0513 00:08:04.253165 140339465779200 trainer.py:518] Training: step 32639
I0513 00:08:04.253212 139674595104768 trainer.py:518] Training: step 32639
I0513 00:08:04.252515 140031916288000 trainer.py:518] Training: step 32639
I0513 00:08:04.252547 140440593786880 trainer.py:518] Training: step 32639
I0513 00:08:04.253237 140311617681408 trainer.py:518] Training: step 32639
I0513 00:08:04.254607 140529434355712 trainer.py:518] Training: step 32639
I0513 00:08:04.254008 140595318249472 trainer.py:518] Training: step 32639
I0513 00:08:22.142938 140339465779200 trainer.py:518] Training: step 32645
I0513 00:08:22.143205 140095893518336 trainer.py:518] Training: step 32646
I0513 00:08:22.142624 140440593786880 trainer.py:518] Training: step 32645
I0513 00:08:22.143680 140529434355712 trainer.py:518] Training: step 32644
I0513 00:08:22.143363 140595318249472 trainer.py:518] Training: step 32645
I0513 00:08:22.147538 140439986517568 logging_writer.py:48] [32630] collection=train accuracy=0.571259, cross_ent_loss=16.469745635986328, cross_ent_loss_per_all_target_tokens=2.09424e-05, experts/auxiliary_loss=0.16069936752319336, experts/expert_usage=7.974879741668701, experts/fraction_tokens_left_behind=0.2388395518064499, experts/router_confidence=3.4524552822113037, experts/router_z_loss=0.0003093690029345453, learning_rate=0.00553641, learning_rate/current=0.00553603, loss=16.638967514038086, loss_per_all_target_tokens=2.11575e-05, loss_per_nonpadding_target_token=2.12755e-05, non_padding_fraction/loss_weights=0.994457, timing/seconds=89.44922637939453, timing/seqs=3840, timing/seqs_per_second=42.92938232421875, timing/seqs_per_second_per_core=1.341543197631836, timing/steps_per_second=0.11179526150226593, timing/target_tokens_per_second=87919.375, timing/target_tokens_per_second_per_core=2747.48046875, timing/uptime=1378.34, z_loss=0.008213717490434647, z_loss_per_all_target_tokens=1.04443e-08
I0513 00:08:22.168393 139674595104768 trainer.py:518] Training: step 32645
I0513 00:08:22.168411 140031916288000 trainer.py:518] Training: step 32647
I0513 00:08:22.173982 140311617681408 trainer.py:518] Training: step 32647
I0513 00:08:57.925904 140095893518336 trainer.py:518] Training: step 32652
I0513 00:08:57.925742 140339465779200 trainer.py:518] Training: step 32652
I0513 00:08:57.925421 139674595104768 trainer.py:518] Training: step 32652
I0513 00:08:57.925297 140440593786880 trainer.py:518] Training: step 32652
I0513 00:08:57.925736 140031916288000 trainer.py:518] Training: step 32652
I0513 00:08:57.926357 140529434355712 trainer.py:518] Training: step 32652
I0513 00:08:57.926214 140311617681408 trainer.py:518] Training: step 32652
I0513 00:08:57.926437 140595318249472 trainer.py:518] Training: step 32652
I0513 00:09:15.816442 140095893518336 trainer.py:518] Training: step 32653
I0513 00:09:15.816869 140339465779200 trainer.py:518] Training: step 32653
I0513 00:09:15.815940 139674595104768 trainer.py:518] Training: step 32653
I0513 00:09:15.815797 140440593786880 trainer.py:518] Training: step 32653
I0513 00:09:15.816045 140031916288000 trainer.py:518] Training: step 32653
I0513 00:09:15.817147 140529434355712 trainer.py:518] Training: step 32653
I0513 00:09:15.816925 140311617681408 trainer.py:518] Training: step 32653
I0513 00:09:15.816886 140595318249472 trainer.py:518] Training: step 32653
I0513 00:09:33.707563 140095893518336 trainer.py:518] Training: step 32654
I0513 00:09:33.707325 140339465779200 trainer.py:518] Training: step 32654
I0513 00:09:33.707072 139674595104768 trainer.py:518] Training: step 32654
I0513 00:09:33.706564 140440593786880 trainer.py:518] Training: step 32654
I0513 00:09:33.707207 140031916288000 trainer.py:518] Training: step 32654
I0513 00:09:33.707822 140529434355712 trainer.py:518] Training: step 32654
I0513 00:09:33.707639 140311617681408 trainer.py:518] Training: step 32654
I0513 00:09:33.707854 140595318249472 trainer.py:518] Training: step 32654
I0513 00:09:51.598662 140339465779200 trainer.py:518] Training: step 32655
I0513 00:09:51.599126 140095893518336 trainer.py:518] Training: step 32655
I0513 00:09:51.598166 139674595104768 trainer.py:518] Training: step 32655
I0513 00:09:51.598192 140440593786880 trainer.py:518] Training: step 32655
I0513 00:09:51.598874 140031916288000 trainer.py:518] Training: step 32655
I0513 00:09:51.599515 140529434355712 trainer.py:518] Training: step 32655
I0513 00:09:51.599458 140311617681408 trainer.py:518] Training: step 32655
I0513 00:09:51.599423 140595318249472 trainer.py:518] Training: step 32655
I0513 00:10:09.489145 140339465779200 trainer.py:518] Training: step 32656
I0513 00:10:09.488992 139674595104768 trainer.py:518] Training: step 32656
I0513 00:10:09.491216 140095893518336 trainer.py:518] Training: step 32656
I0513 00:10:09.488904 140440593786880 trainer.py:518] Training: step 32656
I0513 00:10:09.489519 140031916288000 trainer.py:518] Training: step 32656
I0513 00:10:09.490077 140529434355712 trainer.py:518] Training: step 32656
I0513 00:10:09.490095 140311617681408 trainer.py:518] Training: step 32656
I0513 00:10:09.489956 140595318249472 trainer.py:518] Training: step 32656
I0513 00:10:27.380710 140095893518336 trainer.py:518] Training: step 32657
I0513 00:10:27.380537 139674595104768 trainer.py:518] Training: step 32657
I0513 00:10:27.382590 140339465779200 trainer.py:518] Training: step 32657
I0513 00:10:27.380361 140031916288000 trainer.py:518] Training: step 32657
I0513 00:10:27.380574 140440593786880 trainer.py:518] Training: step 32657
I0513 00:10:27.380996 140529434355712 trainer.py:518] Training: step 32657
I0513 00:10:27.381302 140311617681408 trainer.py:518] Training: step 32657
I0513 00:10:27.381260 140595318249472 trainer.py:518] Training: step 32657
I0513 00:10:45.271254 140095893518336 trainer.py:518] Training: step 32658
I0513 00:10:45.271548 140339465779200 trainer.py:518] Training: step 32658
I0513 00:10:45.270294 139674595104768 trainer.py:518] Training: step 32658
I0513 00:10:45.270480 140440593786880 trainer.py:518] Training: step 32658
I0513 00:10:45.270783 140031916288000 trainer.py:518] Training: step 32658
I0513 00:10:45.271823 140529434355712 trainer.py:518] Training: step 32658
I0513 00:10:45.271289 140311617681408 trainer.py:518] Training: step 32658
I0513 00:10:45.271615 140595318249472 trainer.py:518] Training: step 32658
I0513 00:10:54.218718 140439986517568 logging_writer.py:48] [32640] collection=train accuracy=0.568653, cross_ent_loss=16.59711265563965, cross_ent_loss_per_all_target_tokens=2.11043e-05, experts/auxiliary_loss=0.16067516803741455, experts/expert_usage=7.975477695465088, experts/fraction_tokens_left_behind=0.2591370642185211, experts/router_confidence=3.4717888832092285, experts/router_z_loss=0.00031848400249145925, learning_rate=0.00553556, learning_rate/current=0.00553518, loss=16.765968322753906, loss_per_all_target_tokens=2.1319e-05, loss_per_nonpadding_target_token=2.14497e-05, non_padding_fraction/loss_weights=0.993909, timing/seconds=89.45150756835938, timing/seqs=3840, timing/seqs_per_second=42.928287506103516, timing/seqs_per_second_per_core=1.3415089845657349, timing/steps_per_second=0.1117924153804779, timing/target_tokens_per_second=87917.1328125, timing/target_tokens_per_second_per_core=2747.410400390625, timing/uptime=1539.17, z_loss=0.007859887555241585, z_loss_per_all_target_tokens=9.99436e-09
I0513 00:11:03.162707 140095893518336 trainer.py:518] Training: step 32659
I0513 00:11:03.163323 140339465779200 trainer.py:518] Training: step 32659
I0513 00:11:03.162283 139674595104768 trainer.py:518] Training: step 32659
I0513 00:11:03.161902 140440593786880 trainer.py:518] Training: step 32659
I0513 00:11:03.162289 140031916288000 trainer.py:518] Training: step 32659
I0513 00:11:03.164414 140529434355712 trainer.py:518] Training: step 32659
I0513 00:11:03.163018 140311617681408 trainer.py:518] Training: step 32659
I0513 00:11:03.163145 140595318249472 trainer.py:518] Training: step 32659
I0513 00:11:21.054763 140339465779200 trainer.py:518] Training: step 32664
I0513 00:11:21.053405 139674595104768 trainer.py:518] Training: step 32666
I0513 00:11:21.054562 140031916288000 trainer.py:518] Training: step 32666
I0513 00:11:21.054448 140529434355712 trainer.py:518] Training: step 32664
I0513 00:11:21.053885 140595318249472 trainer.py:518] Training: step 32667
I0513 00:11:21.057399 140439986517568 logging_writer.py:48] [32650] collection=train accuracy=0.573797, cross_ent_loss=16.423633575439453, cross_ent_loss_per_all_target_tokens=2.08837e-05, experts/auxiliary_loss=0.16071179509162903, experts/expert_usage=7.956658840179443, experts/fraction_tokens_left_behind=0.2483607977628708, experts/router_confidence=3.4617981910705566, experts/router_z_loss=0.0003173842851538211, learning_rate=0.00553471, learning_rate/current=0.00553433, loss=16.592514038085938, loss_per_all_target_tokens=2.10985e-05, loss_per_nonpadding_target_token=2.12481e-05, non_padding_fraction/loss_weights=0.992959, timing/seconds=89.44829559326172, timing/seqs=3840, timing/seqs_per_second=42.92982864379883, timing/seqs_per_second_per_core=1.3415571451187134, timing/steps_per_second=0.11179642379283905, timing/target_tokens_per_second=87920.2890625, timing/target_tokens_per_second_per_core=2747.509033203125, timing/uptime=1557.25, z_loss=0.007853317074477673, z_loss_per_all_target_tokens=9.98601e-09
I0513 00:11:21.082228 140095893518336 trainer.py:518] Training: step 32667
I0513 00:11:21.082076 140440593786880 trainer.py:518] Training: step 32666
I0513 00:11:21.083778 140311617681408 trainer.py:518] Training: step 32667
I0513 00:11:56.837694 140095893518336 trainer.py:518] Training: step 32672
I0513 00:11:56.837611 140339465779200 trainer.py:518] Training: step 32672
I0513 00:11:56.837193 139674595104768 trainer.py:518] Training: step 32672
I0513 00:11:56.837429 140031916288000 trainer.py:518] Training: step 32672
I0513 00:11:56.837629 140440593786880 trainer.py:518] Training: step 32672
I0513 00:11:56.837510 140529434355712 trainer.py:518] Training: step 32672
I0513 00:11:56.837603 140311617681408 trainer.py:518] Training: step 32672
I0513 00:11:56.837995 140595318249472 trainer.py:518] Training: step 32672
I0513 00:12:14.727765 140095893518336 trainer.py:518] Training: step 32673
I0513 00:12:14.728519 140339465779200 trainer.py:518] Training: step 32673
I0513 00:12:14.727772 139674595104768 trainer.py:518] Training: step 32673
I0513 00:12:14.727572 140440593786880 trainer.py:518] Training: step 32673
I0513 00:12:14.727823 140529434355712 trainer.py:518] Training: step 32673
I0513 00:12:14.727766 140311617681408 trainer.py:518] Training: step 32673
I0513 00:12:14.727818 140595318249472 trainer.py:518] Training: step 32673
I0513 00:12:23.672961 140031916288000 trainer.py:518] Training: step 32673
I0513 00:12:32.619119 140339465779200 trainer.py:518] Training: step 32674
I0513 00:12:32.620024 140095893518336 trainer.py:518] Training: step 32674
I0513 00:12:32.619057 139674595104768 trainer.py:518] Training: step 32674
I0513 00:12:32.619335 140440593786880 trainer.py:518] Training: step 32674
I0513 00:12:32.619151 140529434355712 trainer.py:518] Training: step 32674
I0513 00:12:32.619170 140311617681408 trainer.py:518] Training: step 32674
I0513 00:12:32.619575 140595318249472 trainer.py:518] Training: step 32674
I0513 00:12:41.564619 140031916288000 trainer.py:518] Training: step 32674
I0513 00:12:50.510053 140339465779200 trainer.py:518] Training: step 32675
I0513 00:12:50.510331 140095893518336 trainer.py:518] Training: step 32675
I0513 00:12:50.509718 139674595104768 trainer.py:518] Training: step 32675
I0513 00:12:50.509670 140440593786880 trainer.py:518] Training: step 32675
I0513 00:12:50.509946 140529434355712 trainer.py:518] Training: step 32675
I0513 00:12:50.509912 140311617681408 trainer.py:518] Training: step 32675
I0513 00:12:50.510358 140595318249472 trainer.py:518] Training: step 32675
I0513 00:12:59.456739 140031916288000 trainer.py:518] Training: step 32675
I0513 00:13:08.400041 140339465779200 trainer.py:518] Training: step 32676
I0513 00:13:08.400604 140095893518336 trainer.py:518] Training: step 32676
I0513 00:13:08.399896 139674595104768 trainer.py:518] Training: step 32676
I0513 00:13:08.399545 140440593786880 trainer.py:518] Training: step 32676
I0513 00:13:08.400293 140529434355712 trainer.py:518] Training: step 32676
I0513 00:13:08.399634 140311617681408 trainer.py:518] Training: step 32676
I0513 00:13:08.400383 140595318249472 trainer.py:518] Training: step 32676
I0513 00:13:17.345072 140031916288000 trainer.py:518] Training: step 32676
I0513 00:13:26.289436 140339465779200 trainer.py:518] Training: step 32677
I0513 00:13:26.289954 140095893518336 trainer.py:518] Training: step 32677
I0513 00:13:26.289642 139674595104768 trainer.py:518] Training: step 32677
I0513 00:13:26.289659 140440593786880 trainer.py:518] Training: step 32677
I0513 00:13:26.290211 140529434355712 trainer.py:518] Training: step 32677
I0513 00:13:26.289520 140311617681408 trainer.py:518] Training: step 32677
I0513 00:13:26.290245 140595318249472 trainer.py:518] Training: step 32677
I0513 00:13:44.179703 140339465779200 trainer.py:518] Training: step 32678
I0513 00:13:44.181092 140095893518336 trainer.py:518] Training: step 32678
I0513 00:13:44.179620 139674595104768 trainer.py:518] Training: step 32678
I0513 00:13:44.179833 140440593786880 trainer.py:518] Training: step 32678
I0513 00:13:44.180082 140031916288000 trainer.py:518] Training: step 32678
I0513 00:13:44.180869 140529434355712 trainer.py:518] Training: step 32678
I0513 00:13:44.180178 140311617681408 trainer.py:518] Training: step 32678
I0513 00:13:44.180291 140595318249472 trainer.py:518] Training: step 32678
I0513 00:13:53.127862 140439986517568 logging_writer.py:48] [32660] collection=train accuracy=0.56671, cross_ent_loss=16.687788009643555, cross_ent_loss_per_all_target_tokens=2.12196e-05, experts/auxiliary_loss=0.16074888408184052, experts/expert_usage=7.9551239013671875, experts/fraction_tokens_left_behind=0.25342416763305664, experts/router_confidence=3.4554378986358643, experts/router_z_loss=0.00031671070610173047, learning_rate=0.00553386, learning_rate/current=0.00553348, loss=16.856830596923828, loss_per_all_target_tokens=2.14346e-05, loss_per_nonpadding_target_token=2.15737e-05, non_padding_fraction/loss_weights=0.993551, timing/seconds=89.45437622070312, timing/seqs=3840, timing/seqs_per_second=42.926910400390625, timing/seqs_per_second_per_core=1.341465950012207, timing/steps_per_second=0.11178882420063019, timing/target_tokens_per_second=87914.3125, timing/target_tokens_per_second_per_core=2747.322265625, timing/uptime=1718.07, z_loss=0.007979506626725197, z_loss_per_all_target_tokens=1.01465e-08
I0513 00:14:02.071587 140339465779200 trainer.py:518] Training: step 32679
I0513 00:14:02.072067 140095893518336 trainer.py:518] Training: step 32679
I0513 00:14:02.071767 139674595104768 trainer.py:518] Training: step 32679
I0513 00:14:02.071155 140440593786880 trainer.py:518] Training: step 32679
I0513 00:14:02.071548 140031916288000 trainer.py:518] Training: step 32679
I0513 00:14:02.072434 140529434355712 trainer.py:518] Training: step 32679
I0513 00:14:02.071692 140311617681408 trainer.py:518] Training: step 32679
I0513 00:14:02.072783 140595318249472 trainer.py:518] Training: step 32679
I0513 00:14:19.962089 140095893518336 trainer.py:518] Training: step 32688
I0513 00:14:19.961347 139674595104768 trainer.py:518] Training: step 32686
I0513 00:14:19.963613 140439986517568 logging_writer.py:48] [32670] collection=train accuracy=0.57136, cross_ent_loss=16.456552505493164, cross_ent_loss_per_all_target_tokens=2.09256e-05, experts/auxiliary_loss=0.16072465479373932, experts/expert_usage=7.9783196449279785, experts/fraction_tokens_left_behind=0.2667819857597351, experts/router_confidence=3.4584877490997314, experts/router_z_loss=0.0003160544147249311, learning_rate=0.00553302, learning_rate/current=0.00553264, loss=16.625085830688477, loss_per_all_target_tokens=2.11399e-05, loss_per_nonpadding_target_token=2.12795e-05, non_padding_fraction/loss_weights=0.993441, timing/seconds=89.44749450683594, timing/seqs=3840, timing/seqs_per_second=42.930213928222656, timing/seqs_per_second_per_core=1.341569185256958, timing/steps_per_second=0.11179743707180023, timing/target_tokens_per_second=87921.078125, timing/target_tokens_per_second_per_core=2747.53369140625, timing/uptime=1736.16, z_loss=0.007493608631193638, z_loss_per_all_target_tokens=9.52862e-09
I0513 00:14:19.987902 140339465779200 trainer.py:518] Training: step 32686
I0513 00:14:19.988279 140440593786880 trainer.py:518] Training: step 32688
I0513 00:14:19.991834 140529434355712 trainer.py:518] Training: step 32684
I0513 00:14:19.990383 140311617681408 trainer.py:518] Training: step 32687
I0513 00:14:19.990628 140595318249472 trainer.py:518] Training: step 32687
I0513 00:14:28.906970 140031916288000 trainer.py:518] Training: step 32690
I0513 00:14:55.743205 140339465779200 trainer.py:518] Training: step 32692
I0513 00:14:55.743567 140095893518336 trainer.py:518] Training: step 32692
I0513 00:14:55.742632 139674595104768 trainer.py:518] Training: step 32692
I0513 00:14:55.742639 140031916288000 trainer.py:518] Training: step 32692
I0513 00:14:55.743577 140440593786880 trainer.py:518] Training: step 32692
I0513 00:14:55.743413 140529434355712 trainer.py:518] Training: step 32692
I0513 00:14:55.742912 140311617681408 trainer.py:518] Training: step 32692
I0513 00:14:55.744154 140595318249472 trainer.py:518] Training: step 32692
I0513 00:15:13.634001 140095893518336 trainer.py:518] Training: step 32693
I0513 00:15:13.633756 140339465779200 trainer.py:518] Training: step 32693
I0513 00:15:13.633198 139674595104768 trainer.py:518] Training: step 32693
I0513 00:15:13.633394 140440593786880 trainer.py:518] Training: step 32693
I0513 00:15:13.633588 140031916288000 trainer.py:518] Training: step 32693
I0513 00:15:13.633811 140529434355712 trainer.py:518] Training: step 32693
I0513 00:15:13.633532 140311617681408 trainer.py:518] Training: step 32693
I0513 00:15:13.634289 140595318249472 trainer.py:518] Training: step 32693
I0513 00:15:31.524743 140339465779200 trainer.py:518] Training: step 32694
I0513 00:15:31.525282 140095893518336 trainer.py:518] Training: step 32694
I0513 00:15:31.524634 139674595104768 trainer.py:518] Training: step 32694
I0513 00:15:31.524621 140031916288000 trainer.py:518] Training: step 32694
I0513 00:15:31.524902 140440593786880 trainer.py:518] Training: step 32694
I0513 00:15:31.524875 140529434355712 trainer.py:518] Training: step 32694
I0513 00:15:31.524955 140311617681408 trainer.py:518] Training: step 32694
I0513 00:15:31.525239 140595318249472 trainer.py:518] Training: step 32694
I0513 00:15:49.414657 140339465779200 trainer.py:518] Training: step 32695
I0513 00:15:49.415747 140095893518336 trainer.py:518] Training: step 32695
I0513 00:15:49.414477 139674595104768 trainer.py:518] Training: step 32695
I0513 00:15:49.414731 140440593786880 trainer.py:518] Training: step 32695
I0513 00:15:49.414861 140031916288000 trainer.py:518] Training: step 32695
I0513 00:15:49.415064 140529434355712 trainer.py:518] Training: step 32695
I0513 00:15:49.415043 140595318249472 trainer.py:518] Training: step 32695
I0513 00:15:49.418738 140311617681408 trainer.py:518] Training: step 32695
I0513 00:16:07.306104 140339465779200 trainer.py:518] Training: step 32696
I0513 00:16:07.306890 140095893518336 trainer.py:518] Training: step 32696
I0513 00:16:07.306057 139674595104768 trainer.py:518] Training: step 32696
I0513 00:16:07.306212 140031916288000 trainer.py:518] Training: step 32696
I0513 00:16:07.306170 140440593786880 trainer.py:518] Training: step 32696
I0513 00:16:07.306650 140529434355712 trainer.py:518] Training: step 32696
I0513 00:16:07.306798 140311617681408 trainer.py:518] Training: step 32696
I0513 00:16:07.307076 140595318249472 trainer.py:518] Training: step 32696
I0513 00:16:25.196418 140339465779200 trainer.py:518] Training: step 32697
I0513 00:16:25.197074 140095893518336 trainer.py:518] Training: step 32697
I0513 00:16:25.196216 139674595104768 trainer.py:518] Training: step 32697
I0513 00:16:25.196229 140031916288000 trainer.py:518] Training: step 32697
I0513 00:16:25.196510 140440593786880 trainer.py:518] Training: step 32697
I0513 00:16:25.197446 140529434355712 trainer.py:518] Training: step 32697
I0513 00:16:25.196592 140311617681408 trainer.py:518] Training: step 32697
I0513 00:16:25.196786 140595318249472 trainer.py:518] Training: step 32697
I0513 00:16:43.087945 140339465779200 trainer.py:518] Training: step 32698
I0513 00:16:43.088643 140095893518336 trainer.py:518] Training: step 32698
I0513 00:16:43.087682 139674595104768 trainer.py:518] Training: step 32698
I0513 00:16:43.087807 140031916288000 trainer.py:518] Training: step 32698
I0513 00:16:43.088011 140440593786880 trainer.py:518] Training: step 32698
I0513 00:16:43.089278 140529434355712 trainer.py:518] Training: step 32698
I0513 00:16:43.088141 140311617681408 trainer.py:518] Training: step 32698
I0513 00:16:43.088562 140595318249472 trainer.py:518] Training: step 32698
I0513 00:16:52.035523 140439986517568 logging_writer.py:48] [32680] collection=train accuracy=0.580673, cross_ent_loss=16.042993545532227, cross_ent_loss_per_all_target_tokens=2.03997e-05, experts/auxiliary_loss=0.16071546077728271, experts/expert_usage=7.964293956756592, experts/fraction_tokens_left_behind=0.26968368887901306, experts/router_confidence=3.4451446533203125, experts/router_z_loss=0.00031601727823726833, learning_rate=0.00553217, learning_rate/current=0.00553179, loss=16.211389541625977, loss_per_all_target_tokens=2.06138e-05, loss_per_nonpadding_target_token=2.07504e-05, non_padding_fraction/loss_weights=0.993417, timing/seconds=89.45095825195312, timing/seqs=3840, timing/seqs_per_second=42.928550720214844, timing/seqs_per_second_per_core=1.3415172100067139, timing/steps_per_second=0.11179310083389282, timing/target_tokens_per_second=87917.671875, timing/target_tokens_per_second_per_core=2747.42724609375, timing/uptime=1896.98, z_loss=0.0073641142807900906, z_loss_per_all_target_tokens=9.36396e-09
I0513 00:17:00.979290 140095893518336 trainer.py:518] Training: step 32699
I0513 00:17:00.979437 140339465779200 trainer.py:518] Training: step 32699
I0513 00:17:00.978238 139674595104768 trainer.py:518] Training: step 32699
I0513 00:17:00.979103 140440593786880 trainer.py:518] Training: step 32699
I0513 00:17:00.978978 140031916288000 trainer.py:518] Training: step 32699
I0513 00:17:00.979598 140311617681408 trainer.py:518] Training: step 32699
I0513 00:17:00.981444 140529434355712 trainer.py:518] Training: step 32699
I0513 00:17:00.979854 140595318249472 trainer.py:518] Training: step 32699
I0513 00:17:11.069972 140095893518336 trainer.py:518] Training: step 32707
I0513 00:17:18.870274 140339465779200 trainer.py:518] Training: step 32706
I0513 00:17:18.870573 139674595104768 trainer.py:518] Training: step 32705
I0513 00:17:18.870182 140031916288000 trainer.py:518] Training: step 32707
I0513 00:17:18.870468 140440593786880 trainer.py:518] Training: step 32706
I0513 00:17:18.872174 140439986517568 logging_writer.py:48] [32690] collection=train accuracy=0.572474, cross_ent_loss=16.45163917541504, cross_ent_loss_per_all_target_tokens=2.09193e-05, experts/auxiliary_loss=0.160744309425354, experts/expert_usage=7.961915493011475, experts/fraction_tokens_left_behind=0.2508838474750519, experts/router_confidence=3.4723527431488037, experts/router_z_loss=0.0003119708562735468, learning_rate=0.00553132, learning_rate/current=0.00553094, loss=16.620302200317383, loss_per_all_target_tokens=2.11338e-05, loss_per_nonpadding_target_token=2.12378e-05, non_padding_fraction/loss_weights=0.995105, timing/seconds=89.44603729248047, timing/seqs=3840, timing/seqs_per_second=42.930912017822266, timing/seqs_per_second_per_core=1.3415910005569458, timing/steps_per_second=0.11179924756288528, timing/target_tokens_per_second=87922.5078125, timing/target_tokens_per_second_per_core=2747.578369140625, timing/uptime=1915.81, z_loss=0.0076074423268437386, z_loss_per_all_target_tokens=9.67336e-09
I0513 00:17:18.903579 140529434355712 trainer.py:518] Training: step 32704
I0513 00:17:18.901278 140595318249472 trainer.py:518] Training: step 32707
I0513 00:17:18.903385 140311617681408 trainer.py:518] Training: step 32707
I0513 00:17:27.816228 140095893518336 trainer.py:518] Training: step 32710
I0513 00:17:54.650599 140339465779200 trainer.py:518] Training: step 32712
I0513 00:17:54.651587 140095893518336 trainer.py:518] Training: step 32712
I0513 00:17:54.650476 139674595104768 trainer.py:518] Training: step 32712
I0513 00:17:54.650300 140031916288000 trainer.py:518] Training: step 32712
I0513 00:17:54.650792 140440593786880 trainer.py:518] Training: step 32712
I0513 00:17:54.651439 140529434355712 trainer.py:518] Training: step 32712
I0513 00:17:54.651363 140311617681408 trainer.py:518] Training: step 32712
I0513 00:17:54.651808 140595318249472 trainer.py:518] Training: step 32712
I0513 00:18:12.541113 140339465779200 trainer.py:518] Training: step 32713
I0513 00:18:12.541676 140095893518336 trainer.py:518] Training: step 32713
I0513 00:18:12.541099 139674595104768 trainer.py:518] Training: step 32713
I0513 00:18:12.540838 140031916288000 trainer.py:518] Training: step 32713
I0513 00:18:12.540897 140440593786880 trainer.py:518] Training: step 32713
I0513 00:18:12.541510 140529434355712 trainer.py:518] Training: step 32713
I0513 00:18:12.541908 140311617681408 trainer.py:518] Training: step 32713
I0513 00:18:12.541849 140595318249472 trainer.py:518] Training: step 32713
I0513 00:18:30.431306 140339465779200 trainer.py:518] Training: step 32714
I0513 00:18:30.431936 140095893518336 trainer.py:518] Training: step 32714
I0513 00:18:30.431319 139674595104768 trainer.py:518] Training: step 32714
I0513 00:18:30.430859 140031916288000 trainer.py:518] Training: step 32714
I0513 00:18:30.432145 140440593786880 trainer.py:518] Training: step 32714
I0513 00:18:30.432214 140529434355712 trainer.py:518] Training: step 32714
I0513 00:18:30.432481 140311617681408 trainer.py:518] Training: step 32714
I0513 00:18:30.432526 140595318249472 trainer.py:518] Training: step 32714
I0513 00:18:48.322972 140339465779200 trainer.py:518] Training: step 32715
I0513 00:18:48.323427 140095893518336 trainer.py:518] Training: step 32715
I0513 00:18:48.322806 139674595104768 trainer.py:518] Training: step 32715
I0513 00:18:48.322784 140031916288000 trainer.py:518] Training: step 32715
I0513 00:18:48.323578 140529434355712 trainer.py:518] Training: step 32715
I0513 00:18:48.323550 140311617681408 trainer.py:518] Training: step 32715
I0513 00:18:48.323903 140595318249472 trainer.py:518] Training: step 32715
I0513 00:18:57.267991 140440593786880 trainer.py:518] Training: step 32715
I0513 00:19:06.214767 140339465779200 trainer.py:518] Training: step 32716
I0513 00:19:06.215316 140095893518336 trainer.py:518] Training: step 32716
I0513 00:19:06.214649 139674595104768 trainer.py:518] Training: step 32716
I0513 00:19:06.214546 140031916288000 trainer.py:518] Training: step 32716
I0513 00:19:06.215799 140529434355712 trainer.py:518] Training: step 32716
I0513 00:19:06.214613 140311617681408 trainer.py:518] Training: step 32716
I0513 00:19:06.215633 140595318249472 trainer.py:518] Training: step 32716
I0513 00:19:15.161549 140440593786880 trainer.py:518] Training: step 32716
I0513 00:19:24.105628 140339465779200 trainer.py:518] Training: step 32717
I0513 00:19:24.106085 140095893518336 trainer.py:518] Training: step 32717
I0513 00:19:24.105921 139674595104768 trainer.py:518] Training: step 32717
I0513 00:19:24.105604 140031916288000 trainer.py:518] Training: step 32717
I0513 00:19:24.106576 140529434355712 trainer.py:518] Training: step 32717
I0513 00:19:24.105995 140311617681408 trainer.py:518] Training: step 32717
I0513 00:19:24.106521 140595318249472 trainer.py:518] Training: step 32717
I0513 00:19:41.996024 140339465779200 trainer.py:518] Training: step 32718
I0513 00:19:41.996906 140095893518336 trainer.py:518] Training: step 32718
I0513 00:19:41.995953 139674595104768 trainer.py:518] Training: step 32718
I0513 00:19:41.996003 140031916288000 trainer.py:518] Training: step 32718
I0513 00:19:41.996534 140440593786880 trainer.py:518] Training: step 32718
I0513 00:19:41.996964 140529434355712 trainer.py:518] Training: step 32718
I0513 00:19:41.996056 140311617681408 trainer.py:518] Training: step 32718
I0513 00:19:41.996791 140595318249472 trainer.py:518] Training: step 32718
I0513 00:19:50.942341 140439986517568 logging_writer.py:48] [32700] collection=train accuracy=0.574315, cross_ent_loss=16.319461822509766, cross_ent_loss_per_all_target_tokens=2.07513e-05, experts/auxiliary_loss=0.16071327030658722, experts/expert_usage=7.9681315422058105, experts/fraction_tokens_left_behind=0.2293887585401535, experts/router_confidence=3.4680092334747314, experts/router_z_loss=0.00030991315725259483, learning_rate=0.00553048, learning_rate/current=0.0055301, loss=16.48805046081543, loss_per_all_target_tokens=2.09656e-05, loss_per_nonpadding_target_token=2.10501e-05, non_padding_fraction/loss_weights=0.995986, timing/seconds=89.45279693603516, timing/seqs=3840, timing/seqs_per_second=42.927669525146484, timing/seqs_per_second_per_core=1.3414896726608276, timing/steps_per_second=0.11179080605506897, timing/target_tokens_per_second=87915.8671875, timing/target_tokens_per_second_per_core=2747.370849609375, timing/uptime=2075.89, z_loss=0.007567220833152533, z_loss_per_all_target_tokens=9.62222e-09
I0513 00:19:59.888126 140095893518336 trainer.py:518] Training: step 32719
I0513 00:19:59.887750 140339465779200 trainer.py:518] Training: step 32719
I0513 00:19:59.887933 139674595104768 trainer.py:518] Training: step 32719
I0513 00:19:59.887644 140440593786880 trainer.py:518] Training: step 32719
I0513 00:19:59.887916 140031916288000 trainer.py:518] Training: step 32719
I0513 00:19:59.889430 140529434355712 trainer.py:518] Training: step 32719
I0513 00:19:59.888582 140311617681408 trainer.py:518] Training: step 32719
I0513 00:19:59.888818 140595318249472 trainer.py:518] Training: step 32719
I0513 00:20:17.779665 140095893518336 trainer.py:518] Training: step 32728
I0513 00:20:17.779236 139674595104768 trainer.py:518] Training: step 32726
I0513 00:20:17.780304 140439986517568 logging_writer.py:48] [32710] collection=train accuracy=0.565661, cross_ent_loss=16.744747161865234, cross_ent_loss_per_all_target_tokens=2.1292e-05, experts/auxiliary_loss=0.16071058809757233, experts/expert_usage=7.961207866668701, experts/fraction_tokens_left_behind=0.25252291560173035, experts/router_confidence=3.4667510986328125, experts/router_z_loss=0.0003194277815055102, learning_rate=0.00552963, learning_rate/current=0.00552925, loss=16.91362762451172, loss_per_all_target_tokens=2.15068e-05, loss_per_nonpadding_target_token=2.16584e-05, non_padding_fraction/loss_weights=0.993, timing/seconds=89.44686126708984, timing/seqs=3840, timing/seqs_per_second=42.930519104003906, timing/seqs_per_second_per_core=1.341578722000122, timing/steps_per_second=0.1117982268333435, timing/target_tokens_per_second=87921.703125, timing/target_tokens_per_second_per_core=2747.55322265625, timing/uptime=2094, z_loss=0.007847681641578674, z_loss_per_all_target_tokens=9.97884e-09
I0513 00:20:17.807224 140339465779200 trainer.py:518] Training: step 32726
I0513 00:20:17.805525 140031916288000 trainer.py:518] Training: step 32729
I0513 00:20:17.807237 140311617681408 trainer.py:518] Training: step 32728
I0513 00:20:17.807922 140595318249472 trainer.py:518] Training: step 32728
I0513 00:20:17.811841 140529434355712 trainer.py:518] Training: step 32726
I0513 00:20:26.729419 140440593786880 trainer.py:518] Training: step 32730
I0513 00:20:53.561398 140095893518336 trainer.py:518] Training: step 32732
I0513 00:20:53.561614 140339465779200 trainer.py:518] Training: step 32732
I0513 00:20:53.561201 139674595104768 trainer.py:518] Training: step 32732
I0513 00:20:53.560993 140031916288000 trainer.py:518] Training: step 32732
I0513 00:20:53.561357 140440593786880 trainer.py:518] Training: step 32732
I0513 00:20:53.562441 140529434355712 trainer.py:518] Training: step 32732
I0513 00:20:53.561457 140311617681408 trainer.py:518] Training: step 32732
I0513 00:20:53.562606 140595318249472 trainer.py:518] Training: step 32732
I0513 00:21:11.451210 140095893518336 trainer.py:518] Training: step 32733
I0513 00:21:11.451483 140339465779200 trainer.py:518] Training: step 32733
I0513 00:21:11.451096 139674595104768 trainer.py:518] Training: step 32733
I0513 00:21:11.451265 140031916288000 trainer.py:518] Training: step 32733
I0513 00:21:11.451636 140440593786880 trainer.py:518] Training: step 32733
I0513 00:21:11.452476 140529434355712 trainer.py:518] Training: step 32733
I0513 00:21:11.451344 140311617681408 trainer.py:518] Training: step 32733
I0513 00:21:11.452318 140595318249472 trainer.py:518] Training: step 32733
I0513 00:21:29.343202 140095893518336 trainer.py:518] Training: step 32734
I0513 00:21:29.343309 140339465779200 trainer.py:518] Training: step 32734
I0513 00:21:29.343941 139674595104768 trainer.py:518] Training: step 32734
I0513 00:21:29.343165 140031916288000 trainer.py:518] Training: step 32734
I0513 00:21:29.343648 140440593786880 trainer.py:518] Training: step 32734
I0513 00:21:29.344353 140529434355712 trainer.py:518] Training: step 32734
I0513 00:21:29.343568 140311617681408 trainer.py:518] Training: step 32734
I0513 00:21:29.344667 140595318249472 trainer.py:518] Training: step 32734
I0513 00:21:47.234038 140339465779200 trainer.py:518] Training: step 32735
I0513 00:21:47.234665 140095893518336 trainer.py:518] Training: step 32735
I0513 00:21:47.234171 139674595104768 trainer.py:518] Training: step 32735
I0513 00:21:47.234558 140031916288000 trainer.py:518] Training: step 32735
I0513 00:21:47.240448 140440593786880 trainer.py:518] Training: step 32735
I0513 00:21:47.234987 140529434355712 trainer.py:518] Training: step 32735
I0513 00:21:47.234138 140311617681408 trainer.py:518] Training: step 32735
I0513 00:21:47.235354 140595318249472 trainer.py:518] Training: step 32735
I0513 00:22:05.125076 140095893518336 trainer.py:518] Training: step 32736
I0513 00:22:05.125149 140339465779200 trainer.py:518] Training: step 32736
I0513 00:22:05.125478 139674595104768 trainer.py:518] Training: step 32736
I0513 00:22:05.124874 140031916288000 trainer.py:518] Training: step 32736
I0513 00:22:05.125479 140440593786880 trainer.py:518] Training: step 32736
I0513 00:22:05.126195 140529434355712 trainer.py:518] Training: step 32736
I0513 00:22:05.125401 140311617681408 trainer.py:518] Training: step 32736
I0513 00:22:05.126220 140595318249472 trainer.py:518] Training: step 32736
I0513 00:22:23.014798 140095893518336 trainer.py:518] Training: step 32737
I0513 00:22:23.014859 140339465779200 trainer.py:518] Training: step 32737
I0513 00:22:23.015043 139674595104768 trainer.py:518] Training: step 32737
I0513 00:22:23.014611 140031916288000 trainer.py:518] Training: step 32737
I0513 00:22:23.015553 140440593786880 trainer.py:518] Training: step 32737
I0513 00:22:23.015852 140529434355712 trainer.py:518] Training: step 32737
I0513 00:22:23.015062 140311617681408 trainer.py:518] Training: step 32737
I0513 00:22:23.015931 140595318249472 trainer.py:518] Training: step 32737
I0513 00:22:40.905507 140095893518336 trainer.py:518] Training: step 32738
I0513 00:22:40.905729 140339465779200 trainer.py:518] Training: step 32738
I0513 00:22:40.905736 139674595104768 trainer.py:518] Training: step 32738
I0513 00:22:40.905300 140031916288000 trainer.py:518] Training: step 32738
I0513 00:22:40.906292 140440593786880 trainer.py:518] Training: step 32738
I0513 00:22:40.907153 140529434355712 trainer.py:518] Training: step 32738
I0513 00:22:40.905914 140311617681408 trainer.py:518] Training: step 32738
I0513 00:22:40.906732 140595318249472 trainer.py:518] Training: step 32738
I0513 00:22:49.852732 140439986517568 logging_writer.py:48] [32720] collection=train accuracy=0.569454, cross_ent_loss=16.4956111907959, cross_ent_loss_per_all_target_tokens=2.09753e-05, experts/auxiliary_loss=0.160742849111557, experts/expert_usage=7.96011209487915, experts/fraction_tokens_left_behind=0.26783713698387146, experts/router_confidence=3.4636521339416504, experts/router_z_loss=0.0003095047431997955, learning_rate=0.00552879, learning_rate/current=0.00552841, loss=16.664295196533203, loss_per_all_target_tokens=2.11897e-05, loss_per_nonpadding_target_token=2.13277e-05, non_padding_fraction/loss_weights=0.993533, timing/seconds=89.45269775390625, timing/seqs=3840, timing/seqs_per_second=42.92771530151367, timing/seqs_per_second_per_core=1.3414911031723022, timing/steps_per_second=0.11179092526435852, timing/target_tokens_per_second=87915.9609375, timing/target_tokens_per_second_per_core=2747.373779296875, timing/uptime=2254.8, z_loss=0.007629143539816141, z_loss_per_all_target_tokens=9.70096e-09
I0513 00:22:58.797301 140095893518336 trainer.py:518] Training: step 32739
I0513 00:22:58.797299 140339465779200 trainer.py:518] Training: step 32739
I0513 00:22:58.797171 139674595104768 trainer.py:518] Training: step 32739
I0513 00:22:58.797031 140031916288000 trainer.py:518] Training: step 32739
I0513 00:22:58.797147 140440593786880 trainer.py:518] Training: step 32739
I0513 00:22:58.799291 140529434355712 trainer.py:518] Training: step 32739
I0513 00:22:58.797688 140311617681408 trainer.py:518] Training: step 32739
I0513 00:22:58.798138 140595318249472 trainer.py:518] Training: step 32739
I0513 00:23:16.687795 140339465779200 trainer.py:518] Training: step 32746
I0513 00:23:16.688351 140095893518336 trainer.py:518] Training: step 32748
I0513 00:23:16.688375 140440593786880 trainer.py:518] Training: step 32747
I0513 00:23:16.688459 140031916288000 trainer.py:518] Training: step 32747
I0513 00:23:16.688936 140439986517568 logging_writer.py:48] [32730] collection=train accuracy=0.568376, cross_ent_loss=16.6207275390625, cross_ent_loss_per_all_target_tokens=2.11343e-05, experts/auxiliary_loss=0.16075573861598969, experts/expert_usage=7.957472324371338, experts/fraction_tokens_left_behind=0.2545894384384155, experts/router_confidence=3.480192184448242, experts/router_z_loss=0.0003093098057433963, learning_rate=0.00552794, learning_rate/current=0.00552756, loss=16.78997802734375, loss_per_all_target_tokens=2.13496e-05, loss_per_nonpadding_target_token=2.14297e-05, non_padding_fraction/loss_weights=0.996261, timing/seconds=89.44731903076172, timing/seqs=3840, timing/seqs_per_second=42.9302978515625, timing/seqs_per_second_per_core=1.3415718078613281, timing/steps_per_second=0.11179764568805695, timing/target_tokens_per_second=87921.25, timing/target_tokens_per_second_per_core=2747.5390625, timing/uptime=2272.82, z_loss=0.00818619504570961, z_loss_per_all_target_tokens=1.04093e-08
I0513 00:23:16.714969 139674595104768 trainer.py:518] Training: step 32746
I0513 00:23:16.719433 140529434355712 trainer.py:518] Training: step 32744
I0513 00:23:16.718421 140311617681408 trainer.py:518] Training: step 32748
I0513 00:23:16.719649 140595318249472 trainer.py:518] Training: step 32747
I0513 00:23:52.470419 140095893518336 trainer.py:518] Training: step 32752
I0513 00:23:52.470797 140339465779200 trainer.py:518] Training: step 32752
I0513 00:23:52.470158 139674595104768 trainer.py:518] Training: step 32752
I0513 00:23:52.470152 140031916288000 trainer.py:518] Training: step 32752
I0513 00:23:52.470489 140440593786880 trainer.py:518] Training: step 32752
I0513 00:23:52.470752 140529434355712 trainer.py:518] Training: step 32752
I0513 00:23:52.470818 140311617681408 trainer.py:518] Training: step 32752
I0513 00:23:52.471186 140595318249472 trainer.py:518] Training: step 32752
I0513 00:24:10.360662 140095893518336 trainer.py:518] Training: step 32753
I0513 00:24:10.360882 140339465779200 trainer.py:518] Training: step 32753
I0513 00:24:10.360612 139674595104768 trainer.py:518] Training: step 32753
I0513 00:24:10.360285 140031916288000 trainer.py:518] Training: step 32753
I0513 00:24:10.360863 140440593786880 trainer.py:518] Training: step 32753
I0513 00:24:10.360738 140529434355712 trainer.py:518] Training: step 32753
I0513 00:24:10.361452 140311617681408 trainer.py:518] Training: step 32753
I0513 00:24:10.361751 140595318249472 trainer.py:518] Training: step 32753
I0513 00:24:28.250606 140095893518336 trainer.py:518] Training: step 32754
I0513 00:24:28.251249 140339465779200 trainer.py:518] Training: step 32754
I0513 00:24:28.250753 139674595104768 trainer.py:518] Training: step 32754
I0513 00:24:28.250679 140031916288000 trainer.py:518] Training: step 32754
I0513 00:24:28.251211 140440593786880 trainer.py:518] Training: step 32754
I0513 00:24:28.251303 140311617681408 trainer.py:518] Training: step 32754
I0513 00:24:28.251815 140595318249472 trainer.py:518] Training: step 32754
I0513 00:24:37.196621 140529434355712 trainer.py:518] Training: step 32754
I0513 00:24:46.142150 140095893518336 trainer.py:518] Training: step 32755
I0513 00:24:46.142914 140339465779200 trainer.py:518] Training: step 32755
I0513 00:24:46.142035 139674595104768 trainer.py:518] Training: step 32755
I0513 00:24:46.142071 140031916288000 trainer.py:518] Training: step 32755
I0513 00:24:46.142505 140440593786880 trainer.py:518] Training: step 32755
I0513 00:24:46.142571 140311617681408 trainer.py:518] Training: step 32755
I0513 00:24:46.143133 140595318249472 trainer.py:518] Training: step 32755
I0513 00:24:55.090535 140529434355712 trainer.py:518] Training: step 32755
I0513 00:25:04.033500 140095893518336 trainer.py:518] Training: step 32756
I0513 00:25:04.034812 140339465779200 trainer.py:518] Training: step 32756
I0513 00:25:04.033438 139674595104768 trainer.py:518] Training: step 32756
I0513 00:25:04.033606 140031916288000 trainer.py:518] Training: step 32756
I0513 00:25:04.033987 140440593786880 trainer.py:518] Training: step 32756
I0513 00:25:04.034462 140311617681408 trainer.py:518] Training: step 32756
I0513 00:25:04.034599 140595318249472 trainer.py:518] Training: step 32756
I0513 00:25:12.981229 140529434355712 trainer.py:518] Training: step 32756
I0513 00:25:21.923554 140095893518336 trainer.py:518] Training: step 32757
I0513 00:25:21.924484 140339465779200 trainer.py:518] Training: step 32757
I0513 00:25:21.923648 139674595104768 trainer.py:518] Training: step 32757
I0513 00:25:21.923799 140031916288000 trainer.py:518] Training: step 32757
I0513 00:25:21.924171 140440593786880 trainer.py:518] Training: step 32757
I0513 00:25:21.924317 140311617681408 trainer.py:518] Training: step 32757
I0513 00:25:21.924781 140595318249472 trainer.py:518] Training: step 32757
I0513 00:25:39.815243 140095893518336 trainer.py:518] Training: step 32758
I0513 00:25:39.815278 140339465779200 trainer.py:518] Training: step 32758
I0513 00:25:39.814976 139674595104768 trainer.py:518] Training: step 32758
I0513 00:25:39.814898 140031916288000 trainer.py:518] Training: step 32758
I0513 00:25:39.815486 140440593786880 trainer.py:518] Training: step 32758
I0513 00:25:39.815710 140529434355712 trainer.py:518] Training: step 32758
I0513 00:25:39.815621 140311617681408 trainer.py:518] Training: step 32758
I0513 00:25:39.815740 140595318249472 trainer.py:518] Training: step 32758
I0513 00:25:48.759513 140439986517568 logging_writer.py:48] [32740] collection=train accuracy=0.584973, cross_ent_loss=15.830729484558105, cross_ent_loss_per_all_target_tokens=2.01298e-05, experts/auxiliary_loss=0.1607544869184494, experts/expert_usage=7.952773571014404, experts/fraction_tokens_left_behind=0.2752477824687958, experts/router_confidence=3.4504172801971436, experts/router_z_loss=0.0003186266985721886, learning_rate=0.0055271, learning_rate/current=0.00552672, loss=15.999510765075684, loss_per_all_target_tokens=2.03444e-05, loss_per_nonpadding_target_token=2.04737e-05, non_padding_fraction/loss_weights=0.993686, timing/seconds=89.45120239257812, timing/seqs=3840, timing/seqs_per_second=42.92843246459961, timing/seqs_per_second_per_core=1.3415135145187378, timing/steps_per_second=0.11179279536008835, timing/target_tokens_per_second=87917.4296875, timing/target_tokens_per_second_per_core=2747.419677734375, timing/uptime=2433.71, z_loss=0.0077078677713871, z_loss_per_all_target_tokens=9.80106e-09
I0513 00:25:57.706296 140095893518336 trainer.py:518] Training: step 32759
I0513 00:25:57.706392 140339465779200 trainer.py:518] Training: step 32759
I0513 00:25:57.706097 139674595104768 trainer.py:518] Training: step 32759
I0513 00:25:57.706232 140440593786880 trainer.py:518] Training: step 32759
I0513 00:25:57.706345 140031916288000 trainer.py:518] Training: step 32759
I0513 00:25:57.706401 140311617681408 trainer.py:518] Training: step 32759
I0513 00:25:57.707269 140595318249472 trainer.py:518] Training: step 32759
I0513 00:26:06.652169 140529434355712 trainer.py:518] Training: step 32760
I0513 00:26:15.596998 140339465779200 trainer.py:518] Training: step 32765
I0513 00:26:15.597741 140439986517568 logging_writer.py:48] [32750] collection=train accuracy=0.578633, cross_ent_loss=16.114606857299805, cross_ent_loss_per_all_target_tokens=2.04908e-05, experts/auxiliary_loss=0.1607128232717514, experts/expert_usage=7.9664740562438965, experts/fraction_tokens_left_behind=0.25065878033638, experts/router_confidence=3.470268964767456, experts/router_z_loss=0.0003202151565346867, learning_rate=0.00552625, learning_rate/current=0.00552587, loss=16.28318977355957, loss_per_all_target_tokens=2.07051e-05, loss_per_nonpadding_target_token=2.08313e-05, non_padding_fraction/loss_weights=0.993943, timing/seconds=89.44734191894531, timing/seqs=3840, timing/seqs_per_second=42.9302864074707, timing/seqs_per_second_per_core=1.3415714502334595, timing/steps_per_second=0.11179761588573456, timing/target_tokens_per_second=87921.2265625, timing/target_tokens_per_second_per_core=2747.538330078125, timing/uptime=2452.51, z_loss=0.007549285423010588, z_loss_per_all_target_tokens=9.59941e-09
I0513 00:26:15.596932 140311617681408 trainer.py:518] Training: step 32769
I0513 00:26:15.596951 140595318249472 trainer.py:518] Training: step 32766
I0513 00:26:15.621861 140440593786880 trainer.py:518] Training: step 32767
I0513 00:26:15.624684 140095893518336 trainer.py:518] Training: step 32768
I0513 00:26:15.623279 139674595104768 trainer.py:518] Training: step 32767
I0513 00:26:15.622995 140031916288000 trainer.py:518] Training: step 32767
I0513 00:26:24.542470 140529434355712 trainer.py:518] Training: step 32770
I0513 00:26:51.377376 140095893518336 trainer.py:518] Training: step 32772
I0513 00:26:51.378472 140339465779200 trainer.py:518] Training: step 32772
I0513 00:26:51.377185 139674595104768 trainer.py:518] Training: step 32772
I0513 00:26:51.377601 140031916288000 trainer.py:518] Training: step 32772
I0513 00:26:51.378242 140440593786880 trainer.py:518] Training: step 32772
I0513 00:26:51.378254 140529434355712 trainer.py:518] Training: step 32772
I0513 00:26:51.377831 140311617681408 trainer.py:518] Training: step 32772
I0513 00:26:51.379261 140595318249472 trainer.py:518] Training: step 32772
I0513 00:27:09.267460 140095893518336 trainer.py:518] Training: step 32773
I0513 00:27:09.267940 140339465779200 trainer.py:518] Training: step 32773
I0513 00:27:09.267243 139674595104768 trainer.py:518] Training: step 32773
I0513 00:27:09.267488 140031916288000 trainer.py:518] Training: step 32773
I0513 00:27:09.268124 140529434355712 trainer.py:518] Training: step 32773
I0513 00:27:09.267736 140311617681408 trainer.py:518] Training: step 32773
I0513 00:27:09.268611 140595318249472 trainer.py:518] Training: step 32773
I0513 00:27:18.213830 140440593786880 trainer.py:518] Training: step 32773
I0513 00:27:27.159026 140095893518336 trainer.py:518] Training: step 32774
I0513 00:27:27.159574 140339465779200 trainer.py:518] Training: step 32774
I0513 00:27:27.159094 139674595104768 trainer.py:518] Training: step 32774
I0513 00:27:27.159179 140031916288000 trainer.py:518] Training: step 32774
I0513 00:27:27.159142 140529434355712 trainer.py:518] Training: step 32774
I0513 00:27:27.159419 140311617681408 trainer.py:518] Training: step 32774
I0513 00:27:27.161088 140595318249472 trainer.py:518] Training: step 32774
I0513 00:27:45.051294 140095893518336 trainer.py:518] Training: step 32775
I0513 00:27:45.051428 140339465779200 trainer.py:518] Training: step 32775
I0513 00:27:45.050871 139674595104768 trainer.py:518] Training: step 32775
I0513 00:27:45.050977 140031916288000 trainer.py:518] Training: step 32775
I0513 00:27:45.051407 140440593786880 trainer.py:518] Training: step 32774
I0513 00:27:45.051215 140529434355712 trainer.py:518] Training: step 32775
I0513 00:27:45.051893 140311617681408 trainer.py:518] Training: step 32775
I0513 00:27:45.052066 140595318249472 trainer.py:518] Training: step 32775
I0513 00:28:02.942209 140095893518336 trainer.py:518] Training: step 32776
I0513 00:28:02.942906 140339465779200 trainer.py:518] Training: step 32776
I0513 00:28:02.942359 139674595104768 trainer.py:518] Training: step 32776
I0513 00:28:02.942198 140031916288000 trainer.py:518] Training: step 32776
I0513 00:28:02.942740 140440593786880 trainer.py:518] Training: step 32775
I0513 00:28:02.942321 140529434355712 trainer.py:518] Training: step 32776
I0513 00:28:02.942696 140311617681408 trainer.py:518] Training: step 32776
I0513 00:28:02.943179 140595318249472 trainer.py:518] Training: step 32776
I0513 00:28:20.832531 140095893518336 trainer.py:518] Training: step 32777
I0513 00:28:20.833338 140339465779200 trainer.py:518] Training: step 32777
I0513 00:28:20.832794 139674595104768 trainer.py:518] Training: step 32777
I0513 00:28:20.832556 140031916288000 trainer.py:518] Training: step 32777
I0513 00:28:20.833067 140440593786880 trainer.py:518] Training: step 32776
I0513 00:28:20.833087 140529434355712 trainer.py:518] Training: step 32777
I0513 00:28:20.833441 140311617681408 trainer.py:518] Training: step 32777
I0513 00:28:20.833476 140595318249472 trainer.py:518] Training: step 32777
I0513 00:28:38.723489 140095893518336 trainer.py:518] Training: step 32778
I0513 00:28:38.723892 140339465779200 trainer.py:518] Training: step 32778
I0513 00:28:38.723368 139674595104768 trainer.py:518] Training: step 32778
I0513 00:28:38.723385 140031916288000 trainer.py:518] Training: step 32778
I0513 00:28:38.724071 140440593786880 trainer.py:518] Training: step 32777
I0513 00:28:38.724211 140529434355712 trainer.py:518] Training: step 32778
I0513 00:28:38.724031 140311617681408 trainer.py:518] Training: step 32778
I0513 00:28:38.724473 140595318249472 trainer.py:518] Training: step 32778
I0513 00:28:47.670862 140439986517568 logging_writer.py:48] [32760] collection=train accuracy=0.558726, cross_ent_loss=16.93011474609375, cross_ent_loss_per_all_target_tokens=2.15278e-05, experts/auxiliary_loss=0.16069835424423218, experts/expert_usage=7.97830057144165, experts/fraction_tokens_left_behind=0.22394242882728577, experts/router_confidence=3.472278594970703, experts/router_z_loss=0.0003148488176520914, learning_rate=0.00552541, learning_rate/current=0.00552503, loss=17.099149703979492, loss_per_all_target_tokens=2.17427e-05, loss_per_nonpadding_target_token=2.18778e-05, non_padding_fraction/loss_weights=0.993823, timing/seconds=89.45220184326172, timing/seqs=3840, timing/seqs_per_second=42.927955627441406, timing/seqs_per_second_per_core=1.341498613357544, timing/steps_per_second=0.11179155111312866, timing/target_tokens_per_second=87916.453125, timing/target_tokens_per_second_per_core=2747.38916015625, timing/uptime=2612.6, z_loss=0.008021420799195766, z_loss_per_all_target_tokens=1.01998e-08
I0513 00:28:56.614162 140095893518336 trainer.py:518] Training: step 32779
I0513 00:28:56.614549 140339465779200 trainer.py:518] Training: step 32779
I0513 00:28:56.613990 139674595104768 trainer.py:518] Training: step 32779
I0513 00:28:56.613963 140031916288000 trainer.py:518] Training: step 32779
I0513 00:28:56.614299 140440593786880 trainer.py:518] Training: step 32782
I0513 00:28:56.615633 140529434355712 trainer.py:518] Training: step 32779
I0513 00:28:56.614642 140311617681408 trainer.py:518] Training: step 32779
I0513 00:28:56.615450 140595318249472 trainer.py:518] Training: step 32779
I0513 00:29:14.505352 140339465779200 trainer.py:518] Training: step 32786
I0513 00:29:14.506131 140439986517568 logging_writer.py:48] [32770] collection=train accuracy=0.56907, cross_ent_loss=16.494110107421875, cross_ent_loss_per_all_target_tokens=2.09733e-05, experts/auxiliary_loss=0.16069908440113068, experts/expert_usage=7.971288204193115, experts/fraction_tokens_left_behind=0.24379535019397736, experts/router_confidence=3.4823899269104004, experts/router_z_loss=0.00030839836108498275, learning_rate=0.00552457, learning_rate/current=0.00552419, loss=16.662883758544922, loss_per_all_target_tokens=2.1188e-05, loss_per_nonpadding_target_token=2.12719e-05, non_padding_fraction/loss_weights=0.996055, timing/seconds=89.44706726074219, timing/seqs=3840, timing/seqs_per_second=42.930419921875, timing/seqs_per_second_per_core=1.3415756225585938, timing/steps_per_second=0.11179796606302261, timing/target_tokens_per_second=87921.5, timing/target_tokens_per_second_per_core=2747.546875, timing/uptime=2630.53, z_loss=0.0077628763392567635, z_loss_per_all_target_tokens=9.87101e-09
I0513 00:29:14.505987 140595318249472 trainer.py:518] Training: step 32787
I0513 00:29:14.529181 139674595104768 trainer.py:518] Training: step 32787
I0513 00:29:14.532387 140031916288000 trainer.py:518] Training: step 32788
I0513 00:29:14.535643 140529434355712 trainer.py:518] Training: step 32784
I0513 00:29:14.534552 140311617681408 trainer.py:518] Training: step 32788
I0513 00:29:23.449737 140095893518336 trainer.py:518] Training: step 32790
I0513 00:29:23.450084 140440593786880 trainer.py:518] Training: step 32790
I0513 00:29:50.286661 140095893518336 trainer.py:518] Training: step 32792
I0513 00:29:50.286778 140339465779200 trainer.py:518] Training: step 32792
I0513 00:29:50.286506 139674595104768 trainer.py:518] Training: step 32792
I0513 00:29:50.286510 140440593786880 trainer.py:518] Training: step 32792
I0513 00:29:50.286578 140031916288000 trainer.py:518] Training: step 32792
I0513 00:29:50.286905 140529434355712 trainer.py:518] Training: step 32792
I0513 00:29:50.286702 140311617681408 trainer.py:518] Training: step 32792
I0513 00:29:50.287603 140595318249472 trainer.py:518] Training: step 32792
I0513 00:30:08.177311 140095893518336 trainer.py:518] Training: step 32793
I0513 00:30:08.177620 140339465779200 trainer.py:518] Training: step 32793
I0513 00:30:08.177297 139674595104768 trainer.py:518] Training: step 32793
I0513 00:30:08.177327 140440593786880 trainer.py:518] Training: step 32793
I0513 00:30:08.177282 140031916288000 trainer.py:518] Training: step 32793
I0513 00:30:08.178480 140595318249472 trainer.py:518] Training: step 32793
I0513 00:30:17.123100 140529434355712 trainer.py:518] Training: step 32793
I0513 00:30:17.125956 140311617681408 trainer.py:518] Training: step 32793
I0513 00:30:26.069344 140095893518336 trainer.py:518] Training: step 32794
I0513 00:30:26.069839 140339465779200 trainer.py:518] Training: step 32794
I0513 00:30:26.069926 139674595104768 trainer.py:518] Training: step 32794
I0513 00:30:26.069557 140031916288000 trainer.py:518] Training: step 32794
I0513 00:30:26.069774 140440593786880 trainer.py:518] Training: step 32794
I0513 00:30:26.070344 140595318249472 trainer.py:518] Training: step 32794
I0513 00:30:35.015278 140529434355712 trainer.py:518] Training: step 32794
I0513 00:30:35.015237 140311617681408 trainer.py:518] Training: step 32794
I0513 00:30:43.959955 140095893518336 trainer.py:518] Training: step 32795
I0513 00:30:43.960311 139674595104768 trainer.py:518] Training: step 32795
I0513 00:30:43.959979 140440593786880 trainer.py:518] Training: step 32795
I0513 00:30:43.959971 140031916288000 trainer.py:518] Training: step 32795
I0513 00:30:43.961021 140595318249472 trainer.py:518] Training: step 32795
I0513 00:30:52.904744 140339465779200 trainer.py:518] Training: step 32795
I0513 00:30:52.908924 140529434355712 trainer.py:518] Training: step 32795
I0513 00:30:52.908771 140311617681408 trainer.py:518] Training: step 32795
I0513 00:31:01.851162 140095893518336 trainer.py:518] Training: step 32796
I0513 00:31:01.850972 139674595104768 trainer.py:518] Training: step 32796
I0513 00:31:01.851257 140031916288000 trainer.py:518] Training: step 32796
I0513 00:31:01.851303 140440593786880 trainer.py:518] Training: step 32796
I0513 00:31:01.851806 140595318249472 trainer.py:518] Training: step 32796
I0513 00:31:10.796820 140339465779200 trainer.py:518] Training: step 32796
I0513 00:31:10.796510 140311617681408 trainer.py:518] Training: step 32796
I0513 00:31:10.798572 140529434355712 trainer.py:518] Training: step 32796
I0513 00:31:19.740639 140095893518336 trainer.py:518] Training: step 32797
I0513 00:31:19.741092 139674595104768 trainer.py:518] Training: step 32797
I0513 00:31:19.740687 140440593786880 trainer.py:518] Training: step 32797
I0513 00:31:19.740938 140031916288000 trainer.py:518] Training: step 32797
I0513 00:31:19.741815 140595318249472 trainer.py:518] Training: step 32797
I0513 00:31:37.633069 140339465779200 trainer.py:518] Training: step 32798
I0513 00:31:37.633646 140095893518336 trainer.py:518] Training: step 32798
I0513 00:31:37.632505 139674595104768 trainer.py:518] Training: step 32798
I0513 00:31:37.632448 140440593786880 trainer.py:518] Training: step 32798
I0513 00:31:37.632423 140031916288000 trainer.py:518] Training: step 32798
I0513 00:31:37.632776 140529434355712 trainer.py:518] Training: step 32798
I0513 00:31:37.632357 140311617681408 trainer.py:518] Training: step 32798
I0513 00:31:37.633221 140595318249472 trainer.py:518] Training: step 32798
I0513 00:31:46.578097 140439986517568 logging_writer.py:48] [32780] collection=train accuracy=0.564085, cross_ent_loss=16.771455764770508, cross_ent_loss_per_all_target_tokens=2.1326e-05, experts/auxiliary_loss=0.1607079803943634, experts/expert_usage=7.958049774169922, experts/fraction_tokens_left_behind=0.25434455275535583, experts/router_confidence=3.4713661670684814, experts/router_z_loss=0.0003132630663458258, learning_rate=0.00552372, learning_rate/current=0.00552334, loss=16.939931869506836, loss_per_all_target_tokens=2.15402e-05, loss_per_nonpadding_target_token=2.16764e-05, non_padding_fraction/loss_weights=0.993718, timing/seconds=89.45146942138672, timing/seqs=3840, timing/seqs_per_second=42.928306579589844, timing/seqs_per_second_per_core=1.3415095806121826, timing/steps_per_second=0.11179246753454208, timing/target_tokens_per_second=87917.171875, timing/target_tokens_per_second_per_core=2747.41162109375, timing/uptime=2791.52, z_loss=0.00745320925489068, z_loss_per_all_target_tokens=9.47725e-09
I0513 00:31:55.524408 140339465779200 trainer.py:518] Training: step 32799
I0513 00:31:55.523864 139674595104768 trainer.py:518] Training: step 32799
I0513 00:31:55.525640 140095893518336 trainer.py:518] Training: step 32799
I0513 00:31:55.523688 140440593786880 trainer.py:518] Training: step 32799
I0513 00:31:55.523911 140031916288000 trainer.py:518] Training: step 32799
I0513 00:31:55.524826 140529434355712 trainer.py:518] Training: step 32799
I0513 00:31:55.524210 140311617681408 trainer.py:518] Training: step 32799
I0513 00:31:55.524894 140595318249472 trainer.py:518] Training: step 32799
I0513 00:32:13.414619 140095893518336 trainer.py:518] Training: step 32808
I0513 00:32:13.413573 140440593786880 trainer.py:518] Training: step 32807
I0513 00:32:13.413777 140031916288000 trainer.py:518] Training: step 32808
I0513 00:32:13.414951 139674595104768 trainer.py:518] Training: step 32806
I0513 00:32:13.413243 140439986517568 logging_writer.py:48] [32790] collection=train accuracy=0.567847, cross_ent_loss=16.56520652770996, cross_ent_loss_per_all_target_tokens=2.10637e-05, experts/auxiliary_loss=0.1606988161802292, experts/expert_usage=7.971167087554932, experts/fraction_tokens_left_behind=0.2475612461566925, experts/router_confidence=3.4669697284698486, experts/router_z_loss=0.00030826241709291935, learning_rate=0.00552288, learning_rate/current=0.0055225, loss=16.733470916748047, loss_per_all_target_tokens=2.12777e-05, loss_per_nonpadding_target_token=2.13933e-05, non_padding_fraction/loss_weights=0.994599, timing/seconds=89.44783782958984, timing/seqs=3840, timing/seqs_per_second=42.93004608154297, timing/seqs_per_second_per_core=1.3415639400482178, timing/steps_per_second=0.11179699748754501, timing/target_tokens_per_second=87920.734375, timing/target_tokens_per_second_per_core=2747.52294921875, timing/uptime=2810.7, z_loss=0.0072575779631733894, z_loss_per_all_target_tokens=9.22849e-09
I0513 00:32:13.414503 140595318249472 trainer.py:518] Training: step 32808
I0513 00:32:22.362183 140339465779200 trainer.py:518] Training: step 32810
I0513 00:32:22.358599 140529434355712 trainer.py:518] Training: step 32810
I0513 00:32:22.358529 140311617681408 trainer.py:518] Training: step 32810
I0513 00:32:49.194656 140095893518336 trainer.py:518] Training: step 32812
I0513 00:32:49.194661 140339465779200 trainer.py:518] Training: step 32812
I0513 00:32:49.194074 139674595104768 trainer.py:518] Training: step 32812
I0513 00:32:49.193766 140440593786880 trainer.py:518] Training: step 32812
I0513 00:32:49.194294 140031916288000 trainer.py:518] Training: step 32812
I0513 00:32:49.194800 140529434355712 trainer.py:518] Training: step 32812
I0513 00:32:49.193919 140311617681408 trainer.py:518] Training: step 32812
I0513 00:32:49.194781 140595318249472 trainer.py:518] Training: step 32812
I0513 00:33:07.085790 140095893518336 trainer.py:518] Training: step 32813
I0513 00:33:07.086065 140339465779200 trainer.py:518] Training: step 32813
I0513 00:33:07.085772 139674595104768 trainer.py:518] Training: step 32813
I0513 00:33:07.085346 140440593786880 trainer.py:518] Training: step 32813
I0513 00:33:07.085527 140031916288000 trainer.py:518] Training: step 32813
I0513 00:33:07.086172 140529434355712 trainer.py:518] Training: step 32813
I0513 00:33:07.085395 140311617681408 trainer.py:518] Training: step 32813
I0513 00:33:07.085943 140595318249472 trainer.py:518] Training: step 32813
I0513 00:33:24.975234 140339465779200 trainer.py:518] Training: step 32814
I0513 00:33:24.975503 140095893518336 trainer.py:518] Training: step 32814
I0513 00:33:24.975348 139674595104768 trainer.py:518] Training: step 32814
I0513 00:33:24.974813 140440593786880 trainer.py:518] Training: step 32814
I0513 00:33:24.975070 140529434355712 trainer.py:518] Training: step 32814
I0513 00:33:24.975253 140311617681408 trainer.py:518] Training: step 32814
I0513 00:33:24.975178 140595318249472 trainer.py:518] Training: step 32814
I0513 00:33:33.919439 140031916288000 trainer.py:518] Training: step 32814
I0513 00:33:42.865064 140339465779200 trainer.py:518] Training: step 32815
I0513 00:33:42.865665 140095893518336 trainer.py:518] Training: step 32815
I0513 00:33:42.864923 139674595104768 trainer.py:518] Training: step 32815
I0513 00:33:42.865033 140440593786880 trainer.py:518] Training: step 32815
I0513 00:33:42.865081 140529434355712 trainer.py:518] Training: step 32815
I0513 00:33:42.864604 140311617681408 trainer.py:518] Training: step 32815
I0513 00:33:42.865280 140595318249472 trainer.py:518] Training: step 32815
I0513 00:33:51.812986 140031916288000 trainer.py:518] Training: step 32815
I0513 00:34:00.756274 140339465779200 trainer.py:518] Training: step 32816
I0513 00:34:00.756683 140095893518336 trainer.py:518] Training: step 32816
I0513 00:34:00.756083 139674595104768 trainer.py:518] Training: step 32816
I0513 00:34:00.755876 140440593786880 trainer.py:518] Training: step 32816
I0513 00:34:00.756322 140529434355712 trainer.py:518] Training: step 32816
I0513 00:34:00.756025 140311617681408 trainer.py:518] Training: step 32816
I0513 00:34:00.756212 140595318249472 trainer.py:518] Training: step 32816
I0513 00:34:09.702709 140031916288000 trainer.py:518] Training: step 32816
I0513 00:34:18.646750 140339465779200 trainer.py:518] Training: step 32817
I0513 00:34:18.647161 140095893518336 trainer.py:518] Training: step 32817
I0513 00:34:18.646819 139674595104768 trainer.py:518] Training: step 32817
I0513 00:34:18.646419 140440593786880 trainer.py:518] Training: step 32817
I0513 00:34:18.646782 140529434355712 trainer.py:518] Training: step 32817
I0513 00:34:18.646407 140311617681408 trainer.py:518] Training: step 32817
I0513 00:34:18.647206 140595318249472 trainer.py:518] Training: step 32817
I0513 00:34:36.538189 140095893518336 trainer.py:518] Training: step 32818
I0513 00:34:36.538061 140339465779200 trainer.py:518] Training: step 32818
I0513 00:34:36.537965 139674595104768 trainer.py:518] Training: step 32818
I0513 00:34:36.538079 140440593786880 trainer.py:518] Training: step 32818
I0513 00:34:36.538037 140031916288000 trainer.py:518] Training: step 32818
I0513 00:34:36.538314 140529434355712 trainer.py:518] Training: step 32818
I0513 00:34:36.538262 140311617681408 trainer.py:518] Training: step 32818
I0513 00:34:36.538320 140595318249472 trainer.py:518] Training: step 32818
I0513 00:34:45.485276 140439986517568 logging_writer.py:48] [32800] collection=train accuracy=0.578946, cross_ent_loss=16.116071701049805, cross_ent_loss_per_all_target_tokens=2.04926e-05, experts/auxiliary_loss=0.16071219742298126, experts/expert_usage=7.966378688812256, experts/fraction_tokens_left_behind=0.2553020417690277, experts/router_confidence=3.4642770290374756, experts/router_z_loss=0.0003050442901439965, learning_rate=0.00552204, learning_rate/current=0.00552166, loss=16.28498077392578, loss_per_all_target_tokens=2.07074e-05, loss_per_nonpadding_target_token=2.07765e-05, non_padding_fraction/loss_weights=0.996674, timing/seconds=89.45104217529297, timing/seqs=3840, timing/seqs_per_second=42.92851257324219, timing/seqs_per_second_per_core=1.3415160179138184, timing/steps_per_second=0.11179299652576447, timing/target_tokens_per_second=87917.59375, timing/target_tokens_per_second_per_core=2747.4248046875, timing/uptime=2970.43, z_loss=0.00789189338684082, z_loss_per_all_target_tokens=1.00351e-08
I0513 00:34:54.429662 140095893518336 trainer.py:518] Training: step 32819
I0513 00:34:54.430154 140339465779200 trainer.py:518] Training: step 32819
I0513 00:34:54.429376 139674595104768 trainer.py:518] Training: step 32819
I0513 00:34:54.429106 140440593786880 trainer.py:518] Training: step 32819
I0513 00:34:54.429020 140031916288000 trainer.py:518] Training: step 32819
I0513 00:34:54.430428 140529434355712 trainer.py:518] Training: step 32819
I0513 00:34:54.429036 140311617681408 trainer.py:518] Training: step 32819
I0513 00:34:54.430559 140595318249472 trainer.py:518] Training: step 32819
I0513 00:35:12.318985 139674595104768 trainer.py:518] Training: step 32826
I0513 00:35:12.319261 140440593786880 trainer.py:518] Training: step 32827
I0513 00:35:12.319574 140529434355712 trainer.py:518] Training: step 32824
I0513 00:35:12.320798 140439986517568 logging_writer.py:48] [32810] collection=train accuracy=0.576869, cross_ent_loss=16.183834075927734, cross_ent_loss_per_all_target_tokens=2.05788e-05, experts/auxiliary_loss=0.1607138216495514, experts/expert_usage=7.966668128967285, experts/fraction_tokens_left_behind=0.24929669499397278, experts/router_confidence=3.4629600048065186, experts/router_z_loss=0.00030985623016022146, learning_rate=0.0055212, learning_rate/current=0.00552082, loss=16.352590560913086, loss_per_all_target_tokens=2.07934e-05, loss_per_nonpadding_target_token=2.08977e-05, non_padding_fraction/loss_weights=0.995008, timing/seconds=89.44415283203125, timing/seqs=3840, timing/seqs_per_second=42.93181610107422, timing/seqs_per_second_per_core=1.3416192531585693, timing/steps_per_second=0.11180160939693451, timing/target_tokens_per_second=87924.359375, timing/target_tokens_per_second_per_core=2747.63623046875, timing/uptime=2988.29, z_loss=0.007733874022960663, z_loss_per_all_target_tokens=9.83413e-09
I0513 00:35:12.347115 140339465779200 trainer.py:518] Training: step 32826
I0513 00:35:12.347614 140095893518336 trainer.py:518] Training: step 32829
I0513 00:35:12.347088 140311617681408 trainer.py:518] Training: step 32828
I0513 00:35:12.347975 140595318249472 trainer.py:518] Training: step 32827
I0513 00:35:21.281189 140031916288000 trainer.py:518] Training: step 32830
I0513 00:35:48.101079 140339465779200 trainer.py:518] Training: step 32832
I0513 00:35:48.101433 140095893518336 trainer.py:518] Training: step 32832
I0513 00:35:48.100645 139674595104768 trainer.py:518] Training: step 32832
I0513 00:35:48.100334 140440593786880 trainer.py:518] Training: step 32832
I0513 00:35:48.100810 140031916288000 trainer.py:518] Training: step 32832
I0513 00:35:48.100915 140529434355712 trainer.py:518] Training: step 32832
I0513 00:35:48.100678 140311617681408 trainer.py:518] Training: step 32832
I0513 00:35:48.101573 140595318249472 trainer.py:518] Training: step 32832
I0513 00:36:05.991667 140339465779200 trainer.py:518] Training: step 32833
I0513 00:36:05.991137 139674595104768 trainer.py:518] Training: step 32833
I0513 00:36:05.990800 140440593786880 trainer.py:518] Training: step 32833
I0513 00:36:05.990946 140031916288000 trainer.py:518] Training: step 32833
I0513 00:36:05.994065 140095893518336 trainer.py:518] Training: step 32833
I0513 00:36:05.991432 140529434355712 trainer.py:518] Training: step 32833
I0513 00:36:05.991157 140311617681408 trainer.py:518] Training: step 32833
I0513 00:36:05.991419 140595318249472 trainer.py:518] Training: step 32833
I0513 00:36:23.881510 140339465779200 trainer.py:518] Training: step 32834
I0513 00:36:23.881940 140095893518336 trainer.py:518] Training: step 32834
I0513 00:36:23.881772 139674595104768 trainer.py:518] Training: step 32834
I0513 00:36:23.881430 140440593786880 trainer.py:518] Training: step 32834
I0513 00:36:23.881432 140529434355712 trainer.py:518] Training: step 32834
I0513 00:36:23.881318 140311617681408 trainer.py:518] Training: step 32834
I0513 00:36:23.882032 140595318249472 trainer.py:518] Training: step 32834
I0513 00:36:32.826453 140031916288000 trainer.py:518] Training: step 32834
I0513 00:36:41.772127 140095893518336 trainer.py:518] Training: step 32835
I0513 00:36:41.771908 140339465779200 trainer.py:518] Training: step 32835
I0513 00:36:41.771715 139674595104768 trainer.py:518] Training: step 32835
I0513 00:36:41.772446 140529434355712 trainer.py:518] Training: step 32835
I0513 00:36:41.771482 140311617681408 trainer.py:518] Training: step 32835
I0513 00:36:41.771854 140595318249472 trainer.py:518] Training: step 32835
I0513 00:36:50.720411 140031916288000 trainer.py:518] Training: step 32835
I0513 00:36:50.720842 140440593786880 trainer.py:518] Training: step 32835
I0513 00:36:59.663531 140095893518336 trainer.py:518] Training: step 32836
I0513 00:36:59.663827 140339465779200 trainer.py:518] Training: step 32836
I0513 00:36:59.663314 139674595104768 trainer.py:518] Training: step 32836
I0513 00:36:59.663650 140529434355712 trainer.py:518] Training: step 32836
I0513 00:36:59.663658 140311617681408 trainer.py:518] Training: step 32836
I0513 00:36:59.663947 140595318249472 trainer.py:518] Training: step 32836
I0513 00:37:08.608882 140440593786880 trainer.py:518] Training: step 32836
I0513 00:37:08.609197 140031916288000 trainer.py:518] Training: step 32836
I0513 00:37:17.553969 140095893518336 trainer.py:518] Training: step 32837
I0513 00:37:17.553868 140339465779200 trainer.py:518] Training: step 32837
I0513 00:37:17.553580 139674595104768 trainer.py:518] Training: step 32837
I0513 00:37:17.553772 140529434355712 trainer.py:518] Training: step 32837
I0513 00:37:17.553936 140311617681408 trainer.py:518] Training: step 32837
I0513 00:37:17.553954 140595318249472 trainer.py:518] Training: step 32837
I0513 00:37:35.443709 140339465779200 trainer.py:518] Training: step 32838
I0513 00:37:35.444363 140095893518336 trainer.py:518] Training: step 32838
I0513 00:37:35.443688 139674595104768 trainer.py:518] Training: step 32838
I0513 00:37:35.443416 140440593786880 trainer.py:518] Training: step 32838
I0513 00:37:35.443656 140031916288000 trainer.py:518] Training: step 32838
I0513 00:37:35.443819 140529434355712 trainer.py:518] Training: step 32838
I0513 00:37:35.443991 140311617681408 trainer.py:518] Training: step 32838
I0513 00:37:35.444049 140595318249472 trainer.py:518] Training: step 32838
I0513 00:37:44.390676 140439986517568 logging_writer.py:48] [32820] collection=train accuracy=0.568518, cross_ent_loss=16.5945987701416, cross_ent_loss_per_all_target_tokens=2.11011e-05, experts/auxiliary_loss=0.1606910079717636, experts/expert_usage=7.972689151763916, experts/fraction_tokens_left_behind=0.24870257079601288, experts/router_confidence=3.476809024810791, experts/router_z_loss=0.0003168000839650631, learning_rate=0.00552036, learning_rate/current=0.00551998, loss=16.763294219970703, loss_per_all_target_tokens=2.13156e-05, loss_per_nonpadding_target_token=2.14342e-05, non_padding_fraction/loss_weights=0.99447, timing/seconds=89.45130920410156, timing/seqs=3840, timing/seqs_per_second=42.928382873535156, timing/seqs_per_second_per_core=1.3415119647979736, timing/steps_per_second=0.1117926687002182, timing/target_tokens_per_second=87917.328125, timing/target_tokens_per_second_per_core=2747.41650390625, timing/uptime=3149.34, z_loss=0.007686443626880646, z_loss_per_all_target_tokens=9.77382e-09
I0513 00:37:53.335369 140339465779200 trainer.py:518] Training: step 32839
I0513 00:37:53.335124 139674595104768 trainer.py:518] Training: step 32839
I0513 00:37:53.336070 140529434355712 trainer.py:518] Training: step 32839
I0513 00:37:53.335025 140440593786880 trainer.py:518] Training: step 32839
I0513 00:37:53.335063 140031916288000 trainer.py:518] Training: step 32839
I0513 00:37:53.336079 140095893518336 trainer.py:518] Training: step 32839
I0513 00:37:53.336123 140595318249472 trainer.py:518] Training: step 32839
I0513 00:37:53.335785 140311617681408 trainer.py:518] Training: step 32839
I0513 00:38:11.225010 140339465779200 trainer.py:518] Training: step 32846
I0513 00:38:11.225032 140529434355712 trainer.py:518] Training: step 32844
I0513 00:38:11.225225 140595318249472 trainer.py:518] Training: step 32846
I0513 00:38:11.227989 140439986517568 logging_writer.py:48] [32830] collection=train accuracy=0.572508, cross_ent_loss=16.440759658813477, cross_ent_loss_per_all_target_tokens=2.09055e-05, experts/auxiliary_loss=0.16067883372306824, experts/expert_usage=7.982858180999756, experts/fraction_tokens_left_behind=0.25405797362327576, experts/router_confidence=3.4727089405059814, experts/router_z_loss=0.0003207025583833456, learning_rate=0.00551952, learning_rate/current=0.00551914, loss=16.60981559753418, loss_per_all_target_tokens=2.11205e-05, loss_per_nonpadding_target_token=2.12676e-05, non_padding_fraction/loss_weights=0.993081, timing/seconds=89.44583129882812, timing/seqs=3840, timing/seqs_per_second=42.93101119995117, timing/seqs_per_second_per_core=1.3415940999984741, timing/steps_per_second=0.11179950833320618, timing/target_tokens_per_second=87922.7109375, timing/target_tokens_per_second_per_core=2747.584716796875, timing/uptime=3167.41, z_loss=0.00805349089205265, z_loss_per_all_target_tokens=1.02405e-08
I0513 00:38:11.251385 139674595104768 trainer.py:518] Training: step 32846
I0513 00:38:11.256809 140095893518336 trainer.py:518] Training: step 32849
I0513 00:38:11.253965 140311617681408 trainer.py:518] Training: step 32848
I0513 00:38:20.170356 140440593786880 trainer.py:518] Training: step 32850
I0513 00:38:20.185904 140031916288000 trainer.py:518] Training: step 32850
I0513 00:38:47.007555 140095893518336 trainer.py:518] Training: step 32852
I0513 00:38:47.007400 140339465779200 trainer.py:518] Training: step 32852
I0513 00:38:47.007395 139674595104768 trainer.py:518] Training: step 32852
I0513 00:38:47.006774 140440593786880 trainer.py:518] Training: step 32852
I0513 00:38:47.007465 140595318249472 trainer.py:518] Training: step 32852
I0513 00:38:47.007348 140529434355712 trainer.py:518] Training: step 32852
I0513 00:38:47.006834 140031916288000 trainer.py:518] Training: step 32852
I0513 00:38:47.007497 140311617681408 trainer.py:518] Training: step 32852
I0513 00:39:04.897368 140339465779200 trainer.py:518] Training: step 32853
I0513 00:39:04.897222 140095893518336 trainer.py:518] Training: step 32853
I0513 00:39:04.896851 140440593786880 trainer.py:518] Training: step 32853
I0513 00:39:04.896748 140031916288000 trainer.py:518] Training: step 32853
I0513 00:39:04.897423 140529434355712 trainer.py:518] Training: step 32853
I0513 00:39:04.897322 139674595104768 trainer.py:518] Training: step 32853
I0513 00:39:04.897239 140311617681408 trainer.py:518] Training: step 32853
I0513 00:39:04.897373 140595318249472 trainer.py:518] Training: step 32853
I0513 00:39:22.788736 140095893518336 trainer.py:518] Training: step 32854
I0513 00:39:22.788609 140031916288000 trainer.py:518] Training: step 32854
I0513 00:39:22.788746 140529434355712 trainer.py:518] Training: step 32854
I0513 00:39:22.788700 139674595104768 trainer.py:518] Training: step 32854
I0513 00:39:22.789007 140311617681408 trainer.py:518] Training: step 32854
I0513 00:39:22.789098 140595318249472 trainer.py:518] Training: step 32854
I0513 00:39:31.734715 140339465779200 trainer.py:518] Training: step 32854
I0513 00:39:31.734024 140440593786880 trainer.py:518] Training: step 32854
I0513 00:39:40.680068 140095893518336 trainer.py:518] Training: step 32855
I0513 00:39:40.679542 140031916288000 trainer.py:518] Training: step 32855
I0513 00:39:40.679552 140529434355712 trainer.py:518] Training: step 32855
I0513 00:39:40.679751 139674595104768 trainer.py:518] Training: step 32855
I0513 00:39:40.679853 140311617681408 trainer.py:518] Training: step 32855
I0513 00:39:40.679609 140595318249472 trainer.py:518] Training: step 32855
I0513 00:39:49.627668 140440593786880 trainer.py:518] Training: step 32855
I0513 00:39:58.571414 140339465779200 trainer.py:518] Training: step 32855
I0513 00:39:58.571553 140095893518336 trainer.py:518] Training: step 32856
I0513 00:39:58.570836 140031916288000 trainer.py:518] Training: step 32856
I0513 00:39:58.571241 140529434355712 trainer.py:518] Training: step 32856
I0513 00:39:58.571422 139674595104768 trainer.py:518] Training: step 32856
I0513 00:39:58.571603 140311617681408 trainer.py:518] Training: step 32856
I0513 00:39:58.571331 140595318249472 trainer.py:518] Training: step 32856
I0513 00:40:07.515912 140440593786880 trainer.py:518] Training: step 32856
I0513 00:40:16.461765 140339465779200 trainer.py:518] Training: step 32856
I0513 00:40:16.461751 140095893518336 trainer.py:518] Training: step 32857
I0513 00:40:16.461225 140031916288000 trainer.py:518] Training: step 32857
I0513 00:40:16.462361 140529434355712 trainer.py:518] Training: step 32857
I0513 00:40:16.462006 139674595104768 trainer.py:518] Training: step 32857
I0513 00:40:16.461863 140311617681408 trainer.py:518] Training: step 32857
I0513 00:40:16.462173 140595318249472 trainer.py:518] Training: step 32857
I0513 00:40:34.352138 140339465779200 trainer.py:518] Training: step 32857
I0513 00:40:34.352239 140095893518336 trainer.py:518] Training: step 32858
I0513 00:40:34.351666 140031916288000 trainer.py:518] Training: step 32858
I0513 00:40:34.352185 140440593786880 trainer.py:518] Training: step 32858
I0513 00:40:34.352058 140529434355712 trainer.py:518] Training: step 32858
I0513 00:40:34.352055 139674595104768 trainer.py:518] Training: step 32858
I0513 00:40:34.352604 140311617681408 trainer.py:518] Training: step 32858
I0513 00:40:34.352173 140595318249472 trainer.py:518] Training: step 32858
I0513 00:40:43.298197 140439986517568 logging_writer.py:48] [32840] collection=train accuracy=0.573424, cross_ent_loss=16.350797653198242, cross_ent_loss_per_all_target_tokens=2.07911e-05, experts/auxiliary_loss=0.16071507334709167, experts/expert_usage=7.974523067474365, experts/fraction_tokens_left_behind=0.25461435317993164, experts/router_confidence=3.4688098430633545, experts/router_z_loss=0.00031259527895599604, learning_rate=0.00551867, learning_rate/current=0.0055183, loss=16.51986312866211, loss_per_all_target_tokens=2.10061e-05, loss_per_nonpadding_target_token=2.11224e-05, non_padding_fraction/loss_weights=0.994491, timing/seconds=89.45012664794922, timing/seqs=3840, timing/seqs_per_second=42.92894744873047, timing/seqs_per_second_per_core=1.3415296077728271, timing/steps_per_second=0.1117941364645958, timing/target_tokens_per_second=87918.484375, timing/target_tokens_per_second_per_core=2747.45263671875, timing/uptime=3328.24, z_loss=0.0080364178866148, z_loss_per_all_target_tokens=1.02188e-08
I0513 00:40:52.243741 140095893518336 trainer.py:518] Training: step 32859
I0513 00:40:52.242982 140440593786880 trainer.py:518] Training: step 32859
I0513 00:40:52.243211 140031916288000 trainer.py:518] Training: step 32859
I0513 00:40:52.257577 140339465779200 trainer.py:518] Training: step 32861
I0513 00:40:52.243192 139674595104768 trainer.py:518] Training: step 32859
I0513 00:40:52.244677 140529434355712 trainer.py:518] Training: step 32859
I0513 00:40:52.243621 140311617681408 trainer.py:518] Training: step 32859
I0513 00:40:52.244205 140595318249472 trainer.py:518] Training: step 32859
I0513 00:41:10.133710 139674595104768 trainer.py:518] Training: step 32865
I0513 00:41:10.135043 140439986517568 logging_writer.py:48] [32850] collection=train accuracy=0.570348, cross_ent_loss=16.462352752685547, cross_ent_loss_per_all_target_tokens=2.0933e-05, experts/auxiliary_loss=0.16069938242435455, experts/expert_usage=7.953639984130859, experts/fraction_tokens_left_behind=0.26268139481544495, experts/router_confidence=3.472076892852783, experts/router_z_loss=0.00030650151893496513, learning_rate=0.00551783, learning_rate/current=0.00551746, loss=16.630878448486328, loss_per_all_target_tokens=2.11473e-05, loss_per_nonpadding_target_token=2.12313e-05, non_padding_fraction/loss_weights=0.996043, timing/seconds=89.44685363769531, timing/seqs=3840, timing/seqs_per_second=42.93052291870117, timing/seqs_per_second_per_core=1.3415788412094116, timing/steps_per_second=0.1117982342839241, timing/target_tokens_per_second=87921.7109375, timing/target_tokens_per_second_per_core=2747.553466796875, timing/uptime=3346.32, z_loss=0.00751911336556077, z_loss_per_all_target_tokens=9.56105e-09
I0513 00:41:10.134363 140311617681408 trainer.py:518] Training: step 32866
I0513 00:41:10.160345 140031916288000 trainer.py:518] Training: step 32867
I0513 00:41:10.165230 140529434355712 trainer.py:518] Training: step 32864
I0513 00:41:10.162032 140595318249472 trainer.py:518] Training: step 32866
I0513 00:41:19.078787 140339465779200 trainer.py:518] Training: step 32870
I0513 00:41:19.079048 140095893518336 trainer.py:518] Training: step 32870
I0513 00:41:19.078660 140440593786880 trainer.py:518] Training: step 32870
I0513 00:41:45.915047 140339465779200 trainer.py:518] Training: step 32872
I0513 00:41:45.914585 140440593786880 trainer.py:518] Training: step 32872
I0513 00:41:45.915681 140095893518336 trainer.py:518] Training: step 32872
I0513 00:41:45.914705 140031916288000 trainer.py:518] Training: step 32872
I0513 00:41:45.915042 140529434355712 trainer.py:518] Training: step 32872
I0513 00:41:45.915040 139674595104768 trainer.py:518] Training: step 32872
I0513 00:41:45.915623 140311617681408 trainer.py:518] Training: step 32872
I0513 00:41:45.915298 140595318249472 trainer.py:518] Training: step 32872
I0513 00:42:03.806293 140095893518336 trainer.py:518] Training: step 32873
I0513 00:42:03.805330 140440593786880 trainer.py:518] Training: step 32873
I0513 00:42:03.805699 140031916288000 trainer.py:518] Training: step 32873
I0513 00:42:03.806277 140529434355712 trainer.py:518] Training: step 32873
I0513 00:42:03.806161 139674595104768 trainer.py:518] Training: step 32873
I0513 00:42:03.806891 140311617681408 trainer.py:518] Training: step 32873
I0513 00:42:03.806373 140595318249472 trainer.py:518] Training: step 32873
I0513 00:42:12.750849 140339465779200 trainer.py:518] Training: step 32873
I0513 00:42:21.696416 140095893518336 trainer.py:518] Training: step 32874
I0513 00:42:21.695902 140031916288000 trainer.py:518] Training: step 32874
I0513 00:42:21.695976 140440593786880 trainer.py:518] Training: step 32874
I0513 00:42:21.696254 140529434355712 trainer.py:518] Training: step 32874
I0513 00:42:21.696561 139674595104768 trainer.py:518] Training: step 32874
I0513 00:42:21.696869 140311617681408 trainer.py:518] Training: step 32874
I0513 00:42:21.697022 140595318249472 trainer.py:518] Training: step 32874
I0513 00:42:30.642216 140339465779200 trainer.py:518] Training: step 32874
I0513 00:42:39.587989 140095893518336 trainer.py:518] Training: step 32875
I0513 00:42:39.587460 140440593786880 trainer.py:518] Training: step 32875
I0513 00:42:39.587503 140031916288000 trainer.py:518] Training: step 32875
I0513 00:42:39.588670 140529434355712 trainer.py:518] Training: step 32875
I0513 00:42:39.588159 139674595104768 trainer.py:518] Training: step 32875
I0513 00:42:39.588260 140311617681408 trainer.py:518] Training: step 32875
I0513 00:42:39.588509 140595318249472 trainer.py:518] Training: step 32875
I0513 00:42:48.532392 140339465779200 trainer.py:518] Training: step 32875
I0513 00:42:57.478134 140095893518336 trainer.py:518] Training: step 32876
I0513 00:42:57.477695 140440593786880 trainer.py:518] Training: step 32876
I0513 00:42:57.477905 140031916288000 trainer.py:518] Training: step 32876
I0513 00:42:57.478834 140529434355712 trainer.py:518] Training: step 32876
I0513 00:42:57.478538 139674595104768 trainer.py:518] Training: step 32876
I0513 00:42:57.478878 140311617681408 trainer.py:518] Training: step 32876
I0513 00:42:57.478930 140595318249472 trainer.py:518] Training: step 32876
I0513 00:43:06.425977 140339465779200 trainer.py:518] Training: step 32876
I0513 00:43:15.370532 140095893518336 trainer.py:518] Training: step 32877
I0513 00:43:15.369767 140440593786880 trainer.py:518] Training: step 32877
I0513 00:43:15.370075 140031916288000 trainer.py:518] Training: step 32877
I0513 00:43:15.371361 140529434355712 trainer.py:518] Training: step 32877
I0513 00:43:15.370212 139674595104768 trainer.py:518] Training: step 32877
I0513 00:43:15.370896 140311617681408 trainer.py:518] Training: step 32877
I0513 00:43:15.370849 140595318249472 trainer.py:518] Training: step 32877
I0513 00:43:33.260552 140339465779200 trainer.py:518] Training: step 32878
I0513 00:43:33.259459 140440593786880 trainer.py:518] Training: step 32878
I0513 00:43:33.260226 140095893518336 trainer.py:518] Training: step 32878
I0513 00:43:33.259729 140031916288000 trainer.py:518] Training: step 32878
I0513 00:43:33.260838 140529434355712 trainer.py:518] Training: step 32878
I0513 00:43:33.259937 139674595104768 trainer.py:518] Training: step 32878
I0513 00:43:33.260757 140311617681408 trainer.py:518] Training: step 32878
I0513 00:43:33.260395 140595318249472 trainer.py:518] Training: step 32878
I0513 00:43:42.205909 140439986517568 logging_writer.py:48] [32860] collection=train accuracy=0.567361, cross_ent_loss=16.609071731567383, cross_ent_loss_per_all_target_tokens=2.11195e-05, experts/auxiliary_loss=0.16067461669445038, experts/expert_usage=7.969838619232178, experts/fraction_tokens_left_behind=0.25535666942596436, experts/router_confidence=3.4777042865753174, experts/router_z_loss=0.00031744592706672847, learning_rate=0.00551699, learning_rate/current=0.00551662, loss=16.77824592590332, loss_per_all_target_tokens=2.13346e-05, loss_per_nonpadding_target_token=2.14641e-05, non_padding_fraction/loss_weights=0.99397, timing/seconds=89.45051574707031, timing/seqs=3840, timing/seqs_per_second=42.92876434326172, timing/seqs_per_second_per_core=1.3415238857269287, timing/steps_per_second=0.111793652176857, timing/target_tokens_per_second=87918.109375, timing/target_tokens_per_second_per_core=2747.44091796875, timing/uptime=3507.15, z_loss=0.008182159624993801, z_loss_per_all_target_tokens=1.04042e-08
I0513 00:43:51.152581 140339465779200 trainer.py:518] Training: step 32879
I0513 00:43:51.151610 140440593786880 trainer.py:518] Training: step 32879
I0513 00:43:51.152878 140095893518336 trainer.py:518] Training: step 32879
I0513 00:43:51.151929 140031916288000 trainer.py:518] Training: step 32879
I0513 00:43:51.153675 140529434355712 trainer.py:518] Training: step 32879
I0513 00:43:51.152437 139674595104768 trainer.py:518] Training: step 32879
I0513 00:43:51.152882 140311617681408 trainer.py:518] Training: step 32879
I0513 00:43:51.153023 140595318249472 trainer.py:518] Training: step 32879
I0513 00:44:09.042467 140440593786880 trainer.py:518] Training: step 32888
I0513 00:44:09.043525 140529434355712 trainer.py:518] Training: step 32885
I0513 00:44:09.043042 139674595104768 trainer.py:518] Training: step 32886
I0513 00:44:09.043815 140311617681408 trainer.py:518] Training: step 32888
I0513 00:44:09.045961 140439986517568 logging_writer.py:48] [32870] collection=train accuracy=0.579759, cross_ent_loss=16.097204208374023, cross_ent_loss_per_all_target_tokens=2.04687e-05, experts/auxiliary_loss=0.16070356965065002, experts/expert_usage=7.967836856842041, experts/fraction_tokens_left_behind=0.2481522411108017, experts/router_confidence=3.459625244140625, experts/router_z_loss=0.0003083774063270539, learning_rate=0.00551616, learning_rate/current=0.00551578, loss=16.266515731811523, loss_per_all_target_tokens=2.06839e-05, loss_per_nonpadding_target_token=2.07866e-05, non_padding_fraction/loss_weights=0.99506, timing/seconds=89.44734191894531, timing/seqs=3840, timing/seqs_per_second=42.9302864074707, timing/seqs_per_second_per_core=1.3415714502334595, timing/steps_per_second=0.11179761588573456, timing/target_tokens_per_second=87921.2265625, timing/target_tokens_per_second_per_core=2747.538330078125, timing/uptime=3525.26, z_loss=0.008299739100039005, z_loss_per_all_target_tokens=1.05537e-08
I0513 00:44:09.043862 140595318249472 trainer.py:518] Training: step 32886
I0513 00:44:09.068638 140031916288000 trainer.py:518] Training: step 32886
I0513 00:44:17.988084 140095893518336 trainer.py:518] Training: step 32890
I0513 00:44:18.003361 140339465779200 trainer.py:518] Training: step 32890
I0513 00:44:44.825002 140339465779200 trainer.py:518] Training: step 32892
I0513 00:44:44.824981 140095893518336 trainer.py:518] Training: step 32892
I0513 00:44:44.824338 140440593786880 trainer.py:518] Training: step 32892
I0513 00:44:44.824579 140031916288000 trainer.py:518] Training: step 32892
I0513 00:44:44.824759 140529434355712 trainer.py:518] Training: step 32892
I0513 00:44:44.824802 139674595104768 trainer.py:518] Training: step 32892
I0513 00:44:44.825398 140311617681408 trainer.py:518] Training: step 32892
I0513 00:44:44.825917 140595318249472 trainer.py:518] Training: step 32892
I0513 00:45:02.715070 140339465779200 trainer.py:518] Training: step 32893
I0513 00:45:02.714813 140095893518336 trainer.py:518] Training: step 32893
I0513 00:45:02.714668 140031916288000 trainer.py:518] Training: step 32893
I0513 00:45:02.715584 140529434355712 trainer.py:518] Training: step 32893
I0513 00:45:02.714854 139674595104768 trainer.py:518] Training: step 32893
I0513 00:45:02.715454 140311617681408 trainer.py:518] Training: step 32893
I0513 00:45:02.715607 140595318249472 trainer.py:518] Training: step 32893
I0513 00:45:11.659227 140440593786880 trainer.py:518] Training: step 32893
I0513 00:45:20.605374 140339465779200 trainer.py:518] Training: step 32894
I0513 00:45:20.605149 140095893518336 trainer.py:518] Training: step 32894
I0513 00:45:20.604850 140031916288000 trainer.py:518] Training: step 32894
I0513 00:45:20.605104 139674595104768 trainer.py:518] Training: step 32894
I0513 00:45:20.605593 140311617681408 trainer.py:518] Training: step 32894
I0513 00:45:20.606086 140595318249472 trainer.py:518] Training: step 32894
I0513 00:45:29.549489 140440593786880 trainer.py:518] Training: step 32894
I0513 00:45:29.550083 140529434355712 trainer.py:518] Training: step 32894
I0513 00:45:38.495947 140339465779200 trainer.py:518] Training: step 32895
I0513 00:45:38.495224 140031916288000 trainer.py:518] Training: step 32895
I0513 00:45:38.495553 139674595104768 trainer.py:518] Training: step 32895
I0513 00:45:38.496682 140311617681408 trainer.py:518] Training: step 32895
I0513 00:45:38.496217 140595318249472 trainer.py:518] Training: step 32895
I0513 00:45:47.440344 140095893518336 trainer.py:518] Training: step 32895
I0513 00:45:47.439801 140440593786880 trainer.py:518] Training: step 32895
I0513 00:45:47.444121 140529434355712 trainer.py:518] Training: step 32895
I0513 00:45:56.386323 140339465779200 trainer.py:518] Training: step 32896
I0513 00:45:56.385951 140031916288000 trainer.py:518] Training: step 32896
I0513 00:45:56.386435 139674595104768 trainer.py:518] Training: step 32896
I0513 00:45:56.387404 140311617681408 trainer.py:518] Training: step 32896
I0513 00:45:56.387039 140595318249472 trainer.py:518] Training: step 32896
I0513 00:46:05.332864 140095893518336 trainer.py:518] Training: step 32896
I0513 00:46:05.332245 140440593786880 trainer.py:518] Training: step 32896
I0513 00:46:05.334342 140529434355712 trainer.py:518] Training: step 32896
I0513 00:46:14.278272 140339465779200 trainer.py:518] Training: step 32897
I0513 00:46:14.277516 140031916288000 trainer.py:518] Training: step 32897
I0513 00:46:14.277977 139674595104768 trainer.py:518] Training: step 32897
I0513 00:46:14.278513 140311617681408 trainer.py:518] Training: step 32897
I0513 00:46:14.278396 140595318249472 trainer.py:518] Training: step 32897
I0513 00:46:32.169590 140339465779200 trainer.py:518] Training: step 32898
I0513 00:46:32.169611 140095893518336 trainer.py:518] Training: step 32898
I0513 00:46:32.168901 140031916288000 trainer.py:518] Training: step 32898
I0513 00:46:32.169174 140440593786880 trainer.py:518] Training: step 32898
I0513 00:46:32.169378 140529434355712 trainer.py:518] Training: step 32898
I0513 00:46:32.169378 139674595104768 trainer.py:518] Training: step 32898
I0513 00:46:32.170298 140311617681408 trainer.py:518] Training: step 32898
I0513 00:46:32.170184 140595318249472 trainer.py:518] Training: step 32898
I0513 00:46:41.113141 140439986517568 logging_writer.py:48] [32880] collection=train accuracy=0.578702, cross_ent_loss=16.192716598510742, cross_ent_loss_per_all_target_tokens=2.05901e-05, experts/auxiliary_loss=0.160708948969841, experts/expert_usage=7.967778205871582, experts/fraction_tokens_left_behind=0.24448786675930023, experts/router_confidence=3.4708592891693115, experts/router_z_loss=0.0003138322208542377, learning_rate=0.00551532, learning_rate/current=0.00551494, loss=16.361684799194336, loss_per_all_target_tokens=2.0805e-05, loss_per_nonpadding_target_token=2.09328e-05, non_padding_fraction/loss_weights=0.993895, timing/seconds=89.4515151977539, timing/seqs=3840, timing/seqs_per_second=42.92828369140625, timing/seqs_per_second_per_core=1.3415088653564453, timing/steps_per_second=0.11179240047931671, timing/target_tokens_per_second=87917.125, timing/target_tokens_per_second_per_core=2747.41015625, timing/uptime=3686.06, z_loss=0.007945367135107517, z_loss_per_all_target_tokens=1.01031e-08
I0513 00:46:50.059753 140339465779200 trainer.py:518] Training: step 32899
I0513 00:46:50.058961 140440593786880 trainer.py:518] Training: step 32899
I0513 00:46:50.060235 140095893518336 trainer.py:518] Training: step 32899
I0513 00:46:50.059316 140031916288000 trainer.py:518] Training: step 32899
I0513 00:46:50.059679 139674595104768 trainer.py:518] Training: step 32899
I0513 00:46:50.060029 140311617681408 trainer.py:518] Training: step 32899
I0513 00:46:50.060339 140595318249472 trainer.py:518] Training: step 32899
I0513 00:46:59.005890 140529434355712 trainer.py:518] Training: step 32900
I0513 00:47:07.949699 140339465779200 trainer.py:518] Training: step 32906
I0513 00:47:07.950821 140439986517568 logging_writer.py:48] [32890] collection=train accuracy=0.556598, cross_ent_loss=17.3087158203125, cross_ent_loss_per_all_target_tokens=2.20092e-05, experts/auxiliary_loss=0.16092734038829803, experts/expert_usage=7.9751482009887695, experts/fraction_tokens_left_behind=0.2619963586330414, experts/router_confidence=3.484718084335327, experts/router_z_loss=0.000322473089909181, learning_rate=0.00551448, learning_rate/current=0.0055141, loss=17.48326301574707, loss_per_all_target_tokens=2.22311e-05, loss_per_nonpadding_target_token=2.2333e-05, non_padding_fraction/loss_weights=0.995438, timing/seconds=89.44503021240234, timing/seqs=3840, timing/seqs_per_second=42.931396484375, timing/seqs_per_second_per_core=1.3416061401367188, timing/steps_per_second=0.11180051416158676, timing/target_tokens_per_second=87923.5, timing/target_tokens_per_second_per_core=2747.609375, timing/uptime=3704.09, z_loss=0.013296782039105892, z_loss_per_all_target_tokens=1.69077e-08
I0513 00:47:07.950266 140311617681408 trainer.py:518] Training: step 32907
I0513 00:47:07.950318 140595318249472 trainer.py:518] Training: step 32906
I0513 00:47:07.976568 140031916288000 trainer.py:518] Training: step 32907
I0513 00:47:07.974061 139674595104768 trainer.py:518] Training: step 32907
I0513 00:47:16.895030 140095893518336 trainer.py:518] Training: step 32910
I0513 00:47:16.894800 140440593786880 trainer.py:518] Training: step 32910
I0513 00:47:16.895651 140529434355712 trainer.py:518] Training: step 32910
I0513 00:47:43.733476 140339465779200 trainer.py:518] Training: step 32912
I0513 00:47:43.732145 140095893518336 trainer.py:518] Training: step 32912
I0513 00:47:43.731878 140440593786880 trainer.py:518] Training: step 32912
I0513 00:47:43.731869 140031916288000 trainer.py:518] Training: step 32912
I0513 00:47:43.732164 140529434355712 trainer.py:518] Training: step 32912
I0513 00:47:43.732195 139674595104768 trainer.py:518] Training: step 32912
I0513 00:47:43.733097 140311617681408 trainer.py:518] Training: step 32912
I0513 00:47:43.733304 140595318249472 trainer.py:518] Training: step 32912
I0513 00:48:01.623091 140095893518336 trainer.py:518] Training: step 32913
I0513 00:48:01.622320 140440593786880 trainer.py:518] Training: step 32913
I0513 00:48:01.622748 140031916288000 trainer.py:518] Training: step 32913
I0513 00:48:01.622979 140529434355712 trainer.py:518] Training: step 32913
I0513 00:48:01.622874 139674595104768 trainer.py:518] Training: step 32913
I0513 00:48:01.623582 140311617681408 trainer.py:518] Training: step 32913
I0513 00:48:01.623536 140595318249472 trainer.py:518] Training: step 32913
I0513 00:48:10.569207 140339465779200 trainer.py:518] Training: step 32913
I0513 00:48:19.514585 140095893518336 trainer.py:518] Training: step 32914
I0513 00:48:19.514494 140031916288000 trainer.py:518] Training: step 32914
I0513 00:48:19.514645 140529434355712 trainer.py:518] Training: step 32914
I0513 00:48:19.515062 139674595104768 trainer.py:518] Training: step 32914
I0513 00:48:19.515676 140311617681408 trainer.py:518] Training: step 32914
I0513 00:48:19.515486 140595318249472 trainer.py:518] Training: step 32914
I0513 00:48:28.460237 140339465779200 trainer.py:518] Training: step 32914
I0513 00:48:28.459563 140440593786880 trainer.py:518] Training: step 32914
I0513 00:48:37.405003 140095893518336 trainer.py:518] Training: step 32915
I0513 00:48:37.404593 140031916288000 trainer.py:518] Training: step 32915
I0513 00:48:37.404927 140529434355712 trainer.py:518] Training: step 32915
I0513 00:48:37.404960 139674595104768 trainer.py:518] Training: step 32915
I0513 00:48:37.405421 140311617681408 trainer.py:518] Training: step 32915
I0513 00:48:37.405713 140595318249472 trainer.py:518] Training: step 32915
I0513 00:48:46.349469 140339465779200 trainer.py:518] Training: step 32915
I0513 00:48:46.348994 140440593786880 trainer.py:518] Training: step 32915
I0513 00:48:55.294706 140095893518336 trainer.py:518] Training: step 32916
I0513 00:48:55.294811 140031916288000 trainer.py:518] Training: step 32916
I0513 00:48:55.295613 140529434355712 trainer.py:518] Training: step 32916
I0513 00:48:55.295098 139674595104768 trainer.py:518] Training: step 32916
I0513 00:48:55.295570 140311617681408 trainer.py:518] Training: step 32916
I0513 00:48:55.296061 140595318249472 trainer.py:518] Training: step 32916
I0513 00:49:04.241401 140339465779200 trainer.py:518] Training: step 32916
I0513 00:49:04.240361 140440593786880 trainer.py:518] Training: step 32916
I0513 00:49:13.185492 140095893518336 trainer.py:518] Training: step 32917
I0513 00:49:13.185915 140031916288000 trainer.py:518] Training: step 32917
I0513 00:49:13.186070 140529434355712 trainer.py:518] Training: step 32917
I0513 00:49:13.185940 139674595104768 trainer.py:518] Training: step 32917
I0513 00:49:13.186695 140311617681408 trainer.py:518] Training: step 32917
I0513 00:49:13.186651 140595318249472 trainer.py:518] Training: step 32917
I0513 00:49:31.075329 140339465779200 trainer.py:518] Training: step 32918
I0513 00:49:31.075530 140095893518336 trainer.py:518] Training: step 32918
I0513 00:49:31.075599 140031916288000 trainer.py:518] Training: step 32918
I0513 00:49:31.075974 140440593786880 trainer.py:518] Training: step 32918
I0513 00:49:31.076126 140529434355712 trainer.py:518] Training: step 32918
I0513 00:49:31.075930 139674595104768 trainer.py:518] Training: step 32918
I0513 00:49:31.076586 140311617681408 trainer.py:518] Training: step 32918
I0513 00:49:31.076270 140595318249472 trainer.py:518] Training: step 32918
I0513 00:49:40.021535 140439986517568 logging_writer.py:48] [32900] collection=train accuracy=0.56642, cross_ent_loss=16.641910552978516, cross_ent_loss_per_all_target_tokens=2.11613e-05, experts/auxiliary_loss=0.160783588886261, experts/expert_usage=7.952096462249756, experts/fraction_tokens_left_behind=0.24725274741649628, experts/router_confidence=3.4861741065979004, experts/router_z_loss=0.0003291774482931942, learning_rate=0.00551364, learning_rate/current=0.00551326, loss=16.816686630249023, loss_per_all_target_tokens=2.13835e-05, loss_per_nonpadding_target_token=2.15103e-05, non_padding_fraction/loss_weights=0.994106, timing/seconds=89.45185089111328, timing/seqs=3840, timing/seqs_per_second=42.928123474121094, timing/seqs_per_second_per_core=1.3415038585662842, timing/steps_per_second=0.11179198324680328, timing/target_tokens_per_second=87916.796875, timing/target_tokens_per_second_per_core=2747.39990234375, timing/uptime=3864.95, z_loss=0.013665774837136269, z_loss_per_all_target_tokens=1.73769e-08
I0513 00:49:48.966217 140339465779200 trainer.py:518] Training: step 32919
I0513 00:49:48.966527 140095893518336 trainer.py:518] Training: step 32919
I0513 00:49:48.965928 140031916288000 trainer.py:518] Training: step 32919
I0513 00:49:48.966057 140440593786880 trainer.py:518] Training: step 32919
I0513 00:49:48.967232 140529434355712 trainer.py:518] Training: step 32919
I0513 00:49:48.966367 139674595104768 trainer.py:518] Training: step 32919
I0513 00:49:48.966961 140311617681408 trainer.py:518] Training: step 32919
I0513 00:49:48.967257 140595318249472 trainer.py:518] Training: step 32919
I0513 00:50:06.858618 140439986517568 logging_writer.py:48] [32910] collection=train accuracy=0.576429, cross_ent_loss=16.223615646362305, cross_ent_loss_per_all_target_tokens=2.06294e-05, experts/auxiliary_loss=0.16076825559139252, experts/expert_usage=7.958699703216553, experts/fraction_tokens_left_behind=0.23397810757160187, experts/router_confidence=3.4863879680633545, experts/router_z_loss=0.00032154758810065687, learning_rate=0.0055128, learning_rate/current=0.00551242, loss=16.394739151000977, loss_per_all_target_tokens=2.0847e-05, loss_per_nonpadding_target_token=2.09595e-05, non_padding_fraction/loss_weights=0.994633, timing/seconds=89.4470443725586, timing/seqs=3840, timing/seqs_per_second=42.93042755126953, timing/seqs_per_second_per_core=1.3415758609771729, timing/steps_per_second=0.1117979884147644, timing/target_tokens_per_second=87921.515625, timing/target_tokens_per_second_per_core=2747.54736328125, timing/uptime=3882.88, z_loss=0.010031885467469692, z_loss_per_all_target_tokens=1.27562e-08
I0513 00:50:06.883939 140031916288000 trainer.py:518] Training: step 32928
I0513 00:50:06.883335 139674595104768 trainer.py:518] Training: step 32927
I0513 00:50:06.886662 140311617681408 trainer.py:518] Training: step 32928
I0513 00:50:06.890165 140529434355712 trainer.py:518] Training: step 32925
I0513 00:50:06.887701 140595318249472 trainer.py:518] Training: step 32928
I0513 00:50:15.802061 140339465779200 trainer.py:518] Training: step 32930
I0513 00:50:15.801939 140095893518336 trainer.py:518] Training: step 32930
I0513 00:50:15.817446 140440593786880 trainer.py:518] Training: step 32930
I0513 00:50:42.639331 140339465779200 trainer.py:518] Training: step 32932
I0513 00:50:42.639357 140095893518336 trainer.py:518] Training: step 32932
I0513 00:50:42.639155 140031916288000 trainer.py:518] Training: step 32932
I0513 00:50:42.639745 140440593786880 trainer.py:518] Training: step 32932
I0513 00:50:42.639595 140529434355712 trainer.py:518] Training: step 32932
I0513 00:50:42.639644 139674595104768 trainer.py:518] Training: step 32932
I0513 00:50:42.640271 140311617681408 trainer.py:518] Training: step 32932
I0513 00:50:42.640327 140595318249472 trainer.py:518] Training: step 32932
I0513 00:51:00.530601 140339465779200 trainer.py:518] Training: step 32933
I0513 00:51:00.530091 140095893518336 trainer.py:518] Training: step 32933
I0513 00:51:00.530289 140031916288000 trainer.py:518] Training: step 32933
I0513 00:51:00.530851 140440593786880 trainer.py:518] Training: step 32933
I0513 00:51:00.530797 140529434355712 trainer.py:518] Training: step 32933
I0513 00:51:00.530696 139674595104768 trainer.py:518] Training: step 32933
I0513 00:51:00.531015 140311617681408 trainer.py:518] Training: step 32933
I0513 00:51:00.531364 140595318249472 trainer.py:518] Training: step 32933
I0513 00:51:18.421100 140440593786880 trainer.py:518] Training: step 32934
I0513 00:51:18.421235 140031916288000 trainer.py:518] Training: step 32934
I0513 00:51:18.421316 140529434355712 trainer.py:518] Training: step 32934
I0513 00:51:18.421272 139674595104768 trainer.py:518] Training: step 32934
I0513 00:51:18.421905 140311617681408 trainer.py:518] Training: step 32934
I0513 00:51:18.422095 140595318249472 trainer.py:518] Training: step 32934
I0513 00:51:27.366824 140339465779200 trainer.py:518] Training: step 32934
I0513 00:51:27.366635 140095893518336 trainer.py:518] Training: step 32934
I0513 00:51:36.311601 140031916288000 trainer.py:518] Training: step 32935
I0513 00:51:36.312037 140440593786880 trainer.py:518] Training: step 32935
I0513 00:51:36.311780 140529434355712 trainer.py:518] Training: step 32935
I0513 00:51:36.312102 139674595104768 trainer.py:518] Training: step 32935
I0513 00:51:36.312654 140311617681408 trainer.py:518] Training: step 32935
I0513 00:51:36.312742 140595318249472 trainer.py:518] Training: step 32935
I0513 00:51:45.256374 140095893518336 trainer.py:518] Training: step 32935
I0513 00:51:45.259245 140339465779200 trainer.py:518] Training: step 32935
I0513 00:51:54.203355 140031916288000 trainer.py:518] Training: step 32936
I0513 00:51:54.203687 140440593786880 trainer.py:518] Training: step 32936
I0513 00:51:54.204211 140529434355712 trainer.py:518] Training: step 32936
I0513 00:51:54.203928 139674595104768 trainer.py:518] Training: step 32936
I0513 00:51:54.204226 140311617681408 trainer.py:518] Training: step 32936
I0513 00:51:54.204493 140595318249472 trainer.py:518] Training: step 32936
I0513 00:52:03.149928 140339465779200 trainer.py:518] Training: step 32936
I0513 00:52:03.150685 140095893518336 trainer.py:518] Training: step 32936
I0513 00:52:12.093747 140031916288000 trainer.py:518] Training: step 32937
I0513 00:52:12.094055 140440593786880 trainer.py:518] Training: step 32937
I0513 00:52:12.093895 140529434355712 trainer.py:518] Training: step 32937
I0513 00:52:12.093923 139674595104768 trainer.py:518] Training: step 32937
I0513 00:52:12.094906 140311617681408 trainer.py:518] Training: step 32937
I0513 00:52:12.094627 140595318249472 trainer.py:518] Training: step 32937
I0513 00:52:29.984437 140339465779200 trainer.py:518] Training: step 32938
I0513 00:52:29.984217 140095893518336 trainer.py:518] Training: step 32938
I0513 00:52:29.983998 140031916288000 trainer.py:518] Training: step 32938
I0513 00:52:29.984561 140440593786880 trainer.py:518] Training: step 32938
I0513 00:52:29.984464 140529434355712 trainer.py:518] Training: step 32938
I0513 00:52:29.984405 139674595104768 trainer.py:518] Training: step 32938
I0513 00:52:29.985066 140311617681408 trainer.py:518] Training: step 32938
I0513 00:52:29.984856 140595318249472 trainer.py:518] Training: step 32938
I0513 00:52:38.931318 140439986517568 logging_writer.py:48] [32920] collection=train accuracy=0.569557, cross_ent_loss=16.597179412841797, cross_ent_loss_per_all_target_tokens=2.11044e-05, experts/auxiliary_loss=0.160751610994339, experts/expert_usage=7.968247413635254, experts/fraction_tokens_left_behind=0.2502015233039856, experts/router_confidence=3.4759552478790283, experts/router_z_loss=0.0003245722909923643, learning_rate=0.00551196, learning_rate/current=0.00551159, loss=16.767629623413086, loss_per_all_target_tokens=2.13211e-05, loss_per_nonpadding_target_token=2.14724e-05, non_padding_fraction/loss_weights=0.992955, timing/seconds=89.45231628417969, timing/seqs=3840, timing/seqs_per_second=42.92790222167969, timing/seqs_per_second_per_core=1.3414969444274902, timing/steps_per_second=0.11179140955209732, timing/target_tokens_per_second=87916.34375, timing/target_tokens_per_second_per_core=2747.3857421875, timing/uptime=4043.87, z_loss=0.00937391072511673, z_loss_per_all_target_tokens=1.19195e-08
I0513 00:52:47.876402 140339465779200 trainer.py:518] Training: step 32939
I0513 00:52:47.876080 140095893518336 trainer.py:518] Training: step 32939
I0513 00:52:47.875543 140031916288000 trainer.py:518] Training: step 32939
I0513 00:52:47.875829 140440593786880 trainer.py:518] Training: step 32939
I0513 00:52:47.877426 140529434355712 trainer.py:518] Training: step 32939
I0513 00:52:47.876464 139674595104768 trainer.py:518] Training: step 32939
I0513 00:52:47.876644 140311617681408 trainer.py:518] Training: step 32939
I0513 00:52:47.876673 140595318249472 trainer.py:518] Training: step 32939
I0513 00:53:05.766843 140529434355712 trainer.py:518] Training: step 32944
I0513 00:53:05.768948 140439986517568 logging_writer.py:48] [32930] collection=train accuracy=0.564034, cross_ent_loss=16.801956176757812, cross_ent_loss_per_all_target_tokens=2.13648e-05, experts/auxiliary_loss=0.16076479852199554, experts/expert_usage=7.97546911239624, experts/fraction_tokens_left_behind=0.23239398002624512, experts/router_confidence=3.4858803749084473, experts/router_z_loss=0.0003223335079383105, learning_rate=0.00551113, learning_rate/current=0.00551075, loss=16.971635818481445, loss_per_all_target_tokens=2.15806e-05, loss_per_nonpadding_target_token=2.16863e-05, non_padding_fraction/loss_weights=0.995122, timing/seconds=89.44540405273438, timing/seqs=3840, timing/seqs_per_second=42.931217193603516, timing/seqs_per_second_per_core=1.3416005373001099, timing/steps_per_second=0.11180004477500916, timing/target_tokens_per_second=87923.1328125, timing/target_tokens_per_second_per_core=2747.597900390625, timing/uptime=4061.93, z_loss=0.008595664985477924, z_loss_per_all_target_tokens=1.093e-08
I0513 00:53:05.791413 140031916288000 trainer.py:518] Training: step 32947
I0513 00:53:05.791730 140440593786880 trainer.py:518] Training: step 32947
I0513 00:53:05.792076 139674595104768 trainer.py:518] Training: step 32947
I0513 00:53:05.795800 140311617681408 trainer.py:518] Training: step 32947
I0513 00:53:05.796753 140595318249472 trainer.py:518] Training: step 32948
I0513 00:53:14.711157 140339465779200 trainer.py:518] Training: step 32950
I0513 00:53:14.711179 140095893518336 trainer.py:518] Training: step 32950
I0513 00:53:41.549208 140339465779200 trainer.py:518] Training: step 32952
I0513 00:53:41.549418 140095893518336 trainer.py:518] Training: step 32952
I0513 00:53:41.549260 140031916288000 trainer.py:518] Training: step 32952
I0513 00:53:41.549596 140440593786880 trainer.py:518] Training: step 32952
I0513 00:53:41.549495 140529434355712 trainer.py:518] Training: step 32952
I0513 00:53:41.549569 139674595104768 trainer.py:518] Training: step 32952
I0513 00:53:41.550050 140311617681408 trainer.py:518] Training: step 32952
I0513 00:53:41.550491 140595318249472 trainer.py:518] Training: step 32952
I0513 00:53:59.439113 140339465779200 trainer.py:518] Training: step 32953
I0513 00:53:59.439376 140095893518336 trainer.py:518] Training: step 32953
I0513 00:53:59.439171 140031916288000 trainer.py:518] Training: step 32953
I0513 00:53:59.439366 140440593786880 trainer.py:518] Training: step 32953
I0513 00:53:59.439512 140529434355712 trainer.py:518] Training: step 32953
I0513 00:53:59.439769 139674595104768 trainer.py:518] Training: step 32953
I0513 00:53:59.440088 140311617681408 trainer.py:518] Training: step 32953
I0513 00:53:59.440053 140595318249472 trainer.py:518] Training: step 32953
I0513 00:54:17.328588 140095893518336 trainer.py:518] Training: step 32954
I0513 00:54:17.328597 140031916288000 trainer.py:518] Training: step 32954
I0513 00:54:17.329106 140440593786880 trainer.py:518] Training: step 32954
I0513 00:54:17.328740 140529434355712 trainer.py:518] Training: step 32954
I0513 00:54:17.329292 139674595104768 trainer.py:518] Training: step 32954
I0513 00:54:17.329913 140311617681408 trainer.py:518] Training: step 32954
I0513 00:54:17.329520 140595318249472 trainer.py:518] Training: step 32954
I0513 00:54:26.274494 140339465779200 trainer.py:518] Training: step 32954
I0513 00:54:35.220658 140095893518336 trainer.py:518] Training: step 32955
I0513 00:54:35.220299 140031916288000 trainer.py:518] Training: step 32955
I0513 00:54:35.220987 140440593786880 trainer.py:518] Training: step 32955
I0513 00:54:35.220677 140529434355712 trainer.py:518] Training: step 32955
I0513 00:54:35.220736 139674595104768 trainer.py:518] Training: step 32955
I0513 00:54:35.221311 140311617681408 trainer.py:518] Training: step 32955
I0513 00:54:35.221208 140595318249472 trainer.py:518] Training: step 32955
I0513 00:54:44.169044 140339465779200 trainer.py:518] Training: step 32955
I0513 00:54:53.110551 140095893518336 trainer.py:518] Training: step 32956
I0513 00:54:53.110482 140031916288000 trainer.py:518] Training: step 32956
I0513 00:54:53.110856 140440593786880 trainer.py:518] Training: step 32956
I0513 00:54:53.111215 140529434355712 trainer.py:518] Training: step 32956
I0513 00:54:53.110810 139674595104768 trainer.py:518] Training: step 32956
I0513 00:54:53.111550 140311617681408 trainer.py:518] Training: step 32956
I0513 00:54:53.111530 140595318249472 trainer.py:518] Training: step 32956
I0513 00:55:02.056969 140339465779200 trainer.py:518] Training: step 32956
I0513 00:55:11.001270 140095893518336 trainer.py:518] Training: step 32957
I0513 00:55:11.001343 140031916288000 trainer.py:518] Training: step 32957
I0513 00:55:11.001589 140440593786880 trainer.py:518] Training: step 32957
I0513 00:55:11.001740 140529434355712 trainer.py:518] Training: step 32957
I0513 00:55:11.001703 139674595104768 trainer.py:518] Training: step 32957
I0513 00:55:11.002104 140311617681408 trainer.py:518] Training: step 32957
I0513 00:55:11.002163 140595318249472 trainer.py:518] Training: step 32957
I0513 00:55:28.893507 140339465779200 trainer.py:518] Training: step 32958
I0513 00:55:28.893512 140095893518336 trainer.py:518] Training: step 32958
I0513 00:55:28.893359 140031916288000 trainer.py:518] Training: step 32958
I0513 00:55:28.893568 140440593786880 trainer.py:518] Training: step 32958
I0513 00:55:28.894226 140529434355712 trainer.py:518] Training: step 32958
I0513 00:55:28.893603 139674595104768 trainer.py:518] Training: step 32958
I0513 00:55:28.894818 140311617681408 trainer.py:518] Training: step 32958
I0513 00:55:28.894174 140595318249472 trainer.py:518] Training: step 32958
I0513 00:55:37.840135 140439986517568 logging_writer.py:48] [32940] collection=train accuracy=0.57108, cross_ent_loss=16.5449161529541, cross_ent_loss_per_all_target_tokens=2.10379e-05, experts/auxiliary_loss=0.16075192391872406, experts/expert_usage=7.9759135246276855, experts/fraction_tokens_left_behind=0.23218367993831635, experts/router_confidence=3.4790055751800537, experts/router_z_loss=0.0003172262222506106, learning_rate=0.00551029, learning_rate/current=0.00550991, loss=16.7141056060791, loss_per_all_target_tokens=2.12531e-05, loss_per_nonpadding_target_token=2.13553e-05, non_padding_fraction/loss_weights=0.995212, timing/seconds=89.4520263671875, timing/seqs=3840, timing/seqs_per_second=42.92803955078125, timing/seqs_per_second_per_core=1.341501235961914, timing/steps_per_second=0.11179177463054657, timing/target_tokens_per_second=87916.625, timing/target_tokens_per_second_per_core=2747.39453125, timing/uptime=4222.78, z_loss=0.008118543773889542, z_loss_per_all_target_tokens=1.03233e-08
I0513 00:55:46.785071 140339465779200 trainer.py:518] Training: step 32959
I0513 00:55:46.785373 140095893518336 trainer.py:518] Training: step 32959
I0513 00:55:46.784734 140031916288000 trainer.py:518] Training: step 32959
I0513 00:55:46.784935 140440593786880 trainer.py:518] Training: step 32959
I0513 00:55:46.786350 140529434355712 trainer.py:518] Training: step 32959
I0513 00:55:46.785365 139674595104768 trainer.py:518] Training: step 32959
I0513 00:55:46.785729 140311617681408 trainer.py:518] Training: step 32959
I0513 00:55:46.785737 140595318249472 trainer.py:518] Training: step 32959
I0513 00:56:04.675228 140031916288000 trainer.py:518] Training: step 32966
I0513 00:56:04.675747 140439986517568 logging_writer.py:48] [32950] collection=train accuracy=0.578396, cross_ent_loss=16.08762550354004, cross_ent_loss_per_all_target_tokens=2.04565e-05, experts/auxiliary_loss=0.1607542783021927, experts/expert_usage=7.969221591949463, experts/fraction_tokens_left_behind=0.2461477816104889, experts/router_confidence=3.450488328933716, experts/router_z_loss=0.0003217920893803239, learning_rate=0.00550945, learning_rate/current=0.00550908, loss=16.256269454956055, loss_per_all_target_tokens=2.06709e-05, loss_per_nonpadding_target_token=2.0833e-05, non_padding_fraction/loss_weights=0.992218, timing/seconds=89.44681549072266, timing/seqs=3840, timing/seqs_per_second=42.930538177490234, timing/seqs_per_second_per_core=1.3415793180465698, timing/steps_per_second=0.11179827898740768, timing/target_tokens_per_second=87921.7421875, timing/target_tokens_per_second_per_core=2747.554443359375, timing/uptime=4240.86, z_loss=0.007566801737993956, z_loss_per_all_target_tokens=9.62169e-09
I0513 00:56:04.676095 140595318249472 trainer.py:518] Training: step 32967
I0513 00:56:04.702442 140440593786880 trainer.py:518] Training: step 32967
I0513 00:56:04.702013 139674595104768 trainer.py:518] Training: step 32967
I0513 00:56:04.705605 140529434355712 trainer.py:518] Training: step 32965
I0513 00:56:04.704157 140311617681408 trainer.py:518] Training: step 32967
I0513 00:56:13.620984 140339465779200 trainer.py:518] Training: step 32970
I0513 00:56:13.621241 140095893518336 trainer.py:518] Training: step 32970
I0513 00:56:40.457509 140339465779200 trainer.py:518] Training: step 32972
I0513 00:56:40.457448 140095893518336 trainer.py:518] Training: step 32972
I0513 00:56:40.457022 140440593786880 trainer.py:518] Training: step 32972
I0513 00:56:40.457561 140031916288000 trainer.py:518] Training: step 32972
I0513 00:56:40.457728 140529434355712 trainer.py:518] Training: step 32972
I0513 00:56:40.457565 139674595104768 trainer.py:518] Training: step 32972
I0513 00:56:40.457892 140311617681408 trainer.py:518] Training: step 32972
I0513 00:56:40.458458 140595318249472 trainer.py:518] Training: step 32972
I0513 00:56:58.348129 140339465779200 trainer.py:518] Training: step 32973
I0513 00:56:58.347442 140095893518336 trainer.py:518] Training: step 32973
I0513 00:56:58.347538 140440593786880 trainer.py:518] Training: step 32973
I0513 00:56:58.348130 140031916288000 trainer.py:518] Training: step 32973
I0513 00:56:58.348380 140529434355712 trainer.py:518] Training: step 32973
I0513 00:56:58.348439 139674595104768 trainer.py:518] Training: step 32973
I0513 00:56:58.348287 140311617681408 trainer.py:518] Training: step 32973
I0513 00:56:58.348767 140595318249472 trainer.py:518] Training: step 32973
I0513 00:57:16.239377 140339465779200 trainer.py:518] Training: step 32974
I0513 00:57:16.238806 140095893518336 trainer.py:518] Training: step 32974
I0513 00:57:16.239814 140031916288000 trainer.py:518] Training: step 32974
I0513 00:57:16.239570 140529434355712 trainer.py:518] Training: step 32974
I0513 00:57:16.240028 139674595104768 trainer.py:518] Training: step 32974
I0513 00:57:16.239936 140311617681408 trainer.py:518] Training: step 32974
I0513 00:57:16.239988 140595318249472 trainer.py:518] Training: step 32974
I0513 00:57:25.184495 140440593786880 trainer.py:518] Training: step 32974
I0513 00:57:34.130603 140339465779200 trainer.py:518] Training: step 32975
I0513 00:57:34.130640 140095893518336 trainer.py:518] Training: step 32975
I0513 00:57:34.130559 140031916288000 trainer.py:518] Training: step 32975
I0513 00:57:34.130545 140529434355712 trainer.py:518] Training: step 32975
I0513 00:57:34.130509 139674595104768 trainer.py:518] Training: step 32975
I0513 00:57:34.130972 140311617681408 trainer.py:518] Training: step 32975
I0513 00:57:34.131133 140595318249472 trainer.py:518] Training: step 32975
I0513 00:57:43.079624 140440593786880 trainer.py:518] Training: step 32975
I0513 00:57:52.022232 140339465779200 trainer.py:518] Training: step 32976
I0513 00:57:52.022246 140095893518336 trainer.py:518] Training: step 32976
I0513 00:57:52.022357 140031916288000 trainer.py:518] Training: step 32976
I0513 00:57:52.022414 140529434355712 trainer.py:518] Training: step 32976
I0513 00:57:52.022256 139674595104768 trainer.py:518] Training: step 32976
I0513 00:57:52.022756 140311617681408 trainer.py:518] Training: step 32976
I0513 00:57:52.022566 140595318249472 trainer.py:518] Training: step 32976
I0513 00:58:00.966994 140440593786880 trainer.py:518] Training: step 32976
I0513 00:58:09.912002 140339465779200 trainer.py:518] Training: step 32977
I0513 00:58:09.911669 140095893518336 trainer.py:518] Training: step 32977
I0513 00:58:09.912398 140031916288000 trainer.py:518] Training: step 32977
I0513 00:58:09.912634 140529434355712 trainer.py:518] Training: step 32977
I0513 00:58:09.912326 139674595104768 trainer.py:518] Training: step 32977
I0513 00:58:09.912766 140311617681408 trainer.py:518] Training: step 32977
I0513 00:58:09.912722 140595318249472 trainer.py:518] Training: step 32977
I0513 00:58:27.804049 140339465779200 trainer.py:518] Training: step 32978
I0513 00:58:27.803792 140095893518336 trainer.py:518] Training: step 32978
I0513 00:58:27.803817 140440593786880 trainer.py:518] Training: step 32978
I0513 00:58:27.804224 140031916288000 trainer.py:518] Training: step 32978
I0513 00:58:27.804862 140529434355712 trainer.py:518] Training: step 32978
I0513 00:58:27.804429 139674595104768 trainer.py:518] Training: step 32978
I0513 00:58:27.804598 140311617681408 trainer.py:518] Training: step 32978
I0513 00:58:27.804802 140595318249472 trainer.py:518] Training: step 32978
I0513 00:58:36.749966 140439986517568 logging_writer.py:48] [32960] collection=train accuracy=0.579649, cross_ent_loss=16.160633087158203, cross_ent_loss_per_all_target_tokens=2.05493e-05, experts/auxiliary_loss=0.16073007881641388, experts/expert_usage=7.963522434234619, experts/fraction_tokens_left_behind=0.2562500834465027, experts/router_confidence=3.46704363822937, experts/router_z_loss=0.000326705863699317, learning_rate=0.00550862, learning_rate/current=0.00550824, loss=16.32951545715332, loss_per_all_target_tokens=2.07641e-05, loss_per_nonpadding_target_token=2.08973e-05, non_padding_fraction/loss_weights=0.993624, timing/seconds=89.45504760742188, timing/seqs=3840, timing/seqs_per_second=42.92658996582031, timing/seqs_per_second_per_core=1.3414559364318848, timing/steps_per_second=0.11178798973560333, timing/target_tokens_per_second=87913.65625, timing/target_tokens_per_second_per_core=2747.3017578125, timing/uptime=4401.96, z_loss=0.007826152257621288, z_loss_per_all_target_tokens=9.95147e-09
I0513 00:58:45.694988 140339465779200 trainer.py:518] Training: step 32979
I0513 00:58:45.695256 140095893518336 trainer.py:518] Training: step 32979
I0513 00:58:45.694471 140031916288000 trainer.py:518] Training: step 32979
I0513 00:58:45.694704 140440593786880 trainer.py:518] Training: step 32979
I0513 00:58:45.695925 140529434355712 trainer.py:518] Training: step 32979
I0513 00:58:45.694853 139674595104768 trainer.py:518] Training: step 32979
I0513 00:58:45.695398 140311617681408 trainer.py:518] Training: step 32979
I0513 00:58:45.696464 140595318249472 trainer.py:518] Training: step 32979
I0513 00:59:03.585252 140339465779200 trainer.py:518] Training: step 32986
I0513 00:59:03.585942 140529434355712 trainer.py:518] Training: step 32985
I0513 00:59:03.585650 140311617681408 trainer.py:518] Training: step 32988
I0513 00:59:03.586154 140595318249472 trainer.py:518] Training: step 32986
I0513 00:59:03.589151 140439986517568 logging_writer.py:48] [32970] collection=train accuracy=0.564759, cross_ent_loss=16.69523048400879, cross_ent_loss_per_all_target_tokens=2.12291e-05, experts/auxiliary_loss=0.16072112321853638, experts/expert_usage=7.965583324432373, experts/fraction_tokens_left_behind=0.24258027970790863, experts/router_confidence=3.4613354206085205, experts/router_z_loss=0.0003186824033036828, learning_rate=0.00550778, learning_rate/current=0.00550741, loss=16.86396026611328, loss_per_all_target_tokens=2.14436e-05, loss_per_nonpadding_target_token=2.15909e-05, non_padding_fraction/loss_weights=0.993179, timing/seconds=89.4470443725586, timing/seqs=3840, timing/seqs_per_second=42.93042755126953, timing/seqs_per_second_per_core=1.3415758609771729, timing/steps_per_second=0.1117979884147644, timing/target_tokens_per_second=87921.515625, timing/target_tokens_per_second_per_core=2747.54736328125, timing/uptime=4419.76, z_loss=0.0076907663606107235, z_loss_per_all_target_tokens=9.77932e-09
I0513 00:59:03.613276 140095893518336 trainer.py:518] Training: step 32989
I0513 00:59:03.612240 140031916288000 trainer.py:518] Training: step 32987
I0513 00:59:03.611370 139674595104768 trainer.py:518] Training: step 32986
I0513 00:59:12.530955 140440593786880 trainer.py:518] Training: step 32990
I0513 00:59:39.367245 140339465779200 trainer.py:518] Training: step 32992
I0513 00:59:39.367076 140095893518336 trainer.py:518] Training: step 32992
I0513 00:59:39.366896 140440593786880 trainer.py:518] Training: step 32992
I0513 00:59:39.367517 140031916288000 trainer.py:518] Training: step 32992
I0513 00:59:39.367439 140529434355712 trainer.py:518] Training: step 32992
I0513 00:59:39.367649 139674595104768 trainer.py:518] Training: step 32992
I0513 00:59:39.367992 140311617681408 trainer.py:518] Training: step 32992
I0513 00:59:39.368184 140595318249472 trainer.py:518] Training: step 32992
I0513 00:59:57.257953 140339465779200 trainer.py:518] Training: step 32993
I0513 00:59:57.257920 140095893518336 trainer.py:518] Training: step 32993
I0513 00:59:57.257875 140440593786880 trainer.py:518] Training: step 32993
I0513 00:59:57.257992 140031916288000 trainer.py:518] Training: step 32993
I0513 00:59:57.258445 140529434355712 trainer.py:518] Training: step 32993
I0513 00:59:57.258331 139674595104768 trainer.py:518] Training: step 32993
I0513 00:59:57.258482 140311617681408 trainer.py:518] Training: step 32993
I0513 00:59:57.258650 140595318249472 trainer.py:518] Training: step 32993
I0513 01:00:15.148895 140339465779200 trainer.py:518] Training: step 32994
I0513 01:00:15.148451 140095893518336 trainer.py:518] Training: step 32994
I0513 01:00:15.149034 140440593786880 trainer.py:518] Training: step 32994
I0513 01:00:15.149257 140529434355712 trainer.py:518] Training: step 32994
I0513 01:00:15.149202 139674595104768 trainer.py:518] Training: step 32994
I0513 01:00:15.149522 140311617681408 trainer.py:518] Training: step 32994
I0513 01:00:15.149707 140595318249472 trainer.py:518] Training: step 32994
I0513 01:00:24.094400 140031916288000 trainer.py:518] Training: step 32994
I0513 01:00:33.039381 140339465779200 trainer.py:518] Training: step 32995
I0513 01:00:33.039063 140095893518336 trainer.py:518] Training: step 32995
I0513 01:00:33.039503 140440593786880 trainer.py:518] Training: step 32995
I0513 01:00:33.039941 140529434355712 trainer.py:518] Training: step 32995
I0513 01:00:33.039880 139674595104768 trainer.py:518] Training: step 32995
I0513 01:00:33.040208 140311617681408 trainer.py:518] Training: step 32995
I0513 01:00:33.040219 140595318249472 trainer.py:518] Training: step 32995
I0513 01:00:41.986510 140031916288000 trainer.py:518] Training: step 32995
I0513 01:00:50.930326 140095893518336 trainer.py:518] Training: step 32996
I0513 01:00:50.931027 140529434355712 trainer.py:518] Training: step 32996
I0513 01:00:50.931286 140595318249472 trainer.py:518] Training: step 32996
I0513 01:00:50.931122 140440593786880 trainer.py:518] Training: step 32996
I0513 01:00:50.930575 140339465779200 trainer.py:518] Training: step 32996
I0513 01:00:50.931674 140311617681408 trainer.py:518] Training: step 32996
I0513 01:00:50.930424 139674595104768 trainer.py:518] Training: step 32996
I0513 01:00:59.877102 140031916288000 trainer.py:518] Training: step 32996
I0513 01:01:08.821666 140095893518336 trainer.py:518] Training: step 32997
I0513 01:01:08.821464 140440593786880 trainer.py:518] Training: step 32997
I0513 01:01:08.822895 140595318249472 trainer.py:518] Training: step 32997
I0513 01:01:08.822646 140311617681408 trainer.py:518] Training: step 32997
I0513 01:01:08.823663 140529434355712 trainer.py:518] Training: step 32997
I0513 01:01:08.822161 140339465779200 trainer.py:518] Training: step 32997
I0513 01:01:08.821756 139674595104768 trainer.py:518] Training: step 32997
I0513 01:01:26.713592 140095893518336 trainer.py:518] Training: step 32998
I0513 01:01:26.713139 140440593786880 trainer.py:518] Training: step 32998
I0513 01:01:26.714123 140529434355712 trainer.py:518] Training: step 32998
I0513 01:01:26.714248 140595318249472 trainer.py:518] Training: step 32998
I0513 01:01:26.714054 140311617681408 trainer.py:518] Training: step 32998
I0513 01:01:26.713514 140339465779200 trainer.py:518] Training: step 32998
I0513 01:01:26.713223 139674595104768 trainer.py:518] Training: step 32998
I0513 01:01:26.713868 140031916288000 trainer.py:518] Training: step 32998
I0513 01:01:35.660290 140439986517568 logging_writer.py:48] [32980] collection=train accuracy=0.572914, cross_ent_loss=16.410398483276367, cross_ent_loss_per_all_target_tokens=2.08669e-05, experts/auxiliary_loss=0.1607116311788559, experts/expert_usage=7.971560001373291, experts/fraction_tokens_left_behind=0.23831744492053986, experts/router_confidence=3.479299545288086, experts/router_z_loss=0.00031485670479014516, learning_rate=0.00550695, learning_rate/current=0.00550657, loss=16.578838348388672, loss_per_all_target_tokens=2.10811e-05, loss_per_nonpadding_target_token=2.11889e-05, non_padding_fraction/loss_weights=0.99491, timing/seconds=89.45287322998047, timing/seqs=3840, timing/seqs_per_second=42.92763137817383, timing/seqs_per_second_per_core=1.3414884805679321, timing/steps_per_second=0.11179070919752121, timing/target_tokens_per_second=87915.7890625, timing/target_tokens_per_second_per_core=2747.368408203125, timing/uptime=4580.6, z_loss=0.007412747945636511, z_loss_per_all_target_tokens=9.4258e-09
I0513 01:01:44.605588 140095893518336 trainer.py:518] Training: step 32999
I0513 01:01:44.605232 140440593786880 trainer.py:518] Training: step 32999
I0513 01:01:44.606219 140595318249472 trainer.py:518] Training: step 32999
I0513 01:01:44.606799 140529434355712 trainer.py:518] Training: step 32999
I0513 01:01:44.606142 140311617681408 trainer.py:518] Training: step 32999
I0513 01:01:44.605536 140339465779200 trainer.py:518] Training: step 32999
I0513 01:01:44.605445 140031916288000 trainer.py:518] Training: step 32999
I0513 01:01:44.605321 139674595104768 trainer.py:518] Training: step 32999
I0513 01:01:54.687120 140440593786880 trainer.py:518] Training: step 33005
I0513 01:01:54.850594 140095893518336 trainer.py:518] Training: step 33005
I0513 01:01:54.953522 140031916288000 trainer.py:518] Training: step 33008
I0513 01:02:02.497319 140529434355712 trainer.py:518] Training: step 33004
I0513 01:02:02.497070 140311617681408 trainer.py:518] Training: step 33007
I0513 01:02:02.496984 140339465779200 trainer.py:518] Training: step 33006
I0513 01:02:02.500986 140439986517568 logging_writer.py:48] [32990] collection=train accuracy=0.574969, cross_ent_loss=16.313232421875, cross_ent_loss_per_all_target_tokens=2.07433e-05, experts/auxiliary_loss=0.16072142124176025, experts/expert_usage=7.970269680023193, experts/fraction_tokens_left_behind=0.22862203419208527, experts/router_confidence=3.4784486293792725, experts/router_z_loss=0.000317821599310264, learning_rate=0.00550611, learning_rate/current=0.00550574, loss=16.48210334777832, loss_per_all_target_tokens=2.09581e-05, loss_per_nonpadding_target_token=2.10447e-05, non_padding_fraction/loss_weights=0.995883, timing/seconds=89.44554901123047, timing/seqs=3840, timing/seqs_per_second=42.931148529052734, timing/seqs_per_second_per_core=1.341598391532898, timing/steps_per_second=0.11179986596107483, timing/target_tokens_per_second=87922.9921875, timing/target_tokens_per_second_per_core=2747.593505859375, timing/uptime=4598.64, z_loss=0.007829324342310429, z_loss_per_all_target_tokens=9.9555e-09
I0513 01:02:02.528511 140595318249472 trainer.py:518] Training: step 33008
I0513 01:02:02.522794 139674595104768 trainer.py:518] Training: step 33006
I0513 01:02:11.442867 140095893518336 trainer.py:518] Training: step 33010
I0513 01:02:11.457642 140440593786880 trainer.py:518] Training: step 33010
I0513 01:02:11.442228 140031916288000 trainer.py:518] Training: step 33010
I0513 01:02:38.279289 140095893518336 trainer.py:518] Training: step 33012
I0513 01:02:38.279184 140529434355712 trainer.py:518] Training: step 33012
I0513 01:02:38.278971 140595318249472 trainer.py:518] Training: step 33012
I0513 01:02:38.279155 140440593786880 trainer.py:518] Training: step 33012
I0513 01:02:38.279169 140339465779200 trainer.py:518] Training: step 33012
I0513 01:02:38.279947 140311617681408 trainer.py:518] Training: step 33012
I0513 01:02:38.278423 139674595104768 trainer.py:518] Training: step 33012
I0513 01:02:38.278773 140031916288000 trainer.py:518] Training: step 33012
I0513 01:02:56.168142 140529434355712 trainer.py:518] Training: step 33013
I0513 01:02:56.168355 140595318249472 trainer.py:518] Training: step 33013
I0513 01:02:56.168148 140440593786880 trainer.py:518] Training: step 33013
I0513 01:02:56.168626 140311617681408 trainer.py:518] Training: step 33013
I0513 01:02:56.168689 140339465779200 trainer.py:518] Training: step 33013
I0513 01:02:56.167495 139674595104768 trainer.py:518] Training: step 33013
I0513 01:02:56.167688 140031916288000 trainer.py:518] Training: step 33013
I0513 01:03:05.113940 140095893518336 trainer.py:518] Training: step 33013
I0513 01:03:14.059406 140595318249472 trainer.py:518] Training: step 33014
I0513 01:03:14.059369 140529434355712 trainer.py:518] Training: step 33014
I0513 01:03:14.059714 140440593786880 trainer.py:518] Training: step 33014
I0513 01:03:14.059870 140311617681408 trainer.py:518] Training: step 33014
I0513 01:03:14.059739 140339465779200 trainer.py:518] Training: step 33014
I0513 01:03:14.058786 139674595104768 trainer.py:518] Training: step 33014
I0513 01:03:14.059451 140031916288000 trainer.py:518] Training: step 33014
I0513 01:03:23.004980 140095893518336 trainer.py:518] Training: step 33014
I0513 01:03:31.949932 140529434355712 trainer.py:518] Training: step 33015
I0513 01:03:31.950239 140440593786880 trainer.py:518] Training: step 33015
I0513 01:03:31.950508 140311617681408 trainer.py:518] Training: step 33015
I0513 01:03:31.950279 140339465779200 trainer.py:518] Training: step 33015
I0513 01:03:31.949881 140031916288000 trainer.py:518] Training: step 33015
I0513 01:03:31.949591 139674595104768 trainer.py:518] Training: step 33015
I0513 01:03:40.894238 140595318249472 trainer.py:518] Training: step 33015
I0513 01:03:49.841607 140095893518336 trainer.py:518] Training: step 33015
I0513 01:03:49.841137 140529434355712 trainer.py:518] Training: step 33016
I0513 01:03:49.841154 140440593786880 trainer.py:518] Training: step 33016
I0513 01:03:49.841903 140311617681408 trainer.py:518] Training: step 33016
I0513 01:03:49.841277 140339465779200 trainer.py:518] Training: step 33016
I0513 01:03:49.840638 139674595104768 trainer.py:518] Training: step 33016
I0513 01:03:49.840906 140031916288000 trainer.py:518] Training: step 33016
I0513 01:03:58.786918 140595318249472 trainer.py:518] Training: step 33016
I0513 01:04:07.731545 140095893518336 trainer.py:518] Training: step 33016
I0513 01:04:07.731986 140529434355712 trainer.py:518] Training: step 33017
I0513 01:04:07.731768 140440593786880 trainer.py:518] Training: step 33017
I0513 01:04:07.731637 140339465779200 trainer.py:518] Training: step 33017
I0513 01:04:07.733318 140311617681408 trainer.py:518] Training: step 33017
I0513 01:04:07.731476 140031916288000 trainer.py:518] Training: step 33017
I0513 01:04:07.731094 139674595104768 trainer.py:518] Training: step 33017
I0513 01:04:25.622934 140095893518336 trainer.py:518] Training: step 33017
I0513 01:04:25.622923 140595318249472 trainer.py:518] Training: step 33018
I0513 01:04:25.623415 140529434355712 trainer.py:518] Training: step 33018
I0513 01:04:25.623482 140311617681408 trainer.py:518] Training: step 33018
I0513 01:04:25.623219 140440593786880 trainer.py:518] Training: step 33018
I0513 01:04:25.622951 140339465779200 trainer.py:518] Training: step 33018
I0513 01:04:25.622907 140031916288000 trainer.py:518] Training: step 33018
I0513 01:04:25.622685 139674595104768 trainer.py:518] Training: step 33018
I0513 01:04:34.570015 140439986517568 logging_writer.py:48] [33000] collection=train accuracy=0.574151, cross_ent_loss=16.318281173706055, cross_ent_loss_per_all_target_tokens=2.07498e-05, experts/auxiliary_loss=0.1606932282447815, experts/expert_usage=7.963024139404297, experts/fraction_tokens_left_behind=0.24140356481075287, experts/router_confidence=3.4789395332336426, experts/router_z_loss=0.0003174560551997274, learning_rate=0.00550528, learning_rate/current=0.0055049, loss=16.486331939697266, loss_per_all_target_tokens=2.09635e-05, loss_per_nonpadding_target_token=2.11086e-05, non_padding_fraction/loss_weights=0.993122, timing/seconds=89.45630645751953, timing/seqs=3840, timing/seqs_per_second=42.92598342895508, timing/seqs_per_second_per_core=1.3414369821548462, timing/steps_per_second=0.11178641766309738, timing/target_tokens_per_second=87912.4140625, timing/target_tokens_per_second_per_core=2747.262939453125, timing/uptime=4759.51, z_loss=0.007038455456495285, z_loss_per_all_target_tokens=8.94986e-09
I0513 01:04:43.514042 140595318249472 trainer.py:518] Training: step 33019
I0513 01:04:43.514493 140311617681408 trainer.py:518] Training: step 33019
I0513 01:04:43.514135 140440593786880 trainer.py:518] Training: step 33019
I0513 01:04:43.514252 140339465779200 trainer.py:518] Training: step 33019
I0513 01:04:43.515569 140529434355712 trainer.py:518] Training: step 33019
I0513 01:04:43.527132 140095893518336 trainer.py:518] Training: step 33021
I0513 01:04:43.513320 140031916288000 trainer.py:518] Training: step 33019
I0513 01:04:43.514039 139674595104768 trainer.py:518] Training: step 33019
I0513 01:05:01.405620 140529434355712 trainer.py:518] Training: step 33024
I0513 01:05:01.404993 140339465779200 trainer.py:518] Training: step 33026
I0513 01:05:01.408891 140439986517568 logging_writer.py:48] [33010] collection=train accuracy=0.567737, cross_ent_loss=16.61457061767578, cross_ent_loss_per_all_target_tokens=2.11265e-05, experts/auxiliary_loss=0.1607263684272766, experts/expert_usage=7.969276428222656, experts/fraction_tokens_left_behind=0.24291543662548065, experts/router_confidence=3.4830520153045654, experts/router_z_loss=0.00031610450241714716, learning_rate=0.00550444, learning_rate/current=0.00550407, loss=16.783226013183594, loss_per_all_target_tokens=2.1341e-05, loss_per_nonpadding_target_token=2.14808e-05, non_padding_fraction/loss_weights=0.99349, timing/seconds=89.44570922851562, timing/seqs=3840, timing/seqs_per_second=42.93107223510742, timing/seqs_per_second_per_core=1.341596007347107, timing/steps_per_second=0.11179967224597931, timing/target_tokens_per_second=87922.8359375, timing/target_tokens_per_second_per_core=2747.588623046875, timing/uptime=4777.59, z_loss=0.007613245397806168, z_loss_per_all_target_tokens=9.68074e-09
I0513 01:05:01.403934 139674595104768 trainer.py:518] Training: step 33025
I0513 01:05:01.429816 140440593786880 trainer.py:518] Training: step 33027
I0513 01:05:01.434644 140311617681408 trainer.py:518] Training: step 33028
I0513 01:05:01.435520 140031916288000 trainer.py:518] Training: step 33029
I0513 01:05:10.350981 140095893518336 trainer.py:518] Training: step 33030
I0513 01:05:10.350891 140595318249472 trainer.py:518] Training: step 33030
I0513 01:05:37.186464 140095893518336 trainer.py:518] Training: step 33032
I0513 01:05:37.186253 140529434355712 trainer.py:518] Training: step 33032
I0513 01:05:37.186060 140595318249472 trainer.py:518] Training: step 33032
I0513 01:05:37.185876 140440593786880 trainer.py:518] Training: step 33032
I0513 01:05:37.186991 140311617681408 trainer.py:518] Training: step 33032
I0513 01:05:37.186512 140339465779200 trainer.py:518] Training: step 33032
I0513 01:05:37.186011 140031916288000 trainer.py:518] Training: step 33032
I0513 01:05:37.186483 139674595104768 trainer.py:518] Training: step 33032
I0513 01:05:55.076697 140095893518336 trainer.py:518] Training: step 33033
I0513 01:05:55.076089 140595318249472 trainer.py:518] Training: step 33033
I0513 01:05:55.076162 140529434355712 trainer.py:518] Training: step 33033
I0513 01:05:55.075997 140440593786880 trainer.py:518] Training: step 33033
I0513 01:05:55.076950 140311617681408 trainer.py:518] Training: step 33033
I0513 01:05:55.076092 139674595104768 trainer.py:518] Training: step 33033
I0513 01:05:55.076171 140031916288000 trainer.py:518] Training: step 33033
I0513 01:06:04.021645 140339465779200 trainer.py:518] Training: step 33033
I0513 01:06:12.966528 140595318249472 trainer.py:518] Training: step 33034
I0513 01:06:12.966732 140529434355712 trainer.py:518] Training: step 33034
I0513 01:06:12.966105 140440593786880 trainer.py:518] Training: step 33034
I0513 01:06:12.967339 140311617681408 trainer.py:518] Training: step 33034
I0513 01:06:12.966017 140031916288000 trainer.py:518] Training: step 33034
I0513 01:06:12.966264 139674595104768 trainer.py:518] Training: step 33034
I0513 01:06:21.911990 140095893518336 trainer.py:518] Training: step 33034
I0513 01:06:21.911960 140339465779200 trainer.py:518] Training: step 33034
I0513 01:06:30.857431 140529434355712 trainer.py:518] Training: step 33035
I0513 01:06:30.857721 140595318249472 trainer.py:518] Training: step 33035
I0513 01:06:30.857353 140440593786880 trainer.py:518] Training: step 33035
I0513 01:06:30.858096 140311617681408 trainer.py:518] Training: step 33035
I0513 01:06:30.857373 140031916288000 trainer.py:518] Training: step 33035
I0513 01:06:30.857180 139674595104768 trainer.py:518] Training: step 33035
I0513 01:06:39.805367 140095893518336 trainer.py:518] Training: step 33035
I0513 01:06:39.804725 140339465779200 trainer.py:518] Training: step 33035
I0513 01:06:48.747662 140595318249472 trainer.py:518] Training: step 33036
I0513 01:06:48.747832 140529434355712 trainer.py:518] Training: step 33036
I0513 01:06:48.747393 140440593786880 trainer.py:518] Training: step 33036
I0513 01:06:48.748652 140311617681408 trainer.py:518] Training: step 33036
I0513 01:06:48.747390 140031916288000 trainer.py:518] Training: step 33036
I0513 01:06:48.747215 139674595104768 trainer.py:518] Training: step 33036
I0513 01:06:57.693408 140095893518336 trainer.py:518] Training: step 33036
I0513 01:06:57.693562 140339465779200 trainer.py:518] Training: step 33036
I0513 01:07:06.638288 140440593786880 trainer.py:518] Training: step 33037
I0513 01:07:06.639020 140595318249472 trainer.py:518] Training: step 33037
I0513 01:07:06.639280 140311617681408 trainer.py:518] Training: step 33037
I0513 01:07:06.640498 140529434355712 trainer.py:518] Training: step 33037
I0513 01:07:06.638695 140031916288000 trainer.py:518] Training: step 33037
I0513 01:07:06.638741 139674595104768 trainer.py:518] Training: step 33037
I0513 01:07:24.530345 140095893518336 trainer.py:518] Training: step 33038
I0513 01:07:24.529699 140595318249472 trainer.py:518] Training: step 33038
I0513 01:07:24.529621 140440593786880 trainer.py:518] Training: step 33038
I0513 01:07:24.530635 140529434355712 trainer.py:518] Training: step 33038
I0513 01:07:24.530610 140311617681408 trainer.py:518] Training: step 33038
I0513 01:07:24.530116 140339465779200 trainer.py:518] Training: step 33038
I0513 01:07:24.529408 139674595104768 trainer.py:518] Training: step 33038
I0513 01:07:24.529896 140031916288000 trainer.py:518] Training: step 33038
I0513 01:07:33.477954 140439986517568 logging_writer.py:48] [33020] collection=train accuracy=0.561775, cross_ent_loss=16.857481002807617, cross_ent_loss_per_all_target_tokens=2.14354e-05, experts/auxiliary_loss=0.16071918606758118, experts/expert_usage=7.966166973114014, experts/fraction_tokens_left_behind=0.22687821090221405, experts/router_confidence=3.4863650798797607, experts/router_z_loss=0.0003089427191298455, learning_rate=0.00550361, learning_rate/current=0.00550323, loss=17.0263671875, loss_per_all_target_tokens=2.16501e-05, loss_per_nonpadding_target_token=2.1723e-05, non_padding_fraction/loss_weights=0.996648, timing/seconds=89.45401763916016, timing/seqs=3840, timing/seqs_per_second=42.92708206176758, timing/seqs_per_second_per_core=1.3414713144302368, timing/steps_per_second=0.111789271235466, timing/target_tokens_per_second=87914.6640625, timing/target_tokens_per_second_per_core=2747.333251953125, timing/uptime=4938.42, z_loss=0.007858946919441223, z_loss_per_all_target_tokens=9.99317e-09
I0513 01:07:42.423252 140095893518336 trainer.py:518] Training: step 33039
I0513 01:07:42.422018 140440593786880 trainer.py:518] Training: step 33039
I0513 01:07:42.422608 140595318249472 trainer.py:518] Training: step 33039
I0513 01:07:42.423580 140311617681408 trainer.py:518] Training: step 33039
I0513 01:07:42.424256 140529434355712 trainer.py:518] Training: step 33039
I0513 01:07:42.422944 140339465779200 trainer.py:518] Training: step 33039
I0513 01:07:42.422584 140031916288000 trainer.py:518] Training: step 33039
I0513 01:07:42.422919 139674595104768 trainer.py:518] Training: step 33039
I0513 01:08:00.314232 140529434355712 trainer.py:518] Training: step 33044
I0513 01:08:00.313977 140595318249472 trainer.py:518] Training: step 33045
I0513 01:08:00.318432 140439986517568 logging_writer.py:48] [33030] collection=train accuracy=0.573766, cross_ent_loss=16.35517692565918, cross_ent_loss_per_all_target_tokens=2.07967e-05, experts/auxiliary_loss=0.16075356304645538, experts/expert_usage=7.960805416107178, experts/fraction_tokens_left_behind=0.24947546422481537, experts/router_confidence=3.471965789794922, experts/router_z_loss=0.0003123312199022621, learning_rate=0.00550278, learning_rate/current=0.0055024, loss=16.52345085144043, loss_per_all_target_tokens=2.10107e-05, loss_per_nonpadding_target_token=2.11238e-05, non_padding_fraction/loss_weights=0.994642, timing/seconds=89.44356536865234, timing/seqs=3840, timing/seqs_per_second=42.932098388671875, timing/seqs_per_second_per_core=1.341628074645996, timing/steps_per_second=0.11180233955383301, timing/target_tokens_per_second=87924.9375, timing/target_tokens_per_second_per_core=2747.654296875, timing/uptime=4956.49, z_loss=0.007206867914646864, z_loss_per_all_target_tokens=9.16401e-09
I0513 01:08:00.346496 140311617681408 trainer.py:518] Training: step 33049
I0513 01:08:00.347997 140440593786880 trainer.py:518] Training: step 33048
I0513 01:08:00.340886 139674595104768 trainer.py:518] Training: step 33046
I0513 01:08:00.348493 140031916288000 trainer.py:518] Training: step 33047
I0513 01:08:09.259032 140095893518336 trainer.py:518] Training: step 33050
I0513 01:08:09.273948 140339465779200 trainer.py:518] Training: step 33050
I0513 01:08:36.094745 140095893518336 trainer.py:518] Training: step 33052
I0513 01:08:36.094679 140529434355712 trainer.py:518] Training: step 33052
I0513 01:08:36.094780 140595318249472 trainer.py:518] Training: step 33052
I0513 01:08:36.094615 140440593786880 trainer.py:518] Training: step 33052
I0513 01:08:36.095098 140311617681408 trainer.py:518] Training: step 33052
I0513 01:08:36.094800 140339465779200 trainer.py:518] Training: step 33052
I0513 01:08:36.094276 140031916288000 trainer.py:518] Training: step 33052
I0513 01:08:36.094126 139674595104768 trainer.py:518] Training: step 33052
I0513 01:08:53.986111 140595318249472 trainer.py:518] Training: step 33053
I0513 01:08:53.986306 140529434355712 trainer.py:518] Training: step 33053
I0513 01:08:53.986460 140440593786880 trainer.py:518] Training: step 33053
I0513 01:08:53.985300 140031916288000 trainer.py:518] Training: step 33053
I0513 01:08:53.985717 139674595104768 trainer.py:518] Training: step 33053
I0513 01:09:02.931361 140095893518336 trainer.py:518] Training: step 33053
I0513 01:09:02.931574 140311617681408 trainer.py:518] Training: step 33053
I0513 01:09:02.931503 140339465779200 trainer.py:518] Training: step 33053
I0513 01:09:11.876189 140595318249472 trainer.py:518] Training: step 33054
I0513 01:09:11.876615 140529434355712 trainer.py:518] Training: step 33054
I0513 01:09:11.876249 140440593786880 trainer.py:518] Training: step 33054
I0513 01:09:11.875956 139674595104768 trainer.py:518] Training: step 33054
I0513 01:09:11.876450 140031916288000 trainer.py:518] Training: step 33054
I0513 01:09:20.821450 140095893518336 trainer.py:518] Training: step 33054
I0513 01:09:20.821798 140311617681408 trainer.py:518] Training: step 33054
I0513 01:09:20.821533 140339465779200 trainer.py:518] Training: step 33054
I0513 01:09:29.767002 140529434355712 trainer.py:518] Training: step 33055
I0513 01:09:29.766955 140595318249472 trainer.py:518] Training: step 33055
I0513 01:09:29.766715 140440593786880 trainer.py:518] Training: step 33055
I0513 01:09:29.766823 140031916288000 trainer.py:518] Training: step 33055
I0513 01:09:29.766615 139674595104768 trainer.py:518] Training: step 33055
I0513 01:09:38.715472 140095893518336 trainer.py:518] Training: step 33055
I0513 01:09:38.715715 140339465779200 trainer.py:518] Training: step 33055
I0513 01:09:47.658499 140595318249472 trainer.py:518] Training: step 33056
I0513 01:09:47.658886 140529434355712 trainer.py:518] Training: step 33056
I0513 01:09:47.658825 140440593786880 trainer.py:518] Training: step 33056
I0513 01:09:47.659921 140311617681408 trainer.py:518] Training: step 33055
I0513 01:09:47.658603 140031916288000 trainer.py:518] Training: step 33056
I0513 01:09:47.658632 139674595104768 trainer.py:518] Training: step 33056
I0513 01:09:56.605451 140095893518336 trainer.py:518] Training: step 33056
I0513 01:09:56.605053 140339465779200 trainer.py:518] Training: step 33056
I0513 01:10:05.549074 140595318249472 trainer.py:518] Training: step 33057
I0513 01:10:05.549509 140529434355712 trainer.py:518] Training: step 33057
I0513 01:10:05.549097 140440593786880 trainer.py:518] Training: step 33057
I0513 01:10:05.549821 140311617681408 trainer.py:518] Training: step 33056
I0513 01:10:05.548664 140031916288000 trainer.py:518] Training: step 33057
I0513 01:10:05.548851 139674595104768 trainer.py:518] Training: step 33057
I0513 01:10:23.439708 140095893518336 trainer.py:518] Training: step 33058
I0513 01:10:23.438972 140595318249472 trainer.py:518] Training: step 33058
I0513 01:10:23.439756 140529434355712 trainer.py:518] Training: step 33058
I0513 01:10:23.439420 140440593786880 trainer.py:518] Training: step 33058
I0513 01:10:23.440104 140311617681408 trainer.py:518] Training: step 33057
I0513 01:10:23.439640 140339465779200 trainer.py:518] Training: step 33058
I0513 01:10:23.439014 140031916288000 trainer.py:518] Training: step 33058
I0513 01:10:23.439403 139674595104768 trainer.py:518] Training: step 33058
I0513 01:10:32.384882 140439986517568 logging_writer.py:48] [33040] collection=train accuracy=0.575381, cross_ent_loss=16.2396297454834, cross_ent_loss_per_all_target_tokens=2.06498e-05, experts/auxiliary_loss=0.16073906421661377, experts/expert_usage=7.9548659324646, experts/fraction_tokens_left_behind=0.23332174122333527, experts/router_confidence=3.474564790725708, experts/router_z_loss=0.00031622740789316595, learning_rate=0.00550194, learning_rate/current=0.00550157, loss=16.40775489807129, loss_per_all_target_tokens=2.08635e-05, loss_per_nonpadding_target_token=2.09615e-05, non_padding_fraction/loss_weights=0.995326, timing/seconds=89.45198059082031, timing/seqs=3840, timing/seqs_per_second=42.928062438964844, timing/seqs_per_second_per_core=1.3415019512176514, timing/steps_per_second=0.11179182678461075, timing/target_tokens_per_second=87916.671875, timing/target_tokens_per_second_per_core=2747.39599609375, timing/uptime=5117.33, z_loss=0.007066793739795685, z_loss_per_all_target_tokens=8.98589e-09
I0513 01:10:41.331325 140095893518336 trainer.py:518] Training: step 33059
I0513 01:10:41.331060 140595318249472 trainer.py:518] Training: step 33059
I0513 01:10:41.330862 140440593786880 trainer.py:518] Training: step 33059
I0513 01:10:41.331189 140311617681408 trainer.py:518] Training: step 33060
I0513 01:10:41.332269 140529434355712 trainer.py:518] Training: step 33059
I0513 01:10:41.331381 140339465779200 trainer.py:518] Training: step 33059
I0513 01:10:41.330727 140031916288000 trainer.py:518] Training: step 33059
I0513 01:10:41.331045 139674595104768 trainer.py:518] Training: step 33059
I0513 01:10:59.222024 140595318249472 trainer.py:518] Training: step 33065
I0513 01:10:59.221883 140440593786880 trainer.py:518] Training: step 33066
I0513 01:10:59.222866 140529434355712 trainer.py:518] Training: step 33065
I0513 01:10:59.224519 140439986517568 logging_writer.py:48] [33050] collection=train accuracy=0.57364, cross_ent_loss=16.31542205810547, cross_ent_loss_per_all_target_tokens=2.07461e-05, experts/auxiliary_loss=0.1607353687286377, experts/expert_usage=7.948646545410156, experts/fraction_tokens_left_behind=0.25210249423980713, experts/router_confidence=3.452308416366577, experts/router_z_loss=0.0003153597062919289, learning_rate=0.00550111, learning_rate/current=0.00550074, loss=16.48347282409668, loss_per_all_target_tokens=2.09598e-05, loss_per_nonpadding_target_token=2.10973e-05, non_padding_fraction/loss_weights=0.993482, timing/seconds=89.44743347167969, timing/seqs=3840, timing/seqs_per_second=42.93024444580078, timing/seqs_per_second_per_core=1.3415701389312744, timing/steps_per_second=0.1117975115776062, timing/target_tokens_per_second=87921.140625, timing/target_tokens_per_second_per_core=2747.53564453125, timing/uptime=5136.08, z_loss=0.006998650263994932, z_loss_per_all_target_tokens=8.89924e-09
I0513 01:10:59.246966 139674595104768 trainer.py:518] Training: step 33065
I0513 01:10:59.253152 140031916288000 trainer.py:518] Training: step 33067
I0513 01:11:08.167302 140095893518336 trainer.py:518] Training: step 33070
I0513 01:11:08.167566 140311617681408 trainer.py:518] Training: step 33070
I0513 01:11:08.167319 140339465779200 trainer.py:518] Training: step 33070
I0513 01:11:35.003301 140095893518336 trainer.py:518] Training: step 33072
I0513 01:11:35.002888 140595318249472 trainer.py:518] Training: step 33072
I0513 01:11:35.003722 140529434355712 trainer.py:518] Training: step 33072
I0513 01:11:35.003077 140440593786880 trainer.py:518] Training: step 33072
I0513 01:11:35.003272 140311617681408 trainer.py:518] Training: step 33072
I0513 01:11:35.003315 140339465779200 trainer.py:518] Training: step 33072
I0513 01:11:35.002918 140031916288000 trainer.py:518] Training: step 33072
I0513 01:11:35.003160 139674595104768 trainer.py:518] Training: step 33072
I0513 01:11:52.892943 140095893518336 trainer.py:518] Training: step 33073
I0513 01:11:52.892209 140595318249472 trainer.py:518] Training: step 33073
I0513 01:11:52.892795 140529434355712 trainer.py:518] Training: step 33073
I0513 01:11:52.892480 140440593786880 trainer.py:518] Training: step 33073
I0513 01:11:52.892590 140339465779200 trainer.py:518] Training: step 33073
I0513 01:11:52.892117 140031916288000 trainer.py:518] Training: step 33073
I0513 01:11:52.892568 139674595104768 trainer.py:518] Training: step 33073
I0513 01:12:01.838785 140311617681408 trainer.py:518] Training: step 33073
I0513 01:12:10.783384 140095893518336 trainer.py:518] Training: step 33074
I0513 01:12:10.783668 140595318249472 trainer.py:518] Training: step 33074
I0513 01:12:10.783540 140440593786880 trainer.py:518] Training: step 33074
I0513 01:12:10.783317 140339465779200 trainer.py:518] Training: step 33074
I0513 01:12:10.783191 140031916288000 trainer.py:518] Training: step 33074
I0513 01:12:10.783246 139674595104768 trainer.py:518] Training: step 33074
I0513 01:12:19.727573 140529434355712 trainer.py:518] Training: step 33074
I0513 01:12:19.727914 140311617681408 trainer.py:518] Training: step 33074
I0513 01:12:28.673419 140095893518336 trainer.py:518] Training: step 33075
I0513 01:12:28.673418 140595318249472 trainer.py:518] Training: step 33075
I0513 01:12:28.673118 140440593786880 trainer.py:518] Training: step 33075
I0513 01:12:28.673425 140339465779200 trainer.py:518] Training: step 33075
I0513 01:12:28.672660 140031916288000 trainer.py:518] Training: step 33075
I0513 01:12:28.673052 139674595104768 trainer.py:518] Training: step 33075
I0513 01:12:37.621702 140311617681408 trainer.py:518] Training: step 33075
I0513 01:12:46.563787 140095893518336 trainer.py:518] Training: step 33076
I0513 01:12:46.563796 140529434355712 trainer.py:518] Training: step 33075
I0513 01:12:46.563836 140595318249472 trainer.py:518] Training: step 33076
I0513 01:12:46.563686 140440593786880 trainer.py:518] Training: step 33076
I0513 01:12:46.564124 140339465779200 trainer.py:518] Training: step 33076
I0513 01:12:46.563349 140031916288000 trainer.py:518] Training: step 33076
I0513 01:12:46.563520 139674595104768 trainer.py:518] Training: step 33076
I0513 01:12:55.509733 140311617681408 trainer.py:518] Training: step 33076
I0513 01:13:04.453702 140095893518336 trainer.py:518] Training: step 33077
I0513 01:13:04.453367 140595318249472 trainer.py:518] Training: step 33077
I0513 01:13:04.453454 140440593786880 trainer.py:518] Training: step 33077
I0513 01:13:04.454643 140529434355712 trainer.py:518] Training: step 33076
I0513 01:13:04.453837 140339465779200 trainer.py:518] Training: step 33077
I0513 01:13:04.453294 140031916288000 trainer.py:518] Training: step 33077
I0513 01:13:04.453190 139674595104768 trainer.py:518] Training: step 33077
I0513 01:13:22.345000 140095893518336 trainer.py:518] Training: step 33078
I0513 01:13:22.344420 140595318249472 trainer.py:518] Training: step 33078
I0513 01:13:22.344882 140529434355712 trainer.py:518] Training: step 33077
I0513 01:13:22.344617 140440593786880 trainer.py:518] Training: step 33078
I0513 01:13:22.345209 140311617681408 trainer.py:518] Training: step 33078
I0513 01:13:22.344939 140339465779200 trainer.py:518] Training: step 33078
I0513 01:13:22.347674 140439986517568 logging_writer.py:48] [33060] collection=train accuracy=0.568078, cross_ent_loss=16.609895706176758, cross_ent_loss_per_all_target_tokens=2.11206e-05, experts/auxiliary_loss=0.1607004553079605, experts/expert_usage=7.953099727630615, experts/fraction_tokens_left_behind=0.23377060890197754, experts/router_confidence=3.4743525981903076, experts/router_z_loss=0.00031746432068757713, learning_rate=0.00550028, learning_rate/current=0.0054999, loss=16.77785873413086, loss_per_all_target_tokens=2.13341e-05, loss_per_nonpadding_target_token=2.14737e-05, non_padding_fraction/loss_weights=0.993501, timing/seconds=89.45098876953125, timing/seqs=3840, timing/seqs_per_second=42.92853546142578, timing/seqs_per_second_per_core=1.3415167331695557, timing/steps_per_second=0.11179306358098984, timing/target_tokens_per_second=87917.640625, timing/target_tokens_per_second_per_core=2747.42626953125, timing/uptime=5296.24, z_loss=0.006945124361664057, z_loss_per_all_target_tokens=8.83118e-09
I0513 01:13:22.344331 139674595104768 trainer.py:518] Training: step 33078
I0513 01:13:22.344631 140031916288000 trainer.py:518] Training: step 33078
I0513 01:13:40.237112 140095893518336 trainer.py:518] Training: step 33079
I0513 01:13:40.236712 140595318249472 trainer.py:518] Training: step 33079
I0513 01:13:40.237106 140529434355712 trainer.py:518] Training: step 33080
I0513 01:13:40.236622 140440593786880 trainer.py:518] Training: step 33079
I0513 01:13:40.237305 140311617681408 trainer.py:518] Training: step 33079
I0513 01:13:40.236935 140339465779200 trainer.py:518] Training: step 33079
I0513 01:13:40.240923 140439986517568 logging_writer.py:48] [33070] collection=train accuracy=0.579724, cross_ent_loss=16.10835838317871, cross_ent_loss_per_all_target_tokens=2.04828e-05, experts/auxiliary_loss=0.160731703042984, experts/expert_usage=7.944056987762451, experts/fraction_tokens_left_behind=0.24535608291625977, experts/router_confidence=3.466778516769409, experts/router_z_loss=0.0003144587390124798, learning_rate=0.00549945, learning_rate/current=0.00549907, loss=16.276348114013672, loss_per_all_target_tokens=2.06964e-05, loss_per_nonpadding_target_token=2.08049e-05, non_padding_fraction/loss_weights=0.994787, timing/seconds=89.44393920898438, timing/seqs=3840, timing/seqs_per_second=42.93191909790039, timing/seqs_per_second_per_core=1.3416224718093872, timing/steps_per_second=0.111801877617836, timing/target_tokens_per_second=87924.5703125, timing/target_tokens_per_second_per_core=2747.642822265625, timing/uptime=5314.28, z_loss=0.00694569805637002, z_loss_per_all_target_tokens=8.83191e-09
I0513 01:13:40.236404 140031916288000 trainer.py:518] Training: step 33079
I0513 01:13:40.236663 139674595104768 trainer.py:518] Training: step 33079
I0513 01:13:58.126596 140595318249472 trainer.py:518] Training: step 33086
I0513 01:13:58.127206 140440593786880 trainer.py:518] Training: step 33086
I0513 01:13:58.127009 140339465779200 trainer.py:518] Training: step 33086
I0513 01:13:58.150758 139674595104768 trainer.py:518] Training: step 33085
I0513 01:13:58.158591 140031916288000 trainer.py:518] Training: step 33087
I0513 01:14:07.071358 140095893518336 trainer.py:518] Training: step 33090
I0513 01:14:07.072313 140529434355712 trainer.py:518] Training: step 33090
I0513 01:14:07.073469 140311617681408 trainer.py:518] Training: step 33090
I0513 01:14:33.906578 140095893518336 trainer.py:518] Training: step 33092
I0513 01:14:33.906943 140595318249472 trainer.py:518] Training: step 33092
I0513 01:14:33.906964 140529434355712 trainer.py:518] Training: step 33092
I0513 01:14:33.907201 140311617681408 trainer.py:518] Training: step 33092
I0513 01:14:33.907295 140440593786880 trainer.py:518] Training: step 33092
I0513 01:14:33.907388 140339465779200 trainer.py:518] Training: step 33092
I0513 01:14:33.906279 139674595104768 trainer.py:518] Training: step 33092
I0513 01:14:33.906577 140031916288000 trainer.py:518] Training: step 33092
I0513 01:14:51.796271 140095893518336 trainer.py:518] Training: step 33093
I0513 01:14:51.796426 140595318249472 trainer.py:518] Training: step 33093
I0513 01:14:51.796573 140440593786880 trainer.py:518] Training: step 33093
I0513 01:14:51.797228 140311617681408 trainer.py:518] Training: step 33093
I0513 01:14:51.797776 140529434355712 trainer.py:518] Training: step 33093
I0513 01:14:51.797058 140339465779200 trainer.py:518] Training: step 33093
I0513 01:14:51.796090 139674595104768 trainer.py:518] Training: step 33093
I0513 01:14:51.796537 140031916288000 trainer.py:518] Training: step 33093
I0513 01:15:09.687903 140095893518336 trainer.py:518] Training: step 33094
I0513 01:15:09.688219 140529434355712 trainer.py:518] Training: step 33094
I0513 01:15:09.688266 140595318249472 trainer.py:518] Training: step 33094
I0513 01:15:09.688139 140440593786880 trainer.py:518] Training: step 33094
I0513 01:15:09.688882 140311617681408 trainer.py:518] Training: step 33094
I0513 01:15:09.688971 140339465779200 trainer.py:518] Training: step 33094
I0513 01:15:09.687649 139674595104768 trainer.py:518] Training: step 33094
I0513 01:15:09.688668 140031916288000 trainer.py:518] Training: step 33094
I0513 01:15:27.578151 140095893518336 trainer.py:518] Training: step 33095
I0513 01:15:27.578181 140595318249472 trainer.py:518] Training: step 33095
I0513 01:15:27.578273 140440593786880 trainer.py:518] Training: step 33095
I0513 01:15:27.578915 140311617681408 trainer.py:518] Training: step 33095
I0513 01:15:27.578142 139674595104768 trainer.py:518] Training: step 33095
I0513 01:15:27.578384 140031916288000 trainer.py:518] Training: step 33095
I0513 01:15:36.522587 140529434355712 trainer.py:518] Training: step 33095
I0513 01:15:36.525982 140339465779200 trainer.py:518] Training: step 33095
I0513 01:15:45.468691 140095893518336 trainer.py:518] Training: step 33096
I0513 01:15:45.468707 140440593786880 trainer.py:518] Training: step 33096
I0513 01:15:45.470219 140311617681408 trainer.py:518] Training: step 33096
I0513 01:15:45.468879 140031916288000 trainer.py:518] Training: step 33096
I0513 01:15:45.468526 139674595104768 trainer.py:518] Training: step 33096
I0513 01:15:54.414381 140595318249472 trainer.py:518] Training: step 33097
I0513 01:15:54.415178 140339465779200 trainer.py:518] Training: step 33096
I0513 01:15:54.417637 140529434355712 trainer.py:518] Training: step 33096
I0513 01:16:03.359379 140095893518336 trainer.py:518] Training: step 33097
I0513 01:16:03.359734 140440593786880 trainer.py:518] Training: step 33097
I0513 01:16:03.360185 140311617681408 trainer.py:518] Training: step 33097
I0513 01:16:03.359213 140031916288000 trainer.py:518] Training: step 33097
I0513 01:16:03.359080 139674595104768 trainer.py:518] Training: step 33097
I0513 01:16:12.305574 140595318249472 trainer.py:518] Training: step 33098
I0513 01:16:21.250631 140095893518336 trainer.py:518] Training: step 33098
I0513 01:16:21.251401 140529434355712 trainer.py:518] Training: step 33098
I0513 01:16:21.251130 140440593786880 trainer.py:518] Training: step 33098
I0513 01:16:21.251507 140311617681408 trainer.py:518] Training: step 33098
I0513 01:16:21.251371 140339465779200 trainer.py:518] Training: step 33098
I0513 01:16:21.250462 140031916288000 trainer.py:518] Training: step 33098
I0513 01:16:21.250722 139674595104768 trainer.py:518] Training: step 33098
I0513 01:16:30.194546 140439986517568 logging_writer.py:48] [33080] collection=train accuracy=0.570388, cross_ent_loss=16.49562644958496, cross_ent_loss_per_all_target_tokens=2.09753e-05, experts/auxiliary_loss=0.16070182621479034, experts/expert_usage=7.961022853851318, experts/fraction_tokens_left_behind=0.22934885323047638, experts/router_confidence=3.4829609394073486, experts/router_z_loss=0.00031900012982077897, learning_rate=0.00549862, learning_rate/current=0.00549824, loss=16.663570404052734, loss_per_all_target_tokens=2.11888e-05, loss_per_nonpadding_target_token=2.1302e-05, non_padding_fraction/loss_weights=0.994688, timing/seconds=89.45368957519531, timing/seqs=3840, timing/seqs_per_second=42.92723846435547, timing/seqs_per_second_per_core=1.3414762020111084, timing/steps_per_second=0.11178968846797943, timing/target_tokens_per_second=87914.984375, timing/target_tokens_per_second_per_core=2747.34326171875, timing/uptime=5466.22, z_loss=0.006922670640051365, z_loss_per_all_target_tokens=8.80263e-09
I0513 01:16:30.196158 140595318249472 trainer.py:518] Training: step 33099
I0513 01:16:39.141846 140095893518336 trainer.py:518] Training: step 33099
I0513 01:16:39.142502 140529434355712 trainer.py:518] Training: step 33099
I0513 01:16:39.142038 140440593786880 trainer.py:518] Training: step 33099
I0513 01:16:39.142651 140311617681408 trainer.py:518] Training: step 33099
I0513 01:16:39.142136 140339465779200 trainer.py:518] Training: step 33099
I0513 01:16:39.141726 140031916288000 trainer.py:518] Training: step 33099
I0513 01:16:39.141653 139674595104768 trainer.py:518] Training: step 33099
I0513 01:16:48.088275 140595318249472 trainer.py:518] Training: step 33100
I0513 01:16:57.032256 140095893518336 trainer.py:518] Training: step 33109
I0513 01:16:57.031466 140439986517568 logging_writer.py:48] [33090] collection=train accuracy=0.577768, cross_ent_loss=16.201101303100586, cross_ent_loss_per_all_target_tokens=2.06008e-05, experts/auxiliary_loss=0.16074083745479584, experts/expert_usage=7.935815334320068, experts/fraction_tokens_left_behind=0.26233962178230286, experts/router_confidence=3.4606587886810303, experts/router_z_loss=0.00032156126690097153, learning_rate=0.00549778, learning_rate/current=0.00549741, loss=16.368894577026367, loss_per_all_target_tokens=2.08141e-05, loss_per_nonpadding_target_token=2.09616e-05, non_padding_fraction/loss_weights=0.992965, timing/seconds=89.44480895996094, timing/seqs=3840, timing/seqs_per_second=42.93150329589844, timing/seqs_per_second_per_core=1.3416094779968262, timing/steps_per_second=0.11180078983306885, timing/target_tokens_per_second=87923.71875, timing/target_tokens_per_second_per_core=2747.6162109375, timing/uptime=5484.06, z_loss=0.006730922497808933, z_loss_per_all_target_tokens=8.55881e-09
I0513 01:16:57.032582 140440593786880 trainer.py:518] Training: step 33106
I0513 01:16:57.031605 139674595104768 trainer.py:518] Training: step 33106
I0513 01:16:57.061578 140031916288000 trainer.py:518] Training: step 33108
I0513 01:16:57.629889 140311617681408 trainer.py:518] Training: step 33107
I0513 01:17:05.978335 140595318249472 trainer.py:518] Training: step 33110
I0513 01:17:05.978560 140529434355712 trainer.py:518] Training: step 33110
I0513 01:17:05.994930 140339465779200 trainer.py:518] Training: step 33110
I0513 01:17:32.813189 140095893518336 trainer.py:518] Training: step 33112
I0513 01:17:32.813529 140529434355712 trainer.py:518] Training: step 33112
I0513 01:17:32.813556 140595318249472 trainer.py:518] Training: step 33112
I0513 01:17:32.813643 140440593786880 trainer.py:518] Training: step 33112
I0513 01:17:32.813950 140311617681408 trainer.py:518] Training: step 33112
I0513 01:17:32.813934 140339465779200 trainer.py:518] Training: step 33112
I0513 01:17:32.812699 139674595104768 trainer.py:518] Training: step 33112
I0513 01:17:32.813509 140031916288000 trainer.py:518] Training: step 33112
I0513 01:17:50.704113 140095893518336 trainer.py:518] Training: step 33113
I0513 01:17:50.704144 140529434355712 trainer.py:518] Training: step 33113
I0513 01:17:50.704257 140595318249472 trainer.py:518] Training: step 33113
I0513 01:17:50.704084 140440593786880 trainer.py:518] Training: step 33113
I0513 01:17:50.704155 140339465779200 trainer.py:518] Training: step 33113
I0513 01:17:50.705113 140311617681408 trainer.py:518] Training: step 33113
I0513 01:17:50.704109 140031916288000 trainer.py:518] Training: step 33113
I0513 01:17:50.703881 139674595104768 trainer.py:518] Training: step 33113
I0513 01:18:08.594380 140095893518336 trainer.py:518] Training: step 33114
I0513 01:18:08.594383 140529434355712 trainer.py:518] Training: step 33114
I0513 01:18:08.594398 140595318249472 trainer.py:518] Training: step 33114
I0513 01:18:08.594893 140440593786880 trainer.py:518] Training: step 33114
I0513 01:18:08.595575 140311617681408 trainer.py:518] Training: step 33114
I0513 01:18:08.594361 140031916288000 trainer.py:518] Training: step 33114
I0513 01:18:08.594425 139674595104768 trainer.py:518] Training: step 33114
I0513 01:18:17.540160 140339465779200 trainer.py:518] Training: step 33114
I0513 01:18:26.484975 140095893518336 trainer.py:518] Training: step 33115
I0513 01:18:26.484722 140595318249472 trainer.py:518] Training: step 33115
I0513 01:18:26.485037 140529434355712 trainer.py:518] Training: step 33115
I0513 01:18:26.485482 140311617681408 trainer.py:518] Training: step 33115
I0513 01:18:26.485686 140440593786880 trainer.py:518] Training: step 33115
I0513 01:18:26.484839 140031916288000 trainer.py:518] Training: step 33115
I0513 01:18:26.484826 139674595104768 trainer.py:518] Training: step 33115
I0513 01:18:35.433342 140339465779200 trainer.py:518] Training: step 33115
I0513 01:18:44.376728 140095893518336 trainer.py:518] Training: step 33116
I0513 01:18:44.376379 140529434355712 trainer.py:518] Training: step 33116
I0513 01:18:44.376338 140595318249472 trainer.py:518] Training: step 33116
I0513 01:18:44.376360 140440593786880 trainer.py:518] Training: step 33116
I0513 01:18:44.376710 140311617681408 trainer.py:518] Training: step 33116
I0513 01:18:44.376187 139674595104768 trainer.py:518] Training: step 33116
I0513 01:18:44.376282 140031916288000 trainer.py:518] Training: step 33116
I0513 01:18:53.323443 140339465779200 trainer.py:518] Training: step 33116
I0513 01:19:02.267792 140095893518336 trainer.py:518] Training: step 33117
I0513 01:19:02.268425 140595318249472 trainer.py:518] Training: step 33117
I0513 01:19:02.269035 140529434355712 trainer.py:518] Training: step 33117
I0513 01:19:02.268179 140440593786880 trainer.py:518] Training: step 33117
I0513 01:19:02.268432 140311617681408 trainer.py:518] Training: step 33117
I0513 01:19:02.267748 140031916288000 trainer.py:518] Training: step 33117
I0513 01:19:02.267685 139674595104768 trainer.py:518] Training: step 33117
I0513 01:19:20.157808 140095893518336 trainer.py:518] Training: step 33118
I0513 01:19:20.158352 140595318249472 trainer.py:518] Training: step 33118
I0513 01:19:20.158946 140529434355712 trainer.py:518] Training: step 33118
I0513 01:19:20.158273 140440593786880 trainer.py:518] Training: step 33118
I0513 01:19:20.158697 140311617681408 trainer.py:518] Training: step 33118
I0513 01:19:20.158871 140339465779200 trainer.py:518] Training: step 33118
I0513 01:19:20.157805 139674595104768 trainer.py:518] Training: step 33118
I0513 01:19:20.158443 140031916288000 trainer.py:518] Training: step 33118
I0513 01:19:29.104105 140439986517568 logging_writer.py:48] [33100] collection=train accuracy=0.577225, cross_ent_loss=16.195823669433594, cross_ent_loss_per_all_target_tokens=2.05941e-05, experts/auxiliary_loss=0.16072019934654236, experts/expert_usage=7.958596229553223, experts/fraction_tokens_left_behind=0.2263370007276535, experts/router_confidence=3.4673638343811035, experts/router_z_loss=0.0003141055931337178, learning_rate=0.00549695, learning_rate/current=0.00549658, loss=16.363571166992188, loss_per_all_target_tokens=2.08074e-05, loss_per_nonpadding_target_token=2.0904e-05, non_padding_fraction/loss_weights=0.995379, timing/seconds=89.45186614990234, timing/seqs=3840, timing/seqs_per_second=42.92811584472656, timing/seqs_per_second_per_core=1.341503620147705, timing/steps_per_second=0.11179196834564209, timing/target_tokens_per_second=87916.78125, timing/target_tokens_per_second_per_core=2747.3994140625, timing/uptime=5654.05, z_loss=0.006712709553539753, z_loss_per_all_target_tokens=8.53565e-09
I0513 01:19:38.049814 140095893518336 trainer.py:518] Training: step 33119
I0513 01:19:38.049130 140595318249472 trainer.py:518] Training: step 33119
I0513 01:19:38.049090 140440593786880 trainer.py:518] Training: step 33119
I0513 01:19:38.049750 140311617681408 trainer.py:518] Training: step 33119
I0513 01:19:38.050749 140529434355712 trainer.py:518] Training: step 33119
I0513 01:19:38.049658 140339465779200 trainer.py:518] Training: step 33119
I0513 01:19:38.048952 140031916288000 trainer.py:518] Training: step 33119
I0513 01:19:38.048921 139674595104768 trainer.py:518] Training: step 33119
I0513 01:19:55.938529 140595318249472 trainer.py:518] Training: step 33126
I0513 01:19:55.939608 140439986517568 logging_writer.py:48] [33110] collection=train accuracy=0.57504, cross_ent_loss=16.308692932128906, cross_ent_loss_per_all_target_tokens=2.07376e-05, experts/auxiliary_loss=0.16074030101299286, experts/expert_usage=7.962418556213379, experts/fraction_tokens_left_behind=0.23739992082118988, experts/router_confidence=3.4674298763275146, experts/router_z_loss=0.0003130426339339465, learning_rate=0.00549612, learning_rate/current=0.00549575, loss=16.47646713256836, loss_per_all_target_tokens=2.09509e-05, loss_per_nonpadding_target_token=2.10924e-05, non_padding_fraction/loss_weights=0.993291, timing/seconds=89.44573211669922, timing/seqs=3840, timing/seqs_per_second=42.93105697631836, timing/seqs_per_second_per_core=1.3415955305099487, timing/steps_per_second=0.11179962754249573, timing/target_tokens_per_second=87922.8046875, timing/target_tokens_per_second_per_core=2747.587646484375, timing/uptime=5671.91, z_loss=0.006718152202665806, z_loss_per_all_target_tokens=8.54257e-09
I0513 01:19:55.966038 140095893518336 trainer.py:518] Training: step 33129
I0513 01:19:55.965212 140440593786880 trainer.py:518] Training: step 33127
I0513 01:19:55.967426 140529434355712 trainer.py:518] Training: step 33124
I0513 01:19:55.968574 140311617681408 trainer.py:518] Training: step 33127
I0513 01:19:55.964819 139674595104768 trainer.py:518] Training: step 33127
I0513 01:19:55.974054 140031916288000 trainer.py:518] Training: step 33127
I0513 01:20:04.886994 140339465779200 trainer.py:518] Training: step 33130
I0513 01:20:31.719946 140095893518336 trainer.py:518] Training: step 33132
I0513 01:20:31.719901 140595318249472 trainer.py:518] Training: step 33132
I0513 01:20:31.720348 140529434355712 trainer.py:518] Training: step 33132
I0513 01:20:31.719660 140440593786880 trainer.py:518] Training: step 33132
I0513 01:20:31.720519 140311617681408 trainer.py:518] Training: step 33132
I0513 01:20:31.720407 140339465779200 trainer.py:518] Training: step 33132
I0513 01:20:31.719409 139674595104768 trainer.py:518] Training: step 33132
I0513 01:20:31.720075 140031916288000 trainer.py:518] Training: step 33132
I0513 01:20:49.609419 140095893518336 trainer.py:518] Training: step 33133
I0513 01:20:49.609948 140529434355712 trainer.py:518] Training: step 33133
I0513 01:20:49.609923 140595318249472 trainer.py:518] Training: step 33133
I0513 01:20:49.609503 140440593786880 trainer.py:518] Training: step 33133
I0513 01:20:49.610458 140339465779200 trainer.py:518] Training: step 33133
I0513 01:20:49.609233 139674595104768 trainer.py:518] Training: step 33133
I0513 01:20:49.610231 140031916288000 trainer.py:518] Training: step 33133
I0513 01:20:58.556751 140311617681408 trainer.py:518] Training: step 33133
I0513 01:21:07.501551 140095893518336 trainer.py:518] Training: step 33134
I0513 01:21:07.501852 140595318249472 trainer.py:518] Training: step 33134
I0513 01:21:07.501362 140440593786880 trainer.py:518] Training: step 33134
I0513 01:21:07.502266 140529434355712 trainer.py:518] Training: step 33134
I0513 01:21:07.502221 140339465779200 trainer.py:518] Training: step 33134
I0513 01:21:07.502037 140031916288000 trainer.py:518] Training: step 33134
I0513 01:21:07.501246 139674595104768 trainer.py:518] Training: step 33134
I0513 01:21:16.447340 140311617681408 trainer.py:518] Training: step 33134
I0513 01:21:25.392222 140095893518336 trainer.py:518] Training: step 33135
I0513 01:21:25.391712 140529434355712 trainer.py:518] Training: step 33135
I0513 01:21:25.392145 140595318249472 trainer.py:518] Training: step 33135
I0513 01:21:25.391849 140440593786880 trainer.py:518] Training: step 33135
I0513 01:21:25.392486 140339465779200 trainer.py:518] Training: step 33135
I0513 01:21:25.391630 139674595104768 trainer.py:518] Training: step 33135
I0513 01:21:25.392650 140031916288000 trainer.py:518] Training: step 33135
I0513 01:21:34.341987 140311617681408 trainer.py:518] Training: step 33135
I0513 01:21:43.284073 140095893518336 trainer.py:518] Training: step 33136
I0513 01:21:43.283710 140595318249472 trainer.py:518] Training: step 33136
I0513 01:21:43.283269 140440593786880 trainer.py:518] Training: step 33136
I0513 01:21:43.284234 140529434355712 trainer.py:518] Training: step 33136
I0513 01:21:43.284328 140339465779200 trainer.py:518] Training: step 33136
I0513 01:21:43.283550 139674595104768 trainer.py:518] Training: step 33136
I0513 01:21:43.284009 140031916288000 trainer.py:518] Training: step 33136
I0513 01:21:52.230718 140311617681408 trainer.py:518] Training: step 33136
I0513 01:22:01.174741 140095893518336 trainer.py:518] Training: step 33137
I0513 01:22:01.174678 140595318249472 trainer.py:518] Training: step 33137
I0513 01:22:01.174234 140440593786880 trainer.py:518] Training: step 33137
I0513 01:22:01.175481 140529434355712 trainer.py:518] Training: step 33137
I0513 01:22:01.175270 140339465779200 trainer.py:518] Training: step 33137
I0513 01:22:01.174257 139674595104768 trainer.py:518] Training: step 33137
I0513 01:22:01.175201 140031916288000 trainer.py:518] Training: step 33137
I0513 01:22:19.064047 140095893518336 trainer.py:518] Training: step 33138
I0513 01:22:19.063886 140440593786880 trainer.py:518] Training: step 33138
I0513 01:22:19.064718 140529434355712 trainer.py:518] Training: step 33138
I0513 01:22:19.064599 140595318249472 trainer.py:518] Training: step 33138
I0513 01:22:19.064948 140311617681408 trainer.py:518] Training: step 33138
I0513 01:22:19.064764 140339465779200 trainer.py:518] Training: step 33138
I0513 01:22:19.063840 139674595104768 trainer.py:518] Training: step 33138
I0513 01:22:19.064542 140031916288000 trainer.py:518] Training: step 33138
I0513 01:22:28.010074 140439986517568 logging_writer.py:48] [33120] collection=train accuracy=0.577161, cross_ent_loss=16.22293472290039, cross_ent_loss_per_all_target_tokens=2.06285e-05, experts/auxiliary_loss=0.16074828803539276, experts/expert_usage=7.970296382904053, experts/fraction_tokens_left_behind=0.2446010410785675, experts/router_confidence=3.469743013381958, experts/router_z_loss=0.0003229935246054083, learning_rate=0.00549529, learning_rate/current=0.00549492, loss=16.391016006469727, loss_per_all_target_tokens=2.08423e-05, loss_per_nonpadding_target_token=2.09693e-05, non_padding_fraction/loss_weights=0.993941, timing/seconds=89.45116424560547, timing/seqs=3840, timing/seqs_per_second=42.92845153808594, timing/seqs_per_second_per_core=1.3415141105651855, timing/steps_per_second=0.11179284006357193, timing/target_tokens_per_second=87917.46875, timing/target_tokens_per_second_per_core=2747.4208984375, timing/uptime=5832.96, z_loss=0.007010454777628183, z_loss_per_all_target_tokens=8.91425e-09
I0513 01:22:36.954934 140095893518336 trainer.py:518] Training: step 33139
I0513 01:22:36.955361 140595318249472 trainer.py:518] Training: step 33139
I0513 01:22:36.955021 140440593786880 trainer.py:518] Training: step 33139
I0513 01:22:36.956401 140529434355712 trainer.py:518] Training: step 33139
I0513 01:22:36.955789 140311617681408 trainer.py:518] Training: step 33139
I0513 01:22:36.955111 140339465779200 trainer.py:518] Training: step 33139
I0513 01:22:36.954697 139674595104768 trainer.py:518] Training: step 33139
I0513 01:22:36.955961 140031916288000 trainer.py:518] Training: step 33139
I0513 01:22:54.846532 140439986517568 logging_writer.py:48] [33130] collection=train accuracy=0.569019, cross_ent_loss=16.602466583251953, cross_ent_loss_per_all_target_tokens=2.11111e-05, experts/auxiliary_loss=0.16071945428848267, experts/expert_usage=7.956357002258301, experts/fraction_tokens_left_behind=0.2752093970775604, experts/router_confidence=3.4710018634796143, experts/router_z_loss=0.0003143733774777502, learning_rate=0.00549446, learning_rate/current=0.00549409, loss=16.77039337158203, loss_per_all_target_tokens=2.13247e-05, loss_per_nonpadding_target_token=2.14405e-05, non_padding_fraction/loss_weights=0.994597, timing/seconds=89.44644165039062, timing/seqs=3840, timing/seqs_per_second=42.93071746826172, timing/seqs_per_second_per_core=1.3415849208831787, timing/steps_per_second=0.11179874837398529, timing/target_tokens_per_second=87922.109375, timing/target_tokens_per_second_per_core=2747.56591796875, timing/uptime=5851.05, z_loss=0.006892041768878698, z_loss_per_all_target_tokens=8.76368e-09
I0513 01:22:54.844709 139674595104768 trainer.py:518] Training: step 33145
I0513 01:22:54.845266 140031916288000 trainer.py:518] Training: step 33144
I0513 01:22:54.872567 140095893518336 trainer.py:518] Training: step 33149
I0513 01:22:54.870062 140440593786880 trainer.py:518] Training: step 33147
I0513 01:22:54.871315 140595318249472 trainer.py:518] Training: step 33148
I0513 01:22:54.874845 140529434355712 trainer.py:518] Training: step 33146
I0513 01:22:54.874025 140339465779200 trainer.py:518] Training: step 33147
I0513 01:23:03.810366 140311617681408 trainer.py:518] Training: step 33150
I0513 01:23:30.627714 140095893518336 trainer.py:518] Training: step 33152
I0513 01:23:30.627820 140529434355712 trainer.py:518] Training: step 33152
I0513 01:23:30.627899 140595318249472 trainer.py:518] Training: step 33152
I0513 01:23:30.627912 140440593786880 trainer.py:518] Training: step 33152
I0513 01:23:30.628568 140311617681408 trainer.py:518] Training: step 33152
I0513 01:23:30.628304 140339465779200 trainer.py:518] Training: step 33152
I0513 01:23:30.628120 140031916288000 trainer.py:518] Training: step 33152
I0513 01:23:30.627407 139674595104768 trainer.py:518] Training: step 33152
I0513 01:23:48.517178 140095893518336 trainer.py:518] Training: step 33153
I0513 01:23:48.517466 140595318249472 trainer.py:518] Training: step 33153
I0513 01:23:48.517723 140339465779200 trainer.py:518] Training: step 33153
I0513 01:23:48.516823 139674595104768 trainer.py:518] Training: step 33153
I0513 01:23:48.517527 140031916288000 trainer.py:518] Training: step 33153
I0513 01:23:57.462289 140529434355712 trainer.py:518] Training: step 33153
I0513 01:23:57.462233 140440593786880 trainer.py:518] Training: step 33153
I0513 01:23:57.462850 140311617681408 trainer.py:518] Training: step 33153
I0513 01:24:06.407840 140095893518336 trainer.py:518] Training: step 33154
I0513 01:24:06.408216 140595318249472 trainer.py:518] Training: step 33154
I0513 01:24:06.408844 140339465779200 trainer.py:518] Training: step 33154
I0513 01:24:06.408356 140031916288000 trainer.py:518] Training: step 33154
I0513 01:24:06.407575 139674595104768 trainer.py:518] Training: step 33154
I0513 01:24:15.353350 140529434355712 trainer.py:518] Training: step 33154
I0513 01:24:15.353407 140440593786880 trainer.py:518] Training: step 33154
I0513 01:24:15.354309 140311617681408 trainer.py:518] Training: step 33154
I0513 01:24:24.298322 140095893518336 trainer.py:518] Training: step 33155
I0513 01:24:24.298674 140595318249472 trainer.py:518] Training: step 33155
I0513 01:24:24.298825 140339465779200 trainer.py:518] Training: step 33155
I0513 01:24:24.298146 139674595104768 trainer.py:518] Training: step 33155
I0513 01:24:24.298840 140031916288000 trainer.py:518] Training: step 33155
I0513 01:24:33.246149 140440593786880 trainer.py:518] Training: step 33155
I0513 01:24:33.247566 140529434355712 trainer.py:518] Training: step 33155
I0513 01:24:33.247356 140311617681408 trainer.py:518] Training: step 33155
I0513 01:24:42.188849 140095893518336 trainer.py:518] Training: step 33156
I0513 01:24:42.189108 140595318249472 trainer.py:518] Training: step 33156
I0513 01:24:42.189583 140339465779200 trainer.py:518] Training: step 33156
I0513 01:24:42.188384 139674595104768 trainer.py:518] Training: step 33156
I0513 01:24:42.189110 140031916288000 trainer.py:518] Training: step 33156
I0513 01:24:51.134882 140440593786880 trainer.py:518] Training: step 33156
I0513 01:24:51.135905 140311617681408 trainer.py:518] Training: step 33156
I0513 01:24:51.136906 140529434355712 trainer.py:518] Training: step 33156
I0513 01:25:00.081720 140095893518336 trainer.py:518] Training: step 33157
I0513 01:25:00.081130 140595318249472 trainer.py:518] Training: step 33157
I0513 01:25:00.081983 140339465779200 trainer.py:518] Training: step 33157
I0513 01:25:00.080501 139674595104768 trainer.py:518] Training: step 33157
I0513 01:25:00.081330 140031916288000 trainer.py:518] Training: step 33157
I0513 01:25:17.972442 140095893518336 trainer.py:518] Training: step 33158
I0513 01:25:17.972336 140595318249472 trainer.py:518] Training: step 33158
I0513 01:25:17.972517 140529434355712 trainer.py:518] Training: step 33158
I0513 01:25:17.971795 140440593786880 trainer.py:518] Training: step 33158
I0513 01:25:17.972867 140311617681408 trainer.py:518] Training: step 33158
I0513 01:25:17.972594 140339465779200 trainer.py:518] Training: step 33158
I0513 01:25:17.971576 139674595104768 trainer.py:518] Training: step 33158
I0513 01:25:17.972796 140031916288000 trainer.py:518] Training: step 33158
I0513 01:25:26.916939 140439986517568 logging_writer.py:48] [33140] collection=train accuracy=0.576602, cross_ent_loss=16.195924758911133, cross_ent_loss_per_all_target_tokens=2.05942e-05, experts/auxiliary_loss=0.16073454916477203, experts/expert_usage=7.958625793457031, experts/fraction_tokens_left_behind=0.24851392209529877, experts/router_confidence=3.464752674102783, experts/router_z_loss=0.00031107556424103677, learning_rate=0.00549363, learning_rate/current=0.00549326, loss=16.36383056640625, loss_per_all_target_tokens=2.08077e-05, loss_per_nonpadding_target_token=2.09098e-05, non_padding_fraction/loss_weights=0.995117, timing/seconds=89.45149993896484, timing/seqs=3840, timing/seqs_per_second=42.92829132080078, timing/seqs_per_second_per_core=1.3415091037750244, timing/steps_per_second=0.1117924228310585, timing/target_tokens_per_second=87917.140625, timing/target_tokens_per_second_per_core=2747.41064453125, timing/uptime=6011.86, z_loss=0.006859970279037952, z_loss_per_all_target_tokens=8.7229e-09
I0513 01:25:35.864553 140095893518336 trainer.py:518] Training: step 33159
I0513 01:25:35.863962 140595318249472 trainer.py:518] Training: step 33159
I0513 01:25:35.863339 140440593786880 trainer.py:518] Training: step 33159
I0513 01:25:35.864633 140311617681408 trainer.py:518] Training: step 33159
I0513 01:25:35.864246 140339465779200 trainer.py:518] Training: step 33159
I0513 01:25:35.863635 139674595104768 trainer.py:518] Training: step 33159
I0513 01:25:35.864981 140031916288000 trainer.py:518] Training: step 33159
I0513 01:25:44.810352 140529434355712 trainer.py:518] Training: step 33160
I0513 01:25:53.754725 140595318249472 trainer.py:518] Training: step 33166
I0513 01:25:53.755771 140439986517568 logging_writer.py:48] [33150] collection=train accuracy=0.572172, cross_ent_loss=16.45435333251953, cross_ent_loss_per_all_target_tokens=2.09228e-05, experts/auxiliary_loss=0.16069087386131287, experts/expert_usage=7.956707954406738, experts/fraction_tokens_left_behind=0.2749268710613251, experts/router_confidence=3.474569082260132, experts/router_z_loss=0.0003188233240507543, learning_rate=0.00549281, learning_rate/current=0.00549243, loss=16.622190475463867, loss_per_all_target_tokens=2.11362e-05, loss_per_nonpadding_target_token=2.12816e-05, non_padding_fraction/loss_weights=0.993167, timing/seconds=89.44498443603516, timing/seqs=3840, timing/seqs_per_second=42.931419372558594, timing/seqs_per_second_per_core=1.341606855392456, timing/steps_per_second=0.11180057376623154, timing/target_tokens_per_second=87923.546875, timing/target_tokens_per_second_per_core=2747.61083984375, timing/uptime=6029.87, z_loss=0.006823583040386438, z_loss_per_all_target_tokens=8.67663e-09
I0513 01:25:53.755149 140339465779200 trainer.py:518] Training: step 33166
I0513 01:25:53.753957 139674595104768 trainer.py:518] Training: step 33165
I0513 01:25:53.754920 140031916288000 trainer.py:518] Training: step 33164
I0513 01:25:53.787087 140095893518336 trainer.py:518] Training: step 33168
I0513 01:26:02.700676 140440593786880 trainer.py:518] Training: step 33170
I0513 01:26:02.701970 140529434355712 trainer.py:518] Training: step 33170
I0513 01:26:02.701797 140311617681408 trainer.py:518] Training: step 33170
I0513 01:26:29.537792 140095893518336 trainer.py:518] Training: step 33172
I0513 01:26:29.538166 140311617681408 trainer.py:518] Training: step 33172
I0513 01:26:29.537696 140595318249472 trainer.py:518] Training: step 33172
I0513 01:26:29.537906 140529434355712 trainer.py:518] Training: step 33172
I0513 01:26:29.538241 140339465779200 trainer.py:518] Training: step 33172
I0513 01:26:29.537596 140440593786880 trainer.py:518] Training: step 33172
I0513 01:26:29.537356 139674595104768 trainer.py:518] Training: step 33172
I0513 01:26:29.537775 140031916288000 trainer.py:518] Training: step 33172
I0513 01:26:47.427977 140095893518336 trainer.py:518] Training: step 33173
I0513 01:26:47.428441 140311617681408 trainer.py:518] Training: step 33173
I0513 01:26:47.428294 140595318249472 trainer.py:518] Training: step 33173
I0513 01:26:47.428305 140529434355712 trainer.py:518] Training: step 33173
I0513 01:26:47.427899 140440593786880 trainer.py:518] Training: step 33173
I0513 01:26:47.428866 140339465779200 trainer.py:518] Training: step 33173
I0513 01:26:47.427573 139674595104768 trainer.py:518] Training: step 33173
I0513 01:26:47.428439 140031916288000 trainer.py:518] Training: step 33173
I0513 01:27:05.319604 140095893518336 trainer.py:518] Training: step 33174
I0513 01:27:05.318510 140529434355712 trainer.py:518] Training: step 33174
I0513 01:27:05.318525 140595318249472 trainer.py:518] Training: step 33174
I0513 01:27:05.317999 140440593786880 trainer.py:518] Training: step 33174
I0513 01:27:05.319132 140339465779200 trainer.py:518] Training: step 33174
I0513 01:27:05.317820 139674595104768 trainer.py:518] Training: step 33174
I0513 01:27:05.318722 140031916288000 trainer.py:518] Training: step 33174
I0513 01:27:14.263496 140311617681408 trainer.py:518] Training: step 33174
I0513 01:27:23.208842 140095893518336 trainer.py:518] Training: step 33175
I0513 01:27:23.208118 140595318249472 trainer.py:518] Training: step 33175
I0513 01:27:23.208798 140339465779200 trainer.py:518] Training: step 33175
I0513 01:27:23.207848 140440593786880 trainer.py:518] Training: step 33175
I0513 01:27:23.207830 139674595104768 trainer.py:518] Training: step 33175
I0513 01:27:23.208703 140031916288000 trainer.py:518] Training: step 33175
I0513 01:27:32.155730 140529434355712 trainer.py:518] Training: step 33175
I0513 01:27:32.157586 140311617681408 trainer.py:518] Training: step 33175
I0513 01:27:41.099935 140095893518336 trainer.py:518] Training: step 33176
I0513 01:27:41.098862 140595318249472 trainer.py:518] Training: step 33176
I0513 01:27:41.098651 140440593786880 trainer.py:518] Training: step 33176
I0513 01:27:41.099854 140339465779200 trainer.py:518] Training: step 33176
I0513 01:27:41.098626 139674595104768 trainer.py:518] Training: step 33176
I0513 01:27:41.099469 140031916288000 trainer.py:518] Training: step 33176
I0513 01:27:50.044742 140311617681408 trainer.py:518] Training: step 33176
I0513 01:27:50.046267 140529434355712 trainer.py:518] Training: step 33176
I0513 01:27:58.989505 140095893518336 trainer.py:518] Training: step 33177
I0513 01:27:58.988893 140595318249472 trainer.py:518] Training: step 33177
I0513 01:27:58.989537 140339465779200 trainer.py:518] Training: step 33177
I0513 01:27:58.988765 140440593786880 trainer.py:518] Training: step 33177
I0513 01:27:58.988353 139674595104768 trainer.py:518] Training: step 33177
I0513 01:27:58.989459 140031916288000 trainer.py:518] Training: step 33177
I0513 01:28:16.879989 140095893518336 trainer.py:518] Training: step 33178
I0513 01:28:16.879245 140595318249472 trainer.py:518] Training: step 33178
I0513 01:28:16.880391 140311617681408 trainer.py:518] Training: step 33178
I0513 01:28:16.879113 140440593786880 trainer.py:518] Training: step 33178
I0513 01:28:16.879929 140339465779200 trainer.py:518] Training: step 33178
I0513 01:28:16.880371 140529434355712 trainer.py:518] Training: step 33178
I0513 01:28:16.878937 139674595104768 trainer.py:518] Training: step 33178
I0513 01:28:16.879445 140031916288000 trainer.py:518] Training: step 33178
I0513 01:28:25.824143 140439986517568 logging_writer.py:48] [33160] collection=train accuracy=0.576803, cross_ent_loss=16.116727828979492, cross_ent_loss_per_all_target_tokens=2.04935e-05, experts/auxiliary_loss=0.1607242077589035, experts/expert_usage=7.961543560028076, experts/fraction_tokens_left_behind=0.2934894263744354, experts/router_confidence=3.4540040493011475, experts/router_z_loss=0.00030408063321374357, learning_rate=0.00549198, learning_rate/current=0.00549161, loss=16.284515380859375, loss_per_all_target_tokens=2.07068e-05, loss_per_nonpadding_target_token=2.08171e-05, non_padding_fraction/loss_weights=0.994705, timing/seconds=89.4545669555664, timing/seqs=3840, timing/seqs_per_second=42.92681884765625, timing/seqs_per_second_per_core=1.3414630889892578, timing/steps_per_second=0.11178859323263168, timing/target_tokens_per_second=87914.125, timing/target_tokens_per_second_per_core=2747.31640625, timing/uptime=6190.75, z_loss=0.006756457965821028, z_loss_per_all_target_tokens=8.59128e-09
I0513 01:28:34.772359 140095893518336 trainer.py:518] Training: step 33179
I0513 01:28:34.772407 140311617681408 trainer.py:518] Training: step 33179
I0513 01:28:34.772156 140339465779200 trainer.py:518] Training: step 33179
I0513 01:28:34.772317 140595318249472 trainer.py:518] Training: step 33179
I0513 01:28:34.771790 140440593786880 trainer.py:518] Training: step 33179
I0513 01:28:34.771743 139674595104768 trainer.py:518] Training: step 33179
I0513 01:28:34.772833 140031916288000 trainer.py:518] Training: step 33179
I0513 01:28:43.718731 140529434355712 trainer.py:518] Training: step 33180
I0513 01:28:52.662751 140095893518336 trainer.py:518] Training: step 33188
I0513 01:28:52.661337 140440593786880 trainer.py:518] Training: step 33186
I0513 01:28:52.662017 140595318249472 trainer.py:518] Training: step 33186
I0513 01:28:52.663038 140439986517568 logging_writer.py:48] [33170] collection=train accuracy=0.578565, cross_ent_loss=16.067123413085938, cross_ent_loss_per_all_target_tokens=2.04304e-05, experts/auxiliary_loss=0.1607535034418106, experts/expert_usage=7.936501979827881, experts/fraction_tokens_left_behind=0.28474894165992737, experts/router_confidence=3.4654769897460938, experts/router_z_loss=0.00031268628663383424, learning_rate=0.00549115, learning_rate/current=0.00549078, loss=16.23518180847168, loss_per_all_target_tokens=2.06441e-05, loss_per_nonpadding_target_token=2.07786e-05, non_padding_fraction/loss_weights=0.993527, timing/seconds=89.44414520263672, timing/seqs=3840, timing/seqs_per_second=42.931819915771484, timing/seqs_per_second_per_core=1.3416193723678589, timing/steps_per_second=0.1118016168475151, timing/target_tokens_per_second=87924.3671875, timing/target_tokens_per_second_per_core=2747.636474609375, timing/uptime=6208.69, z_loss=0.006991088390350342, z_loss_per_all_target_tokens=8.88963e-09
I0513 01:28:52.662208 140031916288000 trainer.py:518] Training: step 33184
I0513 01:28:52.662073 139674595104768 trainer.py:518] Training: step 33186
I0513 01:28:52.691309 140339465779200 trainer.py:518] Training: step 33188
I0513 01:29:01.607298 140311617681408 trainer.py:518] Training: step 33190
I0513 01:29:01.607586 140529434355712 trainer.py:518] Training: step 33190
I0513 01:29:28.444383 140095893518336 trainer.py:518] Training: step 33192
I0513 01:29:28.444422 140311617681408 trainer.py:518] Training: step 33192
I0513 01:29:28.443888 140529434355712 trainer.py:518] Training: step 33192
I0513 01:29:28.443708 140595318249472 trainer.py:518] Training: step 33192
I0513 01:29:28.444580 140339465779200 trainer.py:518] Training: step 33192
I0513 01:29:28.444050 140440593786880 trainer.py:518] Training: step 33192
I0513 01:29:28.443486 139674595104768 trainer.py:518] Training: step 33192
I0513 01:29:28.444272 140031916288000 trainer.py:518] Training: step 33192
I0513 01:29:46.334655 140095893518336 trainer.py:518] Training: step 33193
I0513 01:29:46.334130 140311617681408 trainer.py:518] Training: step 33193
I0513 01:29:46.333367 140595318249472 trainer.py:518] Training: step 33193
I0513 01:29:46.333737 140529434355712 trainer.py:518] Training: step 33193
I0513 01:29:46.333481 140440593786880 trainer.py:518] Training: step 33193
I0513 01:29:46.334465 140339465779200 trainer.py:518] Training: step 33193
I0513 01:29:46.333971 140031916288000 trainer.py:518] Training: step 33193
I0513 01:29:46.333678 139674595104768 trainer.py:518] Training: step 33193
I0513 01:30:04.225496 140095893518336 trainer.py:518] Training: step 33194
I0513 01:30:04.225415 140311617681408 trainer.py:518] Training: step 33194
I0513 01:30:04.224708 140595318249472 trainer.py:518] Training: step 33194
I0513 01:30:04.224659 140529434355712 trainer.py:518] Training: step 33194
I0513 01:30:04.224566 140440593786880 trainer.py:518] Training: step 33194
I0513 01:30:04.225574 140339465779200 trainer.py:518] Training: step 33194
I0513 01:30:04.224539 139674595104768 trainer.py:518] Training: step 33194
I0513 01:30:04.225088 140031916288000 trainer.py:518] Training: step 33194
I0513 01:30:22.116224 140095893518336 trainer.py:518] Training: step 33195
I0513 01:30:22.114686 140595318249472 trainer.py:518] Training: step 33195
I0513 01:30:22.116110 140311617681408 trainer.py:518] Training: step 33195
I0513 01:30:22.115277 140529434355712 trainer.py:518] Training: step 33195
I0513 01:30:22.115698 140339465779200 trainer.py:518] Training: step 33195
I0513 01:30:22.114947 140440593786880 trainer.py:518] Training: step 33195
I0513 01:30:22.114829 139674595104768 trainer.py:518] Training: step 33195
I0513 01:30:22.115527 140031916288000 trainer.py:518] Training: step 33195
I0513 01:30:40.006987 140095893518336 trainer.py:518] Training: step 33196
I0513 01:30:40.006423 140311617681408 trainer.py:518] Training: step 33196
I0513 01:30:40.005862 140529434355712 trainer.py:518] Training: step 33196
I0513 01:30:40.005880 140595318249472 trainer.py:518] Training: step 33196
I0513 01:30:40.006653 140339465779200 trainer.py:518] Training: step 33196
I0513 01:30:40.006853 140440593786880 trainer.py:518] Training: step 33196
I0513 01:30:40.005733 139674595104768 trainer.py:518] Training: step 33196
I0513 01:30:40.007107 140031916288000 trainer.py:518] Training: step 33196
I0513 01:30:57.897457 140095893518336 trainer.py:518] Training: step 33197
I0513 01:30:57.897237 140311617681408 trainer.py:518] Training: step 33197
I0513 01:30:57.896620 140595318249472 trainer.py:518] Training: step 33197
I0513 01:30:57.896207 140440593786880 trainer.py:518] Training: step 33197
I0513 01:30:57.897224 140529434355712 trainer.py:518] Training: step 33197
I0513 01:30:57.897622 140339465779200 trainer.py:518] Training: step 33197
I0513 01:30:57.896272 139674595104768 trainer.py:518] Training: step 33197
I0513 01:30:57.896935 140031916288000 trainer.py:518] Training: step 33197
I0513 01:31:15.789220 140095893518336 trainer.py:518] Training: step 33198
I0513 01:31:15.787891 140529434355712 trainer.py:518] Training: step 33198
I0513 01:31:15.788846 140311617681408 trainer.py:518] Training: step 33198
I0513 01:31:15.788162 140595318249472 trainer.py:518] Training: step 33198
I0513 01:31:15.788033 140440593786880 trainer.py:518] Training: step 33198
I0513 01:31:15.788992 140339465779200 trainer.py:518] Training: step 33198
I0513 01:31:15.787796 139674595104768 trainer.py:518] Training: step 33198
I0513 01:31:15.788774 140031916288000 trainer.py:518] Training: step 33198
I0513 01:31:24.734580 140439986517568 logging_writer.py:48] [33180] collection=train accuracy=0.573158, cross_ent_loss=16.39739990234375, cross_ent_loss_per_all_target_tokens=2.08504e-05, experts/auxiliary_loss=0.1607292741537094, experts/expert_usage=7.950247287750244, experts/fraction_tokens_left_behind=0.286196231842041, experts/router_confidence=3.4719808101654053, experts/router_z_loss=0.0003092931292485446, learning_rate=0.00549032, learning_rate/current=0.00548995, loss=16.564943313598633, loss_per_all_target_tokens=2.10634e-05, loss_per_nonpadding_target_token=2.11857e-05, non_padding_fraction/loss_weights=0.994229, timing/seconds=89.45216369628906, timing/seqs=3840, timing/seqs_per_second=42.92797088623047, timing/seqs_per_second_per_core=1.3414990901947021, timing/steps_per_second=0.11179159581661224, timing/target_tokens_per_second=87916.484375, timing/target_tokens_per_second_per_core=2747.39013671875, timing/uptime=6369.66, z_loss=0.006501877214759588, z_loss_per_all_target_tokens=8.26756e-09
I0513 01:31:33.680246 140095893518336 trainer.py:518] Training: step 33199
I0513 01:31:33.679632 140311617681408 trainer.py:518] Training: step 33199
I0513 01:31:33.678783 140595318249472 trainer.py:518] Training: step 33199
I0513 01:31:33.679430 140339465779200 trainer.py:518] Training: step 33199
I0513 01:31:33.678722 140440593786880 trainer.py:518] Training: step 33199
I0513 01:31:33.680116 140529434355712 trainer.py:518] Training: step 33199
I0513 01:31:33.678612 139674595104768 trainer.py:518] Training: step 33199
I0513 01:31:33.680166 140031916288000 trainer.py:518] Training: step 33199
I0513 01:31:51.569320 140595318249472 trainer.py:518] Training: step 33206
I0513 01:31:51.570116 140339465779200 trainer.py:518] Training: step 33206
I0513 01:31:51.570535 140439986517568 logging_writer.py:48] [33190] collection=train accuracy=0.581126, cross_ent_loss=15.957061767578125, cross_ent_loss_per_all_target_tokens=2.02905e-05, experts/auxiliary_loss=0.16072973608970642, experts/expert_usage=7.952467441558838, experts/fraction_tokens_left_behind=0.2826702296733856, experts/router_confidence=3.4546055793762207, experts/router_z_loss=0.00031302234856411815, learning_rate=0.00548949, learning_rate/current=0.00548912, loss=16.124658584594727, loss_per_all_target_tokens=2.05036e-05, loss_per_nonpadding_target_token=2.06372e-05, non_padding_fraction/loss_weights=0.993525, timing/seconds=89.4462890625, timing/seqs=3840, timing/seqs_per_second=42.93079376220703, timing/seqs_per_second_per_core=1.3415873050689697, timing/steps_per_second=0.11179894208908081, timing/target_tokens_per_second=87922.265625, timing/target_tokens_per_second_per_core=2747.57080078125, timing/uptime=6387.59, z_loss=0.006555541884154081, z_loss_per_all_target_tokens=8.3358e-09
I0513 01:31:51.592630 140440593786880 trainer.py:518] Training: step 33205
I0513 01:31:51.597430 140529434355712 trainer.py:518] Training: step 33204
I0513 01:31:51.599527 140311617681408 trainer.py:518] Training: step 33207
I0513 01:31:51.593677 139674595104768 trainer.py:518] Training: step 33207
I0513 01:31:51.595916 140031916288000 trainer.py:518] Training: step 33204
I0513 01:31:51.974299 140095893518336 trainer.py:518] Training: step 33208
I0513 01:32:27.353091 140095893518336 trainer.py:518] Training: step 33212
I0513 01:32:27.352068 140595318249472 trainer.py:518] Training: step 33212
I0513 01:32:27.351829 140529434355712 trainer.py:518] Training: step 33212
I0513 01:32:27.353028 140311617681408 trainer.py:518] Training: step 33212
I0513 01:32:27.352069 140440593786880 trainer.py:518] Training: step 33212
I0513 01:32:27.352932 140339465779200 trainer.py:518] Training: step 33212
I0513 01:32:27.352128 139674595104768 trainer.py:518] Training: step 33212
I0513 01:32:27.352622 140031916288000 trainer.py:518] Training: step 33212
I0513 01:32:45.242831 140095893518336 trainer.py:518] Training: step 33213
I0513 01:32:45.241406 140529434355712 trainer.py:518] Training: step 33213
I0513 01:32:45.242528 140311617681408 trainer.py:518] Training: step 33213
I0513 01:32:45.241707 140595318249472 trainer.py:518] Training: step 33213
I0513 01:32:45.241713 140440593786880 trainer.py:518] Training: step 33213
I0513 01:32:45.242782 140339465779200 trainer.py:518] Training: step 33213
I0513 01:32:45.241422 139674595104768 trainer.py:518] Training: step 33213
I0513 01:32:45.242443 140031916288000 trainer.py:518] Training: step 33213
I0513 01:33:03.133028 140095893518336 trainer.py:518] Training: step 33214
I0513 01:33:03.131718 140529434355712 trainer.py:518] Training: step 33214
I0513 01:33:03.132632 140311617681408 trainer.py:518] Training: step 33214
I0513 01:33:03.132057 140595318249472 trainer.py:518] Training: step 33214
I0513 01:33:03.131695 140440593786880 trainer.py:518] Training: step 33214
I0513 01:33:03.132902 140339465779200 trainer.py:518] Training: step 33214
I0513 01:33:03.131689 139674595104768 trainer.py:518] Training: step 33214
I0513 01:33:03.132381 140031916288000 trainer.py:518] Training: step 33214
I0513 01:33:21.023863 140095893518336 trainer.py:518] Training: step 33215
I0513 01:33:21.023068 140529434355712 trainer.py:518] Training: step 33215
I0513 01:33:21.023017 140595318249472 trainer.py:518] Training: step 33215
I0513 01:33:21.024302 140311617681408 trainer.py:518] Training: step 33215
I0513 01:33:21.022885 140440593786880 trainer.py:518] Training: step 33215
I0513 01:33:21.023975 140339465779200 trainer.py:518] Training: step 33215
I0513 01:33:21.022871 139674595104768 trainer.py:518] Training: step 33215
I0513 01:33:21.023757 140031916288000 trainer.py:518] Training: step 33215
I0513 01:33:38.915285 140095893518336 trainer.py:518] Training: step 33216
I0513 01:33:38.914162 140529434355712 trainer.py:518] Training: step 33216
I0513 01:33:38.913928 140595318249472 trainer.py:518] Training: step 33216
I0513 01:33:38.915132 140311617681408 trainer.py:518] Training: step 33216
I0513 01:33:38.913829 140440593786880 trainer.py:518] Training: step 33216
I0513 01:33:38.915260 140339465779200 trainer.py:518] Training: step 33216
I0513 01:33:38.914061 139674595104768 trainer.py:518] Training: step 33216
I0513 01:33:38.914925 140031916288000 trainer.py:518] Training: step 33216
I0513 01:33:56.806159 140095893518336 trainer.py:518] Training: step 33217
I0513 01:33:56.805457 140595318249472 trainer.py:518] Training: step 33217
I0513 01:33:56.805279 140440593786880 trainer.py:518] Training: step 33217
I0513 01:33:56.807034 140311617681408 trainer.py:518] Training: step 33217
I0513 01:33:56.806366 140339465779200 trainer.py:518] Training: step 33217
I0513 01:33:56.805256 139674595104768 trainer.py:518] Training: step 33217
I0513 01:33:56.806261 140031916288000 trainer.py:518] Training: step 33217
I0513 01:34:05.751710 140529434355712 trainer.py:518] Training: step 33218
I0513 01:34:14.697277 140095893518336 trainer.py:518] Training: step 33218
I0513 01:34:14.696050 140595318249472 trainer.py:518] Training: step 33218
I0513 01:34:14.697430 140311617681408 trainer.py:518] Training: step 33218
I0513 01:34:14.695994 140440593786880 trainer.py:518] Training: step 33218
I0513 01:34:14.696990 140339465779200 trainer.py:518] Training: step 33218
I0513 01:34:14.696736 140031916288000 trainer.py:518] Training: step 33218
I0513 01:34:14.696276 139674595104768 trainer.py:518] Training: step 33218
I0513 01:34:23.643100 140529434355712 trainer.py:518] Training: step 33219
I0513 01:34:23.645174 140439986517568 logging_writer.py:48] [33200] collection=train accuracy=0.572096, cross_ent_loss=16.397401809692383, cross_ent_loss_per_all_target_tokens=2.08504e-05, experts/auxiliary_loss=0.16070285439491272, experts/expert_usage=7.9596405029296875, experts/fraction_tokens_left_behind=0.24128758907318115, experts/router_confidence=3.4800918102264404, experts/router_z_loss=0.00030428869649767876, learning_rate=0.00548867, learning_rate/current=0.0054883, loss=16.56488037109375, loss_per_all_target_tokens=2.10633e-05, loss_per_nonpadding_target_token=2.1146e-05, non_padding_fraction/loss_weights=0.996089, timing/seconds=89.45207977294922, timing/seqs=3840, timing/seqs_per_second=42.92801284790039, timing/seqs_per_second_per_core=1.3415004014968872, timing/steps_per_second=0.1117917001247406, timing/target_tokens_per_second=87916.5703125, timing/target_tokens_per_second_per_core=2747.392822265625, timing/uptime=6548.58, z_loss=0.006474003195762634, z_loss_per_all_target_tokens=8.23212e-09
I0513 01:34:32.588843 140095893518336 trainer.py:518] Training: step 33219
I0513 01:34:32.588859 140311617681408 trainer.py:518] Training: step 33219
I0513 01:34:32.588646 140339465779200 trainer.py:518] Training: step 33219
I0513 01:34:32.588592 140595318249472 trainer.py:518] Training: step 33219
I0513 01:34:32.588244 140440593786880 trainer.py:518] Training: step 33219
I0513 01:34:32.588115 139674595104768 trainer.py:518] Training: step 33219
I0513 01:34:32.589407 140031916288000 trainer.py:518] Training: step 33219
I0513 01:34:50.479094 140529434355712 trainer.py:518] Training: step 33220
I0513 01:34:50.479012 140440593786880 trainer.py:518] Training: step 33226
I0513 01:34:50.480296 140339465779200 trainer.py:518] Training: step 33226
I0513 01:34:50.486677 140439986517568 logging_writer.py:48] [33210] collection=train accuracy=0.571258, cross_ent_loss=16.474599838256836, cross_ent_loss_per_all_target_tokens=2.09485e-05, experts/auxiliary_loss=0.16071221232414246, experts/expert_usage=7.955741882324219, experts/fraction_tokens_left_behind=0.240049347281456, experts/router_confidence=3.47727632522583, experts/router_z_loss=0.00030721831717528403, learning_rate=0.00548784, learning_rate/current=0.00548747, loss=16.64273452758789, loss_per_all_target_tokens=2.11623e-05, loss_per_nonpadding_target_token=2.12611e-05, non_padding_fraction/loss_weights=0.995352, timing/seconds=89.4472427368164, timing/seqs=3840, timing/seqs_per_second=42.930335998535156, timing/seqs_per_second_per_core=1.3415729999542236, timing/steps_per_second=0.1117977499961853, timing/target_tokens_per_second=87921.328125, timing/target_tokens_per_second_per_core=2747.54150390625, timing/uptime=6566.66, z_loss=0.00711441645398736, z_loss_per_all_target_tokens=9.04645e-09
I0513 01:34:50.479383 139674595104768 trainer.py:518] Training: step 33226
I0513 01:34:50.508486 140095893518336 trainer.py:518] Training: step 33229
I0513 01:34:50.506682 140595318249472 trainer.py:518] Training: step 33227
I0513 01:34:50.509335 140311617681408 trainer.py:518] Training: step 33228
I0513 01:34:50.506205 140031916288000 trainer.py:518] Training: step 33224
I0513 01:35:26.262229 140095893518336 trainer.py:518] Training: step 33232
I0513 01:35:26.261138 140529434355712 trainer.py:518] Training: step 33232
I0513 01:35:26.262182 140311617681408 trainer.py:518] Training: step 33232
I0513 01:35:26.261426 140595318249472 trainer.py:518] Training: step 33232
I0513 01:35:26.261111 140440593786880 trainer.py:518] Training: step 33232
I0513 01:35:26.262215 140339465779200 trainer.py:518] Training: step 33232
I0513 01:35:26.261159 139674595104768 trainer.py:518] Training: step 33232
I0513 01:35:26.262325 140031916288000 trainer.py:518] Training: step 33232
I0513 01:35:44.151587 140095893518336 trainer.py:518] Training: step 33233
I0513 01:35:44.150389 140529434355712 trainer.py:518] Training: step 33233
I0513 01:35:44.150599 140595318249472 trainer.py:518] Training: step 33233
I0513 01:35:44.150258 140440593786880 trainer.py:518] Training: step 33233
I0513 01:35:44.151763 140311617681408 trainer.py:518] Training: step 33233
I0513 01:35:44.151311 140339465779200 trainer.py:518] Training: step 33233
I0513 01:35:44.150374 139674595104768 trainer.py:518] Training: step 33233
I0513 01:35:44.150943 140031916288000 trainer.py:518] Training: step 33233
I0513 01:36:02.041701 140095893518336 trainer.py:518] Training: step 33234
I0513 01:36:02.040509 140529434355712 trainer.py:518] Training: step 33234
I0513 01:36:02.041015 140595318249472 trainer.py:518] Training: step 33234
I0513 01:36:02.042433 140311617681408 trainer.py:518] Training: step 33234
I0513 01:36:02.040837 140440593786880 trainer.py:518] Training: step 33234
I0513 01:36:02.041978 140339465779200 trainer.py:518] Training: step 33234
I0513 01:36:02.041302 140031916288000 trainer.py:518] Training: step 33234
I0513 01:36:02.040946 139674595104768 trainer.py:518] Training: step 33234
I0513 01:36:19.932594 140095893518336 trainer.py:518] Training: step 33235
I0513 01:36:19.931304 140529434355712 trainer.py:518] Training: step 33235
I0513 01:36:19.932759 140311617681408 trainer.py:518] Training: step 33235
I0513 01:36:19.931635 140595318249472 trainer.py:518] Training: step 33235
I0513 01:36:19.931383 140440593786880 trainer.py:518] Training: step 33235
I0513 01:36:19.931654 139674595104768 trainer.py:518] Training: step 33235
I0513 01:36:19.932089 140031916288000 trainer.py:518] Training: step 33235
I0513 01:36:28.881070 140339465779200 trainer.py:518] Training: step 33235
I0513 01:36:37.823800 140095893518336 trainer.py:518] Training: step 33236
I0513 01:36:37.823150 140529434355712 trainer.py:518] Training: step 33236
I0513 01:36:37.823974 140311617681408 trainer.py:518] Training: step 33236
I0513 01:36:37.823149 140595318249472 trainer.py:518] Training: step 33236
I0513 01:36:37.823042 140440593786880 trainer.py:518] Training: step 33236
I0513 01:36:37.822818 139674595104768 trainer.py:518] Training: step 33236
I0513 01:36:37.823481 140031916288000 trainer.py:518] Training: step 33236
I0513 01:36:46.769427 140339465779200 trainer.py:518] Training: step 33236
I0513 01:36:55.713936 140095893518336 trainer.py:518] Training: step 33237
I0513 01:36:55.714214 140311617681408 trainer.py:518] Training: step 33237
I0513 01:36:55.713494 140595318249472 trainer.py:518] Training: step 33237
I0513 01:36:55.713131 140440593786880 trainer.py:518] Training: step 33237
I0513 01:36:55.714341 140529434355712 trainer.py:518] Training: step 33237
I0513 01:36:55.713127 139674595104768 trainer.py:518] Training: step 33237
I0513 01:36:55.713794 140031916288000 trainer.py:518] Training: step 33237
I0513 01:37:13.604493 140095893518336 trainer.py:518] Training: step 33238
I0513 01:37:13.604738 140311617681408 trainer.py:518] Training: step 33238
I0513 01:37:13.603839 140595318249472 trainer.py:518] Training: step 33238
I0513 01:37:13.604332 140529434355712 trainer.py:518] Training: step 33238
I0513 01:37:13.603671 140440593786880 trainer.py:518] Training: step 33238
I0513 01:37:13.604672 140339465779200 trainer.py:518] Training: step 33238
I0513 01:37:13.603364 139674595104768 trainer.py:518] Training: step 33238
I0513 01:37:13.604274 140031916288000 trainer.py:518] Training: step 33238
I0513 01:37:22.550297 140439986517568 logging_writer.py:48] [33220] collection=train accuracy=0.5683, cross_ent_loss=16.50140953063965, cross_ent_loss_per_all_target_tokens=2.09826e-05, experts/auxiliary_loss=0.1607096642255783, experts/expert_usage=7.9490966796875, experts/fraction_tokens_left_behind=0.24012818932533264, experts/router_confidence=3.4738967418670654, experts/router_z_loss=0.000306975853163749, learning_rate=0.00548702, learning_rate/current=0.00548664, loss=16.669597625732422, loss_per_all_target_tokens=2.11965e-05, loss_per_nonpadding_target_token=2.13e-05, non_padding_fraction/loss_weights=0.99514, timing/seconds=89.45352935791016, timing/seqs=3840, timing/seqs_per_second=42.92731857299805, timing/seqs_per_second_per_core=1.341478705406189, timing/steps_per_second=0.11178988963365555, timing/target_tokens_per_second=87915.1484375, timing/target_tokens_per_second_per_core=2747.348388671875, timing/uptime=6727.48, z_loss=0.007169873453676701, z_loss_per_all_target_tokens=9.11697e-09
I0513 01:37:31.495883 140095893518336 trainer.py:518] Training: step 33239
I0513 01:37:31.495990 140311617681408 trainer.py:518] Training: step 33239
I0513 01:37:31.495123 140595318249472 trainer.py:518] Training: step 33239
I0513 01:37:31.495591 140339465779200 trainer.py:518] Training: step 33239
I0513 01:37:31.495692 140529434355712 trainer.py:518] Training: step 33239
I0513 01:37:31.494798 140440593786880 trainer.py:518] Training: step 33239
I0513 01:37:31.495019 139674595104768 trainer.py:518] Training: step 33239
I0513 01:37:31.496326 140031916288000 trainer.py:518] Training: step 33239
I0513 01:37:49.386873 140095893518336 trainer.py:518] Training: step 33247
I0513 01:37:49.385497 140595318249472 trainer.py:518] Training: step 33246
I0513 01:37:49.386547 140439986517568 logging_writer.py:48] [33230] collection=train accuracy=0.571821, cross_ent_loss=16.33757209777832, cross_ent_loss_per_all_target_tokens=2.07743e-05, experts/auxiliary_loss=0.16070879995822906, experts/expert_usage=7.954638957977295, experts/fraction_tokens_left_behind=0.26805379986763, experts/router_confidence=3.4587459564208984, experts/router_z_loss=0.00032364935032092035, learning_rate=0.00548619, learning_rate/current=0.00548582, loss=16.505420684814453, loss_per_all_target_tokens=2.09877e-05, loss_per_nonpadding_target_token=2.11766e-05, non_padding_fraction/loss_weights=0.991082, timing/seconds=89.44750213623047, timing/seqs=3840, timing/seqs_per_second=42.93021011352539, timing/seqs_per_second_per_core=1.3415690660476685, timing/steps_per_second=0.11179742217063904, timing/target_tokens_per_second=87921.0703125, timing/target_tokens_per_second_per_core=2747.533447265625, timing/uptime=6745.7, z_loss=0.006815516855567694, z_loss_per_all_target_tokens=8.66638e-09
I0513 01:37:49.385654 140031916288000 trainer.py:518] Training: step 33244
I0513 01:37:49.385705 139674595104768 trainer.py:518] Training: step 33247
I0513 01:37:49.409860 140440593786880 trainer.py:518] Training: step 33247
I0513 01:37:49.414670 140529434355712 trainer.py:518] Training: step 33245
I0513 01:37:49.415065 140311617681408 trainer.py:518] Training: step 33247
I0513 01:37:58.347896 140339465779200 trainer.py:518] Training: step 33250
I0513 01:38:25.167787 140095893518336 trainer.py:518] Training: step 33252
I0513 01:38:25.166718 140529434355712 trainer.py:518] Training: step 33252
I0513 01:38:25.167993 140311617681408 trainer.py:518] Training: step 33252
I0513 01:38:25.167064 140595318249472 trainer.py:518] Training: step 33252
I0513 01:38:25.166890 140440593786880 trainer.py:518] Training: step 33252
I0513 01:38:25.167862 140339465779200 trainer.py:518] Training: step 33252
I0513 01:38:25.167544 140031916288000 trainer.py:518] Training: step 33252
I0513 01:38:25.167056 139674595104768 trainer.py:518] Training: step 33252
I0513 01:38:43.057395 140095893518336 trainer.py:518] Training: step 33253
I0513 01:38:43.056246 140529434355712 trainer.py:518] Training: step 33253
I0513 01:38:43.057207 140311617681408 trainer.py:518] Training: step 33253
I0513 01:38:43.056571 140595318249472 trainer.py:518] Training: step 33253
I0513 01:38:43.056052 140440593786880 trainer.py:518] Training: step 33253
I0513 01:38:43.057065 140339465779200 trainer.py:518] Training: step 33253
I0513 01:38:43.056027 139674595104768 trainer.py:518] Training: step 33253
I0513 01:38:43.056812 140031916288000 trainer.py:518] Training: step 33253
I0513 01:39:00.947288 140095893518336 trainer.py:518] Training: step 33254
I0513 01:39:00.946224 140529434355712 trainer.py:518] Training: step 33254
I0513 01:39:00.947466 140311617681408 trainer.py:518] Training: step 33254
I0513 01:39:00.946772 140595318249472 trainer.py:518] Training: step 33254
I0513 01:39:00.946320 140440593786880 trainer.py:518] Training: step 33254
I0513 01:39:00.947380 140339465779200 trainer.py:518] Training: step 33254
I0513 01:39:00.946000 139674595104768 trainer.py:518] Training: step 33254
I0513 01:39:00.946828 140031916288000 trainer.py:518] Training: step 33254
I0513 01:39:18.839620 140095893518336 trainer.py:518] Training: step 33255
I0513 01:39:18.838324 140529434355712 trainer.py:518] Training: step 33255
I0513 01:39:18.838527 140595318249472 trainer.py:518] Training: step 33255
I0513 01:39:18.838516 140440593786880 trainer.py:518] Training: step 33255
I0513 01:39:18.839408 140339465779200 trainer.py:518] Training: step 33255
I0513 01:39:18.838144 139674595104768 trainer.py:518] Training: step 33255
I0513 01:39:18.839248 140031916288000 trainer.py:518] Training: step 33255
I0513 01:39:27.783262 140311617681408 trainer.py:518] Training: step 33255
I0513 01:39:36.729649 140095893518336 trainer.py:518] Training: step 33256
I0513 01:39:36.728587 140595318249472 trainer.py:518] Training: step 33256
I0513 01:39:36.729232 140529434355712 trainer.py:518] Training: step 33256
I0513 01:39:36.728354 140440593786880 trainer.py:518] Training: step 33256
I0513 01:39:36.729364 140339465779200 trainer.py:518] Training: step 33256
I0513 01:39:36.728374 139674595104768 trainer.py:518] Training: step 33256
I0513 01:39:36.729356 140031916288000 trainer.py:518] Training: step 33256
I0513 01:39:45.679723 140311617681408 trainer.py:518] Training: step 33256
I0513 01:39:54.620174 140095893518336 trainer.py:518] Training: step 33257
I0513 01:39:54.619791 140529434355712 trainer.py:518] Training: step 33257
I0513 01:39:54.619534 140595318249472 trainer.py:518] Training: step 33257
I0513 01:39:54.618945 140440593786880 trainer.py:518] Training: step 33257
I0513 01:39:54.619841 140339465779200 trainer.py:518] Training: step 33257
I0513 01:39:54.619265 139674595104768 trainer.py:518] Training: step 33257
I0513 01:39:54.619662 140031916288000 trainer.py:518] Training: step 33257
I0513 01:40:12.511008 140095893518336 trainer.py:518] Training: step 33258
I0513 01:40:12.509873 140595318249472 trainer.py:518] Training: step 33258
I0513 01:40:12.511371 140311617681408 trainer.py:518] Training: step 33258
I0513 01:40:12.510610 140529434355712 trainer.py:518] Training: step 33258
I0513 01:40:12.509819 140440593786880 trainer.py:518] Training: step 33258
I0513 01:40:12.510688 140339465779200 trainer.py:518] Training: step 33258
I0513 01:40:12.510248 140031916288000 trainer.py:518] Training: step 33258
I0513 01:40:12.510224 139674595104768 trainer.py:518] Training: step 33258
I0513 01:40:21.456796 140439986517568 logging_writer.py:48] [33240] collection=train accuracy=0.578132, cross_ent_loss=16.123403549194336, cross_ent_loss_per_all_target_tokens=2.0502e-05, experts/auxiliary_loss=0.16074280440807343, experts/expert_usage=7.945272922515869, experts/fraction_tokens_left_behind=0.28230270743370056, experts/router_confidence=3.4443156719207764, experts/router_z_loss=0.0003138293686788529, learning_rate=0.00548536, learning_rate/current=0.00548499, loss=16.291521072387695, loss_per_all_target_tokens=2.07157e-05, loss_per_nonpadding_target_token=2.08543e-05, non_padding_fraction/loss_weights=0.993358, timing/seconds=89.44996643066406, timing/seqs=3840, timing/seqs_per_second=42.92902755737305, timing/seqs_per_second_per_core=1.3415321111679077, timing/steps_per_second=0.11179433763027191, timing/target_tokens_per_second=87918.6484375, timing/target_tokens_per_second_per_core=2747.457763671875, timing/uptime=6906.4, z_loss=0.007060475181788206, z_loss_per_all_target_tokens=8.97786e-09
I0513 01:40:30.402155 140095893518336 trainer.py:518] Training: step 33259
I0513 01:40:30.401092 140595318249472 trainer.py:518] Training: step 33259
I0513 01:40:30.402127 140311617681408 trainer.py:518] Training: step 33259
I0513 01:40:30.401497 140339465779200 trainer.py:518] Training: step 33259
I0513 01:40:30.401333 140440593786880 trainer.py:518] Training: step 33259
I0513 01:40:30.402266 140529434355712 trainer.py:518] Training: step 33259
I0513 01:40:30.401822 140031916288000 trainer.py:518] Training: step 33259
I0513 01:40:30.401281 139674595104768 trainer.py:518] Training: step 33259
I0513 01:40:48.291998 140339465779200 trainer.py:518] Training: step 33266
I0513 01:40:48.290911 140440593786880 trainer.py:518] Training: step 33266
I0513 01:40:48.292221 140439986517568 logging_writer.py:48] [33250] collection=train accuracy=0.57799, cross_ent_loss=16.131025314331055, cross_ent_loss_per_all_target_tokens=2.05117e-05, experts/auxiliary_loss=0.16076314449310303, experts/expert_usage=7.937345027923584, experts/fraction_tokens_left_behind=0.23735235631465912, experts/router_confidence=3.472309112548828, experts/router_z_loss=0.0003139187174383551, learning_rate=0.00548454, learning_rate/current=0.00548417, loss=16.29842185974121, loss_per_all_target_tokens=2.07245e-05, loss_per_nonpadding_target_token=2.08484e-05, non_padding_fraction/loss_weights=0.994057, timing/seconds=89.44725036621094, timing/seqs=3840, timing/seqs_per_second=42.93033218383789, timing/seqs_per_second_per_core=1.341572880744934, timing/steps_per_second=0.1117977425456047, timing/target_tokens_per_second=87921.3203125, timing/target_tokens_per_second_per_core=2747.541259765625, timing/uptime=6924.45, z_loss=0.00632019666954875, z_loss_per_all_target_tokens=8.03655e-09
I0513 01:40:48.318037 140595318249472 trainer.py:518] Training: step 33266
I0513 01:40:48.321011 140529434355712 trainer.py:518] Training: step 33264
I0513 01:40:48.317190 139674595104768 trainer.py:518] Training: step 33267
I0513 01:40:48.318546 140031916288000 trainer.py:518] Training: step 33264
I0513 01:40:57.237575 140095893518336 trainer.py:518] Training: step 33270
I0513 01:40:57.237475 140311617681408 trainer.py:518] Training: step 33270
I0513 01:41:24.073403 140095893518336 trainer.py:518] Training: step 33272
I0513 01:41:24.072528 140529434355712 trainer.py:518] Training: step 33272
I0513 01:41:24.073646 140311617681408 trainer.py:518] Training: step 33272
I0513 01:41:24.072621 140595318249472 trainer.py:518] Training: step 33272
I0513 01:41:24.072232 140440593786880 trainer.py:518] Training: step 33272
I0513 01:41:24.073481 140339465779200 trainer.py:518] Training: step 33272
I0513 01:41:24.072451 139674595104768 trainer.py:518] Training: step 33272
I0513 01:41:24.073573 140031916288000 trainer.py:518] Training: step 33272
I0513 01:41:41.963515 140095893518336 trainer.py:518] Training: step 33273
I0513 01:41:41.962171 140529434355712 trainer.py:518] Training: step 33273
I0513 01:41:41.962349 140595318249472 trainer.py:518] Training: step 33273
I0513 01:41:41.963572 140311617681408 trainer.py:518] Training: step 33273
I0513 01:41:41.962133 140440593786880 trainer.py:518] Training: step 33273
I0513 01:41:41.963063 140339465779200 trainer.py:518] Training: step 33273
I0513 01:41:41.962155 139674595104768 trainer.py:518] Training: step 33273
I0513 01:41:41.963033 140031916288000 trainer.py:518] Training: step 33273
I0513 01:41:59.853556 140095893518336 trainer.py:518] Training: step 33274
I0513 01:41:59.852421 140529434355712 trainer.py:518] Training: step 33274
I0513 01:41:59.852598 140595318249472 trainer.py:518] Training: step 33274
I0513 01:41:59.853998 140311617681408 trainer.py:518] Training: step 33274
I0513 01:41:59.852493 140440593786880 trainer.py:518] Training: step 33274
I0513 01:41:59.852735 139674595104768 trainer.py:518] Training: step 33274
I0513 01:41:59.853325 140031916288000 trainer.py:518] Training: step 33274
I0513 01:42:08.799075 140339465779200 trainer.py:518] Training: step 33274
I0513 01:42:17.744445 140095893518336 trainer.py:518] Training: step 33275
I0513 01:42:17.743215 140595318249472 trainer.py:518] Training: step 33275
I0513 01:42:17.743517 140529434355712 trainer.py:518] Training: step 33275
I0513 01:42:17.743291 140440593786880 trainer.py:518] Training: step 33275
I0513 01:42:17.743401 139674595104768 trainer.py:518] Training: step 33275
I0513 01:42:17.744075 140031916288000 trainer.py:518] Training: step 33275
I0513 01:42:26.689227 140311617681408 trainer.py:518] Training: step 33275
I0513 01:42:26.692587 140339465779200 trainer.py:518] Training: step 33275
I0513 01:42:35.635708 140095893518336 trainer.py:518] Training: step 33276
I0513 01:42:35.634384 140595318249472 trainer.py:518] Training: step 33276
I0513 01:42:35.634838 140529434355712 trainer.py:518] Training: step 33276
I0513 01:42:35.634676 140440593786880 trainer.py:518] Training: step 33276
I0513 01:42:35.634444 139674595104768 trainer.py:518] Training: step 33276
I0513 01:42:35.635599 140031916288000 trainer.py:518] Training: step 33276
I0513 01:42:44.582037 140311617681408 trainer.py:518] Training: step 33276
I0513 01:42:44.582019 140339465779200 trainer.py:518] Training: step 33276
I0513 01:42:53.526329 140095893518336 trainer.py:518] Training: step 33277
I0513 01:42:53.525850 140595318249472 trainer.py:518] Training: step 33277
I0513 01:42:53.525578 140440593786880 trainer.py:518] Training: step 33277
I0513 01:42:53.526465 140529434355712 trainer.py:518] Training: step 33277
I0513 01:42:53.525514 139674595104768 trainer.py:518] Training: step 33277
I0513 01:42:53.526360 140031916288000 trainer.py:518] Training: step 33277
I0513 01:43:11.417366 140095893518336 trainer.py:518] Training: step 33278
I0513 01:43:11.416113 140595318249472 trainer.py:518] Training: step 33278
I0513 01:43:11.417434 140311617681408 trainer.py:518] Training: step 33278
I0513 01:43:11.416911 140529434355712 trainer.py:518] Training: step 33278
I0513 01:43:11.416277 140440593786880 trainer.py:518] Training: step 33278
I0513 01:43:11.417469 140339465779200 trainer.py:518] Training: step 33278
I0513 01:43:11.415925 139674595104768 trainer.py:518] Training: step 33278
I0513 01:43:11.417004 140031916288000 trainer.py:518] Training: step 33278
I0513 01:43:20.362197 140439986517568 logging_writer.py:48] [33260] collection=train accuracy=0.568355, cross_ent_loss=16.579364776611328, cross_ent_loss_per_all_target_tokens=2.10818e-05, experts/auxiliary_loss=0.16073976457118988, experts/expert_usage=7.956043243408203, experts/fraction_tokens_left_behind=0.27463290095329285, experts/router_confidence=3.460355520248413, experts/router_z_loss=0.00031752861104905605, learning_rate=0.00548371, learning_rate/current=0.00548334, loss=16.747346878051758, loss_per_all_target_tokens=2.12954e-05, loss_per_nonpadding_target_token=2.14398e-05, non_padding_fraction/loss_weights=0.993262, timing/seconds=89.45123291015625, timing/seqs=3840, timing/seqs_per_second=42.92842102050781, timing/seqs_per_second_per_core=1.3415131568908691, timing/steps_per_second=0.11179275810718536, timing/target_tokens_per_second=87917.40625, timing/target_tokens_per_second_per_core=2747.4189453125, timing/uptime=7085.31, z_loss=0.006924437824636698, z_loss_per_all_target_tokens=8.80488e-09
I0513 01:43:29.309779 140095893518336 trainer.py:518] Training: step 33279
I0513 01:43:29.307129 140595318249472 trainer.py:518] Training: step 33279
I0513 01:43:29.308414 140311617681408 trainer.py:518] Training: step 33279
I0513 01:43:29.306775 140440593786880 trainer.py:518] Training: step 33279
I0513 01:43:29.307632 140339465779200 trainer.py:518] Training: step 33279
I0513 01:43:29.308621 140529434355712 trainer.py:518] Training: step 33279
I0513 01:43:29.306985 139674595104768 trainer.py:518] Training: step 33279
I0513 01:43:29.308537 140031916288000 trainer.py:518] Training: step 33279
I0513 01:43:47.196639 140595318249472 trainer.py:518] Training: step 33286
I0513 01:43:47.197799 140439986517568 logging_writer.py:48] [33270] collection=train accuracy=0.570717, cross_ent_loss=16.503477096557617, cross_ent_loss_per_all_target_tokens=2.09853e-05, experts/auxiliary_loss=0.16076302528381348, experts/expert_usage=7.938881874084473, experts/fraction_tokens_left_behind=0.2842787206172943, experts/router_confidence=3.4701411724090576, experts/router_z_loss=0.00031380425207316875, learning_rate=0.00548289, learning_rate/current=0.00548252, loss=16.670948028564453, loss_per_all_target_tokens=2.11982e-05, loss_per_nonpadding_target_token=2.13046e-05, non_padding_fraction/loss_weights=0.995008, timing/seconds=89.44622802734375, timing/seqs=3840, timing/seqs_per_second=42.930824279785156, timing/seqs_per_second_per_core=1.3415882587432861, timing/steps_per_second=0.11179901659488678, timing/target_tokens_per_second=87922.328125, timing/target_tokens_per_second_per_core=2747.57275390625, timing/uptime=7103.39, z_loss=0.006394239608198404, z_loss_per_all_target_tokens=8.1307e-09
I0513 01:43:47.197190 140031916288000 trainer.py:518] Training: step 33284
I0513 01:43:47.220318 140440593786880 trainer.py:518] Training: step 33286
I0513 01:43:47.225638 140095893518336 trainer.py:518] Training: step 33288
I0513 01:43:47.225343 140529434355712 trainer.py:518] Training: step 33284
I0513 01:43:47.221506 139674595104768 trainer.py:518] Training: step 33287
I0513 01:43:56.142830 140311617681408 trainer.py:518] Training: step 33290
I0513 01:43:56.158147 140339465779200 trainer.py:518] Training: step 33290
I0513 01:44:22.978615 140095893518336 trainer.py:518] Training: step 33292
I0513 01:44:22.977524 140529434355712 trainer.py:518] Training: step 33292
I0513 01:44:22.977760 140595318249472 trainer.py:518] Training: step 33292
I0513 01:44:22.978937 140311617681408 trainer.py:518] Training: step 33292
I0513 01:44:22.977558 140440593786880 trainer.py:518] Training: step 33292
I0513 01:44:22.978524 140339465779200 trainer.py:518] Training: step 33292
I0513 01:44:22.977828 139674595104768 trainer.py:518] Training: step 33292
I0513 01:44:22.978346 140031916288000 trainer.py:518] Training: step 33292
I0513 01:44:40.869072 140095893518336 trainer.py:518] Training: step 33293
I0513 01:44:40.868230 140529434355712 trainer.py:518] Training: step 33293
I0513 01:44:40.868166 140440593786880 trainer.py:518] Training: step 33293
I0513 01:44:40.868891 140595318249472 trainer.py:518] Training: step 33293
I0513 01:44:40.869398 140339465779200 trainer.py:518] Training: step 33293
I0513 01:44:40.868163 139674595104768 trainer.py:518] Training: step 33293
I0513 01:44:40.868925 140031916288000 trainer.py:518] Training: step 33293
I0513 01:44:49.815644 140311617681408 trainer.py:518] Training: step 33293
I0513 01:44:58.760616 140095893518336 trainer.py:518] Training: step 33294
I0513 01:44:58.759752 140529434355712 trainer.py:518] Training: step 33294
I0513 01:44:58.759246 140440593786880 trainer.py:518] Training: step 33294
I0513 01:44:58.760000 140595318249472 trainer.py:518] Training: step 33294
I0513 01:44:58.760508 140339465779200 trainer.py:518] Training: step 33294
I0513 01:44:58.759671 139674595104768 trainer.py:518] Training: step 33294
I0513 01:44:58.760119 140031916288000 trainer.py:518] Training: step 33294
I0513 01:45:16.650409 140529434355712 trainer.py:518] Training: step 33295
I0513 01:45:16.650429 140595318249472 trainer.py:518] Training: step 33295
I0513 01:45:16.650204 140440593786880 trainer.py:518] Training: step 33295
I0513 01:45:16.651789 140311617681408 trainer.py:518] Training: step 33294
I0513 01:45:16.651311 140339465779200 trainer.py:518] Training: step 33295
I0513 01:45:16.650272 139674595104768 trainer.py:518] Training: step 33295
I0513 01:45:16.651229 140031916288000 trainer.py:518] Training: step 33295
I0513 01:45:25.599175 140095893518336 trainer.py:518] Training: step 33295
I0513 01:45:34.542347 140529434355712 trainer.py:518] Training: step 33296
I0513 01:45:34.542112 140440593786880 trainer.py:518] Training: step 33296
I0513 01:45:34.543932 140311617681408 trainer.py:518] Training: step 33295
I0513 01:45:34.543218 140339465779200 trainer.py:518] Training: step 33296
I0513 01:45:34.542108 139674595104768 trainer.py:518] Training: step 33296
I0513 01:45:34.543105 140031916288000 trainer.py:518] Training: step 33296
I0513 01:45:43.489457 140095893518336 trainer.py:518] Training: step 33296
I0513 01:45:43.487563 140595318249472 trainer.py:518] Training: step 33297
I0513 01:45:52.433905 140311617681408 trainer.py:518] Training: step 33296
I0513 01:45:52.433246 140529434355712 trainer.py:518] Training: step 33297
I0513 01:45:52.433396 140339465779200 trainer.py:518] Training: step 33297
I0513 01:45:52.432803 140440593786880 trainer.py:518] Training: step 33297
I0513 01:45:52.432919 139674595104768 trainer.py:518] Training: step 33297
I0513 01:45:52.433062 140031916288000 trainer.py:518] Training: step 33297
I0513 01:46:01.379627 140595318249472 trainer.py:518] Training: step 33298
I0513 01:46:10.325450 140095893518336 trainer.py:518] Training: step 33298
I0513 01:46:10.325499 140311617681408 trainer.py:518] Training: step 33297
I0513 01:46:10.323998 140440593786880 trainer.py:518] Training: step 33298
I0513 01:46:10.325563 140339465779200 trainer.py:518] Training: step 33298
I0513 01:46:10.326890 140529434355712 trainer.py:518] Training: step 33298
I0513 01:46:10.324688 140031916288000 trainer.py:518] Training: step 33298
I0513 01:46:10.324416 139674595104768 trainer.py:518] Training: step 33298
I0513 01:46:19.270009 140595318249472 trainer.py:518] Training: step 33299
I0513 01:46:19.270781 140439986517568 logging_writer.py:48] [33280] collection=train accuracy=0.569178, cross_ent_loss=16.591564178466797, cross_ent_loss_per_all_target_tokens=2.10973e-05, experts/auxiliary_loss=0.16074320673942566, experts/expert_usage=7.943309783935547, experts/fraction_tokens_left_behind=0.25161653757095337, experts/router_confidence=3.4727001190185547, experts/router_z_loss=0.0003149958502035588, learning_rate=0.00548207, learning_rate/current=0.00548169, loss=16.759363174438477, loss_per_all_target_tokens=2.13106e-05, loss_per_nonpadding_target_token=2.14299e-05, non_padding_fraction/loss_weights=0.994434, timing/seconds=89.45137786865234, timing/seqs=3840, timing/seqs_per_second=42.92835235595703, timing/seqs_per_second_per_core=1.3415110111236572, timing/steps_per_second=0.11179257929325104, timing/target_tokens_per_second=87917.265625, timing/target_tokens_per_second_per_core=2747.41455078125, timing/uptime=7264.21, z_loss=0.0067400820553302765, z_loss_per_all_target_tokens=8.57046e-09
I0513 01:46:28.217004 140095893518336 trainer.py:518] Training: step 33299
I0513 01:46:28.216678 140339465779200 trainer.py:518] Training: step 33299
I0513 01:46:28.216388 140440593786880 trainer.py:518] Training: step 33299
I0513 01:46:28.217993 140529434355712 trainer.py:518] Training: step 33299
I0513 01:46:28.216392 139674595104768 trainer.py:518] Training: step 33299
I0513 01:46:28.216782 140031916288000 trainer.py:518] Training: step 33299
I0513 01:46:28.247452 140311617681408 trainer.py:518] Training: step 33304
I0513 01:46:37.162665 140595318249472 trainer.py:518] Training: step 33300
I0513 01:46:38.283440 140440593786880 trainer.py:518] Training: step 33304
I0513 01:46:38.705876 139674595104768 trainer.py:518] Training: step 33306
I0513 01:46:46.107832 140529434355712 trainer.py:518] Training: step 33304
I0513 01:46:46.111646 140439986517568 logging_writer.py:48] [33290] collection=train accuracy=0.570856, cross_ent_loss=16.45341682434082, cross_ent_loss_per_all_target_tokens=2.09216e-05, experts/auxiliary_loss=0.1607101857662201, experts/expert_usage=7.938180446624756, experts/fraction_tokens_left_behind=0.25802621245384216, experts/router_confidence=3.478186845779419, experts/router_z_loss=0.0003130777913611382, learning_rate=0.00548124, learning_rate/current=0.00548087, loss=16.62092399597168, loss_per_all_target_tokens=2.11346e-05, loss_per_nonpadding_target_token=2.12694e-05, non_padding_fraction/loss_weights=0.99366, timing/seconds=89.44711303710938, timing/seqs=3840, timing/seqs_per_second=42.930397033691406, timing/seqs_per_second_per_core=1.3415749073028564, timing/steps_per_second=0.11179790645837784, timing/target_tokens_per_second=87921.453125, timing/target_tokens_per_second_per_core=2747.54541015625, timing/uptime=7282.3, z_loss=0.006481164135038853, z_loss_per_all_target_tokens=8.24123e-09
I0513 01:46:46.107004 140031916288000 trainer.py:518] Training: step 33306
I0513 01:46:46.137875 140339465779200 trainer.py:518] Training: step 33307
I0513 01:46:55.052501 140095893518336 trainer.py:518] Training: step 33310
I0513 01:46:55.052692 140311617681408 trainer.py:518] Training: step 33310
I0513 01:46:55.052192 140440593786880 trainer.py:518] Training: step 33310
I0513 01:46:55.053367 140595318249472 trainer.py:518] Training: step 33310
I0513 01:46:55.068173 139674595104768 trainer.py:518] Training: step 33310
I0513 01:47:21.889712 140095893518336 trainer.py:518] Training: step 33312
I0513 01:47:21.890503 140311617681408 trainer.py:518] Training: step 33312
I0513 01:47:21.889583 140595318249472 trainer.py:518] Training: step 33312
I0513 01:47:21.889011 140440593786880 trainer.py:518] Training: step 33312
I0513 01:47:21.889903 140339465779200 trainer.py:518] Training: step 33312
I0513 01:47:21.890110 140529434355712 trainer.py:518] Training: step 33312
I0513 01:47:21.889238 139674595104768 trainer.py:518] Training: step 33312
I0513 01:47:21.890026 140031916288000 trainer.py:518] Training: step 33312
I0513 01:47:39.779972 140311617681408 trainer.py:518] Training: step 33313
I0513 01:47:39.779636 140595318249472 trainer.py:518] Training: step 33313
I0513 01:47:39.779942 140339465779200 trainer.py:518] Training: step 33313
I0513 01:47:39.779509 140440593786880 trainer.py:518] Training: step 33313
I0513 01:47:39.780234 140529434355712 trainer.py:518] Training: step 33313
I0513 01:47:39.779190 139674595104768 trainer.py:518] Training: step 33313
I0513 01:47:39.779441 140031916288000 trainer.py:518] Training: step 33313
I0513 01:47:48.725893 140095893518336 trainer.py:518] Training: step 33313
I0513 01:47:57.671596 140311617681408 trainer.py:518] Training: step 33314
I0513 01:47:57.671499 140529434355712 trainer.py:518] Training: step 33314
I0513 01:47:57.671618 140339465779200 trainer.py:518] Training: step 33314
I0513 01:47:57.671412 140595318249472 trainer.py:518] Training: step 33314
I0513 01:47:57.670927 140440593786880 trainer.py:518] Training: step 33314
I0513 01:47:57.670833 140031916288000 trainer.py:518] Training: step 33314
I0513 01:47:57.670920 139674595104768 trainer.py:518] Training: step 33314
I0513 01:48:06.616446 140095893518336 trainer.py:518] Training: step 33314
I0513 01:48:15.562564 140311617681408 trainer.py:518] Training: step 33315
I0513 01:48:15.561719 140595318249472 trainer.py:518] Training: step 33315
I0513 01:48:15.561594 140440593786880 trainer.py:518] Training: step 33315
I0513 01:48:15.562284 140529434355712 trainer.py:518] Training: step 33315
I0513 01:48:15.561736 140031916288000 trainer.py:518] Training: step 33315
I0513 01:48:15.561851 139674595104768 trainer.py:518] Training: step 33315
I0513 01:48:24.506308 140095893518336 trainer.py:518] Training: step 33315
I0513 01:48:24.510016 140339465779200 trainer.py:518] Training: step 33315
I0513 01:48:33.453286 140311617681408 trainer.py:518] Training: step 33316
I0513 01:48:33.452376 140595318249472 trainer.py:518] Training: step 33316
I0513 01:48:33.453228 140529434355712 trainer.py:518] Training: step 33316
I0513 01:48:33.452540 140440593786880 trainer.py:518] Training: step 33316
I0513 01:48:33.452504 140031916288000 trainer.py:518] Training: step 33316
I0513 01:48:33.452462 139674595104768 trainer.py:518] Training: step 33316
I0513 01:48:42.399337 140095893518336 trainer.py:518] Training: step 33316
I0513 01:48:42.398969 140339465779200 trainer.py:518] Training: step 33316
I0513 01:48:51.343609 140311617681408 trainer.py:518] Training: step 33317
I0513 01:48:51.342558 140440593786880 trainer.py:518] Training: step 33317
I0513 01:48:51.343379 140595318249472 trainer.py:518] Training: step 33317
I0513 01:48:51.344259 140529434355712 trainer.py:518] Training: step 33317
I0513 01:48:51.342926 140031916288000 trainer.py:518] Training: step 33317
I0513 01:48:51.343009 139674595104768 trainer.py:518] Training: step 33317
I0513 01:49:09.236132 140095893518336 trainer.py:518] Training: step 33318
I0513 01:49:09.236542 140311617681408 trainer.py:518] Training: step 33318
I0513 01:49:09.235514 140595318249472 trainer.py:518] Training: step 33318
I0513 01:49:09.235547 140440593786880 trainer.py:518] Training: step 33318
I0513 01:49:09.236511 140339465779200 trainer.py:518] Training: step 33318
I0513 01:49:09.237098 140529434355712 trainer.py:518] Training: step 33318
I0513 01:49:09.235472 139674595104768 trainer.py:518] Training: step 33318
I0513 01:49:09.235638 140031916288000 trainer.py:518] Training: step 33318
I0513 01:49:18.182940 140439986517568 logging_writer.py:48] [33300] collection=train accuracy=0.573011, cross_ent_loss=16.410898208618164, cross_ent_loss_per_all_target_tokens=2.08675e-05, experts/auxiliary_loss=0.16072645783424377, experts/expert_usage=7.940049648284912, experts/fraction_tokens_left_behind=0.23352722823619843, experts/router_confidence=3.482424259185791, experts/router_z_loss=0.00031162583036348224, learning_rate=0.00548042, learning_rate/current=0.00548005, loss=16.57880210876465, loss_per_all_target_tokens=2.1081e-05, loss_per_nonpadding_target_token=2.12035e-05, non_padding_fraction/loss_weights=0.994224, timing/seconds=89.45467376708984, timing/seqs=3840, timing/seqs_per_second=42.92676544189453, timing/seqs_per_second_per_core=1.341461420059204, timing/steps_per_second=0.11178845167160034, timing/target_tokens_per_second=87914.015625, timing/target_tokens_per_second_per_core=2747.31298828125, timing/uptime=7443.13, z_loss=0.006866521667689085, z_loss_per_all_target_tokens=8.73123e-09
I0513 01:49:27.127900 140095893518336 trainer.py:518] Training: step 33319
I0513 01:49:27.127739 140311617681408 trainer.py:518] Training: step 33319
I0513 01:49:27.126983 140595318249472 trainer.py:518] Training: step 33319
I0513 01:49:27.126593 140440593786880 trainer.py:518] Training: step 33319
I0513 01:49:27.127501 140339465779200 trainer.py:518] Training: step 33319
I0513 01:49:27.127676 140529434355712 trainer.py:518] Training: step 33319
I0513 01:49:27.126839 139674595104768 trainer.py:518] Training: step 33319
I0513 01:49:27.127566 140031916288000 trainer.py:518] Training: step 33319
I0513 01:49:45.017689 140595318249472 trainer.py:518] Training: step 33326
I0513 01:49:45.019281 140439986517568 logging_writer.py:48] [33310] collection=train accuracy=0.577616, cross_ent_loss=16.182022094726562, cross_ent_loss_per_all_target_tokens=2.05765e-05, experts/auxiliary_loss=0.1607530415058136, experts/expert_usage=7.92062520980835, experts/fraction_tokens_left_behind=0.25972044467926025, experts/router_confidence=3.4549763202667236, experts/router_z_loss=0.0003157884057145566, learning_rate=0.0054796, learning_rate/current=0.00547923, loss=16.349824905395508, loss_per_all_target_tokens=2.07899e-05, loss_per_nonpadding_target_token=2.09255e-05, non_padding_fraction/loss_weights=0.993519, timing/seconds=89.44805145263672, timing/seqs=3840, timing/seqs_per_second=42.92994689941406, timing/seqs_per_second_per_core=1.3415608406066895, timing/steps_per_second=0.11179673671722412, timing/target_tokens_per_second=87920.53125, timing/target_tokens_per_second_per_core=2747.5166015625, timing/uptime=7461.19, z_loss=0.0067320833913981915, z_loss_per_all_target_tokens=8.56029e-09
I0513 01:49:45.043092 140440593786880 trainer.py:518] Training: step 33327
I0513 01:49:45.047304 140311617681408 trainer.py:518] Training: step 33328
I0513 01:49:45.048732 140529434355712 trainer.py:518] Training: step 33325
I0513 01:49:45.042744 139674595104768 trainer.py:518] Training: step 33328
I0513 01:49:45.045443 140031916288000 trainer.py:518] Training: step 33328
I0513 01:49:53.963655 140095893518336 trainer.py:518] Training: step 33330
I0513 01:49:53.966821 140339465779200 trainer.py:518] Training: step 33330
I0513 01:50:20.799613 140095893518336 trainer.py:518] Training: step 33332
I0513 01:50:20.799693 140311617681408 trainer.py:518] Training: step 33332
I0513 01:50:20.798867 140595318249472 trainer.py:518] Training: step 33332
I0513 01:50:20.799395 140529434355712 trainer.py:518] Training: step 33332
I0513 01:50:20.799476 140339465779200 trainer.py:518] Training: step 33332
I0513 01:50:20.799338 140440593786880 trainer.py:518] Training: step 33332
I0513 01:50:20.798772 140031916288000 trainer.py:518] Training: step 33332
I0513 01:50:20.798900 139674595104768 trainer.py:518] Training: step 33332
I0513 01:50:38.689320 140095893518336 trainer.py:518] Training: step 33333
I0513 01:50:38.689710 140311617681408 trainer.py:518] Training: step 33333
I0513 01:50:38.688903 140595318249472 trainer.py:518] Training: step 33333
I0513 01:50:38.689573 140529434355712 trainer.py:518] Training: step 33333
I0513 01:50:38.689856 140339465779200 trainer.py:518] Training: step 33333
I0513 01:50:38.688853 140440593786880 trainer.py:518] Training: step 33333
I0513 01:50:38.688983 140031916288000 trainer.py:518] Training: step 33333
I0513 01:50:38.688927 139674595104768 trainer.py:518] Training: step 33333
I0513 01:50:56.580187 140095893518336 trainer.py:518] Training: step 33334
I0513 01:50:56.580516 140311617681408 trainer.py:518] Training: step 33334
I0513 01:50:56.580237 140529434355712 trainer.py:518] Training: step 33334
I0513 01:50:56.579741 140440593786880 trainer.py:518] Training: step 33334
I0513 01:50:56.580372 140595318249472 trainer.py:518] Training: step 33334
I0513 01:50:56.580742 140339465779200 trainer.py:518] Training: step 33334
I0513 01:50:56.579990 139674595104768 trainer.py:518] Training: step 33334
I0513 01:50:56.580300 140031916288000 trainer.py:518] Training: step 33334
I0513 01:51:14.471376 140095893518336 trainer.py:518] Training: step 33335
I0513 01:51:14.471424 140311617681408 trainer.py:518] Training: step 33335
I0513 01:51:14.471062 140529434355712 trainer.py:518] Training: step 33335
I0513 01:51:14.471002 140595318249472 trainer.py:518] Training: step 33335
I0513 01:51:14.470650 140440593786880 trainer.py:518] Training: step 33335
I0513 01:51:14.471513 140339465779200 trainer.py:518] Training: step 33335
I0513 01:51:14.470874 140031916288000 trainer.py:518] Training: step 33335
I0513 01:51:14.470498 139674595104768 trainer.py:518] Training: step 33335
I0513 01:51:32.362309 140095893518336 trainer.py:518] Training: step 33336
I0513 01:51:32.361971 140311617681408 trainer.py:518] Training: step 33336
I0513 01:51:32.361417 140339465779200 trainer.py:518] Training: step 33336
I0513 01:51:32.361407 140595318249472 trainer.py:518] Training: step 33336
I0513 01:51:32.361011 140440593786880 trainer.py:518] Training: step 33336
I0513 01:51:32.362168 140529434355712 trainer.py:518] Training: step 33336
I0513 01:51:32.361004 139674595104768 trainer.py:518] Training: step 33336
I0513 01:51:32.361651 140031916288000 trainer.py:518] Training: step 33336
I0513 01:51:50.253629 140095893518336 trainer.py:518] Training: step 33337
I0513 01:51:50.253722 140311617681408 trainer.py:518] Training: step 33337
I0513 01:51:50.253056 140595318249472 trainer.py:518] Training: step 33337
I0513 01:51:50.252765 140440593786880 trainer.py:518] Training: step 33337
I0513 01:51:50.253554 140339465779200 trainer.py:518] Training: step 33337
I0513 01:51:50.253699 140529434355712 trainer.py:518] Training: step 33337
I0513 01:51:50.252748 139674595104768 trainer.py:518] Training: step 33337
I0513 01:51:50.252939 140031916288000 trainer.py:518] Training: step 33337
I0513 01:52:08.143968 140095893518336 trainer.py:518] Training: step 33338
I0513 01:52:08.144553 140311617681408 trainer.py:518] Training: step 33338
I0513 01:52:08.143717 140595318249472 trainer.py:518] Training: step 33338
I0513 01:52:08.144045 140339465779200 trainer.py:518] Training: step 33338
I0513 01:52:08.143422 140440593786880 trainer.py:518] Training: step 33338
I0513 01:52:08.145112 140529434355712 trainer.py:518] Training: step 33338
I0513 01:52:08.143495 140031916288000 trainer.py:518] Training: step 33338
I0513 01:52:08.143619 139674595104768 trainer.py:518] Training: step 33338
I0513 01:52:17.090329 140439986517568 logging_writer.py:48] [33320] collection=train accuracy=0.567932, cross_ent_loss=16.641361236572266, cross_ent_loss_per_all_target_tokens=2.11606e-05, experts/auxiliary_loss=0.16071633994579315, experts/expert_usage=7.954488277435303, experts/fraction_tokens_left_behind=0.24811053276062012, experts/router_confidence=3.4720523357391357, experts/router_z_loss=0.00030779821099713445, learning_rate=0.00547877, learning_rate/current=0.0054784, loss=16.80919647216797, loss_per_all_target_tokens=2.1374e-05, loss_per_nonpadding_target_token=2.14789e-05, non_padding_fraction/loss_weights=0.995115, timing/seconds=89.4523696899414, timing/seqs=3840, timing/seqs_per_second=42.92787170410156, timing/seqs_per_second_per_core=1.3414959907531738, timing/steps_per_second=0.11179133504629135, timing/target_tokens_per_second=87916.28125, timing/target_tokens_per_second_per_core=2747.3837890625, timing/uptime=7622.03, z_loss=0.006813600193709135, z_loss_per_all_target_tokens=8.66394e-09
I0513 01:52:26.034986 140095893518336 trainer.py:518] Training: step 33339
I0513 01:52:26.035142 140311617681408 trainer.py:518] Training: step 33339
I0513 01:52:26.034529 140339465779200 trainer.py:518] Training: step 33339
I0513 01:52:26.035407 140595318249472 trainer.py:518] Training: step 33339
I0513 01:52:26.034649 140440593786880 trainer.py:518] Training: step 33339
I0513 01:52:26.036063 140529434355712 trainer.py:518] Training: step 33339
I0513 01:52:26.034538 140031916288000 trainer.py:518] Training: step 33339
I0513 01:52:26.034645 139674595104768 trainer.py:518] Training: step 33339
I0513 01:52:43.927280 140595318249472 trainer.py:518] Training: step 33346
I0513 01:52:43.927889 140339465779200 trainer.py:518] Training: step 33346
I0513 01:52:43.927108 140440593786880 trainer.py:518] Training: step 33347
I0513 01:52:43.928211 140439986517568 logging_writer.py:48] [33330] collection=train accuracy=0.565984, cross_ent_loss=16.701948165893555, cross_ent_loss_per_all_target_tokens=2.12376e-05, experts/auxiliary_loss=0.16072462499141693, experts/expert_usage=7.935953617095947, experts/fraction_tokens_left_behind=0.25218668580055237, experts/router_confidence=3.469196319580078, experts/router_z_loss=0.00031476948061026633, learning_rate=0.00547795, learning_rate/current=0.00547758, loss=16.869714736938477, loss_per_all_target_tokens=2.1451e-05, loss_per_nonpadding_target_token=2.16105e-05, non_padding_fraction/loss_weights=0.992616, timing/seconds=89.44744873046875, timing/seqs=3840, timing/seqs_per_second=42.93023681640625, timing/seqs_per_second_per_core=1.3415699005126953, timing/steps_per_second=0.11179748922586441, timing/target_tokens_per_second=87921.125, timing/target_tokens_per_second_per_core=2747.53515625, timing/uptime=7640.1, z_loss=0.006724583450704813, z_loss_per_all_target_tokens=8.55075e-09
I0513 01:52:43.956909 140095893518336 trainer.py:518] Training: step 33348
I0513 01:52:43.958621 140529434355712 trainer.py:518] Training: step 33344
I0513 01:52:43.960779 140311617681408 trainer.py:518] Training: step 33349
I0513 01:52:43.952250 139674595104768 trainer.py:518] Training: step 33348
I0513 01:52:43.956731 140031916288000 trainer.py:518] Training: step 33348
I0513 01:53:19.708096 140095893518336 trainer.py:518] Training: step 33352
I0513 01:53:19.707735 140529434355712 trainer.py:518] Training: step 33352
I0513 01:53:19.708720 140311617681408 trainer.py:518] Training: step 33352
I0513 01:53:19.707867 140595318249472 trainer.py:518] Training: step 33352
I0513 01:53:19.708648 140339465779200 trainer.py:518] Training: step 33352
I0513 01:53:19.708080 140440593786880 trainer.py:518] Training: step 33352
I0513 01:53:19.707412 139674595104768 trainer.py:518] Training: step 33352
I0513 01:53:19.708199 140031916288000 trainer.py:518] Training: step 33352
I0513 01:53:37.598968 140529434355712 trainer.py:518] Training: step 33353
I0513 01:53:37.599249 140339465779200 trainer.py:518] Training: step 33353
I0513 01:53:37.599396 140595318249472 trainer.py:518] Training: step 33353
I0513 01:53:37.600091 140311617681408 trainer.py:518] Training: step 33353
I0513 01:53:37.599440 140440593786880 trainer.py:518] Training: step 33353
I0513 01:53:37.598610 139674595104768 trainer.py:518] Training: step 33353
I0513 01:53:37.599098 140031916288000 trainer.py:518] Training: step 33353
I0513 01:53:46.545146 140095893518336 trainer.py:518] Training: step 33353
I0513 01:53:55.489810 140529434355712 trainer.py:518] Training: step 33354
I0513 01:53:55.490828 140311617681408 trainer.py:518] Training: step 33354
I0513 01:53:55.490128 140339465779200 trainer.py:518] Training: step 33354
I0513 01:53:55.489973 140595318249472 trainer.py:518] Training: step 33354
I0513 01:53:55.489935 140440593786880 trainer.py:518] Training: step 33354
I0513 01:53:55.489723 140031916288000 trainer.py:518] Training: step 33354
I0513 01:53:55.490154 139674595104768 trainer.py:518] Training: step 33354
I0513 01:54:04.435277 140095893518336 trainer.py:518] Training: step 33354
I0513 01:54:13.380841 140529434355712 trainer.py:518] Training: step 33355
I0513 01:54:13.380920 140339465779200 trainer.py:518] Training: step 33355
I0513 01:54:13.380994 140595318249472 trainer.py:518] Training: step 33355
I0513 01:54:13.381285 140440593786880 trainer.py:518] Training: step 33355
I0513 01:54:13.380564 139674595104768 trainer.py:518] Training: step 33355
I0513 01:54:13.380888 140031916288000 trainer.py:518] Training: step 33355
I0513 01:54:22.325023 140095893518336 trainer.py:518] Training: step 33355
I0513 01:54:22.325723 140311617681408 trainer.py:518] Training: step 33355
I0513 01:54:31.270914 140339465779200 trainer.py:518] Training: step 33356
I0513 01:54:31.270905 140595318249472 trainer.py:518] Training: step 33356
I0513 01:54:31.271465 140529434355712 trainer.py:518] Training: step 33356
I0513 01:54:31.271291 140440593786880 trainer.py:518] Training: step 33356
I0513 01:54:31.270540 139674595104768 trainer.py:518] Training: step 33356
I0513 01:54:31.270872 140031916288000 trainer.py:518] Training: step 33356
I0513 01:54:40.217027 140095893518336 trainer.py:518] Training: step 33356
I0513 01:54:40.217351 140311617681408 trainer.py:518] Training: step 33356
I0513 01:54:49.161349 140339465779200 trainer.py:518] Training: step 33357
I0513 01:54:49.161426 140595318249472 trainer.py:518] Training: step 33357
I0513 01:54:49.161839 140529434355712 trainer.py:518] Training: step 33357
I0513 01:54:49.161111 140440593786880 trainer.py:518] Training: step 33357
I0513 01:54:49.160573 139674595104768 trainer.py:518] Training: step 33357
I0513 01:54:49.161379 140031916288000 trainer.py:518] Training: step 33357
I0513 01:55:07.051579 140095893518336 trainer.py:518] Training: step 33358
I0513 01:55:07.052333 140311617681408 trainer.py:518] Training: step 33358
I0513 01:55:07.051534 140595318249472 trainer.py:518] Training: step 33358
I0513 01:55:07.051959 140529434355712 trainer.py:518] Training: step 33358
I0513 01:55:07.052292 140339465779200 trainer.py:518] Training: step 33358
I0513 01:55:07.051509 140440593786880 trainer.py:518] Training: step 33358
I0513 01:55:07.050739 139674595104768 trainer.py:518] Training: step 33358
I0513 01:55:07.051490 140031916288000 trainer.py:518] Training: step 33358
I0513 01:55:15.998420 140439986517568 logging_writer.py:48] [33340] collection=train accuracy=0.563372, cross_ent_loss=16.780393600463867, cross_ent_loss_per_all_target_tokens=2.13374e-05, experts/auxiliary_loss=0.16067127883434296, experts/expert_usage=7.945271968841553, experts/fraction_tokens_left_behind=0.2464117556810379, experts/router_confidence=3.4756133556365967, experts/router_z_loss=0.00031564090750180185, learning_rate=0.00547713, learning_rate/current=0.00547676, loss=16.94837188720703, loss_per_all_target_tokens=2.1551e-05, loss_per_nonpadding_target_token=2.17138e-05, non_padding_fraction/loss_weights=0.9925, timing/seconds=89.455322265625, timing/seqs=3840, timing/seqs_per_second=42.92646026611328, timing/seqs_per_second_per_core=1.34145188331604, timing/steps_per_second=0.11178765445947647, timing/target_tokens_per_second=87913.390625, timing/target_tokens_per_second_per_core=2747.29345703125, timing/uptime=7800.94, z_loss=0.006991116795688868, z_loss_per_all_target_tokens=8.88966e-09
I0513 01:55:24.943553 140095893518336 trainer.py:518] Training: step 33359
I0513 01:55:24.944061 140311617681408 trainer.py:518] Training: step 33359
I0513 01:55:24.943325 140339465779200 trainer.py:518] Training: step 33359
I0513 01:55:24.943478 140595318249472 trainer.py:518] Training: step 33359
I0513 01:55:24.942998 140440593786880 trainer.py:518] Training: step 33359
I0513 01:55:24.945149 140529434355712 trainer.py:518] Training: step 33359
I0513 01:55:24.942782 139674595104768 trainer.py:518] Training: step 33359
I0513 01:55:24.944468 140031916288000 trainer.py:518] Training: step 33359
I0513 01:55:42.833758 140595318249472 trainer.py:518] Training: step 33366
I0513 01:55:42.835030 140439986517568 logging_writer.py:48] [33350] collection=train accuracy=0.570657, cross_ent_loss=16.486841201782227, cross_ent_loss_per_all_target_tokens=2.09641e-05, experts/auxiliary_loss=0.16070356965065002, experts/expert_usage=7.939809322357178, experts/fraction_tokens_left_behind=0.2446765899658203, experts/router_confidence=3.4787137508392334, experts/router_z_loss=0.0003184470406267792, learning_rate=0.00547631, learning_rate/current=0.00547594, loss=16.654821395874023, loss_per_all_target_tokens=2.11777e-05, loss_per_nonpadding_target_token=2.13095e-05, non_padding_fraction/loss_weights=0.993815, timing/seconds=89.44522857666016, timing/seqs=3840, timing/seqs_per_second=42.93130111694336, timing/seqs_per_second_per_core=1.34160315990448, timing/steps_per_second=0.11180026829242706, timing/target_tokens_per_second=87923.3046875, timing/target_tokens_per_second_per_core=2747.603271484375, timing/uptime=7819.05, z_loss=0.006958224810659885, z_loss_per_all_target_tokens=8.84784e-09
I0513 01:55:42.833252 139674595104768 trainer.py:518] Training: step 33366
I0513 01:55:42.833782 140031916288000 trainer.py:518] Training: step 33365
I0513 01:55:42.858273 140440593786880 trainer.py:518] Training: step 33367
I0513 01:55:42.861115 140339465779200 trainer.py:518] Training: step 33367
I0513 01:55:42.864970 140529434355712 trainer.py:518] Training: step 33364
I0513 01:55:51.780107 140095893518336 trainer.py:518] Training: step 33370
I0513 01:55:51.780444 140311617681408 trainer.py:518] Training: step 33370
I0513 01:56:18.616941 140095893518336 trainer.py:518] Training: step 33372
I0513 01:56:18.616540 140339465779200 trainer.py:518] Training: step 33372
I0513 01:56:18.616659 140529434355712 trainer.py:518] Training: step 33372
I0513 01:56:18.617687 140311617681408 trainer.py:518] Training: step 33372
I0513 01:56:18.616672 140595318249472 trainer.py:518] Training: step 33372
I0513 01:56:18.617022 140440593786880 trainer.py:518] Training: step 33372
I0513 01:56:18.616385 139674595104768 trainer.py:518] Training: step 33372
I0513 01:56:18.616801 140031916288000 trainer.py:518] Training: step 33372
I0513 01:56:36.507286 140095893518336 trainer.py:518] Training: step 33373
I0513 01:56:36.507414 140339465779200 trainer.py:518] Training: step 33373
I0513 01:56:36.507188 140529434355712 trainer.py:518] Training: step 33373
I0513 01:56:36.507480 140595318249472 trainer.py:518] Training: step 33373
I0513 01:56:36.507524 140031916288000 trainer.py:518] Training: step 33373
I0513 01:56:36.506719 139674595104768 trainer.py:518] Training: step 33373
I0513 01:56:36.507549 140440593786880 trainer.py:518] Training: step 33373
I0513 01:56:36.507984 140311617681408 trainer.py:518] Training: step 33373
I0513 01:56:54.399391 140095893518336 trainer.py:518] Training: step 33374
I0513 01:56:54.399613 140339465779200 trainer.py:518] Training: step 33374
I0513 01:56:54.399137 140529434355712 trainer.py:518] Training: step 33374
I0513 01:56:54.399242 140031916288000 trainer.py:518] Training: step 33374
I0513 01:56:54.399426 140440593786880 trainer.py:518] Training: step 33374
I0513 01:56:54.399821 140595318249472 trainer.py:518] Training: step 33374
I0513 01:56:54.398884 139674595104768 trainer.py:518] Training: step 33374
I0513 01:57:03.345259 140311617681408 trainer.py:518] Training: step 33374
I0513 01:57:12.290796 140095893518336 trainer.py:518] Training: step 33375
I0513 01:57:12.290477 140339465779200 trainer.py:518] Training: step 33375
I0513 01:57:12.290214 140529434355712 trainer.py:518] Training: step 33375
I0513 01:57:12.290500 140595318249472 trainer.py:518] Training: step 33375
I0513 01:57:12.289721 139674595104768 trainer.py:518] Training: step 33375
I0513 01:57:12.290428 140031916288000 trainer.py:518] Training: step 33375
I0513 01:57:12.290490 140440593786880 trainer.py:518] Training: step 33375
I0513 01:57:21.235977 140311617681408 trainer.py:518] Training: step 33375
I0513 01:57:30.181784 140339465779200 trainer.py:518] Training: step 33376
I0513 01:57:30.182393 140095893518336 trainer.py:518] Training: step 33376
I0513 01:57:30.182249 140529434355712 trainer.py:518] Training: step 33376
I0513 01:57:30.181904 140595318249472 trainer.py:518] Training: step 33376
I0513 01:57:30.182166 140031916288000 trainer.py:518] Training: step 33376
I0513 01:57:30.182053 140440593786880 trainer.py:518] Training: step 33376
I0513 01:57:30.181541 139674595104768 trainer.py:518] Training: step 33376
I0513 01:57:39.127840 140311617681408 trainer.py:518] Training: step 33376
I0513 01:57:48.072390 140339465779200 trainer.py:518] Training: step 33377
I0513 01:57:48.072926 140095893518336 trainer.py:518] Training: step 33377
I0513 01:57:48.073180 140529434355712 trainer.py:518] Training: step 33377
I0513 01:57:48.072281 140031916288000 trainer.py:518] Training: step 33377
I0513 01:57:48.072406 140595318249472 trainer.py:518] Training: step 33377
I0513 01:57:48.072723 140440593786880 trainer.py:518] Training: step 33377
I0513 01:57:48.072008 139674595104768 trainer.py:518] Training: step 33377
I0513 01:58:05.962706 140339465779200 trainer.py:518] Training: step 33378
I0513 01:58:05.963273 140095893518336 trainer.py:518] Training: step 33378
I0513 01:58:05.963621 140529434355712 trainer.py:518] Training: step 33378
I0513 01:58:05.962767 140440593786880 trainer.py:518] Training: step 33378
I0513 01:58:05.962802 140031916288000 trainer.py:518] Training: step 33378
I0513 01:58:05.963078 140595318249472 trainer.py:518] Training: step 33378
I0513 01:58:05.962374 139674595104768 trainer.py:518] Training: step 33378
I0513 01:58:05.963835 140311617681408 trainer.py:518] Training: step 33378
I0513 01:58:14.908948 140439986517568 logging_writer.py:48] [33360] collection=train accuracy=0.570495, cross_ent_loss=16.40390396118164, cross_ent_loss_per_all_target_tokens=2.08586e-05, experts/auxiliary_loss=0.16073547303676605, experts/expert_usage=7.925184726715088, experts/fraction_tokens_left_behind=0.2430097907781601, experts/router_confidence=3.4756391048431396, experts/router_z_loss=0.0003068213409278542, learning_rate=0.00547549, learning_rate/current=0.00547512, loss=16.57204246520996, loss_per_all_target_tokens=2.10724e-05, loss_per_nonpadding_target_token=2.11495e-05, non_padding_fraction/loss_weights=0.996355, timing/seconds=89.44989013671875, timing/seqs=3840, timing/seqs_per_second=42.92906188964844, timing/seqs_per_second_per_core=1.3415331840515137, timing/steps_per_second=0.11179443448781967, timing/target_tokens_per_second=87918.71875, timing/target_tokens_per_second_per_core=2747.4599609375, timing/uptime=7979.85, z_loss=0.007100049406290054, z_loss_per_all_target_tokens=9.02818e-09
I0513 01:58:23.853955 140339465779200 trainer.py:518] Training: step 33379
I0513 01:58:23.854296 140095893518336 trainer.py:518] Training: step 33379
I0513 01:58:23.855418 140529434355712 trainer.py:518] Training: step 33379
I0513 01:58:23.854089 140440593786880 trainer.py:518] Training: step 33379
I0513 01:58:23.854118 140031916288000 trainer.py:518] Training: step 33379
I0513 01:58:23.854249 140595318249472 trainer.py:518] Training: step 33379
I0513 01:58:23.854753 140311617681408 trainer.py:518] Training: step 33379
I0513 01:58:23.853851 139674595104768 trainer.py:518] Training: step 33379
I0513 01:58:41.745757 140095893518336 trainer.py:518] Training: step 33388
I0513 01:58:41.745624 140529434355712 trainer.py:518] Training: step 33384
I0513 01:58:41.749053 140439986517568 logging_writer.py:48] [33370] collection=train accuracy=0.565193, cross_ent_loss=16.756505966186523, cross_ent_loss_per_all_target_tokens=2.1307e-05, experts/auxiliary_loss=0.1607179045677185, experts/expert_usage=7.9477858543396, experts/fraction_tokens_left_behind=0.25149470567703247, experts/router_confidence=3.4941234588623047, experts/router_z_loss=0.0003127972304355353, learning_rate=0.00547467, learning_rate/current=0.0054743, loss=16.924524307250977, loss_per_all_target_tokens=2.15206e-05, loss_per_nonpadding_target_token=2.16519e-05, non_padding_fraction/loss_weights=0.993939, timing/seconds=89.45160675048828, timing/seqs=3840, timing/seqs_per_second=42.92824172973633, timing/seqs_per_second_per_core=1.3415075540542603, timing/steps_per_second=0.11179229617118835, timing/target_tokens_per_second=87917.0390625, timing/target_tokens_per_second_per_core=2747.407470703125, timing/uptime=7997.95, z_loss=0.006986995227634907, z_loss_per_all_target_tokens=8.88442e-09
I0513 01:58:41.773469 140339465779200 trainer.py:518] Training: step 33387
I0513 01:58:41.772002 140440593786880 trainer.py:518] Training: step 33387
I0513 01:58:41.772682 139674595104768 trainer.py:518] Training: step 33387
I0513 01:58:41.775176 140031916288000 trainer.py:518] Training: step 33388
I0513 01:58:41.780148 140595318249472 trainer.py:518] Training: step 33387
I0513 01:58:50.690887 140311617681408 trainer.py:518] Training: step 33390
I0513 01:59:17.528414 140095893518336 trainer.py:518] Training: step 33392
I0513 01:59:17.527449 140529434355712 trainer.py:518] Training: step 33392
I0513 01:59:17.527572 140339465779200 trainer.py:518] Training: step 33392
I0513 01:59:17.528480 140311617681408 trainer.py:518] Training: step 33392
I0513 01:59:17.527759 140595318249472 trainer.py:518] Training: step 33392
I0513 01:59:17.527491 140440593786880 trainer.py:518] Training: step 33392
I0513 01:59:17.527153 139674595104768 trainer.py:518] Training: step 33392
I0513 01:59:17.528051 140031916288000 trainer.py:518] Training: step 33392
I0513 01:59:35.419714 140095893518336 trainer.py:518] Training: step 33393
I0513 01:59:35.418473 140339465779200 trainer.py:518] Training: step 33393
I0513 01:59:35.419691 140311617681408 trainer.py:518] Training: step 33393
I0513 01:59:35.419049 140529434355712 trainer.py:518] Training: step 33393
I0513 01:59:35.419217 140595318249472 trainer.py:518] Training: step 33393
I0513 01:59:35.418806 140440593786880 trainer.py:518] Training: step 33393
I0513 01:59:35.418790 139674595104768 trainer.py:518] Training: step 33393
I0513 01:59:35.419387 140031916288000 trainer.py:518] Training: step 33393
I0513 01:59:53.309653 140095893518336 trainer.py:518] Training: step 33394
I0513 01:59:53.308804 140529434355712 trainer.py:518] Training: step 33394
I0513 01:59:53.309946 140311617681408 trainer.py:518] Training: step 33394
I0513 01:59:53.309094 140339465779200 trainer.py:518] Training: step 33394
I0513 01:59:53.308839 140440593786880 trainer.py:518] Training: step 33394
I0513 01:59:53.308914 139674595104768 trainer.py:518] Training: step 33394
I0513 01:59:53.309214 140031916288000 trainer.py:518] Training: step 33394
I0513 02:00:02.254211 140595318249472 trainer.py:518] Training: step 33394
I0513 02:00:11.200344 140095893518336 trainer.py:518] Training: step 33395
I0513 02:00:11.199104 140339465779200 trainer.py:518] Training: step 33395
I0513 02:00:11.199636 140529434355712 trainer.py:518] Training: step 33395
I0513 02:00:11.200572 140311617681408 trainer.py:518] Training: step 33395
I0513 02:00:11.199307 140440593786880 trainer.py:518] Training: step 33395
I0513 02:00:11.199524 140031916288000 trainer.py:518] Training: step 33395
I0513 02:00:11.199055 139674595104768 trainer.py:518] Training: step 33395
I0513 02:00:20.147469 140595318249472 trainer.py:518] Training: step 33395
I0513 02:00:29.090677 140095893518336 trainer.py:518] Training: step 33396
I0513 02:00:29.089670 139674595104768 trainer.py:518] Training: step 33396
I0513 02:00:29.089834 140339465779200 trainer.py:518] Training: step 33396
I0513 02:00:29.089989 140529434355712 trainer.py:518] Training: step 33396
I0513 02:00:29.090840 140311617681408 trainer.py:518] Training: step 33396
I0513 02:00:29.090159 140440593786880 trainer.py:518] Training: step 33396
I0513 02:00:29.090304 140031916288000 trainer.py:518] Training: step 33396
I0513 02:00:38.036742 140595318249472 trainer.py:518] Training: step 33396
I0513 02:00:46.982006 140095893518336 trainer.py:518] Training: step 33397
I0513 02:00:46.980508 139674595104768 trainer.py:518] Training: step 33397
I0513 02:00:46.981240 140339465779200 trainer.py:518] Training: step 33397
I0513 02:00:46.981348 140529434355712 trainer.py:518] Training: step 33397
I0513 02:00:46.982414 140311617681408 trainer.py:518] Training: step 33397
I0513 02:00:46.981050 140440593786880 trainer.py:518] Training: step 33397
I0513 02:00:46.981663 140031916288000 trainer.py:518] Training: step 33397
I0513 02:01:04.871889 140095893518336 trainer.py:518] Training: step 33398
I0513 02:01:04.871338 139674595104768 trainer.py:518] Training: step 33398
I0513 02:01:04.871314 140339465779200 trainer.py:518] Training: step 33398
I0513 02:01:04.872408 140311617681408 trainer.py:518] Training: step 33398
I0513 02:01:04.871283 140440593786880 trainer.py:518] Training: step 33398
I0513 02:01:04.872225 140595318249472 trainer.py:518] Training: step 33398
I0513 02:01:04.872895 140529434355712 trainer.py:518] Training: step 33398
I0513 02:01:04.871826 140031916288000 trainer.py:518] Training: step 33398
I0513 02:01:13.818587 140439986517568 logging_writer.py:48] [33380] collection=train accuracy=0.567802, cross_ent_loss=16.55763053894043, cross_ent_loss_per_all_target_tokens=2.10541e-05, experts/auxiliary_loss=0.16070564091205597, experts/expert_usage=7.95534610748291, experts/fraction_tokens_left_behind=0.2475050538778305, experts/router_confidence=3.4773621559143066, experts/router_z_loss=0.0003083042975049466, learning_rate=0.00547385, learning_rate/current=0.00547348, loss=16.726200103759766, loss_per_all_target_tokens=2.12685e-05, loss_per_nonpadding_target_token=2.13895e-05, non_padding_fraction/loss_weights=0.994339, timing/seconds=89.44979858398438, timing/seqs=3840, timing/seqs_per_second=42.929107666015625, timing/seqs_per_second_per_core=1.3415346145629883, timing/steps_per_second=0.11179454624652863, timing/target_tokens_per_second=87918.8125, timing/target_tokens_per_second_per_core=2747.462890625, timing/uptime=8158.76, z_loss=0.0075553664937615395, z_loss_per_all_target_tokens=9.60715e-09
I0513 02:01:22.762790 140095893518336 trainer.py:518] Training: step 33399
I0513 02:01:22.761800 139674595104768 trainer.py:518] Training: step 33399
I0513 02:01:22.762824 140311617681408 trainer.py:518] Training: step 33399
I0513 02:01:22.762225 140339465779200 trainer.py:518] Training: step 33399
I0513 02:01:22.761771 140440593786880 trainer.py:518] Training: step 33399
I0513 02:01:22.762987 140529434355712 trainer.py:518] Training: step 33399
I0513 02:01:22.763079 140595318249472 trainer.py:518] Training: step 33399
I0513 02:01:22.762882 140031916288000 trainer.py:518] Training: step 33399
I0513 02:01:40.652160 140339465779200 trainer.py:518] Training: step 33406
I0513 02:01:40.653004 140529434355712 trainer.py:518] Training: step 33404
I0513 02:01:40.656721 140439986517568 logging_writer.py:48] [33390] collection=train accuracy=0.562661, cross_ent_loss=16.791980743408203, cross_ent_loss_per_all_target_tokens=2.13521e-05, experts/auxiliary_loss=0.16069263219833374, experts/expert_usage=7.942101955413818, experts/fraction_tokens_left_behind=0.24596261978149414, experts/router_confidence=3.47936749458313, experts/router_z_loss=0.0003138876927550882, learning_rate=0.00547303, learning_rate/current=0.00547266, loss=16.959959030151367, loss_per_all_target_tokens=2.15657e-05, loss_per_nonpadding_target_token=2.17227e-05, non_padding_fraction/loss_weights=0.992772, timing/seconds=89.45027923583984, timing/seqs=3840, timing/seqs_per_second=42.92887878417969, timing/seqs_per_second_per_core=1.3415274620056152, timing/steps_per_second=0.11179395020008087, timing/target_tokens_per_second=87918.34375, timing/target_tokens_per_second_per_core=2747.4482421875, timing/uptime=8177.47, z_loss=0.006971112918108702, z_loss_per_all_target_tokens=8.86423e-09
I0513 02:01:40.676594 139674595104768 trainer.py:518] Training: step 33408
I0513 02:01:40.675479 140440593786880 trainer.py:518] Training: step 33407
I0513 02:01:40.686957 140095893518336 trainer.py:518] Training: step 33407
I0513 02:01:40.681797 140031916288000 trainer.py:518] Training: step 33408
I0513 02:01:41.100518 140311617681408 trainer.py:518] Training: step 33408
I0513 02:01:49.599021 140595318249472 trainer.py:518] Training: step 33410
I0513 02:02:16.436390 140095893518336 trainer.py:518] Training: step 33412
I0513 02:02:16.435168 139674595104768 trainer.py:518] Training: step 33412
I0513 02:02:16.435443 140339465779200 trainer.py:518] Training: step 33412
I0513 02:02:16.435495 140529434355712 trainer.py:518] Training: step 33412
I0513 02:02:16.436551 140311617681408 trainer.py:518] Training: step 33412
I0513 02:02:16.435672 140595318249472 trainer.py:518] Training: step 33412
I0513 02:02:16.435359 140440593786880 trainer.py:518] Training: step 33412
I0513 02:02:16.435949 140031916288000 trainer.py:518] Training: step 33412
I0513 02:02:34.325471 140095893518336 trainer.py:518] Training: step 33413
I0513 02:02:34.324508 139674595104768 trainer.py:518] Training: step 33413
I0513 02:02:34.325571 140311617681408 trainer.py:518] Training: step 33413
I0513 02:02:34.324668 140339465779200 trainer.py:518] Training: step 33413
I0513 02:02:34.324873 140529434355712 trainer.py:518] Training: step 33413
I0513 02:02:34.324711 140595318249472 trainer.py:518] Training: step 33413
I0513 02:02:34.324837 140440593786880 trainer.py:518] Training: step 33413
I0513 02:02:34.325072 140031916288000 trainer.py:518] Training: step 33413
I0513 02:02:52.214664 139674595104768 trainer.py:518] Training: step 33414
I0513 02:02:52.215262 140529434355712 trainer.py:518] Training: step 33414
I0513 02:02:52.216227 140311617681408 trainer.py:518] Training: step 33414
I0513 02:02:52.215430 140339465779200 trainer.py:518] Training: step 33414
I0513 02:02:52.215571 140595318249472 trainer.py:518] Training: step 33414
I0513 02:02:52.215170 140440593786880 trainer.py:518] Training: step 33414
I0513 02:02:52.215587 140031916288000 trainer.py:518] Training: step 33414
I0513 02:03:01.161386 140095893518336 trainer.py:518] Training: step 33414
