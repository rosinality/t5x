# Register necessary SeqIO Tasks/Mixtures.
from __gin__ import dynamic_registration
import t5.data.mixtures
import __main__ as train_script
import seqio
import flaxformer
from t5x import utils
# from t5x import trainer
from t5x.contrib.moe import trainer as moe_trainer
from t5x.contrib.moe import models


include 'flaxformer/flaxformer/t5x/configs/moe/models/st_moe_decoder_only_xl.gin'
include 't5x/contrib/moe/configs/runs/pretrain.gin'
# include 't5x/configs/runs/pretrain.gin'

# Vocabulary (shared by encoder and decoder)
VOCABULARY = @seqio.SentencePieceVocabulary()
seqio.SentencePieceVocabulary.sentencepiece_model_file = "gs://fuzhao/tokenizers/umt5.256000/sentencepiece.model"
seqio.SentencePieceVocabulary.extra_ids = 300

# MIXTURE_OR_TASK_NAME = "mix_ul2_v2"
# TASK_FEATURE_LENGTHS = {"inputs": 1024, "targets": 1024}

ROUTER_Z_LOSS_FACTOR = 0.001
GROUP_SIZE = 8192
MIXTURE_OR_TASK_NAME = "mix_full_lm_v2"
TASK_FEATURE_LENGTHS = {"targets": 2048}
USE_CACHED_TASKS = False
TRAIN_STEPS = 500000
DROPOUT_RATE = 0.0
BATCH_SIZE = 1024
NUM_MODEL_PARTITIONS = 16
NUM_EXPERT_PARTITIONS = 32
NUM_EMBEDDINGS = 256384  # vocab size rounded to a multiple of 128 for TPU efficiency

train_script.train:
  eval_period = 2000
  
moe_trainer.MoeTrainer:
  num_microbatches = 16

# Keep slightly fewer checkpoints than pre-training defaults.
utils.SaveCheckpointConfig.period = 2000
utils.SaveCheckpointConfig.keep = 20

# Use 32 on 256 TPU v3
# moe_trainer.MoeTrainer:
#   num_microbatches = 32

# models.MoeDecoderOnlyModel.inputs_bidirectional_attention=True
 
