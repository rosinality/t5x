Using ssh batch size of 8. Attempting to SSH into 1 nodes with a total of 8 workers.
SSH: Attempting to connect to worker 0...
SSH: Attempting to connect to worker 1...
SSH: Attempting to connect to worker 2...
SSH: Attempting to connect to worker 3...
SSH: Attempting to connect to worker 4...
SSH: Attempting to connect to worker 5...
SSH: Attempting to connect to worker 6...
SSH: Attempting to connect to worker 7...
2024-05-12 23:44:39.742643: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2024-05-12 23:44:39.807818: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2024-05-12 23:44:39.815020: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2024-05-12 23:44:39.820551: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2024-05-12 23:44:39.846768: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2024-05-12 23:44:39.936138: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2024-05-12 23:44:40.014239: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2024-05-12 23:44:40.055832: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
WARNING:absl:Type handler registry overriding type "<class 'float'>" collision on scalar
WARNING:absl:Type handler registry overriding type "<class 'bytes'>" collision on scalar
WARNING:absl:Type handler registry overriding type "<class 'numpy.number'>" collision on scalar
WARNING:absl:Type handler registry overriding type "<class 'float'>" collision on scalar
WARNING:absl:Type handler registry overriding type "<class 'bytes'>" collision on scalar
WARNING:absl:Type handler registry overriding type "<class 'numpy.number'>" collision on scalar
WARNING:absl:Type handler registry overriding type "<class 'float'>" collision on scalar
WARNING:absl:Type handler registry overriding type "<class 'bytes'>" collision on scalar
WARNING:absl:Type handler registry overriding type "<class 'numpy.number'>" collision on scalar
WARNING:absl:Type handler registry overriding type "<class 'float'>" collision on scalar
WARNING:absl:Type handler registry overriding type "<class 'bytes'>" collision on scalar
WARNING:absl:Type handler registry overriding type "<class 'numpy.number'>" collision on scalar
WARNING:absl:Type handler registry overriding type "<class 'float'>" collision on scalar
WARNING:absl:Type handler registry overriding type "<class 'bytes'>" collision on scalar
WARNING:absl:Type handler registry overriding type "<class 'numpy.number'>" collision on scalar
WARNING:absl:Type handler registry overriding type "<class 'float'>" collision on scalar
WARNING:absl:Type handler registry overriding type "<class 'bytes'>" collision on scalar
WARNING:absl:Type handler registry overriding type "<class 'numpy.number'>" collision on scalar
WARNING:absl:Type handler registry overriding type "<class 'float'>" collision on scalar
WARNING:absl:Type handler registry overriding type "<class 'bytes'>" collision on scalar
WARNING:absl:Type handler registry overriding type "<class 'numpy.number'>" collision on scalar
WARNING:absl:Type handler registry overriding type "<class 'float'>" collision on scalar
WARNING:absl:Type handler registry overriding type "<class 'bytes'>" collision on scalar
WARNING:absl:Type handler registry overriding type "<class 'numpy.number'>" collision on scalar
I0512 23:44:41.991068 139876724979712 resource_reader.py:50] system_path_file_exists:t5x/examples/t5/t5_1_1/examples/openmoe_large_ul2.gin
I0512 23:44:41.991989 139876724979712 resource_reader.py:37] gin-config opened resource file:/home/rosinality/t5x/t5x/examples/t5/t5_1_1/examples/openmoe_large_ul2.gin
I0512 23:44:42.016283 140143678494720 resource_reader.py:50] system_path_file_exists:t5x/examples/t5/t5_1_1/examples/openmoe_large_ul2.gin
I0512 23:44:42.017135 140143678494720 resource_reader.py:37] gin-config opened resource file:/home/rosinality/t5x/t5x/examples/t5/t5_1_1/examples/openmoe_large_ul2.gin
I0512 23:44:42.049173 139856202532864 resource_reader.py:50] system_path_file_exists:t5x/examples/t5/t5_1_1/examples/openmoe_large_ul2.gin
I0512 23:44:42.050073 139856202532864 resource_reader.py:37] gin-config opened resource file:/home/rosinality/t5x/t5x/examples/t5/t5_1_1/examples/openmoe_large_ul2.gin
I0512 23:44:42.050368 140193652815872 resource_reader.py:50] system_path_file_exists:t5x/examples/t5/t5_1_1/examples/openmoe_large_ul2.gin
I0512 23:44:42.051253 140193652815872 resource_reader.py:37] gin-config opened resource file:/home/rosinality/t5x/t5x/examples/t5/t5_1_1/examples/openmoe_large_ul2.gin
I0512 23:44:42.066251 139666521167872 resource_reader.py:50] system_path_file_exists:t5x/examples/t5/t5_1_1/examples/openmoe_large_ul2.gin
I0512 23:44:42.067105 139666521167872 resource_reader.py:37] gin-config opened resource file:/home/rosinality/t5x/t5x/examples/t5/t5_1_1/examples/openmoe_large_ul2.gin
I0512 23:44:42.178485 140643590531072 resource_reader.py:50] system_path_file_exists:t5x/examples/t5/t5_1_1/examples/openmoe_large_ul2.gin
I0512 23:44:42.179413 140643590531072 resource_reader.py:37] gin-config opened resource file:/home/rosinality/t5x/t5x/examples/t5/t5_1_1/examples/openmoe_large_ul2.gin
I0512 23:44:42.243813 139876724979712 resource_reader.py:50] system_path_file_exists:flaxformer/flaxformer/t5x/configs/moe/models/st_moe_decoder_only_large.gin
E0512 23:44:42.244360 139876724979712 resource_reader.py:55] Path not found: flaxformer/flaxformer/t5x/configs/moe/models/st_moe_decoder_only_large.gin
I0512 23:44:42.244422 139876724979712 resource_reader.py:50] system_path_file_exists:./flaxformer/flaxformer/t5x/configs/moe/models/st_moe_decoder_only_large.gin
E0512 23:44:42.244468 139876724979712 resource_reader.py:55] Path not found: ./flaxformer/flaxformer/t5x/configs/moe/models/st_moe_decoder_only_large.gin
I0512 23:44:42.267415 140143678494720 resource_reader.py:50] system_path_file_exists:flaxformer/flaxformer/t5x/configs/moe/models/st_moe_decoder_only_large.gin
E0512 23:44:42.267871 140143678494720 resource_reader.py:55] Path not found: flaxformer/flaxformer/t5x/configs/moe/models/st_moe_decoder_only_large.gin
I0512 23:44:42.267934 140143678494720 resource_reader.py:50] system_path_file_exists:./flaxformer/flaxformer/t5x/configs/moe/models/st_moe_decoder_only_large.gin
E0512 23:44:42.267980 140143678494720 resource_reader.py:55] Path not found: ./flaxformer/flaxformer/t5x/configs/moe/models/st_moe_decoder_only_large.gin
I0512 23:44:42.300675 139876724979712 resource_reader.py:50] system_path_file_exists:flaxformer/flaxformer/t5x/configs/moe/models/tokens_choose_decoder_only_large.gin
E0512 23:44:42.300845 139876724979712 resource_reader.py:55] Path not found: flaxformer/flaxformer/t5x/configs/moe/models/tokens_choose_decoder_only_large.gin
I0512 23:44:42.300900 139876724979712 resource_reader.py:50] system_path_file_exists:./flaxformer/flaxformer/t5x/configs/moe/models/tokens_choose_decoder_only_large.gin
E0512 23:44:42.300943 139876724979712 resource_reader.py:55] Path not found: ./flaxformer/flaxformer/t5x/configs/moe/models/tokens_choose_decoder_only_large.gin
I0512 23:44:42.301818 139876724979712 resource_reader.py:50] system_path_file_exists:flaxformer/flaxformer/t5x/configs/moe/architectures/moe_decoder_only.gin
E0512 23:44:42.301951 139876724979712 resource_reader.py:55] Path not found: flaxformer/flaxformer/t5x/configs/moe/architectures/moe_decoder_only.gin
I0512 23:44:42.302002 139876724979712 resource_reader.py:50] system_path_file_exists:./flaxformer/flaxformer/t5x/configs/moe/architectures/moe_decoder_only.gin
E0512 23:44:42.302043 139876724979712 resource_reader.py:55] Path not found: ./flaxformer/flaxformer/t5x/configs/moe/architectures/moe_decoder_only.gin
I0512 23:44:42.302628 139856202532864 resource_reader.py:50] system_path_file_exists:flaxformer/flaxformer/t5x/configs/moe/models/st_moe_decoder_only_large.gin
E0512 23:44:42.303111 139856202532864 resource_reader.py:55] Path not found: flaxformer/flaxformer/t5x/configs/moe/models/st_moe_decoder_only_large.gin
I0512 23:44:42.303173 139856202532864 resource_reader.py:50] system_path_file_exists:./flaxformer/flaxformer/t5x/configs/moe/models/st_moe_decoder_only_large.gin
E0512 23:44:42.303218 139856202532864 resource_reader.py:55] Path not found: ./flaxformer/flaxformer/t5x/configs/moe/models/st_moe_decoder_only_large.gin
I0512 23:44:42.303167 140193652815872 resource_reader.py:50] system_path_file_exists:flaxformer/flaxformer/t5x/configs/moe/models/st_moe_decoder_only_large.gin
E0512 23:44:42.303655 140193652815872 resource_reader.py:55] Path not found: flaxformer/flaxformer/t5x/configs/moe/models/st_moe_decoder_only_large.gin
I0512 23:44:42.303717 140193652815872 resource_reader.py:50] system_path_file_exists:./flaxformer/flaxformer/t5x/configs/moe/models/st_moe_decoder_only_large.gin
I0512 23:44:42.304663 139666521167872 resource_reader.py:50] system_path_file_exists:flaxformer/flaxformer/t5x/configs/moe/models/st_moe_decoder_only_large.gin
E0512 23:44:42.305116 139666521167872 resource_reader.py:55] Path not found: flaxformer/flaxformer/t5x/configs/moe/models/st_moe_decoder_only_large.gin
I0512 23:44:42.305173 139666521167872 resource_reader.py:50] system_path_file_exists:./flaxformer/flaxformer/t5x/configs/moe/models/st_moe_decoder_only_large.gin
E0512 23:44:42.303763 140193652815872 resource_reader.py:55] Path not found: ./flaxformer/flaxformer/t5x/configs/moe/models/st_moe_decoder_only_large.gin
E0512 23:44:42.305215 139666521167872 resource_reader.py:55] Path not found: ./flaxformer/flaxformer/t5x/configs/moe/models/st_moe_decoder_only_large.gin
I0512 23:44:42.323995 140143678494720 resource_reader.py:50] system_path_file_exists:flaxformer/flaxformer/t5x/configs/moe/models/tokens_choose_decoder_only_large.gin
E0512 23:44:42.324176 140143678494720 resource_reader.py:55] Path not found: flaxformer/flaxformer/t5x/configs/moe/models/tokens_choose_decoder_only_large.gin
I0512 23:44:42.324236 140143678494720 resource_reader.py:50] system_path_file_exists:./flaxformer/flaxformer/t5x/configs/moe/models/tokens_choose_decoder_only_large.gin
E0512 23:44:42.324281 140143678494720 resource_reader.py:55] Path not found: ./flaxformer/flaxformer/t5x/configs/moe/models/tokens_choose_decoder_only_large.gin
I0512 23:44:42.325174 140143678494720 resource_reader.py:50] system_path_file_exists:flaxformer/flaxformer/t5x/configs/moe/architectures/moe_decoder_only.gin
E0512 23:44:42.325313 140143678494720 resource_reader.py:55] Path not found: flaxformer/flaxformer/t5x/configs/moe/architectures/moe_decoder_only.gin
I0512 23:44:42.325368 140143678494720 resource_reader.py:50] system_path_file_exists:./flaxformer/flaxformer/t5x/configs/moe/architectures/moe_decoder_only.gin
E0512 23:44:42.325410 140143678494720 resource_reader.py:55] Path not found: ./flaxformer/flaxformer/t5x/configs/moe/architectures/moe_decoder_only.gin
I0512 23:44:42.316167 140260698863616 resource_reader.py:50] system_path_file_exists:t5x/examples/t5/t5_1_1/examples/openmoe_large_ul2.gin
I0512 23:44:42.317095 140260698863616 resource_reader.py:37] gin-config opened resource file:/home/rosinality/t5x/t5x/examples/t5/t5_1_1/examples/openmoe_large_ul2.gin
I0512 23:44:42.346398 139876724979712 resource_reader.py:50] system_path_file_exists:t5x/contrib/moe/configs/runs/pretrain.gin
I0512 23:44:42.358248 139666521167872 resource_reader.py:50] system_path_file_exists:flaxformer/flaxformer/t5x/configs/moe/models/tokens_choose_decoder_only_large.gin
E0512 23:44:42.358448 139666521167872 resource_reader.py:55] Path not found: flaxformer/flaxformer/t5x/configs/moe/models/tokens_choose_decoder_only_large.gin
I0512 23:44:42.358504 139666521167872 resource_reader.py:50] system_path_file_exists:./flaxformer/flaxformer/t5x/configs/moe/models/tokens_choose_decoder_only_large.gin
E0512 23:44:42.358546 139666521167872 resource_reader.py:55] Path not found: ./flaxformer/flaxformer/t5x/configs/moe/models/tokens_choose_decoder_only_large.gin
I0512 23:44:42.359412 139666521167872 resource_reader.py:50] system_path_file_exists:flaxformer/flaxformer/t5x/configs/moe/architectures/moe_decoder_only.gin
I0512 23:44:42.359792 139856202532864 resource_reader.py:50] system_path_file_exists:flaxformer/flaxformer/t5x/configs/moe/models/tokens_choose_decoder_only_large.gin
E0512 23:44:42.359972 139856202532864 resource_reader.py:55] Path not found: flaxformer/flaxformer/t5x/configs/moe/models/tokens_choose_decoder_only_large.gin
E0512 23:44:42.359540 139666521167872 resource_reader.py:55] Path not found: flaxformer/flaxformer/t5x/configs/moe/architectures/moe_decoder_only.gin
I0512 23:44:42.359591 139666521167872 resource_reader.py:50] system_path_file_exists:./flaxformer/flaxformer/t5x/configs/moe/architectures/moe_decoder_only.gin
E0512 23:44:42.359630 139666521167872 resource_reader.py:55] Path not found: ./flaxformer/flaxformer/t5x/configs/moe/architectures/moe_decoder_only.gin
I0512 23:44:42.360029 139856202532864 resource_reader.py:50] system_path_file_exists:./flaxformer/flaxformer/t5x/configs/moe/models/tokens_choose_decoder_only_large.gin
E0512 23:44:42.360072 139856202532864 resource_reader.py:55] Path not found: ./flaxformer/flaxformer/t5x/configs/moe/models/tokens_choose_decoder_only_large.gin
I0512 23:44:42.360951 139856202532864 resource_reader.py:50] system_path_file_exists:flaxformer/flaxformer/t5x/configs/moe/architectures/moe_decoder_only.gin
E0512 23:44:42.361086 139856202532864 resource_reader.py:55] Path not found: flaxformer/flaxformer/t5x/configs/moe/architectures/moe_decoder_only.gin
I0512 23:44:42.361137 139856202532864 resource_reader.py:50] system_path_file_exists:./flaxformer/flaxformer/t5x/configs/moe/architectures/moe_decoder_only.gin
E0512 23:44:42.361178 139856202532864 resource_reader.py:55] Path not found: ./flaxformer/flaxformer/t5x/configs/moe/architectures/moe_decoder_only.gin
I0512 23:44:42.360223 140193652815872 resource_reader.py:50] system_path_file_exists:flaxformer/flaxformer/t5x/configs/moe/models/tokens_choose_decoder_only_large.gin
E0512 23:44:42.360418 140193652815872 resource_reader.py:55] Path not found: flaxformer/flaxformer/t5x/configs/moe/models/tokens_choose_decoder_only_large.gin
I0512 23:44:42.360478 140193652815872 resource_reader.py:50] system_path_file_exists:./flaxformer/flaxformer/t5x/configs/moe/models/tokens_choose_decoder_only_large.gin
E0512 23:44:42.360523 140193652815872 resource_reader.py:55] Path not found: ./flaxformer/flaxformer/t5x/configs/moe/models/tokens_choose_decoder_only_large.gin
I0512 23:44:42.361394 140193652815872 resource_reader.py:50] system_path_file_exists:flaxformer/flaxformer/t5x/configs/moe/architectures/moe_decoder_only.gin
E0512 23:44:42.361531 140193652815872 resource_reader.py:55] Path not found: flaxformer/flaxformer/t5x/configs/moe/architectures/moe_decoder_only.gin
I0512 23:44:42.361584 140193652815872 resource_reader.py:50] system_path_file_exists:./flaxformer/flaxformer/t5x/configs/moe/architectures/moe_decoder_only.gin
E0512 23:44:42.361634 140193652815872 resource_reader.py:55] Path not found: ./flaxformer/flaxformer/t5x/configs/moe/architectures/moe_decoder_only.gin
I0512 23:44:42.349107 140180528838656 resource_reader.py:50] system_path_file_exists:t5x/examples/t5/t5_1_1/examples/openmoe_large_ul2.gin
I0512 23:44:42.350016 140180528838656 resource_reader.py:37] gin-config opened resource file:/home/rosinality/t5x/t5x/examples/t5/t5_1_1/examples/openmoe_large_ul2.gin
I0512 23:44:42.369948 140143678494720 resource_reader.py:50] system_path_file_exists:t5x/contrib/moe/configs/runs/pretrain.gin
I0512 23:44:42.401537 139666521167872 resource_reader.py:50] system_path_file_exists:t5x/contrib/moe/configs/runs/pretrain.gin
I0512 23:44:42.405609 139856202532864 resource_reader.py:50] system_path_file_exists:t5x/contrib/moe/configs/runs/pretrain.gin
I0512 23:44:42.405796 140193652815872 resource_reader.py:50] system_path_file_exists:t5x/contrib/moe/configs/runs/pretrain.gin
I0512 23:44:42.346760 139876724979712 resource_reader.py:37] gin-config opened resource file:/home/rosinality/t5x/t5x/contrib/moe/configs/runs/pretrain.gin
I0512 23:44:42.347299 139876724979712 resource_reader.py:50] system_path_file_exists:t5x/configs/runs/pretrain.gin
I0512 23:44:42.347608 139876724979712 resource_reader.py:37] gin-config opened resource file:/home/rosinality/t5x/t5x/configs/runs/pretrain.gin
I0512 23:44:42.361502 139876724979712 gin_utils.py:83] Gin Configuration:
I0512 23:44:42.377366 139876724979712 gin_utils.py:85] from __gin__ import dynamic_registration
I0512 23:44:42.377435 139876724979712 gin_utils.py:85] import __main__ as train_script
I0512 23:44:42.377477 139876724979712 gin_utils.py:85] from flax import linen
I0512 23:44:42.377512 139876724979712 gin_utils.py:85] import flaxformer
I0512 23:44:42.377545 139876724979712 gin_utils.py:85] from flaxformer.architectures.moe import moe_architecture
I0512 23:44:42.377577 139876724979712 gin_utils.py:85] from flaxformer.architectures.moe import moe_enums
I0512 23:44:42.377609 139876724979712 gin_utils.py:85] from flaxformer.architectures.moe import moe_layers
I0512 23:44:42.377640 139876724979712 gin_utils.py:85] from flaxformer.architectures.moe import routing
I0512 23:44:42.377671 139876724979712 gin_utils.py:85] from flaxformer.architectures.t5 import t5_architecture
I0512 23:44:42.377702 139876724979712 gin_utils.py:85] from flaxformer.components.attention import dense_attention
I0512 23:44:42.377733 139876724979712 gin_utils.py:85] from flaxformer.components.attention import memory_efficient_attention
I0512 23:44:42.377764 139876724979712 gin_utils.py:85] from flaxformer.components import dense
I0512 23:44:42.377796 139876724979712 gin_utils.py:85] from flaxformer.components import embedding
I0512 23:44:42.377827 139876724979712 gin_utils.py:85] from flaxformer.components import layer_norm
I0512 23:44:42.377858 139876724979712 gin_utils.py:85] from gin import config
I0512 23:44:42.377889 139876724979712 gin_utils.py:85] import seqio
I0512 23:44:42.377919 139876724979712 gin_utils.py:85] import t5.data.mixtures
I0512 23:44:42.377951 139876724979712 gin_utils.py:85] from t5x import adafactor
I0512 23:44:42.377982 139876724979712 gin_utils.py:85] from t5x.contrib.moe import adafactor_utils
I0512 23:44:42.378013 139876724979712 gin_utils.py:85] from t5x.contrib.moe import models
I0512 23:44:42.378044 139876724979712 gin_utils.py:85] from t5x.contrib.moe import partitioning as moe_partitioning
I0512 23:44:42.378075 139876724979712 gin_utils.py:85] from t5x.contrib.moe import trainer as moe_trainer
I0512 23:44:42.378105 139876724979712 gin_utils.py:85] from t5x import gin_utils
I0512 23:44:42.378137 139876724979712 gin_utils.py:85] from t5x import partitioning
I0512 23:44:42.378168 139876724979712 gin_utils.py:85] from t5x import trainer
I0512 23:44:42.378199 139876724979712 gin_utils.py:85] from t5x import utils
I0512 23:44:42.378239 139876724979712 gin_utils.py:85] 
I0512 23:44:42.378272 139876724979712 gin_utils.py:85] # Macros:
I0512 23:44:42.378304 139876724979712 gin_utils.py:85] # ==============================================================================
I0512 23:44:42.378335 139876724979712 gin_utils.py:85] ACTIVATION_DTYPE = 'bfloat16'
I0512 23:44:42.378366 139876724979712 gin_utils.py:85] ACTIVATION_PARTITIONING_DIMS = 1
I0512 23:44:42.378397 139876724979712 gin_utils.py:85] ARCHITECTURE = @t5_architecture.DecoderOnly()
I0512 23:44:42.378428 139876724979712 gin_utils.py:85] AUX_LOSS_FACTOR = 0.01
I0512 23:44:42.378459 139876724979712 gin_utils.py:85] BATCH_SIZE = 384
I0512 23:44:42.378493 139876724979712 gin_utils.py:85] BIAS_INIT = @bias_init/linen.initializers.normal()
I0512 23:44:42.378525 139876724979712 gin_utils.py:85] DECODER_SPARSE_LAYOUT = %flaxformer.architectures.moe.moe_enums.LayerLayout.MIXED
I0512 23:44:42.378556 139876724979712 gin_utils.py:85] DROPOUT_FACTORY = @dropout_factory/linen.Dropout
I0512 23:44:42.378587 139876724979712 gin_utils.py:85] DROPOUT_RATE = 0.0
I0512 23:44:42.378618 139876724979712 gin_utils.py:85] EMBED_DIM = 2048
I0512 23:44:42.378649 139876724979712 gin_utils.py:85] EVAL_EXPERT_CAPACITY_FACTOR = 2.0
I0512 23:44:42.378680 139876724979712 gin_utils.py:85] EXPERT_DROPOUT_RATE = %DROPOUT_RATE
I0512 23:44:42.378711 139876724979712 gin_utils.py:85] EXPERT_MLP_DIM = %MLP_DIM
I0512 23:44:42.378742 139876724979712 gin_utils.py:85] GROUP_SIZE = 4096
I0512 23:44:42.378773 139876724979712 gin_utils.py:85] HEAD_DIM = 128
I0512 23:44:42.378804 139876724979712 gin_utils.py:85] JITTER_NOISE = 0.0
I0512 23:44:42.378835 139876724979712 gin_utils.py:85] LABEL_SMOOTHING = 0.0
I0512 23:44:42.378866 139876724979712 gin_utils.py:85] LOSS_NORMALIZING_FACTOR = 'NUM_REAL_TARGET_TOKENS'
I0512 23:44:42.378897 139876724979712 gin_utils.py:85] MIXTURE_OR_TASK_MODULE = None
I0512 23:44:42.378928 139876724979712 gin_utils.py:85] MIXTURE_OR_TASK_NAME = 'mix_ul2_test'
I0512 23:44:42.378959 139876724979712 gin_utils.py:85] MLP_DIM = 8192
I0512 23:44:42.378990 139876724979712 gin_utils.py:85] MODEL = @models.MoeDecoderOnlyModel()
I0512 23:44:42.379021 139876724979712 gin_utils.py:85] MODEL_DIR = 'gs://rosinality-tpu-bucket/openmoe_8b_ul2/training'
I0512 23:44:42.379052 139876724979712 gin_utils.py:85] MODEL_PARALLEL_SUBMESH = None
I0512 23:44:42.379101 139876724979712 gin_utils.py:85] MOE_TRUNCATED_DTYPE = 'bfloat16'
I0512 23:44:42.379140 139876724979712 gin_utils.py:85] NUM_DECODER_LAYERS = 24
I0512 23:44:42.379173 139876724979712 gin_utils.py:85] NUM_DECODER_SPARSE_LAYERS = 4
I0512 23:44:42.379204 139876724979712 gin_utils.py:85] NUM_EMBEDDINGS = 256384
I0512 23:44:42.379241 139876724979712 gin_utils.py:85] NUM_EXPERT_PARTITIONS = 8
I0512 23:44:42.379273 139876724979712 gin_utils.py:85] NUM_EXPERTS = 8
I0512 23:44:42.379304 139876724979712 gin_utils.py:85] NUM_HEADS = 24
I0512 23:44:42.379335 139876724979712 gin_utils.py:85] NUM_MODEL_PARTITIONS = 4
I0512 23:44:42.379366 139876724979712 gin_utils.py:85] NUM_SELECTED_EXPERTS = 2
I0512 23:44:42.379397 139876724979712 gin_utils.py:85] OPTIMIZER = @adafactor.Adafactor()
I0512 23:44:42.379428 139876724979712 gin_utils.py:85] RANDOM_SEED = None
I0512 23:44:42.379459 139876724979712 gin_utils.py:85] ROUTER_Z_LOSS_FACTOR = 0.0001
I0512 23:44:42.379492 139876724979712 gin_utils.py:85] SCALE = 0.1
I0512 23:44:42.379523 139876724979712 gin_utils.py:85] SHUFFLE_TRAIN_EXAMPLES = True
I0512 23:44:42.379554 139876724979712 gin_utils.py:85] TASK_FEATURE_LENGTHS = {'inputs': 1024, 'targets': 1024}
I0512 23:44:42.379585 139876724979712 gin_utils.py:85] TRAIN_EXPERT_CAPACITY_FACTOR = 1.25
I0512 23:44:42.379616 139876724979712 gin_utils.py:85] TRAIN_STEPS = 500000
I0512 23:44:42.379647 139876724979712 gin_utils.py:85] USE_CACHED_TASKS = False
I0512 23:44:42.379678 139876724979712 gin_utils.py:85] USE_HARDWARE_RNG = False
I0512 23:44:42.379709 139876724979712 gin_utils.py:85] VOCABULARY = @seqio.SentencePieceVocabulary()
I0512 23:44:42.379739 139876724979712 gin_utils.py:85] Z_LOSS = 0.0001
I0512 23:44:42.379770 139876724979712 gin_utils.py:85] 
I0512 23:44:42.379802 139876724979712 gin_utils.py:85] # Parameters for adafactor.Adafactor:
I0512 23:44:42.379832 139876724979712 gin_utils.py:85] # ==============================================================================
I0512 23:44:42.379864 139876724979712 gin_utils.py:85] adafactor.Adafactor.decay_rate = 0.8
I0512 23:44:42.379895 139876724979712 gin_utils.py:85] adafactor.Adafactor.logical_factor_rules = @adafactor_utils.logical_factor_rules()
I0512 23:44:42.379926 139876724979712 gin_utils.py:85] adafactor.Adafactor.step_offset = 0
I0512 23:44:42.379956 139876724979712 gin_utils.py:85] 
I0512 23:44:42.370314 140143678494720 resource_reader.py:37] gin-config opened resource file:/home/rosinality/t5x/t5x/contrib/moe/configs/runs/pretrain.gin
I0512 23:44:42.370830 140143678494720 resource_reader.py:50] system_path_file_exists:t5x/configs/runs/pretrain.gin
I0512 23:44:42.371138 140143678494720 resource_reader.py:37] gin-config opened resource file:/home/rosinality/t5x/t5x/configs/runs/pretrain.gin
I0512 23:44:42.385050 140143678494720 gin_utils.py:83] Gin Configuration:
I0512 23:44:42.400856 140143678494720 gin_utils.py:85] from __gin__ import dynamic_registration
I0512 23:44:42.400927 140143678494720 gin_utils.py:85] import __main__ as train_script
I0512 23:44:42.400970 140143678494720 gin_utils.py:85] from flax import linen
I0512 23:44:42.401006 140143678494720 gin_utils.py:85] import flaxformer
I0512 23:44:42.401040 140143678494720 gin_utils.py:85] from flaxformer.architectures.moe import moe_architecture
I0512 23:44:42.401074 140143678494720 gin_utils.py:85] from flaxformer.architectures.moe import moe_enums
I0512 23:44:42.401107 140143678494720 gin_utils.py:85] from flaxformer.architectures.moe import moe_layers
I0512 23:44:42.401140 140143678494720 gin_utils.py:85] from flaxformer.architectures.moe import routing
I0512 23:44:42.401174 140143678494720 gin_utils.py:85] from flaxformer.architectures.t5 import t5_architecture
I0512 23:44:42.401206 140143678494720 gin_utils.py:85] from flaxformer.components.attention import dense_attention
I0512 23:44:42.401240 140143678494720 gin_utils.py:85] from flaxformer.components.attention import memory_efficient_attention
I0512 23:44:42.401272 140143678494720 gin_utils.py:85] from flaxformer.components import dense
I0512 23:44:42.401309 140143678494720 gin_utils.py:85] from flaxformer.components import embedding
I0512 23:44:42.401343 140143678494720 gin_utils.py:85] from flaxformer.components import layer_norm
I0512 23:44:42.401376 140143678494720 gin_utils.py:85] from gin import config
I0512 23:44:42.401408 140143678494720 gin_utils.py:85] import seqio
I0512 23:44:42.401441 140143678494720 gin_utils.py:85] import t5.data.mixtures
I0512 23:44:42.401474 140143678494720 gin_utils.py:85] from t5x import adafactor
I0512 23:44:42.401507 140143678494720 gin_utils.py:85] from t5x.contrib.moe import adafactor_utils
I0512 23:44:42.401551 140143678494720 gin_utils.py:85] from t5x.contrib.moe import models
I0512 23:44:42.401586 140143678494720 gin_utils.py:85] from t5x.contrib.moe import partitioning as moe_partitioning
I0512 23:44:42.401620 140143678494720 gin_utils.py:85] from t5x.contrib.moe import trainer as moe_trainer
I0512 23:44:42.401653 140143678494720 gin_utils.py:85] from t5x import gin_utils
I0512 23:44:42.401685 140143678494720 gin_utils.py:85] from t5x import partitioning
I0512 23:44:42.401718 140143678494720 gin_utils.py:85] from t5x import trainer
I0512 23:44:42.401751 140143678494720 gin_utils.py:85] from t5x import utils
I0512 23:44:42.401783 140143678494720 gin_utils.py:85] 
I0512 23:44:42.401816 140143678494720 gin_utils.py:85] # Macros:
I0512 23:44:42.401849 140143678494720 gin_utils.py:85] # ==============================================================================
I0512 23:44:42.401882 140143678494720 gin_utils.py:85] ACTIVATION_DTYPE = 'bfloat16'
I0512 23:44:42.401915 140143678494720 gin_utils.py:85] ACTIVATION_PARTITIONING_DIMS = 1
I0512 23:44:42.401947 140143678494720 gin_utils.py:85] ARCHITECTURE = @t5_architecture.DecoderOnly()
I0512 23:44:42.401980 140143678494720 gin_utils.py:85] AUX_LOSS_FACTOR = 0.01
I0512 23:44:42.402012 140143678494720 gin_utils.py:85] BATCH_SIZE = 384
I0512 23:44:42.402045 140143678494720 gin_utils.py:85] BIAS_INIT = @bias_init/linen.initializers.normal()
I0512 23:44:42.402078 140143678494720 gin_utils.py:85] DECODER_SPARSE_LAYOUT = %flaxformer.architectures.moe.moe_enums.LayerLayout.MIXED
I0512 23:44:42.402111 140143678494720 gin_utils.py:85] DROPOUT_FACTORY = @dropout_factory/linen.Dropout
I0512 23:44:42.402143 140143678494720 gin_utils.py:85] DROPOUT_RATE = 0.0
I0512 23:44:42.402177 140143678494720 gin_utils.py:85] EMBED_DIM = 2048
I0512 23:44:42.402210 140143678494720 gin_utils.py:85] EVAL_EXPERT_CAPACITY_FACTOR = 2.0
I0512 23:44:42.402242 140143678494720 gin_utils.py:85] EXPERT_DROPOUT_RATE = %DROPOUT_RATE
I0512 23:44:42.402275 140143678494720 gin_utils.py:85] EXPERT_MLP_DIM = %MLP_DIM
I0512 23:44:42.402310 140143678494720 gin_utils.py:85] GROUP_SIZE = 4096
I0512 23:44:42.402343 140143678494720 gin_utils.py:85] HEAD_DIM = 128
I0512 23:44:42.402375 140143678494720 gin_utils.py:85] JITTER_NOISE = 0.0
I0512 23:44:42.402407 140143678494720 gin_utils.py:85] LABEL_SMOOTHING = 0.0
I0512 23:44:42.402440 140143678494720 gin_utils.py:85] LOSS_NORMALIZING_FACTOR = 'NUM_REAL_TARGET_TOKENS'
I0512 23:44:42.402472 140143678494720 gin_utils.py:85] MIXTURE_OR_TASK_MODULE = None
I0512 23:44:42.402505 140143678494720 gin_utils.py:85] MIXTURE_OR_TASK_NAME = 'mix_ul2_test'
I0512 23:44:42.402543 140143678494720 gin_utils.py:85] MLP_DIM = 8192
I0512 23:44:42.402577 140143678494720 gin_utils.py:85] MODEL = @models.MoeDecoderOnlyModel()
I0512 23:44:42.402609 140143678494720 gin_utils.py:85] MODEL_DIR = 'gs://rosinality-tpu-bucket/openmoe_8b_ul2/training'
I0512 23:44:42.402642 140143678494720 gin_utils.py:85] MODEL_PARALLEL_SUBMESH = None
I0512 23:44:42.402675 140143678494720 gin_utils.py:85] MOE_TRUNCATED_DTYPE = 'bfloat16'
I0512 23:44:42.402708 140143678494720 gin_utils.py:85] NUM_DECODER_LAYERS = 24
I0512 23:44:42.402740 140143678494720 gin_utils.py:85] NUM_DECODER_SPARSE_LAYERS = 4
I0512 23:44:42.402772 140143678494720 gin_utils.py:85] NUM_EMBEDDINGS = 256384
I0512 23:44:42.402805 140143678494720 gin_utils.py:85] NUM_EXPERT_PARTITIONS = 8
I0512 23:44:42.402837 140143678494720 gin_utils.py:85] NUM_EXPERTS = 8
I0512 23:44:42.402870 140143678494720 gin_utils.py:85] NUM_HEADS = 24
I0512 23:44:42.402903 140143678494720 gin_utils.py:85] NUM_MODEL_PARTITIONS = 4
I0512 23:44:42.402935 140143678494720 gin_utils.py:85] NUM_SELECTED_EXPERTS = 2
I0512 23:44:42.402968 140143678494720 gin_utils.py:85] OPTIMIZER = @adafactor.Adafactor()
I0512 23:44:42.403000 140143678494720 gin_utils.py:85] RANDOM_SEED = None
I0512 23:44:42.403032 140143678494720 gin_utils.py:85] ROUTER_Z_LOSS_FACTOR = 0.0001
I0512 23:44:42.403065 140143678494720 gin_utils.py:85] SCALE = 0.1
I0512 23:44:42.403097 140143678494720 gin_utils.py:85] SHUFFLE_TRAIN_EXAMPLES = True
I0512 23:44:42.403130 140143678494720 gin_utils.py:85] TASK_FEATURE_LENGTHS = {'inputs': 1024, 'targets': 1024}
I0512 23:44:42.403162 140143678494720 gin_utils.py:85] TRAIN_EXPERT_CAPACITY_FACTOR = 1.25
I0512 23:44:42.403195 140143678494720 gin_utils.py:85] TRAIN_STEPS = 500000
I0512 23:44:42.403227 140143678494720 gin_utils.py:85] USE_CACHED_TASKS = False
I0512 23:44:42.403260 140143678494720 gin_utils.py:85] USE_HARDWARE_RNG = False
I0512 23:44:42.403294 140143678494720 gin_utils.py:85] VOCABULARY = @seqio.SentencePieceVocabulary()
I0512 23:44:42.403328 140143678494720 gin_utils.py:85] Z_LOSS = 0.0001
I0512 23:44:42.403361 140143678494720 gin_utils.py:85] 
I0512 23:44:42.403393 140143678494720 gin_utils.py:85] # Parameters for adafactor.Adafactor:
I0512 23:44:42.403426 140143678494720 gin_utils.py:85] # ==============================================================================
I0512 23:44:42.403459 140143678494720 gin_utils.py:85] adafactor.Adafactor.decay_rate = 0.8
I0512 23:44:42.403492 140143678494720 gin_utils.py:85] adafactor.Adafactor.logical_factor_rules = @adafactor_utils.logical_factor_rules()
I0512 23:44:42.403524 140143678494720 gin_utils.py:85] adafactor.Adafactor.step_offset = 0
I0512 23:44:42.403566 140143678494720 gin_utils.py:85] 
I0512 23:44:42.429117 140643590531072 resource_reader.py:50] system_path_file_exists:flaxformer/flaxformer/t5x/configs/moe/models/st_moe_decoder_only_large.gin
E0512 23:44:42.429615 140643590531072 resource_reader.py:55] Path not found: flaxformer/flaxformer/t5x/configs/moe/models/st_moe_decoder_only_large.gin
I0512 23:44:42.429677 140643590531072 resource_reader.py:50] system_path_file_exists:./flaxformer/flaxformer/t5x/configs/moe/models/st_moe_decoder_only_large.gin
E0512 23:44:42.429732 140643590531072 resource_reader.py:55] Path not found: ./flaxformer/flaxformer/t5x/configs/moe/models/st_moe_decoder_only_large.gin
I0512 23:44:42.379987 139876724979712 gin_utils.py:85] # Parameters for utils.CheckpointConfig:
I0512 23:44:42.380018 139876724979712 gin_utils.py:85] # ==============================================================================
I0512 23:44:42.380050 139876724979712 gin_utils.py:85] utils.CheckpointConfig.restore = @utils.RestoreCheckpointConfig()
I0512 23:44:42.380081 139876724979712 gin_utils.py:85] utils.CheckpointConfig.save = @utils.SaveCheckpointConfig()
I0512 23:44:42.380112 139876724979712 gin_utils.py:85] 
I0512 23:44:42.380143 139876724979712 gin_utils.py:85] # Parameters for moe_partitioning.compute_num_model_partitions:
I0512 23:44:42.380174 139876724979712 gin_utils.py:85] # ==============================================================================
I0512 23:44:42.380206 139876724979712 gin_utils.py:85] moe_partitioning.compute_num_model_partitions.model_parallel_submesh = \
I0512 23:44:42.380243 139876724979712 gin_utils.py:85]     %MODEL_PARALLEL_SUBMESH
I0512 23:44:42.380275 139876724979712 gin_utils.py:85] moe_partitioning.compute_num_model_partitions.num_model_partitions = \
I0512 23:44:42.380306 139876724979712 gin_utils.py:85]     %NUM_MODEL_PARTITIONS
I0512 23:44:42.380337 139876724979712 gin_utils.py:85] 
I0512 23:44:42.380368 139876724979712 gin_utils.py:85] # Parameters for utils.create_learning_rate_scheduler:
I0512 23:44:42.380399 139876724979712 gin_utils.py:85] # ==============================================================================
I0512 23:44:42.380431 139876724979712 gin_utils.py:85] utils.create_learning_rate_scheduler.base_learning_rate = 1.0
I0512 23:44:42.380463 139876724979712 gin_utils.py:85] utils.create_learning_rate_scheduler.factors = 'constant * rsqrt_decay'
I0512 23:44:42.380496 139876724979712 gin_utils.py:85] utils.create_learning_rate_scheduler.warmup_steps = 10000
I0512 23:44:42.380527 139876724979712 gin_utils.py:85] 
I0512 23:44:42.380559 139876724979712 gin_utils.py:85] # Parameters for train/utils.DatasetConfig:
I0512 23:44:42.380589 139876724979712 gin_utils.py:85] # ==============================================================================
I0512 23:44:42.380621 139876724979712 gin_utils.py:85] train/utils.DatasetConfig.batch_size = %BATCH_SIZE
I0512 23:44:42.380652 139876724979712 gin_utils.py:85] train/utils.DatasetConfig.mixture_or_task_name = %MIXTURE_OR_TASK_NAME
I0512 23:44:42.380683 139876724979712 gin_utils.py:85] train/utils.DatasetConfig.module = %MIXTURE_OR_TASK_MODULE
I0512 23:44:42.380714 139876724979712 gin_utils.py:85] train/utils.DatasetConfig.pack = True
I0512 23:44:42.380745 139876724979712 gin_utils.py:85] train/utils.DatasetConfig.seed = None
I0512 23:44:42.380777 139876724979712 gin_utils.py:85] train/utils.DatasetConfig.shuffle = %SHUFFLE_TRAIN_EXAMPLES
I0512 23:44:42.380808 139876724979712 gin_utils.py:85] train/utils.DatasetConfig.split = 'train'
I0512 23:44:42.380839 139876724979712 gin_utils.py:85] train/utils.DatasetConfig.task_feature_lengths = %TASK_FEATURE_LENGTHS
I0512 23:44:42.380870 139876724979712 gin_utils.py:85] train/utils.DatasetConfig.use_cached = %USE_CACHED_TASKS
I0512 23:44:42.380900 139876724979712 gin_utils.py:85] 
I0512 23:44:42.380931 139876724979712 gin_utils.py:85] # Parameters for train_eval/utils.DatasetConfig:
I0512 23:44:42.380962 139876724979712 gin_utils.py:85] # ==============================================================================
I0512 23:44:42.380993 139876724979712 gin_utils.py:85] train_eval/utils.DatasetConfig.batch_size = 128
I0512 23:44:42.381024 139876724979712 gin_utils.py:85] train_eval/utils.DatasetConfig.mixture_or_task_name = %MIXTURE_OR_TASK_NAME
I0512 23:44:42.381055 139876724979712 gin_utils.py:85] train_eval/utils.DatasetConfig.module = %MIXTURE_OR_TASK_MODULE
I0512 23:44:42.381086 139876724979712 gin_utils.py:85] train_eval/utils.DatasetConfig.pack = True
I0512 23:44:42.381117 139876724979712 gin_utils.py:85] train_eval/utils.DatasetConfig.seed = 42
I0512 23:44:42.381148 139876724979712 gin_utils.py:85] train_eval/utils.DatasetConfig.shuffle = False
I0512 23:44:42.381179 139876724979712 gin_utils.py:85] train_eval/utils.DatasetConfig.split = 'validation'
I0512 23:44:42.381214 139876724979712 gin_utils.py:85] train_eval/utils.DatasetConfig.task_feature_lengths = %TASK_FEATURE_LENGTHS
I0512 23:44:42.381247 139876724979712 gin_utils.py:85] train_eval/utils.DatasetConfig.use_cached = %USE_CACHED_TASKS
I0512 23:44:42.381279 139876724979712 gin_utils.py:85] 
I0512 23:44:42.381309 139876724979712 gin_utils.py:85] # Parameters for t5_architecture.DecoderLayer:
I0512 23:44:42.381341 139876724979712 gin_utils.py:85] # ==============================================================================
I0512 23:44:42.381372 139876724979712 gin_utils.py:85] t5_architecture.DecoderLayer.activation_partitioning_dims = \
I0512 23:44:42.381403 139876724979712 gin_utils.py:85]     %ACTIVATION_PARTITIONING_DIMS
I0512 23:44:42.381434 139876724979712 gin_utils.py:85] t5_architecture.DecoderLayer.dropout_factory = %DROPOUT_FACTORY
I0512 23:44:42.381467 139876724979712 gin_utils.py:85] t5_architecture.DecoderLayer.encoder_decoder_attention = None
I0512 23:44:42.381500 139876724979712 gin_utils.py:85] t5_architecture.DecoderLayer.layer_norm_factory = @layer_norm.T5LayerNorm
I0512 23:44:42.381531 139876724979712 gin_utils.py:85] t5_architecture.DecoderLayer.mlp = @dense.MlpBlock()
I0512 23:44:42.381562 139876724979712 gin_utils.py:85] t5_architecture.DecoderLayer.scanned = False
I0512 23:44:42.381593 139876724979712 gin_utils.py:85] t5_architecture.DecoderLayer.self_attention = \
I0512 23:44:42.381624 139876724979712 gin_utils.py:85]     @dense_attention.MultiHeadDotProductAttention()
I0512 23:44:42.381654 139876724979712 gin_utils.py:85] 
I0512 23:44:42.381685 139876724979712 gin_utils.py:85] # Parameters for t5_architecture.DecoderOnly:
I0512 23:44:42.381716 139876724979712 gin_utils.py:85] # ==============================================================================
I0512 23:44:42.381747 139876724979712 gin_utils.py:85] t5_architecture.DecoderOnly.decoder_factory = @moe_architecture.SparseDecoder
I0512 23:44:42.381778 139876724979712 gin_utils.py:85] t5_architecture.DecoderOnly.dtype = %ACTIVATION_DTYPE
I0512 23:44:42.381809 139876724979712 gin_utils.py:85] t5_architecture.DecoderOnly.shared_token_embedder_factory = @embedding.Embed
I0512 23:44:42.381840 139876724979712 gin_utils.py:85] 
I0512 23:44:42.381871 139876724979712 gin_utils.py:85] # Parameters for output_logits/dense.DenseGeneral:
I0512 23:44:42.381902 139876724979712 gin_utils.py:85] # ==============================================================================
I0512 23:44:42.381934 139876724979712 gin_utils.py:85] output_logits/dense.DenseGeneral.bias_init = %BIAS_INIT
I0512 23:44:42.381965 139876724979712 gin_utils.py:85] output_logits/dense.DenseGeneral.dtype = 'float32'
I0512 23:44:42.381995 139876724979712 gin_utils.py:85] output_logits/dense.DenseGeneral.features = %NUM_EMBEDDINGS
I0512 23:44:42.382026 139876724979712 gin_utils.py:85] output_logits/dense.DenseGeneral.kernel_axis_names = ['embed', 'vocab']
I0512 23:44:42.382057 139876724979712 gin_utils.py:85] output_logits/dense.DenseGeneral.kernel_init = \
I0512 23:44:42.382088 139876724979712 gin_utils.py:85]     @output_logits_kernel_init/linen.initializers.variance_scaling()
I0512 23:44:42.382119 139876724979712 gin_utils.py:85] output_logits/dense.DenseGeneral.use_bias = False
I0512 23:44:42.382150 139876724979712 gin_utils.py:85] 
I0512 23:44:42.382181 139876724979712 gin_utils.py:85] # Parameters for dropout_factory/linen.Dropout:
I0512 23:44:42.382218 139876724979712 gin_utils.py:85] # ==============================================================================
I0512 23:44:42.382251 139876724979712 gin_utils.py:85] dropout_factory/linen.Dropout.broadcast_dims = (-2,)
I0512 23:44:42.382282 139876724979712 gin_utils.py:85] dropout_factory/linen.Dropout.rate = %DROPOUT_RATE
I0512 23:44:42.382313 139876724979712 gin_utils.py:85] 
I0512 23:44:42.382344 139876724979712 gin_utils.py:85] # Parameters for embedding.Embed:
I0512 23:44:42.382375 139876724979712 gin_utils.py:85] # ==============================================================================
I0512 23:44:42.382407 139876724979712 gin_utils.py:85] embedding.Embed.attend_dtype = 'float32'
I0512 23:44:42.382438 139876724979712 gin_utils.py:85] embedding.Embed.cast_input_dtype = 'int32'
I0512 23:44:42.382470 139876724979712 gin_utils.py:85] embedding.Embed.dtype = %ACTIVATION_DTYPE
I0512 23:44:42.382502 139876724979712 gin_utils.py:85] embedding.Embed.embedding_init = @token_embedder_init/linen.initializers.normal()
I0512 23:44:42.382533 139876724979712 gin_utils.py:85] embedding.Embed.features = %EMBED_DIM
I0512 23:44:42.382564 139876724979712 gin_utils.py:85] embedding.Embed.name = 'token_embedder'
I0512 23:44:42.382595 139876724979712 gin_utils.py:85] embedding.Embed.num_embeddings = %NUM_EMBEDDINGS
I0512 23:44:42.382627 139876724979712 gin_utils.py:85] embedding.Embed.one_hot = True
I0512 23:44:42.382658 139876724979712 gin_utils.py:85] 
I0512 23:44:42.382689 139876724979712 gin_utils.py:85] # Parameters for dense.MlpBlock:
I0512 23:44:42.382720 139876724979712 gin_utils.py:85] # ==============================================================================
I0512 23:44:42.382752 139876724979712 gin_utils.py:85] dense.MlpBlock.activations = ('swiglu', 'linear')
I0512 23:44:42.382782 139876724979712 gin_utils.py:85] dense.MlpBlock.bias_init = %BIAS_INIT
I0512 23:44:42.382813 139876724979712 gin_utils.py:85] dense.MlpBlock.dtype = %ACTIVATION_DTYPE
I0512 23:44:42.382844 139876724979712 gin_utils.py:85] dense.MlpBlock.final_dropout_rate = 0
I0512 23:44:42.382875 139876724979712 gin_utils.py:85] dense.MlpBlock.input_axis_name = 'mlp_embed'
I0512 23:44:42.382906 139876724979712 gin_utils.py:85] dense.MlpBlock.intermediate_dim = %MLP_DIM
I0512 23:44:42.382937 139876724979712 gin_utils.py:85] dense.MlpBlock.intermediate_dropout_rate = %DROPOUT_RATE
I0512 23:44:42.406008 139856202532864 resource_reader.py:37] gin-config opened resource file:/home/rosinality/t5x/t5x/contrib/moe/configs/runs/pretrain.gin
I0512 23:44:42.406512 139856202532864 resource_reader.py:50] system_path_file_exists:t5x/configs/runs/pretrain.gin
I0512 23:44:42.406823 139856202532864 resource_reader.py:37] gin-config opened resource file:/home/rosinality/t5x/t5x/configs/runs/pretrain.gin
I0512 23:44:42.420778 139856202532864 gin_utils.py:83] Gin Configuration:
I0512 23:44:42.436744 139856202532864 gin_utils.py:85] from __gin__ import dynamic_registration
I0512 23:44:42.436814 139856202532864 gin_utils.py:85] import __main__ as train_script
I0512 23:44:42.436861 139856202532864 gin_utils.py:85] from flax import linen
I0512 23:44:42.436897 139856202532864 gin_utils.py:85] import flaxformer
I0512 23:44:42.436929 139856202532864 gin_utils.py:85] from flaxformer.architectures.moe import moe_architecture
I0512 23:44:42.436962 139856202532864 gin_utils.py:85] from flaxformer.architectures.moe import moe_enums
I0512 23:44:42.436994 139856202532864 gin_utils.py:85] from flaxformer.architectures.moe import moe_layers
I0512 23:44:42.437025 139856202532864 gin_utils.py:85] from flaxformer.architectures.moe import routing
I0512 23:44:42.437057 139856202532864 gin_utils.py:85] from flaxformer.architectures.t5 import t5_architecture
I0512 23:44:42.437088 139856202532864 gin_utils.py:85] from flaxformer.components.attention import dense_attention
I0512 23:44:42.437119 139856202532864 gin_utils.py:85] from flaxformer.components.attention import memory_efficient_attention
I0512 23:44:42.437151 139856202532864 gin_utils.py:85] from flaxformer.components import dense
I0512 23:44:42.437182 139856202532864 gin_utils.py:85] from flaxformer.components import embedding
I0512 23:44:42.437213 139856202532864 gin_utils.py:85] from flaxformer.components import layer_norm
I0512 23:44:42.437245 139856202532864 gin_utils.py:85] from gin import config
I0512 23:44:42.437276 139856202532864 gin_utils.py:85] import seqio
I0512 23:44:42.437307 139856202532864 gin_utils.py:85] import t5.data.mixtures
I0512 23:44:42.437338 139856202532864 gin_utils.py:85] from t5x import adafactor
I0512 23:44:42.437370 139856202532864 gin_utils.py:85] from t5x.contrib.moe import adafactor_utils
I0512 23:44:42.437401 139856202532864 gin_utils.py:85] from t5x.contrib.moe import models
I0512 23:44:42.437432 139856202532864 gin_utils.py:85] from t5x.contrib.moe import partitioning as moe_partitioning
I0512 23:44:42.437463 139856202532864 gin_utils.py:85] from t5x.contrib.moe import trainer as moe_trainer
I0512 23:44:42.437494 139856202532864 gin_utils.py:85] from t5x import gin_utils
I0512 23:44:42.437526 139856202532864 gin_utils.py:85] from t5x import partitioning
I0512 23:44:42.437557 139856202532864 gin_utils.py:85] from t5x import trainer
I0512 23:44:42.437588 139856202532864 gin_utils.py:85] from t5x import utils
I0512 23:44:42.437623 139856202532864 gin_utils.py:85] 
I0512 23:44:42.437655 139856202532864 gin_utils.py:85] # Macros:
I0512 23:44:42.437687 139856202532864 gin_utils.py:85] # ==============================================================================
I0512 23:44:42.437740 139856202532864 gin_utils.py:85] ACTIVATION_DTYPE = 'bfloat16'
I0512 23:44:42.437776 139856202532864 gin_utils.py:85] ACTIVATION_PARTITIONING_DIMS = 1
I0512 23:44:42.437809 139856202532864 gin_utils.py:85] ARCHITECTURE = @t5_architecture.DecoderOnly()
I0512 23:44:42.437845 139856202532864 gin_utils.py:85] AUX_LOSS_FACTOR = 0.01
I0512 23:44:42.437878 139856202532864 gin_utils.py:85] BATCH_SIZE = 384
I0512 23:44:42.437910 139856202532864 gin_utils.py:85] BIAS_INIT = @bias_init/linen.initializers.normal()
I0512 23:44:42.437941 139856202532864 gin_utils.py:85] DECODER_SPARSE_LAYOUT = %flaxformer.architectures.moe.moe_enums.LayerLayout.MIXED
I0512 23:44:42.437973 139856202532864 gin_utils.py:85] DROPOUT_FACTORY = @dropout_factory/linen.Dropout
I0512 23:44:42.438004 139856202532864 gin_utils.py:85] DROPOUT_RATE = 0.0
I0512 23:44:42.438036 139856202532864 gin_utils.py:85] EMBED_DIM = 2048
I0512 23:44:42.438067 139856202532864 gin_utils.py:85] EVAL_EXPERT_CAPACITY_FACTOR = 2.0
I0512 23:44:42.438098 139856202532864 gin_utils.py:85] EXPERT_DROPOUT_RATE = %DROPOUT_RATE
I0512 23:44:42.438129 139856202532864 gin_utils.py:85] EXPERT_MLP_DIM = %MLP_DIM
I0512 23:44:42.438161 139856202532864 gin_utils.py:85] GROUP_SIZE = 4096
I0512 23:44:42.438192 139856202532864 gin_utils.py:85] HEAD_DIM = 128
I0512 23:44:42.438223 139856202532864 gin_utils.py:85] JITTER_NOISE = 0.0
I0512 23:44:42.438254 139856202532864 gin_utils.py:85] LABEL_SMOOTHING = 0.0
I0512 23:44:42.438285 139856202532864 gin_utils.py:85] LOSS_NORMALIZING_FACTOR = 'NUM_REAL_TARGET_TOKENS'
I0512 23:44:42.438316 139856202532864 gin_utils.py:85] MIXTURE_OR_TASK_MODULE = None
I0512 23:44:42.438348 139856202532864 gin_utils.py:85] MIXTURE_OR_TASK_NAME = 'mix_ul2_test'
I0512 23:44:42.438379 139856202532864 gin_utils.py:85] MLP_DIM = 8192
I0512 23:44:42.438410 139856202532864 gin_utils.py:85] MODEL = @models.MoeDecoderOnlyModel()
I0512 23:44:42.438441 139856202532864 gin_utils.py:85] MODEL_DIR = 'gs://rosinality-tpu-bucket/openmoe_8b_ul2/training'
I0512 23:44:42.438473 139856202532864 gin_utils.py:85] MODEL_PARALLEL_SUBMESH = None
I0512 23:44:42.438504 139856202532864 gin_utils.py:85] MOE_TRUNCATED_DTYPE = 'bfloat16'
I0512 23:44:42.438536 139856202532864 gin_utils.py:85] NUM_DECODER_LAYERS = 24
I0512 23:44:42.438567 139856202532864 gin_utils.py:85] NUM_DECODER_SPARSE_LAYERS = 4
I0512 23:44:42.438600 139856202532864 gin_utils.py:85] NUM_EMBEDDINGS = 256384
I0512 23:44:42.438633 139856202532864 gin_utils.py:85] NUM_EXPERT_PARTITIONS = 8
I0512 23:44:42.438665 139856202532864 gin_utils.py:85] NUM_EXPERTS = 8
I0512 23:44:42.438697 139856202532864 gin_utils.py:85] NUM_HEADS = 24
I0512 23:44:42.438728 139856202532864 gin_utils.py:85] NUM_MODEL_PARTITIONS = 4
I0512 23:44:42.438760 139856202532864 gin_utils.py:85] NUM_SELECTED_EXPERTS = 2
I0512 23:44:42.438791 139856202532864 gin_utils.py:85] OPTIMIZER = @adafactor.Adafactor()
I0512 23:44:42.438822 139856202532864 gin_utils.py:85] RANDOM_SEED = None
I0512 23:44:42.438859 139856202532864 gin_utils.py:85] ROUTER_Z_LOSS_FACTOR = 0.0001
I0512 23:44:42.438892 139856202532864 gin_utils.py:85] SCALE = 0.1
I0512 23:44:42.438923 139856202532864 gin_utils.py:85] SHUFFLE_TRAIN_EXAMPLES = True
I0512 23:44:42.438955 139856202532864 gin_utils.py:85] TASK_FEATURE_LENGTHS = {'inputs': 1024, 'targets': 1024}
I0512 23:44:42.438986 139856202532864 gin_utils.py:85] TRAIN_EXPERT_CAPACITY_FACTOR = 1.25
I0512 23:44:42.439017 139856202532864 gin_utils.py:85] TRAIN_STEPS = 500000
I0512 23:44:42.439049 139856202532864 gin_utils.py:85] USE_CACHED_TASKS = False
I0512 23:44:42.439080 139856202532864 gin_utils.py:85] USE_HARDWARE_RNG = False
I0512 23:44:42.439111 139856202532864 gin_utils.py:85] VOCABULARY = @seqio.SentencePieceVocabulary()
I0512 23:44:42.439142 139856202532864 gin_utils.py:85] Z_LOSS = 0.0001
I0512 23:44:42.439173 139856202532864 gin_utils.py:85] 
I0512 23:44:42.439205 139856202532864 gin_utils.py:85] # Parameters for adafactor.Adafactor:
I0512 23:44:42.439237 139856202532864 gin_utils.py:85] # ==============================================================================
I0512 23:44:42.439268 139856202532864 gin_utils.py:85] adafactor.Adafactor.decay_rate = 0.8
I0512 23:44:42.439299 139856202532864 gin_utils.py:85] adafactor.Adafactor.logical_factor_rules = @adafactor_utils.logical_factor_rules()
I0512 23:44:42.439331 139856202532864 gin_utils.py:85] adafactor.Adafactor.step_offset = 0
I0512 23:44:42.439362 139856202532864 gin_utils.py:85] 
I0512 23:44:42.401887 139666521167872 resource_reader.py:37] gin-config opened resource file:/home/rosinality/t5x/t5x/contrib/moe/configs/runs/pretrain.gin
I0512 23:44:42.402380 139666521167872 resource_reader.py:50] system_path_file_exists:t5x/configs/runs/pretrain.gin
I0512 23:44:42.402675 139666521167872 resource_reader.py:37] gin-config opened resource file:/home/rosinality/t5x/t5x/configs/runs/pretrain.gin
I0512 23:44:42.415887 139666521167872 gin_utils.py:83] Gin Configuration:
I0512 23:44:42.430823 139666521167872 gin_utils.py:85] from __gin__ import dynamic_registration
I0512 23:44:42.430890 139666521167872 gin_utils.py:85] import __main__ as train_script
I0512 23:44:42.430930 139666521167872 gin_utils.py:85] from flax import linen
I0512 23:44:42.430963 139666521167872 gin_utils.py:85] import flaxformer
I0512 23:44:42.430994 139666521167872 gin_utils.py:85] from flaxformer.architectures.moe import moe_architecture
I0512 23:44:42.431025 139666521167872 gin_utils.py:85] from flaxformer.architectures.moe import moe_enums
I0512 23:44:42.431056 139666521167872 gin_utils.py:85] from flaxformer.architectures.moe import moe_layers
I0512 23:44:42.431105 139666521167872 gin_utils.py:85] from flaxformer.architectures.moe import routing
I0512 23:44:42.431143 139666521167872 gin_utils.py:85] from flaxformer.architectures.t5 import t5_architecture
I0512 23:44:42.431174 139666521167872 gin_utils.py:85] from flaxformer.components.attention import dense_attention
I0512 23:44:42.431204 139666521167872 gin_utils.py:85] from flaxformer.components.attention import memory_efficient_attention
I0512 23:44:42.431235 139666521167872 gin_utils.py:85] from flaxformer.components import dense
I0512 23:44:42.431266 139666521167872 gin_utils.py:85] from flaxformer.components import embedding
I0512 23:44:42.431296 139666521167872 gin_utils.py:85] from flaxformer.components import layer_norm
I0512 23:44:42.431333 139666521167872 gin_utils.py:85] from gin import config
I0512 23:44:42.431364 139666521167872 gin_utils.py:85] import seqio
I0512 23:44:42.431394 139666521167872 gin_utils.py:85] import t5.data.mixtures
I0512 23:44:42.431425 139666521167872 gin_utils.py:85] from t5x import adafactor
I0512 23:44:42.431455 139666521167872 gin_utils.py:85] from t5x.contrib.moe import adafactor_utils
I0512 23:44:42.431485 139666521167872 gin_utils.py:85] from t5x.contrib.moe import models
I0512 23:44:42.431516 139666521167872 gin_utils.py:85] from t5x.contrib.moe import partitioning as moe_partitioning
I0512 23:44:42.431546 139666521167872 gin_utils.py:85] from t5x.contrib.moe import trainer as moe_trainer
I0512 23:44:42.431578 139666521167872 gin_utils.py:85] from t5x import gin_utils
I0512 23:44:42.431610 139666521167872 gin_utils.py:85] from t5x import partitioning
I0512 23:44:42.431640 139666521167872 gin_utils.py:85] from t5x import trainer
I0512 23:44:42.431671 139666521167872 gin_utils.py:85] from t5x import utils
I0512 23:44:42.431701 139666521167872 gin_utils.py:85] 
I0512 23:44:42.431732 139666521167872 gin_utils.py:85] # Macros:
I0512 23:44:42.431762 139666521167872 gin_utils.py:85] # ==============================================================================
I0512 23:44:42.431793 139666521167872 gin_utils.py:85] ACTIVATION_DTYPE = 'bfloat16'
I0512 23:44:42.431824 139666521167872 gin_utils.py:85] ACTIVATION_PARTITIONING_DIMS = 1
I0512 23:44:42.431856 139666521167872 gin_utils.py:85] ARCHITECTURE = @t5_architecture.DecoderOnly()
I0512 23:44:42.431892 139666521167872 gin_utils.py:85] AUX_LOSS_FACTOR = 0.01
I0512 23:44:42.431923 139666521167872 gin_utils.py:85] BATCH_SIZE = 384
I0512 23:44:42.431953 139666521167872 gin_utils.py:85] BIAS_INIT = @bias_init/linen.initializers.normal()
I0512 23:44:42.431983 139666521167872 gin_utils.py:85] DECODER_SPARSE_LAYOUT = %flaxformer.architectures.moe.moe_enums.LayerLayout.MIXED
I0512 23:44:42.432014 139666521167872 gin_utils.py:85] DROPOUT_FACTORY = @dropout_factory/linen.Dropout
I0512 23:44:42.432044 139666521167872 gin_utils.py:85] DROPOUT_RATE = 0.0
I0512 23:44:42.432075 139666521167872 gin_utils.py:85] EMBED_DIM = 2048
I0512 23:44:42.432105 139666521167872 gin_utils.py:85] EVAL_EXPERT_CAPACITY_FACTOR = 2.0
I0512 23:44:42.432135 139666521167872 gin_utils.py:85] EXPERT_DROPOUT_RATE = %DROPOUT_RATE
I0512 23:44:42.432165 139666521167872 gin_utils.py:85] EXPERT_MLP_DIM = %MLP_DIM
I0512 23:44:42.432195 139666521167872 gin_utils.py:85] GROUP_SIZE = 4096
I0512 23:44:42.432225 139666521167872 gin_utils.py:85] HEAD_DIM = 128
I0512 23:44:42.432255 139666521167872 gin_utils.py:85] JITTER_NOISE = 0.0
I0512 23:44:42.432286 139666521167872 gin_utils.py:85] LABEL_SMOOTHING = 0.0
I0512 23:44:42.432326 139666521167872 gin_utils.py:85] LOSS_NORMALIZING_FACTOR = 'NUM_REAL_TARGET_TOKENS'
I0512 23:44:42.432388 139666521167872 gin_utils.py:85] MIXTURE_OR_TASK_MODULE = None
I0512 23:44:42.432419 139666521167872 gin_utils.py:85] MIXTURE_OR_TASK_NAME = 'mix_ul2_test'
I0512 23:44:42.432449 139666521167872 gin_utils.py:85] MLP_DIM = 8192
I0512 23:44:42.432480 139666521167872 gin_utils.py:85] MODEL = @models.MoeDecoderOnlyModel()
I0512 23:44:42.432510 139666521167872 gin_utils.py:85] MODEL_DIR = 'gs://rosinality-tpu-bucket/openmoe_8b_ul2/training'
I0512 23:44:42.432540 139666521167872 gin_utils.py:85] MODEL_PARALLEL_SUBMESH = None
I0512 23:44:42.432572 139666521167872 gin_utils.py:85] MOE_TRUNCATED_DTYPE = 'bfloat16'
I0512 23:44:42.432603 139666521167872 gin_utils.py:85] NUM_DECODER_LAYERS = 24
I0512 23:44:42.432634 139666521167872 gin_utils.py:85] NUM_DECODER_SPARSE_LAYERS = 4
I0512 23:44:42.432664 139666521167872 gin_utils.py:85] NUM_EMBEDDINGS = 256384
I0512 23:44:42.432694 139666521167872 gin_utils.py:85] NUM_EXPERT_PARTITIONS = 8
I0512 23:44:42.432725 139666521167872 gin_utils.py:85] NUM_EXPERTS = 8
I0512 23:44:42.406159 140193652815872 resource_reader.py:37] gin-config opened resource file:/home/rosinality/t5x/t5x/contrib/moe/configs/runs/pretrain.gin
I0512 23:44:42.406667 140193652815872 resource_reader.py:50] system_path_file_exists:t5x/configs/runs/pretrain.gin
I0512 23:44:42.406978 140193652815872 resource_reader.py:37] gin-config opened resource file:/home/rosinality/t5x/t5x/configs/runs/pretrain.gin
I0512 23:44:42.420837 140193652815872 gin_utils.py:83] Gin Configuration:
I0512 23:44:42.436702 140193652815872 gin_utils.py:85] from __gin__ import dynamic_registration
I0512 23:44:42.436773 140193652815872 gin_utils.py:85] import __main__ as train_script
I0512 23:44:42.436814 140193652815872 gin_utils.py:85] from flax import linen
I0512 23:44:42.436849 140193652815872 gin_utils.py:85] import flaxformer
I0512 23:44:42.436882 140193652815872 gin_utils.py:85] from flaxformer.architectures.moe import moe_architecture
I0512 23:44:42.436915 140193652815872 gin_utils.py:85] from flaxformer.architectures.moe import moe_enums
I0512 23:44:42.436948 140193652815872 gin_utils.py:85] from flaxformer.architectures.moe import moe_layers
I0512 23:44:42.436980 140193652815872 gin_utils.py:85] from flaxformer.architectures.moe import routing
I0512 23:44:42.437013 140193652815872 gin_utils.py:85] from flaxformer.architectures.t5 import t5_architecture
I0512 23:44:42.437045 140193652815872 gin_utils.py:85] from flaxformer.components.attention import dense_attention
I0512 23:44:42.437078 140193652815872 gin_utils.py:85] from flaxformer.components.attention import memory_efficient_attention
I0512 23:44:42.437110 140193652815872 gin_utils.py:85] from flaxformer.components import dense
I0512 23:44:42.437142 140193652815872 gin_utils.py:85] from flaxformer.components import embedding
I0512 23:44:42.437174 140193652815872 gin_utils.py:85] from flaxformer.components import layer_norm
I0512 23:44:42.437206 140193652815872 gin_utils.py:85] from gin import config
I0512 23:44:42.437238 140193652815872 gin_utils.py:85] import seqio
I0512 23:44:42.437271 140193652815872 gin_utils.py:85] import t5.data.mixtures
I0512 23:44:42.437303 140193652815872 gin_utils.py:85] from t5x import adafactor
I0512 23:44:42.437335 140193652815872 gin_utils.py:85] from t5x.contrib.moe import adafactor_utils
I0512 23:44:42.437370 140193652815872 gin_utils.py:85] from t5x.contrib.moe import models
I0512 23:44:42.437403 140193652815872 gin_utils.py:85] from t5x.contrib.moe import partitioning as moe_partitioning
I0512 23:44:42.437435 140193652815872 gin_utils.py:85] from t5x.contrib.moe import trainer as moe_trainer
I0512 23:44:42.437468 140193652815872 gin_utils.py:85] from t5x import gin_utils
I0512 23:44:42.437500 140193652815872 gin_utils.py:85] from t5x import partitioning
I0512 23:44:42.437532 140193652815872 gin_utils.py:85] from t5x import trainer
I0512 23:44:42.437565 140193652815872 gin_utils.py:85] from t5x import utils
I0512 23:44:42.437602 140193652815872 gin_utils.py:85] 
I0512 23:44:42.437637 140193652815872 gin_utils.py:85] # Macros:
I0512 23:44:42.437669 140193652815872 gin_utils.py:85] # ==============================================================================
I0512 23:44:42.437702 140193652815872 gin_utils.py:85] ACTIVATION_DTYPE = 'bfloat16'
I0512 23:44:42.437734 140193652815872 gin_utils.py:85] ACTIVATION_PARTITIONING_DIMS = 1
I0512 23:44:42.437767 140193652815872 gin_utils.py:85] ARCHITECTURE = @t5_architecture.DecoderOnly()
I0512 23:44:42.437799 140193652815872 gin_utils.py:85] AUX_LOSS_FACTOR = 0.01
I0512 23:44:42.437831 140193652815872 gin_utils.py:85] BATCH_SIZE = 384
I0512 23:44:42.437863 140193652815872 gin_utils.py:85] BIAS_INIT = @bias_init/linen.initializers.normal()
I0512 23:44:42.437895 140193652815872 gin_utils.py:85] DECODER_SPARSE_LAYOUT = %flaxformer.architectures.moe.moe_enums.LayerLayout.MIXED
I0512 23:44:42.437927 140193652815872 gin_utils.py:85] DROPOUT_FACTORY = @dropout_factory/linen.Dropout
I0512 23:44:42.437960 140193652815872 gin_utils.py:85] DROPOUT_RATE = 0.0
I0512 23:44:42.437992 140193652815872 gin_utils.py:85] EMBED_DIM = 2048
I0512 23:44:42.438024 140193652815872 gin_utils.py:85] EVAL_EXPERT_CAPACITY_FACTOR = 2.0
I0512 23:44:42.438056 140193652815872 gin_utils.py:85] EXPERT_DROPOUT_RATE = %DROPOUT_RATE
I0512 23:44:42.438088 140193652815872 gin_utils.py:85] EXPERT_MLP_DIM = %MLP_DIM
I0512 23:44:42.438120 140193652815872 gin_utils.py:85] GROUP_SIZE = 4096
I0512 23:44:42.438153 140193652815872 gin_utils.py:85] HEAD_DIM = 128
I0512 23:44:42.438184 140193652815872 gin_utils.py:85] JITTER_NOISE = 0.0
I0512 23:44:42.438216 140193652815872 gin_utils.py:85] LABEL_SMOOTHING = 0.0
I0512 23:44:42.438248 140193652815872 gin_utils.py:85] LOSS_NORMALIZING_FACTOR = 'NUM_REAL_TARGET_TOKENS'
I0512 23:44:42.438279 140193652815872 gin_utils.py:85] MIXTURE_OR_TASK_MODULE = None
I0512 23:44:42.438312 140193652815872 gin_utils.py:85] MIXTURE_OR_TASK_NAME = 'mix_ul2_test'
I0512 23:44:42.438344 140193652815872 gin_utils.py:85] MLP_DIM = 8192
I0512 23:44:42.438378 140193652815872 gin_utils.py:85] MODEL = @models.MoeDecoderOnlyModel()
I0512 23:44:42.438410 140193652815872 gin_utils.py:85] MODEL_DIR = 'gs://rosinality-tpu-bucket/openmoe_8b_ul2/training'
I0512 23:44:42.438443 140193652815872 gin_utils.py:85] MODEL_PARALLEL_SUBMESH = None
I0512 23:44:42.438475 140193652815872 gin_utils.py:85] MOE_TRUNCATED_DTYPE = 'bfloat16'
I0512 23:44:42.438507 140193652815872 gin_utils.py:85] NUM_DECODER_LAYERS = 24
I0512 23:44:42.438539 140193652815872 gin_utils.py:85] NUM_DECODER_SPARSE_LAYERS = 4
I0512 23:44:42.438570 140193652815872 gin_utils.py:85] NUM_EMBEDDINGS = 256384
I0512 23:44:42.438608 140193652815872 gin_utils.py:85] NUM_EXPERT_PARTITIONS = 8
I0512 23:44:42.438642 140193652815872 gin_utils.py:85] NUM_EXPERTS = 8
I0512 23:44:42.432755 139666521167872 gin_utils.py:85] NUM_HEADS = 24
I0512 23:44:42.432785 139666521167872 gin_utils.py:85] NUM_MODEL_PARTITIONS = 4
I0512 23:44:42.432815 139666521167872 gin_utils.py:85] NUM_SELECTED_EXPERTS = 2
I0512 23:44:42.432846 139666521167872 gin_utils.py:85] OPTIMIZER = @adafactor.Adafactor()
I0512 23:44:42.432876 139666521167872 gin_utils.py:85] RANDOM_SEED = None
I0512 23:44:42.432906 139666521167872 gin_utils.py:85] ROUTER_Z_LOSS_FACTOR = 0.0001
I0512 23:44:42.432937 139666521167872 gin_utils.py:85] SCALE = 0.1
I0512 23:44:42.432967 139666521167872 gin_utils.py:85] SHUFFLE_TRAIN_EXAMPLES = True
I0512 23:44:42.432997 139666521167872 gin_utils.py:85] TASK_FEATURE_LENGTHS = {'inputs': 1024, 'targets': 1024}
I0512 23:44:42.433028 139666521167872 gin_utils.py:85] TRAIN_EXPERT_CAPACITY_FACTOR = 1.25
I0512 23:44:42.433058 139666521167872 gin_utils.py:85] TRAIN_STEPS = 500000
I0512 23:44:42.433089 139666521167872 gin_utils.py:85] USE_CACHED_TASKS = False
I0512 23:44:42.433119 139666521167872 gin_utils.py:85] USE_HARDWARE_RNG = False
I0512 23:44:42.433149 139666521167872 gin_utils.py:85] VOCABULARY = @seqio.SentencePieceVocabulary()
I0512 23:44:42.433179 139666521167872 gin_utils.py:85] Z_LOSS = 0.0001
I0512 23:44:42.433209 139666521167872 gin_utils.py:85] 
I0512 23:44:42.433240 139666521167872 gin_utils.py:85] # Parameters for adafactor.Adafactor:
I0512 23:44:42.433270 139666521167872 gin_utils.py:85] # ==============================================================================
I0512 23:44:42.433300 139666521167872 gin_utils.py:85] adafactor.Adafactor.decay_rate = 0.8
I0512 23:44:42.433336 139666521167872 gin_utils.py:85] adafactor.Adafactor.logical_factor_rules = @adafactor_utils.logical_factor_rules()
I0512 23:44:42.433367 139666521167872 gin_utils.py:85] adafactor.Adafactor.step_offset = 0
I0512 23:44:42.433397 139666521167872 gin_utils.py:85] 
I0512 23:44:42.438674 140193652815872 gin_utils.py:85] NUM_HEADS = 24
I0512 23:44:42.438707 140193652815872 gin_utils.py:85] NUM_MODEL_PARTITIONS = 4
I0512 23:44:42.438739 140193652815872 gin_utils.py:85] NUM_SELECTED_EXPERTS = 2
I0512 23:44:42.438771 140193652815872 gin_utils.py:85] OPTIMIZER = @adafactor.Adafactor()
I0512 23:44:42.438803 140193652815872 gin_utils.py:85] RANDOM_SEED = None
I0512 23:44:42.438835 140193652815872 gin_utils.py:85] ROUTER_Z_LOSS_FACTOR = 0.0001
I0512 23:44:42.438867 140193652815872 gin_utils.py:85] SCALE = 0.1
I0512 23:44:42.438899 140193652815872 gin_utils.py:85] SHUFFLE_TRAIN_EXAMPLES = True
I0512 23:44:42.438930 140193652815872 gin_utils.py:85] TASK_FEATURE_LENGTHS = {'inputs': 1024, 'targets': 1024}
I0512 23:44:42.438962 140193652815872 gin_utils.py:85] TRAIN_EXPERT_CAPACITY_FACTOR = 1.25
I0512 23:44:42.438994 140193652815872 gin_utils.py:85] TRAIN_STEPS = 500000
I0512 23:44:42.439026 140193652815872 gin_utils.py:85] USE_CACHED_TASKS = False
I0512 23:44:42.439059 140193652815872 gin_utils.py:85] USE_HARDWARE_RNG = False
I0512 23:44:42.439091 140193652815872 gin_utils.py:85] VOCABULARY = @seqio.SentencePieceVocabulary()
I0512 23:44:42.439123 140193652815872 gin_utils.py:85] Z_LOSS = 0.0001
I0512 23:44:42.439155 140193652815872 gin_utils.py:85] 
I0512 23:44:42.439187 140193652815872 gin_utils.py:85] # Parameters for adafactor.Adafactor:
I0512 23:44:42.439219 140193652815872 gin_utils.py:85] # ==============================================================================
I0512 23:44:42.439251 140193652815872 gin_utils.py:85] adafactor.Adafactor.decay_rate = 0.8
I0512 23:44:42.439283 140193652815872 gin_utils.py:85] adafactor.Adafactor.logical_factor_rules = @adafactor_utils.logical_factor_rules()
I0512 23:44:42.439315 140193652815872 gin_utils.py:85] adafactor.Adafactor.step_offset = 0
I0512 23:44:42.439348 140193652815872 gin_utils.py:85] 
I0512 23:44:42.403600 140143678494720 gin_utils.py:85] # Parameters for utils.CheckpointConfig:
I0512 23:44:42.403632 140143678494720 gin_utils.py:85] # ==============================================================================
I0512 23:44:42.403665 140143678494720 gin_utils.py:85] utils.CheckpointConfig.restore = @utils.RestoreCheckpointConfig()
I0512 23:44:42.403698 140143678494720 gin_utils.py:85] utils.CheckpointConfig.save = @utils.SaveCheckpointConfig()
I0512 23:44:42.403730 140143678494720 gin_utils.py:85] 
I0512 23:44:42.403763 140143678494720 gin_utils.py:85] # Parameters for moe_partitioning.compute_num_model_partitions:
I0512 23:44:42.403795 140143678494720 gin_utils.py:85] # ==============================================================================
I0512 23:44:42.403828 140143678494720 gin_utils.py:85] moe_partitioning.compute_num_model_partitions.model_parallel_submesh = \
I0512 23:44:42.403861 140143678494720 gin_utils.py:85]     %MODEL_PARALLEL_SUBMESH
I0512 23:44:42.403894 140143678494720 gin_utils.py:85] moe_partitioning.compute_num_model_partitions.num_model_partitions = \
I0512 23:44:42.403926 140143678494720 gin_utils.py:85]     %NUM_MODEL_PARTITIONS
I0512 23:44:42.403959 140143678494720 gin_utils.py:85] 
I0512 23:44:42.403992 140143678494720 gin_utils.py:85] # Parameters for utils.create_learning_rate_scheduler:
I0512 23:44:42.404024 140143678494720 gin_utils.py:85] # ==============================================================================
I0512 23:44:42.404057 140143678494720 gin_utils.py:85] utils.create_learning_rate_scheduler.base_learning_rate = 1.0
I0512 23:44:42.404090 140143678494720 gin_utils.py:85] utils.create_learning_rate_scheduler.factors = 'constant * rsqrt_decay'
I0512 23:44:42.404122 140143678494720 gin_utils.py:85] utils.create_learning_rate_scheduler.warmup_steps = 10000
I0512 23:44:42.404155 140143678494720 gin_utils.py:85] 
I0512 23:44:42.404187 140143678494720 gin_utils.py:85] # Parameters for train/utils.DatasetConfig:
I0512 23:44:42.404220 140143678494720 gin_utils.py:85] # ==============================================================================
I0512 23:44:42.404253 140143678494720 gin_utils.py:85] train/utils.DatasetConfig.batch_size = %BATCH_SIZE
I0512 23:44:42.404287 140143678494720 gin_utils.py:85] train/utils.DatasetConfig.mixture_or_task_name = %MIXTURE_OR_TASK_NAME
I0512 23:44:42.404321 140143678494720 gin_utils.py:85] train/utils.DatasetConfig.module = %MIXTURE_OR_TASK_MODULE
I0512 23:44:42.404354 140143678494720 gin_utils.py:85] train/utils.DatasetConfig.pack = True
I0512 23:44:42.404387 140143678494720 gin_utils.py:85] train/utils.DatasetConfig.seed = None
I0512 23:44:42.404420 140143678494720 gin_utils.py:85] train/utils.DatasetConfig.shuffle = %SHUFFLE_TRAIN_EXAMPLES
I0512 23:44:42.404453 140143678494720 gin_utils.py:85] train/utils.DatasetConfig.split = 'train'
I0512 23:44:42.404485 140143678494720 gin_utils.py:85] train/utils.DatasetConfig.task_feature_lengths = %TASK_FEATURE_LENGTHS
I0512 23:44:42.404518 140143678494720 gin_utils.py:85] train/utils.DatasetConfig.use_cached = %USE_CACHED_TASKS
I0512 23:44:42.404576 140143678494720 gin_utils.py:85] 
I0512 23:44:42.404612 140143678494720 gin_utils.py:85] # Parameters for train_eval/utils.DatasetConfig:
I0512 23:44:42.404645 140143678494720 gin_utils.py:85] # ==============================================================================
I0512 23:44:42.404678 140143678494720 gin_utils.py:85] train_eval/utils.DatasetConfig.batch_size = 128
I0512 23:44:42.404711 140143678494720 gin_utils.py:85] train_eval/utils.DatasetConfig.mixture_or_task_name = %MIXTURE_OR_TASK_NAME
I0512 23:44:42.404743 140143678494720 gin_utils.py:85] train_eval/utils.DatasetConfig.module = %MIXTURE_OR_TASK_MODULE
I0512 23:44:42.404776 140143678494720 gin_utils.py:85] train_eval/utils.DatasetConfig.pack = True
I0512 23:44:42.404808 140143678494720 gin_utils.py:85] train_eval/utils.DatasetConfig.seed = 42
I0512 23:44:42.404841 140143678494720 gin_utils.py:85] train_eval/utils.DatasetConfig.shuffle = False
I0512 23:44:42.404874 140143678494720 gin_utils.py:85] train_eval/utils.DatasetConfig.split = 'validation'
I0512 23:44:42.404906 140143678494720 gin_utils.py:85] train_eval/utils.DatasetConfig.task_feature_lengths = %TASK_FEATURE_LENGTHS
I0512 23:44:42.404939 140143678494720 gin_utils.py:85] train_eval/utils.DatasetConfig.use_cached = %USE_CACHED_TASKS
I0512 23:44:42.404972 140143678494720 gin_utils.py:85] 
I0512 23:44:42.405004 140143678494720 gin_utils.py:85] # Parameters for t5_architecture.DecoderLayer:
I0512 23:44:42.405037 140143678494720 gin_utils.py:85] # ==============================================================================
I0512 23:44:42.405070 140143678494720 gin_utils.py:85] t5_architecture.DecoderLayer.activation_partitioning_dims = \
I0512 23:44:42.405102 140143678494720 gin_utils.py:85]     %ACTIVATION_PARTITIONING_DIMS
I0512 23:44:42.405135 140143678494720 gin_utils.py:85] t5_architecture.DecoderLayer.dropout_factory = %DROPOUT_FACTORY
I0512 23:44:42.405168 140143678494720 gin_utils.py:85] t5_architecture.DecoderLayer.encoder_decoder_attention = None
I0512 23:44:42.405200 140143678494720 gin_utils.py:85] t5_architecture.DecoderLayer.layer_norm_factory = @layer_norm.T5LayerNorm
I0512 23:44:42.405233 140143678494720 gin_utils.py:85] t5_architecture.DecoderLayer.mlp = @dense.MlpBlock()
I0512 23:44:42.405265 140143678494720 gin_utils.py:85] t5_architecture.DecoderLayer.scanned = False
I0512 23:44:42.405300 140143678494720 gin_utils.py:85] t5_architecture.DecoderLayer.self_attention = \
I0512 23:44:42.405334 140143678494720 gin_utils.py:85]     @dense_attention.MultiHeadDotProductAttention()
I0512 23:44:42.405367 140143678494720 gin_utils.py:85] 
I0512 23:44:42.405400 140143678494720 gin_utils.py:85] # Parameters for t5_architecture.DecoderOnly:
I0512 23:44:42.405433 140143678494720 gin_utils.py:85] # ==============================================================================
I0512 23:44:42.405466 140143678494720 gin_utils.py:85] t5_architecture.DecoderOnly.decoder_factory = @moe_architecture.SparseDecoder
I0512 23:44:42.405499 140143678494720 gin_utils.py:85] t5_architecture.DecoderOnly.dtype = %ACTIVATION_DTYPE
I0512 23:44:42.405537 140143678494720 gin_utils.py:85] t5_architecture.DecoderOnly.shared_token_embedder_factory = @embedding.Embed
I0512 23:44:42.405572 140143678494720 gin_utils.py:85] 
I0512 23:44:42.405605 140143678494720 gin_utils.py:85] # Parameters for output_logits/dense.DenseGeneral:
I0512 23:44:42.405638 140143678494720 gin_utils.py:85] # ==============================================================================
I0512 23:44:42.405671 140143678494720 gin_utils.py:85] output_logits/dense.DenseGeneral.bias_init = %BIAS_INIT
I0512 23:44:42.405704 140143678494720 gin_utils.py:85] output_logits/dense.DenseGeneral.dtype = 'float32'
I0512 23:44:42.405736 140143678494720 gin_utils.py:85] output_logits/dense.DenseGeneral.features = %NUM_EMBEDDINGS
I0512 23:44:42.405769 140143678494720 gin_utils.py:85] output_logits/dense.DenseGeneral.kernel_axis_names = ['embed', 'vocab']
I0512 23:44:42.405802 140143678494720 gin_utils.py:85] output_logits/dense.DenseGeneral.kernel_init = \
I0512 23:44:42.405834 140143678494720 gin_utils.py:85]     @output_logits_kernel_init/linen.initializers.variance_scaling()
I0512 23:44:42.405867 140143678494720 gin_utils.py:85] output_logits/dense.DenseGeneral.use_bias = False
I0512 23:44:42.405899 140143678494720 gin_utils.py:85] 
I0512 23:44:42.405932 140143678494720 gin_utils.py:85] # Parameters for dropout_factory/linen.Dropout:
I0512 23:44:42.405965 140143678494720 gin_utils.py:85] # ==============================================================================
I0512 23:44:42.405998 140143678494720 gin_utils.py:85] dropout_factory/linen.Dropout.broadcast_dims = (-2,)
I0512 23:44:42.406031 140143678494720 gin_utils.py:85] dropout_factory/linen.Dropout.rate = %DROPOUT_RATE
I0512 23:44:42.406063 140143678494720 gin_utils.py:85] 
I0512 23:44:42.406096 140143678494720 gin_utils.py:85] # Parameters for embedding.Embed:
I0512 23:44:42.406128 140143678494720 gin_utils.py:85] # ==============================================================================
I0512 23:44:42.406161 140143678494720 gin_utils.py:85] embedding.Embed.attend_dtype = 'float32'
I0512 23:44:42.406194 140143678494720 gin_utils.py:85] embedding.Embed.cast_input_dtype = 'int32'
I0512 23:44:42.406226 140143678494720 gin_utils.py:85] embedding.Embed.dtype = %ACTIVATION_DTYPE
I0512 23:44:42.406259 140143678494720 gin_utils.py:85] embedding.Embed.embedding_init = @token_embedder_init/linen.initializers.normal()
I0512 23:44:42.406293 140143678494720 gin_utils.py:85] embedding.Embed.features = %EMBED_DIM
I0512 23:44:42.406327 140143678494720 gin_utils.py:85] embedding.Embed.name = 'token_embedder'
I0512 23:44:42.406360 140143678494720 gin_utils.py:85] embedding.Embed.num_embeddings = %NUM_EMBEDDINGS
I0512 23:44:42.406393 140143678494720 gin_utils.py:85] embedding.Embed.one_hot = True
I0512 23:44:42.406426 140143678494720 gin_utils.py:85] 
I0512 23:44:42.406458 140143678494720 gin_utils.py:85] # Parameters for dense.MlpBlock:
I0512 23:44:42.406491 140143678494720 gin_utils.py:85] # ==============================================================================
I0512 23:44:42.406525 140143678494720 gin_utils.py:85] dense.MlpBlock.activations = ('swiglu', 'linear')
I0512 23:44:42.406565 140143678494720 gin_utils.py:85] dense.MlpBlock.bias_init = %BIAS_INIT
I0512 23:44:42.406598 140143678494720 gin_utils.py:85] dense.MlpBlock.dtype = %ACTIVATION_DTYPE
I0512 23:44:42.406631 140143678494720 gin_utils.py:85] dense.MlpBlock.final_dropout_rate = 0
I0512 23:44:42.406663 140143678494720 gin_utils.py:85] dense.MlpBlock.input_axis_name = 'mlp_embed'
I0512 23:44:42.406696 140143678494720 gin_utils.py:85] dense.MlpBlock.intermediate_dim = %MLP_DIM
I0512 23:44:42.406728 140143678494720 gin_utils.py:85] dense.MlpBlock.intermediate_dropout_rate = %DROPOUT_RATE
I0512 23:44:42.486167 140643590531072 resource_reader.py:50] system_path_file_exists:flaxformer/flaxformer/t5x/configs/moe/models/tokens_choose_decoder_only_large.gin
E0512 23:44:42.486339 140643590531072 resource_reader.py:55] Path not found: flaxformer/flaxformer/t5x/configs/moe/models/tokens_choose_decoder_only_large.gin
I0512 23:44:42.486396 140643590531072 resource_reader.py:50] system_path_file_exists:./flaxformer/flaxformer/t5x/configs/moe/models/tokens_choose_decoder_only_large.gin
E0512 23:44:42.486437 140643590531072 resource_reader.py:55] Path not found: ./flaxformer/flaxformer/t5x/configs/moe/models/tokens_choose_decoder_only_large.gin
I0512 23:44:42.487312 140643590531072 resource_reader.py:50] system_path_file_exists:flaxformer/flaxformer/t5x/configs/moe/architectures/moe_decoder_only.gin
E0512 23:44:42.487445 140643590531072 resource_reader.py:55] Path not found: flaxformer/flaxformer/t5x/configs/moe/architectures/moe_decoder_only.gin
I0512 23:44:42.487496 140643590531072 resource_reader.py:50] system_path_file_exists:./flaxformer/flaxformer/t5x/configs/moe/architectures/moe_decoder_only.gin
E0512 23:44:42.487536 140643590531072 resource_reader.py:55] Path not found: ./flaxformer/flaxformer/t5x/configs/moe/architectures/moe_decoder_only.gin
I0512 23:44:42.382968 139876724979712 gin_utils.py:85] dense.MlpBlock.kernel_init = @mlp_kernel_init/linen.initializers.variance_scaling()
I0512 23:44:42.382999 139876724979712 gin_utils.py:85] dense.MlpBlock.output_axis_name = 'mlp_embed'
I0512 23:44:42.383030 139876724979712 gin_utils.py:85] dense.MlpBlock.use_bias = False
I0512 23:44:42.383061 139876724979712 gin_utils.py:85] 
I0512 23:44:42.383114 139876724979712 gin_utils.py:85] # Parameters for expert/dense.MlpBlock:
I0512 23:44:42.383150 139876724979712 gin_utils.py:85] # ==============================================================================
I0512 23:44:42.383182 139876724979712 gin_utils.py:85] expert/dense.MlpBlock.activation_partitioning_dims = 1
I0512 23:44:42.383218 139876724979712 gin_utils.py:85] expert/dense.MlpBlock.activations = ('swiglu', 'linear')
I0512 23:44:42.383251 139876724979712 gin_utils.py:85] expert/dense.MlpBlock.bias_init = %BIAS_INIT
I0512 23:44:42.383282 139876724979712 gin_utils.py:85] expert/dense.MlpBlock.data_sharding_constraints = ('expert_replicas', 'mlp')
I0512 23:44:42.383313 139876724979712 gin_utils.py:85] expert/dense.MlpBlock.dtype = %MOE_TRUNCATED_DTYPE
I0512 23:44:42.383344 139876724979712 gin_utils.py:85] expert/dense.MlpBlock.final_dropout_rate = 0.0
I0512 23:44:42.383375 139876724979712 gin_utils.py:85] expert/dense.MlpBlock.input_axis_name = 'mlp_embed'
I0512 23:44:42.383405 139876724979712 gin_utils.py:85] expert/dense.MlpBlock.intermediate_axis_name = 'expert_mlp'
I0512 23:44:42.383436 139876724979712 gin_utils.py:85] expert/dense.MlpBlock.intermediate_dim = %EXPERT_MLP_DIM
I0512 23:44:42.383468 139876724979712 gin_utils.py:85] expert/dense.MlpBlock.intermediate_dropout_rate = %EXPERT_DROPOUT_RATE
I0512 23:44:42.383501 139876724979712 gin_utils.py:85] expert/dense.MlpBlock.kernel_init = \
I0512 23:44:42.383533 139876724979712 gin_utils.py:85]     @expert_kernel_init/linen.initializers.variance_scaling()
I0512 23:44:42.383564 139876724979712 gin_utils.py:85] expert/dense.MlpBlock.output_axis_name = 'mlp_embed'
I0512 23:44:42.433428 139666521167872 gin_utils.py:85] # Parameters for utils.CheckpointConfig:
I0512 23:44:42.433458 139666521167872 gin_utils.py:85] # ==============================================================================
I0512 23:44:42.433488 139666521167872 gin_utils.py:85] utils.CheckpointConfig.restore = @utils.RestoreCheckpointConfig()
I0512 23:44:42.433518 139666521167872 gin_utils.py:85] utils.CheckpointConfig.save = @utils.SaveCheckpointConfig()
I0512 23:44:42.433549 139666521167872 gin_utils.py:85] 
I0512 23:44:42.433581 139666521167872 gin_utils.py:85] # Parameters for moe_partitioning.compute_num_model_partitions:
I0512 23:44:42.433612 139666521167872 gin_utils.py:85] # ==============================================================================
I0512 23:44:42.433643 139666521167872 gin_utils.py:85] moe_partitioning.compute_num_model_partitions.model_parallel_submesh = \
I0512 23:44:42.433673 139666521167872 gin_utils.py:85]     %MODEL_PARALLEL_SUBMESH
I0512 23:44:42.433703 139666521167872 gin_utils.py:85] moe_partitioning.compute_num_model_partitions.num_model_partitions = \
I0512 23:44:42.433733 139666521167872 gin_utils.py:85]     %NUM_MODEL_PARTITIONS
I0512 23:44:42.433764 139666521167872 gin_utils.py:85] 
I0512 23:44:42.433794 139666521167872 gin_utils.py:85] # Parameters for utils.create_learning_rate_scheduler:
I0512 23:44:42.433824 139666521167872 gin_utils.py:85] # ==============================================================================
I0512 23:44:42.433855 139666521167872 gin_utils.py:85] utils.create_learning_rate_scheduler.base_learning_rate = 1.0
I0512 23:44:42.433885 139666521167872 gin_utils.py:85] utils.create_learning_rate_scheduler.factors = 'constant * rsqrt_decay'
I0512 23:44:42.433916 139666521167872 gin_utils.py:85] utils.create_learning_rate_scheduler.warmup_steps = 10000
I0512 23:44:42.433946 139666521167872 gin_utils.py:85] 
I0512 23:44:42.433976 139666521167872 gin_utils.py:85] # Parameters for train/utils.DatasetConfig:
I0512 23:44:42.439393 139856202532864 gin_utils.py:85] # Parameters for utils.CheckpointConfig:
I0512 23:44:42.439425 139856202532864 gin_utils.py:85] # ==============================================================================
I0512 23:44:42.439456 139856202532864 gin_utils.py:85] utils.CheckpointConfig.restore = @utils.RestoreCheckpointConfig()
I0512 23:44:42.439487 139856202532864 gin_utils.py:85] utils.CheckpointConfig.save = @utils.SaveCheckpointConfig()
I0512 23:44:42.439519 139856202532864 gin_utils.py:85] 
I0512 23:44:42.439550 139856202532864 gin_utils.py:85] # Parameters for moe_partitioning.compute_num_model_partitions:
I0512 23:44:42.439581 139856202532864 gin_utils.py:85] # ==============================================================================
I0512 23:44:42.439615 139856202532864 gin_utils.py:85] moe_partitioning.compute_num_model_partitions.model_parallel_submesh = \
I0512 23:44:42.439647 139856202532864 gin_utils.py:85]     %MODEL_PARALLEL_SUBMESH
I0512 23:44:42.439679 139856202532864 gin_utils.py:85] moe_partitioning.compute_num_model_partitions.num_model_partitions = \
I0512 23:44:42.439711 139856202532864 gin_utils.py:85]     %NUM_MODEL_PARTITIONS
I0512 23:44:42.439742 139856202532864 gin_utils.py:85] 
I0512 23:44:42.439773 139856202532864 gin_utils.py:85] # Parameters for utils.create_learning_rate_scheduler:
I0512 23:44:42.439804 139856202532864 gin_utils.py:85] # ==============================================================================
I0512 23:44:42.439835 139856202532864 gin_utils.py:85] utils.create_learning_rate_scheduler.base_learning_rate = 1.0
I0512 23:44:42.439873 139856202532864 gin_utils.py:85] utils.create_learning_rate_scheduler.factors = 'constant * rsqrt_decay'
I0512 23:44:42.439906 139856202532864 gin_utils.py:85] utils.create_learning_rate_scheduler.warmup_steps = 10000
I0512 23:44:42.439938 139856202532864 gin_utils.py:85] 
I0512 23:44:42.439969 139856202532864 gin_utils.py:85] # Parameters for train/utils.DatasetConfig:
I0512 23:44:42.440001 139856202532864 gin_utils.py:85] # ==============================================================================
I0512 23:44:42.440032 139856202532864 gin_utils.py:85] train/utils.DatasetConfig.batch_size = %BATCH_SIZE
I0512 23:44:42.440064 139856202532864 gin_utils.py:85] train/utils.DatasetConfig.mixture_or_task_name = %MIXTURE_OR_TASK_NAME
I0512 23:44:42.440095 139856202532864 gin_utils.py:85] train/utils.DatasetConfig.module = %MIXTURE_OR_TASK_MODULE
I0512 23:44:42.440127 139856202532864 gin_utils.py:85] train/utils.DatasetConfig.pack = True
I0512 23:44:42.440159 139856202532864 gin_utils.py:85] train/utils.DatasetConfig.seed = None
I0512 23:44:42.440190 139856202532864 gin_utils.py:85] train/utils.DatasetConfig.shuffle = %SHUFFLE_TRAIN_EXAMPLES
I0512 23:44:42.440221 139856202532864 gin_utils.py:85] train/utils.DatasetConfig.split = 'train'
I0512 23:44:42.440252 139856202532864 gin_utils.py:85] train/utils.DatasetConfig.task_feature_lengths = %TASK_FEATURE_LENGTHS
I0512 23:44:42.440284 139856202532864 gin_utils.py:85] train/utils.DatasetConfig.use_cached = %USE_CACHED_TASKS
I0512 23:44:42.440315 139856202532864 gin_utils.py:85] 
I0512 23:44:42.440346 139856202532864 gin_utils.py:85] # Parameters for train_eval/utils.DatasetConfig:
I0512 23:44:42.440377 139856202532864 gin_utils.py:85] # ==============================================================================
I0512 23:44:42.440408 139856202532864 gin_utils.py:85] train_eval/utils.DatasetConfig.batch_size = 128
I0512 23:44:42.440440 139856202532864 gin_utils.py:85] train_eval/utils.DatasetConfig.mixture_or_task_name = %MIXTURE_OR_TASK_NAME
I0512 23:44:42.440471 139856202532864 gin_utils.py:85] train_eval/utils.DatasetConfig.module = %MIXTURE_OR_TASK_MODULE
I0512 23:44:42.440503 139856202532864 gin_utils.py:85] train_eval/utils.DatasetConfig.pack = True
I0512 23:44:42.440534 139856202532864 gin_utils.py:85] train_eval/utils.DatasetConfig.seed = 42
I0512 23:44:42.440565 139856202532864 gin_utils.py:85] train_eval/utils.DatasetConfig.shuffle = False
I0512 23:44:42.440598 139856202532864 gin_utils.py:85] train_eval/utils.DatasetConfig.split = 'validation'
I0512 23:44:42.440632 139856202532864 gin_utils.py:85] train_eval/utils.DatasetConfig.task_feature_lengths = %TASK_FEATURE_LENGTHS
I0512 23:44:42.440663 139856202532864 gin_utils.py:85] train_eval/utils.DatasetConfig.use_cached = %USE_CACHED_TASKS
I0512 23:44:42.440695 139856202532864 gin_utils.py:85] 
I0512 23:44:42.440726 139856202532864 gin_utils.py:85] # Parameters for t5_architecture.DecoderLayer:
I0512 23:44:42.440757 139856202532864 gin_utils.py:85] # ==============================================================================
I0512 23:44:42.440788 139856202532864 gin_utils.py:85] t5_architecture.DecoderLayer.activation_partitioning_dims = \
I0512 23:44:42.440820 139856202532864 gin_utils.py:85]     %ACTIVATION_PARTITIONING_DIMS
I0512 23:44:42.440857 139856202532864 gin_utils.py:85] t5_architecture.DecoderLayer.dropout_factory = %DROPOUT_FACTORY
I0512 23:44:42.440890 139856202532864 gin_utils.py:85] t5_architecture.DecoderLayer.encoder_decoder_attention = None
I0512 23:44:42.440921 139856202532864 gin_utils.py:85] t5_architecture.DecoderLayer.layer_norm_factory = @layer_norm.T5LayerNorm
I0512 23:44:42.440953 139856202532864 gin_utils.py:85] t5_architecture.DecoderLayer.mlp = @dense.MlpBlock()
I0512 23:44:42.440985 139856202532864 gin_utils.py:85] t5_architecture.DecoderLayer.scanned = False
I0512 23:44:42.441016 139856202532864 gin_utils.py:85] t5_architecture.DecoderLayer.self_attention = \
I0512 23:44:42.441048 139856202532864 gin_utils.py:85]     @dense_attention.MultiHeadDotProductAttention()
I0512 23:44:42.441079 139856202532864 gin_utils.py:85] 
I0512 23:44:42.441110 139856202532864 gin_utils.py:85] # Parameters for t5_architecture.DecoderOnly:
I0512 23:44:42.441142 139856202532864 gin_utils.py:85] # ==============================================================================
I0512 23:44:42.441173 139856202532864 gin_utils.py:85] t5_architecture.DecoderOnly.decoder_factory = @moe_architecture.SparseDecoder
I0512 23:44:42.441204 139856202532864 gin_utils.py:85] t5_architecture.DecoderOnly.dtype = %ACTIVATION_DTYPE
I0512 23:44:42.441236 139856202532864 gin_utils.py:85] t5_architecture.DecoderOnly.shared_token_embedder_factory = @embedding.Embed
I0512 23:44:42.441267 139856202532864 gin_utils.py:85] 
I0512 23:44:42.441298 139856202532864 gin_utils.py:85] # Parameters for output_logits/dense.DenseGeneral:
I0512 23:44:42.441329 139856202532864 gin_utils.py:85] # ==============================================================================
I0512 23:44:42.441361 139856202532864 gin_utils.py:85] output_logits/dense.DenseGeneral.bias_init = %BIAS_INIT
I0512 23:44:42.441392 139856202532864 gin_utils.py:85] output_logits/dense.DenseGeneral.dtype = 'float32'
I0512 23:44:42.441423 139856202532864 gin_utils.py:85] output_logits/dense.DenseGeneral.features = %NUM_EMBEDDINGS
I0512 23:44:42.434006 139666521167872 gin_utils.py:85] # ==============================================================================
I0512 23:44:42.434037 139666521167872 gin_utils.py:85] train/utils.DatasetConfig.batch_size = %BATCH_SIZE
I0512 23:44:42.434067 139666521167872 gin_utils.py:85] train/utils.DatasetConfig.mixture_or_task_name = %MIXTURE_OR_TASK_NAME
I0512 23:44:42.434097 139666521167872 gin_utils.py:85] train/utils.DatasetConfig.module = %MIXTURE_OR_TASK_MODULE
I0512 23:44:42.434128 139666521167872 gin_utils.py:85] train/utils.DatasetConfig.pack = True
I0512 23:44:42.434158 139666521167872 gin_utils.py:85] train/utils.DatasetConfig.seed = None
I0512 23:44:42.434188 139666521167872 gin_utils.py:85] train/utils.DatasetConfig.shuffle = %SHUFFLE_TRAIN_EXAMPLES
I0512 23:44:42.434218 139666521167872 gin_utils.py:85] train/utils.DatasetConfig.split = 'train'
I0512 23:44:42.434248 139666521167872 gin_utils.py:85] train/utils.DatasetConfig.task_feature_lengths = %TASK_FEATURE_LENGTHS
I0512 23:44:42.434279 139666521167872 gin_utils.py:85] train/utils.DatasetConfig.use_cached = %USE_CACHED_TASKS
I0512 23:44:42.434314 139666521167872 gin_utils.py:85] 
I0512 23:44:42.434345 139666521167872 gin_utils.py:85] # Parameters for train_eval/utils.DatasetConfig:
I0512 23:44:42.434376 139666521167872 gin_utils.py:85] # ==============================================================================
I0512 23:44:42.434406 139666521167872 gin_utils.py:85] train_eval/utils.DatasetConfig.batch_size = 128
I0512 23:44:42.434437 139666521167872 gin_utils.py:85] train_eval/utils.DatasetConfig.mixture_or_task_name = %MIXTURE_OR_TASK_NAME
I0512 23:44:42.434467 139666521167872 gin_utils.py:85] train_eval/utils.DatasetConfig.module = %MIXTURE_OR_TASK_MODULE
I0512 23:44:42.434498 139666521167872 gin_utils.py:85] train_eval/utils.DatasetConfig.pack = True
I0512 23:44:42.434528 139666521167872 gin_utils.py:85] train_eval/utils.DatasetConfig.seed = 42
I0512 23:44:42.434559 139666521167872 gin_utils.py:85] train_eval/utils.DatasetConfig.shuffle = False
I0512 23:44:42.434592 139666521167872 gin_utils.py:85] train_eval/utils.DatasetConfig.split = 'validation'
I0512 23:44:42.434622 139666521167872 gin_utils.py:85] train_eval/utils.DatasetConfig.task_feature_lengths = %TASK_FEATURE_LENGTHS
I0512 23:44:42.434653 139666521167872 gin_utils.py:85] train_eval/utils.DatasetConfig.use_cached = %USE_CACHED_TASKS
I0512 23:44:42.434683 139666521167872 gin_utils.py:85] 
I0512 23:44:42.434713 139666521167872 gin_utils.py:85] # Parameters for t5_architecture.DecoderLayer:
I0512 23:44:42.434743 139666521167872 gin_utils.py:85] # ==============================================================================
I0512 23:44:42.434773 139666521167872 gin_utils.py:85] t5_architecture.DecoderLayer.activation_partitioning_dims = \
I0512 23:44:42.434804 139666521167872 gin_utils.py:85]     %ACTIVATION_PARTITIONING_DIMS
I0512 23:44:42.434834 139666521167872 gin_utils.py:85] t5_architecture.DecoderLayer.dropout_factory = %DROPOUT_FACTORY
I0512 23:44:42.434864 139666521167872 gin_utils.py:85] t5_architecture.DecoderLayer.encoder_decoder_attention = None
I0512 23:44:42.434895 139666521167872 gin_utils.py:85] t5_architecture.DecoderLayer.layer_norm_factory = @layer_norm.T5LayerNorm
I0512 23:44:42.434925 139666521167872 gin_utils.py:85] t5_architecture.DecoderLayer.mlp = @dense.MlpBlock()
I0512 23:44:42.434955 139666521167872 gin_utils.py:85] t5_architecture.DecoderLayer.scanned = False
I0512 23:44:42.434985 139666521167872 gin_utils.py:85] t5_architecture.DecoderLayer.self_attention = \
I0512 23:44:42.435015 139666521167872 gin_utils.py:85]     @dense_attention.MultiHeadDotProductAttention()
I0512 23:44:42.435045 139666521167872 gin_utils.py:85] 
I0512 23:44:42.435075 139666521167872 gin_utils.py:85] # Parameters for t5_architecture.DecoderOnly:
I0512 23:44:42.435129 139666521167872 gin_utils.py:85] # ==============================================================================
I0512 23:44:42.441455 139856202532864 gin_utils.py:85] output_logits/dense.DenseGeneral.kernel_axis_names = ['embed', 'vocab']
I0512 23:44:42.441486 139856202532864 gin_utils.py:85] output_logits/dense.DenseGeneral.kernel_init = \
I0512 23:44:42.441517 139856202532864 gin_utils.py:85]     @output_logits_kernel_init/linen.initializers.variance_scaling()
I0512 23:44:42.441549 139856202532864 gin_utils.py:85] output_logits/dense.DenseGeneral.use_bias = False
I0512 23:44:42.441580 139856202532864 gin_utils.py:85] 
I0512 23:44:42.441613 139856202532864 gin_utils.py:85] # Parameters for dropout_factory/linen.Dropout:
I0512 23:44:42.441645 139856202532864 gin_utils.py:85] # ==============================================================================
I0512 23:44:42.441677 139856202532864 gin_utils.py:85] dropout_factory/linen.Dropout.broadcast_dims = (-2,)
I0512 23:44:42.441728 139856202532864 gin_utils.py:85] dropout_factory/linen.Dropout.rate = %DROPOUT_RATE
I0512 23:44:42.441796 139856202532864 gin_utils.py:85] 
I0512 23:44:42.441831 139856202532864 gin_utils.py:85] # Parameters for embedding.Embed:
I0512 23:44:42.441869 139856202532864 gin_utils.py:85] # ==============================================================================
I0512 23:44:42.441900 139856202532864 gin_utils.py:85] embedding.Embed.attend_dtype = 'float32'
I0512 23:44:42.441932 139856202532864 gin_utils.py:85] embedding.Embed.cast_input_dtype = 'int32'
I0512 23:44:42.441963 139856202532864 gin_utils.py:85] embedding.Embed.dtype = %ACTIVATION_DTYPE
I0512 23:44:42.441994 139856202532864 gin_utils.py:85] embedding.Embed.embedding_init = @token_embedder_init/linen.initializers.normal()
I0512 23:44:42.442026 139856202532864 gin_utils.py:85] embedding.Embed.features = %EMBED_DIM
I0512 23:44:42.442057 139856202532864 gin_utils.py:85] embedding.Embed.name = 'token_embedder'
I0512 23:44:42.442088 139856202532864 gin_utils.py:85] embedding.Embed.num_embeddings = %NUM_EMBEDDINGS
I0512 23:44:42.442120 139856202532864 gin_utils.py:85] embedding.Embed.one_hot = True
I0512 23:44:42.442151 139856202532864 gin_utils.py:85] 
I0512 23:44:42.442183 139856202532864 gin_utils.py:85] # Parameters for dense.MlpBlock:
I0512 23:44:42.442214 139856202532864 gin_utils.py:85] # ==============================================================================
I0512 23:44:42.442246 139856202532864 gin_utils.py:85] dense.MlpBlock.activations = ('swiglu', 'linear')
I0512 23:44:42.442277 139856202532864 gin_utils.py:85] dense.MlpBlock.bias_init = %BIAS_INIT
I0512 23:44:42.442309 139856202532864 gin_utils.py:85] dense.MlpBlock.dtype = %ACTIVATION_DTYPE
I0512 23:44:42.442341 139856202532864 gin_utils.py:85] dense.MlpBlock.final_dropout_rate = 0
I0512 23:44:42.442372 139856202532864 gin_utils.py:85] dense.MlpBlock.input_axis_name = 'mlp_embed'
I0512 23:44:42.442404 139856202532864 gin_utils.py:85] dense.MlpBlock.intermediate_dim = %MLP_DIM
I0512 23:44:42.442435 139856202532864 gin_utils.py:85] dense.MlpBlock.intermediate_dropout_rate = %DROPOUT_RATE
I0512 23:44:42.435162 139666521167872 gin_utils.py:85] t5_architecture.DecoderOnly.decoder_factory = @moe_architecture.SparseDecoder
I0512 23:44:42.435192 139666521167872 gin_utils.py:85] t5_architecture.DecoderOnly.dtype = %ACTIVATION_DTYPE
I0512 23:44:42.435222 139666521167872 gin_utils.py:85] t5_architecture.DecoderOnly.shared_token_embedder_factory = @embedding.Embed
I0512 23:44:42.435253 139666521167872 gin_utils.py:85] 
I0512 23:44:42.435283 139666521167872 gin_utils.py:85] # Parameters for output_logits/dense.DenseGeneral:
I0512 23:44:42.435317 139666521167872 gin_utils.py:85] # ==============================================================================
I0512 23:44:42.435349 139666521167872 gin_utils.py:85] output_logits/dense.DenseGeneral.bias_init = %BIAS_INIT
I0512 23:44:42.435379 139666521167872 gin_utils.py:85] output_logits/dense.DenseGeneral.dtype = 'float32'
I0512 23:44:42.435410 139666521167872 gin_utils.py:85] output_logits/dense.DenseGeneral.features = %NUM_EMBEDDINGS
I0512 23:44:42.435440 139666521167872 gin_utils.py:85] output_logits/dense.DenseGeneral.kernel_axis_names = ['embed', 'vocab']
I0512 23:44:42.435470 139666521167872 gin_utils.py:85] output_logits/dense.DenseGeneral.kernel_init = \
I0512 23:44:42.435500 139666521167872 gin_utils.py:85]     @output_logits_kernel_init/linen.initializers.variance_scaling()
I0512 23:44:42.435530 139666521167872 gin_utils.py:85] output_logits/dense.DenseGeneral.use_bias = False
I0512 23:44:42.435561 139666521167872 gin_utils.py:85] 
I0512 23:44:42.435593 139666521167872 gin_utils.py:85] # Parameters for dropout_factory/linen.Dropout:
I0512 23:44:42.435624 139666521167872 gin_utils.py:85] # ==============================================================================
I0512 23:44:42.435654 139666521167872 gin_utils.py:85] dropout_factory/linen.Dropout.broadcast_dims = (-2,)
I0512 23:44:42.435685 139666521167872 gin_utils.py:85] dropout_factory/linen.Dropout.rate = %DROPOUT_RATE
I0512 23:44:42.435715 139666521167872 gin_utils.py:85] 
I0512 23:44:42.435745 139666521167872 gin_utils.py:85] # Parameters for embedding.Embed:
I0512 23:44:42.435775 139666521167872 gin_utils.py:85] # ==============================================================================
I0512 23:44:42.435806 139666521167872 gin_utils.py:85] embedding.Embed.attend_dtype = 'float32'
I0512 23:44:42.435836 139666521167872 gin_utils.py:85] embedding.Embed.cast_input_dtype = 'int32'
I0512 23:44:42.435866 139666521167872 gin_utils.py:85] embedding.Embed.dtype = %ACTIVATION_DTYPE
I0512 23:44:42.435896 139666521167872 gin_utils.py:85] embedding.Embed.embedding_init = @token_embedder_init/linen.initializers.normal()
I0512 23:44:42.435927 139666521167872 gin_utils.py:85] embedding.Embed.features = %EMBED_DIM
I0512 23:44:42.435956 139666521167872 gin_utils.py:85] embedding.Embed.name = 'token_embedder'
I0512 23:44:42.435987 139666521167872 gin_utils.py:85] embedding.Embed.num_embeddings = %NUM_EMBEDDINGS
I0512 23:44:42.436017 139666521167872 gin_utils.py:85] embedding.Embed.one_hot = True
I0512 23:44:42.436047 139666521167872 gin_utils.py:85] 
I0512 23:44:42.436077 139666521167872 gin_utils.py:85] # Parameters for dense.MlpBlock:
I0512 23:44:42.436108 139666521167872 gin_utils.py:85] # ==============================================================================
I0512 23:44:42.436139 139666521167872 gin_utils.py:85] dense.MlpBlock.activations = ('swiglu', 'linear')
I0512 23:44:42.436169 139666521167872 gin_utils.py:85] dense.MlpBlock.bias_init = %BIAS_INIT
I0512 23:44:42.436200 139666521167872 gin_utils.py:85] dense.MlpBlock.dtype = %ACTIVATION_DTYPE
I0512 23:44:42.436230 139666521167872 gin_utils.py:85] dense.MlpBlock.final_dropout_rate = 0
I0512 23:44:42.436260 139666521167872 gin_utils.py:85] dense.MlpBlock.input_axis_name = 'mlp_embed'
I0512 23:44:42.436290 139666521167872 gin_utils.py:85] dense.MlpBlock.intermediate_dim = %MLP_DIM
I0512 23:44:42.436326 139666521167872 gin_utils.py:85] dense.MlpBlock.intermediate_dropout_rate = %DROPOUT_RATE
I0512 23:44:42.439382 140193652815872 gin_utils.py:85] # Parameters for utils.CheckpointConfig:
I0512 23:44:42.439415 140193652815872 gin_utils.py:85] # ==============================================================================
I0512 23:44:42.439447 140193652815872 gin_utils.py:85] utils.CheckpointConfig.restore = @utils.RestoreCheckpointConfig()
I0512 23:44:42.439479 140193652815872 gin_utils.py:85] utils.CheckpointConfig.save = @utils.SaveCheckpointConfig()
I0512 23:44:42.439511 140193652815872 gin_utils.py:85] 
I0512 23:44:42.439543 140193652815872 gin_utils.py:85] # Parameters for moe_partitioning.compute_num_model_partitions:
I0512 23:44:42.439575 140193652815872 gin_utils.py:85] # ==============================================================================
I0512 23:44:42.439613 140193652815872 gin_utils.py:85] moe_partitioning.compute_num_model_partitions.model_parallel_submesh = \
I0512 23:44:42.439646 140193652815872 gin_utils.py:85]     %MODEL_PARALLEL_SUBMESH
I0512 23:44:42.439678 140193652815872 gin_utils.py:85] moe_partitioning.compute_num_model_partitions.num_model_partitions = \
I0512 23:44:42.439710 140193652815872 gin_utils.py:85]     %NUM_MODEL_PARTITIONS
I0512 23:44:42.439742 140193652815872 gin_utils.py:85] 
I0512 23:44:42.439774 140193652815872 gin_utils.py:85] # Parameters for utils.create_learning_rate_scheduler:
I0512 23:44:42.439806 140193652815872 gin_utils.py:85] # ==============================================================================
I0512 23:44:42.439838 140193652815872 gin_utils.py:85] utils.create_learning_rate_scheduler.base_learning_rate = 1.0
I0512 23:44:42.439870 140193652815872 gin_utils.py:85] utils.create_learning_rate_scheduler.factors = 'constant * rsqrt_decay'
I0512 23:44:42.439902 140193652815872 gin_utils.py:85] utils.create_learning_rate_scheduler.warmup_steps = 10000
I0512 23:44:42.439934 140193652815872 gin_utils.py:85] 
I0512 23:44:42.439966 140193652815872 gin_utils.py:85] # Parameters for train/utils.DatasetConfig:
I0512 23:44:42.439999 140193652815872 gin_utils.py:85] # ==============================================================================
I0512 23:44:42.440031 140193652815872 gin_utils.py:85] train/utils.DatasetConfig.batch_size = %BATCH_SIZE
I0512 23:44:42.440064 140193652815872 gin_utils.py:85] train/utils.DatasetConfig.mixture_or_task_name = %MIXTURE_OR_TASK_NAME
I0512 23:44:42.440096 140193652815872 gin_utils.py:85] train/utils.DatasetConfig.module = %MIXTURE_OR_TASK_MODULE
I0512 23:44:42.440128 140193652815872 gin_utils.py:85] train/utils.DatasetConfig.pack = True
I0512 23:44:42.440160 140193652815872 gin_utils.py:85] train/utils.DatasetConfig.seed = None
I0512 23:44:42.440192 140193652815872 gin_utils.py:85] train/utils.DatasetConfig.shuffle = %SHUFFLE_TRAIN_EXAMPLES
I0512 23:44:42.440224 140193652815872 gin_utils.py:85] train/utils.DatasetConfig.split = 'train'
I0512 23:44:42.440256 140193652815872 gin_utils.py:85] train/utils.DatasetConfig.task_feature_lengths = %TASK_FEATURE_LENGTHS
I0512 23:44:42.440288 140193652815872 gin_utils.py:85] train/utils.DatasetConfig.use_cached = %USE_CACHED_TASKS
I0512 23:44:42.440345 140193652815872 gin_utils.py:85] 
I0512 23:44:42.440392 140193652815872 gin_utils.py:85] # Parameters for train_eval/utils.DatasetConfig:
I0512 23:44:42.440426 140193652815872 gin_utils.py:85] # ==============================================================================
I0512 23:44:42.440459 140193652815872 gin_utils.py:85] train_eval/utils.DatasetConfig.batch_size = 128
I0512 23:44:42.440490 140193652815872 gin_utils.py:85] train_eval/utils.DatasetConfig.mixture_or_task_name = %MIXTURE_OR_TASK_NAME
I0512 23:44:42.440522 140193652815872 gin_utils.py:85] train_eval/utils.DatasetConfig.module = %MIXTURE_OR_TASK_MODULE
I0512 23:44:42.440555 140193652815872 gin_utils.py:85] train_eval/utils.DatasetConfig.pack = True
I0512 23:44:42.440587 140193652815872 gin_utils.py:85] train_eval/utils.DatasetConfig.seed = 42
I0512 23:44:42.440625 140193652815872 gin_utils.py:85] train_eval/utils.DatasetConfig.shuffle = False
I0512 23:44:42.440658 140193652815872 gin_utils.py:85] train_eval/utils.DatasetConfig.split = 'validation'
I0512 23:44:42.440690 140193652815872 gin_utils.py:85] train_eval/utils.DatasetConfig.task_feature_lengths = %TASK_FEATURE_LENGTHS
I0512 23:44:42.440722 140193652815872 gin_utils.py:85] train_eval/utils.DatasetConfig.use_cached = %USE_CACHED_TASKS
I0512 23:44:42.440754 140193652815872 gin_utils.py:85] 
I0512 23:44:42.440786 140193652815872 gin_utils.py:85] # Parameters for t5_architecture.DecoderLayer:
I0512 23:44:42.440818 140193652815872 gin_utils.py:85] # ==============================================================================
I0512 23:44:42.440850 140193652815872 gin_utils.py:85] t5_architecture.DecoderLayer.activation_partitioning_dims = \
I0512 23:44:42.440883 140193652815872 gin_utils.py:85]     %ACTIVATION_PARTITIONING_DIMS
I0512 23:44:42.440914 140193652815872 gin_utils.py:85] t5_architecture.DecoderLayer.dropout_factory = %DROPOUT_FACTORY
I0512 23:44:42.440947 140193652815872 gin_utils.py:85] t5_architecture.DecoderLayer.encoder_decoder_attention = None
I0512 23:44:42.440979 140193652815872 gin_utils.py:85] t5_architecture.DecoderLayer.layer_norm_factory = @layer_norm.T5LayerNorm
I0512 23:44:42.441011 140193652815872 gin_utils.py:85] t5_architecture.DecoderLayer.mlp = @dense.MlpBlock()
I0512 23:44:42.441043 140193652815872 gin_utils.py:85] t5_architecture.DecoderLayer.scanned = False
I0512 23:44:42.441075 140193652815872 gin_utils.py:85] t5_architecture.DecoderLayer.self_attention = \
I0512 23:44:42.441107 140193652815872 gin_utils.py:85]     @dense_attention.MultiHeadDotProductAttention()
I0512 23:44:42.441139 140193652815872 gin_utils.py:85] 
I0512 23:44:42.441171 140193652815872 gin_utils.py:85] # Parameters for t5_architecture.DecoderOnly:
I0512 23:44:42.441204 140193652815872 gin_utils.py:85] # ==============================================================================
I0512 23:44:42.441236 140193652815872 gin_utils.py:85] t5_architecture.DecoderOnly.decoder_factory = @moe_architecture.SparseDecoder
I0512 23:44:42.441267 140193652815872 gin_utils.py:85] t5_architecture.DecoderOnly.dtype = %ACTIVATION_DTYPE
I0512 23:44:42.441299 140193652815872 gin_utils.py:85] t5_architecture.DecoderOnly.shared_token_embedder_factory = @embedding.Embed
I0512 23:44:42.441331 140193652815872 gin_utils.py:85] 
I0512 23:44:42.441365 140193652815872 gin_utils.py:85] # Parameters for output_logits/dense.DenseGeneral:
I0512 23:44:42.441399 140193652815872 gin_utils.py:85] # ==============================================================================
I0512 23:44:42.441431 140193652815872 gin_utils.py:85] output_logits/dense.DenseGeneral.bias_init = %BIAS_INIT
I0512 23:44:42.441463 140193652815872 gin_utils.py:85] output_logits/dense.DenseGeneral.dtype = 'float32'
I0512 23:44:42.441495 140193652815872 gin_utils.py:85] output_logits/dense.DenseGeneral.features = %NUM_EMBEDDINGS
I0512 23:44:42.441527 140193652815872 gin_utils.py:85] output_logits/dense.DenseGeneral.kernel_axis_names = ['embed', 'vocab']
I0512 23:44:42.441559 140193652815872 gin_utils.py:85] output_logits/dense.DenseGeneral.kernel_init = \
I0512 23:44:42.441595 140193652815872 gin_utils.py:85]     @output_logits_kernel_init/linen.initializers.variance_scaling()
I0512 23:44:42.441629 140193652815872 gin_utils.py:85] output_logits/dense.DenseGeneral.use_bias = False
I0512 23:44:42.441662 140193652815872 gin_utils.py:85] 
I0512 23:44:42.441694 140193652815872 gin_utils.py:85] # Parameters for dropout_factory/linen.Dropout:
I0512 23:44:42.441726 140193652815872 gin_utils.py:85] # ==============================================================================
I0512 23:44:42.441758 140193652815872 gin_utils.py:85] dropout_factory/linen.Dropout.broadcast_dims = (-2,)
I0512 23:44:42.441790 140193652815872 gin_utils.py:85] dropout_factory/linen.Dropout.rate = %DROPOUT_RATE
I0512 23:44:42.441823 140193652815872 gin_utils.py:85] 
I0512 23:44:42.441855 140193652815872 gin_utils.py:85] # Parameters for embedding.Embed:
I0512 23:44:42.441887 140193652815872 gin_utils.py:85] # ==============================================================================
I0512 23:44:42.441919 140193652815872 gin_utils.py:85] embedding.Embed.attend_dtype = 'float32'
I0512 23:44:42.441951 140193652815872 gin_utils.py:85] embedding.Embed.cast_input_dtype = 'int32'
I0512 23:44:42.441983 140193652815872 gin_utils.py:85] embedding.Embed.dtype = %ACTIVATION_DTYPE
I0512 23:44:42.442015 140193652815872 gin_utils.py:85] embedding.Embed.embedding_init = @token_embedder_init/linen.initializers.normal()
I0512 23:44:42.442047 140193652815872 gin_utils.py:85] embedding.Embed.features = %EMBED_DIM
I0512 23:44:42.442079 140193652815872 gin_utils.py:85] embedding.Embed.name = 'token_embedder'
I0512 23:44:42.442111 140193652815872 gin_utils.py:85] embedding.Embed.num_embeddings = %NUM_EMBEDDINGS
I0512 23:44:42.442143 140193652815872 gin_utils.py:85] embedding.Embed.one_hot = True
I0512 23:44:42.442176 140193652815872 gin_utils.py:85] 
I0512 23:44:42.442208 140193652815872 gin_utils.py:85] # Parameters for dense.MlpBlock:
I0512 23:44:42.442240 140193652815872 gin_utils.py:85] # ==============================================================================
I0512 23:44:42.442273 140193652815872 gin_utils.py:85] dense.MlpBlock.activations = ('swiglu', 'linear')
I0512 23:44:42.442305 140193652815872 gin_utils.py:85] dense.MlpBlock.bias_init = %BIAS_INIT
I0512 23:44:42.442337 140193652815872 gin_utils.py:85] dense.MlpBlock.dtype = %ACTIVATION_DTYPE
I0512 23:44:42.442372 140193652815872 gin_utils.py:85] dense.MlpBlock.final_dropout_rate = 0
I0512 23:44:42.442405 140193652815872 gin_utils.py:85] dense.MlpBlock.input_axis_name = 'mlp_embed'
I0512 23:44:42.442437 140193652815872 gin_utils.py:85] dense.MlpBlock.intermediate_dim = %MLP_DIM
I0512 23:44:42.442469 140193652815872 gin_utils.py:85] dense.MlpBlock.intermediate_dropout_rate = %DROPOUT_RATE
I0512 23:44:42.406760 140143678494720 gin_utils.py:85] dense.MlpBlock.kernel_init = @mlp_kernel_init/linen.initializers.variance_scaling()
I0512 23:44:42.406793 140143678494720 gin_utils.py:85] dense.MlpBlock.output_axis_name = 'mlp_embed'
I0512 23:44:42.406825 140143678494720 gin_utils.py:85] dense.MlpBlock.use_bias = False
I0512 23:44:42.406857 140143678494720 gin_utils.py:85] 
I0512 23:44:42.406890 140143678494720 gin_utils.py:85] # Parameters for expert/dense.MlpBlock:
I0512 23:44:42.406922 140143678494720 gin_utils.py:85] # ==============================================================================
I0512 23:44:42.406955 140143678494720 gin_utils.py:85] expert/dense.MlpBlock.activation_partitioning_dims = 1
I0512 23:44:42.406987 140143678494720 gin_utils.py:85] expert/dense.MlpBlock.activations = ('swiglu', 'linear')
I0512 23:44:42.407020 140143678494720 gin_utils.py:85] expert/dense.MlpBlock.bias_init = %BIAS_INIT
I0512 23:44:42.407052 140143678494720 gin_utils.py:85] expert/dense.MlpBlock.data_sharding_constraints = ('expert_replicas', 'mlp')
I0512 23:44:42.407084 140143678494720 gin_utils.py:85] expert/dense.MlpBlock.dtype = %MOE_TRUNCATED_DTYPE
I0512 23:44:42.407116 140143678494720 gin_utils.py:85] expert/dense.MlpBlock.final_dropout_rate = 0.0
I0512 23:44:42.407149 140143678494720 gin_utils.py:85] expert/dense.MlpBlock.input_axis_name = 'mlp_embed'
I0512 23:44:42.407181 140143678494720 gin_utils.py:85] expert/dense.MlpBlock.intermediate_axis_name = 'expert_mlp'
I0512 23:44:42.407213 140143678494720 gin_utils.py:85] expert/dense.MlpBlock.intermediate_dim = %EXPERT_MLP_DIM
I0512 23:44:42.407246 140143678494720 gin_utils.py:85] expert/dense.MlpBlock.intermediate_dropout_rate = %EXPERT_DROPOUT_RATE
I0512 23:44:42.407278 140143678494720 gin_utils.py:85] expert/dense.MlpBlock.kernel_init = \
I0512 23:44:42.407313 140143678494720 gin_utils.py:85]     @expert_kernel_init/linen.initializers.variance_scaling()
I0512 23:44:42.407346 140143678494720 gin_utils.py:85] expert/dense.MlpBlock.output_axis_name = 'mlp_embed'
I0512 23:44:42.531786 140643590531072 resource_reader.py:50] system_path_file_exists:t5x/contrib/moe/configs/runs/pretrain.gin
I0512 23:44:42.436357 139666521167872 gin_utils.py:85] dense.MlpBlock.kernel_init = @mlp_kernel_init/linen.initializers.variance_scaling()
I0512 23:44:42.436388 139666521167872 gin_utils.py:85] dense.MlpBlock.output_axis_name = 'mlp_embed'
I0512 23:44:42.436418 139666521167872 gin_utils.py:85] dense.MlpBlock.use_bias = False
I0512 23:44:42.436449 139666521167872 gin_utils.py:85] 
I0512 23:44:42.436479 139666521167872 gin_utils.py:85] # Parameters for expert/dense.MlpBlock:
I0512 23:44:42.436509 139666521167872 gin_utils.py:85] # ==============================================================================
I0512 23:44:42.436539 139666521167872 gin_utils.py:85] expert/dense.MlpBlock.activation_partitioning_dims = 1
I0512 23:44:42.436571 139666521167872 gin_utils.py:85] expert/dense.MlpBlock.activations = ('swiglu', 'linear')
I0512 23:44:42.436603 139666521167872 gin_utils.py:85] expert/dense.MlpBlock.bias_init = %BIAS_INIT
I0512 23:44:42.436634 139666521167872 gin_utils.py:85] expert/dense.MlpBlock.data_sharding_constraints = ('expert_replicas', 'mlp')
I0512 23:44:42.436664 139666521167872 gin_utils.py:85] expert/dense.MlpBlock.dtype = %MOE_TRUNCATED_DTYPE
I0512 23:44:42.436695 139666521167872 gin_utils.py:85] expert/dense.MlpBlock.final_dropout_rate = 0.0
I0512 23:44:42.436725 139666521167872 gin_utils.py:85] expert/dense.MlpBlock.input_axis_name = 'mlp_embed'
I0512 23:44:42.436755 139666521167872 gin_utils.py:85] expert/dense.MlpBlock.intermediate_axis_name = 'expert_mlp'
I0512 23:44:42.436785 139666521167872 gin_utils.py:85] expert/dense.MlpBlock.intermediate_dim = %EXPERT_MLP_DIM
I0512 23:44:42.436816 139666521167872 gin_utils.py:85] expert/dense.MlpBlock.intermediate_dropout_rate = %EXPERT_DROPOUT_RATE
I0512 23:44:42.436846 139666521167872 gin_utils.py:85] expert/dense.MlpBlock.kernel_init = \
I0512 23:44:42.436876 139666521167872 gin_utils.py:85]     @expert_kernel_init/linen.initializers.variance_scaling()
I0512 23:44:42.436907 139666521167872 gin_utils.py:85] expert/dense.MlpBlock.output_axis_name = 'mlp_embed'
I0512 23:44:42.442467 139856202532864 gin_utils.py:85] dense.MlpBlock.kernel_init = @mlp_kernel_init/linen.initializers.variance_scaling()
I0512 23:44:42.442499 139856202532864 gin_utils.py:85] dense.MlpBlock.output_axis_name = 'mlp_embed'
I0512 23:44:42.442530 139856202532864 gin_utils.py:85] dense.MlpBlock.use_bias = False
I0512 23:44:42.442562 139856202532864 gin_utils.py:85] 
I0512 23:44:42.442595 139856202532864 gin_utils.py:85] # Parameters for expert/dense.MlpBlock:
I0512 23:44:42.442628 139856202532864 gin_utils.py:85] # ==============================================================================
I0512 23:44:42.442660 139856202532864 gin_utils.py:85] expert/dense.MlpBlock.activation_partitioning_dims = 1
I0512 23:44:42.442692 139856202532864 gin_utils.py:85] expert/dense.MlpBlock.activations = ('swiglu', 'linear')
I0512 23:44:42.442723 139856202532864 gin_utils.py:85] expert/dense.MlpBlock.bias_init = %BIAS_INIT
I0512 23:44:42.442755 139856202532864 gin_utils.py:85] expert/dense.MlpBlock.data_sharding_constraints = ('expert_replicas', 'mlp')
I0512 23:44:42.442787 139856202532864 gin_utils.py:85] expert/dense.MlpBlock.dtype = %MOE_TRUNCATED_DTYPE
I0512 23:44:42.442819 139856202532864 gin_utils.py:85] expert/dense.MlpBlock.final_dropout_rate = 0.0
I0512 23:44:42.442856 139856202532864 gin_utils.py:85] expert/dense.MlpBlock.input_axis_name = 'mlp_embed'
I0512 23:44:42.442889 139856202532864 gin_utils.py:85] expert/dense.MlpBlock.intermediate_axis_name = 'expert_mlp'
I0512 23:44:42.442921 139856202532864 gin_utils.py:85] expert/dense.MlpBlock.intermediate_dim = %EXPERT_MLP_DIM
I0512 23:44:42.442952 139856202532864 gin_utils.py:85] expert/dense.MlpBlock.intermediate_dropout_rate = %EXPERT_DROPOUT_RATE
I0512 23:44:42.442984 139856202532864 gin_utils.py:85] expert/dense.MlpBlock.kernel_init = \
I0512 23:44:42.443016 139856202532864 gin_utils.py:85]     @expert_kernel_init/linen.initializers.variance_scaling()
I0512 23:44:42.443048 139856202532864 gin_utils.py:85] expert/dense.MlpBlock.output_axis_name = 'mlp_embed'
I0512 23:44:42.442501 140193652815872 gin_utils.py:85] dense.MlpBlock.kernel_init = @mlp_kernel_init/linen.initializers.variance_scaling()
I0512 23:44:42.442533 140193652815872 gin_utils.py:85] dense.MlpBlock.output_axis_name = 'mlp_embed'
I0512 23:44:42.442565 140193652815872 gin_utils.py:85] dense.MlpBlock.use_bias = False
I0512 23:44:42.442602 140193652815872 gin_utils.py:85] 
I0512 23:44:42.442636 140193652815872 gin_utils.py:85] # Parameters for expert/dense.MlpBlock:
I0512 23:44:42.442668 140193652815872 gin_utils.py:85] # ==============================================================================
I0512 23:44:42.442700 140193652815872 gin_utils.py:85] expert/dense.MlpBlock.activation_partitioning_dims = 1
I0512 23:44:42.442732 140193652815872 gin_utils.py:85] expert/dense.MlpBlock.activations = ('swiglu', 'linear')
I0512 23:44:42.442764 140193652815872 gin_utils.py:85] expert/dense.MlpBlock.bias_init = %BIAS_INIT
I0512 23:44:42.442820 140193652815872 gin_utils.py:85] expert/dense.MlpBlock.data_sharding_constraints = ('expert_replicas', 'mlp')
I0512 23:44:42.442854 140193652815872 gin_utils.py:85] expert/dense.MlpBlock.dtype = %MOE_TRUNCATED_DTYPE
I0512 23:44:42.442887 140193652815872 gin_utils.py:85] expert/dense.MlpBlock.final_dropout_rate = 0.0
I0512 23:44:42.442919 140193652815872 gin_utils.py:85] expert/dense.MlpBlock.input_axis_name = 'mlp_embed'
I0512 23:44:42.442951 140193652815872 gin_utils.py:85] expert/dense.MlpBlock.intermediate_axis_name = 'expert_mlp'
I0512 23:44:42.442983 140193652815872 gin_utils.py:85] expert/dense.MlpBlock.intermediate_dim = %EXPERT_MLP_DIM
I0512 23:44:42.443015 140193652815872 gin_utils.py:85] expert/dense.MlpBlock.intermediate_dropout_rate = %EXPERT_DROPOUT_RATE
I0512 23:44:42.443046 140193652815872 gin_utils.py:85] expert/dense.MlpBlock.kernel_init = \
I0512 23:44:42.443078 140193652815872 gin_utils.py:85]     @expert_kernel_init/linen.initializers.variance_scaling()
I0512 23:44:42.443110 140193652815872 gin_utils.py:85] expert/dense.MlpBlock.output_axis_name = 'mlp_embed'
I0512 23:44:42.383595 139876724979712 gin_utils.py:85] expert/dense.MlpBlock.use_bias = False
I0512 23:44:42.383625 139876724979712 gin_utils.py:85] 
I0512 23:44:42.383656 139876724979712 gin_utils.py:85] # Parameters for models.MoeDecoderOnlyModel:
I0512 23:44:42.383687 139876724979712 gin_utils.py:85] # ==============================================================================
I0512 23:44:42.383718 139876724979712 gin_utils.py:85] models.MoeDecoderOnlyModel.aux_loss_factor = %AUX_LOSS_FACTOR
I0512 23:44:42.383749 139876724979712 gin_utils.py:85] models.MoeDecoderOnlyModel.inputs_bidirectional_attention = True
I0512 23:44:42.383780 139876724979712 gin_utils.py:85] models.MoeDecoderOnlyModel.label_smoothing = %LABEL_SMOOTHING
I0512 23:44:42.383810 139876724979712 gin_utils.py:85] models.MoeDecoderOnlyModel.loss_normalizing_factor = %LOSS_NORMALIZING_FACTOR
I0512 23:44:42.383841 139876724979712 gin_utils.py:85] models.MoeDecoderOnlyModel.module = %ARCHITECTURE
I0512 23:44:42.383872 139876724979712 gin_utils.py:85] models.MoeDecoderOnlyModel.optimizer_def = %OPTIMIZER
I0512 23:44:42.383903 139876724979712 gin_utils.py:85] models.MoeDecoderOnlyModel.router_z_loss_factor = %ROUTER_Z_LOSS_FACTOR
I0512 23:44:42.383969 139876724979712 gin_utils.py:85] models.MoeDecoderOnlyModel.vocabulary = %VOCABULARY
I0512 23:44:42.384003 139876724979712 gin_utils.py:85] models.MoeDecoderOnlyModel.z_loss = %Z_LOSS
I0512 23:44:42.384035 139876724979712 gin_utils.py:85] 
I0512 23:44:42.384066 139876724979712 gin_utils.py:85] # Parameters for moe_layers.MoeLayer:
I0512 23:44:42.384098 139876724979712 gin_utils.py:85] # ==============================================================================
I0512 23:44:42.384129 139876724979712 gin_utils.py:85] moe_layers.MoeLayer.dtype = %MOE_TRUNCATED_DTYPE
I0512 23:44:42.384160 139876724979712 gin_utils.py:85] moe_layers.MoeLayer.eval_capacity_factor = %EVAL_EXPERT_CAPACITY_FACTOR
I0512 23:44:42.384191 139876724979712 gin_utils.py:85] moe_layers.MoeLayer.expert = @expert/dense.MlpBlock()
I0512 23:44:42.384227 139876724979712 gin_utils.py:85] moe_layers.MoeLayer.max_group_size = %GROUP_SIZE
I0512 23:44:42.384260 139876724979712 gin_utils.py:85] moe_layers.MoeLayer.num_expert_partitions = %NUM_EXPERT_PARTITIONS
I0512 23:44:42.384291 139876724979712 gin_utils.py:85] moe_layers.MoeLayer.num_experts = %NUM_EXPERTS
I0512 23:44:42.384322 139876724979712 gin_utils.py:85] moe_layers.MoeLayer.num_model_partitions = \
I0512 23:44:42.384353 139876724979712 gin_utils.py:85]     @moe_partitioning.compute_num_model_partitions()
I0512 23:44:42.384384 139876724979712 gin_utils.py:85] moe_layers.MoeLayer.train_capacity_factor = %TRAIN_EXPERT_CAPACITY_FACTOR
I0512 23:44:42.384416 139876724979712 gin_utils.py:85] 
I0512 23:44:42.384447 139876724979712 gin_utils.py:85] # Parameters for sparse_decoder/moe_layers.MoeLayer:
I0512 23:44:42.384480 139876724979712 gin_utils.py:85] # ==============================================================================
I0512 23:44:42.384512 139876724979712 gin_utils.py:85] sparse_decoder/moe_layers.MoeLayer.router = \
I0512 23:44:42.384543 139876724979712 gin_utils.py:85]     @sparse_decoder/routing.TokensChooseMaskedRouter()
I0512 23:44:42.384575 139876724979712 gin_utils.py:85] 
I0512 23:44:42.384606 139876724979712 gin_utils.py:85] # Parameters for moe_partitioning.MoePjitPartitioner:
I0512 23:44:42.384637 139876724979712 gin_utils.py:85] # ==============================================================================
I0512 23:44:42.384668 139876724979712 gin_utils.py:85] moe_partitioning.MoePjitPartitioner.model_parallel_submesh = \
I0512 23:44:42.384699 139876724979712 gin_utils.py:85]     %MODEL_PARALLEL_SUBMESH
I0512 23:44:42.384730 139876724979712 gin_utils.py:85] moe_partitioning.MoePjitPartitioner.num_expert_partitions = %NUM_EXPERT_PARTITIONS
I0512 23:44:42.384761 139876724979712 gin_utils.py:85] moe_partitioning.MoePjitPartitioner.num_partitions = %NUM_MODEL_PARTITIONS
I0512 23:44:42.384792 139876724979712 gin_utils.py:85] 
I0512 23:44:42.384823 139876724979712 gin_utils.py:85] # Parameters for moe_trainer.MoeTrainer:
I0512 23:44:42.384854 139876724979712 gin_utils.py:85] # ==============================================================================
I0512 23:44:42.384885 139876724979712 gin_utils.py:85] moe_trainer.MoeTrainer.learning_rate_fn = @utils.create_learning_rate_scheduler()
I0512 23:44:42.384916 139876724979712 gin_utils.py:85] moe_trainer.MoeTrainer.num_expert_partitions = %NUM_EXPERT_PARTITIONS
I0512 23:44:42.384947 139876724979712 gin_utils.py:85] moe_trainer.MoeTrainer.num_microbatches = 8
I0512 23:44:42.384978 139876724979712 gin_utils.py:85] 
I0512 23:44:42.385008 139876724979712 gin_utils.py:85] # Parameters for dense_attention.MultiHeadDotProductAttention:
I0512 23:44:42.385039 139876724979712 gin_utils.py:85] # ==============================================================================
I0512 23:44:42.385071 139876724979712 gin_utils.py:85] dense_attention.MultiHeadDotProductAttention.bias_init = %BIAS_INIT
I0512 23:44:42.385102 139876724979712 gin_utils.py:85] dense_attention.MultiHeadDotProductAttention.broadcast_dropout = True
I0512 23:44:42.385133 139876724979712 gin_utils.py:85] dense_attention.MultiHeadDotProductAttention.dropout_rate = %DROPOUT_RATE
I0512 23:44:42.385164 139876724979712 gin_utils.py:85] dense_attention.MultiHeadDotProductAttention.dtype = %ACTIVATION_DTYPE
I0512 23:44:42.385195 139876724979712 gin_utils.py:85] dense_attention.MultiHeadDotProductAttention.head_dim = %HEAD_DIM
I0512 23:44:42.385231 139876724979712 gin_utils.py:85] dense_attention.MultiHeadDotProductAttention.kernel_init = \
I0512 23:44:42.385264 139876724979712 gin_utils.py:85]     @attention_kernel_init/linen.initializers.variance_scaling()
I0512 23:44:42.385295 139876724979712 gin_utils.py:85] dense_attention.MultiHeadDotProductAttention.num_heads = %NUM_HEADS
I0512 23:44:42.385326 139876724979712 gin_utils.py:85] dense_attention.MultiHeadDotProductAttention.use_bias = False
I0512 23:44:42.385357 139876724979712 gin_utils.py:85] dense_attention.MultiHeadDotProductAttention.use_rotary_embedding = True
I0512 23:44:42.385388 139876724979712 gin_utils.py:85] 
I0512 23:44:42.385420 139876724979712 gin_utils.py:85] # Parameters for bias_init/linen.initializers.normal:
I0512 23:44:42.385451 139876724979712 gin_utils.py:85] # ==============================================================================
I0512 23:44:42.385484 139876724979712 gin_utils.py:85] bias_init/linen.initializers.normal.stddev = 1e-06
I0512 23:44:42.385517 139876724979712 gin_utils.py:85] 
I0512 23:44:42.385549 139876724979712 gin_utils.py:85] # Parameters for router_init/linen.initializers.normal:
I0512 23:44:42.385580 139876724979712 gin_utils.py:85] # ==============================================================================
I0512 23:44:42.385611 139876724979712 gin_utils.py:85] router_init/linen.initializers.normal.stddev = 0.02
I0512 23:44:42.385643 139876724979712 gin_utils.py:85] 
I0512 23:44:42.385674 139876724979712 gin_utils.py:85] # Parameters for token_embedder_init/linen.initializers.normal:
I0512 23:44:42.385705 139876724979712 gin_utils.py:85] # ==============================================================================
I0512 23:44:42.385737 139876724979712 gin_utils.py:85] token_embedder_init/linen.initializers.normal.stddev = 1.0
I0512 23:44:42.385768 139876724979712 gin_utils.py:85] 
I0512 23:44:42.385799 139876724979712 gin_utils.py:85] # Parameters for partitioning.PjitPartitioner:
I0512 23:44:42.385830 139876724979712 gin_utils.py:85] # ==============================================================================
I0512 23:44:42.385861 139876724979712 gin_utils.py:85] partitioning.PjitPartitioner.logical_axis_rules = \
I0512 23:44:42.385892 139876724979712 gin_utils.py:85]     @partitioning.standard_logical_axis_rules()
I0512 23:44:42.385924 139876724979712 gin_utils.py:85] partitioning.PjitPartitioner.model_parallel_submesh = None
I0512 23:44:42.385955 139876724979712 gin_utils.py:85] partitioning.PjitPartitioner.num_partitions = 1
I0512 23:44:42.385986 139876724979712 gin_utils.py:85] 
I0512 23:44:42.386017 139876724979712 gin_utils.py:85] # Parameters for utils.RestoreCheckpointConfig:
I0512 23:44:42.386048 139876724979712 gin_utils.py:85] # ==============================================================================
I0512 23:44:42.386079 139876724979712 gin_utils.py:85] utils.RestoreCheckpointConfig.path = []
I0512 23:44:42.386111 139876724979712 gin_utils.py:85] 
I0512 23:44:42.386142 139876724979712 gin_utils.py:85] # Parameters for routing.RouterWeights:
I0512 23:44:42.386173 139876724979712 gin_utils.py:85] # ==============================================================================
I0512 23:44:42.386204 139876724979712 gin_utils.py:85] routing.RouterWeights.bias_init = %BIAS_INIT
I0512 23:44:42.386243 139876724979712 gin_utils.py:85] routing.RouterWeights.dtype = 'float32'
I0512 23:44:42.386274 139876724979712 gin_utils.py:85] routing.RouterWeights.kernel_init = @router_init/linen.initializers.normal()
I0512 23:44:42.386305 139876724979712 gin_utils.py:85] routing.RouterWeights.use_bias = False
I0512 23:44:42.386336 139876724979712 gin_utils.py:85] 
I0512 23:44:42.386367 139876724979712 gin_utils.py:85] # Parameters for utils.SaveCheckpointConfig:
I0512 23:44:42.386398 139876724979712 gin_utils.py:85] # ==============================================================================
I0512 23:44:42.386429 139876724979712 gin_utils.py:85] utils.SaveCheckpointConfig.dtype = 'float32'
I0512 23:44:42.386461 139876724979712 gin_utils.py:85] utils.SaveCheckpointConfig.keep = 20
I0512 23:44:42.386494 139876724979712 gin_utils.py:85] utils.SaveCheckpointConfig.period = 2500
I0512 23:44:42.386526 139876724979712 gin_utils.py:85] utils.SaveCheckpointConfig.save_dataset = False
I0512 23:44:42.386557 139876724979712 gin_utils.py:85] 
I0512 23:44:42.386588 139876724979712 gin_utils.py:85] # Parameters for seqio.SentencePieceVocabulary:
I0512 23:44:42.386619 139876724979712 gin_utils.py:85] # ==============================================================================
I0512 23:44:42.386650 139876724979712 gin_utils.py:85] seqio.SentencePieceVocabulary.extra_ids = 300
I0512 23:44:42.386682 139876724979712 gin_utils.py:85] seqio.SentencePieceVocabulary.sentencepiece_model_file = \
I0512 23:44:42.386713 139876724979712 gin_utils.py:85]     'gs://rosinality-tpu-bucket/sentencepiece.model'
I0512 23:44:42.386744 139876724979712 gin_utils.py:85] 
I0512 23:44:42.386775 139876724979712 gin_utils.py:85] # Parameters for moe_architecture.SparseDecoder:
I0512 23:44:42.386806 139876724979712 gin_utils.py:85] # ==============================================================================
I0512 23:44:42.386837 139876724979712 gin_utils.py:85] moe_architecture.SparseDecoder.dropout_factory = %DROPOUT_FACTORY
I0512 23:44:42.386868 139876724979712 gin_utils.py:85] moe_architecture.SparseDecoder.dtype = %ACTIVATION_DTYPE
I0512 23:44:42.386899 139876724979712 gin_utils.py:85] moe_architecture.SparseDecoder.layer_factory = @t5_architecture.DecoderLayer
I0512 23:44:42.386930 139876724979712 gin_utils.py:85] moe_architecture.SparseDecoder.layer_norm_factory = @layer_norm.T5LayerNorm
I0512 23:44:42.386961 139876724979712 gin_utils.py:85] moe_architecture.SparseDecoder.num_layers = %NUM_DECODER_LAYERS
I0512 23:44:42.386992 139876724979712 gin_utils.py:85] moe_architecture.SparseDecoder.num_sparse_layers = %NUM_DECODER_SPARSE_LAYERS
I0512 23:44:42.387023 139876724979712 gin_utils.py:85] moe_architecture.SparseDecoder.output_logits_factory = \
I0512 23:44:42.387054 139876724979712 gin_utils.py:85]     @output_logits/dense.DenseGeneral
I0512 23:44:42.387103 139876724979712 gin_utils.py:85] moe_architecture.SparseDecoder.sparse_layer_factory = \
I0512 23:44:42.387140 139876724979712 gin_utils.py:85]     @moe_architecture.SparseDecoderLayer
I0512 23:44:42.387172 139876724979712 gin_utils.py:85] moe_architecture.SparseDecoder.sparse_layout = %DECODER_SPARSE_LAYOUT
I0512 23:44:42.387204 139876724979712 gin_utils.py:85] 
I0512 23:44:42.387240 139876724979712 gin_utils.py:85] # Parameters for moe_architecture.SparseDecoderLayer:
I0512 23:44:42.387272 139876724979712 gin_utils.py:85] # ==============================================================================
I0512 23:44:42.387303 139876724979712 gin_utils.py:85] moe_architecture.SparseDecoderLayer.activation_partitioning_dims = \
I0512 23:44:42.387334 139876724979712 gin_utils.py:85]     %ACTIVATION_PARTITIONING_DIMS
I0512 23:44:42.387365 139876724979712 gin_utils.py:85] moe_architecture.SparseDecoderLayer.dropout_factory = %DROPOUT_FACTORY
I0512 23:44:42.387397 139876724979712 gin_utils.py:85] moe_architecture.SparseDecoderLayer.encoder_decoder_attention = None
I0512 23:44:42.387428 139876724979712 gin_utils.py:85] moe_architecture.SparseDecoderLayer.extra_mlp = @dense.MlpBlock()
I0512 23:44:42.387459 139876724979712 gin_utils.py:85] moe_architecture.SparseDecoderLayer.layer_norm_factory = @layer_norm.T5LayerNorm
I0512 23:44:42.387494 139876724979712 gin_utils.py:85] moe_architecture.SparseDecoderLayer.mlp = @sparse_decoder/moe_layers.MoeLayer()
I0512 23:44:42.387525 139876724979712 gin_utils.py:85] moe_architecture.SparseDecoderLayer.scanned = False
I0512 23:44:42.387556 139876724979712 gin_utils.py:85] moe_architecture.SparseDecoderLayer.self_attention = \
I0512 23:44:42.387587 139876724979712 gin_utils.py:85]     @dense_attention.MultiHeadDotProductAttention()
I0512 23:44:42.387619 139876724979712 gin_utils.py:85] 
I0512 23:44:42.387650 139876724979712 gin_utils.py:85] # Parameters for layer_norm.T5LayerNorm:
I0512 23:44:42.387681 139876724979712 gin_utils.py:85] # ==============================================================================
I0512 23:44:42.387713 139876724979712 gin_utils.py:85] layer_norm.T5LayerNorm.dtype = %ACTIVATION_DTYPE
I0512 23:44:42.387744 139876724979712 gin_utils.py:85] 
I0512 23:44:42.387775 139876724979712 gin_utils.py:85] # Parameters for routing.TokensChooseMaskedRouter:
I0512 23:44:42.387806 139876724979712 gin_utils.py:85] # ==============================================================================
I0512 23:44:42.387838 139876724979712 gin_utils.py:85] routing.TokensChooseMaskedRouter.dtype = 'float32'
I0512 23:44:42.387869 139876724979712 gin_utils.py:85] routing.TokensChooseMaskedRouter.ignore_padding_tokens = False
I0512 23:44:42.387900 139876724979712 gin_utils.py:85] routing.TokensChooseMaskedRouter.jitter_noise = %JITTER_NOISE
I0512 23:44:42.387931 139876724979712 gin_utils.py:85] routing.TokensChooseMaskedRouter.num_selected_experts = %NUM_SELECTED_EXPERTS
I0512 23:44:42.387962 139876724979712 gin_utils.py:85] routing.TokensChooseMaskedRouter.router_weights = @routing.RouterWeights()
I0512 23:44:42.387993 139876724979712 gin_utils.py:85] 
I0512 23:44:42.388024 139876724979712 gin_utils.py:85] # Parameters for sparse_decoder/routing.TokensChooseMaskedRouter:
I0512 23:44:42.388055 139876724979712 gin_utils.py:85] # ==============================================================================
I0512 23:44:42.388086 139876724979712 gin_utils.py:85] sparse_decoder/routing.TokensChooseMaskedRouter.batch_prioritized_routing = False
I0512 23:44:42.388117 139876724979712 gin_utils.py:85] 
I0512 23:44:42.388148 139876724979712 gin_utils.py:85] # Parameters for sparse_encoder/routing.TokensChooseMaskedRouter:
I0512 23:44:42.388179 139876724979712 gin_utils.py:85] # ==============================================================================
I0512 23:44:42.388215 139876724979712 gin_utils.py:85] sparse_encoder/routing.TokensChooseMaskedRouter.batch_prioritized_routing = False
I0512 23:44:42.388249 139876724979712 gin_utils.py:85] 
I0512 23:44:42.388281 139876724979712 gin_utils.py:85] # Parameters for train_script.train:
I0512 23:44:42.388312 139876724979712 gin_utils.py:85] # ==============================================================================
I0512 23:44:42.388343 139876724979712 gin_utils.py:85] train_script.train.checkpoint_cfg = @utils.CheckpointConfig()
I0512 23:44:42.572940 140260698863616 resource_reader.py:50] system_path_file_exists:flaxformer/flaxformer/t5x/configs/moe/models/st_moe_decoder_only_large.gin
E0512 23:44:42.573402 140260698863616 resource_reader.py:55] Path not found: flaxformer/flaxformer/t5x/configs/moe/models/st_moe_decoder_only_large.gin
I0512 23:44:42.573465 140260698863616 resource_reader.py:50] system_path_file_exists:./flaxformer/flaxformer/t5x/configs/moe/models/st_moe_decoder_only_large.gin
E0512 23:44:42.573512 140260698863616 resource_reader.py:55] Path not found: ./flaxformer/flaxformer/t5x/configs/moe/models/st_moe_decoder_only_large.gin
I0512 23:44:42.407378 140143678494720 gin_utils.py:85] expert/dense.MlpBlock.use_bias = False
I0512 23:44:42.407411 140143678494720 gin_utils.py:85] 
I0512 23:44:42.407443 140143678494720 gin_utils.py:85] # Parameters for models.MoeDecoderOnlyModel:
I0512 23:44:42.407475 140143678494720 gin_utils.py:85] # ==============================================================================
I0512 23:44:42.407508 140143678494720 gin_utils.py:85] models.MoeDecoderOnlyModel.aux_loss_factor = %AUX_LOSS_FACTOR
I0512 23:44:42.407546 140143678494720 gin_utils.py:85] models.MoeDecoderOnlyModel.inputs_bidirectional_attention = True
I0512 23:44:42.407580 140143678494720 gin_utils.py:85] models.MoeDecoderOnlyModel.label_smoothing = %LABEL_SMOOTHING
I0512 23:44:42.407613 140143678494720 gin_utils.py:85] models.MoeDecoderOnlyModel.loss_normalizing_factor = %LOSS_NORMALIZING_FACTOR
I0512 23:44:42.407645 140143678494720 gin_utils.py:85] models.MoeDecoderOnlyModel.module = %ARCHITECTURE
I0512 23:44:42.407678 140143678494720 gin_utils.py:85] models.MoeDecoderOnlyModel.optimizer_def = %OPTIMIZER
I0512 23:44:42.407711 140143678494720 gin_utils.py:85] models.MoeDecoderOnlyModel.router_z_loss_factor = %ROUTER_Z_LOSS_FACTOR
I0512 23:44:42.407743 140143678494720 gin_utils.py:85] models.MoeDecoderOnlyModel.vocabulary = %VOCABULARY
I0512 23:44:42.407776 140143678494720 gin_utils.py:85] models.MoeDecoderOnlyModel.z_loss = %Z_LOSS
I0512 23:44:42.407808 140143678494720 gin_utils.py:85] 
I0512 23:44:42.407841 140143678494720 gin_utils.py:85] # Parameters for moe_layers.MoeLayer:
I0512 23:44:42.407873 140143678494720 gin_utils.py:85] # ==============================================================================
I0512 23:44:42.407906 140143678494720 gin_utils.py:85] moe_layers.MoeLayer.dtype = %MOE_TRUNCATED_DTYPE
I0512 23:44:42.407938 140143678494720 gin_utils.py:85] moe_layers.MoeLayer.eval_capacity_factor = %EVAL_EXPERT_CAPACITY_FACTOR
I0512 23:44:42.407970 140143678494720 gin_utils.py:85] moe_layers.MoeLayer.expert = @expert/dense.MlpBlock()
I0512 23:44:42.408003 140143678494720 gin_utils.py:85] moe_layers.MoeLayer.max_group_size = %GROUP_SIZE
I0512 23:44:42.408035 140143678494720 gin_utils.py:85] moe_layers.MoeLayer.num_expert_partitions = %NUM_EXPERT_PARTITIONS
I0512 23:44:42.408067 140143678494720 gin_utils.py:85] moe_layers.MoeLayer.num_experts = %NUM_EXPERTS
I0512 23:44:42.408100 140143678494720 gin_utils.py:85] moe_layers.MoeLayer.num_model_partitions = \
I0512 23:44:42.408132 140143678494720 gin_utils.py:85]     @moe_partitioning.compute_num_model_partitions()
I0512 23:44:42.408164 140143678494720 gin_utils.py:85] moe_layers.MoeLayer.train_capacity_factor = %TRAIN_EXPERT_CAPACITY_FACTOR
I0512 23:44:42.408197 140143678494720 gin_utils.py:85] 
I0512 23:44:42.408229 140143678494720 gin_utils.py:85] # Parameters for sparse_decoder/moe_layers.MoeLayer:
I0512 23:44:42.408262 140143678494720 gin_utils.py:85] # ==============================================================================
I0512 23:44:42.408296 140143678494720 gin_utils.py:85] sparse_decoder/moe_layers.MoeLayer.router = \
I0512 23:44:42.408329 140143678494720 gin_utils.py:85]     @sparse_decoder/routing.TokensChooseMaskedRouter()
I0512 23:44:42.408362 140143678494720 gin_utils.py:85] 
I0512 23:44:42.408394 140143678494720 gin_utils.py:85] # Parameters for moe_partitioning.MoePjitPartitioner:
I0512 23:44:42.408427 140143678494720 gin_utils.py:85] # ==============================================================================
I0512 23:44:42.408459 140143678494720 gin_utils.py:85] moe_partitioning.MoePjitPartitioner.model_parallel_submesh = \
I0512 23:44:42.408491 140143678494720 gin_utils.py:85]     %MODEL_PARALLEL_SUBMESH
I0512 23:44:42.408524 140143678494720 gin_utils.py:85] moe_partitioning.MoePjitPartitioner.num_expert_partitions = %NUM_EXPERT_PARTITIONS
I0512 23:44:42.408581 140143678494720 gin_utils.py:85] moe_partitioning.MoePjitPartitioner.num_partitions = %NUM_MODEL_PARTITIONS
I0512 23:44:42.408616 140143678494720 gin_utils.py:85] 
I0512 23:44:42.408649 140143678494720 gin_utils.py:85] # Parameters for moe_trainer.MoeTrainer:
I0512 23:44:42.408682 140143678494720 gin_utils.py:85] # ==============================================================================
I0512 23:44:42.408714 140143678494720 gin_utils.py:85] moe_trainer.MoeTrainer.learning_rate_fn = @utils.create_learning_rate_scheduler()
I0512 23:44:42.408747 140143678494720 gin_utils.py:85] moe_trainer.MoeTrainer.num_expert_partitions = %NUM_EXPERT_PARTITIONS
I0512 23:44:42.408779 140143678494720 gin_utils.py:85] moe_trainer.MoeTrainer.num_microbatches = 8
I0512 23:44:42.408812 140143678494720 gin_utils.py:85] 
I0512 23:44:42.408844 140143678494720 gin_utils.py:85] # Parameters for dense_attention.MultiHeadDotProductAttention:
I0512 23:44:42.408878 140143678494720 gin_utils.py:85] # ==============================================================================
I0512 23:44:42.408910 140143678494720 gin_utils.py:85] dense_attention.MultiHeadDotProductAttention.bias_init = %BIAS_INIT
I0512 23:44:42.408942 140143678494720 gin_utils.py:85] dense_attention.MultiHeadDotProductAttention.broadcast_dropout = True
I0512 23:44:42.408975 140143678494720 gin_utils.py:85] dense_attention.MultiHeadDotProductAttention.dropout_rate = %DROPOUT_RATE
I0512 23:44:42.409007 140143678494720 gin_utils.py:85] dense_attention.MultiHeadDotProductAttention.dtype = %ACTIVATION_DTYPE
I0512 23:44:42.409039 140143678494720 gin_utils.py:85] dense_attention.MultiHeadDotProductAttention.head_dim = %HEAD_DIM
I0512 23:44:42.409071 140143678494720 gin_utils.py:85] dense_attention.MultiHeadDotProductAttention.kernel_init = \
I0512 23:44:42.409104 140143678494720 gin_utils.py:85]     @attention_kernel_init/linen.initializers.variance_scaling()
I0512 23:44:42.409136 140143678494720 gin_utils.py:85] dense_attention.MultiHeadDotProductAttention.num_heads = %NUM_HEADS
I0512 23:44:42.409168 140143678494720 gin_utils.py:85] dense_attention.MultiHeadDotProductAttention.use_bias = False
I0512 23:44:42.409201 140143678494720 gin_utils.py:85] dense_attention.MultiHeadDotProductAttention.use_rotary_embedding = True
I0512 23:44:42.409233 140143678494720 gin_utils.py:85] 
I0512 23:44:42.409265 140143678494720 gin_utils.py:85] # Parameters for bias_init/linen.initializers.normal:
I0512 23:44:42.409300 140143678494720 gin_utils.py:85] # ==============================================================================
I0512 23:44:42.409334 140143678494720 gin_utils.py:85] bias_init/linen.initializers.normal.stddev = 1e-06
I0512 23:44:42.409367 140143678494720 gin_utils.py:85] 
I0512 23:44:42.409399 140143678494720 gin_utils.py:85] # Parameters for router_init/linen.initializers.normal:
I0512 23:44:42.409431 140143678494720 gin_utils.py:85] # ==============================================================================
I0512 23:44:42.409463 140143678494720 gin_utils.py:85] router_init/linen.initializers.normal.stddev = 0.02
I0512 23:44:42.409495 140143678494720 gin_utils.py:85] 
I0512 23:44:42.409527 140143678494720 gin_utils.py:85] # Parameters for token_embedder_init/linen.initializers.normal:
I0512 23:44:42.409569 140143678494720 gin_utils.py:85] # ==============================================================================
I0512 23:44:42.409600 140143678494720 gin_utils.py:85] token_embedder_init/linen.initializers.normal.stddev = 1.0
I0512 23:44:42.409632 140143678494720 gin_utils.py:85] 
I0512 23:44:42.409665 140143678494720 gin_utils.py:85] # Parameters for partitioning.PjitPartitioner:
I0512 23:44:42.409697 140143678494720 gin_utils.py:85] # ==============================================================================
I0512 23:44:42.409729 140143678494720 gin_utils.py:85] partitioning.PjitPartitioner.logical_axis_rules = \
I0512 23:44:42.409762 140143678494720 gin_utils.py:85]     @partitioning.standard_logical_axis_rules()
I0512 23:44:42.409794 140143678494720 gin_utils.py:85] partitioning.PjitPartitioner.model_parallel_submesh = None
I0512 23:44:42.409826 140143678494720 gin_utils.py:85] partitioning.PjitPartitioner.num_partitions = 1
I0512 23:44:42.409858 140143678494720 gin_utils.py:85] 
I0512 23:44:42.409890 140143678494720 gin_utils.py:85] # Parameters for utils.RestoreCheckpointConfig:
I0512 23:44:42.409922 140143678494720 gin_utils.py:85] # ==============================================================================
I0512 23:44:42.409954 140143678494720 gin_utils.py:85] utils.RestoreCheckpointConfig.path = []
I0512 23:44:42.409986 140143678494720 gin_utils.py:85] 
I0512 23:44:42.410018 140143678494720 gin_utils.py:85] # Parameters for routing.RouterWeights:
I0512 23:44:42.410050 140143678494720 gin_utils.py:85] # ==============================================================================
I0512 23:44:42.410083 140143678494720 gin_utils.py:85] routing.RouterWeights.bias_init = %BIAS_INIT
I0512 23:44:42.410115 140143678494720 gin_utils.py:85] routing.RouterWeights.dtype = 'float32'
I0512 23:44:42.410147 140143678494720 gin_utils.py:85] routing.RouterWeights.kernel_init = @router_init/linen.initializers.normal()
I0512 23:44:42.410179 140143678494720 gin_utils.py:85] routing.RouterWeights.use_bias = False
I0512 23:44:42.410211 140143678494720 gin_utils.py:85] 
I0512 23:44:42.410243 140143678494720 gin_utils.py:85] # Parameters for utils.SaveCheckpointConfig:
I0512 23:44:42.410275 140143678494720 gin_utils.py:85] # ==============================================================================
I0512 23:44:42.410310 140143678494720 gin_utils.py:85] utils.SaveCheckpointConfig.dtype = 'float32'
I0512 23:44:42.410343 140143678494720 gin_utils.py:85] utils.SaveCheckpointConfig.keep = 20
I0512 23:44:42.410375 140143678494720 gin_utils.py:85] utils.SaveCheckpointConfig.period = 2500
I0512 23:44:42.410407 140143678494720 gin_utils.py:85] utils.SaveCheckpointConfig.save_dataset = False
I0512 23:44:42.410439 140143678494720 gin_utils.py:85] 
I0512 23:44:42.410471 140143678494720 gin_utils.py:85] # Parameters for seqio.SentencePieceVocabulary:
I0512 23:44:42.410503 140143678494720 gin_utils.py:85] # ==============================================================================
I0512 23:44:42.410542 140143678494720 gin_utils.py:85] seqio.SentencePieceVocabulary.extra_ids = 300
I0512 23:44:42.410575 140143678494720 gin_utils.py:85] seqio.SentencePieceVocabulary.sentencepiece_model_file = \
I0512 23:44:42.410608 140143678494720 gin_utils.py:85]     'gs://rosinality-tpu-bucket/sentencepiece.model'
I0512 23:44:42.410640 140143678494720 gin_utils.py:85] 
I0512 23:44:42.410672 140143678494720 gin_utils.py:85] # Parameters for moe_architecture.SparseDecoder:
I0512 23:44:42.410704 140143678494720 gin_utils.py:85] # ==============================================================================
I0512 23:44:42.410736 140143678494720 gin_utils.py:85] moe_architecture.SparseDecoder.dropout_factory = %DROPOUT_FACTORY
I0512 23:44:42.410769 140143678494720 gin_utils.py:85] moe_architecture.SparseDecoder.dtype = %ACTIVATION_DTYPE
I0512 23:44:42.410801 140143678494720 gin_utils.py:85] moe_architecture.SparseDecoder.layer_factory = @t5_architecture.DecoderLayer
I0512 23:44:42.410833 140143678494720 gin_utils.py:85] moe_architecture.SparseDecoder.layer_norm_factory = @layer_norm.T5LayerNorm
I0512 23:44:42.410865 140143678494720 gin_utils.py:85] moe_architecture.SparseDecoder.num_layers = %NUM_DECODER_LAYERS
I0512 23:44:42.410898 140143678494720 gin_utils.py:85] moe_architecture.SparseDecoder.num_sparse_layers = %NUM_DECODER_SPARSE_LAYERS
I0512 23:44:42.410930 140143678494720 gin_utils.py:85] moe_architecture.SparseDecoder.output_logits_factory = \
I0512 23:44:42.410962 140143678494720 gin_utils.py:85]     @output_logits/dense.DenseGeneral
I0512 23:44:42.410994 140143678494720 gin_utils.py:85] moe_architecture.SparseDecoder.sparse_layer_factory = \
I0512 23:44:42.411026 140143678494720 gin_utils.py:85]     @moe_architecture.SparseDecoderLayer
I0512 23:44:42.411058 140143678494720 gin_utils.py:85] moe_architecture.SparseDecoder.sparse_layout = %DECODER_SPARSE_LAYOUT
I0512 23:44:42.411090 140143678494720 gin_utils.py:85] 
I0512 23:44:42.411122 140143678494720 gin_utils.py:85] # Parameters for moe_architecture.SparseDecoderLayer:
I0512 23:44:42.411154 140143678494720 gin_utils.py:85] # ==============================================================================
I0512 23:44:42.411186 140143678494720 gin_utils.py:85] moe_architecture.SparseDecoderLayer.activation_partitioning_dims = \
I0512 23:44:42.411218 140143678494720 gin_utils.py:85]     %ACTIVATION_PARTITIONING_DIMS
I0512 23:44:42.411250 140143678494720 gin_utils.py:85] moe_architecture.SparseDecoderLayer.dropout_factory = %DROPOUT_FACTORY
I0512 23:44:42.411284 140143678494720 gin_utils.py:85] moe_architecture.SparseDecoderLayer.encoder_decoder_attention = None
I0512 23:44:42.411319 140143678494720 gin_utils.py:85] moe_architecture.SparseDecoderLayer.extra_mlp = @dense.MlpBlock()
I0512 23:44:42.411352 140143678494720 gin_utils.py:85] moe_architecture.SparseDecoderLayer.layer_norm_factory = @layer_norm.T5LayerNorm
I0512 23:44:42.411385 140143678494720 gin_utils.py:85] moe_architecture.SparseDecoderLayer.mlp = @sparse_decoder/moe_layers.MoeLayer()
I0512 23:44:42.411418 140143678494720 gin_utils.py:85] moe_architecture.SparseDecoderLayer.scanned = False
I0512 23:44:42.411450 140143678494720 gin_utils.py:85] moe_architecture.SparseDecoderLayer.self_attention = \
I0512 23:44:42.411482 140143678494720 gin_utils.py:85]     @dense_attention.MultiHeadDotProductAttention()
I0512 23:44:42.411515 140143678494720 gin_utils.py:85] 
I0512 23:44:42.411555 140143678494720 gin_utils.py:85] # Parameters for layer_norm.T5LayerNorm:
I0512 23:44:42.411588 140143678494720 gin_utils.py:85] # ==============================================================================
I0512 23:44:42.411620 140143678494720 gin_utils.py:85] layer_norm.T5LayerNorm.dtype = %ACTIVATION_DTYPE
I0512 23:44:42.411652 140143678494720 gin_utils.py:85] 
I0512 23:44:42.411684 140143678494720 gin_utils.py:85] # Parameters for routing.TokensChooseMaskedRouter:
I0512 23:44:42.411716 140143678494720 gin_utils.py:85] # ==============================================================================
I0512 23:44:42.411749 140143678494720 gin_utils.py:85] routing.TokensChooseMaskedRouter.dtype = 'float32'
I0512 23:44:42.411781 140143678494720 gin_utils.py:85] routing.TokensChooseMaskedRouter.ignore_padding_tokens = False
I0512 23:44:42.411813 140143678494720 gin_utils.py:85] routing.TokensChooseMaskedRouter.jitter_noise = %JITTER_NOISE
I0512 23:44:42.411845 140143678494720 gin_utils.py:85] routing.TokensChooseMaskedRouter.num_selected_experts = %NUM_SELECTED_EXPERTS
I0512 23:44:42.411877 140143678494720 gin_utils.py:85] routing.TokensChooseMaskedRouter.router_weights = @routing.RouterWeights()
I0512 23:44:42.411909 140143678494720 gin_utils.py:85] 
I0512 23:44:42.411940 140143678494720 gin_utils.py:85] # Parameters for sparse_decoder/routing.TokensChooseMaskedRouter:
I0512 23:44:42.411972 140143678494720 gin_utils.py:85] # ==============================================================================
I0512 23:44:42.412004 140143678494720 gin_utils.py:85] sparse_decoder/routing.TokensChooseMaskedRouter.batch_prioritized_routing = False
I0512 23:44:42.412036 140143678494720 gin_utils.py:85] 
I0512 23:44:42.412068 140143678494720 gin_utils.py:85] # Parameters for sparse_encoder/routing.TokensChooseMaskedRouter:
I0512 23:44:42.412101 140143678494720 gin_utils.py:85] # ==============================================================================
I0512 23:44:42.412132 140143678494720 gin_utils.py:85] sparse_encoder/routing.TokensChooseMaskedRouter.batch_prioritized_routing = False
I0512 23:44:42.412165 140143678494720 gin_utils.py:85] 
I0512 23:44:42.412198 140143678494720 gin_utils.py:85] # Parameters for train_script.train:
I0512 23:44:42.412231 140143678494720 gin_utils.py:85] # ==============================================================================
I0512 23:44:42.412263 140143678494720 gin_utils.py:85] train_script.train.checkpoint_cfg = @utils.CheckpointConfig()
I0512 23:44:42.604650 140180528838656 resource_reader.py:50] system_path_file_exists:flaxformer/flaxformer/t5x/configs/moe/models/st_moe_decoder_only_large.gin
E0512 23:44:42.605131 140180528838656 resource_reader.py:55] Path not found: flaxformer/flaxformer/t5x/configs/moe/models/st_moe_decoder_only_large.gin
I0512 23:44:42.605196 140180528838656 resource_reader.py:50] system_path_file_exists:./flaxformer/flaxformer/t5x/configs/moe/models/st_moe_decoder_only_large.gin
E0512 23:44:42.605245 140180528838656 resource_reader.py:55] Path not found: ./flaxformer/flaxformer/t5x/configs/moe/models/st_moe_decoder_only_large.gin
I0512 23:44:42.532152 140643590531072 resource_reader.py:37] gin-config opened resource file:/home/rosinality/t5x/t5x/contrib/moe/configs/runs/pretrain.gin
I0512 23:44:42.532661 140643590531072 resource_reader.py:50] system_path_file_exists:t5x/configs/runs/pretrain.gin
I0512 23:44:42.532982 140643590531072 resource_reader.py:37] gin-config opened resource file:/home/rosinality/t5x/t5x/configs/runs/pretrain.gin
I0512 23:44:42.546915 140643590531072 gin_utils.py:83] Gin Configuration:
I0512 23:44:42.562772 140643590531072 gin_utils.py:85] from __gin__ import dynamic_registration
I0512 23:44:42.562841 140643590531072 gin_utils.py:85] import __main__ as train_script
I0512 23:44:42.562882 140643590531072 gin_utils.py:85] from flax import linen
I0512 23:44:42.562917 140643590531072 gin_utils.py:85] import flaxformer
I0512 23:44:42.562952 140643590531072 gin_utils.py:85] from flaxformer.architectures.moe import moe_architecture
I0512 23:44:42.562987 140643590531072 gin_utils.py:85] from flaxformer.architectures.moe import moe_enums
I0512 23:44:42.563020 140643590531072 gin_utils.py:85] from flaxformer.architectures.moe import moe_layers
I0512 23:44:42.563051 140643590531072 gin_utils.py:85] from flaxformer.architectures.moe import routing
I0512 23:44:42.563083 140643590531072 gin_utils.py:85] from flaxformer.architectures.t5 import t5_architecture
I0512 23:44:42.563115 140643590531072 gin_utils.py:85] from flaxformer.components.attention import dense_attention
I0512 23:44:42.563146 140643590531072 gin_utils.py:85] from flaxformer.components.attention import memory_efficient_attention
I0512 23:44:42.563178 140643590531072 gin_utils.py:85] from flaxformer.components import dense
I0512 23:44:42.563210 140643590531072 gin_utils.py:85] from flaxformer.components import embedding
I0512 23:44:42.563241 140643590531072 gin_utils.py:85] from flaxformer.components import layer_norm
I0512 23:44:42.563272 140643590531072 gin_utils.py:85] from gin import config
I0512 23:44:42.563303 140643590531072 gin_utils.py:85] import seqio
I0512 23:44:42.563335 140643590531072 gin_utils.py:85] import t5.data.mixtures
I0512 23:44:42.563366 140643590531072 gin_utils.py:85] from t5x import adafactor
I0512 23:44:42.563398 140643590531072 gin_utils.py:85] from t5x.contrib.moe import adafactor_utils
I0512 23:44:42.563429 140643590531072 gin_utils.py:85] from t5x.contrib.moe import models
I0512 23:44:42.563461 140643590531072 gin_utils.py:85] from t5x.contrib.moe import partitioning as moe_partitioning
I0512 23:44:42.563492 140643590531072 gin_utils.py:85] from t5x.contrib.moe import trainer as moe_trainer
I0512 23:44:42.563524 140643590531072 gin_utils.py:85] from t5x import gin_utils
I0512 23:44:42.563555 140643590531072 gin_utils.py:85] from t5x import partitioning
I0512 23:44:42.563613 140643590531072 gin_utils.py:85] from t5x import trainer
I0512 23:44:42.563647 140643590531072 gin_utils.py:85] from t5x import utils
I0512 23:44:42.563678 140643590531072 gin_utils.py:85] 
I0512 23:44:42.563716 140643590531072 gin_utils.py:85] # Macros:
I0512 23:44:42.563749 140643590531072 gin_utils.py:85] # ==============================================================================
I0512 23:44:42.563780 140643590531072 gin_utils.py:85] ACTIVATION_DTYPE = 'bfloat16'
I0512 23:44:42.563812 140643590531072 gin_utils.py:85] ACTIVATION_PARTITIONING_DIMS = 1
I0512 23:44:42.563843 140643590531072 gin_utils.py:85] ARCHITECTURE = @t5_architecture.DecoderOnly()
I0512 23:44:42.563875 140643590531072 gin_utils.py:85] AUX_LOSS_FACTOR = 0.01
I0512 23:44:42.563906 140643590531072 gin_utils.py:85] BATCH_SIZE = 384
I0512 23:44:42.563938 140643590531072 gin_utils.py:85] BIAS_INIT = @bias_init/linen.initializers.normal()
I0512 23:44:42.563971 140643590531072 gin_utils.py:85] DECODER_SPARSE_LAYOUT = %flaxformer.architectures.moe.moe_enums.LayerLayout.MIXED
I0512 23:44:42.564003 140643590531072 gin_utils.py:85] DROPOUT_FACTORY = @dropout_factory/linen.Dropout
I0512 23:44:42.564035 140643590531072 gin_utils.py:85] DROPOUT_RATE = 0.0
I0512 23:44:42.564066 140643590531072 gin_utils.py:85] EMBED_DIM = 2048
I0512 23:44:42.564097 140643590531072 gin_utils.py:85] EVAL_EXPERT_CAPACITY_FACTOR = 2.0
I0512 23:44:42.564129 140643590531072 gin_utils.py:85] EXPERT_DROPOUT_RATE = %DROPOUT_RATE
I0512 23:44:42.564160 140643590531072 gin_utils.py:85] EXPERT_MLP_DIM = %MLP_DIM
I0512 23:44:42.564192 140643590531072 gin_utils.py:85] GROUP_SIZE = 4096
I0512 23:44:42.564223 140643590531072 gin_utils.py:85] HEAD_DIM = 128
I0512 23:44:42.564254 140643590531072 gin_utils.py:85] JITTER_NOISE = 0.0
I0512 23:44:42.564285 140643590531072 gin_utils.py:85] LABEL_SMOOTHING = 0.0
I0512 23:44:42.564316 140643590531072 gin_utils.py:85] LOSS_NORMALIZING_FACTOR = 'NUM_REAL_TARGET_TOKENS'
I0512 23:44:42.564347 140643590531072 gin_utils.py:85] MIXTURE_OR_TASK_MODULE = None
I0512 23:44:42.564379 140643590531072 gin_utils.py:85] MIXTURE_OR_TASK_NAME = 'mix_ul2_test'
I0512 23:44:42.564410 140643590531072 gin_utils.py:85] MLP_DIM = 8192
I0512 23:44:42.564441 140643590531072 gin_utils.py:85] MODEL = @models.MoeDecoderOnlyModel()
I0512 23:44:42.564472 140643590531072 gin_utils.py:85] MODEL_DIR = 'gs://rosinality-tpu-bucket/openmoe_8b_ul2/training'
I0512 23:44:42.564504 140643590531072 gin_utils.py:85] MODEL_PARALLEL_SUBMESH = None
I0512 23:44:42.564535 140643590531072 gin_utils.py:85] MOE_TRUNCATED_DTYPE = 'bfloat16'
I0512 23:44:42.564567 140643590531072 gin_utils.py:85] NUM_DECODER_LAYERS = 24
I0512 23:44:42.564598 140643590531072 gin_utils.py:85] NUM_DECODER_SPARSE_LAYERS = 4
I0512 23:44:42.564629 140643590531072 gin_utils.py:85] NUM_EMBEDDINGS = 256384
I0512 23:44:42.564660 140643590531072 gin_utils.py:85] NUM_EXPERT_PARTITIONS = 8
I0512 23:44:42.564697 140643590531072 gin_utils.py:85] NUM_EXPERTS = 8
I0512 23:44:42.564730 140643590531072 gin_utils.py:85] NUM_HEADS = 24
I0512 23:44:42.564761 140643590531072 gin_utils.py:85] NUM_MODEL_PARTITIONS = 4
I0512 23:44:42.564793 140643590531072 gin_utils.py:85] NUM_SELECTED_EXPERTS = 2
I0512 23:44:42.564824 140643590531072 gin_utils.py:85] OPTIMIZER = @adafactor.Adafactor()
I0512 23:44:42.564855 140643590531072 gin_utils.py:85] RANDOM_SEED = None
I0512 23:44:42.564886 140643590531072 gin_utils.py:85] ROUTER_Z_LOSS_FACTOR = 0.0001
I0512 23:44:42.564917 140643590531072 gin_utils.py:85] SCALE = 0.1
I0512 23:44:42.564950 140643590531072 gin_utils.py:85] SHUFFLE_TRAIN_EXAMPLES = True
I0512 23:44:42.564983 140643590531072 gin_utils.py:85] TASK_FEATURE_LENGTHS = {'inputs': 1024, 'targets': 1024}
I0512 23:44:42.565015 140643590531072 gin_utils.py:85] TRAIN_EXPERT_CAPACITY_FACTOR = 1.25
I0512 23:44:42.565047 140643590531072 gin_utils.py:85] TRAIN_STEPS = 500000
I0512 23:44:42.565078 140643590531072 gin_utils.py:85] USE_CACHED_TASKS = False
I0512 23:44:42.565109 140643590531072 gin_utils.py:85] USE_HARDWARE_RNG = False
I0512 23:44:42.565140 140643590531072 gin_utils.py:85] VOCABULARY = @seqio.SentencePieceVocabulary()
I0512 23:44:42.565171 140643590531072 gin_utils.py:85] Z_LOSS = 0.0001
I0512 23:44:42.565203 140643590531072 gin_utils.py:85] 
I0512 23:44:42.565234 140643590531072 gin_utils.py:85] # Parameters for adafactor.Adafactor:
I0512 23:44:42.565266 140643590531072 gin_utils.py:85] # ==============================================================================
I0512 23:44:42.565298 140643590531072 gin_utils.py:85] adafactor.Adafactor.decay_rate = 0.8
I0512 23:44:42.565329 140643590531072 gin_utils.py:85] adafactor.Adafactor.logical_factor_rules = @adafactor_utils.logical_factor_rules()
I0512 23:44:42.565360 140643590531072 gin_utils.py:85] adafactor.Adafactor.step_offset = 0
I0512 23:44:42.565392 140643590531072 gin_utils.py:85] 
I0512 23:44:42.388375 139876724979712 gin_utils.py:85] train_script.train.eval_period = 2500
I0512 23:44:42.388406 139876724979712 gin_utils.py:85] train_script.train.eval_steps = 20
I0512 23:44:42.388437 139876724979712 gin_utils.py:85] train_script.train.infer_eval_dataset_cfg = None
I0512 23:44:42.388469 139876724979712 gin_utils.py:85] train_script.train.model = %MODEL
I0512 23:44:42.388502 139876724979712 gin_utils.py:85] train_script.train.model_dir = %MODEL_DIR
I0512 23:44:42.388533 139876724979712 gin_utils.py:85] train_script.train.partitioner = @moe_partitioning.MoePjitPartitioner()
I0512 23:44:42.388564 139876724979712 gin_utils.py:85] train_script.train.random_seed = %RANDOM_SEED
I0512 23:44:42.388596 139876724979712 gin_utils.py:85] train_script.train.stats_period = 10
I0512 23:44:42.388627 139876724979712 gin_utils.py:85] train_script.train.summarize_config_fn = @gin_utils.summarize_gin_config
I0512 23:44:42.388658 139876724979712 gin_utils.py:85] train_script.train.total_steps = %TRAIN_STEPS
I0512 23:44:42.388689 139876724979712 gin_utils.py:85] train_script.train.train_dataset_cfg = @train/utils.DatasetConfig()
I0512 23:44:42.388720 139876724979712 gin_utils.py:85] train_script.train.train_eval_dataset_cfg = None
I0512 23:44:42.388751 139876724979712 gin_utils.py:85] train_script.train.trainer_cls = @moe_trainer.MoeTrainer
I0512 23:44:42.388782 139876724979712 gin_utils.py:85] train_script.train.use_hardware_rng = %USE_HARDWARE_RNG
I0512 23:44:42.388813 139876724979712 gin_utils.py:85] 
I0512 23:44:42.388844 139876724979712 gin_utils.py:85] # Parameters for trainer.Trainer:
I0512 23:44:42.388875 139876724979712 gin_utils.py:85] # ==============================================================================
I0512 23:44:42.388906 139876724979712 gin_utils.py:85] trainer.Trainer.learning_rate_fn = @utils.create_learning_rate_scheduler()
I0512 23:44:42.388937 139876724979712 gin_utils.py:85] trainer.Trainer.num_microbatches = None
I0512 23:44:42.388968 139876724979712 gin_utils.py:85] 
I0512 23:44:42.388999 139876724979712 gin_utils.py:85] # Parameters for attention_kernel_init/linen.initializers.variance_scaling:
I0512 23:44:42.389031 139876724979712 gin_utils.py:85] # ==============================================================================
I0512 23:44:42.389062 139876724979712 gin_utils.py:85] attention_kernel_init/linen.initializers.variance_scaling.distribution = 'normal'
I0512 23:44:42.389093 139876724979712 gin_utils.py:85] attention_kernel_init/linen.initializers.variance_scaling.mode = 'fan_in'
I0512 23:44:42.389123 139876724979712 gin_utils.py:85] attention_kernel_init/linen.initializers.variance_scaling.scale = %SCALE
I0512 23:44:42.389154 139876724979712 gin_utils.py:85] 
I0512 23:44:42.389185 139876724979712 gin_utils.py:85] # Parameters for expert_kernel_init/linen.initializers.variance_scaling:
I0512 23:44:42.389222 139876724979712 gin_utils.py:85] # ==============================================================================
I0512 23:44:42.389254 139876724979712 gin_utils.py:85] expert_kernel_init/linen.initializers.variance_scaling.distribution = 'normal'
I0512 23:44:42.389285 139876724979712 gin_utils.py:85] expert_kernel_init/linen.initializers.variance_scaling.mode = 'fan_in'
I0512 23:44:42.389316 139876724979712 gin_utils.py:85] expert_kernel_init/linen.initializers.variance_scaling.scale = %SCALE
I0512 23:44:42.389347 139876724979712 gin_utils.py:85] 
I0512 23:44:42.389379 139876724979712 gin_utils.py:85] # Parameters for mlp_kernel_init/linen.initializers.variance_scaling:
I0512 23:44:42.389410 139876724979712 gin_utils.py:85] # ==============================================================================
I0512 23:44:42.389441 139876724979712 gin_utils.py:85] mlp_kernel_init/linen.initializers.variance_scaling.distribution = \
I0512 23:44:42.389473 139876724979712 gin_utils.py:85]     'truncated_normal'
I0512 23:44:42.389506 139876724979712 gin_utils.py:85] mlp_kernel_init/linen.initializers.variance_scaling.mode = 'fan_in'
I0512 23:44:42.389537 139876724979712 gin_utils.py:85] mlp_kernel_init/linen.initializers.variance_scaling.scale = %SCALE
I0512 23:44:42.389568 139876724979712 gin_utils.py:85] 
I0512 23:44:42.389599 139876724979712 gin_utils.py:85] # Parameters for output_logits_kernel_init/linen.initializers.variance_scaling:
I0512 23:44:42.389630 139876724979712 gin_utils.py:85] # ==============================================================================
I0512 23:44:42.389662 139876724979712 gin_utils.py:85] output_logits_kernel_init/linen.initializers.variance_scaling.distribution = \
I0512 23:44:42.389693 139876724979712 gin_utils.py:85]     'truncated_normal'
I0512 23:44:42.389724 139876724979712 gin_utils.py:85] output_logits_kernel_init/linen.initializers.variance_scaling.mode = 'fan_in'
I0512 23:44:42.389755 139876724979712 gin_utils.py:85] output_logits_kernel_init/linen.initializers.variance_scaling.scale = %SCALE
I0512 23:44:42.390673 139876724979712 partitioning.py:559] `activation_partitioning_dims` = 2, `parameter_partitioning_dims` = 1
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
I0000 00:00:1715557482.477468  368289 pjrt_api.cc:98] GetPjrtApi was found for tpu at /home/rosinality/openmoe_venv/lib/python3.10/site-packages/libtpu/libtpu.so
I0000 00:00:1715557482.477518  368289 pjrt_api.cc:67] PJRT_Api is set for device type tpu
I0000 00:00:1715557482.477521  368289 pjrt_api.cc:72] PJRT plugin for tpu has PJRT API version 0.30. The framework PJRT API version is 0.30.
I0512 23:44:42.443079 139856202532864 gin_utils.py:85] expert/dense.MlpBlock.use_bias = False
I0512 23:44:42.443111 139856202532864 gin_utils.py:85] 
I0512 23:44:42.443143 139856202532864 gin_utils.py:85] # Parameters for models.MoeDecoderOnlyModel:
I0512 23:44:42.443175 139856202532864 gin_utils.py:85] # ==============================================================================
I0512 23:44:42.443206 139856202532864 gin_utils.py:85] models.MoeDecoderOnlyModel.aux_loss_factor = %AUX_LOSS_FACTOR
I0512 23:44:42.443238 139856202532864 gin_utils.py:85] models.MoeDecoderOnlyModel.inputs_bidirectional_attention = True
I0512 23:44:42.443269 139856202532864 gin_utils.py:85] models.MoeDecoderOnlyModel.label_smoothing = %LABEL_SMOOTHING
I0512 23:44:42.443301 139856202532864 gin_utils.py:85] models.MoeDecoderOnlyModel.loss_normalizing_factor = %LOSS_NORMALIZING_FACTOR
I0512 23:44:42.443332 139856202532864 gin_utils.py:85] models.MoeDecoderOnlyModel.module = %ARCHITECTURE
I0512 23:44:42.443364 139856202532864 gin_utils.py:85] models.MoeDecoderOnlyModel.optimizer_def = %OPTIMIZER
I0512 23:44:42.443396 139856202532864 gin_utils.py:85] models.MoeDecoderOnlyModel.router_z_loss_factor = %ROUTER_Z_LOSS_FACTOR
I0512 23:44:42.443428 139856202532864 gin_utils.py:85] models.MoeDecoderOnlyModel.vocabulary = %VOCABULARY
I0512 23:44:42.443459 139856202532864 gin_utils.py:85] models.MoeDecoderOnlyModel.z_loss = %Z_LOSS
I0512 23:44:42.443491 139856202532864 gin_utils.py:85] 
I0512 23:44:42.443523 139856202532864 gin_utils.py:85] # Parameters for moe_layers.MoeLayer:
I0512 23:44:42.443554 139856202532864 gin_utils.py:85] # ==============================================================================
I0512 23:44:42.443586 139856202532864 gin_utils.py:85] moe_layers.MoeLayer.dtype = %MOE_TRUNCATED_DTYPE
I0512 23:44:42.443620 139856202532864 gin_utils.py:85] moe_layers.MoeLayer.eval_capacity_factor = %EVAL_EXPERT_CAPACITY_FACTOR
I0512 23:44:42.443652 139856202532864 gin_utils.py:85] moe_layers.MoeLayer.expert = @expert/dense.MlpBlock()
I0512 23:44:42.443684 139856202532864 gin_utils.py:85] moe_layers.MoeLayer.max_group_size = %GROUP_SIZE
I0512 23:44:42.443715 139856202532864 gin_utils.py:85] moe_layers.MoeLayer.num_expert_partitions = %NUM_EXPERT_PARTITIONS
I0512 23:44:42.443747 139856202532864 gin_utils.py:85] moe_layers.MoeLayer.num_experts = %NUM_EXPERTS
I0512 23:44:42.443778 139856202532864 gin_utils.py:85] moe_layers.MoeLayer.num_model_partitions = \
I0512 23:44:42.443810 139856202532864 gin_utils.py:85]     @moe_partitioning.compute_num_model_partitions()
I0512 23:44:42.443846 139856202532864 gin_utils.py:85] moe_layers.MoeLayer.train_capacity_factor = %TRAIN_EXPERT_CAPACITY_FACTOR
I0512 23:44:42.443879 139856202532864 gin_utils.py:85] 
I0512 23:44:42.443911 139856202532864 gin_utils.py:85] # Parameters for sparse_decoder/moe_layers.MoeLayer:
I0512 23:44:42.443943 139856202532864 gin_utils.py:85] # ==============================================================================
I0512 23:44:42.443974 139856202532864 gin_utils.py:85] sparse_decoder/moe_layers.MoeLayer.router = \
I0512 23:44:42.444005 139856202532864 gin_utils.py:85]     @sparse_decoder/routing.TokensChooseMaskedRouter()
I0512 23:44:42.444037 139856202532864 gin_utils.py:85] 
I0512 23:44:42.444068 139856202532864 gin_utils.py:85] # Parameters for moe_partitioning.MoePjitPartitioner:
I0512 23:44:42.444099 139856202532864 gin_utils.py:85] # ==============================================================================
I0512 23:44:42.444131 139856202532864 gin_utils.py:85] moe_partitioning.MoePjitPartitioner.model_parallel_submesh = \
I0512 23:44:42.444163 139856202532864 gin_utils.py:85]     %MODEL_PARALLEL_SUBMESH
I0512 23:44:42.444194 139856202532864 gin_utils.py:85] moe_partitioning.MoePjitPartitioner.num_expert_partitions = %NUM_EXPERT_PARTITIONS
I0512 23:44:42.444226 139856202532864 gin_utils.py:85] moe_partitioning.MoePjitPartitioner.num_partitions = %NUM_MODEL_PARTITIONS
I0512 23:44:42.444257 139856202532864 gin_utils.py:85] 
I0512 23:44:42.444289 139856202532864 gin_utils.py:85] # Parameters for moe_trainer.MoeTrainer:
I0512 23:44:42.444320 139856202532864 gin_utils.py:85] # ==============================================================================
I0512 23:44:42.444352 139856202532864 gin_utils.py:85] moe_trainer.MoeTrainer.learning_rate_fn = @utils.create_learning_rate_scheduler()
I0512 23:44:42.444383 139856202532864 gin_utils.py:85] moe_trainer.MoeTrainer.num_expert_partitions = %NUM_EXPERT_PARTITIONS
I0512 23:44:42.444415 139856202532864 gin_utils.py:85] moe_trainer.MoeTrainer.num_microbatches = 8
I0512 23:44:42.444447 139856202532864 gin_utils.py:85] 
I0512 23:44:42.444478 139856202532864 gin_utils.py:85] # Parameters for dense_attention.MultiHeadDotProductAttention:
I0512 23:44:42.444510 139856202532864 gin_utils.py:85] # ==============================================================================
I0512 23:44:42.444541 139856202532864 gin_utils.py:85] dense_attention.MultiHeadDotProductAttention.bias_init = %BIAS_INIT
I0512 23:44:42.444573 139856202532864 gin_utils.py:85] dense_attention.MultiHeadDotProductAttention.broadcast_dropout = True
I0512 23:44:42.444607 139856202532864 gin_utils.py:85] dense_attention.MultiHeadDotProductAttention.dropout_rate = %DROPOUT_RATE
I0512 23:44:42.444640 139856202532864 gin_utils.py:85] dense_attention.MultiHeadDotProductAttention.dtype = %ACTIVATION_DTYPE
I0512 23:44:42.444672 139856202532864 gin_utils.py:85] dense_attention.MultiHeadDotProductAttention.head_dim = %HEAD_DIM
I0512 23:44:42.444704 139856202532864 gin_utils.py:85] dense_attention.MultiHeadDotProductAttention.kernel_init = \
I0512 23:44:42.444736 139856202532864 gin_utils.py:85]     @attention_kernel_init/linen.initializers.variance_scaling()
I0512 23:44:42.444768 139856202532864 gin_utils.py:85] dense_attention.MultiHeadDotProductAttention.num_heads = %NUM_HEADS
I0512 23:44:42.444799 139856202532864 gin_utils.py:85] dense_attention.MultiHeadDotProductAttention.use_bias = False
I0512 23:44:42.444831 139856202532864 gin_utils.py:85] dense_attention.MultiHeadDotProductAttention.use_rotary_embedding = True
I0512 23:44:42.444869 139856202532864 gin_utils.py:85] 
I0512 23:44:42.444901 139856202532864 gin_utils.py:85] # Parameters for bias_init/linen.initializers.normal:
I0512 23:44:42.444933 139856202532864 gin_utils.py:85] # ==============================================================================
I0512 23:44:42.444964 139856202532864 gin_utils.py:85] bias_init/linen.initializers.normal.stddev = 1e-06
I0512 23:44:42.444997 139856202532864 gin_utils.py:85] 
I0512 23:44:42.445028 139856202532864 gin_utils.py:85] # Parameters for router_init/linen.initializers.normal:
I0512 23:44:42.445060 139856202532864 gin_utils.py:85] # ==============================================================================
I0512 23:44:42.445091 139856202532864 gin_utils.py:85] router_init/linen.initializers.normal.stddev = 0.02
I0512 23:44:42.445123 139856202532864 gin_utils.py:85] 
I0512 23:44:42.445154 139856202532864 gin_utils.py:85] # Parameters for token_embedder_init/linen.initializers.normal:
I0512 23:44:42.445186 139856202532864 gin_utils.py:85] # ==============================================================================
I0512 23:44:42.445218 139856202532864 gin_utils.py:85] token_embedder_init/linen.initializers.normal.stddev = 1.0
I0512 23:44:42.445250 139856202532864 gin_utils.py:85] 
I0512 23:44:42.445281 139856202532864 gin_utils.py:85] # Parameters for partitioning.PjitPartitioner:
I0512 23:44:42.445313 139856202532864 gin_utils.py:85] # ==============================================================================
I0512 23:44:42.445344 139856202532864 gin_utils.py:85] partitioning.PjitPartitioner.logical_axis_rules = \
I0512 23:44:42.445376 139856202532864 gin_utils.py:85]     @partitioning.standard_logical_axis_rules()
I0512 23:44:42.445408 139856202532864 gin_utils.py:85] partitioning.PjitPartitioner.model_parallel_submesh = None
I0512 23:44:42.445440 139856202532864 gin_utils.py:85] partitioning.PjitPartitioner.num_partitions = 1
I0512 23:44:42.445471 139856202532864 gin_utils.py:85] 
I0512 23:44:42.445502 139856202532864 gin_utils.py:85] # Parameters for utils.RestoreCheckpointConfig:
I0512 23:44:42.445534 139856202532864 gin_utils.py:85] # ==============================================================================
I0512 23:44:42.445565 139856202532864 gin_utils.py:85] utils.RestoreCheckpointConfig.path = []
I0512 23:44:42.445597 139856202532864 gin_utils.py:85] 
I0512 23:44:42.445631 139856202532864 gin_utils.py:85] # Parameters for routing.RouterWeights:
I0512 23:44:42.445662 139856202532864 gin_utils.py:85] # ==============================================================================
I0512 23:44:42.445694 139856202532864 gin_utils.py:85] routing.RouterWeights.bias_init = %BIAS_INIT
I0512 23:44:42.445749 139856202532864 gin_utils.py:85] routing.RouterWeights.dtype = 'float32'
I0512 23:44:42.445783 139856202532864 gin_utils.py:85] routing.RouterWeights.kernel_init = @router_init/linen.initializers.normal()
I0512 23:44:42.445815 139856202532864 gin_utils.py:85] routing.RouterWeights.use_bias = False
I0512 23:44:42.445853 139856202532864 gin_utils.py:85] 
I0512 23:44:42.445886 139856202532864 gin_utils.py:85] # Parameters for utils.SaveCheckpointConfig:
I0512 23:44:42.445918 139856202532864 gin_utils.py:85] # ==============================================================================
I0512 23:44:42.445950 139856202532864 gin_utils.py:85] utils.SaveCheckpointConfig.dtype = 'float32'
I0512 23:44:42.445981 139856202532864 gin_utils.py:85] utils.SaveCheckpointConfig.keep = 20
I0512 23:44:42.446013 139856202532864 gin_utils.py:85] utils.SaveCheckpointConfig.period = 2500
I0512 23:44:42.446044 139856202532864 gin_utils.py:85] utils.SaveCheckpointConfig.save_dataset = False
I0512 23:44:42.446076 139856202532864 gin_utils.py:85] 
I0512 23:44:42.446107 139856202532864 gin_utils.py:85] # Parameters for seqio.SentencePieceVocabulary:
I0512 23:44:42.446139 139856202532864 gin_utils.py:85] # ==============================================================================
I0512 23:44:42.446170 139856202532864 gin_utils.py:85] seqio.SentencePieceVocabulary.extra_ids = 300
I0512 23:44:42.446202 139856202532864 gin_utils.py:85] seqio.SentencePieceVocabulary.sentencepiece_model_file = \
I0512 23:44:42.446234 139856202532864 gin_utils.py:85]     'gs://rosinality-tpu-bucket/sentencepiece.model'
I0512 23:44:42.446265 139856202532864 gin_utils.py:85] 
I0512 23:44:42.446297 139856202532864 gin_utils.py:85] # Parameters for moe_architecture.SparseDecoder:
I0512 23:44:42.446328 139856202532864 gin_utils.py:85] # ==============================================================================
I0512 23:44:42.446360 139856202532864 gin_utils.py:85] moe_architecture.SparseDecoder.dropout_factory = %DROPOUT_FACTORY
I0512 23:44:42.446391 139856202532864 gin_utils.py:85] moe_architecture.SparseDecoder.dtype = %ACTIVATION_DTYPE
I0512 23:44:42.446423 139856202532864 gin_utils.py:85] moe_architecture.SparseDecoder.layer_factory = @t5_architecture.DecoderLayer
I0512 23:44:42.446454 139856202532864 gin_utils.py:85] moe_architecture.SparseDecoder.layer_norm_factory = @layer_norm.T5LayerNorm
I0512 23:44:42.446486 139856202532864 gin_utils.py:85] moe_architecture.SparseDecoder.num_layers = %NUM_DECODER_LAYERS
I0512 23:44:42.446517 139856202532864 gin_utils.py:85] moe_architecture.SparseDecoder.num_sparse_layers = %NUM_DECODER_SPARSE_LAYERS
I0512 23:44:42.446549 139856202532864 gin_utils.py:85] moe_architecture.SparseDecoder.output_logits_factory = \
I0512 23:44:42.446580 139856202532864 gin_utils.py:85]     @output_logits/dense.DenseGeneral
I0512 23:44:42.446614 139856202532864 gin_utils.py:85] moe_architecture.SparseDecoder.sparse_layer_factory = \
I0512 23:44:42.446647 139856202532864 gin_utils.py:85]     @moe_architecture.SparseDecoderLayer
I0512 23:44:42.446679 139856202532864 gin_utils.py:85] moe_architecture.SparseDecoder.sparse_layout = %DECODER_SPARSE_LAYOUT
I0512 23:44:42.446711 139856202532864 gin_utils.py:85] 
I0512 23:44:42.446743 139856202532864 gin_utils.py:85] # Parameters for moe_architecture.SparseDecoderLayer:
I0512 23:44:42.446774 139856202532864 gin_utils.py:85] # ==============================================================================
I0512 23:44:42.446806 139856202532864 gin_utils.py:85] moe_architecture.SparseDecoderLayer.activation_partitioning_dims = \
I0512 23:44:42.446842 139856202532864 gin_utils.py:85]     %ACTIVATION_PARTITIONING_DIMS
I0512 23:44:42.446876 139856202532864 gin_utils.py:85] moe_architecture.SparseDecoderLayer.dropout_factory = %DROPOUT_FACTORY
I0512 23:44:42.446909 139856202532864 gin_utils.py:85] moe_architecture.SparseDecoderLayer.encoder_decoder_attention = None
I0512 23:44:42.446941 139856202532864 gin_utils.py:85] moe_architecture.SparseDecoderLayer.extra_mlp = @dense.MlpBlock()
I0512 23:44:42.446972 139856202532864 gin_utils.py:85] moe_architecture.SparseDecoderLayer.layer_norm_factory = @layer_norm.T5LayerNorm
I0512 23:44:42.447004 139856202532864 gin_utils.py:85] moe_architecture.SparseDecoderLayer.mlp = @sparse_decoder/moe_layers.MoeLayer()
I0512 23:44:42.447036 139856202532864 gin_utils.py:85] moe_architecture.SparseDecoderLayer.scanned = False
I0512 23:44:42.447067 139856202532864 gin_utils.py:85] moe_architecture.SparseDecoderLayer.self_attention = \
I0512 23:44:42.447100 139856202532864 gin_utils.py:85]     @dense_attention.MultiHeadDotProductAttention()
I0512 23:44:42.447135 139856202532864 gin_utils.py:85] 
I0512 23:44:42.447166 139856202532864 gin_utils.py:85] # Parameters for layer_norm.T5LayerNorm:
I0512 23:44:42.447197 139856202532864 gin_utils.py:85] # ==============================================================================
I0512 23:44:42.447228 139856202532864 gin_utils.py:85] layer_norm.T5LayerNorm.dtype = %ACTIVATION_DTYPE
I0512 23:44:42.447260 139856202532864 gin_utils.py:85] 
I0512 23:44:42.447291 139856202532864 gin_utils.py:85] # Parameters for routing.TokensChooseMaskedRouter:
I0512 23:44:42.447323 139856202532864 gin_utils.py:85] # ==============================================================================
I0512 23:44:42.447354 139856202532864 gin_utils.py:85] routing.TokensChooseMaskedRouter.dtype = 'float32'
I0512 23:44:42.447386 139856202532864 gin_utils.py:85] routing.TokensChooseMaskedRouter.ignore_padding_tokens = False
I0512 23:44:42.447417 139856202532864 gin_utils.py:85] routing.TokensChooseMaskedRouter.jitter_noise = %JITTER_NOISE
I0512 23:44:42.447448 139856202532864 gin_utils.py:85] routing.TokensChooseMaskedRouter.num_selected_experts = %NUM_SELECTED_EXPERTS
I0512 23:44:42.447480 139856202532864 gin_utils.py:85] routing.TokensChooseMaskedRouter.router_weights = @routing.RouterWeights()
I0512 23:44:42.447512 139856202532864 gin_utils.py:85] 
I0512 23:44:42.447543 139856202532864 gin_utils.py:85] # Parameters for sparse_decoder/routing.TokensChooseMaskedRouter:
I0512 23:44:42.447575 139856202532864 gin_utils.py:85] # ==============================================================================
I0512 23:44:42.447608 139856202532864 gin_utils.py:85] sparse_decoder/routing.TokensChooseMaskedRouter.batch_prioritized_routing = False
I0512 23:44:42.447640 139856202532864 gin_utils.py:85] 
I0512 23:44:42.447672 139856202532864 gin_utils.py:85] # Parameters for sparse_encoder/routing.TokensChooseMaskedRouter:
I0512 23:44:42.447704 139856202532864 gin_utils.py:85] # ==============================================================================
I0512 23:44:42.447735 139856202532864 gin_utils.py:85] sparse_encoder/routing.TokensChooseMaskedRouter.batch_prioritized_routing = False
I0512 23:44:42.447767 139856202532864 gin_utils.py:85] 
I0512 23:44:42.447799 139856202532864 gin_utils.py:85] # Parameters for train_script.train:
I0512 23:44:42.447832 139856202532864 gin_utils.py:85] # ==============================================================================
I0512 23:44:42.447869 139856202532864 gin_utils.py:85] train_script.train.checkpoint_cfg = @utils.CheckpointConfig()
I0512 23:44:42.436937 139666521167872 gin_utils.py:85] expert/dense.MlpBlock.use_bias = False
I0512 23:44:42.436967 139666521167872 gin_utils.py:85] 
I0512 23:44:42.436998 139666521167872 gin_utils.py:85] # Parameters for models.MoeDecoderOnlyModel:
I0512 23:44:42.437028 139666521167872 gin_utils.py:85] # ==============================================================================
I0512 23:44:42.437058 139666521167872 gin_utils.py:85] models.MoeDecoderOnlyModel.aux_loss_factor = %AUX_LOSS_FACTOR
I0512 23:44:42.437088 139666521167872 gin_utils.py:85] models.MoeDecoderOnlyModel.inputs_bidirectional_attention = True
I0512 23:44:42.437119 139666521167872 gin_utils.py:85] models.MoeDecoderOnlyModel.label_smoothing = %LABEL_SMOOTHING
I0512 23:44:42.437149 139666521167872 gin_utils.py:85] models.MoeDecoderOnlyModel.loss_normalizing_factor = %LOSS_NORMALIZING_FACTOR
I0512 23:44:42.437179 139666521167872 gin_utils.py:85] models.MoeDecoderOnlyModel.module = %ARCHITECTURE
I0512 23:44:42.437210 139666521167872 gin_utils.py:85] models.MoeDecoderOnlyModel.optimizer_def = %OPTIMIZER
I0512 23:44:42.437240 139666521167872 gin_utils.py:85] models.MoeDecoderOnlyModel.router_z_loss_factor = %ROUTER_Z_LOSS_FACTOR
I0512 23:44:42.437271 139666521167872 gin_utils.py:85] models.MoeDecoderOnlyModel.vocabulary = %VOCABULARY
I0512 23:44:42.437300 139666521167872 gin_utils.py:85] models.MoeDecoderOnlyModel.z_loss = %Z_LOSS
I0512 23:44:42.437336 139666521167872 gin_utils.py:85] 
I0512 23:44:42.437366 139666521167872 gin_utils.py:85] # Parameters for moe_layers.MoeLayer:
I0512 23:44:42.437397 139666521167872 gin_utils.py:85] # ==============================================================================
I0512 23:44:42.437427 139666521167872 gin_utils.py:85] moe_layers.MoeLayer.dtype = %MOE_TRUNCATED_DTYPE
I0512 23:44:42.437458 139666521167872 gin_utils.py:85] moe_layers.MoeLayer.eval_capacity_factor = %EVAL_EXPERT_CAPACITY_FACTOR
I0512 23:44:42.437488 139666521167872 gin_utils.py:85] moe_layers.MoeLayer.expert = @expert/dense.MlpBlock()
I0512 23:44:42.437518 139666521167872 gin_utils.py:85] moe_layers.MoeLayer.max_group_size = %GROUP_SIZE
I0512 23:44:42.437548 139666521167872 gin_utils.py:85] moe_layers.MoeLayer.num_expert_partitions = %NUM_EXPERT_PARTITIONS
I0512 23:44:42.437580 139666521167872 gin_utils.py:85] moe_layers.MoeLayer.num_experts = %NUM_EXPERTS
I0512 23:44:42.437611 139666521167872 gin_utils.py:85] moe_layers.MoeLayer.num_model_partitions = \
I0512 23:44:42.437640 139666521167872 gin_utils.py:85]     @moe_partitioning.compute_num_model_partitions()
I0512 23:44:42.437671 139666521167872 gin_utils.py:85] moe_layers.MoeLayer.train_capacity_factor = %TRAIN_EXPERT_CAPACITY_FACTOR
I0512 23:44:42.437700 139666521167872 gin_utils.py:85] 
I0512 23:44:42.437730 139666521167872 gin_utils.py:85] # Parameters for sparse_decoder/moe_layers.MoeLayer:
I0512 23:44:42.437761 139666521167872 gin_utils.py:85] # ==============================================================================
I0512 23:44:42.437791 139666521167872 gin_utils.py:85] sparse_decoder/moe_layers.MoeLayer.router = \
I0512 23:44:42.437821 139666521167872 gin_utils.py:85]     @sparse_decoder/routing.TokensChooseMaskedRouter()
I0512 23:44:42.437851 139666521167872 gin_utils.py:85] 
I0512 23:44:42.437881 139666521167872 gin_utils.py:85] # Parameters for moe_partitioning.MoePjitPartitioner:
I0512 23:44:42.437911 139666521167872 gin_utils.py:85] # ==============================================================================
I0512 23:44:42.437941 139666521167872 gin_utils.py:85] moe_partitioning.MoePjitPartitioner.model_parallel_submesh = \
I0512 23:44:42.437972 139666521167872 gin_utils.py:85]     %MODEL_PARALLEL_SUBMESH
I0512 23:44:42.438002 139666521167872 gin_utils.py:85] moe_partitioning.MoePjitPartitioner.num_expert_partitions = %NUM_EXPERT_PARTITIONS
I0512 23:44:42.438032 139666521167872 gin_utils.py:85] moe_partitioning.MoePjitPartitioner.num_partitions = %NUM_MODEL_PARTITIONS
I0512 23:44:42.438062 139666521167872 gin_utils.py:85] 
I0512 23:44:42.438092 139666521167872 gin_utils.py:85] # Parameters for moe_trainer.MoeTrainer:
I0512 23:44:42.438122 139666521167872 gin_utils.py:85] # ==============================================================================
I0512 23:44:42.438152 139666521167872 gin_utils.py:85] moe_trainer.MoeTrainer.learning_rate_fn = @utils.create_learning_rate_scheduler()
I0512 23:44:42.438182 139666521167872 gin_utils.py:85] moe_trainer.MoeTrainer.num_expert_partitions = %NUM_EXPERT_PARTITIONS
I0512 23:44:42.438213 139666521167872 gin_utils.py:85] moe_trainer.MoeTrainer.num_microbatches = 8
I0512 23:44:42.438243 139666521167872 gin_utils.py:85] 
I0512 23:44:42.438272 139666521167872 gin_utils.py:85] # Parameters for dense_attention.MultiHeadDotProductAttention:
I0512 23:44:42.438302 139666521167872 gin_utils.py:85] # ==============================================================================
I0512 23:44:42.438339 139666521167872 gin_utils.py:85] dense_attention.MultiHeadDotProductAttention.bias_init = %BIAS_INIT
I0512 23:44:42.438369 139666521167872 gin_utils.py:85] dense_attention.MultiHeadDotProductAttention.broadcast_dropout = True
I0512 23:44:42.438400 139666521167872 gin_utils.py:85] dense_attention.MultiHeadDotProductAttention.dropout_rate = %DROPOUT_RATE
I0512 23:44:42.438430 139666521167872 gin_utils.py:85] dense_attention.MultiHeadDotProductAttention.dtype = %ACTIVATION_DTYPE
I0512 23:44:42.438460 139666521167872 gin_utils.py:85] dense_attention.MultiHeadDotProductAttention.head_dim = %HEAD_DIM
I0512 23:44:42.438490 139666521167872 gin_utils.py:85] dense_attention.MultiHeadDotProductAttention.kernel_init = \
I0512 23:44:42.438520 139666521167872 gin_utils.py:85]     @attention_kernel_init/linen.initializers.variance_scaling()
I0512 23:44:42.438550 139666521167872 gin_utils.py:85] dense_attention.MultiHeadDotProductAttention.num_heads = %NUM_HEADS
I0512 23:44:42.438582 139666521167872 gin_utils.py:85] dense_attention.MultiHeadDotProductAttention.use_bias = False
I0512 23:44:42.438613 139666521167872 gin_utils.py:85] dense_attention.MultiHeadDotProductAttention.use_rotary_embedding = True
I0512 23:44:42.438643 139666521167872 gin_utils.py:85] 
I0512 23:44:42.438673 139666521167872 gin_utils.py:85] # Parameters for bias_init/linen.initializers.normal:
I0512 23:44:42.438703 139666521167872 gin_utils.py:85] # ==============================================================================
I0512 23:44:42.438733 139666521167872 gin_utils.py:85] bias_init/linen.initializers.normal.stddev = 1e-06
I0512 23:44:42.438764 139666521167872 gin_utils.py:85] 
I0512 23:44:42.438794 139666521167872 gin_utils.py:85] # Parameters for router_init/linen.initializers.normal:
I0512 23:44:42.438824 139666521167872 gin_utils.py:85] # ==============================================================================
I0512 23:44:42.438854 139666521167872 gin_utils.py:85] router_init/linen.initializers.normal.stddev = 0.02
I0512 23:44:42.438884 139666521167872 gin_utils.py:85] 
I0512 23:44:42.438914 139666521167872 gin_utils.py:85] # Parameters for token_embedder_init/linen.initializers.normal:
I0512 23:44:42.438944 139666521167872 gin_utils.py:85] # ==============================================================================
I0512 23:44:42.438974 139666521167872 gin_utils.py:85] token_embedder_init/linen.initializers.normal.stddev = 1.0
I0512 23:44:42.439004 139666521167872 gin_utils.py:85] 
I0512 23:44:42.439034 139666521167872 gin_utils.py:85] # Parameters for partitioning.PjitPartitioner:
I0512 23:44:42.439064 139666521167872 gin_utils.py:85] # ==============================================================================
I0512 23:44:42.439113 139666521167872 gin_utils.py:85] partitioning.PjitPartitioner.logical_axis_rules = \
I0512 23:44:42.439146 139666521167872 gin_utils.py:85]     @partitioning.standard_logical_axis_rules()
I0512 23:44:42.439177 139666521167872 gin_utils.py:85] partitioning.PjitPartitioner.model_parallel_submesh = None
I0512 23:44:42.443142 140193652815872 gin_utils.py:85] expert/dense.MlpBlock.use_bias = False
I0512 23:44:42.443174 140193652815872 gin_utils.py:85] 
I0512 23:44:42.443206 140193652815872 gin_utils.py:85] # Parameters for models.MoeDecoderOnlyModel:
I0512 23:44:42.443238 140193652815872 gin_utils.py:85] # ==============================================================================
I0512 23:44:42.443270 140193652815872 gin_utils.py:85] models.MoeDecoderOnlyModel.aux_loss_factor = %AUX_LOSS_FACTOR
I0512 23:44:42.443302 140193652815872 gin_utils.py:85] models.MoeDecoderOnlyModel.inputs_bidirectional_attention = True
I0512 23:44:42.443334 140193652815872 gin_utils.py:85] models.MoeDecoderOnlyModel.label_smoothing = %LABEL_SMOOTHING
I0512 23:44:42.443368 140193652815872 gin_utils.py:85] models.MoeDecoderOnlyModel.loss_normalizing_factor = %LOSS_NORMALIZING_FACTOR
I0512 23:44:42.443401 140193652815872 gin_utils.py:85] models.MoeDecoderOnlyModel.module = %ARCHITECTURE
I0512 23:44:42.443433 140193652815872 gin_utils.py:85] models.MoeDecoderOnlyModel.optimizer_def = %OPTIMIZER
I0512 23:44:42.443465 140193652815872 gin_utils.py:85] models.MoeDecoderOnlyModel.router_z_loss_factor = %ROUTER_Z_LOSS_FACTOR
I0512 23:44:42.443497 140193652815872 gin_utils.py:85] models.MoeDecoderOnlyModel.vocabulary = %VOCABULARY
I0512 23:44:42.443529 140193652815872 gin_utils.py:85] models.MoeDecoderOnlyModel.z_loss = %Z_LOSS
I0512 23:44:42.443562 140193652815872 gin_utils.py:85] 
I0512 23:44:42.443598 140193652815872 gin_utils.py:85] # Parameters for moe_layers.MoeLayer:
I0512 23:44:42.443632 140193652815872 gin_utils.py:85] # ==============================================================================
I0512 23:44:42.443664 140193652815872 gin_utils.py:85] moe_layers.MoeLayer.dtype = %MOE_TRUNCATED_DTYPE
I0512 23:44:42.443696 140193652815872 gin_utils.py:85] moe_layers.MoeLayer.eval_capacity_factor = %EVAL_EXPERT_CAPACITY_FACTOR
I0512 23:44:42.443728 140193652815872 gin_utils.py:85] moe_layers.MoeLayer.expert = @expert/dense.MlpBlock()
I0512 23:44:42.443761 140193652815872 gin_utils.py:85] moe_layers.MoeLayer.max_group_size = %GROUP_SIZE
I0512 23:44:42.443793 140193652815872 gin_utils.py:85] moe_layers.MoeLayer.num_expert_partitions = %NUM_EXPERT_PARTITIONS
I0512 23:44:42.443825 140193652815872 gin_utils.py:85] moe_layers.MoeLayer.num_experts = %NUM_EXPERTS
I0512 23:44:42.443857 140193652815872 gin_utils.py:85] moe_layers.MoeLayer.num_model_partitions = \
I0512 23:44:42.443889 140193652815872 gin_utils.py:85]     @moe_partitioning.compute_num_model_partitions()
I0512 23:44:42.443920 140193652815872 gin_utils.py:85] moe_layers.MoeLayer.train_capacity_factor = %TRAIN_EXPERT_CAPACITY_FACTOR
I0512 23:44:42.443952 140193652815872 gin_utils.py:85] 
I0512 23:44:42.443984 140193652815872 gin_utils.py:85] # Parameters for sparse_decoder/moe_layers.MoeLayer:
I0512 23:44:42.444016 140193652815872 gin_utils.py:85] # ==============================================================================
I0512 23:44:42.444048 140193652815872 gin_utils.py:85] sparse_decoder/moe_layers.MoeLayer.router = \
I0512 23:44:42.444080 140193652815872 gin_utils.py:85]     @sparse_decoder/routing.TokensChooseMaskedRouter()
I0512 23:44:42.444112 140193652815872 gin_utils.py:85] 
I0512 23:44:42.444144 140193652815872 gin_utils.py:85] # Parameters for moe_partitioning.MoePjitPartitioner:
I0512 23:44:42.444176 140193652815872 gin_utils.py:85] # ==============================================================================
I0512 23:44:42.444209 140193652815872 gin_utils.py:85] moe_partitioning.MoePjitPartitioner.model_parallel_submesh = \
I0512 23:44:42.444241 140193652815872 gin_utils.py:85]     %MODEL_PARALLEL_SUBMESH
I0512 23:44:42.444273 140193652815872 gin_utils.py:85] moe_partitioning.MoePjitPartitioner.num_expert_partitions = %NUM_EXPERT_PARTITIONS
I0512 23:44:42.444328 140193652815872 gin_utils.py:85] moe_partitioning.MoePjitPartitioner.num_partitions = %NUM_MODEL_PARTITIONS
I0512 23:44:42.444366 140193652815872 gin_utils.py:85] 
I0512 23:44:42.439207 139666521167872 gin_utils.py:85] partitioning.PjitPartitioner.num_partitions = 1
I0512 23:44:42.439237 139666521167872 gin_utils.py:85] 
I0512 23:44:42.439267 139666521167872 gin_utils.py:85] # Parameters for utils.RestoreCheckpointConfig:
I0512 23:44:42.439297 139666521167872 gin_utils.py:85] # ==============================================================================
I0512 23:44:42.439333 139666521167872 gin_utils.py:85] utils.RestoreCheckpointConfig.path = []
I0512 23:44:42.439363 139666521167872 gin_utils.py:85] 
I0512 23:44:42.439393 139666521167872 gin_utils.py:85] # Parameters for routing.RouterWeights:
I0512 23:44:42.439423 139666521167872 gin_utils.py:85] # ==============================================================================
I0512 23:44:42.439453 139666521167872 gin_utils.py:85] routing.RouterWeights.bias_init = %BIAS_INIT
I0512 23:44:42.439483 139666521167872 gin_utils.py:85] routing.RouterWeights.dtype = 'float32'
I0512 23:44:42.439513 139666521167872 gin_utils.py:85] routing.RouterWeights.kernel_init = @router_init/linen.initializers.normal()
I0512 23:44:42.439543 139666521167872 gin_utils.py:85] routing.RouterWeights.use_bias = False
I0512 23:44:42.439574 139666521167872 gin_utils.py:85] 
I0512 23:44:42.439606 139666521167872 gin_utils.py:85] # Parameters for utils.SaveCheckpointConfig:
I0512 23:44:42.439636 139666521167872 gin_utils.py:85] # ==============================================================================
I0512 23:44:42.439666 139666521167872 gin_utils.py:85] utils.SaveCheckpointConfig.dtype = 'float32'
I0512 23:44:42.439696 139666521167872 gin_utils.py:85] utils.SaveCheckpointConfig.keep = 20
I0512 23:44:42.439726 139666521167872 gin_utils.py:85] utils.SaveCheckpointConfig.period = 2500
I0512 23:44:42.439755 139666521167872 gin_utils.py:85] utils.SaveCheckpointConfig.save_dataset = False
I0512 23:44:42.439785 139666521167872 gin_utils.py:85] 
I0512 23:44:42.439815 139666521167872 gin_utils.py:85] # Parameters for seqio.SentencePieceVocabulary:
I0512 23:44:42.439845 139666521167872 gin_utils.py:85] # ==============================================================================
I0512 23:44:42.439875 139666521167872 gin_utils.py:85] seqio.SentencePieceVocabulary.extra_ids = 300
I0512 23:44:42.439905 139666521167872 gin_utils.py:85] seqio.SentencePieceVocabulary.sentencepiece_model_file = \
I0512 23:44:42.439935 139666521167872 gin_utils.py:85]     'gs://rosinality-tpu-bucket/sentencepiece.model'
I0512 23:44:42.439965 139666521167872 gin_utils.py:85] 
I0512 23:44:42.439994 139666521167872 gin_utils.py:85] # Parameters for moe_architecture.SparseDecoder:
I0512 23:44:42.440024 139666521167872 gin_utils.py:85] # ==============================================================================
I0512 23:44:42.440054 139666521167872 gin_utils.py:85] moe_architecture.SparseDecoder.dropout_factory = %DROPOUT_FACTORY
I0512 23:44:42.440084 139666521167872 gin_utils.py:85] moe_architecture.SparseDecoder.dtype = %ACTIVATION_DTYPE
I0512 23:44:42.440114 139666521167872 gin_utils.py:85] moe_architecture.SparseDecoder.layer_factory = @t5_architecture.DecoderLayer
I0512 23:44:42.440144 139666521167872 gin_utils.py:85] moe_architecture.SparseDecoder.layer_norm_factory = @layer_norm.T5LayerNorm
I0512 23:44:42.440174 139666521167872 gin_utils.py:85] moe_architecture.SparseDecoder.num_layers = %NUM_DECODER_LAYERS
I0512 23:44:42.440204 139666521167872 gin_utils.py:85] moe_architecture.SparseDecoder.num_sparse_layers = %NUM_DECODER_SPARSE_LAYERS
I0512 23:44:42.440233 139666521167872 gin_utils.py:85] moe_architecture.SparseDecoder.output_logits_factory = \
I0512 23:44:42.440263 139666521167872 gin_utils.py:85]     @output_logits/dense.DenseGeneral
I0512 23:44:42.440293 139666521167872 gin_utils.py:85] moe_architecture.SparseDecoder.sparse_layer_factory = \
I0512 23:44:42.440329 139666521167872 gin_utils.py:85]     @moe_architecture.SparseDecoderLayer
I0512 23:44:42.440360 139666521167872 gin_utils.py:85] moe_architecture.SparseDecoder.sparse_layout = %DECODER_SPARSE_LAYOUT
I0512 23:44:42.440390 139666521167872 gin_utils.py:85] 
I0512 23:44:42.440420 139666521167872 gin_utils.py:85] # Parameters for moe_architecture.SparseDecoderLayer:
I0512 23:44:42.440450 139666521167872 gin_utils.py:85] # ==============================================================================
I0512 23:44:42.440480 139666521167872 gin_utils.py:85] moe_architecture.SparseDecoderLayer.activation_partitioning_dims = \
I0512 23:44:42.440510 139666521167872 gin_utils.py:85]     %ACTIVATION_PARTITIONING_DIMS
I0512 23:44:42.440540 139666521167872 gin_utils.py:85] moe_architecture.SparseDecoderLayer.dropout_factory = %DROPOUT_FACTORY
I0512 23:44:42.440572 139666521167872 gin_utils.py:85] moe_architecture.SparseDecoderLayer.encoder_decoder_attention = None
I0512 23:44:42.440603 139666521167872 gin_utils.py:85] moe_architecture.SparseDecoderLayer.extra_mlp = @dense.MlpBlock()
I0512 23:44:42.440634 139666521167872 gin_utils.py:85] moe_architecture.SparseDecoderLayer.layer_norm_factory = @layer_norm.T5LayerNorm
I0512 23:44:42.440664 139666521167872 gin_utils.py:85] moe_architecture.SparseDecoderLayer.mlp = @sparse_decoder/moe_layers.MoeLayer()
I0512 23:44:42.440694 139666521167872 gin_utils.py:85] moe_architecture.SparseDecoderLayer.scanned = False
I0512 23:44:42.440725 139666521167872 gin_utils.py:85] moe_architecture.SparseDecoderLayer.self_attention = \
I0512 23:44:42.440755 139666521167872 gin_utils.py:85]     @dense_attention.MultiHeadDotProductAttention()
I0512 23:44:42.440785 139666521167872 gin_utils.py:85] 
I0512 23:44:42.440815 139666521167872 gin_utils.py:85] # Parameters for layer_norm.T5LayerNorm:
I0512 23:44:42.440845 139666521167872 gin_utils.py:85] # ==============================================================================
I0512 23:44:42.440875 139666521167872 gin_utils.py:85] layer_norm.T5LayerNorm.dtype = %ACTIVATION_DTYPE
I0512 23:44:42.440905 139666521167872 gin_utils.py:85] 
I0512 23:44:42.440935 139666521167872 gin_utils.py:85] # Parameters for routing.TokensChooseMaskedRouter:
I0512 23:44:42.444399 140193652815872 gin_utils.py:85] # Parameters for moe_trainer.MoeTrainer:
I0512 23:44:42.444431 140193652815872 gin_utils.py:85] # ==============================================================================
I0512 23:44:42.444463 140193652815872 gin_utils.py:85] moe_trainer.MoeTrainer.learning_rate_fn = @utils.create_learning_rate_scheduler()
I0512 23:44:42.444495 140193652815872 gin_utils.py:85] moe_trainer.MoeTrainer.num_expert_partitions = %NUM_EXPERT_PARTITIONS
I0512 23:44:42.444528 140193652815872 gin_utils.py:85] moe_trainer.MoeTrainer.num_microbatches = 8
I0512 23:44:42.444559 140193652815872 gin_utils.py:85] 
I0512 23:44:42.444596 140193652815872 gin_utils.py:85] # Parameters for dense_attention.MultiHeadDotProductAttention:
I0512 23:44:42.444629 140193652815872 gin_utils.py:85] # ==============================================================================
I0512 23:44:42.444662 140193652815872 gin_utils.py:85] dense_attention.MultiHeadDotProductAttention.bias_init = %BIAS_INIT
I0512 23:44:42.444694 140193652815872 gin_utils.py:85] dense_attention.MultiHeadDotProductAttention.broadcast_dropout = True
I0512 23:44:42.444726 140193652815872 gin_utils.py:85] dense_attention.MultiHeadDotProductAttention.dropout_rate = %DROPOUT_RATE
I0512 23:44:42.444757 140193652815872 gin_utils.py:85] dense_attention.MultiHeadDotProductAttention.dtype = %ACTIVATION_DTYPE
I0512 23:44:42.444790 140193652815872 gin_utils.py:85] dense_attention.MultiHeadDotProductAttention.head_dim = %HEAD_DIM
I0512 23:44:42.444822 140193652815872 gin_utils.py:85] dense_attention.MultiHeadDotProductAttention.kernel_init = \
I0512 23:44:42.444853 140193652815872 gin_utils.py:85]     @attention_kernel_init/linen.initializers.variance_scaling()
I0512 23:44:42.444885 140193652815872 gin_utils.py:85] dense_attention.MultiHeadDotProductAttention.num_heads = %NUM_HEADS
I0512 23:44:42.444917 140193652815872 gin_utils.py:85] dense_attention.MultiHeadDotProductAttention.use_bias = False
I0512 23:44:42.444949 140193652815872 gin_utils.py:85] dense_attention.MultiHeadDotProductAttention.use_rotary_embedding = True
I0512 23:44:42.444981 140193652815872 gin_utils.py:85] 
I0512 23:44:42.445013 140193652815872 gin_utils.py:85] # Parameters for bias_init/linen.initializers.normal:
I0512 23:44:42.445045 140193652815872 gin_utils.py:85] # ==============================================================================
I0512 23:44:42.445077 140193652815872 gin_utils.py:85] bias_init/linen.initializers.normal.stddev = 1e-06
I0512 23:44:42.445110 140193652815872 gin_utils.py:85] 
I0512 23:44:42.445142 140193652815872 gin_utils.py:85] # Parameters for router_init/linen.initializers.normal:
I0512 23:44:42.445174 140193652815872 gin_utils.py:85] # ==============================================================================
I0512 23:44:42.445206 140193652815872 gin_utils.py:85] router_init/linen.initializers.normal.stddev = 0.02
I0512 23:44:42.445238 140193652815872 gin_utils.py:85] 
I0512 23:44:42.445270 140193652815872 gin_utils.py:85] # Parameters for token_embedder_init/linen.initializers.normal:
I0512 23:44:42.445302 140193652815872 gin_utils.py:85] # ==============================================================================
I0512 23:44:42.445333 140193652815872 gin_utils.py:85] token_embedder_init/linen.initializers.normal.stddev = 1.0
I0512 23:44:42.445369 140193652815872 gin_utils.py:85] 
I0512 23:44:42.445401 140193652815872 gin_utils.py:85] # Parameters for partitioning.PjitPartitioner:
I0512 23:44:42.445434 140193652815872 gin_utils.py:85] # ==============================================================================
I0512 23:44:42.445466 140193652815872 gin_utils.py:85] partitioning.PjitPartitioner.logical_axis_rules = \
I0512 23:44:42.445498 140193652815872 gin_utils.py:85]     @partitioning.standard_logical_axis_rules()
I0512 23:44:42.445530 140193652815872 gin_utils.py:85] partitioning.PjitPartitioner.model_parallel_submesh = None
I0512 23:44:42.445562 140193652815872 gin_utils.py:85] partitioning.PjitPartitioner.num_partitions = 1
I0512 23:44:42.445600 140193652815872 gin_utils.py:85] 
I0512 23:44:42.445635 140193652815872 gin_utils.py:85] # Parameters for utils.RestoreCheckpointConfig:
I0512 23:44:42.445667 140193652815872 gin_utils.py:85] # ==============================================================================
I0512 23:44:42.445699 140193652815872 gin_utils.py:85] utils.RestoreCheckpointConfig.path = []
I0512 23:44:42.445731 140193652815872 gin_utils.py:85] 
I0512 23:44:42.445763 140193652815872 gin_utils.py:85] # Parameters for routing.RouterWeights:
I0512 23:44:42.445795 140193652815872 gin_utils.py:85] # ==============================================================================
I0512 23:44:42.445828 140193652815872 gin_utils.py:85] routing.RouterWeights.bias_init = %BIAS_INIT
I0512 23:44:42.445860 140193652815872 gin_utils.py:85] routing.RouterWeights.dtype = 'float32'
I0512 23:44:42.445892 140193652815872 gin_utils.py:85] routing.RouterWeights.kernel_init = @router_init/linen.initializers.normal()
I0512 23:44:42.445924 140193652815872 gin_utils.py:85] routing.RouterWeights.use_bias = False
I0512 23:44:42.445956 140193652815872 gin_utils.py:85] 
I0512 23:44:42.445988 140193652815872 gin_utils.py:85] # Parameters for utils.SaveCheckpointConfig:
I0512 23:44:42.446020 140193652815872 gin_utils.py:85] # ==============================================================================
I0512 23:44:42.446052 140193652815872 gin_utils.py:85] utils.SaveCheckpointConfig.dtype = 'float32'
I0512 23:44:42.446084 140193652815872 gin_utils.py:85] utils.SaveCheckpointConfig.keep = 20
I0512 23:44:42.446116 140193652815872 gin_utils.py:85] utils.SaveCheckpointConfig.period = 2500
I0512 23:44:42.446148 140193652815872 gin_utils.py:85] utils.SaveCheckpointConfig.save_dataset = False
I0512 23:44:42.446180 140193652815872 gin_utils.py:85] 
I0512 23:44:42.446212 140193652815872 gin_utils.py:85] # Parameters for seqio.SentencePieceVocabulary:
I0512 23:44:42.446244 140193652815872 gin_utils.py:85] # ==============================================================================
I0512 23:44:42.446276 140193652815872 gin_utils.py:85] seqio.SentencePieceVocabulary.extra_ids = 300
I0512 23:44:42.446308 140193652815872 gin_utils.py:85] seqio.SentencePieceVocabulary.sentencepiece_model_file = \
I0512 23:44:42.446340 140193652815872 gin_utils.py:85]     'gs://rosinality-tpu-bucket/sentencepiece.model'
I0512 23:44:42.446374 140193652815872 gin_utils.py:85] 
I0512 23:44:42.446406 140193652815872 gin_utils.py:85] # Parameters for moe_architecture.SparseDecoder:
I0512 23:44:42.446439 140193652815872 gin_utils.py:85] # ==============================================================================
I0512 23:44:42.446470 140193652815872 gin_utils.py:85] moe_architecture.SparseDecoder.dropout_factory = %DROPOUT_FACTORY
I0512 23:44:42.446502 140193652815872 gin_utils.py:85] moe_architecture.SparseDecoder.dtype = %ACTIVATION_DTYPE
I0512 23:44:42.446534 140193652815872 gin_utils.py:85] moe_architecture.SparseDecoder.layer_factory = @t5_architecture.DecoderLayer
I0512 23:44:42.446566 140193652815872 gin_utils.py:85] moe_architecture.SparseDecoder.layer_norm_factory = @layer_norm.T5LayerNorm
I0512 23:44:42.446604 140193652815872 gin_utils.py:85] moe_architecture.SparseDecoder.num_layers = %NUM_DECODER_LAYERS
I0512 23:44:42.446637 140193652815872 gin_utils.py:85] moe_architecture.SparseDecoder.num_sparse_layers = %NUM_DECODER_SPARSE_LAYERS
I0512 23:44:42.446670 140193652815872 gin_utils.py:85] moe_architecture.SparseDecoder.output_logits_factory = \
I0512 23:44:42.446702 140193652815872 gin_utils.py:85]     @output_logits/dense.DenseGeneral
I0512 23:44:42.446734 140193652815872 gin_utils.py:85] moe_architecture.SparseDecoder.sparse_layer_factory = \
I0512 23:44:42.446766 140193652815872 gin_utils.py:85]     @moe_architecture.SparseDecoderLayer
I0512 23:44:42.446798 140193652815872 gin_utils.py:85] moe_architecture.SparseDecoder.sparse_layout = %DECODER_SPARSE_LAYOUT
I0512 23:44:42.440964 139666521167872 gin_utils.py:85] # ==============================================================================
I0512 23:44:42.440994 139666521167872 gin_utils.py:85] routing.TokensChooseMaskedRouter.dtype = 'float32'
I0512 23:44:42.441025 139666521167872 gin_utils.py:85] routing.TokensChooseMaskedRouter.ignore_padding_tokens = False
I0512 23:44:42.441055 139666521167872 gin_utils.py:85] routing.TokensChooseMaskedRouter.jitter_noise = %JITTER_NOISE
I0512 23:44:42.441086 139666521167872 gin_utils.py:85] routing.TokensChooseMaskedRouter.num_selected_experts = %NUM_SELECTED_EXPERTS
I0512 23:44:42.441115 139666521167872 gin_utils.py:85] routing.TokensChooseMaskedRouter.router_weights = @routing.RouterWeights()
I0512 23:44:42.441145 139666521167872 gin_utils.py:85] 
I0512 23:44:42.441175 139666521167872 gin_utils.py:85] # Parameters for sparse_decoder/routing.TokensChooseMaskedRouter:
I0512 23:44:42.441205 139666521167872 gin_utils.py:85] # ==============================================================================
I0512 23:44:42.441234 139666521167872 gin_utils.py:85] sparse_decoder/routing.TokensChooseMaskedRouter.batch_prioritized_routing = False
I0512 23:44:42.441264 139666521167872 gin_utils.py:85] 
I0512 23:44:42.441294 139666521167872 gin_utils.py:85] # Parameters for sparse_encoder/routing.TokensChooseMaskedRouter:
I0512 23:44:42.441330 139666521167872 gin_utils.py:85] # ==============================================================================
I0512 23:44:42.441361 139666521167872 gin_utils.py:85] sparse_encoder/routing.TokensChooseMaskedRouter.batch_prioritized_routing = False
I0512 23:44:42.441392 139666521167872 gin_utils.py:85] 
I0512 23:44:42.441422 139666521167872 gin_utils.py:85] # Parameters for train_script.train:
I0512 23:44:42.441452 139666521167872 gin_utils.py:85] # ==============================================================================
I0512 23:44:42.441482 139666521167872 gin_utils.py:85] train_script.train.checkpoint_cfg = @utils.CheckpointConfig()
I0512 23:44:42.446830 140193652815872 gin_utils.py:85] 
I0512 23:44:42.446861 140193652815872 gin_utils.py:85] # Parameters for moe_architecture.SparseDecoderLayer:
I0512 23:44:42.446893 140193652815872 gin_utils.py:85] # ==============================================================================
I0512 23:44:42.446926 140193652815872 gin_utils.py:85] moe_architecture.SparseDecoderLayer.activation_partitioning_dims = \
I0512 23:44:42.446958 140193652815872 gin_utils.py:85]     %ACTIVATION_PARTITIONING_DIMS
I0512 23:44:42.446990 140193652815872 gin_utils.py:85] moe_architecture.SparseDecoderLayer.dropout_factory = %DROPOUT_FACTORY
I0512 23:44:42.447023 140193652815872 gin_utils.py:85] moe_architecture.SparseDecoderLayer.encoder_decoder_attention = None
I0512 23:44:42.447055 140193652815872 gin_utils.py:85] moe_architecture.SparseDecoderLayer.extra_mlp = @dense.MlpBlock()
I0512 23:44:42.447087 140193652815872 gin_utils.py:85] moe_architecture.SparseDecoderLayer.layer_norm_factory = @layer_norm.T5LayerNorm
I0512 23:44:42.447120 140193652815872 gin_utils.py:85] moe_architecture.SparseDecoderLayer.mlp = @sparse_decoder/moe_layers.MoeLayer()
I0512 23:44:42.447152 140193652815872 gin_utils.py:85] moe_architecture.SparseDecoderLayer.scanned = False
I0512 23:44:42.447184 140193652815872 gin_utils.py:85] moe_architecture.SparseDecoderLayer.self_attention = \
I0512 23:44:42.447216 140193652815872 gin_utils.py:85]     @dense_attention.MultiHeadDotProductAttention()
I0512 23:44:42.447248 140193652815872 gin_utils.py:85] 
I0512 23:44:42.447280 140193652815872 gin_utils.py:85] # Parameters for layer_norm.T5LayerNorm:
I0512 23:44:42.447311 140193652815872 gin_utils.py:85] # ==============================================================================
I0512 23:44:42.447344 140193652815872 gin_utils.py:85] layer_norm.T5LayerNorm.dtype = %ACTIVATION_DTYPE
I0512 23:44:42.447378 140193652815872 gin_utils.py:85] 
I0512 23:44:42.447411 140193652815872 gin_utils.py:85] # Parameters for routing.TokensChooseMaskedRouter:
I0512 23:44:42.447443 140193652815872 gin_utils.py:85] # ==============================================================================
I0512 23:44:42.447475 140193652815872 gin_utils.py:85] routing.TokensChooseMaskedRouter.dtype = 'float32'
I0512 23:44:42.447507 140193652815872 gin_utils.py:85] routing.TokensChooseMaskedRouter.ignore_padding_tokens = False
I0512 23:44:42.447538 140193652815872 gin_utils.py:85] routing.TokensChooseMaskedRouter.jitter_noise = %JITTER_NOISE
I0512 23:44:42.447570 140193652815872 gin_utils.py:85] routing.TokensChooseMaskedRouter.num_selected_experts = %NUM_SELECTED_EXPERTS
I0512 23:44:42.447607 140193652815872 gin_utils.py:85] routing.TokensChooseMaskedRouter.router_weights = @routing.RouterWeights()
I0512 23:44:42.447640 140193652815872 gin_utils.py:85] 
I0512 23:44:42.447672 140193652815872 gin_utils.py:85] # Parameters for sparse_decoder/routing.TokensChooseMaskedRouter:
I0512 23:44:42.447704 140193652815872 gin_utils.py:85] # ==============================================================================
I0512 23:44:42.447736 140193652815872 gin_utils.py:85] sparse_decoder/routing.TokensChooseMaskedRouter.batch_prioritized_routing = False
I0512 23:44:42.447768 140193652815872 gin_utils.py:85] 
I0512 23:44:42.447800 140193652815872 gin_utils.py:85] # Parameters for sparse_encoder/routing.TokensChooseMaskedRouter:
I0512 23:44:42.447832 140193652815872 gin_utils.py:85] # ==============================================================================
I0512 23:44:42.447864 140193652815872 gin_utils.py:85] sparse_encoder/routing.TokensChooseMaskedRouter.batch_prioritized_routing = False
I0512 23:44:42.447897 140193652815872 gin_utils.py:85] 
I0512 23:44:42.447929 140193652815872 gin_utils.py:85] # Parameters for train_script.train:
I0512 23:44:42.447962 140193652815872 gin_utils.py:85] # ==============================================================================
I0512 23:44:42.447994 140193652815872 gin_utils.py:85] train_script.train.checkpoint_cfg = @utils.CheckpointConfig()
I0512 23:44:42.630034 140260698863616 resource_reader.py:50] system_path_file_exists:flaxformer/flaxformer/t5x/configs/moe/models/tokens_choose_decoder_only_large.gin
E0512 23:44:42.630215 140260698863616 resource_reader.py:55] Path not found: flaxformer/flaxformer/t5x/configs/moe/models/tokens_choose_decoder_only_large.gin
I0512 23:44:42.630281 140260698863616 resource_reader.py:50] system_path_file_exists:./flaxformer/flaxformer/t5x/configs/moe/models/tokens_choose_decoder_only_large.gin
E0512 23:44:42.630326 140260698863616 resource_reader.py:55] Path not found: ./flaxformer/flaxformer/t5x/configs/moe/models/tokens_choose_decoder_only_large.gin
I0512 23:44:42.631244 140260698863616 resource_reader.py:50] system_path_file_exists:flaxformer/flaxformer/t5x/configs/moe/architectures/moe_decoder_only.gin
E0512 23:44:42.631388 140260698863616 resource_reader.py:55] Path not found: flaxformer/flaxformer/t5x/configs/moe/architectures/moe_decoder_only.gin
I0512 23:44:42.631441 140260698863616 resource_reader.py:50] system_path_file_exists:./flaxformer/flaxformer/t5x/configs/moe/architectures/moe_decoder_only.gin
E0512 23:44:42.631481 140260698863616 resource_reader.py:55] Path not found: ./flaxformer/flaxformer/t5x/configs/moe/architectures/moe_decoder_only.gin
I0512 23:44:42.661664 140180528838656 resource_reader.py:50] system_path_file_exists:flaxformer/flaxformer/t5x/configs/moe/models/tokens_choose_decoder_only_large.gin
E0512 23:44:42.661848 140180528838656 resource_reader.py:55] Path not found: flaxformer/flaxformer/t5x/configs/moe/models/tokens_choose_decoder_only_large.gin
I0512 23:44:42.661906 140180528838656 resource_reader.py:50] system_path_file_exists:./flaxformer/flaxformer/t5x/configs/moe/models/tokens_choose_decoder_only_large.gin
E0512 23:44:42.661949 140180528838656 resource_reader.py:55] Path not found: ./flaxformer/flaxformer/t5x/configs/moe/models/tokens_choose_decoder_only_large.gin
I0512 23:44:42.662829 140180528838656 resource_reader.py:50] system_path_file_exists:flaxformer/flaxformer/t5x/configs/moe/architectures/moe_decoder_only.gin
E0512 23:44:42.662961 140180528838656 resource_reader.py:55] Path not found: flaxformer/flaxformer/t5x/configs/moe/architectures/moe_decoder_only.gin
I0512 23:44:42.663013 140180528838656 resource_reader.py:50] system_path_file_exists:./flaxformer/flaxformer/t5x/configs/moe/architectures/moe_decoder_only.gin
E0512 23:44:42.663053 140180528838656 resource_reader.py:55] Path not found: ./flaxformer/flaxformer/t5x/configs/moe/architectures/moe_decoder_only.gin
I0512 23:44:42.565423 140643590531072 gin_utils.py:85] # Parameters for utils.CheckpointConfig:
I0512 23:44:42.565455 140643590531072 gin_utils.py:85] # ==============================================================================
I0512 23:44:42.565487 140643590531072 gin_utils.py:85] utils.CheckpointConfig.restore = @utils.RestoreCheckpointConfig()
I0512 23:44:42.565518 140643590531072 gin_utils.py:85] utils.CheckpointConfig.save = @utils.SaveCheckpointConfig()
I0512 23:44:42.565550 140643590531072 gin_utils.py:85] 
I0512 23:44:42.565582 140643590531072 gin_utils.py:85] # Parameters for moe_partitioning.compute_num_model_partitions:
I0512 23:44:42.565613 140643590531072 gin_utils.py:85] # ==============================================================================
I0512 23:44:42.565645 140643590531072 gin_utils.py:85] moe_partitioning.compute_num_model_partitions.model_parallel_submesh = \
I0512 23:44:42.565676 140643590531072 gin_utils.py:85]     %MODEL_PARALLEL_SUBMESH
I0512 23:44:42.565714 140643590531072 gin_utils.py:85] moe_partitioning.compute_num_model_partitions.num_model_partitions = \
I0512 23:44:42.565747 140643590531072 gin_utils.py:85]     %NUM_MODEL_PARTITIONS
I0512 23:44:42.565778 140643590531072 gin_utils.py:85] 
I0512 23:44:42.565810 140643590531072 gin_utils.py:85] # Parameters for utils.create_learning_rate_scheduler:
I0512 23:44:42.565842 140643590531072 gin_utils.py:85] # ==============================================================================
I0512 23:44:42.565873 140643590531072 gin_utils.py:85] utils.create_learning_rate_scheduler.base_learning_rate = 1.0
I0512 23:44:42.565904 140643590531072 gin_utils.py:85] utils.create_learning_rate_scheduler.factors = 'constant * rsqrt_decay'
I0512 23:44:42.565936 140643590531072 gin_utils.py:85] utils.create_learning_rate_scheduler.warmup_steps = 10000
I0512 23:44:42.565970 140643590531072 gin_utils.py:85] 
I0512 23:44:42.566002 140643590531072 gin_utils.py:85] # Parameters for train/utils.DatasetConfig:
I0512 23:44:42.566034 140643590531072 gin_utils.py:85] # ==============================================================================
I0512 23:44:42.566066 140643590531072 gin_utils.py:85] train/utils.DatasetConfig.batch_size = %BATCH_SIZE
I0512 23:44:42.566097 140643590531072 gin_utils.py:85] train/utils.DatasetConfig.mixture_or_task_name = %MIXTURE_OR_TASK_NAME
I0512 23:44:42.566129 140643590531072 gin_utils.py:85] train/utils.DatasetConfig.module = %MIXTURE_OR_TASK_MODULE
I0512 23:44:42.566160 140643590531072 gin_utils.py:85] train/utils.DatasetConfig.pack = True
I0512 23:44:42.566192 140643590531072 gin_utils.py:85] train/utils.DatasetConfig.seed = None
I0512 23:44:42.566223 140643590531072 gin_utils.py:85] train/utils.DatasetConfig.shuffle = %SHUFFLE_TRAIN_EXAMPLES
I0512 23:44:42.566254 140643590531072 gin_utils.py:85] train/utils.DatasetConfig.split = 'train'
I0512 23:44:42.566286 140643590531072 gin_utils.py:85] train/utils.DatasetConfig.task_feature_lengths = %TASK_FEATURE_LENGTHS
I0512 23:44:42.566317 140643590531072 gin_utils.py:85] train/utils.DatasetConfig.use_cached = %USE_CACHED_TASKS
I0512 23:44:42.566349 140643590531072 gin_utils.py:85] 
I0512 23:44:42.566380 140643590531072 gin_utils.py:85] # Parameters for train_eval/utils.DatasetConfig:
I0512 23:44:42.566412 140643590531072 gin_utils.py:85] # ==============================================================================
I0512 23:44:42.566443 140643590531072 gin_utils.py:85] train_eval/utils.DatasetConfig.batch_size = 128
I0512 23:44:42.566474 140643590531072 gin_utils.py:85] train_eval/utils.DatasetConfig.mixture_or_task_name = %MIXTURE_OR_TASK_NAME
I0512 23:44:42.566505 140643590531072 gin_utils.py:85] train_eval/utils.DatasetConfig.module = %MIXTURE_OR_TASK_MODULE
I0512 23:44:42.566537 140643590531072 gin_utils.py:85] train_eval/utils.DatasetConfig.pack = True
I0512 23:44:42.566568 140643590531072 gin_utils.py:85] train_eval/utils.DatasetConfig.seed = 42
I0512 23:44:42.566599 140643590531072 gin_utils.py:85] train_eval/utils.DatasetConfig.shuffle = False
I0512 23:44:42.566630 140643590531072 gin_utils.py:85] train_eval/utils.DatasetConfig.split = 'validation'
I0512 23:44:42.566662 140643590531072 gin_utils.py:85] train_eval/utils.DatasetConfig.task_feature_lengths = %TASK_FEATURE_LENGTHS
I0512 23:44:42.566699 140643590531072 gin_utils.py:85] train_eval/utils.DatasetConfig.use_cached = %USE_CACHED_TASKS
I0512 23:44:42.566733 140643590531072 gin_utils.py:85] 
I0512 23:44:42.566765 140643590531072 gin_utils.py:85] # Parameters for t5_architecture.DecoderLayer:
I0512 23:44:42.566796 140643590531072 gin_utils.py:85] # ==============================================================================
I0512 23:44:42.566828 140643590531072 gin_utils.py:85] t5_architecture.DecoderLayer.activation_partitioning_dims = \
I0512 23:44:42.566859 140643590531072 gin_utils.py:85]     %ACTIVATION_PARTITIONING_DIMS
I0512 23:44:42.566891 140643590531072 gin_utils.py:85] t5_architecture.DecoderLayer.dropout_factory = %DROPOUT_FACTORY
I0512 23:44:42.566922 140643590531072 gin_utils.py:85] t5_architecture.DecoderLayer.encoder_decoder_attention = None
I0512 23:44:42.566955 140643590531072 gin_utils.py:85] t5_architecture.DecoderLayer.layer_norm_factory = @layer_norm.T5LayerNorm
I0512 23:44:42.566988 140643590531072 gin_utils.py:85] t5_architecture.DecoderLayer.mlp = @dense.MlpBlock()
I0512 23:44:42.567019 140643590531072 gin_utils.py:85] t5_architecture.DecoderLayer.scanned = False
I0512 23:44:42.567051 140643590531072 gin_utils.py:85] t5_architecture.DecoderLayer.self_attention = \
I0512 23:44:42.567082 140643590531072 gin_utils.py:85]     @dense_attention.MultiHeadDotProductAttention()
I0512 23:44:42.567113 140643590531072 gin_utils.py:85] 
I0512 23:44:42.567145 140643590531072 gin_utils.py:85] # Parameters for t5_architecture.DecoderOnly:
I0512 23:44:42.567176 140643590531072 gin_utils.py:85] # ==============================================================================
I0512 23:44:42.567208 140643590531072 gin_utils.py:85] t5_architecture.DecoderOnly.decoder_factory = @moe_architecture.SparseDecoder
I0512 23:44:42.567239 140643590531072 gin_utils.py:85] t5_architecture.DecoderOnly.dtype = %ACTIVATION_DTYPE
I0512 23:44:42.567270 140643590531072 gin_utils.py:85] t5_architecture.DecoderOnly.shared_token_embedder_factory = @embedding.Embed
I0512 23:44:42.567302 140643590531072 gin_utils.py:85] 
I0512 23:44:42.567333 140643590531072 gin_utils.py:85] # Parameters for output_logits/dense.DenseGeneral:
I0512 23:44:42.567365 140643590531072 gin_utils.py:85] # ==============================================================================
I0512 23:44:42.567396 140643590531072 gin_utils.py:85] output_logits/dense.DenseGeneral.bias_init = %BIAS_INIT
I0512 23:44:42.567427 140643590531072 gin_utils.py:85] output_logits/dense.DenseGeneral.dtype = 'float32'
I0512 23:44:42.567458 140643590531072 gin_utils.py:85] output_logits/dense.DenseGeneral.features = %NUM_EMBEDDINGS
I0512 23:44:42.567490 140643590531072 gin_utils.py:85] output_logits/dense.DenseGeneral.kernel_axis_names = ['embed', 'vocab']
I0512 23:44:42.567521 140643590531072 gin_utils.py:85] output_logits/dense.DenseGeneral.kernel_init = \
I0512 23:44:42.567553 140643590531072 gin_utils.py:85]     @output_logits_kernel_init/linen.initializers.variance_scaling()
I0512 23:44:42.567608 140643590531072 gin_utils.py:85] output_logits/dense.DenseGeneral.use_bias = False
I0512 23:44:42.567642 140643590531072 gin_utils.py:85] 
I0512 23:44:42.567674 140643590531072 gin_utils.py:85] # Parameters for dropout_factory/linen.Dropout:
I0512 23:44:42.567712 140643590531072 gin_utils.py:85] # ==============================================================================
I0512 23:44:42.567744 140643590531072 gin_utils.py:85] dropout_factory/linen.Dropout.broadcast_dims = (-2,)
I0512 23:44:42.567775 140643590531072 gin_utils.py:85] dropout_factory/linen.Dropout.rate = %DROPOUT_RATE
I0512 23:44:42.567807 140643590531072 gin_utils.py:85] 
I0512 23:44:42.567838 140643590531072 gin_utils.py:85] # Parameters for embedding.Embed:
I0512 23:44:42.567869 140643590531072 gin_utils.py:85] # ==============================================================================
I0512 23:44:42.567901 140643590531072 gin_utils.py:85] embedding.Embed.attend_dtype = 'float32'
I0512 23:44:42.567932 140643590531072 gin_utils.py:85] embedding.Embed.cast_input_dtype = 'int32'
I0512 23:44:42.567966 140643590531072 gin_utils.py:85] embedding.Embed.dtype = %ACTIVATION_DTYPE
I0512 23:44:42.567998 140643590531072 gin_utils.py:85] embedding.Embed.embedding_init = @token_embedder_init/linen.initializers.normal()
I0512 23:44:42.568028 140643590531072 gin_utils.py:85] embedding.Embed.features = %EMBED_DIM
I0512 23:44:42.568059 140643590531072 gin_utils.py:85] embedding.Embed.name = 'token_embedder'
I0512 23:44:42.568090 140643590531072 gin_utils.py:85] embedding.Embed.num_embeddings = %NUM_EMBEDDINGS
I0512 23:44:42.568121 140643590531072 gin_utils.py:85] embedding.Embed.one_hot = True
I0512 23:44:42.568152 140643590531072 gin_utils.py:85] 
I0512 23:44:42.568183 140643590531072 gin_utils.py:85] # Parameters for dense.MlpBlock:
I0512 23:44:42.568215 140643590531072 gin_utils.py:85] # ==============================================================================
I0512 23:44:42.568247 140643590531072 gin_utils.py:85] dense.MlpBlock.activations = ('swiglu', 'linear')
I0512 23:44:42.568279 140643590531072 gin_utils.py:85] dense.MlpBlock.bias_init = %BIAS_INIT
I0512 23:44:42.568310 140643590531072 gin_utils.py:85] dense.MlpBlock.dtype = %ACTIVATION_DTYPE
I0512 23:44:42.568341 140643590531072 gin_utils.py:85] dense.MlpBlock.final_dropout_rate = 0
I0512 23:44:42.568372 140643590531072 gin_utils.py:85] dense.MlpBlock.input_axis_name = 'mlp_embed'
I0512 23:44:42.568403 140643590531072 gin_utils.py:85] dense.MlpBlock.intermediate_dim = %MLP_DIM
I0512 23:44:42.568435 140643590531072 gin_utils.py:85] dense.MlpBlock.intermediate_dropout_rate = %DROPOUT_RATE
I0512 23:44:42.568466 140643590531072 gin_utils.py:85] dense.MlpBlock.kernel_init = @mlp_kernel_init/linen.initializers.variance_scaling()
I0512 23:44:42.441512 139666521167872 gin_utils.py:85] train_script.train.eval_period = 2500
I0512 23:44:42.441542 139666521167872 gin_utils.py:85] train_script.train.eval_steps = 20
I0512 23:44:42.441573 139666521167872 gin_utils.py:85] train_script.train.infer_eval_dataset_cfg = None
I0512 23:44:42.441604 139666521167872 gin_utils.py:85] train_script.train.model = %MODEL
I0512 23:44:42.441635 139666521167872 gin_utils.py:85] train_script.train.model_dir = %MODEL_DIR
I0512 23:44:42.441664 139666521167872 gin_utils.py:85] train_script.train.partitioner = @moe_partitioning.MoePjitPartitioner()
I0512 23:44:42.441694 139666521167872 gin_utils.py:85] train_script.train.random_seed = %RANDOM_SEED
I0512 23:44:42.441724 139666521167872 gin_utils.py:85] train_script.train.stats_period = 10
I0512 23:44:42.441754 139666521167872 gin_utils.py:85] train_script.train.summarize_config_fn = @gin_utils.summarize_gin_config
I0512 23:44:42.441784 139666521167872 gin_utils.py:85] train_script.train.total_steps = %TRAIN_STEPS
I0512 23:44:42.441814 139666521167872 gin_utils.py:85] train_script.train.train_dataset_cfg = @train/utils.DatasetConfig()
I0512 23:44:42.441843 139666521167872 gin_utils.py:85] train_script.train.train_eval_dataset_cfg = None
I0512 23:44:42.441874 139666521167872 gin_utils.py:85] train_script.train.trainer_cls = @moe_trainer.MoeTrainer
I0512 23:44:42.441903 139666521167872 gin_utils.py:85] train_script.train.use_hardware_rng = %USE_HARDWARE_RNG
I0512 23:44:42.441933 139666521167872 gin_utils.py:85] 
I0512 23:44:42.441963 139666521167872 gin_utils.py:85] # Parameters for trainer.Trainer:
I0512 23:44:42.441993 139666521167872 gin_utils.py:85] # ==============================================================================
I0512 23:44:42.442023 139666521167872 gin_utils.py:85] trainer.Trainer.learning_rate_fn = @utils.create_learning_rate_scheduler()
I0512 23:44:42.442053 139666521167872 gin_utils.py:85] trainer.Trainer.num_microbatches = None
I0512 23:44:42.442083 139666521167872 gin_utils.py:85] 
I0512 23:44:42.442112 139666521167872 gin_utils.py:85] # Parameters for attention_kernel_init/linen.initializers.variance_scaling:
I0512 23:44:42.442142 139666521167872 gin_utils.py:85] # ==============================================================================
I0512 23:44:42.442172 139666521167872 gin_utils.py:85] attention_kernel_init/linen.initializers.variance_scaling.distribution = 'normal'
I0512 23:44:42.442202 139666521167872 gin_utils.py:85] attention_kernel_init/linen.initializers.variance_scaling.mode = 'fan_in'
I0512 23:44:42.442232 139666521167872 gin_utils.py:85] attention_kernel_init/linen.initializers.variance_scaling.scale = %SCALE
I0512 23:44:42.442262 139666521167872 gin_utils.py:85] 
I0512 23:44:42.442292 139666521167872 gin_utils.py:85] # Parameters for expert_kernel_init/linen.initializers.variance_scaling:
I0512 23:44:42.442327 139666521167872 gin_utils.py:85] # ==============================================================================
I0512 23:44:42.442358 139666521167872 gin_utils.py:85] expert_kernel_init/linen.initializers.variance_scaling.distribution = 'normal'
I0512 23:44:42.442389 139666521167872 gin_utils.py:85] expert_kernel_init/linen.initializers.variance_scaling.mode = 'fan_in'
I0512 23:44:42.442419 139666521167872 gin_utils.py:85] expert_kernel_init/linen.initializers.variance_scaling.scale = %SCALE
I0512 23:44:42.442448 139666521167872 gin_utils.py:85] 
I0512 23:44:42.442478 139666521167872 gin_utils.py:85] # Parameters for mlp_kernel_init/linen.initializers.variance_scaling:
I0512 23:44:42.442508 139666521167872 gin_utils.py:85] # ==============================================================================
I0512 23:44:42.442538 139666521167872 gin_utils.py:85] mlp_kernel_init/linen.initializers.variance_scaling.distribution = \
I0512 23:44:42.442569 139666521167872 gin_utils.py:85]     'truncated_normal'
I0512 23:44:42.442600 139666521167872 gin_utils.py:85] mlp_kernel_init/linen.initializers.variance_scaling.mode = 'fan_in'
I0512 23:44:42.676630 140260698863616 resource_reader.py:50] system_path_file_exists:t5x/contrib/moe/configs/runs/pretrain.gin
I0512 23:44:42.447901 139856202532864 gin_utils.py:85] train_script.train.eval_period = 2500
I0512 23:44:42.447932 139856202532864 gin_utils.py:85] train_script.train.eval_steps = 20
I0512 23:44:42.447964 139856202532864 gin_utils.py:85] train_script.train.infer_eval_dataset_cfg = None
I0512 23:44:42.447995 139856202532864 gin_utils.py:85] train_script.train.model = %MODEL
I0512 23:44:42.448025 139856202532864 gin_utils.py:85] train_script.train.model_dir = %MODEL_DIR
I0512 23:44:42.448056 139856202532864 gin_utils.py:85] train_script.train.partitioner = @moe_partitioning.MoePjitPartitioner()
I0512 23:44:42.448088 139856202532864 gin_utils.py:85] train_script.train.random_seed = %RANDOM_SEED
I0512 23:44:42.448118 139856202532864 gin_utils.py:85] train_script.train.stats_period = 10
I0512 23:44:42.448150 139856202532864 gin_utils.py:85] train_script.train.summarize_config_fn = @gin_utils.summarize_gin_config
I0512 23:44:42.448181 139856202532864 gin_utils.py:85] train_script.train.total_steps = %TRAIN_STEPS
I0512 23:44:42.448211 139856202532864 gin_utils.py:85] train_script.train.train_dataset_cfg = @train/utils.DatasetConfig()
I0512 23:44:42.448242 139856202532864 gin_utils.py:85] train_script.train.train_eval_dataset_cfg = None
I0512 23:44:42.448273 139856202532864 gin_utils.py:85] train_script.train.trainer_cls = @moe_trainer.MoeTrainer
I0512 23:44:42.448305 139856202532864 gin_utils.py:85] train_script.train.use_hardware_rng = %USE_HARDWARE_RNG
I0512 23:44:42.448336 139856202532864 gin_utils.py:85] 
I0512 23:44:42.448367 139856202532864 gin_utils.py:85] # Parameters for trainer.Trainer:
I0512 23:44:42.448398 139856202532864 gin_utils.py:85] # ==============================================================================
I0512 23:44:42.448429 139856202532864 gin_utils.py:85] trainer.Trainer.learning_rate_fn = @utils.create_learning_rate_scheduler()
I0512 23:44:42.448460 139856202532864 gin_utils.py:85] trainer.Trainer.num_microbatches = None
I0512 23:44:42.448491 139856202532864 gin_utils.py:85] 
I0512 23:44:42.448522 139856202532864 gin_utils.py:85] # Parameters for attention_kernel_init/linen.initializers.variance_scaling:
I0512 23:44:42.448553 139856202532864 gin_utils.py:85] # ==============================================================================
I0512 23:44:42.448584 139856202532864 gin_utils.py:85] attention_kernel_init/linen.initializers.variance_scaling.distribution = 'normal'
I0512 23:44:42.448617 139856202532864 gin_utils.py:85] attention_kernel_init/linen.initializers.variance_scaling.mode = 'fan_in'
I0512 23:44:42.448649 139856202532864 gin_utils.py:85] attention_kernel_init/linen.initializers.variance_scaling.scale = %SCALE
I0512 23:44:42.448680 139856202532864 gin_utils.py:85] 
I0512 23:44:42.448711 139856202532864 gin_utils.py:85] # Parameters for expert_kernel_init/linen.initializers.variance_scaling:
I0512 23:44:42.448743 139856202532864 gin_utils.py:85] # ==============================================================================
I0512 23:44:42.448774 139856202532864 gin_utils.py:85] expert_kernel_init/linen.initializers.variance_scaling.distribution = 'normal'
I0512 23:44:42.448805 139856202532864 gin_utils.py:85] expert_kernel_init/linen.initializers.variance_scaling.mode = 'fan_in'
I0512 23:44:42.448841 139856202532864 gin_utils.py:85] expert_kernel_init/linen.initializers.variance_scaling.scale = %SCALE
I0512 23:44:42.448874 139856202532864 gin_utils.py:85] 
I0512 23:44:42.448906 139856202532864 gin_utils.py:85] # Parameters for mlp_kernel_init/linen.initializers.variance_scaling:
I0512 23:44:42.448937 139856202532864 gin_utils.py:85] # ==============================================================================
I0512 23:44:42.448969 139856202532864 gin_utils.py:85] mlp_kernel_init/linen.initializers.variance_scaling.distribution = \
I0512 23:44:42.449000 139856202532864 gin_utils.py:85]     'truncated_normal'
I0512 23:44:42.449031 139856202532864 gin_utils.py:85] mlp_kernel_init/linen.initializers.variance_scaling.mode = 'fan_in'
I0512 23:44:42.449062 139856202532864 gin_utils.py:85] mlp_kernel_init/linen.initializers.variance_scaling.scale = %SCALE
I0512 23:44:42.449094 139856202532864 gin_utils.py:85] 
I0512 23:44:42.449125 139856202532864 gin_utils.py:85] # Parameters for output_logits_kernel_init/linen.initializers.variance_scaling:
I0512 23:44:42.449156 139856202532864 gin_utils.py:85] # ==============================================================================
I0512 23:44:42.449187 139856202532864 gin_utils.py:85] output_logits_kernel_init/linen.initializers.variance_scaling.distribution = \
I0512 23:44:42.449218 139856202532864 gin_utils.py:85]     'truncated_normal'
I0512 23:44:42.449249 139856202532864 gin_utils.py:85] output_logits_kernel_init/linen.initializers.variance_scaling.mode = 'fan_in'
I0512 23:44:42.449280 139856202532864 gin_utils.py:85] output_logits_kernel_init/linen.initializers.variance_scaling.scale = %SCALE
I0512 23:44:42.450238 139856202532864 partitioning.py:559] `activation_partitioning_dims` = 2, `parameter_partitioning_dims` = 1
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
I0000 00:00:1715557482.528399  393698 pjrt_api.cc:98] GetPjrtApi was found for tpu at /home/rosinality/openmoe_venv/lib/python3.10/site-packages/libtpu/libtpu.so
I0000 00:00:1715557482.528464  393698 pjrt_api.cc:67] PJRT_Api is set for device type tpu
I0000 00:00:1715557482.528466  393698 pjrt_api.cc:72] PJRT plugin for tpu has PJRT API version 0.30. The framework PJRT API version is 0.30.
I0512 23:44:42.442630 139666521167872 gin_utils.py:85] mlp_kernel_init/linen.initializers.variance_scaling.scale = %SCALE
I0512 23:44:42.442660 139666521167872 gin_utils.py:85] 
I0512 23:44:42.442690 139666521167872 gin_utils.py:85] # Parameters for output_logits_kernel_init/linen.initializers.variance_scaling:
I0512 23:44:42.442719 139666521167872 gin_utils.py:85] # ==============================================================================
I0512 23:44:42.442749 139666521167872 gin_utils.py:85] output_logits_kernel_init/linen.initializers.variance_scaling.distribution = \
I0512 23:44:42.442780 139666521167872 gin_utils.py:85]     'truncated_normal'
I0512 23:44:42.442809 139666521167872 gin_utils.py:85] output_logits_kernel_init/linen.initializers.variance_scaling.mode = 'fan_in'
I0512 23:44:42.442839 139666521167872 gin_utils.py:85] output_logits_kernel_init/linen.initializers.variance_scaling.scale = %SCALE
I0512 23:44:42.443732 139666521167872 partitioning.py:559] `activation_partitioning_dims` = 2, `parameter_partitioning_dims` = 1
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
I0000 00:00:1715557482.517562  368954 pjrt_api.cc:98] GetPjrtApi was found for tpu at /home/rosinality/openmoe_venv/lib/python3.10/site-packages/libtpu/libtpu.so
I0000 00:00:1715557482.517624  368954 pjrt_api.cc:67] PJRT_Api is set for device type tpu
I0000 00:00:1715557482.517626  368954 pjrt_api.cc:72] PJRT plugin for tpu has PJRT API version 0.30. The framework PJRT API version is 0.30.
I0512 23:44:42.448026 140193652815872 gin_utils.py:85] train_script.train.eval_period = 2500
I0512 23:44:42.448058 140193652815872 gin_utils.py:85] train_script.train.eval_steps = 20
I0512 23:44:42.448090 140193652815872 gin_utils.py:85] train_script.train.infer_eval_dataset_cfg = None
I0512 23:44:42.448122 140193652815872 gin_utils.py:85] train_script.train.model = %MODEL
I0512 23:44:42.448154 140193652815872 gin_utils.py:85] train_script.train.model_dir = %MODEL_DIR
I0512 23:44:42.448186 140193652815872 gin_utils.py:85] train_script.train.partitioner = @moe_partitioning.MoePjitPartitioner()
I0512 23:44:42.448218 140193652815872 gin_utils.py:85] train_script.train.random_seed = %RANDOM_SEED
I0512 23:44:42.448250 140193652815872 gin_utils.py:85] train_script.train.stats_period = 10
I0512 23:44:42.448281 140193652815872 gin_utils.py:85] train_script.train.summarize_config_fn = @gin_utils.summarize_gin_config
I0512 23:44:42.448336 140193652815872 gin_utils.py:85] train_script.train.total_steps = %TRAIN_STEPS
I0512 23:44:42.448372 140193652815872 gin_utils.py:85] train_script.train.train_dataset_cfg = @train/utils.DatasetConfig()
I0512 23:44:42.448405 140193652815872 gin_utils.py:85] train_script.train.train_eval_dataset_cfg = None
I0512 23:44:42.448437 140193652815872 gin_utils.py:85] train_script.train.trainer_cls = @moe_trainer.MoeTrainer
I0512 23:44:42.448468 140193652815872 gin_utils.py:85] train_script.train.use_hardware_rng = %USE_HARDWARE_RNG
I0512 23:44:42.448501 140193652815872 gin_utils.py:85] 
I0512 23:44:42.448532 140193652815872 gin_utils.py:85] # Parameters for trainer.Trainer:
I0512 23:44:42.448564 140193652815872 gin_utils.py:85] # ==============================================================================
I0512 23:44:42.448600 140193652815872 gin_utils.py:85] trainer.Trainer.learning_rate_fn = @utils.create_learning_rate_scheduler()
I0512 23:44:42.448634 140193652815872 gin_utils.py:85] trainer.Trainer.num_microbatches = None
I0512 23:44:42.448666 140193652815872 gin_utils.py:85] 
I0512 23:44:42.448698 140193652815872 gin_utils.py:85] # Parameters for attention_kernel_init/linen.initializers.variance_scaling:
I0512 23:44:42.448729 140193652815872 gin_utils.py:85] # ==============================================================================
I0512 23:44:42.448761 140193652815872 gin_utils.py:85] attention_kernel_init/linen.initializers.variance_scaling.distribution = 'normal'
I0512 23:44:42.448793 140193652815872 gin_utils.py:85] attention_kernel_init/linen.initializers.variance_scaling.mode = 'fan_in'
I0512 23:44:42.448825 140193652815872 gin_utils.py:85] attention_kernel_init/linen.initializers.variance_scaling.scale = %SCALE
I0512 23:44:42.448856 140193652815872 gin_utils.py:85] 
I0512 23:44:42.448888 140193652815872 gin_utils.py:85] # Parameters for expert_kernel_init/linen.initializers.variance_scaling:
I0512 23:44:42.448920 140193652815872 gin_utils.py:85] # ==============================================================================
I0512 23:44:42.448951 140193652815872 gin_utils.py:85] expert_kernel_init/linen.initializers.variance_scaling.distribution = 'normal'
I0512 23:44:42.448983 140193652815872 gin_utils.py:85] expert_kernel_init/linen.initializers.variance_scaling.mode = 'fan_in'
I0512 23:44:42.449015 140193652815872 gin_utils.py:85] expert_kernel_init/linen.initializers.variance_scaling.scale = %SCALE
I0512 23:44:42.449047 140193652815872 gin_utils.py:85] 
I0512 23:44:42.449079 140193652815872 gin_utils.py:85] # Parameters for mlp_kernel_init/linen.initializers.variance_scaling:
I0512 23:44:42.449111 140193652815872 gin_utils.py:85] # ==============================================================================
I0512 23:44:42.449143 140193652815872 gin_utils.py:85] mlp_kernel_init/linen.initializers.variance_scaling.distribution = \
I0512 23:44:42.449175 140193652815872 gin_utils.py:85]     'truncated_normal'
I0512 23:44:42.449207 140193652815872 gin_utils.py:85] mlp_kernel_init/linen.initializers.variance_scaling.mode = 'fan_in'
I0512 23:44:42.449239 140193652815872 gin_utils.py:85] mlp_kernel_init/linen.initializers.variance_scaling.scale = %SCALE
I0512 23:44:42.449270 140193652815872 gin_utils.py:85] 
I0512 23:44:42.449302 140193652815872 gin_utils.py:85] # Parameters for output_logits_kernel_init/linen.initializers.variance_scaling:
I0512 23:44:42.449334 140193652815872 gin_utils.py:85] # ==============================================================================
I0512 23:44:42.449368 140193652815872 gin_utils.py:85] output_logits_kernel_init/linen.initializers.variance_scaling.distribution = \
I0512 23:44:42.449400 140193652815872 gin_utils.py:85]     'truncated_normal'
I0512 23:44:42.449432 140193652815872 gin_utils.py:85] output_logits_kernel_init/linen.initializers.variance_scaling.mode = 'fan_in'
I0512 23:44:42.449464 140193652815872 gin_utils.py:85] output_logits_kernel_init/linen.initializers.variance_scaling.scale = %SCALE
I0512 23:44:42.450383 140193652815872 partitioning.py:559] `activation_partitioning_dims` = 2, `parameter_partitioning_dims` = 1
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
I0000 00:00:1715557482.526980  374664 pjrt_api.cc:98] GetPjrtApi was found for tpu at /home/rosinality/openmoe_venv/lib/python3.10/site-packages/libtpu/libtpu.so
I0000 00:00:1715557482.527040  374664 pjrt_api.cc:67] PJRT_Api is set for device type tpu
I0000 00:00:1715557482.527043  374664 pjrt_api.cc:72] PJRT plugin for tpu has PJRT API version 0.30. The framework PJRT API version is 0.30.
I0512 23:44:42.707284 140180528838656 resource_reader.py:50] system_path_file_exists:t5x/contrib/moe/configs/runs/pretrain.gin
I0512 23:44:42.568498 140643590531072 gin_utils.py:85] dense.MlpBlock.output_axis_name = 'mlp_embed'
I0512 23:44:42.568529 140643590531072 gin_utils.py:85] dense.MlpBlock.use_bias = False
I0512 23:44:42.568561 140643590531072 gin_utils.py:85] 
I0512 23:44:42.568592 140643590531072 gin_utils.py:85] # Parameters for expert/dense.MlpBlock:
I0512 23:44:42.568623 140643590531072 gin_utils.py:85] # ==============================================================================
I0512 23:44:42.568655 140643590531072 gin_utils.py:85] expert/dense.MlpBlock.activation_partitioning_dims = 1
I0512 23:44:42.568692 140643590531072 gin_utils.py:85] expert/dense.MlpBlock.activations = ('swiglu', 'linear')
I0512 23:44:42.568725 140643590531072 gin_utils.py:85] expert/dense.MlpBlock.bias_init = %BIAS_INIT
I0512 23:44:42.568757 140643590531072 gin_utils.py:85] expert/dense.MlpBlock.data_sharding_constraints = ('expert_replicas', 'mlp')
I0512 23:44:42.568788 140643590531072 gin_utils.py:85] expert/dense.MlpBlock.dtype = %MOE_TRUNCATED_DTYPE
I0512 23:44:42.568819 140643590531072 gin_utils.py:85] expert/dense.MlpBlock.final_dropout_rate = 0.0
I0512 23:44:42.568850 140643590531072 gin_utils.py:85] expert/dense.MlpBlock.input_axis_name = 'mlp_embed'
I0512 23:44:42.568882 140643590531072 gin_utils.py:85] expert/dense.MlpBlock.intermediate_axis_name = 'expert_mlp'
I0512 23:44:42.568913 140643590531072 gin_utils.py:85] expert/dense.MlpBlock.intermediate_dim = %EXPERT_MLP_DIM
I0512 23:44:42.568946 140643590531072 gin_utils.py:85] expert/dense.MlpBlock.intermediate_dropout_rate = %EXPERT_DROPOUT_RATE
I0512 23:44:42.568979 140643590531072 gin_utils.py:85] expert/dense.MlpBlock.kernel_init = \
I0512 23:44:42.569011 140643590531072 gin_utils.py:85]     @expert_kernel_init/linen.initializers.variance_scaling()
I0512 23:44:42.569042 140643590531072 gin_utils.py:85] expert/dense.MlpBlock.output_axis_name = 'mlp_embed'
I0512 23:44:42.569074 140643590531072 gin_utils.py:85] expert/dense.MlpBlock.use_bias = False
I0512 23:44:42.569106 140643590531072 gin_utils.py:85] 
I0512 23:44:42.677002 140260698863616 resource_reader.py:37] gin-config opened resource file:/home/rosinality/t5x/t5x/contrib/moe/configs/runs/pretrain.gin
I0512 23:44:42.677540 140260698863616 resource_reader.py:50] system_path_file_exists:t5x/configs/runs/pretrain.gin
I0512 23:44:42.677857 140260698863616 resource_reader.py:37] gin-config opened resource file:/home/rosinality/t5x/t5x/configs/runs/pretrain.gin
I0512 23:44:42.692073 140260698863616 gin_utils.py:83] Gin Configuration:
I0512 23:44:42.708245 140260698863616 gin_utils.py:85] from __gin__ import dynamic_registration
I0512 23:44:42.708324 140260698863616 gin_utils.py:85] import __main__ as train_script
I0512 23:44:42.708366 140260698863616 gin_utils.py:85] from flax import linen
I0512 23:44:42.708401 140260698863616 gin_utils.py:85] import flaxformer
I0512 23:44:42.708435 140260698863616 gin_utils.py:85] from flaxformer.architectures.moe import moe_architecture
I0512 23:44:42.708469 140260698863616 gin_utils.py:85] from flaxformer.architectures.moe import moe_enums
I0512 23:44:42.708502 140260698863616 gin_utils.py:85] from flaxformer.architectures.moe import moe_layers
I0512 23:44:42.708538 140260698863616 gin_utils.py:85] from flaxformer.architectures.moe import routing
I0512 23:44:42.708571 140260698863616 gin_utils.py:85] from flaxformer.architectures.t5 import t5_architecture
I0512 23:44:42.708604 140260698863616 gin_utils.py:85] from flaxformer.components.attention import dense_attention
I0512 23:44:42.708636 140260698863616 gin_utils.py:85] from flaxformer.components.attention import memory_efficient_attention
I0512 23:44:42.708668 140260698863616 gin_utils.py:85] from flaxformer.components import dense
I0512 23:44:42.708701 140260698863616 gin_utils.py:85] from flaxformer.components import embedding
I0512 23:44:42.708733 140260698863616 gin_utils.py:85] from flaxformer.components import layer_norm
I0512 23:44:42.708765 140260698863616 gin_utils.py:85] from gin import config
I0512 23:44:42.708798 140260698863616 gin_utils.py:85] import seqio
I0512 23:44:42.708830 140260698863616 gin_utils.py:85] import t5.data.mixtures
I0512 23:44:42.708862 140260698863616 gin_utils.py:85] from t5x import adafactor
I0512 23:44:42.708894 140260698863616 gin_utils.py:85] from t5x.contrib.moe import adafactor_utils
I0512 23:44:42.708927 140260698863616 gin_utils.py:85] from t5x.contrib.moe import models
I0512 23:44:42.708959 140260698863616 gin_utils.py:85] from t5x.contrib.moe import partitioning as moe_partitioning
I0512 23:44:42.708991 140260698863616 gin_utils.py:85] from t5x.contrib.moe import trainer as moe_trainer
I0512 23:44:42.709023 140260698863616 gin_utils.py:85] from t5x import gin_utils
I0512 23:44:42.709055 140260698863616 gin_utils.py:85] from t5x import partitioning
I0512 23:44:42.709087 140260698863616 gin_utils.py:85] from t5x import trainer
I0512 23:44:42.709119 140260698863616 gin_utils.py:85] from t5x import utils
I0512 23:44:42.709151 140260698863616 gin_utils.py:85] 
I0512 23:44:42.709182 140260698863616 gin_utils.py:85] # Macros:
I0512 23:44:42.709215 140260698863616 gin_utils.py:85] # ==============================================================================
I0512 23:44:42.709248 140260698863616 gin_utils.py:85] ACTIVATION_DTYPE = 'bfloat16'
I0512 23:44:42.709287 140260698863616 gin_utils.py:85] ACTIVATION_PARTITIONING_DIMS = 1
I0512 23:44:42.709320 140260698863616 gin_utils.py:85] ARCHITECTURE = @t5_architecture.DecoderOnly()
I0512 23:44:42.709352 140260698863616 gin_utils.py:85] AUX_LOSS_FACTOR = 0.01
I0512 23:44:42.709384 140260698863616 gin_utils.py:85] BATCH_SIZE = 384
I0512 23:44:42.709416 140260698863616 gin_utils.py:85] BIAS_INIT = @bias_init/linen.initializers.normal()
I0512 23:44:42.709449 140260698863616 gin_utils.py:85] DECODER_SPARSE_LAYOUT = %flaxformer.architectures.moe.moe_enums.LayerLayout.MIXED
I0512 23:44:42.709481 140260698863616 gin_utils.py:85] DROPOUT_FACTORY = @dropout_factory/linen.Dropout
I0512 23:44:42.709514 140260698863616 gin_utils.py:85] DROPOUT_RATE = 0.0
I0512 23:44:42.709548 140260698863616 gin_utils.py:85] EMBED_DIM = 2048
I0512 23:44:42.709581 140260698863616 gin_utils.py:85] EVAL_EXPERT_CAPACITY_FACTOR = 2.0
I0512 23:44:42.709613 140260698863616 gin_utils.py:85] EXPERT_DROPOUT_RATE = %DROPOUT_RATE
I0512 23:44:42.709645 140260698863616 gin_utils.py:85] EXPERT_MLP_DIM = %MLP_DIM
I0512 23:44:42.709676 140260698863616 gin_utils.py:85] GROUP_SIZE = 4096
I0512 23:44:42.709709 140260698863616 gin_utils.py:85] HEAD_DIM = 128
I0512 23:44:42.709741 140260698863616 gin_utils.py:85] JITTER_NOISE = 0.0
I0512 23:44:42.709773 140260698863616 gin_utils.py:85] LABEL_SMOOTHING = 0.0
I0512 23:44:42.709805 140260698863616 gin_utils.py:85] LOSS_NORMALIZING_FACTOR = 'NUM_REAL_TARGET_TOKENS'
I0512 23:44:42.709836 140260698863616 gin_utils.py:85] MIXTURE_OR_TASK_MODULE = None
I0512 23:44:42.709869 140260698863616 gin_utils.py:85] MIXTURE_OR_TASK_NAME = 'mix_ul2_test'
I0512 23:44:42.709901 140260698863616 gin_utils.py:85] MLP_DIM = 8192
I0512 23:44:42.709933 140260698863616 gin_utils.py:85] MODEL = @models.MoeDecoderOnlyModel()
I0512 23:44:42.709965 140260698863616 gin_utils.py:85] MODEL_DIR = 'gs://rosinality-tpu-bucket/openmoe_8b_ul2/training'
I0512 23:44:42.709997 140260698863616 gin_utils.py:85] MODEL_PARALLEL_SUBMESH = None
I0512 23:44:42.710029 140260698863616 gin_utils.py:85] MOE_TRUNCATED_DTYPE = 'bfloat16'
I0512 23:44:42.710062 140260698863616 gin_utils.py:85] NUM_DECODER_LAYERS = 24
I0512 23:44:42.710093 140260698863616 gin_utils.py:85] NUM_DECODER_SPARSE_LAYERS = 4
I0512 23:44:42.710126 140260698863616 gin_utils.py:85] NUM_EMBEDDINGS = 256384
I0512 23:44:42.710158 140260698863616 gin_utils.py:85] NUM_EXPERT_PARTITIONS = 8
I0512 23:44:42.710190 140260698863616 gin_utils.py:85] NUM_EXPERTS = 8
I0512 23:44:42.710222 140260698863616 gin_utils.py:85] NUM_HEADS = 24
I0512 23:44:42.710259 140260698863616 gin_utils.py:85] NUM_MODEL_PARTITIONS = 4
I0512 23:44:42.710293 140260698863616 gin_utils.py:85] NUM_SELECTED_EXPERTS = 2
I0512 23:44:42.710325 140260698863616 gin_utils.py:85] OPTIMIZER = @adafactor.Adafactor()
I0512 23:44:42.710357 140260698863616 gin_utils.py:85] RANDOM_SEED = None
I0512 23:44:42.710389 140260698863616 gin_utils.py:85] ROUTER_Z_LOSS_FACTOR = 0.0001
I0512 23:44:42.710422 140260698863616 gin_utils.py:85] SCALE = 0.1
I0512 23:44:42.710453 140260698863616 gin_utils.py:85] SHUFFLE_TRAIN_EXAMPLES = True
I0512 23:44:42.710485 140260698863616 gin_utils.py:85] TASK_FEATURE_LENGTHS = {'inputs': 1024, 'targets': 1024}
I0512 23:44:42.710519 140260698863616 gin_utils.py:85] TRAIN_EXPERT_CAPACITY_FACTOR = 1.25
I0512 23:44:42.710553 140260698863616 gin_utils.py:85] TRAIN_STEPS = 500000
I0512 23:44:42.710585 140260698863616 gin_utils.py:85] USE_CACHED_TASKS = False
I0512 23:44:42.710617 140260698863616 gin_utils.py:85] USE_HARDWARE_RNG = False
I0512 23:44:42.710649 140260698863616 gin_utils.py:85] VOCABULARY = @seqio.SentencePieceVocabulary()
I0512 23:44:42.710681 140260698863616 gin_utils.py:85] Z_LOSS = 0.0001
I0512 23:44:42.710739 140260698863616 gin_utils.py:85] 
I0512 23:44:42.710773 140260698863616 gin_utils.py:85] # Parameters for adafactor.Adafactor:
I0512 23:44:42.710806 140260698863616 gin_utils.py:85] # ==============================================================================
I0512 23:44:42.710839 140260698863616 gin_utils.py:85] adafactor.Adafactor.decay_rate = 0.8
I0512 23:44:42.710872 140260698863616 gin_utils.py:85] adafactor.Adafactor.logical_factor_rules = @adafactor_utils.logical_factor_rules()
I0512 23:44:42.710904 140260698863616 gin_utils.py:85] adafactor.Adafactor.step_offset = 0
I0512 23:44:42.710936 140260698863616 gin_utils.py:85] 
I0512 23:44:42.707660 140180528838656 resource_reader.py:37] gin-config opened resource file:/home/rosinality/t5x/t5x/contrib/moe/configs/runs/pretrain.gin
I0512 23:44:42.708165 140180528838656 resource_reader.py:50] system_path_file_exists:t5x/configs/runs/pretrain.gin
I0512 23:44:42.708484 140180528838656 resource_reader.py:37] gin-config opened resource file:/home/rosinality/t5x/t5x/configs/runs/pretrain.gin
I0512 23:44:42.722340 140180528838656 gin_utils.py:83] Gin Configuration:
I0512 23:44:42.738241 140180528838656 gin_utils.py:85] from __gin__ import dynamic_registration
I0512 23:44:42.738311 140180528838656 gin_utils.py:85] import __main__ as train_script
I0512 23:44:42.738353 140180528838656 gin_utils.py:85] from flax import linen
I0512 23:44:42.738389 140180528838656 gin_utils.py:85] import flaxformer
I0512 23:44:42.738423 140180528838656 gin_utils.py:85] from flaxformer.architectures.moe import moe_architecture
I0512 23:44:42.738457 140180528838656 gin_utils.py:85] from flaxformer.architectures.moe import moe_enums
I0512 23:44:42.738498 140180528838656 gin_utils.py:85] from flaxformer.architectures.moe import moe_layers
I0512 23:44:42.738532 140180528838656 gin_utils.py:85] from flaxformer.architectures.moe import routing
I0512 23:44:42.738565 140180528838656 gin_utils.py:85] from flaxformer.architectures.t5 import t5_architecture
I0512 23:44:42.738599 140180528838656 gin_utils.py:85] from flaxformer.components.attention import dense_attention
I0512 23:44:42.738632 140180528838656 gin_utils.py:85] from flaxformer.components.attention import memory_efficient_attention
I0512 23:44:42.738664 140180528838656 gin_utils.py:85] from flaxformer.components import dense
I0512 23:44:42.738697 140180528838656 gin_utils.py:85] from flaxformer.components import embedding
I0512 23:44:42.738729 140180528838656 gin_utils.py:85] from flaxformer.components import layer_norm
I0512 23:44:42.738761 140180528838656 gin_utils.py:85] from gin import config
I0512 23:44:42.738793 140180528838656 gin_utils.py:85] import seqio
I0512 23:44:42.738826 140180528838656 gin_utils.py:85] import t5.data.mixtures
I0512 23:44:42.738857 140180528838656 gin_utils.py:85] from t5x import adafactor
I0512 23:44:42.738889 140180528838656 gin_utils.py:85] from t5x.contrib.moe import adafactor_utils
I0512 23:44:42.738921 140180528838656 gin_utils.py:85] from t5x.contrib.moe import models
I0512 23:44:42.738953 140180528838656 gin_utils.py:85] from t5x.contrib.moe import partitioning
I0512 23:44:42.738985 140180528838656 gin_utils.py:85] from t5x.contrib.moe import trainer as moe_trainer
I0512 23:44:42.739017 140180528838656 gin_utils.py:85] from t5x import gin_utils
I0512 23:44:42.739049 140180528838656 gin_utils.py:85] from t5x import partitioning as partitioning2
I0512 23:44:42.739081 140180528838656 gin_utils.py:85] from t5x import trainer
I0512 23:44:42.739113 140180528838656 gin_utils.py:85] from t5x import utils
I0512 23:44:42.739145 140180528838656 gin_utils.py:85] 
I0512 23:44:42.739176 140180528838656 gin_utils.py:85] # Macros:
I0512 23:44:42.739208 140180528838656 gin_utils.py:85] # ==============================================================================
I0512 23:44:42.739243 140180528838656 gin_utils.py:85] ACTIVATION_DTYPE = 'bfloat16'
I0512 23:44:42.739277 140180528838656 gin_utils.py:85] ACTIVATION_PARTITIONING_DIMS = 1
I0512 23:44:42.739309 140180528838656 gin_utils.py:85] ARCHITECTURE = @t5_architecture.DecoderOnly()
I0512 23:44:42.739341 140180528838656 gin_utils.py:85] AUX_LOSS_FACTOR = 0.01
I0512 23:44:42.739373 140180528838656 gin_utils.py:85] BATCH_SIZE = 384
I0512 23:44:42.739405 140180528838656 gin_utils.py:85] BIAS_INIT = @bias_init/linen.initializers.normal()
I0512 23:44:42.739437 140180528838656 gin_utils.py:85] DECODER_SPARSE_LAYOUT = %flaxformer.architectures.moe.moe_enums.LayerLayout.MIXED
I0512 23:44:42.739475 140180528838656 gin_utils.py:85] DROPOUT_FACTORY = @dropout_factory/linen.Dropout
I0512 23:44:42.739508 140180528838656 gin_utils.py:85] DROPOUT_RATE = 0.0
I0512 23:44:42.739541 140180528838656 gin_utils.py:85] EMBED_DIM = 2048
I0512 23:44:42.739573 140180528838656 gin_utils.py:85] EVAL_EXPERT_CAPACITY_FACTOR = 2.0
I0512 23:44:42.739605 140180528838656 gin_utils.py:85] EXPERT_DROPOUT_RATE = %DROPOUT_RATE
I0512 23:44:42.739636 140180528838656 gin_utils.py:85] EXPERT_MLP_DIM = %MLP_DIM
I0512 23:44:42.739669 140180528838656 gin_utils.py:85] GROUP_SIZE = 4096
I0512 23:44:42.739700 140180528838656 gin_utils.py:85] HEAD_DIM = 128
I0512 23:44:42.739732 140180528838656 gin_utils.py:85] JITTER_NOISE = 0.0
I0512 23:44:42.739764 140180528838656 gin_utils.py:85] LABEL_SMOOTHING = 0.0
I0512 23:44:42.739796 140180528838656 gin_utils.py:85] LOSS_NORMALIZING_FACTOR = 'NUM_REAL_TARGET_TOKENS'
I0512 23:44:42.739827 140180528838656 gin_utils.py:85] MIXTURE_OR_TASK_MODULE = None
I0512 23:44:42.739860 140180528838656 gin_utils.py:85] MIXTURE_OR_TASK_NAME = 'mix_ul2_test'
I0512 23:44:42.739892 140180528838656 gin_utils.py:85] MLP_DIM = 8192
I0512 23:44:42.739923 140180528838656 gin_utils.py:85] MODEL = @models.MoeDecoderOnlyModel()
I0512 23:44:42.739955 140180528838656 gin_utils.py:85] MODEL_DIR = 'gs://rosinality-tpu-bucket/openmoe_8b_ul2/training'
I0512 23:44:42.739987 140180528838656 gin_utils.py:85] MODEL_PARALLEL_SUBMESH = None
I0512 23:44:42.740019 140180528838656 gin_utils.py:85] MOE_TRUNCATED_DTYPE = 'bfloat16'
I0512 23:44:42.740051 140180528838656 gin_utils.py:85] NUM_DECODER_LAYERS = 24
I0512 23:44:42.740083 140180528838656 gin_utils.py:85] NUM_DECODER_SPARSE_LAYERS = 4
I0512 23:44:42.740114 140180528838656 gin_utils.py:85] NUM_EMBEDDINGS = 256384
I0512 23:44:42.740146 140180528838656 gin_utils.py:85] NUM_EXPERT_PARTITIONS = 8
I0512 23:44:42.740178 140180528838656 gin_utils.py:85] NUM_EXPERTS = 8
I0512 23:44:42.740210 140180528838656 gin_utils.py:85] NUM_HEADS = 24
I0512 23:44:42.740245 140180528838656 gin_utils.py:85] NUM_MODEL_PARTITIONS = 4
I0512 23:44:42.740278 140180528838656 gin_utils.py:85] NUM_SELECTED_EXPERTS = 2
I0512 23:44:42.740310 140180528838656 gin_utils.py:85] OPTIMIZER = @adafactor.Adafactor()
I0512 23:44:42.740341 140180528838656 gin_utils.py:85] RANDOM_SEED = None
I0512 23:44:42.740373 140180528838656 gin_utils.py:85] ROUTER_Z_LOSS_FACTOR = 0.0001
I0512 23:44:42.740405 140180528838656 gin_utils.py:85] SCALE = 0.1
I0512 23:44:42.740437 140180528838656 gin_utils.py:85] SHUFFLE_TRAIN_EXAMPLES = True
I0512 23:44:42.740474 140180528838656 gin_utils.py:85] TASK_FEATURE_LENGTHS = {'inputs': 1024, 'targets': 1024}
I0512 23:44:42.740508 140180528838656 gin_utils.py:85] TRAIN_EXPERT_CAPACITY_FACTOR = 1.25
I0512 23:44:42.740540 140180528838656 gin_utils.py:85] TRAIN_STEPS = 500000
I0512 23:44:42.740572 140180528838656 gin_utils.py:85] USE_CACHED_TASKS = False
I0512 23:44:42.740604 140180528838656 gin_utils.py:85] USE_HARDWARE_RNG = False
I0512 23:44:42.740635 140180528838656 gin_utils.py:85] VOCABULARY = @seqio.SentencePieceVocabulary()
I0512 23:44:42.740667 140180528838656 gin_utils.py:85] Z_LOSS = 0.0001
I0512 23:44:42.740699 140180528838656 gin_utils.py:85] 
I0512 23:44:42.740731 140180528838656 gin_utils.py:85] # Parameters for adafactor.Adafactor:
I0512 23:44:42.740763 140180528838656 gin_utils.py:85] # ==============================================================================
I0512 23:44:42.740795 140180528838656 gin_utils.py:85] adafactor.Adafactor.decay_rate = 0.8
I0512 23:44:42.740827 140180528838656 gin_utils.py:85] adafactor.Adafactor.logical_factor_rules = @adafactor_utils.logical_factor_rules()
I0512 23:44:42.740859 140180528838656 gin_utils.py:85] adafactor.Adafactor.step_offset = 0
I0512 23:44:42.740891 140180528838656 gin_utils.py:85] 
I0512 23:44:42.569137 140643590531072 gin_utils.py:85] # Parameters for models.MoeDecoderOnlyModel:
I0512 23:44:42.569169 140643590531072 gin_utils.py:85] # ==============================================================================
I0512 23:44:42.569200 140643590531072 gin_utils.py:85] models.MoeDecoderOnlyModel.aux_loss_factor = %AUX_LOSS_FACTOR
I0512 23:44:42.569231 140643590531072 gin_utils.py:85] models.MoeDecoderOnlyModel.inputs_bidirectional_attention = True
I0512 23:44:42.569263 140643590531072 gin_utils.py:85] models.MoeDecoderOnlyModel.label_smoothing = %LABEL_SMOOTHING
I0512 23:44:42.569294 140643590531072 gin_utils.py:85] models.MoeDecoderOnlyModel.loss_normalizing_factor = %LOSS_NORMALIZING_FACTOR
I0512 23:44:42.569325 140643590531072 gin_utils.py:85] models.MoeDecoderOnlyModel.module = %ARCHITECTURE
I0512 23:44:42.569357 140643590531072 gin_utils.py:85] models.MoeDecoderOnlyModel.optimizer_def = %OPTIMIZER
I0512 23:44:42.569389 140643590531072 gin_utils.py:85] models.MoeDecoderOnlyModel.router_z_loss_factor = %ROUTER_Z_LOSS_FACTOR
I0512 23:44:42.569420 140643590531072 gin_utils.py:85] models.MoeDecoderOnlyModel.vocabulary = %VOCABULARY
I0512 23:44:42.569452 140643590531072 gin_utils.py:85] models.MoeDecoderOnlyModel.z_loss = %Z_LOSS
I0512 23:44:42.569483 140643590531072 gin_utils.py:85] 
I0512 23:44:42.569515 140643590531072 gin_utils.py:85] # Parameters for moe_layers.MoeLayer:
I0512 23:44:42.569546 140643590531072 gin_utils.py:85] # ==============================================================================
I0512 23:44:42.569578 140643590531072 gin_utils.py:85] moe_layers.MoeLayer.dtype = %MOE_TRUNCATED_DTYPE
I0512 23:44:42.569609 140643590531072 gin_utils.py:85] moe_layers.MoeLayer.eval_capacity_factor = %EVAL_EXPERT_CAPACITY_FACTOR
I0512 23:44:42.569641 140643590531072 gin_utils.py:85] moe_layers.MoeLayer.expert = @expert/dense.MlpBlock()
I0512 23:44:42.569673 140643590531072 gin_utils.py:85] moe_layers.MoeLayer.max_group_size = %GROUP_SIZE
I0512 23:44:42.569710 140643590531072 gin_utils.py:85] moe_layers.MoeLayer.num_expert_partitions = %NUM_EXPERT_PARTITIONS
I0512 23:44:42.569742 140643590531072 gin_utils.py:85] moe_layers.MoeLayer.num_experts = %NUM_EXPERTS
I0512 23:44:42.569774 140643590531072 gin_utils.py:85] moe_layers.MoeLayer.num_model_partitions = \
I0512 23:44:42.569805 140643590531072 gin_utils.py:85]     @moe_partitioning.compute_num_model_partitions()
I0512 23:44:42.569837 140643590531072 gin_utils.py:85] moe_layers.MoeLayer.train_capacity_factor = %TRAIN_EXPERT_CAPACITY_FACTOR
I0512 23:44:42.569868 140643590531072 gin_utils.py:85] 
I0512 23:44:42.569900 140643590531072 gin_utils.py:85] # Parameters for sparse_decoder/moe_layers.MoeLayer:
I0512 23:44:42.569931 140643590531072 gin_utils.py:85] # ==============================================================================
I0512 23:44:42.569964 140643590531072 gin_utils.py:85] sparse_decoder/moe_layers.MoeLayer.router = \
I0512 23:44:42.569997 140643590531072 gin_utils.py:85]     @sparse_decoder/routing.TokensChooseMaskedRouter()
I0512 23:44:42.570028 140643590531072 gin_utils.py:85] 
I0512 23:44:42.570059 140643590531072 gin_utils.py:85] # Parameters for moe_partitioning.MoePjitPartitioner:
I0512 23:44:42.570091 140643590531072 gin_utils.py:85] # ==============================================================================
I0512 23:44:42.570122 140643590531072 gin_utils.py:85] moe_partitioning.MoePjitPartitioner.model_parallel_submesh = \
I0512 23:44:42.570153 140643590531072 gin_utils.py:85]     %MODEL_PARALLEL_SUBMESH
I0512 23:44:42.570184 140643590531072 gin_utils.py:85] moe_partitioning.MoePjitPartitioner.num_expert_partitions = %NUM_EXPERT_PARTITIONS
I0512 23:44:42.570215 140643590531072 gin_utils.py:85] moe_partitioning.MoePjitPartitioner.num_partitions = %NUM_MODEL_PARTITIONS
I0512 23:44:42.570247 140643590531072 gin_utils.py:85] 
I0512 23:44:42.570278 140643590531072 gin_utils.py:85] # Parameters for moe_trainer.MoeTrainer:
I0512 23:44:42.570309 140643590531072 gin_utils.py:85] # ==============================================================================
I0512 23:44:42.570341 140643590531072 gin_utils.py:85] moe_trainer.MoeTrainer.learning_rate_fn = @utils.create_learning_rate_scheduler()
I0512 23:44:42.570373 140643590531072 gin_utils.py:85] moe_trainer.MoeTrainer.num_expert_partitions = %NUM_EXPERT_PARTITIONS
I0512 23:44:42.570404 140643590531072 gin_utils.py:85] moe_trainer.MoeTrainer.num_microbatches = 8
I0512 23:44:42.570436 140643590531072 gin_utils.py:85] 
I0512 23:44:42.570467 140643590531072 gin_utils.py:85] # Parameters for dense_attention.MultiHeadDotProductAttention:
I0512 23:44:42.570499 140643590531072 gin_utils.py:85] # ==============================================================================
I0512 23:44:42.570531 140643590531072 gin_utils.py:85] dense_attention.MultiHeadDotProductAttention.bias_init = %BIAS_INIT
I0512 23:44:42.570562 140643590531072 gin_utils.py:85] dense_attention.MultiHeadDotProductAttention.broadcast_dropout = True
I0512 23:44:42.570594 140643590531072 gin_utils.py:85] dense_attention.MultiHeadDotProductAttention.dropout_rate = %DROPOUT_RATE
I0512 23:44:42.570626 140643590531072 gin_utils.py:85] dense_attention.MultiHeadDotProductAttention.dtype = %ACTIVATION_DTYPE
I0512 23:44:42.570657 140643590531072 gin_utils.py:85] dense_attention.MultiHeadDotProductAttention.head_dim = %HEAD_DIM
I0512 23:44:42.570694 140643590531072 gin_utils.py:85] dense_attention.MultiHeadDotProductAttention.kernel_init = \
I0512 23:44:42.570727 140643590531072 gin_utils.py:85]     @attention_kernel_init/linen.initializers.variance_scaling()
I0512 23:44:42.570759 140643590531072 gin_utils.py:85] dense_attention.MultiHeadDotProductAttention.num_heads = %NUM_HEADS
I0512 23:44:42.570791 140643590531072 gin_utils.py:85] dense_attention.MultiHeadDotProductAttention.use_bias = False
I0512 23:44:42.570822 140643590531072 gin_utils.py:85] dense_attention.MultiHeadDotProductAttention.use_rotary_embedding = True
I0512 23:44:42.570854 140643590531072 gin_utils.py:85] 
I0512 23:44:42.570886 140643590531072 gin_utils.py:85] # Parameters for bias_init/linen.initializers.normal:
I0512 23:44:42.570917 140643590531072 gin_utils.py:85] # ==============================================================================
I0512 23:44:42.570950 140643590531072 gin_utils.py:85] bias_init/linen.initializers.normal.stddev = 1e-06
I0512 23:44:42.570984 140643590531072 gin_utils.py:85] 
I0512 23:44:42.571016 140643590531072 gin_utils.py:85] # Parameters for router_init/linen.initializers.normal:
I0512 23:44:42.571048 140643590531072 gin_utils.py:85] # ==============================================================================
I0512 23:44:42.571079 140643590531072 gin_utils.py:85] router_init/linen.initializers.normal.stddev = 0.02
I0512 23:44:42.571110 140643590531072 gin_utils.py:85] 
I0512 23:44:42.571141 140643590531072 gin_utils.py:85] # Parameters for token_embedder_init/linen.initializers.normal:
I0512 23:44:42.571172 140643590531072 gin_utils.py:85] # ==============================================================================
I0512 23:44:42.571204 140643590531072 gin_utils.py:85] token_embedder_init/linen.initializers.normal.stddev = 1.0
I0512 23:44:42.571236 140643590531072 gin_utils.py:85] 
I0512 23:44:42.571267 140643590531072 gin_utils.py:85] # Parameters for partitioning.PjitPartitioner:
I0512 23:44:42.571298 140643590531072 gin_utils.py:85] # ==============================================================================
I0512 23:44:42.571329 140643590531072 gin_utils.py:85] partitioning.PjitPartitioner.logical_axis_rules = \
I0512 23:44:42.571361 140643590531072 gin_utils.py:85]     @partitioning.standard_logical_axis_rules()
I0512 23:44:42.571392 140643590531072 gin_utils.py:85] partitioning.PjitPartitioner.model_parallel_submesh = None
I0512 23:44:42.571423 140643590531072 gin_utils.py:85] partitioning.PjitPartitioner.num_partitions = 1
I0512 23:44:42.571455 140643590531072 gin_utils.py:85] 
I0512 23:44:42.571486 140643590531072 gin_utils.py:85] # Parameters for utils.RestoreCheckpointConfig:
I0512 23:44:42.571518 140643590531072 gin_utils.py:85] # ==============================================================================
I0512 23:44:42.571549 140643590531072 gin_utils.py:85] utils.RestoreCheckpointConfig.path = []
I0512 23:44:42.571600 140643590531072 gin_utils.py:85] 
I0512 23:44:42.571635 140643590531072 gin_utils.py:85] # Parameters for routing.RouterWeights:
I0512 23:44:42.571666 140643590531072 gin_utils.py:85] # ==============================================================================
I0512 23:44:42.571703 140643590531072 gin_utils.py:85] routing.RouterWeights.bias_init = %BIAS_INIT
I0512 23:44:42.571735 140643590531072 gin_utils.py:85] routing.RouterWeights.dtype = 'float32'
I0512 23:44:42.571767 140643590531072 gin_utils.py:85] routing.RouterWeights.kernel_init = @router_init/linen.initializers.normal()
I0512 23:44:42.571798 140643590531072 gin_utils.py:85] routing.RouterWeights.use_bias = False
I0512 23:44:42.571829 140643590531072 gin_utils.py:85] 
I0512 23:44:42.571860 140643590531072 gin_utils.py:85] # Parameters for utils.SaveCheckpointConfig:
I0512 23:44:42.571892 140643590531072 gin_utils.py:85] # ==============================================================================
I0512 23:44:42.571923 140643590531072 gin_utils.py:85] utils.SaveCheckpointConfig.dtype = 'float32'
I0512 23:44:42.571956 140643590531072 gin_utils.py:85] utils.SaveCheckpointConfig.keep = 20
I0512 23:44:42.571988 140643590531072 gin_utils.py:85] utils.SaveCheckpointConfig.period = 2500
I0512 23:44:42.572020 140643590531072 gin_utils.py:85] utils.SaveCheckpointConfig.save_dataset = False
I0512 23:44:42.572051 140643590531072 gin_utils.py:85] 
I0512 23:44:42.572082 140643590531072 gin_utils.py:85] # Parameters for seqio.SentencePieceVocabulary:
I0512 23:44:42.572114 140643590531072 gin_utils.py:85] # ==============================================================================
I0512 23:44:42.572145 140643590531072 gin_utils.py:85] seqio.SentencePieceVocabulary.extra_ids = 300
I0512 23:44:42.572177 140643590531072 gin_utils.py:85] seqio.SentencePieceVocabulary.sentencepiece_model_file = \
I0512 23:44:42.572208 140643590531072 gin_utils.py:85]     'gs://rosinality-tpu-bucket/sentencepiece.model'
I0512 23:44:42.572239 140643590531072 gin_utils.py:85] 
I0512 23:44:42.572271 140643590531072 gin_utils.py:85] # Parameters for moe_architecture.SparseDecoder:
I0512 23:44:42.572302 140643590531072 gin_utils.py:85] # ==============================================================================
I0512 23:44:42.572334 140643590531072 gin_utils.py:85] moe_architecture.SparseDecoder.dropout_factory = %DROPOUT_FACTORY
I0512 23:44:42.572365 140643590531072 gin_utils.py:85] moe_architecture.SparseDecoder.dtype = %ACTIVATION_DTYPE
I0512 23:44:42.572396 140643590531072 gin_utils.py:85] moe_architecture.SparseDecoder.layer_factory = @t5_architecture.DecoderLayer
I0512 23:44:42.572428 140643590531072 gin_utils.py:85] moe_architecture.SparseDecoder.layer_norm_factory = @layer_norm.T5LayerNorm
I0512 23:44:42.572459 140643590531072 gin_utils.py:85] moe_architecture.SparseDecoder.num_layers = %NUM_DECODER_LAYERS
I0512 23:44:42.572491 140643590531072 gin_utils.py:85] moe_architecture.SparseDecoder.num_sparse_layers = %NUM_DECODER_SPARSE_LAYERS
I0512 23:44:42.572522 140643590531072 gin_utils.py:85] moe_architecture.SparseDecoder.output_logits_factory = \
I0512 23:44:42.572553 140643590531072 gin_utils.py:85]     @output_logits/dense.DenseGeneral
I0512 23:44:42.572584 140643590531072 gin_utils.py:85] moe_architecture.SparseDecoder.sparse_layer_factory = \
I0512 23:44:42.572616 140643590531072 gin_utils.py:85]     @moe_architecture.SparseDecoderLayer
I0512 23:44:42.572647 140643590531072 gin_utils.py:85] moe_architecture.SparseDecoder.sparse_layout = %DECODER_SPARSE_LAYOUT
I0512 23:44:42.572678 140643590531072 gin_utils.py:85] 
I0512 23:44:42.572717 140643590531072 gin_utils.py:85] # Parameters for moe_architecture.SparseDecoderLayer:
I0512 23:44:42.572749 140643590531072 gin_utils.py:85] # ==============================================================================
I0512 23:44:42.572781 140643590531072 gin_utils.py:85] moe_architecture.SparseDecoderLayer.activation_partitioning_dims = \
I0512 23:44:42.572812 140643590531072 gin_utils.py:85]     %ACTIVATION_PARTITIONING_DIMS
I0512 23:44:42.572843 140643590531072 gin_utils.py:85] moe_architecture.SparseDecoderLayer.dropout_factory = %DROPOUT_FACTORY
I0512 23:44:42.572875 140643590531072 gin_utils.py:85] moe_architecture.SparseDecoderLayer.encoder_decoder_attention = None
I0512 23:44:42.572907 140643590531072 gin_utils.py:85] moe_architecture.SparseDecoderLayer.extra_mlp = @dense.MlpBlock()
I0512 23:44:42.572940 140643590531072 gin_utils.py:85] moe_architecture.SparseDecoderLayer.layer_norm_factory = @layer_norm.T5LayerNorm
I0512 23:44:42.572974 140643590531072 gin_utils.py:85] moe_architecture.SparseDecoderLayer.mlp = @sparse_decoder/moe_layers.MoeLayer()
I0512 23:44:42.573006 140643590531072 gin_utils.py:85] moe_architecture.SparseDecoderLayer.scanned = False
I0512 23:44:42.573038 140643590531072 gin_utils.py:85] moe_architecture.SparseDecoderLayer.self_attention = \
I0512 23:44:42.573069 140643590531072 gin_utils.py:85]     @dense_attention.MultiHeadDotProductAttention()
I0512 23:44:42.573101 140643590531072 gin_utils.py:85] 
I0512 23:44:42.573134 140643590531072 gin_utils.py:85] # Parameters for layer_norm.T5LayerNorm:
I0512 23:44:42.573165 140643590531072 gin_utils.py:85] # ==============================================================================
I0512 23:44:42.573197 140643590531072 gin_utils.py:85] layer_norm.T5LayerNorm.dtype = %ACTIVATION_DTYPE
I0512 23:44:42.573229 140643590531072 gin_utils.py:85] 
I0512 23:44:42.573261 140643590531072 gin_utils.py:85] # Parameters for routing.TokensChooseMaskedRouter:
I0512 23:44:42.573292 140643590531072 gin_utils.py:85] # ==============================================================================
I0512 23:44:42.573324 140643590531072 gin_utils.py:85] routing.TokensChooseMaskedRouter.dtype = 'float32'
I0512 23:44:42.573355 140643590531072 gin_utils.py:85] routing.TokensChooseMaskedRouter.ignore_padding_tokens = False
I0512 23:44:42.573387 140643590531072 gin_utils.py:85] routing.TokensChooseMaskedRouter.jitter_noise = %JITTER_NOISE
I0512 23:44:42.573419 140643590531072 gin_utils.py:85] routing.TokensChooseMaskedRouter.num_selected_experts = %NUM_SELECTED_EXPERTS
I0512 23:44:42.573451 140643590531072 gin_utils.py:85] routing.TokensChooseMaskedRouter.router_weights = @routing.RouterWeights()
I0512 23:44:42.573482 140643590531072 gin_utils.py:85] 
I0512 23:44:42.573513 140643590531072 gin_utils.py:85] # Parameters for sparse_decoder/routing.TokensChooseMaskedRouter:
I0512 23:44:42.573544 140643590531072 gin_utils.py:85] # ==============================================================================
I0512 23:44:42.573576 140643590531072 gin_utils.py:85] sparse_decoder/routing.TokensChooseMaskedRouter.batch_prioritized_routing = False
I0512 23:44:42.573607 140643590531072 gin_utils.py:85] 
I0512 23:44:42.573639 140643590531072 gin_utils.py:85] # Parameters for sparse_encoder/routing.TokensChooseMaskedRouter:
I0512 23:44:42.573671 140643590531072 gin_utils.py:85] # ==============================================================================
I0512 23:44:42.573709 140643590531072 gin_utils.py:85] sparse_encoder/routing.TokensChooseMaskedRouter.batch_prioritized_routing = False
I0512 23:44:42.573742 140643590531072 gin_utils.py:85] 
I0512 23:44:42.573775 140643590531072 gin_utils.py:85] # Parameters for train_script.train:
I0512 23:44:42.573806 140643590531072 gin_utils.py:85] # ==============================================================================
I0512 23:44:42.573838 140643590531072 gin_utils.py:85] train_script.train.checkpoint_cfg = @utils.CheckpointConfig()
I0512 23:44:42.573870 140643590531072 gin_utils.py:85] train_script.train.eval_period = 2500
I0512 23:44:42.573901 140643590531072 gin_utils.py:85] train_script.train.eval_steps = 20
I0512 23:44:42.710968 140260698863616 gin_utils.py:85] # Parameters for utils.CheckpointConfig:
I0512 23:44:42.711001 140260698863616 gin_utils.py:85] # ==============================================================================
I0512 23:44:42.711033 140260698863616 gin_utils.py:85] utils.CheckpointConfig.restore = @utils.RestoreCheckpointConfig()
I0512 23:44:42.711065 140260698863616 gin_utils.py:85] utils.CheckpointConfig.save = @utils.SaveCheckpointConfig()
I0512 23:44:42.711098 140260698863616 gin_utils.py:85] 
I0512 23:44:42.711130 140260698863616 gin_utils.py:85] # Parameters for moe_partitioning.compute_num_model_partitions:
I0512 23:44:42.711162 140260698863616 gin_utils.py:85] # ==============================================================================
I0512 23:44:42.711195 140260698863616 gin_utils.py:85] moe_partitioning.compute_num_model_partitions.model_parallel_submesh = \
I0512 23:44:42.711227 140260698863616 gin_utils.py:85]     %MODEL_PARALLEL_SUBMESH
I0512 23:44:42.711266 140260698863616 gin_utils.py:85] moe_partitioning.compute_num_model_partitions.num_model_partitions = \
I0512 23:44:42.711300 140260698863616 gin_utils.py:85]     %NUM_MODEL_PARTITIONS
I0512 23:44:42.711333 140260698863616 gin_utils.py:85] 
I0512 23:44:42.711365 140260698863616 gin_utils.py:85] # Parameters for utils.create_learning_rate_scheduler:
I0512 23:44:42.711397 140260698863616 gin_utils.py:85] # ==============================================================================
I0512 23:44:42.711429 140260698863616 gin_utils.py:85] utils.create_learning_rate_scheduler.base_learning_rate = 1.0
I0512 23:44:42.711462 140260698863616 gin_utils.py:85] utils.create_learning_rate_scheduler.factors = 'constant * rsqrt_decay'
I0512 23:44:42.711494 140260698863616 gin_utils.py:85] utils.create_learning_rate_scheduler.warmup_steps = 10000
I0512 23:44:42.711529 140260698863616 gin_utils.py:85] 
I0512 23:44:42.711562 140260698863616 gin_utils.py:85] # Parameters for train/utils.DatasetConfig:
I0512 23:44:42.711594 140260698863616 gin_utils.py:85] # ==============================================================================
I0512 23:44:42.711627 140260698863616 gin_utils.py:85] train/utils.DatasetConfig.batch_size = %BATCH_SIZE
I0512 23:44:42.711660 140260698863616 gin_utils.py:85] train/utils.DatasetConfig.mixture_or_task_name = %MIXTURE_OR_TASK_NAME
I0512 23:44:42.711692 140260698863616 gin_utils.py:85] train/utils.DatasetConfig.module = %MIXTURE_OR_TASK_MODULE
I0512 23:44:42.711724 140260698863616 gin_utils.py:85] train/utils.DatasetConfig.pack = True
I0512 23:44:42.711756 140260698863616 gin_utils.py:85] train/utils.DatasetConfig.seed = None
I0512 23:44:42.711789 140260698863616 gin_utils.py:85] train/utils.DatasetConfig.shuffle = %SHUFFLE_TRAIN_EXAMPLES
I0512 23:44:42.711821 140260698863616 gin_utils.py:85] train/utils.DatasetConfig.split = 'train'
I0512 23:44:42.711853 140260698863616 gin_utils.py:85] train/utils.DatasetConfig.task_feature_lengths = %TASK_FEATURE_LENGTHS
I0512 23:44:42.711885 140260698863616 gin_utils.py:85] train/utils.DatasetConfig.use_cached = %USE_CACHED_TASKS
I0512 23:44:42.711918 140260698863616 gin_utils.py:85] 
I0512 23:44:42.711950 140260698863616 gin_utils.py:85] # Parameters for train_eval/utils.DatasetConfig:
I0512 23:44:42.711982 140260698863616 gin_utils.py:85] # ==============================================================================
I0512 23:44:42.712014 140260698863616 gin_utils.py:85] train_eval/utils.DatasetConfig.batch_size = 128
I0512 23:44:42.712047 140260698863616 gin_utils.py:85] train_eval/utils.DatasetConfig.mixture_or_task_name = %MIXTURE_OR_TASK_NAME
I0512 23:44:42.712079 140260698863616 gin_utils.py:85] train_eval/utils.DatasetConfig.module = %MIXTURE_OR_TASK_MODULE
I0512 23:44:42.712111 140260698863616 gin_utils.py:85] train_eval/utils.DatasetConfig.pack = True
I0512 23:44:42.712143 140260698863616 gin_utils.py:85] train_eval/utils.DatasetConfig.seed = 42
I0512 23:44:42.712176 140260698863616 gin_utils.py:85] train_eval/utils.DatasetConfig.shuffle = False
I0512 23:44:42.712208 140260698863616 gin_utils.py:85] train_eval/utils.DatasetConfig.split = 'validation'
I0512 23:44:42.712240 140260698863616 gin_utils.py:85] train_eval/utils.DatasetConfig.task_feature_lengths = %TASK_FEATURE_LENGTHS
I0512 23:44:42.712279 140260698863616 gin_utils.py:85] train_eval/utils.DatasetConfig.use_cached = %USE_CACHED_TASKS
I0512 23:44:42.712312 140260698863616 gin_utils.py:85] 
I0512 23:44:42.712344 140260698863616 gin_utils.py:85] # Parameters for t5_architecture.DecoderLayer:
I0512 23:44:42.712376 140260698863616 gin_utils.py:85] # ==============================================================================
I0512 23:44:42.712409 140260698863616 gin_utils.py:85] t5_architecture.DecoderLayer.activation_partitioning_dims = \
I0512 23:44:42.712441 140260698863616 gin_utils.py:85]     %ACTIVATION_PARTITIONING_DIMS
I0512 23:44:42.712473 140260698863616 gin_utils.py:85] t5_architecture.DecoderLayer.dropout_factory = %DROPOUT_FACTORY
I0512 23:44:42.712506 140260698863616 gin_utils.py:85] t5_architecture.DecoderLayer.encoder_decoder_attention = None
I0512 23:44:42.712540 140260698863616 gin_utils.py:85] t5_architecture.DecoderLayer.layer_norm_factory = @layer_norm.T5LayerNorm
I0512 23:44:42.712573 140260698863616 gin_utils.py:85] t5_architecture.DecoderLayer.mlp = @dense.MlpBlock()
I0512 23:44:42.712605 140260698863616 gin_utils.py:85] t5_architecture.DecoderLayer.scanned = False
I0512 23:44:42.712637 140260698863616 gin_utils.py:85] t5_architecture.DecoderLayer.self_attention = \
I0512 23:44:42.712669 140260698863616 gin_utils.py:85]     @dense_attention.MultiHeadDotProductAttention()
I0512 23:44:42.712702 140260698863616 gin_utils.py:85] 
I0512 23:44:42.712734 140260698863616 gin_utils.py:85] # Parameters for t5_architecture.DecoderOnly:
I0512 23:44:42.712766 140260698863616 gin_utils.py:85] # ==============================================================================
I0512 23:44:42.712799 140260698863616 gin_utils.py:85] t5_architecture.DecoderOnly.decoder_factory = @moe_architecture.SparseDecoder
I0512 23:44:42.712831 140260698863616 gin_utils.py:85] t5_architecture.DecoderOnly.dtype = %ACTIVATION_DTYPE
I0512 23:44:42.712863 140260698863616 gin_utils.py:85] t5_architecture.DecoderOnly.shared_token_embedder_factory = @embedding.Embed
I0512 23:44:42.712895 140260698863616 gin_utils.py:85] 
I0512 23:44:42.712927 140260698863616 gin_utils.py:85] # Parameters for output_logits/dense.DenseGeneral:
I0512 23:44:42.712959 140260698863616 gin_utils.py:85] # ==============================================================================
I0512 23:44:42.712992 140260698863616 gin_utils.py:85] output_logits/dense.DenseGeneral.bias_init = %BIAS_INIT
I0512 23:44:42.713024 140260698863616 gin_utils.py:85] output_logits/dense.DenseGeneral.dtype = 'float32'
I0512 23:44:42.713056 140260698863616 gin_utils.py:85] output_logits/dense.DenseGeneral.features = %NUM_EMBEDDINGS
I0512 23:44:42.713088 140260698863616 gin_utils.py:85] output_logits/dense.DenseGeneral.kernel_axis_names = ['embed', 'vocab']
I0512 23:44:42.713120 140260698863616 gin_utils.py:85] output_logits/dense.DenseGeneral.kernel_init = \
I0512 23:44:42.713153 140260698863616 gin_utils.py:85]     @output_logits_kernel_init/linen.initializers.variance_scaling()
I0512 23:44:42.713185 140260698863616 gin_utils.py:85] output_logits/dense.DenseGeneral.use_bias = False
I0512 23:44:42.713218 140260698863616 gin_utils.py:85] 
I0512 23:44:42.713254 140260698863616 gin_utils.py:85] # Parameters for dropout_factory/linen.Dropout:
I0512 23:44:42.713289 140260698863616 gin_utils.py:85] # ==============================================================================
I0512 23:44:42.713322 140260698863616 gin_utils.py:85] dropout_factory/linen.Dropout.broadcast_dims = (-2,)
I0512 23:44:42.713354 140260698863616 gin_utils.py:85] dropout_factory/linen.Dropout.rate = %DROPOUT_RATE
I0512 23:44:42.713387 140260698863616 gin_utils.py:85] 
I0512 23:44:42.713419 140260698863616 gin_utils.py:85] # Parameters for embedding.Embed:
I0512 23:44:42.713451 140260698863616 gin_utils.py:85] # ==============================================================================
I0512 23:44:42.713483 140260698863616 gin_utils.py:85] embedding.Embed.attend_dtype = 'float32'
I0512 23:44:42.713517 140260698863616 gin_utils.py:85] embedding.Embed.cast_input_dtype = 'int32'
I0512 23:44:42.713551 140260698863616 gin_utils.py:85] embedding.Embed.dtype = %ACTIVATION_DTYPE
I0512 23:44:42.713583 140260698863616 gin_utils.py:85] embedding.Embed.embedding_init = @token_embedder_init/linen.initializers.normal()
I0512 23:44:42.713616 140260698863616 gin_utils.py:85] embedding.Embed.features = %EMBED_DIM
I0512 23:44:42.713648 140260698863616 gin_utils.py:85] embedding.Embed.name = 'token_embedder'
I0512 23:44:42.713680 140260698863616 gin_utils.py:85] embedding.Embed.num_embeddings = %NUM_EMBEDDINGS
I0512 23:44:42.713712 140260698863616 gin_utils.py:85] embedding.Embed.one_hot = True
I0512 23:44:42.713745 140260698863616 gin_utils.py:85] 
I0512 23:44:42.713777 140260698863616 gin_utils.py:85] # Parameters for dense.MlpBlock:
I0512 23:44:42.713809 140260698863616 gin_utils.py:85] # ==============================================================================
I0512 23:44:42.713842 140260698863616 gin_utils.py:85] dense.MlpBlock.activations = ('swiglu', 'linear')
I0512 23:44:42.713874 140260698863616 gin_utils.py:85] dense.MlpBlock.bias_init = %BIAS_INIT
I0512 23:44:42.713907 140260698863616 gin_utils.py:85] dense.MlpBlock.dtype = %ACTIVATION_DTYPE
I0512 23:44:42.713939 140260698863616 gin_utils.py:85] dense.MlpBlock.final_dropout_rate = 0
I0512 23:44:42.713971 140260698863616 gin_utils.py:85] dense.MlpBlock.input_axis_name = 'mlp_embed'
I0512 23:44:42.714003 140260698863616 gin_utils.py:85] dense.MlpBlock.intermediate_dim = %MLP_DIM
I0512 23:44:42.714035 140260698863616 gin_utils.py:85] dense.MlpBlock.intermediate_dropout_rate = %DROPOUT_RATE
I0512 23:44:42.740923 140180528838656 gin_utils.py:85] # Parameters for utils.CheckpointConfig:
I0512 23:44:42.740955 140180528838656 gin_utils.py:85] # ==============================================================================
I0512 23:44:42.740986 140180528838656 gin_utils.py:85] utils.CheckpointConfig.restore = @utils.RestoreCheckpointConfig()
I0512 23:44:42.741018 140180528838656 gin_utils.py:85] utils.CheckpointConfig.save = @utils.SaveCheckpointConfig()
I0512 23:44:42.741074 140180528838656 gin_utils.py:85] 
I0512 23:44:42.741108 140180528838656 gin_utils.py:85] # Parameters for partitioning.compute_num_model_partitions:
I0512 23:44:42.741140 140180528838656 gin_utils.py:85] # ==============================================================================
I0512 23:44:42.741172 140180528838656 gin_utils.py:85] partitioning.compute_num_model_partitions.model_parallel_submesh = \
I0512 23:44:42.741204 140180528838656 gin_utils.py:85]     %MODEL_PARALLEL_SUBMESH
I0512 23:44:42.741238 140180528838656 gin_utils.py:85] partitioning.compute_num_model_partitions.num_model_partitions = \
I0512 23:44:42.741271 140180528838656 gin_utils.py:85]     %NUM_MODEL_PARTITIONS
I0512 23:44:42.741302 140180528838656 gin_utils.py:85] 
I0512 23:44:42.741334 140180528838656 gin_utils.py:85] # Parameters for utils.create_learning_rate_scheduler:
I0512 23:44:42.741365 140180528838656 gin_utils.py:85] # ==============================================================================
I0512 23:44:42.741402 140180528838656 gin_utils.py:85] utils.create_learning_rate_scheduler.base_learning_rate = 1.0
I0512 23:44:42.741434 140180528838656 gin_utils.py:85] utils.create_learning_rate_scheduler.factors = 'constant * rsqrt_decay'
I0512 23:44:42.741471 140180528838656 gin_utils.py:85] utils.create_learning_rate_scheduler.warmup_steps = 10000
I0512 23:44:42.741505 140180528838656 gin_utils.py:85] 
I0512 23:44:42.741537 140180528838656 gin_utils.py:85] # Parameters for train/utils.DatasetConfig:
I0512 23:44:42.741568 140180528838656 gin_utils.py:85] # ==============================================================================
I0512 23:44:42.741600 140180528838656 gin_utils.py:85] train/utils.DatasetConfig.batch_size = %BATCH_SIZE
I0512 23:44:42.741632 140180528838656 gin_utils.py:85] train/utils.DatasetConfig.mixture_or_task_name = %MIXTURE_OR_TASK_NAME
I0512 23:44:42.741664 140180528838656 gin_utils.py:85] train/utils.DatasetConfig.module = %MIXTURE_OR_TASK_MODULE
I0512 23:44:42.741695 140180528838656 gin_utils.py:85] train/utils.DatasetConfig.pack = True
I0512 23:44:42.741727 140180528838656 gin_utils.py:85] train/utils.DatasetConfig.seed = None
I0512 23:44:42.741759 140180528838656 gin_utils.py:85] train/utils.DatasetConfig.shuffle = %SHUFFLE_TRAIN_EXAMPLES
I0512 23:44:42.741790 140180528838656 gin_utils.py:85] train/utils.DatasetConfig.split = 'train'
I0512 23:44:42.741822 140180528838656 gin_utils.py:85] train/utils.DatasetConfig.task_feature_lengths = %TASK_FEATURE_LENGTHS
I0512 23:44:42.741854 140180528838656 gin_utils.py:85] train/utils.DatasetConfig.use_cached = %USE_CACHED_TASKS
I0512 23:44:42.741885 140180528838656 gin_utils.py:85] 
I0512 23:44:42.741917 140180528838656 gin_utils.py:85] # Parameters for train_eval/utils.DatasetConfig:
I0512 23:44:42.741948 140180528838656 gin_utils.py:85] # ==============================================================================
I0512 23:44:42.741980 140180528838656 gin_utils.py:85] train_eval/utils.DatasetConfig.batch_size = 128
I0512 23:44:42.742012 140180528838656 gin_utils.py:85] train_eval/utils.DatasetConfig.mixture_or_task_name = %MIXTURE_OR_TASK_NAME
I0512 23:44:42.742044 140180528838656 gin_utils.py:85] train_eval/utils.DatasetConfig.module = %MIXTURE_OR_TASK_MODULE
I0512 23:44:42.742076 140180528838656 gin_utils.py:85] train_eval/utils.DatasetConfig.pack = True
I0512 23:44:42.742108 140180528838656 gin_utils.py:85] train_eval/utils.DatasetConfig.seed = 42
I0512 23:44:42.742140 140180528838656 gin_utils.py:85] train_eval/utils.DatasetConfig.shuffle = False
I0512 23:44:42.742172 140180528838656 gin_utils.py:85] train_eval/utils.DatasetConfig.split = 'validation'
I0512 23:44:42.742203 140180528838656 gin_utils.py:85] train_eval/utils.DatasetConfig.task_feature_lengths = %TASK_FEATURE_LENGTHS
I0512 23:44:42.742237 140180528838656 gin_utils.py:85] train_eval/utils.DatasetConfig.use_cached = %USE_CACHED_TASKS
I0512 23:44:42.742270 140180528838656 gin_utils.py:85] 
I0512 23:44:42.742302 140180528838656 gin_utils.py:85] # Parameters for t5_architecture.DecoderLayer:
I0512 23:44:42.742334 140180528838656 gin_utils.py:85] # ==============================================================================
I0512 23:44:42.742366 140180528838656 gin_utils.py:85] t5_architecture.DecoderLayer.activation_partitioning_dims = \
I0512 23:44:42.742398 140180528838656 gin_utils.py:85]     %ACTIVATION_PARTITIONING_DIMS
I0512 23:44:42.742430 140180528838656 gin_utils.py:85] t5_architecture.DecoderLayer.dropout_factory = %DROPOUT_FACTORY
I0512 23:44:42.742465 140180528838656 gin_utils.py:85] t5_architecture.DecoderLayer.encoder_decoder_attention = None
I0512 23:44:42.742499 140180528838656 gin_utils.py:85] t5_architecture.DecoderLayer.layer_norm_factory = @layer_norm.T5LayerNorm
I0512 23:44:42.742531 140180528838656 gin_utils.py:85] t5_architecture.DecoderLayer.mlp = @dense.MlpBlock()
I0512 23:44:42.742563 140180528838656 gin_utils.py:85] t5_architecture.DecoderLayer.scanned = False
I0512 23:44:42.742595 140180528838656 gin_utils.py:85] t5_architecture.DecoderLayer.self_attention = \
I0512 23:44:42.742627 140180528838656 gin_utils.py:85]     @dense_attention.MultiHeadDotProductAttention()
I0512 23:44:42.742659 140180528838656 gin_utils.py:85] 
I0512 23:44:42.742690 140180528838656 gin_utils.py:85] # Parameters for t5_architecture.DecoderOnly:
I0512 23:44:42.742722 140180528838656 gin_utils.py:85] # ==============================================================================
I0512 23:44:42.742753 140180528838656 gin_utils.py:85] t5_architecture.DecoderOnly.decoder_factory = @moe_architecture.SparseDecoder
I0512 23:44:42.742785 140180528838656 gin_utils.py:85] t5_architecture.DecoderOnly.dtype = %ACTIVATION_DTYPE
I0512 23:44:42.742816 140180528838656 gin_utils.py:85] t5_architecture.DecoderOnly.shared_token_embedder_factory = @embedding.Embed
I0512 23:44:42.742849 140180528838656 gin_utils.py:85] 
I0512 23:44:42.742880 140180528838656 gin_utils.py:85] # Parameters for output_logits/dense.DenseGeneral:
I0512 23:44:42.742912 140180528838656 gin_utils.py:85] # ==============================================================================
I0512 23:44:42.742944 140180528838656 gin_utils.py:85] output_logits/dense.DenseGeneral.bias_init = %BIAS_INIT
I0512 23:44:42.742975 140180528838656 gin_utils.py:85] output_logits/dense.DenseGeneral.dtype = 'float32'
I0512 23:44:42.743007 140180528838656 gin_utils.py:85] output_logits/dense.DenseGeneral.features = %NUM_EMBEDDINGS
I0512 23:44:42.743039 140180528838656 gin_utils.py:85] output_logits/dense.DenseGeneral.kernel_axis_names = ['embed', 'vocab']
I0512 23:44:42.743071 140180528838656 gin_utils.py:85] output_logits/dense.DenseGeneral.kernel_init = \
I0512 23:44:42.743103 140180528838656 gin_utils.py:85]     @output_logits_kernel_init/linen.initializers.variance_scaling()
I0512 23:44:42.743134 140180528838656 gin_utils.py:85] output_logits/dense.DenseGeneral.use_bias = False
I0512 23:44:42.743166 140180528838656 gin_utils.py:85] 
I0512 23:44:42.743198 140180528838656 gin_utils.py:85] # Parameters for dropout_factory/linen.Dropout:
I0512 23:44:42.743231 140180528838656 gin_utils.py:85] # ==============================================================================
I0512 23:44:42.743264 140180528838656 gin_utils.py:85] dropout_factory/linen.Dropout.broadcast_dims = (-2,)
I0512 23:44:42.743296 140180528838656 gin_utils.py:85] dropout_factory/linen.Dropout.rate = %DROPOUT_RATE
I0512 23:44:42.743328 140180528838656 gin_utils.py:85] 
I0512 23:44:42.743360 140180528838656 gin_utils.py:85] # Parameters for embedding.Embed:
I0512 23:44:42.743392 140180528838656 gin_utils.py:85] # ==============================================================================
I0512 23:44:42.743423 140180528838656 gin_utils.py:85] embedding.Embed.attend_dtype = 'float32'
I0512 23:44:42.743455 140180528838656 gin_utils.py:85] embedding.Embed.cast_input_dtype = 'int32'
I0512 23:44:42.743493 140180528838656 gin_utils.py:85] embedding.Embed.dtype = %ACTIVATION_DTYPE
I0512 23:44:42.743526 140180528838656 gin_utils.py:85] embedding.Embed.embedding_init = @token_embedder_init/linen.initializers.normal()
I0512 23:44:42.743558 140180528838656 gin_utils.py:85] embedding.Embed.features = %EMBED_DIM
I0512 23:44:42.743590 140180528838656 gin_utils.py:85] embedding.Embed.name = 'token_embedder'
I0512 23:44:42.743621 140180528838656 gin_utils.py:85] embedding.Embed.num_embeddings = %NUM_EMBEDDINGS
I0512 23:44:42.743653 140180528838656 gin_utils.py:85] embedding.Embed.one_hot = True
I0512 23:44:42.743685 140180528838656 gin_utils.py:85] 
I0512 23:44:42.743716 140180528838656 gin_utils.py:85] # Parameters for dense.MlpBlock:
I0512 23:44:42.743747 140180528838656 gin_utils.py:85] # ==============================================================================
I0512 23:44:42.743780 140180528838656 gin_utils.py:85] dense.MlpBlock.activations = ('swiglu', 'linear')
I0512 23:44:42.743812 140180528838656 gin_utils.py:85] dense.MlpBlock.bias_init = %BIAS_INIT
I0512 23:44:42.743844 140180528838656 gin_utils.py:85] dense.MlpBlock.dtype = %ACTIVATION_DTYPE
I0512 23:44:42.743875 140180528838656 gin_utils.py:85] dense.MlpBlock.final_dropout_rate = 0
I0512 23:44:42.743907 140180528838656 gin_utils.py:85] dense.MlpBlock.input_axis_name = 'mlp_embed'
I0512 23:44:42.743939 140180528838656 gin_utils.py:85] dense.MlpBlock.intermediate_dim = %MLP_DIM
I0512 23:44:42.743970 140180528838656 gin_utils.py:85] dense.MlpBlock.intermediate_dropout_rate = %DROPOUT_RATE
I0512 23:44:42.714067 140260698863616 gin_utils.py:85] dense.MlpBlock.kernel_init = @mlp_kernel_init/linen.initializers.variance_scaling()
I0512 23:44:42.714099 140260698863616 gin_utils.py:85] dense.MlpBlock.output_axis_name = 'mlp_embed'
I0512 23:44:42.714132 140260698863616 gin_utils.py:85] dense.MlpBlock.use_bias = False
I0512 23:44:42.714164 140260698863616 gin_utils.py:85] 
I0512 23:44:42.714196 140260698863616 gin_utils.py:85] # Parameters for expert/dense.MlpBlock:
I0512 23:44:42.714228 140260698863616 gin_utils.py:85] # ==============================================================================
I0512 23:44:42.714266 140260698863616 gin_utils.py:85] expert/dense.MlpBlock.activation_partitioning_dims = 1
I0512 23:44:42.714321 140260698863616 gin_utils.py:85] expert/dense.MlpBlock.activations = ('swiglu', 'linear')
I0512 23:44:42.714357 140260698863616 gin_utils.py:85] expert/dense.MlpBlock.bias_init = %BIAS_INIT
I0512 23:44:42.714390 140260698863616 gin_utils.py:85] expert/dense.MlpBlock.data_sharding_constraints = ('expert_replicas', 'mlp')
I0512 23:44:42.714422 140260698863616 gin_utils.py:85] expert/dense.MlpBlock.dtype = %MOE_TRUNCATED_DTYPE
I0512 23:44:42.714455 140260698863616 gin_utils.py:85] expert/dense.MlpBlock.final_dropout_rate = 0.0
I0512 23:44:42.714487 140260698863616 gin_utils.py:85] expert/dense.MlpBlock.input_axis_name = 'mlp_embed'
I0512 23:44:42.714521 140260698863616 gin_utils.py:85] expert/dense.MlpBlock.intermediate_axis_name = 'expert_mlp'
I0512 23:44:42.714554 140260698863616 gin_utils.py:85] expert/dense.MlpBlock.intermediate_dim = %EXPERT_MLP_DIM
I0512 23:44:42.714586 140260698863616 gin_utils.py:85] expert/dense.MlpBlock.intermediate_dropout_rate = %EXPERT_DROPOUT_RATE
I0512 23:44:42.714619 140260698863616 gin_utils.py:85] expert/dense.MlpBlock.kernel_init = \
I0512 23:44:42.714651 140260698863616 gin_utils.py:85]     @expert_kernel_init/linen.initializers.variance_scaling()
I0512 23:44:42.714683 140260698863616 gin_utils.py:85] expert/dense.MlpBlock.output_axis_name = 'mlp_embed'
I0512 23:44:42.744002 140180528838656 gin_utils.py:85] dense.MlpBlock.kernel_init = @mlp_kernel_init/linen.initializers.variance_scaling()
I0512 23:44:42.744033 140180528838656 gin_utils.py:85] dense.MlpBlock.output_axis_name = 'mlp_embed'
I0512 23:44:42.744065 140180528838656 gin_utils.py:85] dense.MlpBlock.use_bias = False
I0512 23:44:42.744097 140180528838656 gin_utils.py:85] 
I0512 23:44:42.744128 140180528838656 gin_utils.py:85] # Parameters for expert/dense.MlpBlock:
I0512 23:44:42.744160 140180528838656 gin_utils.py:85] # ==============================================================================
I0512 23:44:42.744192 140180528838656 gin_utils.py:85] expert/dense.MlpBlock.activation_partitioning_dims = 1
I0512 23:44:42.744225 140180528838656 gin_utils.py:85] expert/dense.MlpBlock.activations = ('swiglu', 'linear')
I0512 23:44:42.744258 140180528838656 gin_utils.py:85] expert/dense.MlpBlock.bias_init = %BIAS_INIT
I0512 23:44:42.744290 140180528838656 gin_utils.py:85] expert/dense.MlpBlock.data_sharding_constraints = ('expert_replicas', 'mlp')
I0512 23:44:42.744322 140180528838656 gin_utils.py:85] expert/dense.MlpBlock.dtype = %MOE_TRUNCATED_DTYPE
I0512 23:44:42.744354 140180528838656 gin_utils.py:85] expert/dense.MlpBlock.final_dropout_rate = 0.0
I0512 23:44:42.744385 140180528838656 gin_utils.py:85] expert/dense.MlpBlock.input_axis_name = 'mlp_embed'
I0512 23:44:42.744441 140180528838656 gin_utils.py:85] expert/dense.MlpBlock.intermediate_axis_name = 'expert_mlp'
I0512 23:44:42.744480 140180528838656 gin_utils.py:85] expert/dense.MlpBlock.intermediate_dim = %EXPERT_MLP_DIM
I0512 23:44:42.744513 140180528838656 gin_utils.py:85] expert/dense.MlpBlock.intermediate_dropout_rate = %EXPERT_DROPOUT_RATE
I0512 23:44:42.744545 140180528838656 gin_utils.py:85] expert/dense.MlpBlock.kernel_init = \
I0512 23:44:42.744577 140180528838656 gin_utils.py:85]     @expert_kernel_init/linen.initializers.variance_scaling()
I0512 23:44:42.744609 140180528838656 gin_utils.py:85] expert/dense.MlpBlock.output_axis_name = 'mlp_embed'
I0512 23:44:42.573933 140643590531072 gin_utils.py:85] train_script.train.infer_eval_dataset_cfg = None
I0512 23:44:42.573967 140643590531072 gin_utils.py:85] train_script.train.model = %MODEL
I0512 23:44:42.573999 140643590531072 gin_utils.py:85] train_script.train.model_dir = %MODEL_DIR
I0512 23:44:42.574031 140643590531072 gin_utils.py:85] train_script.train.partitioner = @moe_partitioning.MoePjitPartitioner()
I0512 23:44:42.574062 140643590531072 gin_utils.py:85] train_script.train.random_seed = %RANDOM_SEED
I0512 23:44:42.574094 140643590531072 gin_utils.py:85] train_script.train.stats_period = 10
I0512 23:44:42.574126 140643590531072 gin_utils.py:85] train_script.train.summarize_config_fn = @gin_utils.summarize_gin_config
I0512 23:44:42.574157 140643590531072 gin_utils.py:85] train_script.train.total_steps = %TRAIN_STEPS
I0512 23:44:42.574189 140643590531072 gin_utils.py:85] train_script.train.train_dataset_cfg = @train/utils.DatasetConfig()
I0512 23:44:42.574221 140643590531072 gin_utils.py:85] train_script.train.train_eval_dataset_cfg = None
I0512 23:44:42.574252 140643590531072 gin_utils.py:85] train_script.train.trainer_cls = @moe_trainer.MoeTrainer
I0512 23:44:42.574284 140643590531072 gin_utils.py:85] train_script.train.use_hardware_rng = %USE_HARDWARE_RNG
I0512 23:44:42.574316 140643590531072 gin_utils.py:85] 
I0512 23:44:42.574347 140643590531072 gin_utils.py:85] # Parameters for trainer.Trainer:
I0512 23:44:42.574379 140643590531072 gin_utils.py:85] # ==============================================================================
I0512 23:44:42.574410 140643590531072 gin_utils.py:85] trainer.Trainer.learning_rate_fn = @utils.create_learning_rate_scheduler()
I0512 23:44:42.574442 140643590531072 gin_utils.py:85] trainer.Trainer.num_microbatches = None
I0512 23:44:42.574473 140643590531072 gin_utils.py:85] 
I0512 23:44:42.574505 140643590531072 gin_utils.py:85] # Parameters for attention_kernel_init/linen.initializers.variance_scaling:
I0512 23:44:42.574537 140643590531072 gin_utils.py:85] # ==============================================================================
I0512 23:44:42.574568 140643590531072 gin_utils.py:85] attention_kernel_init/linen.initializers.variance_scaling.distribution = 'normal'
I0512 23:44:42.574600 140643590531072 gin_utils.py:85] attention_kernel_init/linen.initializers.variance_scaling.mode = 'fan_in'
I0512 23:44:42.574631 140643590531072 gin_utils.py:85] attention_kernel_init/linen.initializers.variance_scaling.scale = %SCALE
I0512 23:44:42.574663 140643590531072 gin_utils.py:85] 
I0512 23:44:42.574704 140643590531072 gin_utils.py:85] # Parameters for expert_kernel_init/linen.initializers.variance_scaling:
I0512 23:44:42.574738 140643590531072 gin_utils.py:85] # ==============================================================================
I0512 23:44:42.574769 140643590531072 gin_utils.py:85] expert_kernel_init/linen.initializers.variance_scaling.distribution = 'normal'
I0512 23:44:42.574800 140643590531072 gin_utils.py:85] expert_kernel_init/linen.initializers.variance_scaling.mode = 'fan_in'
I0512 23:44:42.574832 140643590531072 gin_utils.py:85] expert_kernel_init/linen.initializers.variance_scaling.scale = %SCALE
I0512 23:44:42.574863 140643590531072 gin_utils.py:85] 
I0512 23:44:42.574895 140643590531072 gin_utils.py:85] # Parameters for mlp_kernel_init/linen.initializers.variance_scaling:
I0512 23:44:42.574926 140643590531072 gin_utils.py:85] # ==============================================================================
I0512 23:44:42.574960 140643590531072 gin_utils.py:85] mlp_kernel_init/linen.initializers.variance_scaling.distribution = \
I0512 23:44:42.574992 140643590531072 gin_utils.py:85]     'truncated_normal'
I0512 23:44:42.575024 140643590531072 gin_utils.py:85] mlp_kernel_init/linen.initializers.variance_scaling.mode = 'fan_in'
I0512 23:44:42.575055 140643590531072 gin_utils.py:85] mlp_kernel_init/linen.initializers.variance_scaling.scale = %SCALE
I0512 23:44:42.575113 140643590531072 gin_utils.py:85] 
I0512 23:44:42.575148 140643590531072 gin_utils.py:85] # Parameters for output_logits_kernel_init/linen.initializers.variance_scaling:
I0512 23:44:42.575180 140643590531072 gin_utils.py:85] # ==============================================================================
I0512 23:44:42.575211 140643590531072 gin_utils.py:85] output_logits_kernel_init/linen.initializers.variance_scaling.distribution = \
I0512 23:44:42.575243 140643590531072 gin_utils.py:85]     'truncated_normal'
I0512 23:44:42.575274 140643590531072 gin_utils.py:85] output_logits_kernel_init/linen.initializers.variance_scaling.mode = 'fan_in'
I0512 23:44:42.575306 140643590531072 gin_utils.py:85] output_logits_kernel_init/linen.initializers.variance_scaling.scale = %SCALE
I0512 23:44:42.576251 140643590531072 partitioning.py:559] `activation_partitioning_dims` = 2, `parameter_partitioning_dims` = 1
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
I0000 00:00:1715557482.638150  370028 pjrt_api.cc:98] GetPjrtApi was found for tpu at /home/rosinality/openmoe_venv/lib/python3.10/site-packages/libtpu/libtpu.so
I0000 00:00:1715557482.638206  370028 pjrt_api.cc:67] PJRT_Api is set for device type tpu
I0000 00:00:1715557482.638208  370028 pjrt_api.cc:72] PJRT plugin for tpu has PJRT API version 0.30. The framework PJRT API version is 0.30.
I0512 23:44:42.412297 140143678494720 gin_utils.py:85] train_script.train.eval_period = 2500
I0512 23:44:42.412331 140143678494720 gin_utils.py:85] train_script.train.eval_steps = 20
I0512 23:44:42.412363 140143678494720 gin_utils.py:85] train_script.train.infer_eval_dataset_cfg = None
I0512 23:44:42.412395 140143678494720 gin_utils.py:85] train_script.train.model = %MODEL
I0512 23:44:42.412428 140143678494720 gin_utils.py:85] train_script.train.model_dir = %MODEL_DIR
I0512 23:44:42.412459 140143678494720 gin_utils.py:85] train_script.train.partitioner = @moe_partitioning.MoePjitPartitioner()
I0512 23:44:42.412491 140143678494720 gin_utils.py:85] train_script.train.random_seed = %RANDOM_SEED
I0512 23:44:42.412523 140143678494720 gin_utils.py:85] train_script.train.stats_period = 10
I0512 23:44:42.412583 140143678494720 gin_utils.py:85] train_script.train.summarize_config_fn = @gin_utils.summarize_gin_config
I0512 23:44:42.714740 140260698863616 gin_utils.py:85] expert/dense.MlpBlock.use_bias = False
I0512 23:44:42.714774 140260698863616 gin_utils.py:85] 
I0512 23:44:42.714807 140260698863616 gin_utils.py:85] # Parameters for models.MoeDecoderOnlyModel:
I0512 23:44:42.714839 140260698863616 gin_utils.py:85] # ==============================================================================
I0512 23:44:42.714872 140260698863616 gin_utils.py:85] models.MoeDecoderOnlyModel.aux_loss_factor = %AUX_LOSS_FACTOR
I0512 23:44:42.714904 140260698863616 gin_utils.py:85] models.MoeDecoderOnlyModel.inputs_bidirectional_attention = True
I0512 23:44:42.714936 140260698863616 gin_utils.py:85] models.MoeDecoderOnlyModel.label_smoothing = %LABEL_SMOOTHING
I0512 23:44:42.714969 140260698863616 gin_utils.py:85] models.MoeDecoderOnlyModel.loss_normalizing_factor = %LOSS_NORMALIZING_FACTOR
I0512 23:44:42.715001 140260698863616 gin_utils.py:85] models.MoeDecoderOnlyModel.module = %ARCHITECTURE
I0512 23:44:42.715033 140260698863616 gin_utils.py:85] models.MoeDecoderOnlyModel.optimizer_def = %OPTIMIZER
I0512 23:44:42.715065 140260698863616 gin_utils.py:85] models.MoeDecoderOnlyModel.router_z_loss_factor = %ROUTER_Z_LOSS_FACTOR
I0512 23:44:42.715098 140260698863616 gin_utils.py:85] models.MoeDecoderOnlyModel.vocabulary = %VOCABULARY
I0512 23:44:42.715130 140260698863616 gin_utils.py:85] models.MoeDecoderOnlyModel.z_loss = %Z_LOSS
I0512 23:44:42.715163 140260698863616 gin_utils.py:85] 
I0512 23:44:42.715195 140260698863616 gin_utils.py:85] # Parameters for moe_layers.MoeLayer:
I0512 23:44:42.715227 140260698863616 gin_utils.py:85] # ==============================================================================
I0512 23:44:42.715265 140260698863616 gin_utils.py:85] moe_layers.MoeLayer.dtype = %MOE_TRUNCATED_DTYPE
I0512 23:44:42.715299 140260698863616 gin_utils.py:85] moe_layers.MoeLayer.eval_capacity_factor = %EVAL_EXPERT_CAPACITY_FACTOR
I0512 23:44:42.715332 140260698863616 gin_utils.py:85] moe_layers.MoeLayer.expert = @expert/dense.MlpBlock()
I0512 23:44:42.715365 140260698863616 gin_utils.py:85] moe_layers.MoeLayer.max_group_size = %GROUP_SIZE
I0512 23:44:42.715397 140260698863616 gin_utils.py:85] moe_layers.MoeLayer.num_expert_partitions = %NUM_EXPERT_PARTITIONS
I0512 23:44:42.715430 140260698863616 gin_utils.py:85] moe_layers.MoeLayer.num_experts = %NUM_EXPERTS
I0512 23:44:42.715462 140260698863616 gin_utils.py:85] moe_layers.MoeLayer.num_model_partitions = \
I0512 23:44:42.715494 140260698863616 gin_utils.py:85]     @moe_partitioning.compute_num_model_partitions()
I0512 23:44:42.715529 140260698863616 gin_utils.py:85] moe_layers.MoeLayer.train_capacity_factor = %TRAIN_EXPERT_CAPACITY_FACTOR
I0512 23:44:42.715562 140260698863616 gin_utils.py:85] 
I0512 23:44:42.715594 140260698863616 gin_utils.py:85] # Parameters for sparse_decoder/moe_layers.MoeLayer:
I0512 23:44:42.715626 140260698863616 gin_utils.py:85] # ==============================================================================
I0512 23:44:42.715659 140260698863616 gin_utils.py:85] sparse_decoder/moe_layers.MoeLayer.router = \
I0512 23:44:42.715692 140260698863616 gin_utils.py:85]     @sparse_decoder/routing.TokensChooseMaskedRouter()
I0512 23:44:42.715724 140260698863616 gin_utils.py:85] 
I0512 23:44:42.715757 140260698863616 gin_utils.py:85] # Parameters for moe_partitioning.MoePjitPartitioner:
I0512 23:44:42.715789 140260698863616 gin_utils.py:85] # ==============================================================================
I0512 23:44:42.715821 140260698863616 gin_utils.py:85] moe_partitioning.MoePjitPartitioner.model_parallel_submesh = \
I0512 23:44:42.715853 140260698863616 gin_utils.py:85]     %MODEL_PARALLEL_SUBMESH
I0512 23:44:42.715885 140260698863616 gin_utils.py:85] moe_partitioning.MoePjitPartitioner.num_expert_partitions = %NUM_EXPERT_PARTITIONS
I0512 23:44:42.715918 140260698863616 gin_utils.py:85] moe_partitioning.MoePjitPartitioner.num_partitions = %NUM_MODEL_PARTITIONS
I0512 23:44:42.715950 140260698863616 gin_utils.py:85] 
I0512 23:44:42.715982 140260698863616 gin_utils.py:85] # Parameters for moe_trainer.MoeTrainer:
I0512 23:44:42.716014 140260698863616 gin_utils.py:85] # ==============================================================================
I0512 23:44:42.716046 140260698863616 gin_utils.py:85] moe_trainer.MoeTrainer.learning_rate_fn = @utils.create_learning_rate_scheduler()
I0512 23:44:42.716078 140260698863616 gin_utils.py:85] moe_trainer.MoeTrainer.num_expert_partitions = %NUM_EXPERT_PARTITIONS
I0512 23:44:42.716111 140260698863616 gin_utils.py:85] moe_trainer.MoeTrainer.num_microbatches = 8
I0512 23:44:42.716143 140260698863616 gin_utils.py:85] 
I0512 23:44:42.716175 140260698863616 gin_utils.py:85] # Parameters for dense_attention.MultiHeadDotProductAttention:
I0512 23:44:42.716207 140260698863616 gin_utils.py:85] # ==============================================================================
I0512 23:44:42.716240 140260698863616 gin_utils.py:85] dense_attention.MultiHeadDotProductAttention.bias_init = %BIAS_INIT
I0512 23:44:42.716278 140260698863616 gin_utils.py:85] dense_attention.MultiHeadDotProductAttention.broadcast_dropout = True
I0512 23:44:42.716311 140260698863616 gin_utils.py:85] dense_attention.MultiHeadDotProductAttention.dropout_rate = %DROPOUT_RATE
I0512 23:44:42.716344 140260698863616 gin_utils.py:85] dense_attention.MultiHeadDotProductAttention.dtype = %ACTIVATION_DTYPE
I0512 23:44:42.716376 140260698863616 gin_utils.py:85] dense_attention.MultiHeadDotProductAttention.head_dim = %HEAD_DIM
I0512 23:44:42.716409 140260698863616 gin_utils.py:85] dense_attention.MultiHeadDotProductAttention.kernel_init = \
I0512 23:44:42.716441 140260698863616 gin_utils.py:85]     @attention_kernel_init/linen.initializers.variance_scaling()
I0512 23:44:42.716473 140260698863616 gin_utils.py:85] dense_attention.MultiHeadDotProductAttention.num_heads = %NUM_HEADS
I0512 23:44:42.716506 140260698863616 gin_utils.py:85] dense_attention.MultiHeadDotProductAttention.use_bias = False
I0512 23:44:42.716540 140260698863616 gin_utils.py:85] dense_attention.MultiHeadDotProductAttention.use_rotary_embedding = True
I0512 23:44:42.716573 140260698863616 gin_utils.py:85] 
I0512 23:44:42.716605 140260698863616 gin_utils.py:85] # Parameters for bias_init/linen.initializers.normal:
I0512 23:44:42.716638 140260698863616 gin_utils.py:85] # ==============================================================================
I0512 23:44:42.716670 140260698863616 gin_utils.py:85] bias_init/linen.initializers.normal.stddev = 1e-06
I0512 23:44:42.716704 140260698863616 gin_utils.py:85] 
I0512 23:44:42.716736 140260698863616 gin_utils.py:85] # Parameters for router_init/linen.initializers.normal:
I0512 23:44:42.716769 140260698863616 gin_utils.py:85] # ==============================================================================
I0512 23:44:42.716801 140260698863616 gin_utils.py:85] router_init/linen.initializers.normal.stddev = 0.02
I0512 23:44:42.716834 140260698863616 gin_utils.py:85] 
I0512 23:44:42.716866 140260698863616 gin_utils.py:85] # Parameters for token_embedder_init/linen.initializers.normal:
I0512 23:44:42.716898 140260698863616 gin_utils.py:85] # ==============================================================================
I0512 23:44:42.716930 140260698863616 gin_utils.py:85] token_embedder_init/linen.initializers.normal.stddev = 1.0
I0512 23:44:42.716963 140260698863616 gin_utils.py:85] 
I0512 23:44:42.716994 140260698863616 gin_utils.py:85] # Parameters for partitioning.PjitPartitioner:
I0512 23:44:42.717027 140260698863616 gin_utils.py:85] # ==============================================================================
I0512 23:44:42.717059 140260698863616 gin_utils.py:85] partitioning.PjitPartitioner.logical_axis_rules = \
I0512 23:44:42.717091 140260698863616 gin_utils.py:85]     @partitioning.standard_logical_axis_rules()
I0512 23:44:42.717123 140260698863616 gin_utils.py:85] partitioning.PjitPartitioner.model_parallel_submesh = None
I0512 23:44:42.717156 140260698863616 gin_utils.py:85] partitioning.PjitPartitioner.num_partitions = 1
I0512 23:44:42.717187 140260698863616 gin_utils.py:85] 
I0512 23:44:42.717219 140260698863616 gin_utils.py:85] # Parameters for utils.RestoreCheckpointConfig:
I0512 23:44:42.717256 140260698863616 gin_utils.py:85] # ==============================================================================
I0512 23:44:42.717290 140260698863616 gin_utils.py:85] utils.RestoreCheckpointConfig.path = []
I0512 23:44:42.717323 140260698863616 gin_utils.py:85] 
I0512 23:44:42.717355 140260698863616 gin_utils.py:85] # Parameters for routing.RouterWeights:
I0512 23:44:42.717387 140260698863616 gin_utils.py:85] # ==============================================================================
I0512 23:44:42.717419 140260698863616 gin_utils.py:85] routing.RouterWeights.bias_init = %BIAS_INIT
I0512 23:44:42.717452 140260698863616 gin_utils.py:85] routing.RouterWeights.dtype = 'float32'
I0512 23:44:42.717484 140260698863616 gin_utils.py:85] routing.RouterWeights.kernel_init = @router_init/linen.initializers.normal()
I0512 23:44:42.717518 140260698863616 gin_utils.py:85] routing.RouterWeights.use_bias = False
I0512 23:44:42.717551 140260698863616 gin_utils.py:85] 
I0512 23:44:42.717583 140260698863616 gin_utils.py:85] # Parameters for utils.SaveCheckpointConfig:
I0512 23:44:42.717615 140260698863616 gin_utils.py:85] # ==============================================================================
I0512 23:44:42.717647 140260698863616 gin_utils.py:85] utils.SaveCheckpointConfig.dtype = 'float32'
I0512 23:44:42.717680 140260698863616 gin_utils.py:85] utils.SaveCheckpointConfig.keep = 20
I0512 23:44:42.717712 140260698863616 gin_utils.py:85] utils.SaveCheckpointConfig.period = 2500
I0512 23:44:42.717744 140260698863616 gin_utils.py:85] utils.SaveCheckpointConfig.save_dataset = False
I0512 23:44:42.717776 140260698863616 gin_utils.py:85] 
I0512 23:44:42.717808 140260698863616 gin_utils.py:85] # Parameters for seqio.SentencePieceVocabulary:
I0512 23:44:42.717840 140260698863616 gin_utils.py:85] # ==============================================================================
I0512 23:44:42.717872 140260698863616 gin_utils.py:85] seqio.SentencePieceVocabulary.extra_ids = 300
I0512 23:44:42.717904 140260698863616 gin_utils.py:85] seqio.SentencePieceVocabulary.sentencepiece_model_file = \
I0512 23:44:42.717936 140260698863616 gin_utils.py:85]     'gs://rosinality-tpu-bucket/sentencepiece.model'
I0512 23:44:42.717968 140260698863616 gin_utils.py:85] 
I0512 23:44:42.718000 140260698863616 gin_utils.py:85] # Parameters for moe_architecture.SparseDecoder:
I0512 23:44:42.718032 140260698863616 gin_utils.py:85] # ==============================================================================
I0512 23:44:42.718065 140260698863616 gin_utils.py:85] moe_architecture.SparseDecoder.dropout_factory = %DROPOUT_FACTORY
I0512 23:44:42.718097 140260698863616 gin_utils.py:85] moe_architecture.SparseDecoder.dtype = %ACTIVATION_DTYPE
I0512 23:44:42.718129 140260698863616 gin_utils.py:85] moe_architecture.SparseDecoder.layer_factory = @t5_architecture.DecoderLayer
I0512 23:44:42.718161 140260698863616 gin_utils.py:85] moe_architecture.SparseDecoder.layer_norm_factory = @layer_norm.T5LayerNorm
I0512 23:44:42.718193 140260698863616 gin_utils.py:85] moe_architecture.SparseDecoder.num_layers = %NUM_DECODER_LAYERS
I0512 23:44:42.718225 140260698863616 gin_utils.py:85] moe_architecture.SparseDecoder.num_sparse_layers = %NUM_DECODER_SPARSE_LAYERS
I0512 23:44:42.718262 140260698863616 gin_utils.py:85] moe_architecture.SparseDecoder.output_logits_factory = \
I0512 23:44:42.718296 140260698863616 gin_utils.py:85]     @output_logits/dense.DenseGeneral
I0512 23:44:42.718329 140260698863616 gin_utils.py:85] moe_architecture.SparseDecoder.sparse_layer_factory = \
I0512 23:44:42.718361 140260698863616 gin_utils.py:85]     @moe_architecture.SparseDecoderLayer
I0512 23:44:42.718393 140260698863616 gin_utils.py:85] moe_architecture.SparseDecoder.sparse_layout = %DECODER_SPARSE_LAYOUT
I0512 23:44:42.718425 140260698863616 gin_utils.py:85] 
I0512 23:44:42.718457 140260698863616 gin_utils.py:85] # Parameters for moe_architecture.SparseDecoderLayer:
I0512 23:44:42.718489 140260698863616 gin_utils.py:85] # ==============================================================================
I0512 23:44:42.718523 140260698863616 gin_utils.py:85] moe_architecture.SparseDecoderLayer.activation_partitioning_dims = \
I0512 23:44:42.718557 140260698863616 gin_utils.py:85]     %ACTIVATION_PARTITIONING_DIMS
I0512 23:44:42.718590 140260698863616 gin_utils.py:85] moe_architecture.SparseDecoderLayer.dropout_factory = %DROPOUT_FACTORY
I0512 23:44:42.718623 140260698863616 gin_utils.py:85] moe_architecture.SparseDecoderLayer.encoder_decoder_attention = None
I0512 23:44:42.718655 140260698863616 gin_utils.py:85] moe_architecture.SparseDecoderLayer.extra_mlp = @dense.MlpBlock()
I0512 23:44:42.718704 140260698863616 gin_utils.py:85] moe_architecture.SparseDecoderLayer.layer_norm_factory = @layer_norm.T5LayerNorm
I0512 23:44:42.718749 140260698863616 gin_utils.py:85] moe_architecture.SparseDecoderLayer.mlp = @sparse_decoder/moe_layers.MoeLayer()
I0512 23:44:42.718782 140260698863616 gin_utils.py:85] moe_architecture.SparseDecoderLayer.scanned = False
I0512 23:44:42.718815 140260698863616 gin_utils.py:85] moe_architecture.SparseDecoderLayer.self_attention = \
I0512 23:44:42.718847 140260698863616 gin_utils.py:85]     @dense_attention.MultiHeadDotProductAttention()
I0512 23:44:42.718880 140260698863616 gin_utils.py:85] 
I0512 23:44:42.718912 140260698863616 gin_utils.py:85] # Parameters for layer_norm.T5LayerNorm:
I0512 23:44:42.718945 140260698863616 gin_utils.py:85] # ==============================================================================
I0512 23:44:42.718977 140260698863616 gin_utils.py:85] layer_norm.T5LayerNorm.dtype = %ACTIVATION_DTYPE
I0512 23:44:42.719009 140260698863616 gin_utils.py:85] 
I0512 23:44:42.719041 140260698863616 gin_utils.py:85] # Parameters for routing.TokensChooseMaskedRouter:
I0512 23:44:42.719073 140260698863616 gin_utils.py:85] # ==============================================================================
I0512 23:44:42.719105 140260698863616 gin_utils.py:85] routing.TokensChooseMaskedRouter.dtype = 'float32'
I0512 23:44:42.719137 140260698863616 gin_utils.py:85] routing.TokensChooseMaskedRouter.ignore_padding_tokens = False
I0512 23:44:42.719169 140260698863616 gin_utils.py:85] routing.TokensChooseMaskedRouter.jitter_noise = %JITTER_NOISE
I0512 23:44:42.719201 140260698863616 gin_utils.py:85] routing.TokensChooseMaskedRouter.num_selected_experts = %NUM_SELECTED_EXPERTS
I0512 23:44:42.719233 140260698863616 gin_utils.py:85] routing.TokensChooseMaskedRouter.router_weights = @routing.RouterWeights()
I0512 23:44:42.719271 140260698863616 gin_utils.py:85] 
I0512 23:44:42.719304 140260698863616 gin_utils.py:85] # Parameters for sparse_decoder/routing.TokensChooseMaskedRouter:
I0512 23:44:42.719336 140260698863616 gin_utils.py:85] # ==============================================================================
I0512 23:44:42.719368 140260698863616 gin_utils.py:85] sparse_decoder/routing.TokensChooseMaskedRouter.batch_prioritized_routing = False
I0512 23:44:42.719400 140260698863616 gin_utils.py:85] 
I0512 23:44:42.719432 140260698863616 gin_utils.py:85] # Parameters for sparse_encoder/routing.TokensChooseMaskedRouter:
I0512 23:44:42.719464 140260698863616 gin_utils.py:85] # ==============================================================================
I0512 23:44:42.719496 140260698863616 gin_utils.py:85] sparse_encoder/routing.TokensChooseMaskedRouter.batch_prioritized_routing = False
I0512 23:44:42.719531 140260698863616 gin_utils.py:85] 
I0512 23:44:42.719564 140260698863616 gin_utils.py:85] # Parameters for train_script.train:
I0512 23:44:42.719596 140260698863616 gin_utils.py:85] # ==============================================================================
I0512 23:44:42.719628 140260698863616 gin_utils.py:85] train_script.train.checkpoint_cfg = @utils.CheckpointConfig()
I0512 23:44:42.744641 140180528838656 gin_utils.py:85] expert/dense.MlpBlock.use_bias = False
I0512 23:44:42.744672 140180528838656 gin_utils.py:85] 
I0512 23:44:42.744704 140180528838656 gin_utils.py:85] # Parameters for models.MoeDecoderOnlyModel:
I0512 23:44:42.744735 140180528838656 gin_utils.py:85] # ==============================================================================
I0512 23:44:42.744767 140180528838656 gin_utils.py:85] models.MoeDecoderOnlyModel.aux_loss_factor = %AUX_LOSS_FACTOR
I0512 23:44:42.744799 140180528838656 gin_utils.py:85] models.MoeDecoderOnlyModel.inputs_bidirectional_attention = True
I0512 23:44:42.744831 140180528838656 gin_utils.py:85] models.MoeDecoderOnlyModel.label_smoothing = %LABEL_SMOOTHING
I0512 23:44:42.744863 140180528838656 gin_utils.py:85] models.MoeDecoderOnlyModel.loss_normalizing_factor = %LOSS_NORMALIZING_FACTOR
I0512 23:44:42.744895 140180528838656 gin_utils.py:85] models.MoeDecoderOnlyModel.module = %ARCHITECTURE
I0512 23:44:42.744927 140180528838656 gin_utils.py:85] models.MoeDecoderOnlyModel.optimizer_def = %OPTIMIZER
I0512 23:44:42.744959 140180528838656 gin_utils.py:85] models.MoeDecoderOnlyModel.router_z_loss_factor = %ROUTER_Z_LOSS_FACTOR
I0512 23:44:42.744991 140180528838656 gin_utils.py:85] models.MoeDecoderOnlyModel.vocabulary = %VOCABULARY
I0512 23:44:42.745023 140180528838656 gin_utils.py:85] models.MoeDecoderOnlyModel.z_loss = %Z_LOSS
I0512 23:44:42.745079 140180528838656 gin_utils.py:85] 
I0512 23:44:42.745112 140180528838656 gin_utils.py:85] # Parameters for moe_layers.MoeLayer:
I0512 23:44:42.745144 140180528838656 gin_utils.py:85] # ==============================================================================
I0512 23:44:42.745176 140180528838656 gin_utils.py:85] moe_layers.MoeLayer.dtype = %MOE_TRUNCATED_DTYPE
I0512 23:44:42.745208 140180528838656 gin_utils.py:85] moe_layers.MoeLayer.eval_capacity_factor = %EVAL_EXPERT_CAPACITY_FACTOR
I0512 23:44:42.745242 140180528838656 gin_utils.py:85] moe_layers.MoeLayer.expert = @expert/dense.MlpBlock()
I0512 23:44:42.745274 140180528838656 gin_utils.py:85] moe_layers.MoeLayer.max_group_size = %GROUP_SIZE
I0512 23:44:42.745306 140180528838656 gin_utils.py:85] moe_layers.MoeLayer.num_expert_partitions = %NUM_EXPERT_PARTITIONS
I0512 23:44:42.745338 140180528838656 gin_utils.py:85] moe_layers.MoeLayer.num_experts = %NUM_EXPERTS
I0512 23:44:42.745370 140180528838656 gin_utils.py:85] moe_layers.MoeLayer.num_model_partitions = \
I0512 23:44:42.745402 140180528838656 gin_utils.py:85]     @partitioning.compute_num_model_partitions()
I0512 23:44:42.745434 140180528838656 gin_utils.py:85] moe_layers.MoeLayer.train_capacity_factor = %TRAIN_EXPERT_CAPACITY_FACTOR
I0512 23:44:42.745471 140180528838656 gin_utils.py:85] 
I0512 23:44:42.745505 140180528838656 gin_utils.py:85] # Parameters for sparse_decoder/moe_layers.MoeLayer:
I0512 23:44:42.745537 140180528838656 gin_utils.py:85] # ==============================================================================
I0512 23:44:42.745569 140180528838656 gin_utils.py:85] sparse_decoder/moe_layers.MoeLayer.router = \
I0512 23:44:42.745601 140180528838656 gin_utils.py:85]     @sparse_decoder/routing.TokensChooseMaskedRouter()
I0512 23:44:42.745633 140180528838656 gin_utils.py:85] 
I0512 23:44:42.745664 140180528838656 gin_utils.py:85] # Parameters for partitioning.MoePjitPartitioner:
I0512 23:44:42.745696 140180528838656 gin_utils.py:85] # ==============================================================================
I0512 23:44:42.745728 140180528838656 gin_utils.py:85] partitioning.MoePjitPartitioner.model_parallel_submesh = %MODEL_PARALLEL_SUBMESH
I0512 23:44:42.745760 140180528838656 gin_utils.py:85] partitioning.MoePjitPartitioner.num_expert_partitions = %NUM_EXPERT_PARTITIONS
I0512 23:44:42.745791 140180528838656 gin_utils.py:85] partitioning.MoePjitPartitioner.num_partitions = %NUM_MODEL_PARTITIONS
I0512 23:44:42.745823 140180528838656 gin_utils.py:85] 
I0512 23:44:42.745855 140180528838656 gin_utils.py:85] # Parameters for moe_trainer.MoeTrainer:
I0512 23:44:42.745887 140180528838656 gin_utils.py:85] # ==============================================================================
I0512 23:44:42.745918 140180528838656 gin_utils.py:85] moe_trainer.MoeTrainer.learning_rate_fn = @utils.create_learning_rate_scheduler()
I0512 23:44:42.745950 140180528838656 gin_utils.py:85] moe_trainer.MoeTrainer.num_expert_partitions = %NUM_EXPERT_PARTITIONS
I0512 23:44:42.745981 140180528838656 gin_utils.py:85] moe_trainer.MoeTrainer.num_microbatches = 8
I0512 23:44:42.746013 140180528838656 gin_utils.py:85] 
I0512 23:44:42.746045 140180528838656 gin_utils.py:85] # Parameters for dense_attention.MultiHeadDotProductAttention:
I0512 23:44:42.746076 140180528838656 gin_utils.py:85] # ==============================================================================
I0512 23:44:42.746108 140180528838656 gin_utils.py:85] dense_attention.MultiHeadDotProductAttention.bias_init = %BIAS_INIT
I0512 23:44:42.746140 140180528838656 gin_utils.py:85] dense_attention.MultiHeadDotProductAttention.broadcast_dropout = True
I0512 23:44:42.746172 140180528838656 gin_utils.py:85] dense_attention.MultiHeadDotProductAttention.dropout_rate = %DROPOUT_RATE
I0512 23:44:42.746203 140180528838656 gin_utils.py:85] dense_attention.MultiHeadDotProductAttention.dtype = %ACTIVATION_DTYPE
I0512 23:44:42.746237 140180528838656 gin_utils.py:85] dense_attention.MultiHeadDotProductAttention.head_dim = %HEAD_DIM
I0512 23:44:42.746270 140180528838656 gin_utils.py:85] dense_attention.MultiHeadDotProductAttention.kernel_init = \
I0512 23:44:42.746302 140180528838656 gin_utils.py:85]     @attention_kernel_init/linen.initializers.variance_scaling()
I0512 23:44:42.746334 140180528838656 gin_utils.py:85] dense_attention.MultiHeadDotProductAttention.num_heads = %NUM_HEADS
I0512 23:44:42.746366 140180528838656 gin_utils.py:85] dense_attention.MultiHeadDotProductAttention.use_bias = False
I0512 23:44:42.746397 140180528838656 gin_utils.py:85] dense_attention.MultiHeadDotProductAttention.use_rotary_embedding = True
I0512 23:44:42.746429 140180528838656 gin_utils.py:85] 
I0512 23:44:42.746461 140180528838656 gin_utils.py:85] # Parameters for bias_init/linen.initializers.normal:
I0512 23:44:42.746499 140180528838656 gin_utils.py:85] # ==============================================================================
I0512 23:44:42.746531 140180528838656 gin_utils.py:85] bias_init/linen.initializers.normal.stddev = 1e-06
I0512 23:44:42.746563 140180528838656 gin_utils.py:85] 
I0512 23:44:42.746595 140180528838656 gin_utils.py:85] # Parameters for router_init/linen.initializers.normal:
I0512 23:44:42.746627 140180528838656 gin_utils.py:85] # ==============================================================================
I0512 23:44:42.746659 140180528838656 gin_utils.py:85] router_init/linen.initializers.normal.stddev = 0.02
I0512 23:44:42.746690 140180528838656 gin_utils.py:85] 
I0512 23:44:42.746721 140180528838656 gin_utils.py:85] # Parameters for token_embedder_init/linen.initializers.normal:
I0512 23:44:42.746753 140180528838656 gin_utils.py:85] # ==============================================================================
I0512 23:44:42.746785 140180528838656 gin_utils.py:85] token_embedder_init/linen.initializers.normal.stddev = 1.0
I0512 23:44:42.746817 140180528838656 gin_utils.py:85] 
I0512 23:44:42.746849 140180528838656 gin_utils.py:85] # Parameters for partitioning2.PjitPartitioner:
I0512 23:44:42.746880 140180528838656 gin_utils.py:85] # ==============================================================================
I0512 23:44:42.746912 140180528838656 gin_utils.py:85] partitioning2.PjitPartitioner.logical_axis_rules = \
I0512 23:44:42.746943 140180528838656 gin_utils.py:85]     @partitioning2.standard_logical_axis_rules()
I0512 23:44:42.746975 140180528838656 gin_utils.py:85] partitioning2.PjitPartitioner.model_parallel_submesh = None
I0512 23:44:42.747007 140180528838656 gin_utils.py:85] partitioning2.PjitPartitioner.num_partitions = 1
I0512 23:44:42.747038 140180528838656 gin_utils.py:85] 
I0512 23:44:42.747069 140180528838656 gin_utils.py:85] # Parameters for utils.RestoreCheckpointConfig:
I0512 23:44:42.747101 140180528838656 gin_utils.py:85] # ==============================================================================
I0512 23:44:42.747132 140180528838656 gin_utils.py:85] utils.RestoreCheckpointConfig.path = []
I0512 23:44:42.747163 140180528838656 gin_utils.py:85] 
I0512 23:44:42.747195 140180528838656 gin_utils.py:85] # Parameters for routing.RouterWeights:
I0512 23:44:42.747227 140180528838656 gin_utils.py:85] # ==============================================================================
I0512 23:44:42.747261 140180528838656 gin_utils.py:85] routing.RouterWeights.bias_init = %BIAS_INIT
I0512 23:44:42.747293 140180528838656 gin_utils.py:85] routing.RouterWeights.dtype = 'float32'
I0512 23:44:42.747324 140180528838656 gin_utils.py:85] routing.RouterWeights.kernel_init = @router_init/linen.initializers.normal()
I0512 23:44:42.747356 140180528838656 gin_utils.py:85] routing.RouterWeights.use_bias = False
I0512 23:44:42.747387 140180528838656 gin_utils.py:85] 
I0512 23:44:42.747419 140180528838656 gin_utils.py:85] # Parameters for utils.SaveCheckpointConfig:
I0512 23:44:42.747450 140180528838656 gin_utils.py:85] # ==============================================================================
I0512 23:44:42.747488 140180528838656 gin_utils.py:85] utils.SaveCheckpointConfig.dtype = 'float32'
I0512 23:44:42.747520 140180528838656 gin_utils.py:85] utils.SaveCheckpointConfig.keep = 20
I0512 23:44:42.747552 140180528838656 gin_utils.py:85] utils.SaveCheckpointConfig.period = 2500
I0512 23:44:42.747584 140180528838656 gin_utils.py:85] utils.SaveCheckpointConfig.save_dataset = False
I0512 23:44:42.747615 140180528838656 gin_utils.py:85] 
I0512 23:44:42.747647 140180528838656 gin_utils.py:85] # Parameters for seqio.SentencePieceVocabulary:
I0512 23:44:42.747678 140180528838656 gin_utils.py:85] # ==============================================================================
I0512 23:44:42.747709 140180528838656 gin_utils.py:85] seqio.SentencePieceVocabulary.extra_ids = 300
I0512 23:44:42.747741 140180528838656 gin_utils.py:85] seqio.SentencePieceVocabulary.sentencepiece_model_file = \
I0512 23:44:42.747772 140180528838656 gin_utils.py:85]     'gs://rosinality-tpu-bucket/sentencepiece.model'
I0512 23:44:42.747804 140180528838656 gin_utils.py:85] 
I0512 23:44:42.747835 140180528838656 gin_utils.py:85] # Parameters for moe_architecture.SparseDecoder:
I0512 23:44:42.747867 140180528838656 gin_utils.py:85] # ==============================================================================
I0512 23:44:42.747898 140180528838656 gin_utils.py:85] moe_architecture.SparseDecoder.dropout_factory = %DROPOUT_FACTORY
I0512 23:44:42.747930 140180528838656 gin_utils.py:85] moe_architecture.SparseDecoder.dtype = %ACTIVATION_DTYPE
I0512 23:44:42.747961 140180528838656 gin_utils.py:85] moe_architecture.SparseDecoder.layer_factory = @t5_architecture.DecoderLayer
I0512 23:44:42.747993 140180528838656 gin_utils.py:85] moe_architecture.SparseDecoder.layer_norm_factory = @layer_norm.T5LayerNorm
I0512 23:44:42.748025 140180528838656 gin_utils.py:85] moe_architecture.SparseDecoder.num_layers = %NUM_DECODER_LAYERS
I0512 23:44:42.748057 140180528838656 gin_utils.py:85] moe_architecture.SparseDecoder.num_sparse_layers = %NUM_DECODER_SPARSE_LAYERS
I0512 23:44:42.748088 140180528838656 gin_utils.py:85] moe_architecture.SparseDecoder.output_logits_factory = \
I0512 23:44:42.748120 140180528838656 gin_utils.py:85]     @output_logits/dense.DenseGeneral
I0512 23:44:42.748151 140180528838656 gin_utils.py:85] moe_architecture.SparseDecoder.sparse_layer_factory = \
I0512 23:44:42.748182 140180528838656 gin_utils.py:85]     @moe_architecture.SparseDecoderLayer
I0512 23:44:42.748214 140180528838656 gin_utils.py:85] moe_architecture.SparseDecoder.sparse_layout = %DECODER_SPARSE_LAYOUT
I0512 23:44:42.748248 140180528838656 gin_utils.py:85] 
I0512 23:44:42.748280 140180528838656 gin_utils.py:85] # Parameters for moe_architecture.SparseDecoderLayer:
I0512 23:44:42.748311 140180528838656 gin_utils.py:85] # ==============================================================================
I0512 23:44:42.748343 140180528838656 gin_utils.py:85] moe_architecture.SparseDecoderLayer.activation_partitioning_dims = \
I0512 23:44:42.748375 140180528838656 gin_utils.py:85]     %ACTIVATION_PARTITIONING_DIMS
I0512 23:44:42.748406 140180528838656 gin_utils.py:85] moe_architecture.SparseDecoderLayer.dropout_factory = %DROPOUT_FACTORY
I0512 23:44:42.748438 140180528838656 gin_utils.py:85] moe_architecture.SparseDecoderLayer.encoder_decoder_attention = None
I0512 23:44:42.748475 140180528838656 gin_utils.py:85] moe_architecture.SparseDecoderLayer.extra_mlp = @dense.MlpBlock()
I0512 23:44:42.748509 140180528838656 gin_utils.py:85] moe_architecture.SparseDecoderLayer.layer_norm_factory = @layer_norm.T5LayerNorm
I0512 23:44:42.748541 140180528838656 gin_utils.py:85] moe_architecture.SparseDecoderLayer.mlp = @sparse_decoder/moe_layers.MoeLayer()
I0512 23:44:42.748573 140180528838656 gin_utils.py:85] moe_architecture.SparseDecoderLayer.scanned = False
I0512 23:44:42.748605 140180528838656 gin_utils.py:85] moe_architecture.SparseDecoderLayer.self_attention = \
I0512 23:44:42.748636 140180528838656 gin_utils.py:85]     @dense_attention.MultiHeadDotProductAttention()
I0512 23:44:42.748667 140180528838656 gin_utils.py:85] 
I0512 23:44:42.748699 140180528838656 gin_utils.py:85] # Parameters for layer_norm.T5LayerNorm:
I0512 23:44:42.748731 140180528838656 gin_utils.py:85] # ==============================================================================
I0512 23:44:42.748785 140180528838656 gin_utils.py:85] layer_norm.T5LayerNorm.dtype = %ACTIVATION_DTYPE
I0512 23:44:42.748819 140180528838656 gin_utils.py:85] 
I0512 23:44:42.748851 140180528838656 gin_utils.py:85] # Parameters for routing.TokensChooseMaskedRouter:
I0512 23:44:42.748882 140180528838656 gin_utils.py:85] # ==============================================================================
I0512 23:44:42.748914 140180528838656 gin_utils.py:85] routing.TokensChooseMaskedRouter.dtype = 'float32'
I0512 23:44:42.748946 140180528838656 gin_utils.py:85] routing.TokensChooseMaskedRouter.ignore_padding_tokens = False
I0512 23:44:42.748977 140180528838656 gin_utils.py:85] routing.TokensChooseMaskedRouter.jitter_noise = %JITTER_NOISE
I0512 23:44:42.749009 140180528838656 gin_utils.py:85] routing.TokensChooseMaskedRouter.num_selected_experts = %NUM_SELECTED_EXPERTS
I0512 23:44:42.749060 140180528838656 gin_utils.py:85] routing.TokensChooseMaskedRouter.router_weights = @routing.RouterWeights()
I0512 23:44:42.749096 140180528838656 gin_utils.py:85] 
I0512 23:44:42.749129 140180528838656 gin_utils.py:85] # Parameters for sparse_decoder/routing.TokensChooseMaskedRouter:
I0512 23:44:42.749161 140180528838656 gin_utils.py:85] # ==============================================================================
I0512 23:44:42.749192 140180528838656 gin_utils.py:85] sparse_decoder/routing.TokensChooseMaskedRouter.batch_prioritized_routing = False
I0512 23:44:42.749225 140180528838656 gin_utils.py:85] 
I0512 23:44:42.749258 140180528838656 gin_utils.py:85] # Parameters for sparse_encoder/routing.TokensChooseMaskedRouter:
I0512 23:44:42.749290 140180528838656 gin_utils.py:85] # ==============================================================================
I0512 23:44:42.749322 140180528838656 gin_utils.py:85] sparse_encoder/routing.TokensChooseMaskedRouter.batch_prioritized_routing = False
I0512 23:44:42.749353 140180528838656 gin_utils.py:85] 
I0512 23:44:42.749386 140180528838656 gin_utils.py:85] # Parameters for train_script.train:
I0512 23:44:42.749418 140180528838656 gin_utils.py:85] # ==============================================================================
I0512 23:44:42.749450 140180528838656 gin_utils.py:85] train_script.train.checkpoint_cfg = @utils.CheckpointConfig()
I0512 23:44:42.749488 140180528838656 gin_utils.py:85] train_script.train.eval_period = 2500
I0512 23:44:42.749521 140180528838656 gin_utils.py:85] train_script.train.eval_steps = 20
I0512 23:44:42.749553 140180528838656 gin_utils.py:85] train_script.train.infer_eval_dataset_cfg = None
I0512 23:44:42.749584 140180528838656 gin_utils.py:85] train_script.train.model = %MODEL
I0512 23:44:42.749616 140180528838656 gin_utils.py:85] train_script.train.model_dir = %MODEL_DIR
I0512 23:44:42.749647 140180528838656 gin_utils.py:85] train_script.train.partitioner = @partitioning.MoePjitPartitioner()
I0512 23:44:42.749679 140180528838656 gin_utils.py:85] train_script.train.random_seed = %RANDOM_SEED
I0512 23:44:42.749710 140180528838656 gin_utils.py:85] train_script.train.stats_period = 10
I0512 23:44:42.749742 140180528838656 gin_utils.py:85] train_script.train.summarize_config_fn = @gin_utils.summarize_gin_config
I0512 23:44:42.749773 140180528838656 gin_utils.py:85] train_script.train.total_steps = %TRAIN_STEPS
I0512 23:44:42.749805 140180528838656 gin_utils.py:85] train_script.train.train_dataset_cfg = @train/utils.DatasetConfig()
I0512 23:44:42.749837 140180528838656 gin_utils.py:85] train_script.train.train_eval_dataset_cfg = None
I0512 23:44:42.749869 140180528838656 gin_utils.py:85] train_script.train.trainer_cls = @moe_trainer.MoeTrainer
I0512 23:44:42.749900 140180528838656 gin_utils.py:85] train_script.train.use_hardware_rng = %USE_HARDWARE_RNG
I0512 23:44:42.749932 140180528838656 gin_utils.py:85] 
I0512 23:44:42.749963 140180528838656 gin_utils.py:85] # Parameters for trainer.Trainer:
I0512 23:44:42.749995 140180528838656 gin_utils.py:85] # ==============================================================================
I0512 23:44:42.750026 140180528838656 gin_utils.py:85] trainer.Trainer.learning_rate_fn = @utils.create_learning_rate_scheduler()
I0512 23:44:42.750058 140180528838656 gin_utils.py:85] trainer.Trainer.num_microbatches = None
I0512 23:44:42.750089 140180528838656 gin_utils.py:85] 
I0512 23:44:42.750121 140180528838656 gin_utils.py:85] # Parameters for attention_kernel_init/linen.initializers.variance_scaling:
I0512 23:44:42.750153 140180528838656 gin_utils.py:85] # ==============================================================================
I0512 23:44:42.750184 140180528838656 gin_utils.py:85] attention_kernel_init/linen.initializers.variance_scaling.distribution = 'normal'
I0512 23:44:42.750217 140180528838656 gin_utils.py:85] attention_kernel_init/linen.initializers.variance_scaling.mode = 'fan_in'
I0512 23:44:42.750251 140180528838656 gin_utils.py:85] attention_kernel_init/linen.initializers.variance_scaling.scale = %SCALE
I0512 23:44:42.750283 140180528838656 gin_utils.py:85] 
I0512 23:44:42.750314 140180528838656 gin_utils.py:85] # Parameters for expert_kernel_init/linen.initializers.variance_scaling:
I0512 23:44:42.750345 140180528838656 gin_utils.py:85] # ==============================================================================
I0512 23:44:42.750377 140180528838656 gin_utils.py:85] expert_kernel_init/linen.initializers.variance_scaling.distribution = 'normal'
I0512 23:44:42.750409 140180528838656 gin_utils.py:85] expert_kernel_init/linen.initializers.variance_scaling.mode = 'fan_in'
I0512 23:44:42.750440 140180528838656 gin_utils.py:85] expert_kernel_init/linen.initializers.variance_scaling.scale = %SCALE
I0512 23:44:42.750477 140180528838656 gin_utils.py:85] 
I0512 23:44:42.750510 140180528838656 gin_utils.py:85] # Parameters for mlp_kernel_init/linen.initializers.variance_scaling:
I0512 23:44:42.750542 140180528838656 gin_utils.py:85] # ==============================================================================
I0512 23:44:42.750574 140180528838656 gin_utils.py:85] mlp_kernel_init/linen.initializers.variance_scaling.distribution = \
I0512 23:44:42.750605 140180528838656 gin_utils.py:85]     'truncated_normal'
I0512 23:44:42.750637 140180528838656 gin_utils.py:85] mlp_kernel_init/linen.initializers.variance_scaling.mode = 'fan_in'
I0512 23:44:42.750669 140180528838656 gin_utils.py:85] mlp_kernel_init/linen.initializers.variance_scaling.scale = %SCALE
I0512 23:44:42.750700 140180528838656 gin_utils.py:85] 
I0512 23:44:42.750731 140180528838656 gin_utils.py:85] # Parameters for output_logits_kernel_init/linen.initializers.variance_scaling:
I0512 23:44:42.750762 140180528838656 gin_utils.py:85] # ==============================================================================
I0512 23:44:42.750795 140180528838656 gin_utils.py:85] output_logits_kernel_init/linen.initializers.variance_scaling.distribution = \
I0512 23:44:42.750826 140180528838656 gin_utils.py:85]     'truncated_normal'
I0512 23:44:42.750858 140180528838656 gin_utils.py:85] output_logits_kernel_init/linen.initializers.variance_scaling.mode = 'fan_in'
I0512 23:44:42.750890 140180528838656 gin_utils.py:85] output_logits_kernel_init/linen.initializers.variance_scaling.scale = %SCALE
I0512 23:44:42.751817 140180528838656 partitioning.py:559] `activation_partitioning_dims` = 2, `parameter_partitioning_dims` = 1
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
I0000 00:00:1715557482.815636  369400 pjrt_api.cc:98] GetPjrtApi was found for tpu at /home/rosinality/openmoe_venv/lib/python3.10/site-packages/libtpu/libtpu.so
I0000 00:00:1715557482.815700  369400 pjrt_api.cc:67] PJRT_Api is set for device type tpu
I0000 00:00:1715557482.815702  369400 pjrt_api.cc:72] PJRT plugin for tpu has PJRT API version 0.30. The framework PJRT API version is 0.30.
I0512 23:44:42.412619 140143678494720 gin_utils.py:85] train_script.train.total_steps = %TRAIN_STEPS
I0512 23:44:42.412651 140143678494720 gin_utils.py:85] train_script.train.train_dataset_cfg = @train/utils.DatasetConfig()
I0512 23:44:42.412683 140143678494720 gin_utils.py:85] train_script.train.train_eval_dataset_cfg = None
I0512 23:44:42.412714 140143678494720 gin_utils.py:85] train_script.train.trainer_cls = @moe_trainer.MoeTrainer
I0512 23:44:42.412746 140143678494720 gin_utils.py:85] train_script.train.use_hardware_rng = %USE_HARDWARE_RNG
I0512 23:44:42.412778 140143678494720 gin_utils.py:85] 
I0512 23:44:42.412810 140143678494720 gin_utils.py:85] # Parameters for trainer.Trainer:
I0512 23:44:42.412842 140143678494720 gin_utils.py:85] # ==============================================================================
I0512 23:44:42.412874 140143678494720 gin_utils.py:85] trainer.Trainer.learning_rate_fn = @utils.create_learning_rate_scheduler()
I0512 23:44:42.412906 140143678494720 gin_utils.py:85] trainer.Trainer.num_microbatches = None
I0512 23:44:42.412938 140143678494720 gin_utils.py:85] 
I0512 23:44:42.412970 140143678494720 gin_utils.py:85] # Parameters for attention_kernel_init/linen.initializers.variance_scaling:
I0512 23:44:42.413002 140143678494720 gin_utils.py:85] # ==============================================================================
I0512 23:44:42.413033 140143678494720 gin_utils.py:85] attention_kernel_init/linen.initializers.variance_scaling.distribution = 'normal'
I0512 23:44:42.413065 140143678494720 gin_utils.py:85] attention_kernel_init/linen.initializers.variance_scaling.mode = 'fan_in'
I0512 23:44:42.413097 140143678494720 gin_utils.py:85] attention_kernel_init/linen.initializers.variance_scaling.scale = %SCALE
I0512 23:44:42.413129 140143678494720 gin_utils.py:85] 
I0512 23:44:42.413161 140143678494720 gin_utils.py:85] # Parameters for expert_kernel_init/linen.initializers.variance_scaling:
I0512 23:44:42.413193 140143678494720 gin_utils.py:85] # ==============================================================================
I0512 23:44:42.413225 140143678494720 gin_utils.py:85] expert_kernel_init/linen.initializers.variance_scaling.distribution = 'normal'
I0512 23:44:42.413256 140143678494720 gin_utils.py:85] expert_kernel_init/linen.initializers.variance_scaling.mode = 'fan_in'
I0512 23:44:42.413290 140143678494720 gin_utils.py:85] expert_kernel_init/linen.initializers.variance_scaling.scale = %SCALE
I0512 23:44:42.413323 140143678494720 gin_utils.py:85] 
I0512 23:44:42.413356 140143678494720 gin_utils.py:85] # Parameters for mlp_kernel_init/linen.initializers.variance_scaling:
I0512 23:44:42.413388 140143678494720 gin_utils.py:85] # ==============================================================================
I0512 23:44:42.413419 140143678494720 gin_utils.py:85] mlp_kernel_init/linen.initializers.variance_scaling.distribution = \
I0512 23:44:42.413452 140143678494720 gin_utils.py:85]     'truncated_normal'
I0512 23:44:42.413484 140143678494720 gin_utils.py:85] mlp_kernel_init/linen.initializers.variance_scaling.mode = 'fan_in'
I0512 23:44:42.413515 140143678494720 gin_utils.py:85] mlp_kernel_init/linen.initializers.variance_scaling.scale = %SCALE
I0512 23:44:42.413554 140143678494720 gin_utils.py:85] 
I0512 23:44:42.413586 140143678494720 gin_utils.py:85] # Parameters for output_logits_kernel_init/linen.initializers.variance_scaling:
I0512 23:44:42.413619 140143678494720 gin_utils.py:85] # ==============================================================================
I0512 23:44:42.413650 140143678494720 gin_utils.py:85] output_logits_kernel_init/linen.initializers.variance_scaling.distribution = \
I0512 23:44:42.413682 140143678494720 gin_utils.py:85]     'truncated_normal'
I0512 23:44:42.413714 140143678494720 gin_utils.py:85] output_logits_kernel_init/linen.initializers.variance_scaling.mode = 'fan_in'
I0512 23:44:42.413746 140143678494720 gin_utils.py:85] output_logits_kernel_init/linen.initializers.variance_scaling.scale = %SCALE
I0512 23:44:42.414678 140143678494720 partitioning.py:559] `activation_partitioning_dims` = 2, `parameter_partitioning_dims` = 1
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
I0000 00:00:1715557482.462770  406878 pjrt_api.cc:98] GetPjrtApi was found for tpu at /home/rosinality/openmoe_venv/lib/python3.10/site-packages/libtpu/libtpu.so
I0000 00:00:1715557482.462829  406878 pjrt_api.cc:67] PJRT_Api is set for device type tpu
I0000 00:00:1715557482.462832  406878 pjrt_api.cc:72] PJRT plugin for tpu has PJRT API version 0.30. The framework PJRT API version is 0.30.
I0512 23:44:42.719660 140260698863616 gin_utils.py:85] train_script.train.eval_period = 2500
I0512 23:44:42.719692 140260698863616 gin_utils.py:85] train_script.train.eval_steps = 20
I0512 23:44:42.719726 140260698863616 gin_utils.py:85] train_script.train.infer_eval_dataset_cfg = None
I0512 23:44:42.719759 140260698863616 gin_utils.py:85] train_script.train.model = %MODEL
I0512 23:44:42.719791 140260698863616 gin_utils.py:85] train_script.train.model_dir = %MODEL_DIR
I0512 23:44:42.719822 140260698863616 gin_utils.py:85] train_script.train.partitioner = @moe_partitioning.MoePjitPartitioner()
I0512 23:44:42.719854 140260698863616 gin_utils.py:85] train_script.train.random_seed = %RANDOM_SEED
I0512 23:44:42.719885 140260698863616 gin_utils.py:85] train_script.train.stats_period = 10
I0512 23:44:42.719916 140260698863616 gin_utils.py:85] train_script.train.summarize_config_fn = @gin_utils.summarize_gin_config
I0512 23:44:42.719948 140260698863616 gin_utils.py:85] train_script.train.total_steps = %TRAIN_STEPS
I0512 23:44:42.719980 140260698863616 gin_utils.py:85] train_script.train.train_dataset_cfg = @train/utils.DatasetConfig()
I0512 23:44:42.720011 140260698863616 gin_utils.py:85] train_script.train.train_eval_dataset_cfg = None
I0512 23:44:42.720042 140260698863616 gin_utils.py:85] train_script.train.trainer_cls = @moe_trainer.MoeTrainer
I0512 23:44:42.720074 140260698863616 gin_utils.py:85] train_script.train.use_hardware_rng = %USE_HARDWARE_RNG
I0512 23:44:42.720105 140260698863616 gin_utils.py:85] 
I0512 23:44:42.720136 140260698863616 gin_utils.py:85] # Parameters for trainer.Trainer:
I0512 23:44:42.720168 140260698863616 gin_utils.py:85] # ==============================================================================
I0512 23:44:42.720199 140260698863616 gin_utils.py:85] trainer.Trainer.learning_rate_fn = @utils.create_learning_rate_scheduler()
I0512 23:44:42.720231 140260698863616 gin_utils.py:85] trainer.Trainer.num_microbatches = None
I0512 23:44:42.720268 140260698863616 gin_utils.py:85] 
I0512 23:44:42.720301 140260698863616 gin_utils.py:85] # Parameters for attention_kernel_init/linen.initializers.variance_scaling:
I0512 23:44:42.720333 140260698863616 gin_utils.py:85] # ==============================================================================
I0512 23:44:42.720364 140260698863616 gin_utils.py:85] attention_kernel_init/linen.initializers.variance_scaling.distribution = 'normal'
I0512 23:44:42.720396 140260698863616 gin_utils.py:85] attention_kernel_init/linen.initializers.variance_scaling.mode = 'fan_in'
I0512 23:44:42.720427 140260698863616 gin_utils.py:85] attention_kernel_init/linen.initializers.variance_scaling.scale = %SCALE
I0512 23:44:42.720458 140260698863616 gin_utils.py:85] 
I0512 23:44:42.720489 140260698863616 gin_utils.py:85] # Parameters for expert_kernel_init/linen.initializers.variance_scaling:
I0512 23:44:42.720523 140260698863616 gin_utils.py:85] # ==============================================================================
I0512 23:44:42.720555 140260698863616 gin_utils.py:85] expert_kernel_init/linen.initializers.variance_scaling.distribution = 'normal'
I0512 23:44:42.720587 140260698863616 gin_utils.py:85] expert_kernel_init/linen.initializers.variance_scaling.mode = 'fan_in'
I0512 23:44:42.720618 140260698863616 gin_utils.py:85] expert_kernel_init/linen.initializers.variance_scaling.scale = %SCALE
I0512 23:44:42.720650 140260698863616 gin_utils.py:85] 
I0512 23:44:42.720681 140260698863616 gin_utils.py:85] # Parameters for mlp_kernel_init/linen.initializers.variance_scaling:
I0512 23:44:42.720712 140260698863616 gin_utils.py:85] # ==============================================================================
I0512 23:44:42.720744 140260698863616 gin_utils.py:85] mlp_kernel_init/linen.initializers.variance_scaling.distribution = \
I0512 23:44:42.720775 140260698863616 gin_utils.py:85]     'truncated_normal'
I0512 23:44:42.720807 140260698863616 gin_utils.py:85] mlp_kernel_init/linen.initializers.variance_scaling.mode = 'fan_in'
I0512 23:44:42.720839 140260698863616 gin_utils.py:85] mlp_kernel_init/linen.initializers.variance_scaling.scale = %SCALE
I0512 23:44:42.720870 140260698863616 gin_utils.py:85] 
I0512 23:44:42.720901 140260698863616 gin_utils.py:85] # Parameters for output_logits_kernel_init/linen.initializers.variance_scaling:
I0512 23:44:42.720933 140260698863616 gin_utils.py:85] # ==============================================================================
I0512 23:44:42.720964 140260698863616 gin_utils.py:85] output_logits_kernel_init/linen.initializers.variance_scaling.distribution = \
I0512 23:44:42.720996 140260698863616 gin_utils.py:85]     'truncated_normal'
I0512 23:44:42.721027 140260698863616 gin_utils.py:85] output_logits_kernel_init/linen.initializers.variance_scaling.mode = 'fan_in'
I0512 23:44:42.721059 140260698863616 gin_utils.py:85] output_logits_kernel_init/linen.initializers.variance_scaling.scale = %SCALE
I0512 23:44:42.721983 140260698863616 partitioning.py:559] `activation_partitioning_dims` = 2, `parameter_partitioning_dims` = 1
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
I0000 00:00:1715557482.797367  390926 pjrt_api.cc:98] GetPjrtApi was found for tpu at /home/rosinality/openmoe_venv/lib/python3.10/site-packages/libtpu/libtpu.so
I0000 00:00:1715557482.797438  390926 pjrt_api.cc:67] PJRT_Api is set for device type tpu
I0000 00:00:1715557482.797441  390926 pjrt_api.cc:72] PJRT plugin for tpu has PJRT API version 0.30. The framework PJRT API version is 0.30.
I0000 00:00:1715557486.271465  368954 tfrt_cpu_pjrt_client.cc:349] TfrtCpuClient created.
I0512 23:44:46.271717 139666521167872 train.py:196] Process ID: 3
I0000 00:00:1715557486.289053  406878 tfrt_cpu_pjrt_client.cc:349] TfrtCpuClient created.
I0512 23:44:46.289263 140143678494720 train.py:196] Process ID: 0
I0000 00:00:1715557486.298455  368289 tfrt_cpu_pjrt_client.cc:349] TfrtCpuClient created.
I0512 23:44:46.298697 139876724979712 train.py:196] Process ID: 2
I0000 00:00:1715557486.301753  374664 tfrt_cpu_pjrt_client.cc:349] TfrtCpuClient created.
I0512 23:44:46.302025 140193652815872 train.py:196] Process ID: 4
I0000 00:00:1715557486.304245  369400 tfrt_cpu_pjrt_client.cc:349] TfrtCpuClient created.
I0512 23:44:46.304536 140180528838656 train.py:196] Process ID: 5
I0000 00:00:1715557486.306533  390926 tfrt_cpu_pjrt_client.cc:349] TfrtCpuClient created.
I0512 23:44:46.306798 140260698863616 train.py:196] Process ID: 6
I0000 00:00:1715557486.333780  393698 tfrt_cpu_pjrt_client.cc:349] TfrtCpuClient created.
I0512 23:44:46.334027 139856202532864 train.py:196] Process ID: 7
I0512 23:44:46.338235 139666521167872 train.py:250] Using fast RngBitGenerator PRNG for initialization and dropout.
I0512 23:44:46.350752 140143678494720 train.py:250] Using fast RngBitGenerator PRNG for initialization and dropout.
I0512 23:44:46.365909 139876724979712 train.py:250] Using fast RngBitGenerator PRNG for initialization and dropout.
I0000 00:00:1715557486.348307  370028 tfrt_cpu_pjrt_client.cc:349] TfrtCpuClient created.
I0512 23:44:46.348536 140643590531072 train.py:196] Process ID: 1
I0512 23:44:46.370997 140193652815872 train.py:250] Using fast RngBitGenerator PRNG for initialization and dropout.
I0512 23:44:46.366362 140180528838656 train.py:250] Using fast RngBitGenerator PRNG for initialization and dropout.
I0512 23:44:46.374716 140260698863616 train.py:250] Using fast RngBitGenerator PRNG for initialization and dropout.
I0512 23:44:46.399488 139856202532864 train.py:250] Using fast RngBitGenerator PRNG for initialization and dropout.
I0512 23:44:46.414685 140643590531072 train.py:250] Using fast RngBitGenerator PRNG for initialization and dropout.
I0512 23:44:46.499838 139876724979712 train.py:256] Random seed not provided, using RNG seed 1715557486
I0512 23:44:46.499806 139856202532864 train.py:256] Random seed not provided, using RNG seed 1715557486
I0512 23:44:46.499856 139666521167872 train.py:256] Random seed not provided, using RNG seed 1715557486
I0512 23:44:46.499826 140193652815872 train.py:256] Random seed not provided, using RNG seed 1715557486
I0512 23:44:46.499793 140143678494720 train.py:256] Random seed not provided, using RNG seed 1715557486
I0512 23:44:46.499929 140643590531072 train.py:256] Random seed not provided, using RNG seed 1715557486
I0512 23:44:46.499919 140180528838656 train.py:256] Random seed not provided, using RNG seed 1715557486
I0512 23:44:46.499916 140260698863616 train.py:256] Random seed not provided, using RNG seed 1715557486
I0512 23:44:46.687127 140143678494720 partitioning.py:94] For MoE, first construct vanilla T5X (data, model) mesh.
I0512 23:44:46.687488 140143678494720 partitioning.py:151] last device coords : (1, 3, 3, 0)
last local device coords: (1, 1, 0, 0)
W0512 23:44:46.687677 140143678494720 partitioning.py:197] Tiling device assignment mesh by hosts, which may lead to reduced XLA collective performance. To avoid this, modify the model parallel submesh or run with more tasks per host.
I0512 23:44:46.687850 140143678494720 partitioning.py:255] global_mesh axis_names: ('data', 'model')
I0512 23:44:46.687903 140143678494720 partitioning.py:256] global_mesh devices: [[TpuDevice(id=0, process_index=0, coords=(0,0,0), core_on_chip=0)
  TpuDevice(id=2, process_index=0, coords=(0,1,0), core_on_chip=0)
  TpuDevice(id=1, process_index=0, coords=(1,0,0), core_on_chip=0)
  TpuDevice(id=3, process_index=0, coords=(1,1,0), core_on_chip=0)]
 [TpuDevice(id=8, process_index=2, coords=(0,0,1), core_on_chip=0)
  TpuDevice(id=10, process_index=2, coords=(0,1,1), core_on_chip=0)
  TpuDevice(id=9, process_index=2, coords=(1,0,1), core_on_chip=0)
  TpuDevice(id=11, process_index=2, coords=(1,1,1), core_on_chip=0)]
 [TpuDevice(id=16, process_index=4, coords=(0,0,2), core_on_chip=0)
  TpuDevice(id=18, process_index=4, coords=(0,1,2), core_on_chip=0)
  TpuDevice(id=17, process_index=4, coords=(1,0,2), core_on_chip=0)
  TpuDevice(id=19, process_index=4, coords=(1,1,2), core_on_chip=0)]
 [TpuDevice(id=24, process_index=6, coords=(0,0,3), core_on_chip=0)
  TpuDevice(id=26, process_index=6, coords=(0,1,3), core_on_chip=0)
  TpuDevice(id=25, process_index=6, coords=(1,0,3), core_on_chip=0)
  TpuDevice(id=27, process_index=6, coords=(1,1,3), core_on_chip=0)]
 [TpuDevice(id=4, process_index=1, coords=(0,2,0), core_on_chip=0)
  TpuDevice(id=6, process_index=1, coords=(0,3,0), core_on_chip=0)
  TpuDevice(id=5, process_index=1, coords=(1,2,0), core_on_chip=0)
  TpuDevice(id=7, process_index=1, coords=(1,3,0), core_on_chip=0)]
 [TpuDevice(id=12, process_index=3, coords=(0,2,1), core_on_chip=0)
  TpuDevice(id=14, process_index=3, coords=(0,3,1), core_on_chip=0)
  TpuDevice(id=13, process_index=3, coords=(1,2,1), core_on_chip=0)
  TpuDevice(id=15, process_index=3, coords=(1,3,1), core_on_chip=0)]
 [TpuDevice(id=20, process_index=5, coords=(0,2,2), core_on_chip=0)
  TpuDevice(id=22, process_index=5, coords=(0,3,2), core_on_chip=0)
  TpuDevice(id=21, process_index=5, coords=(1,2,2), core_on_chip=0)
  TpuDevice(id=23, process_index=5, coords=(1,3,2), core_on_chip=0)]
 [TpuDevice(id=28, process_index=7, coords=(0,2,3), core_on_chip=0)
  TpuDevice(id=30, process_index=7, coords=(0,3,3), core_on_chip=0)
  TpuDevice(id=29, process_index=7, coords=(1,2,3), core_on_chip=0)
  TpuDevice(id=31, process_index=7, coords=(1,3,3), core_on_chip=0)]]
I0512 23:44:46.688122 140143678494720 partitioning.py:257] global_mesh devices shape: (8, 4)
I0512 23:44:46.688182 140143678494720 partitioning.py:111] Overridden MoE global_mesh axes_names: ('data', 'expert', 'model')
I0512 23:44:46.688223 140143678494720 partitioning.py:114] Overridden MoE global_mesh devices: [[[TpuDevice(id=0, process_index=0, coords=(0,0,0), core_on_chip=0)
   TpuDevice(id=2, process_index=0, coords=(0,1,0), core_on_chip=0)
   TpuDevice(id=1, process_index=0, coords=(1,0,0), core_on_chip=0)
   TpuDevice(id=3, process_index=0, coords=(1,1,0), core_on_chip=0)]
  [TpuDevice(id=8, process_index=2, coords=(0,0,1), core_on_chip=0)
   TpuDevice(id=10, process_index=2, coords=(0,1,1), core_on_chip=0)
   TpuDevice(id=9, process_index=2, coords=(1,0,1), core_on_chip=0)
   TpuDevice(id=11, process_index=2, coords=(1,1,1), core_on_chip=0)]
  [TpuDevice(id=16, process_index=4, coords=(0,0,2), core_on_chip=0)
   TpuDevice(id=18, process_index=4, coords=(0,1,2), core_on_chip=0)
   TpuDevice(id=17, process_index=4, coords=(1,0,2), core_on_chip=0)
   TpuDevice(id=19, process_index=4, coords=(1,1,2), core_on_chip=0)]
  [TpuDevice(id=24, process_index=6, coords=(0,0,3), core_on_chip=0)
   TpuDevice(id=26, process_index=6, coords=(0,1,3), core_on_chip=0)
   TpuDevice(id=25, process_index=6, coords=(1,0,3), core_on_chip=0)
   TpuDevice(id=27, process_index=6, coords=(1,1,3), core_on_chip=0)]
  [TpuDevice(id=4, process_index=1, coords=(0,2,0), core_on_chip=0)
   TpuDevice(id=6, process_index=1, coords=(0,3,0), core_on_chip=0)
   TpuDevice(id=5, process_index=1, coords=(1,2,0), core_on_chip=0)
   TpuDevice(id=7, process_index=1, coords=(1,3,0), core_on_chip=0)]
  [TpuDevice(id=12, process_index=3, coords=(0,2,1), core_on_chip=0)
   TpuDevice(id=14, process_index=3, coords=(0,3,1), core_on_chip=0)
   TpuDevice(id=13, process_index=3, coords=(1,2,1), core_on_chip=0)
   TpuDevice(id=15, process_index=3, coords=(1,3,1), core_on_chip=0)]
  [TpuDevice(id=20, process_index=5, coords=(0,2,2), core_on_chip=0)
   TpuDevice(id=22, process_index=5, coords=(0,3,2), core_on_chip=0)
   TpuDevice(id=21, process_index=5, coords=(1,2,2), core_on_chip=0)
   TpuDevice(id=23, process_index=5, coords=(1,3,2), core_on_chip=0)]
  [TpuDevice(id=28, process_index=7, coords=(0,2,3), core_on_chip=0)
   TpuDevice(id=30, process_index=7, coords=(0,3,3), core_on_chip=0)
   TpuDevice(id=29, process_index=7, coords=(1,2,3), core_on_chip=0)
   TpuDevice(id=31, process_index=7, coords=(1,3,3), core_on_chip=0)]]]
I0512 23:44:46.688401 140143678494720 partitioning.py:115] Overridden MoE global_mesh shape: (1, 8, 4)
I0512 23:44:46.693441 139666521167872 partitioning.py:94] For MoE, first construct vanilla T5X (data, model) mesh.
I0512 23:44:46.693818 139666521167872 partitioning.py:151] last device coords : (1, 3, 3, 0)
last local device coords: (1, 1, 0, 0)
W0512 23:44:46.694012 139666521167872 partitioning.py:197] Tiling device assignment mesh by hosts, which may lead to reduced XLA collective performance. To avoid this, modify the model parallel submesh or run with more tasks per host.
I0512 23:44:46.694186 139666521167872 partitioning.py:255] global_mesh axis_names: ('data', 'model')
I0512 23:44:46.694242 139666521167872 partitioning.py:256] global_mesh devices: [[TpuDevice(id=0, process_index=0, coords=(0,0,0), core_on_chip=0)
  TpuDevice(id=2, process_index=0, coords=(0,1,0), core_on_chip=0)
  TpuDevice(id=1, process_index=0, coords=(1,0,0), core_on_chip=0)
  TpuDevice(id=3, process_index=0, coords=(1,1,0), core_on_chip=0)]
 [TpuDevice(id=8, process_index=2, coords=(0,0,1), core_on_chip=0)
  TpuDevice(id=10, process_index=2, coords=(0,1,1), core_on_chip=0)
  TpuDevice(id=9, process_index=2, coords=(1,0,1), core_on_chip=0)
  TpuDevice(id=11, process_index=2, coords=(1,1,1), core_on_chip=0)]
 [TpuDevice(id=16, process_index=4, coords=(0,0,2), core_on_chip=0)
  TpuDevice(id=18, process_index=4, coords=(0,1,2), core_on_chip=0)
  TpuDevice(id=17, process_index=4, coords=(1,0,2), core_on_chip=0)
  TpuDevice(id=19, process_index=4, coords=(1,1,2), core_on_chip=0)]
 [TpuDevice(id=24, process_index=6, coords=(0,0,3), core_on_chip=0)
  TpuDevice(id=26, process_index=6, coords=(0,1,3), core_on_chip=0)
  TpuDevice(id=25, process_index=6, coords=(1,0,3), core_on_chip=0)
  TpuDevice(id=27, process_index=6, coords=(1,1,3), core_on_chip=0)]
 [TpuDevice(id=4, process_index=1, coords=(0,2,0), core_on_chip=0)
  TpuDevice(id=6, process_index=1, coords=(0,3,0), core_on_chip=0)
  TpuDevice(id=5, process_index=1, coords=(1,2,0), core_on_chip=0)
  TpuDevice(id=7, process_index=1, coords=(1,3,0), core_on_chip=0)]
 [TpuDevice(id=12, process_index=3, coords=(0,2,1), core_on_chip=0)
  TpuDevice(id=14, process_index=3, coords=(0,3,1), core_on_chip=0)
  TpuDevice(id=13, process_index=3, coords=(1,2,1), core_on_chip=0)
  TpuDevice(id=15, process_index=3, coords=(1,3,1), core_on_chip=0)]
 [TpuDevice(id=20, process_index=5, coords=(0,2,2), core_on_chip=0)
  TpuDevice(id=22, process_index=5, coords=(0,3,2), core_on_chip=0)
  TpuDevice(id=21, process_index=5, coords=(1,2,2), core_on_chip=0)
  TpuDevice(id=23, process_index=5, coords=(1,3,2), core_on_chip=0)]
 [TpuDevice(id=28, process_index=7, coords=(0,2,3), core_on_chip=0)
  TpuDevice(id=30, process_index=7, coords=(0,3,3), core_on_chip=0)
  TpuDevice(id=29, process_index=7, coords=(1,2,3), core_on_chip=0)
  TpuDevice(id=31, process_index=7, coords=(1,3,3), core_on_chip=0)]]
I0512 23:44:46.694484 139666521167872 partitioning.py:257] global_mesh devices shape: (8, 4)
I0512 23:44:46.694551 139666521167872 partitioning.py:111] Overridden MoE global_mesh axes_names: ('data', 'expert', 'model')
I0512 23:44:46.694596 139666521167872 partitioning.py:114] Overridden MoE global_mesh devices: [[[TpuDevice(id=0, process_index=0, coords=(0,0,0), core_on_chip=0)
   TpuDevice(id=2, process_index=0, coords=(0,1,0), core_on_chip=0)
   TpuDevice(id=1, process_index=0, coords=(1,0,0), core_on_chip=0)
   TpuDevice(id=3, process_index=0, coords=(1,1,0), core_on_chip=0)]
  [TpuDevice(id=8, process_index=2, coords=(0,0,1), core_on_chip=0)
   TpuDevice(id=10, process_index=2, coords=(0,1,1), core_on_chip=0)
   TpuDevice(id=9, process_index=2, coords=(1,0,1), core_on_chip=0)
   TpuDevice(id=11, process_index=2, coords=(1,1,1), core_on_chip=0)]
  [TpuDevice(id=16, process_index=4, coords=(0,0,2), core_on_chip=0)
   TpuDevice(id=18, process_index=4, coords=(0,1,2), core_on_chip=0)
   TpuDevice(id=17, process_index=4, coords=(1,0,2), core_on_chip=0)
   TpuDevice(id=19, process_index=4, coords=(1,1,2), core_on_chip=0)]
  [TpuDevice(id=24, process_index=6, coords=(0,0,3), core_on_chip=0)
   TpuDevice(id=26, process_index=6, coords=(0,1,3), core_on_chip=0)
   TpuDevice(id=25, process_index=6, coords=(1,0,3), core_on_chip=0)
   TpuDevice(id=27, process_index=6, coords=(1,1,3), core_on_chip=0)]
  [TpuDevice(id=4, process_index=1, coords=(0,2,0), core_on_chip=0)
   TpuDevice(id=6, process_index=1, coords=(0,3,0), core_on_chip=0)
   TpuDevice(id=5, process_index=1, coords=(1,2,0), core_on_chip=0)
   TpuDevice(id=7, process_index=1, coords=(1,3,0), core_on_chip=0)]
  [TpuDevice(id=12, process_index=3, coords=(0,2,1), core_on_chip=0)
   TpuDevice(id=14, process_index=3, coords=(0,3,1), core_on_chip=0)
   TpuDevice(id=13, process_index=3, coords=(1,2,1), core_on_chip=0)
   TpuDevice(id=15, process_index=3, coords=(1,3,1), core_on_chip=0)]
  [TpuDevice(id=20, process_index=5, coords=(0,2,2), core_on_chip=0)
   TpuDevice(id=22, process_index=5, coords=(0,3,2), core_on_chip=0)
   TpuDevice(id=21, process_index=5, coords=(1,2,2), core_on_chip=0)
   TpuDevice(id=23, process_index=5, coords=(1,3,2), core_on_chip=0)]
  [TpuDevice(id=28, process_index=7, coords=(0,2,3), core_on_chip=0)
   TpuDevice(id=30, process_index=7, coords=(0,3,3), core_on_chip=0)
   TpuDevice(id=29, process_index=7, coords=(1,2,3), core_on_chip=0)
   TpuDevice(id=31, process_index=7, coords=(1,3,3), core_on_chip=0)]]]
I0512 23:44:46.695376 139876724979712 partitioning.py:94] For MoE, first construct vanilla T5X (data, model) mesh.
I0512 23:44:46.695777 139876724979712 partitioning.py:151] last device coords : (1, 3, 3, 0)
last local device coords: (1, 1, 0, 0)
W0512 23:44:46.695973 139876724979712 partitioning.py:197] Tiling device assignment mesh by hosts, which may lead to reduced XLA collective performance. To avoid this, modify the model parallel submesh or run with more tasks per host.
I0512 23:44:46.694783 139666521167872 partitioning.py:115] Overridden MoE global_mesh shape: (1, 8, 4)
I0512 23:44:46.696136 139876724979712 partitioning.py:255] global_mesh axis_names: ('data', 'model')
I0512 23:44:46.696191 139876724979712 partitioning.py:256] global_mesh devices: [[TpuDevice(id=0, process_index=0, coords=(0,0,0), core_on_chip=0)
  TpuDevice(id=2, process_index=0, coords=(0,1,0), core_on_chip=0)
  TpuDevice(id=1, process_index=0, coords=(1,0,0), core_on_chip=0)
  TpuDevice(id=3, process_index=0, coords=(1,1,0), core_on_chip=0)]
 [TpuDevice(id=8, process_index=2, coords=(0,0,1), core_on_chip=0)
  TpuDevice(id=10, process_index=2, coords=(0,1,1), core_on_chip=0)
  TpuDevice(id=9, process_index=2, coords=(1,0,1), core_on_chip=0)
  TpuDevice(id=11, process_index=2, coords=(1,1,1), core_on_chip=0)]
 [TpuDevice(id=16, process_index=4, coords=(0,0,2), core_on_chip=0)
  TpuDevice(id=18, process_index=4, coords=(0,1,2), core_on_chip=0)
  TpuDevice(id=17, process_index=4, coords=(1,0,2), core_on_chip=0)
  TpuDevice(id=19, process_index=4, coords=(1,1,2), core_on_chip=0)]
 [TpuDevice(id=24, process_index=6, coords=(0,0,3), core_on_chip=0)
  TpuDevice(id=26, process_index=6, coords=(0,1,3), core_on_chip=0)
  TpuDevice(id=25, process_index=6, coords=(1,0,3), core_on_chip=0)
  TpuDevice(id=27, process_index=6, coords=(1,1,3), core_on_chip=0)]
 [TpuDevice(id=4, process_index=1, coords=(0,2,0), core_on_chip=0)
  TpuDevice(id=6, process_index=1, coords=(0,3,0), core_on_chip=0)
  TpuDevice(id=5, process_index=1, coords=(1,2,0), core_on_chip=0)
  TpuDevice(id=7, process_index=1, coords=(1,3,0), core_on_chip=0)]
 [TpuDevice(id=12, process_index=3, coords=(0,2,1), core_on_chip=0)
  TpuDevice(id=14, process_index=3, coords=(0,3,1), core_on_chip=0)
  TpuDevice(id=13, process_index=3, coords=(1,2,1), core_on_chip=0)
  TpuDevice(id=15, process_index=3, coords=(1,3,1), core_on_chip=0)]
 [TpuDevice(id=20, process_index=5, coords=(0,2,2), core_on_chip=0)
  TpuDevice(id=22, process_index=5, coords=(0,3,2), core_on_chip=0)
  TpuDevice(id=21, process_index=5, coords=(1,2,2), core_on_chip=0)
  TpuDevice(id=23, process_index=5, coords=(1,3,2), core_on_chip=0)]
 [TpuDevice(id=28, process_index=7, coords=(0,2,3), core_on_chip=0)
  TpuDevice(id=30, process_index=7, coords=(0,3,3), core_on_chip=0)
  TpuDevice(id=29, process_index=7, coords=(1,2,3), core_on_chip=0)
  TpuDevice(id=31, process_index=7, coords=(1,3,3), core_on_chip=0)]]
I0512 23:44:46.696429 139876724979712 partitioning.py:257] global_mesh devices shape: (8, 4)
I0512 23:44:46.696497 139876724979712 partitioning.py:111] Overridden MoE global_mesh axes_names: ('data', 'expert', 'model')
I0512 23:44:46.696549 139876724979712 partitioning.py:114] Overridden MoE global_mesh devices: [[[TpuDevice(id=0, process_index=0, coords=(0,0,0), core_on_chip=0)
   TpuDevice(id=2, process_index=0, coords=(0,1,0), core_on_chip=0)
   TpuDevice(id=1, process_index=0, coords=(1,0,0), core_on_chip=0)
   TpuDevice(id=3, process_index=0, coords=(1,1,0), core_on_chip=0)]
  [TpuDevice(id=8, process_index=2, coords=(0,0,1), core_on_chip=0)
   TpuDevice(id=10, process_index=2, coords=(0,1,1), core_on_chip=0)
   TpuDevice(id=9, process_index=2, coords=(1,0,1), core_on_chip=0)
   TpuDevice(id=11, process_index=2, coords=(1,1,1), core_on_chip=0)]
  [TpuDevice(id=16, process_index=4, coords=(0,0,2), core_on_chip=0)
   TpuDevice(id=18, process_index=4, coords=(0,1,2), core_on_chip=0)
   TpuDevice(id=17, process_index=4, coords=(1,0,2), core_on_chip=0)
   TpuDevice(id=19, process_index=4, coords=(1,1,2), core_on_chip=0)]
  [TpuDevice(id=24, process_index=6, coords=(0,0,3), core_on_chip=0)
   TpuDevice(id=26, process_index=6, coords=(0,1,3), core_on_chip=0)
   TpuDevice(id=25, process_index=6, coords=(1,0,3), core_on_chip=0)
   TpuDevice(id=27, process_index=6, coords=(1,1,3), core_on_chip=0)]
  [TpuDevice(id=4, process_index=1, coords=(0,2,0), core_on_chip=0)
   TpuDevice(id=6, process_index=1, coords=(0,3,0), core_on_chip=0)
   TpuDevice(id=5, process_index=1, coords=(1,2,0), core_on_chip=0)
   TpuDevice(id=7, process_index=1, coords=(1,3,0), core_on_chip=0)]
  [TpuDevice(id=12, process_index=3, coords=(0,2,1), core_on_chip=0)
   TpuDevice(id=14, process_index=3, coords=(0,3,1), core_on_chip=0)
   TpuDevice(id=13, process_index=3, coords=(1,2,1), core_on_chip=0)
   TpuDevice(id=15, process_index=3, coords=(1,3,1), core_on_chip=0)]
  [TpuDevice(id=20, process_index=5, coords=(0,2,2), core_on_chip=0)
   TpuDevice(id=22, process_index=5, coords=(0,3,2), core_on_chip=0)
   TpuDevice(id=21, process_index=5, coords=(1,2,2), core_on_chip=0)
   TpuDevice(id=23, process_index=5, coords=(1,3,2), core_on_chip=0)]
  [TpuDevice(id=28, process_index=7, coords=(0,2,3), core_on_chip=0)
   TpuDevice(id=30, process_index=7, coords=(0,3,3), core_on_chip=0)
   TpuDevice(id=29, process_index=7, coords=(1,2,3), core_on_chip=0)
   TpuDevice(id=31, process_index=7, coords=(1,3,3), core_on_chip=0)]]]
I0512 23:44:46.696736 139876724979712 partitioning.py:115] Overridden MoE global_mesh shape: (1, 8, 4)
I0512 23:44:46.702802 139856202532864 partitioning.py:94] For MoE, first construct vanilla T5X (data, model) mesh.
I0512 23:44:46.703219 139856202532864 partitioning.py:151] last device coords : (1, 3, 3, 0)
last local device coords: (1, 1, 0, 0)
W0512 23:44:46.703433 139856202532864 partitioning.py:197] Tiling device assignment mesh by hosts, which may lead to reduced XLA collective performance. To avoid this, modify the model parallel submesh or run with more tasks per host.
I0512 23:44:46.703624 139856202532864 partitioning.py:255] global_mesh axis_names: ('data', 'model')
I0512 23:44:46.703681 139856202532864 partitioning.py:256] global_mesh devices: [[TpuDevice(id=0, process_index=0, coords=(0,0,0), core_on_chip=0)
  TpuDevice(id=2, process_index=0, coords=(0,1,0), core_on_chip=0)
  TpuDevice(id=1, process_index=0, coords=(1,0,0), core_on_chip=0)
  TpuDevice(id=3, process_index=0, coords=(1,1,0), core_on_chip=0)]
 [TpuDevice(id=8, process_index=2, coords=(0,0,1), core_on_chip=0)
  TpuDevice(id=10, process_index=2, coords=(0,1,1), core_on_chip=0)
  TpuDevice(id=9, process_index=2, coords=(1,0,1), core_on_chip=0)
  TpuDevice(id=11, process_index=2, coords=(1,1,1), core_on_chip=0)]
 [TpuDevice(id=16, process_index=4, coords=(0,0,2), core_on_chip=0)
  TpuDevice(id=18, process_index=4, coords=(0,1,2), core_on_chip=0)
  TpuDevice(id=17, process_index=4, coords=(1,0,2), core_on_chip=0)
  TpuDevice(id=19, process_index=4, coords=(1,1,2), core_on_chip=0)]
 [TpuDevice(id=24, process_index=6, coords=(0,0,3), core_on_chip=0)
  TpuDevice(id=26, process_index=6, coords=(0,1,3), core_on_chip=0)
  TpuDevice(id=25, process_index=6, coords=(1,0,3), core_on_chip=0)
  TpuDevice(id=27, process_index=6, coords=(1,1,3), core_on_chip=0)]
 [TpuDevice(id=4, process_index=1, coords=(0,2,0), core_on_chip=0)
  TpuDevice(id=6, process_index=1, coords=(0,3,0), core_on_chip=0)
  TpuDevice(id=5, process_index=1, coords=(1,2,0), core_on_chip=0)
  TpuDevice(id=7, process_index=1, coords=(1,3,0), core_on_chip=0)]
 [TpuDevice(id=12, process_index=3, coords=(0,2,1), core_on_chip=0)
  TpuDevice(id=14, process_index=3, coords=(0,3,1), core_on_chip=0)
  TpuDevice(id=13, process_index=3, coords=(1,2,1), core_on_chip=0)
  TpuDevice(id=15, process_index=3, coords=(1,3,1), core_on_chip=0)]
 [TpuDevice(id=20, process_index=5, coords=(0,2,2), core_on_chip=0)
  TpuDevice(id=22, process_index=5, coords=(0,3,2), core_on_chip=0)
  TpuDevice(id=21, process_index=5, coords=(1,2,2), core_on_chip=0)
  TpuDevice(id=23, process_index=5, coords=(1,3,2), core_on_chip=0)]
 [TpuDevice(id=28, process_index=7, coords=(0,2,3), core_on_chip=0)
  TpuDevice(id=30, process_index=7, coords=(0,3,3), core_on_chip=0)
  TpuDevice(id=29, process_index=7, coords=(1,2,3), core_on_chip=0)
  TpuDevice(id=31, process_index=7, coords=(1,3,3), core_on_chip=0)]]
I0512 23:44:46.703937 139856202532864 partitioning.py:257] global_mesh devices shape: (8, 4)
I0512 23:44:46.704002 139856202532864 partitioning.py:111] Overridden MoE global_mesh axes_names: ('data', 'expert', 'model')
I0512 23:44:46.704044 139856202532864 partitioning.py:114] Overridden MoE global_mesh devices: [[[TpuDevice(id=0, process_index=0, coords=(0,0,0), core_on_chip=0)
   TpuDevice(id=2, process_index=0, coords=(0,1,0), core_on_chip=0)
   TpuDevice(id=1, process_index=0, coords=(1,0,0), core_on_chip=0)
   TpuDevice(id=3, process_index=0, coords=(1,1,0), core_on_chip=0)]
  [TpuDevice(id=8, process_index=2, coords=(0,0,1), core_on_chip=0)
   TpuDevice(id=10, process_index=2, coords=(0,1,1), core_on_chip=0)
   TpuDevice(id=9, process_index=2, coords=(1,0,1), core_on_chip=0)
   TpuDevice(id=11, process_index=2, coords=(1,1,1), core_on_chip=0)]
  [TpuDevice(id=16, process_index=4, coords=(0,0,2), core_on_chip=0)
   TpuDevice(id=18, process_index=4, coords=(0,1,2), core_on_chip=0)
   TpuDevice(id=17, process_index=4, coords=(1,0,2), core_on_chip=0)
   TpuDevice(id=19, process_index=4, coords=(1,1,2), core_on_chip=0)]
  [TpuDevice(id=24, process_index=6, coords=(0,0,3), core_on_chip=0)
   TpuDevice(id=26, process_index=6, coords=(0,1,3), core_on_chip=0)
   TpuDevice(id=25, process_index=6, coords=(1,0,3), core_on_chip=0)
   TpuDevice(id=27, process_index=6, coords=(1,1,3), core_on_chip=0)]
  [TpuDevice(id=4, process_index=1, coords=(0,2,0), core_on_chip=0)
   TpuDevice(id=6, process_index=1, coords=(0,3,0), core_on_chip=0)
   TpuDevice(id=5, process_index=1, coords=(1,2,0), core_on_chip=0)
   TpuDevice(id=7, process_index=1, coords=(1,3,0), core_on_chip=0)]
  [TpuDevice(id=12, process_index=3, coords=(0,2,1), core_on_chip=0)
   TpuDevice(id=14, process_index=3, coords=(0,3,1), core_on_chip=0)
   TpuDevice(id=13, process_index=3, coords=(1,2,1), core_on_chip=0)
   TpuDevice(id=15, process_index=3, coords=(1,3,1), core_on_chip=0)]
  [TpuDevice(id=20, process_index=5, coords=(0,2,2), core_on_chip=0)
   TpuDevice(id=22, process_index=5, coords=(0,3,2), core_on_chip=0)
   TpuDevice(id=21, process_index=5, coords=(1,2,2), core_on_chip=0)
   TpuDevice(id=23, process_index=5, coords=(1,3,2), core_on_chip=0)]
  [TpuDevice(id=28, process_index=7, coords=(0,2,3), core_on_chip=0)
   TpuDevice(id=30, process_index=7, coords=(0,3,3), core_on_chip=0)
   TpuDevice(id=29, process_index=7, coords=(1,2,3), core_on_chip=0)
   TpuDevice(id=31, process_index=7, coords=(1,3,3), core_on_chip=0)]]]
I0512 23:44:46.704232 139856202532864 partitioning.py:115] Overridden MoE global_mesh shape: (1, 8, 4)
I0512 23:44:46.707127 140193652815872 partitioning.py:94] For MoE, first construct vanilla T5X (data, model) mesh.
I0512 23:44:46.707576 140193652815872 partitioning.py:151] last device coords : (1, 3, 3, 0)
last local device coords: (1, 1, 0, 0)
W0512 23:44:46.707791 140193652815872 partitioning.py:197] Tiling device assignment mesh by hosts, which may lead to reduced XLA collective performance. To avoid this, modify the model parallel submesh or run with more tasks per host.
I0512 23:44:46.707978 140193652815872 partitioning.py:255] global_mesh axis_names: ('data', 'model')
I0512 23:44:46.708033 140193652815872 partitioning.py:256] global_mesh devices: [[TpuDevice(id=0, process_index=0, coords=(0,0,0), core_on_chip=0)
  TpuDevice(id=2, process_index=0, coords=(0,1,0), core_on_chip=0)
  TpuDevice(id=1, process_index=0, coords=(1,0,0), core_on_chip=0)
  TpuDevice(id=3, process_index=0, coords=(1,1,0), core_on_chip=0)]
 [TpuDevice(id=8, process_index=2, coords=(0,0,1), core_on_chip=0)
  TpuDevice(id=10, process_index=2, coords=(0,1,1), core_on_chip=0)
  TpuDevice(id=9, process_index=2, coords=(1,0,1), core_on_chip=0)
  TpuDevice(id=11, process_index=2, coords=(1,1,1), core_on_chip=0)]
 [TpuDevice(id=16, process_index=4, coords=(0,0,2), core_on_chip=0)
  TpuDevice(id=18, process_index=4, coords=(0,1,2), core_on_chip=0)
  TpuDevice(id=17, process_index=4, coords=(1,0,2), core_on_chip=0)
  TpuDevice(id=19, process_index=4, coords=(1,1,2), core_on_chip=0)]
 [TpuDevice(id=24, process_index=6, coords=(0,0,3), core_on_chip=0)
  TpuDevice(id=26, process_index=6, coords=(0,1,3), core_on_chip=0)
  TpuDevice(id=25, process_index=6, coords=(1,0,3), core_on_chip=0)
  TpuDevice(id=27, process_index=6, coords=(1,1,3), core_on_chip=0)]
 [TpuDevice(id=4, process_index=1, coords=(0,2,0), core_on_chip=0)
  TpuDevice(id=6, process_index=1, coords=(0,3,0), core_on_chip=0)
  TpuDevice(id=5, process_index=1, coords=(1,2,0), core_on_chip=0)
  TpuDevice(id=7, process_index=1, coords=(1,3,0), core_on_chip=0)]
 [TpuDevice(id=12, process_index=3, coords=(0,2,1), core_on_chip=0)
  TpuDevice(id=14, process_index=3, coords=(0,3,1), core_on_chip=0)
  TpuDevice(id=13, process_index=3, coords=(1,2,1), core_on_chip=0)
  TpuDevice(id=15, process_index=3, coords=(1,3,1), core_on_chip=0)]
 [TpuDevice(id=20, process_index=5, coords=(0,2,2), core_on_chip=0)
  TpuDevice(id=22, process_index=5, coords=(0,3,2), core_on_chip=0)
  TpuDevice(id=21, process_index=5, coords=(1,2,2), core_on_chip=0)
  TpuDevice(id=23, process_index=5, coords=(1,3,2), core_on_chip=0)]
 [TpuDevice(id=28, process_index=7, coords=(0,2,3), core_on_chip=0)
  TpuDevice(id=30, process_index=7, coords=(0,3,3), core_on_chip=0)
  TpuDevice(id=29, process_index=7, coords=(1,2,3), core_on_chip=0)
  TpuDevice(id=31, process_index=7, coords=(1,3,3), core_on_chip=0)]]
I0512 23:44:46.708330 140193652815872 partitioning.py:257] global_mesh devices shape: (8, 4)
I0512 23:44:46.708399 140193652815872 partitioning.py:111] Overridden MoE global_mesh axes_names: ('data', 'expert', 'model')
I0512 23:44:46.708442 140193652815872 partitioning.py:114] Overridden MoE global_mesh devices: [[[TpuDevice(id=0, process_index=0, coords=(0,0,0), core_on_chip=0)
   TpuDevice(id=2, process_index=0, coords=(0,1,0), core_on_chip=0)
   TpuDevice(id=1, process_index=0, coords=(1,0,0), core_on_chip=0)
   TpuDevice(id=3, process_index=0, coords=(1,1,0), core_on_chip=0)]
  [TpuDevice(id=8, process_index=2, coords=(0,0,1), core_on_chip=0)
   TpuDevice(id=10, process_index=2, coords=(0,1,1), core_on_chip=0)
   TpuDevice(id=9, process_index=2, coords=(1,0,1), core_on_chip=0)
   TpuDevice(id=11, process_index=2, coords=(1,1,1), core_on_chip=0)]
  [TpuDevice(id=16, process_index=4, coords=(0,0,2), core_on_chip=0)
   TpuDevice(id=18, process_index=4, coords=(0,1,2), core_on_chip=0)
   TpuDevice(id=17, process_index=4, coords=(1,0,2), core_on_chip=0)
   TpuDevice(id=19, process_index=4, coords=(1,1,2), core_on_chip=0)]
  [TpuDevice(id=24, process_index=6, coords=(0,0,3), core_on_chip=0)
   TpuDevice(id=26, process_index=6, coords=(0,1,3), core_on_chip=0)
   TpuDevice(id=25, process_index=6, coords=(1,0,3), core_on_chip=0)
   TpuDevice(id=27, process_index=6, coords=(1,1,3), core_on_chip=0)]
  [TpuDevice(id=4, process_index=1, coords=(0,2,0), core_on_chip=0)
   TpuDevice(id=6, process_index=1, coords=(0,3,0), core_on_chip=0)
   TpuDevice(id=5, process_index=1, coords=(1,2,0), core_on_chip=0)
   TpuDevice(id=7, process_index=1, coords=(1,3,0), core_on_chip=0)]
  [TpuDevice(id=12, process_index=3, coords=(0,2,1), core_on_chip=0)
   TpuDevice(id=14, process_index=3, coords=(0,3,1), core_on_chip=0)
   TpuDevice(id=13, process_index=3, coords=(1,2,1), core_on_chip=0)
   TpuDevice(id=15, process_index=3, coords=(1,3,1), core_on_chip=0)]
  [TpuDevice(id=20, process_index=5, coords=(0,2,2), core_on_chip=0)
   TpuDevice(id=22, process_index=5, coords=(0,3,2), core_on_chip=0)
   TpuDevice(id=21, process_index=5, coords=(1,2,2), core_on_chip=0)
   TpuDevice(id=23, process_index=5, coords=(1,3,2), core_on_chip=0)]
  [TpuDevice(id=28, process_index=7, coords=(0,2,3), core_on_chip=0)
   TpuDevice(id=30, process_index=7, coords=(0,3,3), core_on_chip=0)
   TpuDevice(id=29, process_index=7, coords=(1,2,3), core_on_chip=0)
   TpuDevice(id=31, process_index=7, coords=(1,3,3), core_on_chip=0)]]]
I0512 23:44:46.708627 140193652815872 partitioning.py:115] Overridden MoE global_mesh shape: (1, 8, 4)
I0512 23:44:46.697091 140643590531072 partitioning.py:94] For MoE, first construct vanilla T5X (data, model) mesh.
I0512 23:44:46.697477 140643590531072 partitioning.py:151] last device coords : (1, 3, 3, 0)
last local device coords: (1, 1, 0, 0)
W0512 23:44:46.697669 140643590531072 partitioning.py:197] Tiling device assignment mesh by hosts, which may lead to reduced XLA collective performance. To avoid this, modify the model parallel submesh or run with more tasks per host.
I0512 23:44:46.697836 140643590531072 partitioning.py:255] global_mesh axis_names: ('data', 'model')
I0512 23:44:46.697889 140643590531072 partitioning.py:256] global_mesh devices: [[TpuDevice(id=0, process_index=0, coords=(0,0,0), core_on_chip=0)
  TpuDevice(id=2, process_index=0, coords=(0,1,0), core_on_chip=0)
  TpuDevice(id=1, process_index=0, coords=(1,0,0), core_on_chip=0)
  TpuDevice(id=3, process_index=0, coords=(1,1,0), core_on_chip=0)]
 [TpuDevice(id=8, process_index=2, coords=(0,0,1), core_on_chip=0)
  TpuDevice(id=10, process_index=2, coords=(0,1,1), core_on_chip=0)
  TpuDevice(id=9, process_index=2, coords=(1,0,1), core_on_chip=0)
  TpuDevice(id=11, process_index=2, coords=(1,1,1), core_on_chip=0)]
 [TpuDevice(id=16, process_index=4, coords=(0,0,2), core_on_chip=0)
  TpuDevice(id=18, process_index=4, coords=(0,1,2), core_on_chip=0)
  TpuDevice(id=17, process_index=4, coords=(1,0,2), core_on_chip=0)
  TpuDevice(id=19, process_index=4, coords=(1,1,2), core_on_chip=0)]
 [TpuDevice(id=24, process_index=6, coords=(0,0,3), core_on_chip=0)
  TpuDevice(id=26, process_index=6, coords=(0,1,3), core_on_chip=0)
  TpuDevice(id=25, process_index=6, coords=(1,0,3), core_on_chip=0)
  TpuDevice(id=27, process_index=6, coords=(1,1,3), core_on_chip=0)]
 [TpuDevice(id=4, process_index=1, coords=(0,2,0), core_on_chip=0)
  TpuDevice(id=6, process_index=1, coords=(0,3,0), core_on_chip=0)
  TpuDevice(id=5, process_index=1, coords=(1,2,0), core_on_chip=0)
  TpuDevice(id=7, process_index=1, coords=(1,3,0), core_on_chip=0)]
 [TpuDevice(id=12, process_index=3, coords=(0,2,1), core_on_chip=0)
  TpuDevice(id=14, process_index=3, coords=(0,3,1), core_on_chip=0)
  TpuDevice(id=13, process_index=3, coords=(1,2,1), core_on_chip=0)
  TpuDevice(id=15, process_index=3, coords=(1,3,1), core_on_chip=0)]
 [TpuDevice(id=20, process_index=5, coords=(0,2,2), core_on_chip=0)
  TpuDevice(id=22, process_index=5, coords=(0,3,2), core_on_chip=0)
  TpuDevice(id=21, process_index=5, coords=(1,2,2), core_on_chip=0)
  TpuDevice(id=23, process_index=5, coords=(1,3,2), core_on_chip=0)]
 [TpuDevice(id=28, process_index=7, coords=(0,2,3), core_on_chip=0)
  TpuDevice(id=30, process_index=7, coords=(0,3,3), core_on_chip=0)
  TpuDevice(id=29, process_index=7, coords=(1,2,3), core_on_chip=0)
  TpuDevice(id=31, process_index=7, coords=(1,3,3), core_on_chip=0)]]
I0512 23:44:46.698124 140643590531072 partitioning.py:257] global_mesh devices shape: (8, 4)
I0512 23:44:46.698185 140643590531072 partitioning.py:111] Overridden MoE global_mesh axes_names: ('data', 'expert', 'model')
I0512 23:44:46.698227 140643590531072 partitioning.py:114] Overridden MoE global_mesh devices: [[[TpuDevice(id=0, process_index=0, coords=(0,0,0), core_on_chip=0)
   TpuDevice(id=2, process_index=0, coords=(0,1,0), core_on_chip=0)
   TpuDevice(id=1, process_index=0, coords=(1,0,0), core_on_chip=0)
   TpuDevice(id=3, process_index=0, coords=(1,1,0), core_on_chip=0)]
  [TpuDevice(id=8, process_index=2, coords=(0,0,1), core_on_chip=0)
   TpuDevice(id=10, process_index=2, coords=(0,1,1), core_on_chip=0)
   TpuDevice(id=9, process_index=2, coords=(1,0,1), core_on_chip=0)
   TpuDevice(id=11, process_index=2, coords=(1,1,1), core_on_chip=0)]
  [TpuDevice(id=16, process_index=4, coords=(0,0,2), core_on_chip=0)
   TpuDevice(id=18, process_index=4, coords=(0,1,2), core_on_chip=0)
   TpuDevice(id=17, process_index=4, coords=(1,0,2), core_on_chip=0)
   TpuDevice(id=19, process_index=4, coords=(1,1,2), core_on_chip=0)]
  [TpuDevice(id=24, process_index=6, coords=(0,0,3), core_on_chip=0)
   TpuDevice(id=26, process_index=6, coords=(0,1,3), core_on_chip=0)
   TpuDevice(id=25, process_index=6, coords=(1,0,3), core_on_chip=0)
   TpuDevice(id=27, process_index=6, coords=(1,1,3), core_on_chip=0)]
  [TpuDevice(id=4, process_index=1, coords=(0,2,0), core_on_chip=0)
   TpuDevice(id=6, process_index=1, coords=(0,3,0), core_on_chip=0)
   TpuDevice(id=5, process_index=1, coords=(1,2,0), core_on_chip=0)
   TpuDevice(id=7, process_index=1, coords=(1,3,0), core_on_chip=0)]
  [TpuDevice(id=12, process_index=3, coords=(0,2,1), core_on_chip=0)
   TpuDevice(id=14, process_index=3, coords=(0,3,1), core_on_chip=0)
   TpuDevice(id=13, process_index=3, coords=(1,2,1), core_on_chip=0)
   TpuDevice(id=15, process_index=3, coords=(1,3,1), core_on_chip=0)]
  [TpuDevice(id=20, process_index=5, coords=(0,2,2), core_on_chip=0)
   TpuDevice(id=22, process_index=5, coords=(0,3,2), core_on_chip=0)
   TpuDevice(id=21, process_index=5, coords=(1,2,2), core_on_chip=0)
   TpuDevice(id=23, process_index=5, coords=(1,3,2), core_on_chip=0)]
  [TpuDevice(id=28, process_index=7, coords=(0,2,3), core_on_chip=0)
   TpuDevice(id=30, process_index=7, coords=(0,3,3), core_on_chip=0)
   TpuDevice(id=29, process_index=7, coords=(1,2,3), core_on_chip=0)
   TpuDevice(id=31, process_index=7, coords=(1,3,3), core_on_chip=0)]]]
I0512 23:44:46.698405 140643590531072 partitioning.py:115] Overridden MoE global_mesh shape: (1, 8, 4)
I0512 23:44:46.707058 140180528838656 partitioning.py:94] For MoE, first construct vanilla T5X (data, model) mesh.
I0512 23:44:46.707552 140180528838656 partitioning.py:151] last device coords : (1, 3, 3, 0)
last local device coords: (1, 1, 0, 0)
W0512 23:44:46.707777 140180528838656 partitioning.py:197] Tiling device assignment mesh by hosts, which may lead to reduced XLA collective performance. To avoid this, modify the model parallel submesh or run with more tasks per host.
I0512 23:44:46.707996 140180528838656 partitioning.py:255] global_mesh axis_names: ('data', 'model')
I0512 23:44:46.708055 140180528838656 partitioning.py:256] global_mesh devices: [[TpuDevice(id=0, process_index=0, coords=(0,0,0), core_on_chip=0)
  TpuDevice(id=2, process_index=0, coords=(0,1,0), core_on_chip=0)
  TpuDevice(id=1, process_index=0, coords=(1,0,0), core_on_chip=0)
  TpuDevice(id=3, process_index=0, coords=(1,1,0), core_on_chip=0)]
 [TpuDevice(id=8, process_index=2, coords=(0,0,1), core_on_chip=0)
  TpuDevice(id=10, process_index=2, coords=(0,1,1), core_on_chip=0)
  TpuDevice(id=9, process_index=2, coords=(1,0,1), core_on_chip=0)
  TpuDevice(id=11, process_index=2, coords=(1,1,1), core_on_chip=0)]
 [TpuDevice(id=16, process_index=4, coords=(0,0,2), core_on_chip=0)
  TpuDevice(id=18, process_index=4, coords=(0,1,2), core_on_chip=0)
  TpuDevice(id=17, process_index=4, coords=(1,0,2), core_on_chip=0)
  TpuDevice(id=19, process_index=4, coords=(1,1,2), core_on_chip=0)]
 [TpuDevice(id=24, process_index=6, coords=(0,0,3), core_on_chip=0)
  TpuDevice(id=26, process_index=6, coords=(0,1,3), core_on_chip=0)
  TpuDevice(id=25, process_index=6, coords=(1,0,3), core_on_chip=0)
  TpuDevice(id=27, process_index=6, coords=(1,1,3), core_on_chip=0)]
 [TpuDevice(id=4, process_index=1, coords=(0,2,0), core_on_chip=0)
  TpuDevice(id=6, process_index=1, coords=(0,3,0), core_on_chip=0)
  TpuDevice(id=5, process_index=1, coords=(1,2,0), core_on_chip=0)
  TpuDevice(id=7, process_index=1, coords=(1,3,0), core_on_chip=0)]
 [TpuDevice(id=12, process_index=3, coords=(0,2,1), core_on_chip=0)
  TpuDevice(id=14, process_index=3, coords=(0,3,1), core_on_chip=0)
  TpuDevice(id=13, process_index=3, coords=(1,2,1), core_on_chip=0)
  TpuDevice(id=15, process_index=3, coords=(1,3,1), core_on_chip=0)]
 [TpuDevice(id=20, process_index=5, coords=(0,2,2), core_on_chip=0)
  TpuDevice(id=22, process_index=5, coords=(0,3,2), core_on_chip=0)
  TpuDevice(id=21, process_index=5, coords=(1,2,2), core_on_chip=0)
  TpuDevice(id=23, process_index=5, coords=(1,3,2), core_on_chip=0)]
 [TpuDevice(id=28, process_index=7, coords=(0,2,3), core_on_chip=0)
  TpuDevice(id=30, process_index=7, coords=(0,3,3), core_on_chip=0)
  TpuDevice(id=29, process_index=7, coords=(1,2,3), core_on_chip=0)
  TpuDevice(id=31, process_index=7, coords=(1,3,3), core_on_chip=0)]]
I0512 23:44:46.708348 140180528838656 partitioning.py:257] global_mesh devices shape: (8, 4)
I0512 23:44:46.708413 140180528838656 partitioning.py:111] Overridden MoE global_mesh axes_names: ('data', 'expert', 'model')
I0512 23:44:46.708457 140180528838656 partitioning.py:114] Overridden MoE global_mesh devices: [[[TpuDevice(id=0, process_index=0, coords=(0,0,0), core_on_chip=0)
   TpuDevice(id=2, process_index=0, coords=(0,1,0), core_on_chip=0)
   TpuDevice(id=1, process_index=0, coords=(1,0,0), core_on_chip=0)
   TpuDevice(id=3, process_index=0, coords=(1,1,0), core_on_chip=0)]
  [TpuDevice(id=8, process_index=2, coords=(0,0,1), core_on_chip=0)
   TpuDevice(id=10, process_index=2, coords=(0,1,1), core_on_chip=0)
   TpuDevice(id=9, process_index=2, coords=(1,0,1), core_on_chip=0)
   TpuDevice(id=11, process_index=2, coords=(1,1,1), core_on_chip=0)]
  [TpuDevice(id=16, process_index=4, coords=(0,0,2), core_on_chip=0)
   TpuDevice(id=18, process_index=4, coords=(0,1,2), core_on_chip=0)
   TpuDevice(id=17, process_index=4, coords=(1,0,2), core_on_chip=0)
   TpuDevice(id=19, process_index=4, coords=(1,1,2), core_on_chip=0)]
  [TpuDevice(id=24, process_index=6, coords=(0,0,3), core_on_chip=0)
   TpuDevice(id=26, process_index=6, coords=(0,1,3), core_on_chip=0)
   TpuDevice(id=25, process_index=6, coords=(1,0,3), core_on_chip=0)
   TpuDevice(id=27, process_index=6, coords=(1,1,3), core_on_chip=0)]
  [TpuDevice(id=4, process_index=1, coords=(0,2,0), core_on_chip=0)
   TpuDevice(id=6, process_index=1, coords=(0,3,0), core_on_chip=0)
   TpuDevice(id=5, process_index=1, coords=(1,2,0), core_on_chip=0)
   TpuDevice(id=7, process_index=1, coords=(1,3,0), core_on_chip=0)]
  [TpuDevice(id=12, process_index=3, coords=(0,2,1), core_on_chip=0)
   TpuDevice(id=14, process_index=3, coords=(0,3,1), core_on_chip=0)
   TpuDevice(id=13, process_index=3, coords=(1,2,1), core_on_chip=0)
   TpuDevice(id=15, process_index=3, coords=(1,3,1), core_on_chip=0)]
  [TpuDevice(id=20, process_index=5, coords=(0,2,2), core_on_chip=0)
   TpuDevice(id=22, process_index=5, coords=(0,3,2), core_on_chip=0)
   TpuDevice(id=21, process_index=5, coords=(1,2,2), core_on_chip=0)
   TpuDevice(id=23, process_index=5, coords=(1,3,2), core_on_chip=0)]
  [TpuDevice(id=28, process_index=7, coords=(0,2,3), core_on_chip=0)
   TpuDevice(id=30, process_index=7, coords=(0,3,3), core_on_chip=0)
   TpuDevice(id=29, process_index=7, coords=(1,2,3), core_on_chip=0)
   TpuDevice(id=31, process_index=7, coords=(1,3,3), core_on_chip=0)]]]
I0512 23:44:46.708645 140180528838656 partitioning.py:115] Overridden MoE global_mesh shape: (1, 8, 4)
I0512 23:44:46.711767 140260698863616 partitioning.py:94] For MoE, first construct vanilla T5X (data, model) mesh.
I0512 23:44:46.712195 140260698863616 partitioning.py:151] last device coords : (1, 3, 3, 0)
last local device coords: (1, 1, 0, 0)
W0512 23:44:46.712402 140260698863616 partitioning.py:197] Tiling device assignment mesh by hosts, which may lead to reduced XLA collective performance. To avoid this, modify the model parallel submesh or run with more tasks per host.
I0512 23:44:46.712596 140260698863616 partitioning.py:255] global_mesh axis_names: ('data', 'model')
I0512 23:44:46.712651 140260698863616 partitioning.py:256] global_mesh devices: [[TpuDevice(id=0, process_index=0, coords=(0,0,0), core_on_chip=0)
  TpuDevice(id=2, process_index=0, coords=(0,1,0), core_on_chip=0)
  TpuDevice(id=1, process_index=0, coords=(1,0,0), core_on_chip=0)
  TpuDevice(id=3, process_index=0, coords=(1,1,0), core_on_chip=0)]
 [TpuDevice(id=8, process_index=2, coords=(0,0,1), core_on_chip=0)
  TpuDevice(id=10, process_index=2, coords=(0,1,1), core_on_chip=0)
  TpuDevice(id=9, process_index=2, coords=(1,0,1), core_on_chip=0)
  TpuDevice(id=11, process_index=2, coords=(1,1,1), core_on_chip=0)]
 [TpuDevice(id=16, process_index=4, coords=(0,0,2), core_on_chip=0)
  TpuDevice(id=18, process_index=4, coords=(0,1,2), core_on_chip=0)
  TpuDevice(id=17, process_index=4, coords=(1,0,2), core_on_chip=0)
  TpuDevice(id=19, process_index=4, coords=(1,1,2), core_on_chip=0)]
 [TpuDevice(id=24, process_index=6, coords=(0,0,3), core_on_chip=0)
  TpuDevice(id=26, process_index=6, coords=(0,1,3), core_on_chip=0)
  TpuDevice(id=25, process_index=6, coords=(1,0,3), core_on_chip=0)
  TpuDevice(id=27, process_index=6, coords=(1,1,3), core_on_chip=0)]
 [TpuDevice(id=4, process_index=1, coords=(0,2,0), core_on_chip=0)
  TpuDevice(id=6, process_index=1, coords=(0,3,0), core_on_chip=0)
  TpuDevice(id=5, process_index=1, coords=(1,2,0), core_on_chip=0)
  TpuDevice(id=7, process_index=1, coords=(1,3,0), core_on_chip=0)]
 [TpuDevice(id=12, process_index=3, coords=(0,2,1), core_on_chip=0)
  TpuDevice(id=14, process_index=3, coords=(0,3,1), core_on_chip=0)
  TpuDevice(id=13, process_index=3, coords=(1,2,1), core_on_chip=0)
  TpuDevice(id=15, process_index=3, coords=(1,3,1), core_on_chip=0)]
 [TpuDevice(id=20, process_index=5, coords=(0,2,2), core_on_chip=0)
  TpuDevice(id=22, process_index=5, coords=(0,3,2), core_on_chip=0)
  TpuDevice(id=21, process_index=5, coords=(1,2,2), core_on_chip=0)
  TpuDevice(id=23, process_index=5, coords=(1,3,2), core_on_chip=0)]
 [TpuDevice(id=28, process_index=7, coords=(0,2,3), core_on_chip=0)
  TpuDevice(id=30, process_index=7, coords=(0,3,3), core_on_chip=0)
  TpuDevice(id=29, process_index=7, coords=(1,2,3), core_on_chip=0)
  TpuDevice(id=31, process_index=7, coords=(1,3,3), core_on_chip=0)]]
I0512 23:44:46.712923 140260698863616 partitioning.py:257] global_mesh devices shape: (8, 4)
I0512 23:44:46.712987 140260698863616 partitioning.py:111] Overridden MoE global_mesh axes_names: ('data', 'expert', 'model')
I0512 23:44:46.713029 140260698863616 partitioning.py:114] Overridden MoE global_mesh devices: [[[TpuDevice(id=0, process_index=0, coords=(0,0,0), core_on_chip=0)
   TpuDevice(id=2, process_index=0, coords=(0,1,0), core_on_chip=0)
   TpuDevice(id=1, process_index=0, coords=(1,0,0), core_on_chip=0)
   TpuDevice(id=3, process_index=0, coords=(1,1,0), core_on_chip=0)]
  [TpuDevice(id=8, process_index=2, coords=(0,0,1), core_on_chip=0)
   TpuDevice(id=10, process_index=2, coords=(0,1,1), core_on_chip=0)
   TpuDevice(id=9, process_index=2, coords=(1,0,1), core_on_chip=0)
   TpuDevice(id=11, process_index=2, coords=(1,1,1), core_on_chip=0)]
  [TpuDevice(id=16, process_index=4, coords=(0,0,2), core_on_chip=0)
   TpuDevice(id=18, process_index=4, coords=(0,1,2), core_on_chip=0)
   TpuDevice(id=17, process_index=4, coords=(1,0,2), core_on_chip=0)
   TpuDevice(id=19, process_index=4, coords=(1,1,2), core_on_chip=0)]
  [TpuDevice(id=24, process_index=6, coords=(0,0,3), core_on_chip=0)
   TpuDevice(id=26, process_index=6, coords=(0,1,3), core_on_chip=0)
   TpuDevice(id=25, process_index=6, coords=(1,0,3), core_on_chip=0)
   TpuDevice(id=27, process_index=6, coords=(1,1,3), core_on_chip=0)]
  [TpuDevice(id=4, process_index=1, coords=(0,2,0), core_on_chip=0)
   TpuDevice(id=6, process_index=1, coords=(0,3,0), core_on_chip=0)
   TpuDevice(id=5, process_index=1, coords=(1,2,0), core_on_chip=0)
   TpuDevice(id=7, process_index=1, coords=(1,3,0), core_on_chip=0)]
  [TpuDevice(id=12, process_index=3, coords=(0,2,1), core_on_chip=0)
   TpuDevice(id=14, process_index=3, coords=(0,3,1), core_on_chip=0)
   TpuDevice(id=13, process_index=3, coords=(1,2,1), core_on_chip=0)
   TpuDevice(id=15, process_index=3, coords=(1,3,1), core_on_chip=0)]
  [TpuDevice(id=20, process_index=5, coords=(0,2,2), core_on_chip=0)
   TpuDevice(id=22, process_index=5, coords=(0,3,2), core_on_chip=0)
   TpuDevice(id=21, process_index=5, coords=(1,2,2), core_on_chip=0)
   TpuDevice(id=23, process_index=5, coords=(1,3,2), core_on_chip=0)]
  [TpuDevice(id=28, process_index=7, coords=(0,2,3), core_on_chip=0)
   TpuDevice(id=30, process_index=7, coords=(0,3,3), core_on_chip=0)
   TpuDevice(id=29, process_index=7, coords=(1,2,3), core_on_chip=0)
   TpuDevice(id=31, process_index=7, coords=(1,3,3), core_on_chip=0)]]]
I0512 23:44:46.713214 140260698863616 partitioning.py:115] Overridden MoE global_mesh shape: (1, 8, 4)
I0512 23:44:47.607295 139876724979712 utils.py:1979] Initializing dataset for task 'mix_ul2_test' with a replica batch size of 48 and a seed of 1715557487
I0512 23:44:47.607330 139856202532864 utils.py:1979] Initializing dataset for task 'mix_ul2_test' with a replica batch size of 48 and a seed of 1715557487
I0512 23:44:47.607345 139666521167872 utils.py:1979] Initializing dataset for task 'mix_ul2_test' with a replica batch size of 48 and a seed of 1715557487
I0512 23:44:47.607395 140193652815872 utils.py:1979] Initializing dataset for task 'mix_ul2_test' with a replica batch size of 48 and a seed of 1715557487
I0512 23:44:47.607239 140143678494720 utils.py:1979] Initializing dataset for task 'mix_ul2_test' with a replica batch size of 48 and a seed of 1715557487
I0512 23:44:47.607255 140643590531072 utils.py:1979] Initializing dataset for task 'mix_ul2_test' with a replica batch size of 48 and a seed of 1715557487
I0512 23:44:47.607296 140180528838656 utils.py:1979] Initializing dataset for task 'mix_ul2_test' with a replica batch size of 48 and a seed of 1715557487
I0512 23:44:47.607397 140260698863616 utils.py:1979] Initializing dataset for task 'mix_ul2_test' with a replica batch size of 48 and a seed of 1715557487
I0512 23:44:47.930083 140143678494720 dataset_info.py:540] Load dataset info from gs://rosinality-tpu-bucket/redpajama_arxiv/1.0.0
I0512 23:44:47.944081 139856202532864 dataset_info.py:540] Load dataset info from gs://rosinality-tpu-bucket/redpajama_arxiv/1.0.0
I0512 23:44:47.953973 139876724979712 dataset_info.py:540] Load dataset info from gs://rosinality-tpu-bucket/redpajama_arxiv/1.0.0
I0512 23:44:47.969353 139666521167872 dataset_info.py:540] Load dataset info from gs://rosinality-tpu-bucket/redpajama_arxiv/1.0.0
I0512 23:44:47.951955 140180528838656 dataset_info.py:540] Load dataset info from gs://rosinality-tpu-bucket/redpajama_arxiv/1.0.0
I0512 23:44:47.974074 140193652815872 dataset_info.py:540] Load dataset info from gs://rosinality-tpu-bucket/redpajama_arxiv/1.0.0
I0512 23:44:47.964131 140643590531072 dataset_info.py:540] Load dataset info from gs://rosinality-tpu-bucket/redpajama_arxiv/1.0.0
I0512 23:44:47.970313 140260698863616 dataset_info.py:540] Load dataset info from gs://rosinality-tpu-bucket/redpajama_arxiv/1.0.0
I0512 23:44:48.173182 139856202532864 dataset_providers.py:1597] Sharding at the data source: 8 of 8
I0512 23:44:48.179945 140143678494720 dataset_providers.py:1597] Sharding at the data source: 1 of 8
I0512 23:44:48.190883 139876724979712 dataset_providers.py:1597] Sharding at the data source: 2 of 8
I0512 23:44:48.203351 139666521167872 dataset_providers.py:1597] Sharding at the data source: 6 of 8
I0512 23:44:48.193275 140643590531072 dataset_providers.py:1597] Sharding at the data source: 5 of 8
I0512 23:44:48.224678 140193652815872 dataset_providers.py:1597] Sharding at the data source: 3 of 8
I0512 23:44:48.207896 140260698863616 dataset_providers.py:1597] Sharding at the data source: 4 of 8
I0512 23:44:48.210566 140180528838656 dataset_providers.py:1597] Sharding at the data source: 7 of 8
I0512 23:44:48.239063 139856202532864 dataset_info.py:540] Load dataset info from gs://rosinality-tpu-bucket/redpajama_arxiv/1.0.0
I0512 23:44:48.247864 140143678494720 dataset_info.py:540] Load dataset info from gs://rosinality-tpu-bucket/redpajama_arxiv/1.0.0
I0512 23:44:48.261955 139666521167872 dataset_info.py:540] Load dataset info from gs://rosinality-tpu-bucket/redpajama_arxiv/1.0.0
I0512 23:44:48.266326 139876724979712 dataset_info.py:540] Load dataset info from gs://rosinality-tpu-bucket/redpajama_arxiv/1.0.0
I0512 23:44:48.253104 140643590531072 dataset_info.py:540] Load dataset info from gs://rosinality-tpu-bucket/redpajama_arxiv/1.0.0
I0512 23:44:48.277672 140260698863616 dataset_info.py:540] Load dataset info from gs://rosinality-tpu-bucket/redpajama_arxiv/1.0.0
I0512 23:44:48.280815 140180528838656 dataset_info.py:540] Load dataset info from gs://rosinality-tpu-bucket/redpajama_arxiv/1.0.0
I0512 23:44:48.299872 140193652815872 dataset_info.py:540] Load dataset info from gs://rosinality-tpu-bucket/redpajama_arxiv/1.0.0
I0512 23:44:48.486763 139666521167872 dataset_builder.py:421] Reusing dataset redpajama_arxiv (gs://rosinality-tpu-bucket/redpajama_arxiv/1.0.0)
I0512 23:44:48.500850 139876724979712 dataset_builder.py:421] Reusing dataset redpajama_arxiv (gs://rosinality-tpu-bucket/redpajama_arxiv/1.0.0)
I0512 23:44:48.491221 140643590531072 dataset_builder.py:421] Reusing dataset redpajama_arxiv (gs://rosinality-tpu-bucket/redpajama_arxiv/1.0.0)
I0512 23:44:48.492882 140260698863616 dataset_builder.py:421] Reusing dataset redpajama_arxiv (gs://rosinality-tpu-bucket/redpajama_arxiv/1.0.0)
I0512 23:44:48.514903 139856202532864 dataset_builder.py:421] Reusing dataset redpajama_arxiv (gs://rosinality-tpu-bucket/redpajama_arxiv/1.0.0)
I0512 23:44:48.504478 140180528838656 dataset_builder.py:421] Reusing dataset redpajama_arxiv (gs://rosinality-tpu-bucket/redpajama_arxiv/1.0.0)
I0512 23:44:48.543743 140193652815872 dataset_builder.py:421] Reusing dataset redpajama_arxiv (gs://rosinality-tpu-bucket/redpajama_arxiv/1.0.0)
I0512 23:44:48.550059 140143678494720 dataset_builder.py:421] Reusing dataset redpajama_arxiv (gs://rosinality-tpu-bucket/redpajama_arxiv/1.0.0)
I0512 23:44:48.647026 139666521167872 logging_logger.py:49] Constructing tf.data.Dataset redpajama_arxiv for split train, from gs://rosinality-tpu-bucket/redpajama_arxiv/1.0.0
I0512 23:44:48.662622 139876724979712 logging_logger.py:49] Constructing tf.data.Dataset redpajama_arxiv for split train, from gs://rosinality-tpu-bucket/redpajama_arxiv/1.0.0
I0512 23:44:48.656029 140643590531072 logging_logger.py:49] Constructing tf.data.Dataset redpajama_arxiv for split train, from gs://rosinality-tpu-bucket/redpajama_arxiv/1.0.0
I0512 23:44:48.658683 140260698863616 logging_logger.py:49] Constructing tf.data.Dataset redpajama_arxiv for split train, from gs://rosinality-tpu-bucket/redpajama_arxiv/1.0.0
I0512 23:44:48.680229 139856202532864 logging_logger.py:49] Constructing tf.data.Dataset redpajama_arxiv for split train, from gs://rosinality-tpu-bucket/redpajama_arxiv/1.0.0
I0512 23:44:48.679154 140180528838656 logging_logger.py:49] Constructing tf.data.Dataset redpajama_arxiv for split train, from gs://rosinality-tpu-bucket/redpajama_arxiv/1.0.0
I0512 23:44:48.709739 140193652815872 logging_logger.py:49] Constructing tf.data.Dataset redpajama_arxiv for split train, from gs://rosinality-tpu-bucket/redpajama_arxiv/1.0.0
I0512 23:44:48.709717 140143678494720 logging_logger.py:49] Constructing tf.data.Dataset redpajama_arxiv for split train, from gs://rosinality-tpu-bucket/redpajama_arxiv/1.0.0
WARNING:tensorflow:From /home/rosinality/openmoe_venv/lib/python3.10/site-packages/tensorflow/python/util/nest_util.py:1105: _RandomDataset.__init__ (from tensorflow.python.data.experimental.ops.random_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.random(...)`.
W0512 23:44:49.639977 140143678494720 deprecation.py:50] From /home/rosinality/openmoe_venv/lib/python3.10/site-packages/tensorflow/python/util/nest_util.py:1105: _RandomDataset.__init__ (from tensorflow.python.data.experimental.ops.random_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.random(...)`.
WARNING:tensorflow:From /home/rosinality/openmoe_venv/lib/python3.10/site-packages/tensorflow/python/util/nest_util.py:1105: _RandomDataset.__init__ (from tensorflow.python.data.experimental.ops.random_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.random(...)`.
W0512 23:44:49.643659 139666521167872 deprecation.py:50] From /home/rosinality/openmoe_venv/lib/python3.10/site-packages/tensorflow/python/util/nest_util.py:1105: _RandomDataset.__init__ (from tensorflow.python.data.experimental.ops.random_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.random(...)`.
WARNING:tensorflow:From /home/rosinality/openmoe_venv/lib/python3.10/site-packages/tensorflow/python/util/nest_util.py:1105: _RandomDataset.__init__ (from tensorflow.python.data.experimental.ops.random_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.random(...)`.
W0512 23:44:49.659471 139876724979712 deprecation.py:50] From /home/rosinality/openmoe_venv/lib/python3.10/site-packages/tensorflow/python/util/nest_util.py:1105: _RandomDataset.__init__ (from tensorflow.python.data.experimental.ops.random_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.random(...)`.
WARNING:tensorflow:From /home/rosinality/openmoe_venv/lib/python3.10/site-packages/tensorflow/python/util/nest_util.py:1105: _RandomDataset.__init__ (from tensorflow.python.data.experimental.ops.random_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.random(...)`.
W0512 23:44:49.647400 140643590531072 deprecation.py:50] From /home/rosinality/openmoe_venv/lib/python3.10/site-packages/tensorflow/python/util/nest_util.py:1105: _RandomDataset.__init__ (from tensorflow.python.data.experimental.ops.random_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.random(...)`.
WARNING:tensorflow:From /home/rosinality/openmoe_venv/lib/python3.10/site-packages/tensorflow/python/util/nest_util.py:1105: _RandomDataset.__init__ (from tensorflow.python.data.experimental.ops.random_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.random(...)`.
W0512 23:44:49.707597 139856202532864 deprecation.py:50] From /home/rosinality/openmoe_venv/lib/python3.10/site-packages/tensorflow/python/util/nest_util.py:1105: _RandomDataset.__init__ (from tensorflow.python.data.experimental.ops.random_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.random(...)`.
WARNING:tensorflow:From /home/rosinality/openmoe_venv/lib/python3.10/site-packages/tensorflow/python/util/nest_util.py:1105: _RandomDataset.__init__ (from tensorflow.python.data.experimental.ops.random_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.random(...)`.
W0512 23:44:49.710854 140180528838656 deprecation.py:50] From /home/rosinality/openmoe_venv/lib/python3.10/site-packages/tensorflow/python/util/nest_util.py:1105: _RandomDataset.__init__ (from tensorflow.python.data.experimental.ops.random_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.random(...)`.
WARNING:tensorflow:From /home/rosinality/openmoe_venv/lib/python3.10/site-packages/tensorflow/python/util/nest_util.py:1105: _RandomDataset.__init__ (from tensorflow.python.data.experimental.ops.random_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.random(...)`.
W0512 23:44:49.737864 140260698863616 deprecation.py:50] From /home/rosinality/openmoe_venv/lib/python3.10/site-packages/tensorflow/python/util/nest_util.py:1105: _RandomDataset.__init__ (from tensorflow.python.data.experimental.ops.random_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.random(...)`.
WARNING:tensorflow:From /home/rosinality/openmoe_venv/lib/python3.10/site-packages/tensorflow/python/util/nest_util.py:1105: _RandomDataset.__init__ (from tensorflow.python.data.experimental.ops.random_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.random(...)`.
W0512 23:44:49.770027 140193652815872 deprecation.py:50] From /home/rosinality/openmoe_venv/lib/python3.10/site-packages/tensorflow/python/util/nest_util.py:1105: _RandomDataset.__init__ (from tensorflow.python.data.experimental.ops.random_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.random(...)`.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
WARNING:tensorflow:From /home/rosinality/t5x/text-to-text-transfer-transformer/t5/data/preprocessors.py:2902: sample_from_datasets_v2 (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.sample_from_datasets(...)`.
W0512 23:44:53.907794 140143678494720 deprecation.py:50] From /home/rosinality/t5x/text-to-text-transfer-transformer/t5/data/preprocessors.py:2902: sample_from_datasets_v2 (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.sample_from_datasets(...)`.
WARNING:tensorflow:From /home/rosinality/t5x/text-to-text-transfer-transformer/t5/data/preprocessors.py:2902: sample_from_datasets_v2 (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.sample_from_datasets(...)`.
W0512 23:44:54.167277 139666521167872 deprecation.py:50] From /home/rosinality/t5x/text-to-text-transfer-transformer/t5/data/preprocessors.py:2902: sample_from_datasets_v2 (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.sample_from_datasets(...)`.
WARNING:tensorflow:From /home/rosinality/t5x/text-to-text-transfer-transformer/t5/data/preprocessors.py:2902: sample_from_datasets_v2 (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.sample_from_datasets(...)`.
W0512 23:44:54.167656 140643590531072 deprecation.py:50] From /home/rosinality/t5x/text-to-text-transfer-transformer/t5/data/preprocessors.py:2902: sample_from_datasets_v2 (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.sample_from_datasets(...)`.
WARNING:tensorflow:From /home/rosinality/t5x/text-to-text-transfer-transformer/t5/data/preprocessors.py:2902: sample_from_datasets_v2 (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.sample_from_datasets(...)`.
W0512 23:44:54.212708 139876724979712 deprecation.py:50] From /home/rosinality/t5x/text-to-text-transfer-transformer/t5/data/preprocessors.py:2902: sample_from_datasets_v2 (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.sample_from_datasets(...)`.
WARNING:tensorflow:From /home/rosinality/t5x/text-to-text-transfer-transformer/t5/data/preprocessors.py:2902: sample_from_datasets_v2 (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.sample_from_datasets(...)`.
W0512 23:44:54.337677 139856202532864 deprecation.py:50] From /home/rosinality/t5x/text-to-text-transfer-transformer/t5/data/preprocessors.py:2902: sample_from_datasets_v2 (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.sample_from_datasets(...)`.
WARNING:tensorflow:From /home/rosinality/t5x/text-to-text-transfer-transformer/t5/data/preprocessors.py:2902: sample_from_datasets_v2 (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.sample_from_datasets(...)`.
W0512 23:44:54.354951 140180528838656 deprecation.py:50] From /home/rosinality/t5x/text-to-text-transfer-transformer/t5/data/preprocessors.py:2902: sample_from_datasets_v2 (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.sample_from_datasets(...)`.
WARNING:tensorflow:From /home/rosinality/t5x/text-to-text-transfer-transformer/t5/data/preprocessors.py:2902: sample_from_datasets_v2 (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.sample_from_datasets(...)`.
W0512 23:44:54.449684 140193652815872 deprecation.py:50] From /home/rosinality/t5x/text-to-text-transfer-transformer/t5/data/preprocessors.py:2902: sample_from_datasets_v2 (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.sample_from_datasets(...)`.
I0512 23:44:54.481874 140143678494720 dataset_info.py:540] Load dataset info from gs://rosinality-tpu-bucket/redpajama_c4/1.0.0
WARNING:tensorflow:From /home/rosinality/t5x/text-to-text-transfer-transformer/t5/data/preprocessors.py:2902: sample_from_datasets_v2 (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.sample_from_datasets(...)`.
W0512 23:44:54.536289 140260698863616 deprecation.py:50] From /home/rosinality/t5x/text-to-text-transfer-transformer/t5/data/preprocessors.py:2902: sample_from_datasets_v2 (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.sample_from_datasets(...)`.
I0512 23:44:54.690809 139666521167872 dataset_info.py:540] Load dataset info from gs://rosinality-tpu-bucket/redpajama_c4/1.0.0
I0512 23:44:54.693803 140643590531072 dataset_info.py:540] Load dataset info from gs://rosinality-tpu-bucket/redpajama_c4/1.0.0
I0512 23:44:54.743874 139876724979712 dataset_info.py:540] Load dataset info from gs://rosinality-tpu-bucket/redpajama_c4/1.0.0
I0512 23:44:54.743565 140143678494720 dataset_providers.py:1597] Sharding at the data source: 1 of 8
I0512 23:44:54.826222 140143678494720 dataset_info.py:540] Load dataset info from gs://rosinality-tpu-bucket/redpajama_c4/1.0.0
I0512 23:44:54.903451 139856202532864 dataset_info.py:540] Load dataset info from gs://rosinality-tpu-bucket/redpajama_c4/1.0.0
I0512 23:44:54.924787 139666521167872 dataset_providers.py:1597] Sharding at the data source: 6 of 8
I0512 23:44:54.913173 140180528838656 dataset_info.py:540] Load dataset info from gs://rosinality-tpu-bucket/redpajama_c4/1.0.0
I0512 23:44:54.917801 140643590531072 dataset_providers.py:1597] Sharding at the data source: 5 of 8
I0512 23:44:54.997805 139666521167872 dataset_info.py:540] Load dataset info from gs://rosinality-tpu-bucket/redpajama_c4/1.0.0
I0512 23:44:54.982007 140643590531072 dataset_info.py:540] Load dataset info from gs://rosinality-tpu-bucket/redpajama_c4/1.0.0
I0512 23:44:55.000487 140193652815872 dataset_info.py:540] Load dataset info from gs://rosinality-tpu-bucket/redpajama_c4/1.0.0
I0512 23:44:55.039780 139876724979712 dataset_providers.py:1597] Sharding at the data source: 2 of 8
I0512 23:44:55.048323 140143678494720 dataset_builder.py:421] Reusing dataset redpajama_c4 (gs://rosinality-tpu-bucket/redpajama_c4/1.0.0)
I0512 23:44:55.109199 139876724979712 dataset_info.py:540] Load dataset info from gs://rosinality-tpu-bucket/redpajama_c4/1.0.0
I0512 23:44:55.121550 139856202532864 dataset_providers.py:1597] Sharding at the data source: 8 of 8
I0512 23:44:55.104358 140260698863616 dataset_info.py:540] Load dataset info from gs://rosinality-tpu-bucket/redpajama_c4/1.0.0
I0512 23:44:55.121692 140143678494720 logging_logger.py:49] Constructing tf.data.Dataset redpajama_c4 for split train[:-2000], from gs://rosinality-tpu-bucket/redpajama_c4/1.0.0
I0512 23:44:55.128497 140180528838656 dataset_providers.py:1597] Sharding at the data source: 7 of 8
I0512 23:44:55.178016 139856202532864 dataset_info.py:540] Load dataset info from gs://rosinality-tpu-bucket/redpajama_c4/1.0.0
I0512 23:44:55.207760 140193652815872 dataset_providers.py:1597] Sharding at the data source: 3 of 8
I0512 23:44:55.192608 140643590531072 dataset_builder.py:421] Reusing dataset redpajama_c4 (gs://rosinality-tpu-bucket/redpajama_c4/1.0.0)
I0512 23:44:55.243680 139666521167872 dataset_builder.py:421] Reusing dataset redpajama_c4 (gs://rosinality-tpu-bucket/redpajama_c4/1.0.0)
I0512 23:44:55.270964 140193652815872 dataset_info.py:540] Load dataset info from gs://rosinality-tpu-bucket/redpajama_c4/1.0.0
I0512 23:44:55.274215 140643590531072 logging_logger.py:49] Constructing tf.data.Dataset redpajama_c4 for split train[:-2000], from gs://rosinality-tpu-bucket/redpajama_c4/1.0.0
I0512 23:44:55.315964 139876724979712 dataset_builder.py:421] Reusing dataset redpajama_c4 (gs://rosinality-tpu-bucket/redpajama_c4/1.0.0)
I0512 23:44:55.302251 140180528838656 dataset_info.py:540] Load dataset info from gs://rosinality-tpu-bucket/redpajama_c4/1.0.0
I0512 23:44:55.321311 139666521167872 logging_logger.py:49] Constructing tf.data.Dataset redpajama_c4 for split train[:-2000], from gs://rosinality-tpu-bucket/redpajama_c4/1.0.0
I0512 23:44:55.335197 140260698863616 dataset_providers.py:1597] Sharding at the data source: 4 of 8
I0512 23:44:55.393179 139876724979712 logging_logger.py:49] Constructing tf.data.Dataset redpajama_c4 for split train[:-2000], from gs://rosinality-tpu-bucket/redpajama_c4/1.0.0
I0512 23:44:55.397534 139856202532864 dataset_builder.py:421] Reusing dataset redpajama_c4 (gs://rosinality-tpu-bucket/redpajama_c4/1.0.0)
I0512 23:44:55.401570 140260698863616 dataset_info.py:540] Load dataset info from gs://rosinality-tpu-bucket/redpajama_c4/1.0.0
I0512 23:44:55.474502 139856202532864 logging_logger.py:49] Constructing tf.data.Dataset redpajama_c4 for split train[:-2000], from gs://rosinality-tpu-bucket/redpajama_c4/1.0.0
I0512 23:44:55.498376 140193652815872 dataset_builder.py:421] Reusing dataset redpajama_c4 (gs://rosinality-tpu-bucket/redpajama_c4/1.0.0)
I0512 23:44:55.535604 140180528838656 dataset_builder.py:421] Reusing dataset redpajama_c4 (gs://rosinality-tpu-bucket/redpajama_c4/1.0.0)
I0512 23:44:55.575345 140193652815872 logging_logger.py:49] Constructing tf.data.Dataset redpajama_c4 for split train[:-2000], from gs://rosinality-tpu-bucket/redpajama_c4/1.0.0
I0512 23:44:55.610210 140260698863616 dataset_builder.py:421] Reusing dataset redpajama_c4 (gs://rosinality-tpu-bucket/redpajama_c4/1.0.0)
I0512 23:44:55.615044 140180528838656 logging_logger.py:49] Constructing tf.data.Dataset redpajama_c4 for split train[:-2000], from gs://rosinality-tpu-bucket/redpajama_c4/1.0.0
I0512 23:44:55.690505 140260698863616 logging_logger.py:49] Constructing tf.data.Dataset redpajama_c4 for split train[:-2000], from gs://rosinality-tpu-bucket/redpajama_c4/1.0.0
I0512 23:44:56.455088 140143678494720 dataset_info.py:540] Load dataset info from gs://rosinality-tpu-bucket/redpajama_common_crawl/1.0.0
I0512 23:44:56.575690 140643590531072 dataset_info.py:540] Load dataset info from gs://rosinality-tpu-bucket/redpajama_common_crawl/1.0.0
I0512 23:44:56.646104 139666521167872 dataset_info.py:540] Load dataset info from gs://rosinality-tpu-bucket/redpajama_common_crawl/1.0.0
I0512 23:44:56.720251 139876724979712 dataset_info.py:540] Load dataset info from gs://rosinality-tpu-bucket/redpajama_common_crawl/1.0.0
I0512 23:44:56.825650 139856202532864 dataset_info.py:540] Load dataset info from gs://rosinality-tpu-bucket/redpajama_common_crawl/1.0.0
I0512 23:44:56.902336 139666521167872 dataset_providers.py:1597] Sharding at the data source: 6 of 8
I0512 23:44:56.889105 140643590531072 dataset_providers.py:1597] Sharding at the data source: 5 of 8
I0512 23:44:56.907473 140143678494720 dataset_providers.py:1597] Sharding at the data source: 1 of 8
I0512 23:44:56.933571 140193652815872 dataset_info.py:540] Load dataset info from gs://rosinality-tpu-bucket/redpajama_common_crawl/1.0.0
I0512 23:44:56.967888 140143678494720 dataset_info.py:540] Load dataset info from gs://rosinality-tpu-bucket/redpajama_common_crawl/1.0.0
I0512 23:44:56.974403 139666521167872 dataset_info.py:540] Load dataset info from gs://rosinality-tpu-bucket/redpajama_common_crawl/1.0.0
I0512 23:44:56.971031 140643590531072 dataset_info.py:540] Load dataset info from gs://rosinality-tpu-bucket/redpajama_common_crawl/1.0.0
I0512 23:44:56.991041 140180528838656 dataset_info.py:540] Load dataset info from gs://rosinality-tpu-bucket/redpajama_common_crawl/1.0.0
I0512 23:44:57.027501 139876724979712 dataset_providers.py:1597] Sharding at the data source: 2 of 8
I0512 23:44:57.068245 140260698863616 dataset_info.py:540] Load dataset info from gs://rosinality-tpu-bucket/redpajama_common_crawl/1.0.0
I0512 23:44:57.088046 139876724979712 dataset_info.py:540] Load dataset info from gs://rosinality-tpu-bucket/redpajama_common_crawl/1.0.0
I0512 23:44:57.118742 139856202532864 dataset_providers.py:1597] Sharding at the data source: 8 of 8
I0512 23:44:57.179788 139856202532864 dataset_info.py:540] Load dataset info from gs://rosinality-tpu-bucket/redpajama_common_crawl/1.0.0
I0512 23:44:57.220140 140193652815872 dataset_providers.py:1597] Sharding at the data source: 3 of 8
I0512 23:44:57.246762 140643590531072 dataset_builder.py:421] Reusing dataset redpajama_common_crawl (gs://rosinality-tpu-bucket/redpajama_common_crawl/1.0.0)
I0512 23:44:57.267808 140143678494720 dataset_builder.py:421] Reusing dataset redpajama_common_crawl (gs://rosinality-tpu-bucket/redpajama_common_crawl/1.0.0)
I0512 23:44:57.289020 139666521167872 dataset_builder.py:421] Reusing dataset redpajama_common_crawl (gs://rosinality-tpu-bucket/redpajama_common_crawl/1.0.0)
I0512 23:44:57.288937 140193652815872 dataset_info.py:540] Load dataset info from gs://rosinality-tpu-bucket/redpajama_common_crawl/1.0.0
I0512 23:44:57.296907 140180528838656 dataset_providers.py:1597] Sharding at the data source: 7 of 8
I0512 23:44:57.335970 140643590531072 logging_logger.py:49] Constructing tf.data.Dataset redpajama_common_crawl for split train, from gs://rosinality-tpu-bucket/redpajama_common_crawl/1.0.0
I0512 23:44:57.367009 140143678494720 logging_logger.py:49] Constructing tf.data.Dataset redpajama_common_crawl for split train, from gs://rosinality-tpu-bucket/redpajama_common_crawl/1.0.0
I0512 23:44:57.374937 139876724979712 dataset_builder.py:421] Reusing dataset redpajama_common_crawl (gs://rosinality-tpu-bucket/redpajama_common_crawl/1.0.0)
I0512 23:44:57.380695 139666521167872 logging_logger.py:49] Constructing tf.data.Dataset redpajama_common_crawl for split train, from gs://rosinality-tpu-bucket/redpajama_common_crawl/1.0.0
I0512 23:44:57.375055 140180528838656 dataset_info.py:540] Load dataset info from gs://rosinality-tpu-bucket/redpajama_common_crawl/1.0.0
I0512 23:44:57.375051 140260698863616 dataset_providers.py:1597] Sharding at the data source: 4 of 8
I0512 23:44:57.436080 140260698863616 dataset_info.py:540] Load dataset info from gs://rosinality-tpu-bucket/redpajama_common_crawl/1.0.0
I0512 23:44:57.466996 139876724979712 logging_logger.py:49] Constructing tf.data.Dataset redpajama_common_crawl for split train, from gs://rosinality-tpu-bucket/redpajama_common_crawl/1.0.0
I0512 23:44:57.473303 139856202532864 dataset_builder.py:421] Reusing dataset redpajama_common_crawl (gs://rosinality-tpu-bucket/redpajama_common_crawl/1.0.0)
I0512 23:44:57.553396 140193652815872 dataset_builder.py:421] Reusing dataset redpajama_common_crawl (gs://rosinality-tpu-bucket/redpajama_common_crawl/1.0.0)
I0512 23:44:57.565533 139856202532864 logging_logger.py:49] Constructing tf.data.Dataset redpajama_common_crawl for split train, from gs://rosinality-tpu-bucket/redpajama_common_crawl/1.0.0
I0512 23:44:57.651443 140193652815872 logging_logger.py:49] Constructing tf.data.Dataset redpajama_common_crawl for split train, from gs://rosinality-tpu-bucket/redpajama_common_crawl/1.0.0
I0512 23:44:57.640336 140180528838656 dataset_builder.py:421] Reusing dataset redpajama_common_crawl (gs://rosinality-tpu-bucket/redpajama_common_crawl/1.0.0)
I0512 23:44:57.705835 140260698863616 dataset_builder.py:421] Reusing dataset redpajama_common_crawl (gs://rosinality-tpu-bucket/redpajama_common_crawl/1.0.0)
I0512 23:44:57.739168 140180528838656 logging_logger.py:49] Constructing tf.data.Dataset redpajama_common_crawl for split train, from gs://rosinality-tpu-bucket/redpajama_common_crawl/1.0.0
I0512 23:44:57.806168 140260698863616 logging_logger.py:49] Constructing tf.data.Dataset redpajama_common_crawl for split train, from gs://rosinality-tpu-bucket/redpajama_common_crawl/1.0.0
I0512 23:44:58.614163 140643590531072 dataset_info.py:540] Load dataset info from gs://rosinality-tpu-bucket/redpajama_github/1.0.0
I0512 23:44:58.646916 140143678494720 dataset_info.py:540] Load dataset info from gs://rosinality-tpu-bucket/redpajama_github/1.0.0
I0512 23:44:58.661295 139666521167872 dataset_info.py:540] Load dataset info from gs://rosinality-tpu-bucket/redpajama_github/1.0.0
I0512 23:44:58.729626 139876724979712 dataset_info.py:540] Load dataset info from gs://rosinality-tpu-bucket/redpajama_github/1.0.0
I0512 23:44:58.832128 140643590531072 dataset_providers.py:1597] Sharding at the data source: 5 of 8
I0512 23:44:58.871618 139856202532864 dataset_info.py:540] Load dataset info from gs://rosinality-tpu-bucket/redpajama_github/1.0.0
I0512 23:44:58.893650 140143678494720 dataset_providers.py:1597] Sharding at the data source: 1 of 8
I0512 23:44:58.901131 139666521167872 dataset_providers.py:1597] Sharding at the data source: 6 of 8
I0512 23:44:58.886465 140643590531072 dataset_info.py:540] Load dataset info from gs://rosinality-tpu-bucket/redpajama_github/1.0.0
I0512 23:44:58.940431 139876724979712 dataset_providers.py:1597] Sharding at the data source: 2 of 8
I0512 23:44:58.951001 140143678494720 dataset_info.py:540] Load dataset info from gs://rosinality-tpu-bucket/redpajama_github/1.0.0
I0512 23:44:58.971443 140193652815872 dataset_info.py:540] Load dataset info from gs://rosinality-tpu-bucket/redpajama_github/1.0.0
I0512 23:44:58.990714 139666521167872 dataset_info.py:540] Load dataset info from gs://rosinality-tpu-bucket/redpajama_github/1.0.0
I0512 23:44:59.004667 139876724979712 dataset_info.py:540] Load dataset info from gs://rosinality-tpu-bucket/redpajama_github/1.0.0
I0512 23:44:59.030355 140180528838656 dataset_info.py:540] Load dataset info from gs://rosinality-tpu-bucket/redpajama_github/1.0.0
I0512 23:44:59.073760 139856202532864 dataset_providers.py:1597] Sharding at the data source: 8 of 8
I0512 23:44:59.135235 139856202532864 dataset_info.py:540] Load dataset info from gs://rosinality-tpu-bucket/redpajama_github/1.0.0
I0512 23:44:59.128789 140260698863616 dataset_info.py:540] Load dataset info from gs://rosinality-tpu-bucket/redpajama_github/1.0.0
I0512 23:44:59.158599 140643590531072 dataset_builder.py:421] Reusing dataset redpajama_github (gs://rosinality-tpu-bucket/redpajama_github/1.0.0)
I0512 23:44:59.179411 140143678494720 dataset_builder.py:421] Reusing dataset redpajama_github (gs://rosinality-tpu-bucket/redpajama_github/1.0.0)
I0512 23:44:59.183585 140193652815872 dataset_providers.py:1597] Sharding at the data source: 3 of 8
I0512 23:44:59.193816 139666521167872 dataset_builder.py:421] Reusing dataset redpajama_github (gs://rosinality-tpu-bucket/redpajama_github/1.0.0)
I0512 23:44:59.196276 139876724979712 dataset_builder.py:421] Reusing dataset redpajama_github (gs://rosinality-tpu-bucket/redpajama_github/1.0.0)
I0512 23:44:59.236071 140643590531072 logging_logger.py:49] Constructing tf.data.Dataset redpajama_github for split train, from gs://rosinality-tpu-bucket/redpajama_github/1.0.0
I0512 23:44:59.238929 140180528838656 dataset_providers.py:1597] Sharding at the data source: 7 of 8
I0512 23:44:59.255028 140143678494720 logging_logger.py:49] Constructing tf.data.Dataset redpajama_github for split train, from gs://rosinality-tpu-bucket/redpajama_github/1.0.0
I0512 23:44:59.266428 139666521167872 logging_logger.py:49] Constructing tf.data.Dataset redpajama_github for split train, from gs://rosinality-tpu-bucket/redpajama_github/1.0.0
I0512 23:44:59.277506 140193652815872 dataset_info.py:540] Load dataset info from gs://rosinality-tpu-bucket/redpajama_github/1.0.0
I0512 23:44:59.280234 139876724979712 logging_logger.py:49] Constructing tf.data.Dataset redpajama_github for split train, from gs://rosinality-tpu-bucket/redpajama_github/1.0.0
I0512 23:44:59.296800 140180528838656 dataset_info.py:540] Load dataset info from gs://rosinality-tpu-bucket/redpajama_github/1.0.0
I0512 23:44:59.323942 140260698863616 dataset_providers.py:1597] Sharding at the data source: 4 of 8
I0512 23:44:59.352006 139856202532864 dataset_builder.py:421] Reusing dataset redpajama_github (gs://rosinality-tpu-bucket/redpajama_github/1.0.0)
I0512 23:44:59.391845 140260698863616 dataset_info.py:540] Load dataset info from gs://rosinality-tpu-bucket/redpajama_github/1.0.0
I0512 23:44:59.429940 139856202532864 logging_logger.py:49] Constructing tf.data.Dataset redpajama_github for split train, from gs://rosinality-tpu-bucket/redpajama_github/1.0.0
I0512 23:44:59.500844 140193652815872 dataset_builder.py:421] Reusing dataset redpajama_github (gs://rosinality-tpu-bucket/redpajama_github/1.0.0)
I0512 23:44:59.498219 140180528838656 dataset_builder.py:421] Reusing dataset redpajama_github (gs://rosinality-tpu-bucket/redpajama_github/1.0.0)
I0512 23:44:59.571360 140193652815872 logging_logger.py:49] Constructing tf.data.Dataset redpajama_github for split train, from gs://rosinality-tpu-bucket/redpajama_github/1.0.0
I0512 23:44:59.565303 140180528838656 logging_logger.py:49] Constructing tf.data.Dataset redpajama_github for split train, from gs://rosinality-tpu-bucket/redpajama_github/1.0.0
I0512 23:44:59.615284 140260698863616 dataset_builder.py:421] Reusing dataset redpajama_github (gs://rosinality-tpu-bucket/redpajama_github/1.0.0)
I0512 23:44:59.686304 140260698863616 logging_logger.py:49] Constructing tf.data.Dataset redpajama_github for split train, from gs://rosinality-tpu-bucket/redpajama_github/1.0.0
I0512 23:45:00.508838 140643590531072 dataset_info.py:540] Load dataset info from gs://rosinality-tpu-bucket/redpajama_stackexchange/1.0.0
I0512 23:45:00.531789 140143678494720 dataset_info.py:540] Load dataset info from gs://rosinality-tpu-bucket/redpajama_stackexchange/1.0.0
I0512 23:45:00.564415 139876724979712 dataset_info.py:540] Load dataset info from gs://rosinality-tpu-bucket/redpajama_stackexchange/1.0.0
I0512 23:45:00.620432 139666521167872 dataset_info.py:540] Load dataset info from gs://rosinality-tpu-bucket/redpajama_stackexchange/1.0.0
I0512 23:45:00.723196 139856202532864 dataset_info.py:540] Load dataset info from gs://rosinality-tpu-bucket/redpajama_stackexchange/1.0.0
I0512 23:45:00.726814 140143678494720 dataset_providers.py:1597] Sharding at the data source: 1 of 8
I0512 23:45:00.731392 140643590531072 dataset_providers.py:1597] Sharding at the data source: 5 of 8
I0512 23:45:00.782987 139876724979712 dataset_providers.py:1597] Sharding at the data source: 2 of 8
I0512 23:45:00.787571 140143678494720 dataset_info.py:540] Load dataset info from gs://rosinality-tpu-bucket/redpajama_stackexchange/1.0.0
I0512 23:45:00.796383 140643590531072 dataset_info.py:540] Load dataset info from gs://rosinality-tpu-bucket/redpajama_stackexchange/1.0.0
I0512 23:45:00.840790 139876724979712 dataset_info.py:540] Load dataset info from gs://rosinality-tpu-bucket/redpajama_stackexchange/1.0.0
I0512 23:45:00.846727 139666521167872 dataset_providers.py:1597] Sharding at the data source: 6 of 8
I0512 23:45:00.859824 140180528838656 dataset_info.py:540] Load dataset info from gs://rosinality-tpu-bucket/redpajama_stackexchange/1.0.0
I0512 23:45:00.880920 140193652815872 dataset_info.py:540] Load dataset info from gs://rosinality-tpu-bucket/redpajama_stackexchange/1.0.0
I0512 23:45:00.922390 139666521167872 dataset_info.py:540] Load dataset info from gs://rosinality-tpu-bucket/redpajama_stackexchange/1.0.0
I0512 23:45:00.936245 139856202532864 dataset_providers.py:1597] Sharding at the data source: 8 of 8
I0512 23:45:00.996380 139856202532864 dataset_info.py:540] Load dataset info from gs://rosinality-tpu-bucket/redpajama_stackexchange/1.0.0
I0512 23:45:00.994605 140143678494720 dataset_builder.py:421] Reusing dataset redpajama_stackexchange (gs://rosinality-tpu-bucket/redpajama_stackexchange/1.0.0)
I0512 23:45:00.999858 140643590531072 dataset_builder.py:421] Reusing dataset redpajama_stackexchange (gs://rosinality-tpu-bucket/redpajama_stackexchange/1.0.0)
I0512 23:45:00.998906 140260698863616 dataset_info.py:540] Load dataset info from gs://rosinality-tpu-bucket/redpajama_stackexchange/1.0.0
I0512 23:45:01.068327 139876724979712 dataset_builder.py:421] Reusing dataset redpajama_stackexchange (gs://rosinality-tpu-bucket/redpajama_stackexchange/1.0.0)
I0512 23:45:01.069473 140143678494720 logging_logger.py:49] Constructing tf.data.Dataset redpajama_stackexchange for split train, from gs://rosinality-tpu-bucket/redpajama_stackexchange/1.0.0
I0512 23:45:01.063268 140180528838656 dataset_providers.py:1597] Sharding at the data source: 7 of 8
I0512 23:45:01.080541 140643590531072 logging_logger.py:49] Constructing tf.data.Dataset redpajama_stackexchange for split train, from gs://rosinality-tpu-bucket/redpajama_stackexchange/1.0.0
I0512 23:45:01.127702 140193652815872 dataset_providers.py:1597] Sharding at the data source: 3 of 8
I0512 23:45:01.140449 139876724979712 logging_logger.py:49] Constructing tf.data.Dataset redpajama_stackexchange for split train, from gs://rosinality-tpu-bucket/redpajama_stackexchange/1.0.0
I0512 23:45:01.122307 140180528838656 dataset_info.py:540] Load dataset info from gs://rosinality-tpu-bucket/redpajama_stackexchange/1.0.0
I0512 23:45:01.141923 139666521167872 dataset_builder.py:421] Reusing dataset redpajama_stackexchange (gs://rosinality-tpu-bucket/redpajama_stackexchange/1.0.0)
I0512 23:45:01.194497 140193652815872 dataset_info.py:540] Load dataset info from gs://rosinality-tpu-bucket/redpajama_stackexchange/1.0.0
I0512 23:45:01.199262 139856202532864 dataset_builder.py:421] Reusing dataset redpajama_stackexchange (gs://rosinality-tpu-bucket/redpajama_stackexchange/1.0.0)
I0512 23:45:01.196052 140260698863616 dataset_providers.py:1597] Sharding at the data source: 4 of 8
I0512 23:45:01.215786 139666521167872 logging_logger.py:49] Constructing tf.data.Dataset redpajama_stackexchange for split train, from gs://rosinality-tpu-bucket/redpajama_stackexchange/1.0.0
I0512 23:45:01.274074 139856202532864 logging_logger.py:49] Constructing tf.data.Dataset redpajama_stackexchange for split train, from gs://rosinality-tpu-bucket/redpajama_stackexchange/1.0.0
I0512 23:45:01.261462 140260698863616 dataset_info.py:540] Load dataset info from gs://rosinality-tpu-bucket/redpajama_stackexchange/1.0.0
I0512 23:45:01.364616 140180528838656 dataset_builder.py:421] Reusing dataset redpajama_stackexchange (gs://rosinality-tpu-bucket/redpajama_stackexchange/1.0.0)
I0512 23:45:01.406746 140193652815872 dataset_builder.py:421] Reusing dataset redpajama_stackexchange (gs://rosinality-tpu-bucket/redpajama_stackexchange/1.0.0)
I0512 23:45:01.438405 140180528838656 logging_logger.py:49] Constructing tf.data.Dataset redpajama_stackexchange for split train, from gs://rosinality-tpu-bucket/redpajama_stackexchange/1.0.0
I0512 23:45:01.484196 140193652815872 logging_logger.py:49] Constructing tf.data.Dataset redpajama_stackexchange for split train, from gs://rosinality-tpu-bucket/redpajama_stackexchange/1.0.0
I0512 23:45:01.469094 140260698863616 dataset_builder.py:421] Reusing dataset redpajama_stackexchange (gs://rosinality-tpu-bucket/redpajama_stackexchange/1.0.0)
I0512 23:45:01.543525 140260698863616 logging_logger.py:49] Constructing tf.data.Dataset redpajama_stackexchange for split train, from gs://rosinality-tpu-bucket/redpajama_stackexchange/1.0.0
I0512 23:45:02.351581 140643590531072 dataset_info.py:540] Load dataset info from gs://rosinality-tpu-bucket/redpajama_wikipedia/1.0.0
I0512 23:45:02.379689 140143678494720 dataset_info.py:540] Load dataset info from gs://rosinality-tpu-bucket/redpajama_wikipedia/1.0.0
I0512 23:45:02.420926 139876724979712 dataset_info.py:540] Load dataset info from gs://rosinality-tpu-bucket/redpajama_wikipedia/1.0.0
I0512 23:45:02.508514 139666521167872 dataset_info.py:540] Load dataset info from gs://rosinality-tpu-bucket/redpajama_wikipedia/1.0.0
I0512 23:45:02.589810 140143678494720 dataset_providers.py:1597] Sharding at the data source: 1 of 8
I0512 23:45:02.604488 139856202532864 dataset_info.py:540] Load dataset info from gs://rosinality-tpu-bucket/redpajama_wikipedia/1.0.0
I0512 23:45:02.614682 140643590531072 dataset_providers.py:1597] Sharding at the data source: 5 of 8
I0512 23:45:02.644032 139876724979712 dataset_providers.py:1597] Sharding at the data source: 2 of 8
I0512 23:45:02.647667 140143678494720 dataset_info.py:540] Load dataset info from gs://rosinality-tpu-bucket/redpajama_wikipedia/1.0.0
I0512 23:45:02.674117 140643590531072 dataset_info.py:540] Load dataset info from gs://rosinality-tpu-bucket/redpajama_wikipedia/1.0.0
I0512 23:45:02.700062 139876724979712 dataset_info.py:540] Load dataset info from gs://rosinality-tpu-bucket/redpajama_wikipedia/1.0.0
I0512 23:45:02.732703 139666521167872 dataset_providers.py:1597] Sharding at the data source: 6 of 8
I0512 23:45:02.752385 140180528838656 dataset_info.py:540] Load dataset info from gs://rosinality-tpu-bucket/redpajama_wikipedia/1.0.0
I0512 23:45:02.792382 139666521167872 dataset_info.py:540] Load dataset info from gs://rosinality-tpu-bucket/redpajama_wikipedia/1.0.0
I0512 23:45:02.827049 140193652815872 dataset_info.py:540] Load dataset info from gs://rosinality-tpu-bucket/redpajama_wikipedia/1.0.0
I0512 23:45:02.845779 139856202532864 dataset_providers.py:1597] Sharding at the data source: 8 of 8
I0512 23:45:02.854471 140143678494720 dataset_builder.py:421] Reusing dataset redpajama_wikipedia (gs://rosinality-tpu-bucket/redpajama_wikipedia/1.0.0)
I0512 23:45:02.876999 140260698863616 dataset_info.py:540] Load dataset info from gs://rosinality-tpu-bucket/redpajama_wikipedia/1.0.0
I0512 23:45:02.881781 140643590531072 dataset_builder.py:421] Reusing dataset redpajama_wikipedia (gs://rosinality-tpu-bucket/redpajama_wikipedia/1.0.0)
I0512 23:45:02.907665 139856202532864 dataset_info.py:540] Load dataset info from gs://rosinality-tpu-bucket/redpajama_wikipedia/1.0.0
I0512 23:45:02.930834 140143678494720 logging_logger.py:49] Constructing tf.data.Dataset redpajama_wikipedia for split train, from gs://rosinality-tpu-bucket/redpajama_wikipedia/1.0.0
I0512 23:45:02.941257 139876724979712 dataset_builder.py:421] Reusing dataset redpajama_wikipedia (gs://rosinality-tpu-bucket/redpajama_wikipedia/1.0.0)
I0512 23:45:02.952266 140643590531072 logging_logger.py:49] Constructing tf.data.Dataset redpajama_wikipedia for split train, from gs://rosinality-tpu-bucket/redpajama_wikipedia/1.0.0
I0512 23:45:02.955023 140180528838656 dataset_providers.py:1597] Sharding at the data source: 7 of 8
I0512 23:45:02.987559 139666521167872 dataset_builder.py:421] Reusing dataset redpajama_wikipedia (gs://rosinality-tpu-bucket/redpajama_wikipedia/1.0.0)
I0512 23:45:03.011810 139876724979712 logging_logger.py:49] Constructing tf.data.Dataset redpajama_wikipedia for split train, from gs://rosinality-tpu-bucket/redpajama_wikipedia/1.0.0
I0512 23:45:03.039886 140193652815872 dataset_providers.py:1597] Sharding at the data source: 3 of 8
I0512 23:45:03.026033 140180528838656 dataset_info.py:540] Load dataset info from gs://rosinality-tpu-bucket/redpajama_wikipedia/1.0.0
I0512 23:45:03.063381 139666521167872 logging_logger.py:49] Constructing tf.data.Dataset redpajama_wikipedia for split train, from gs://rosinality-tpu-bucket/redpajama_wikipedia/1.0.0
I0512 23:45:03.115241 139856202532864 dataset_builder.py:421] Reusing dataset redpajama_wikipedia (gs://rosinality-tpu-bucket/redpajama_wikipedia/1.0.0)
I0512 23:45:03.114627 140193652815872 dataset_info.py:540] Load dataset info from gs://rosinality-tpu-bucket/redpajama_wikipedia/1.0.0
I0512 23:45:03.131510 140260698863616 dataset_providers.py:1597] Sharding at the data source: 4 of 8
I0512 23:45:03.191000 139856202532864 logging_logger.py:49] Constructing tf.data.Dataset redpajama_wikipedia for split train, from gs://rosinality-tpu-bucket/redpajama_wikipedia/1.0.0
I0512 23:45:03.188736 140260698863616 dataset_info.py:540] Load dataset info from gs://rosinality-tpu-bucket/redpajama_wikipedia/1.0.0
I0512 23:45:03.222776 140180528838656 dataset_builder.py:421] Reusing dataset redpajama_wikipedia (gs://rosinality-tpu-bucket/redpajama_wikipedia/1.0.0)
I0512 23:45:03.305504 140180528838656 logging_logger.py:49] Constructing tf.data.Dataset redpajama_wikipedia for split train, from gs://rosinality-tpu-bucket/redpajama_wikipedia/1.0.0
I0512 23:45:03.392580 140193652815872 dataset_builder.py:421] Reusing dataset redpajama_wikipedia (gs://rosinality-tpu-bucket/redpajama_wikipedia/1.0.0)
I0512 23:45:03.385250 140260698863616 dataset_builder.py:421] Reusing dataset redpajama_wikipedia (gs://rosinality-tpu-bucket/redpajama_wikipedia/1.0.0)
I0512 23:45:03.472613 140193652815872 logging_logger.py:49] Constructing tf.data.Dataset redpajama_wikipedia for split train, from gs://rosinality-tpu-bucket/redpajama_wikipedia/1.0.0
I0512 23:45:03.457876 140260698863616 logging_logger.py:49] Constructing tf.data.Dataset redpajama_wikipedia for split train, from gs://rosinality-tpu-bucket/redpajama_wikipedia/1.0.0
I0512 23:45:04.235372 140143678494720 dataset_info.py:540] Load dataset info from gs://rosinality-tpu-bucket/the_stack_dedup/1.0.0
I0512 23:45:04.255121 140643590531072 dataset_info.py:540] Load dataset info from gs://rosinality-tpu-bucket/the_stack_dedup/1.0.0
I0512 23:45:04.308571 139876724979712 dataset_info.py:540] Load dataset info from gs://rosinality-tpu-bucket/the_stack_dedup/1.0.0
I0512 23:45:04.371173 139666521167872 dataset_info.py:540] Load dataset info from gs://rosinality-tpu-bucket/the_stack_dedup/1.0.0
I0512 23:45:04.513679 139856202532864 dataset_info.py:540] Load dataset info from gs://rosinality-tpu-bucket/the_stack_dedup/1.0.0
I0512 23:45:04.583650 140143678494720 dataset_providers.py:1597] Sharding at the data source: 1 of 8
I0512 23:45:04.603004 140180528838656 dataset_info.py:540] Load dataset info from gs://rosinality-tpu-bucket/the_stack_dedup/1.0.0
I0512 23:45:04.657684 140143678494720 dataset_info.py:540] Load dataset info from gs://rosinality-tpu-bucket/the_stack_dedup/1.0.0
I0512 23:45:04.660521 140643590531072 dataset_providers.py:1597] Sharding at the data source: 5 of 8
I0512 23:45:04.701370 139666521167872 dataset_providers.py:1597] Sharding at the data source: 6 of 8
I0512 23:45:04.713544 139876724979712 dataset_providers.py:1597] Sharding at the data source: 2 of 8
I0512 23:45:04.736160 140643590531072 dataset_info.py:540] Load dataset info from gs://rosinality-tpu-bucket/the_stack_dedup/1.0.0
I0512 23:45:04.762589 139666521167872 dataset_info.py:540] Load dataset info from gs://rosinality-tpu-bucket/the_stack_dedup/1.0.0
I0512 23:45:04.774597 139876724979712 dataset_info.py:540] Load dataset info from gs://rosinality-tpu-bucket/the_stack_dedup/1.0.0
I0512 23:45:04.806974 140193652815872 dataset_info.py:540] Load dataset info from gs://rosinality-tpu-bucket/the_stack_dedup/1.0.0
I0512 23:45:04.823997 140260698863616 dataset_info.py:540] Load dataset info from gs://rosinality-tpu-bucket/the_stack_dedup/1.0.0
I0512 23:45:04.854373 139856202532864 dataset_providers.py:1597] Sharding at the data source: 8 of 8
I0512 23:45:04.917951 139856202532864 dataset_info.py:540] Load dataset info from gs://rosinality-tpu-bucket/the_stack_dedup/1.0.0
I0512 23:45:04.942356 140180528838656 dataset_providers.py:1597] Sharding at the data source: 7 of 8
I0512 23:45:04.966784 140143678494720 dataset_builder.py:421] Reusing dataset the_stack_dedup (gs://rosinality-tpu-bucket/the_stack_dedup/1.0.0)
I0512 23:45:05.003506 140180528838656 dataset_info.py:540] Load dataset info from gs://rosinality-tpu-bucket/the_stack_dedup/1.0.0
I0512 23:45:05.076703 140143678494720 logging_logger.py:49] Constructing tf.data.Dataset the_stack_dedup for split train, from gs://rosinality-tpu-bucket/the_stack_dedup/1.0.0
I0512 23:45:05.089632 139876724979712 dataset_builder.py:421] Reusing dataset the_stack_dedup (gs://rosinality-tpu-bucket/the_stack_dedup/1.0.0)
I0512 23:45:05.118949 139666521167872 dataset_builder.py:421] Reusing dataset the_stack_dedup (gs://rosinality-tpu-bucket/the_stack_dedup/1.0.0)
I0512 23:45:05.116937 140643590531072 dataset_builder.py:421] Reusing dataset the_stack_dedup (gs://rosinality-tpu-bucket/the_stack_dedup/1.0.0)
I0512 23:45:05.156705 140193652815872 dataset_providers.py:1597] Sharding at the data source: 3 of 8
I0512 23:45:05.200895 139876724979712 logging_logger.py:49] Constructing tf.data.Dataset the_stack_dedup for split train, from gs://rosinality-tpu-bucket/the_stack_dedup/1.0.0
I0512 23:45:05.188784 140260698863616 dataset_providers.py:1597] Sharding at the data source: 4 of 8
I0512 23:45:05.220064 140193652815872 dataset_info.py:540] Load dataset info from gs://rosinality-tpu-bucket/the_stack_dedup/1.0.0
I0512 23:45:05.234442 139666521167872 logging_logger.py:49] Constructing tf.data.Dataset the_stack_dedup for split train, from gs://rosinality-tpu-bucket/the_stack_dedup/1.0.0
I0512 23:45:05.246347 139856202532864 dataset_builder.py:421] Reusing dataset the_stack_dedup (gs://rosinality-tpu-bucket/the_stack_dedup/1.0.0)
I0512 23:45:05.228262 140643590531072 logging_logger.py:49] Constructing tf.data.Dataset the_stack_dedup for split train, from gs://rosinality-tpu-bucket/the_stack_dedup/1.0.0
I0512 23:45:05.269479 140260698863616 dataset_info.py:540] Load dataset info from gs://rosinality-tpu-bucket/the_stack_dedup/1.0.0
I0512 23:45:05.320378 140180528838656 dataset_builder.py:421] Reusing dataset the_stack_dedup (gs://rosinality-tpu-bucket/the_stack_dedup/1.0.0)
I0512 23:45:05.359457 139856202532864 logging_logger.py:49] Constructing tf.data.Dataset the_stack_dedup for split train, from gs://rosinality-tpu-bucket/the_stack_dedup/1.0.0
I0512 23:45:05.437827 140180528838656 logging_logger.py:49] Constructing tf.data.Dataset the_stack_dedup for split train, from gs://rosinality-tpu-bucket/the_stack_dedup/1.0.0
I0512 23:45:05.540862 140193652815872 dataset_builder.py:421] Reusing dataset the_stack_dedup (gs://rosinality-tpu-bucket/the_stack_dedup/1.0.0)
I0512 23:45:05.577862 140260698863616 dataset_builder.py:421] Reusing dataset the_stack_dedup (gs://rosinality-tpu-bucket/the_stack_dedup/1.0.0)
I0512 23:45:05.656152 140193652815872 logging_logger.py:49] Constructing tf.data.Dataset the_stack_dedup for split train, from gs://rosinality-tpu-bucket/the_stack_dedup/1.0.0
I0512 23:45:05.693345 140260698863616 logging_logger.py:49] Constructing tf.data.Dataset the_stack_dedup for split train, from gs://rosinality-tpu-bucket/the_stack_dedup/1.0.0
I0512 23:45:06.489300 140143678494720 feature_converters.py:1406] prefix <_DirectedInterleaveDataset element_spec={'inputs': TensorSpec(shape=(None,), dtype=tf.int32, name=None), 'targets': TensorSpec(shape=(None,), dtype=tf.int32, name=None)}> {'inputs': 1024, 'targets': 1024}
I0512 23:45:06.654647 139876724979712 feature_converters.py:1406] prefix <_DirectedInterleaveDataset element_spec={'inputs': TensorSpec(shape=(None,), dtype=tf.int32, name=None), 'targets': TensorSpec(shape=(None,), dtype=tf.int32, name=None)}> {'inputs': 1024, 'targets': 1024}
I0512 23:45:06.649634 140643590531072 feature_converters.py:1406] prefix <_DirectedInterleaveDataset element_spec={'inputs': TensorSpec(shape=(None,), dtype=tf.int32, name=None), 'targets': TensorSpec(shape=(None,), dtype=tf.int32, name=None)}> {'inputs': 1024, 'targets': 1024}
I0512 23:45:06.682070 139666521167872 feature_converters.py:1406] prefix <_DirectedInterleaveDataset element_spec={'inputs': TensorSpec(shape=(None,), dtype=tf.int32, name=None), 'targets': TensorSpec(shape=(None,), dtype=tf.int32, name=None)}> {'inputs': 1024, 'targets': 1024}
I0512 23:45:06.870756 139856202532864 feature_converters.py:1406] prefix <_DirectedInterleaveDataset element_spec={'inputs': TensorSpec(shape=(None,), dtype=tf.int32, name=None), 'targets': TensorSpec(shape=(None,), dtype=tf.int32, name=None)}> {'inputs': 1024, 'targets': 1024}
I0512 23:45:06.982576 140180528838656 feature_converters.py:1406] prefix <_DirectedInterleaveDataset element_spec={'inputs': TensorSpec(shape=(None,), dtype=tf.int32, name=None), 'targets': TensorSpec(shape=(None,), dtype=tf.int32, name=None)}> {'inputs': 1024, 'targets': 1024}
I0512 23:45:07.170845 140193652815872 feature_converters.py:1406] prefix <_DirectedInterleaveDataset element_spec={'inputs': TensorSpec(shape=(None,), dtype=tf.int32, name=None), 'targets': TensorSpec(shape=(None,), dtype=tf.int32, name=None)}> {'inputs': 1024, 'targets': 1024}
I0512 23:45:07.242009 140260698863616 feature_converters.py:1406] prefix <_DirectedInterleaveDataset element_spec={'inputs': TensorSpec(shape=(None,), dtype=tf.int32, name=None), 'targets': TensorSpec(shape=(None,), dtype=tf.int32, name=None)}> {'inputs': 1024, 'targets': 1024}
I0512 23:45:08.283915 140143678494720 dataset_providers.py:2432] The output dataset from seqio.get_dataset has the following features
I0512 23:45:08.284070 140143678494720 dataset_providers.py:2438] feature: decoder_target_tokens 	 shape: [48, 2048] 	 dtype: int32
I0512 23:45:08.284124 140143678494720 dataset_providers.py:2438] feature: decoder_input_tokens 	 shape: [48, 2048] 	 dtype: int32
I0512 23:45:08.284166 140143678494720 dataset_providers.py:2438] feature: decoder_loss_weights 	 shape: [48, 2048] 	 dtype: int32
I0512 23:45:08.284204 140143678494720 dataset_providers.py:2438] feature: decoder_segment_ids 	 shape: [48, 2048] 	 dtype: int32
I0512 23:45:08.284242 140143678494720 dataset_providers.py:2438] feature: decoder_positions 	 shape: [48, 2048] 	 dtype: int32
I0512 23:45:08.284289 140143678494720 dataset_providers.py:2438] feature: decoder_causal_attention 	 shape: [48, 2048] 	 dtype: int32
I0512 23:45:08.412768 140643590531072 dataset_providers.py:2432] The output dataset from seqio.get_dataset has the following features
I0512 23:45:08.412949 140643590531072 dataset_providers.py:2438] feature: decoder_target_tokens 	 shape: [48, 2048] 	 dtype: int32
I0512 23:45:08.413002 140643590531072 dataset_providers.py:2438] feature: decoder_input_tokens 	 shape: [48, 2048] 	 dtype: int32
I0512 23:45:08.413039 140643590531072 dataset_providers.py:2438] feature: decoder_loss_weights 	 shape: [48, 2048] 	 dtype: int32
I0512 23:45:08.413076 140643590531072 dataset_providers.py:2438] feature: decoder_segment_ids 	 shape: [48, 2048] 	 dtype: int32
I0512 23:45:08.413111 140643590531072 dataset_providers.py:2438] feature: decoder_positions 	 shape: [48, 2048] 	 dtype: int32
I0512 23:45:08.430897 139876724979712 dataset_providers.py:2432] The output dataset from seqio.get_dataset has the following features
I0512 23:45:08.431117 139876724979712 dataset_providers.py:2438] feature: decoder_target_tokens 	 shape: [48, 2048] 	 dtype: int32
I0512 23:45:08.431172 139876724979712 dataset_providers.py:2438] feature: decoder_input_tokens 	 shape: [48, 2048] 	 dtype: int32
I0512 23:45:08.431211 139876724979712 dataset_providers.py:2438] feature: decoder_loss_weights 	 shape: [48, 2048] 	 dtype: int32
I0512 23:45:08.431246 139876724979712 dataset_providers.py:2438] feature: decoder_segment_ids 	 shape: [48, 2048] 	 dtype: int32
I0512 23:45:08.431281 139876724979712 dataset_providers.py:2438] feature: decoder_positions 	 shape: [48, 2048] 	 dtype: int32
I0512 23:45:08.413147 140643590531072 dataset_providers.py:2438] feature: decoder_causal_attention 	 shape: [48, 2048] 	 dtype: int32
I0512 23:45:08.431319 139876724979712 dataset_providers.py:2438] feature: decoder_causal_attention 	 shape: [48, 2048] 	 dtype: int32
I0512 23:45:08.458501 139666521167872 dataset_providers.py:2432] The output dataset from seqio.get_dataset has the following features
I0512 23:45:08.458681 139666521167872 dataset_providers.py:2438] feature: decoder_target_tokens 	 shape: [48, 2048] 	 dtype: int32
I0512 23:45:08.458732 139666521167872 dataset_providers.py:2438] feature: decoder_input_tokens 	 shape: [48, 2048] 	 dtype: int32
I0512 23:45:08.458770 139666521167872 dataset_providers.py:2438] feature: decoder_loss_weights 	 shape: [48, 2048] 	 dtype: int32
I0512 23:45:08.458808 139666521167872 dataset_providers.py:2438] feature: decoder_segment_ids 	 shape: [48, 2048] 	 dtype: int32
I0512 23:45:08.458848 139666521167872 dataset_providers.py:2438] feature: decoder_positions 	 shape: [48, 2048] 	 dtype: int32
I0512 23:45:08.458891 139666521167872 dataset_providers.py:2438] feature: decoder_causal_attention 	 shape: [48, 2048] 	 dtype: int32
I0512 23:45:08.708557 139856202532864 dataset_providers.py:2432] The output dataset from seqio.get_dataset has the following features
I0512 23:45:08.708821 139856202532864 dataset_providers.py:2438] feature: decoder_target_tokens 	 shape: [48, 2048] 	 dtype: int32
I0512 23:45:08.708880 139856202532864 dataset_providers.py:2438] feature: decoder_input_tokens 	 shape: [48, 2048] 	 dtype: int32
I0512 23:45:08.708919 139856202532864 dataset_providers.py:2438] feature: decoder_loss_weights 	 shape: [48, 2048] 	 dtype: int32
I0512 23:45:08.708955 139856202532864 dataset_providers.py:2438] feature: decoder_segment_ids 	 shape: [48, 2048] 	 dtype: int32
I0512 23:45:08.708989 139856202532864 dataset_providers.py:2438] feature: decoder_positions 	 shape: [48, 2048] 	 dtype: int32
I0512 23:45:08.709023 139856202532864 dataset_providers.py:2438] feature: decoder_causal_attention 	 shape: [48, 2048] 	 dtype: int32
I0512 23:45:08.827921 140180528838656 dataset_providers.py:2432] The output dataset from seqio.get_dataset has the following features
I0512 23:45:08.828197 140180528838656 dataset_providers.py:2438] feature: decoder_target_tokens 	 shape: [48, 2048] 	 dtype: int32
I0512 23:45:08.828265 140180528838656 dataset_providers.py:2438] feature: decoder_input_tokens 	 shape: [48, 2048] 	 dtype: int32
I0512 23:45:08.828309 140180528838656 dataset_providers.py:2438] feature: decoder_loss_weights 	 shape: [48, 2048] 	 dtype: int32
I0512 23:45:08.828346 140180528838656 dataset_providers.py:2438] feature: decoder_segment_ids 	 shape: [48, 2048] 	 dtype: int32
I0512 23:45:08.828382 140180528838656 dataset_providers.py:2438] feature: decoder_positions 	 shape: [48, 2048] 	 dtype: int32
I0512 23:45:08.828417 140180528838656 dataset_providers.py:2438] feature: decoder_causal_attention 	 shape: [48, 2048] 	 dtype: int32
I0512 23:45:09.029179 140193652815872 dataset_providers.py:2432] The output dataset from seqio.get_dataset has the following features
I0512 23:45:09.029390 140193652815872 dataset_providers.py:2438] feature: decoder_target_tokens 	 shape: [48, 2048] 	 dtype: int32
I0512 23:45:09.029442 140193652815872 dataset_providers.py:2438] feature: decoder_input_tokens 	 shape: [48, 2048] 	 dtype: int32
I0512 23:45:09.029480 140193652815872 dataset_providers.py:2438] feature: decoder_loss_weights 	 shape: [48, 2048] 	 dtype: int32
I0512 23:45:09.029520 140193652815872 dataset_providers.py:2438] feature: decoder_segment_ids 	 shape: [48, 2048] 	 dtype: int32
I0512 23:45:09.029557 140193652815872 dataset_providers.py:2438] feature: decoder_positions 	 shape: [48, 2048] 	 dtype: int32
I0512 23:45:09.029592 140193652815872 dataset_providers.py:2438] feature: decoder_causal_attention 	 shape: [48, 2048] 	 dtype: int32
I0512 23:45:09.132059 140260698863616 dataset_providers.py:2432] The output dataset from seqio.get_dataset has the following features
I0512 23:45:09.132275 140260698863616 dataset_providers.py:2438] feature: decoder_target_tokens 	 shape: [48, 2048] 	 dtype: int32
I0512 23:45:09.132330 140260698863616 dataset_providers.py:2438] feature: decoder_input_tokens 	 shape: [48, 2048] 	 dtype: int32
I0512 23:45:09.132370 140260698863616 dataset_providers.py:2438] feature: decoder_loss_weights 	 shape: [48, 2048] 	 dtype: int32
I0512 23:45:09.132405 140260698863616 dataset_providers.py:2438] feature: decoder_segment_ids 	 shape: [48, 2048] 	 dtype: int32
I0512 23:45:09.132452 140260698863616 dataset_providers.py:2438] feature: decoder_positions 	 shape: [48, 2048] 	 dtype: int32
I0512 23:45:09.132488 140260698863616 dataset_providers.py:2438] feature: decoder_causal_attention 	 shape: [48, 2048] 	 dtype: int32
W0512 23:45:10.156104 140143678494720 activation_partitioning.py:145] In /home/rosinality/t5x/flaxformer/flaxformer/architectures/t5/t5_architecture.py:520, activation_partitioning_dims was set, but it is deprecated and will be removed soon.
W0512 23:45:10.163416 140143678494720 activation_partitioning.py:145] In /home/rosinality/t5x/flaxformer/flaxformer/architectures/t5/t5_architecture.py:586, activation_partitioning_dims was set, but it is deprecated and will be removed soon.
W0512 23:45:10.214437 140143678494720 activation_partitioning.py:145] In /home/rosinality/t5x/flaxformer/flaxformer/architectures/t5/t5_architecture.py:623, activation_partitioning_dims was set, but it is deprecated and will be removed soon.
W0512 23:45:10.217723 140143678494720 activation_partitioning.py:145] In /home/rosinality/t5x/flaxformer/flaxformer/architectures/t5/t5_architecture.py:663, activation_partitioning_dims was set, but it is deprecated and will be removed soon.
W0512 23:45:10.262963 140143678494720 activation_partitioning.py:145] In /home/rosinality/t5x/flaxformer/flaxformer/architectures/t5/t5_architecture.py:678, activation_partitioning_dims was set, but it is deprecated and will be removed soon.
W0512 23:45:10.267296 140143678494720 activation_partitioning.py:145] In /home/rosinality/t5x/flaxformer/flaxformer/architectures/t5/t5_architecture.py:520, activation_partitioning_dims was set, but it is deprecated and will be removed soon.
W0512 23:45:10.270495 140143678494720 activation_partitioning.py:145] In /home/rosinality/t5x/flaxformer/flaxformer/architectures/t5/t5_architecture.py:586, activation_partitioning_dims was set, but it is deprecated and will be removed soon.
W0512 23:45:10.292448 140143678494720 activation_partitioning.py:145] In /home/rosinality/t5x/flaxformer/flaxformer/architectures/t5/t5_architecture.py:623, activation_partitioning_dims was set, but it is deprecated and will be removed soon.
W0512 23:45:10.295623 140143678494720 activation_partitioning.py:145] In /home/rosinality/t5x/flaxformer/flaxformer/architectures/t5/t5_architecture.py:663, activation_partitioning_dims was set, but it is deprecated and will be removed soon.
W0512 23:45:10.308813 140143678494720 activation_partitioning.py:145] In /home/rosinality/t5x/flaxformer/flaxformer/architectures/t5/t5_architecture.py:678, activation_partitioning_dims was set, but it is deprecated and will be removed soon.
W0512 23:45:10.312780 140643590531072 activation_partitioning.py:145] In /home/rosinality/t5x/flaxformer/flaxformer/architectures/t5/t5_architecture.py:520, activation_partitioning_dims was set, but it is deprecated and will be removed soon.
W0512 23:45:10.319927 140643590531072 activation_partitioning.py:145] In /home/rosinality/t5x/flaxformer/flaxformer/architectures/t5/t5_architecture.py:586, activation_partitioning_dims was set, but it is deprecated and will be removed soon.
W0512 23:45:10.371136 140643590531072 activation_partitioning.py:145] In /home/rosinality/t5x/flaxformer/flaxformer/architectures/t5/t5_architecture.py:623, activation_partitioning_dims was set, but it is deprecated and will be removed soon.
W0512 23:45:10.374401 140643590531072 activation_partitioning.py:145] In /home/rosinality/t5x/flaxformer/flaxformer/architectures/t5/t5_architecture.py:663, activation_partitioning_dims was set, but it is deprecated and will be removed soon.
W0512 23:45:10.395586 139876724979712 activation_partitioning.py:145] In /home/rosinality/t5x/flaxformer/flaxformer/architectures/t5/t5_architecture.py:520, activation_partitioning_dims was set, but it is deprecated and will be removed soon.
W0512 23:45:10.400484 139666521167872 activation_partitioning.py:145] In /home/rosinality/t5x/flaxformer/flaxformer/architectures/t5/t5_architecture.py:520, activation_partitioning_dims was set, but it is deprecated and will be removed soon.
W0512 23:45:10.402815 139876724979712 activation_partitioning.py:145] In /home/rosinality/t5x/flaxformer/flaxformer/architectures/t5/t5_architecture.py:586, activation_partitioning_dims was set, but it is deprecated and will be removed soon.
W0512 23:45:10.407726 139666521167872 activation_partitioning.py:145] In /home/rosinality/t5x/flaxformer/flaxformer/architectures/t5/t5_architecture.py:586, activation_partitioning_dims was set, but it is deprecated and will be removed soon.
W0512 23:45:10.419744 140643590531072 activation_partitioning.py:145] In /home/rosinality/t5x/flaxformer/flaxformer/architectures/t5/t5_architecture.py:678, activation_partitioning_dims was set, but it is deprecated and will be removed soon.
W0512 23:45:10.424020 140643590531072 activation_partitioning.py:145] In /home/rosinality/t5x/flaxformer/flaxformer/architectures/t5/t5_architecture.py:520, activation_partitioning_dims was set, but it is deprecated and will be removed soon.
W0512 23:45:10.427188 140643590531072 activation_partitioning.py:145] In /home/rosinality/t5x/flaxformer/flaxformer/architectures/t5/t5_architecture.py:586, activation_partitioning_dims was set, but it is deprecated and will be removed soon.
W0512 23:45:10.454675 139876724979712 activation_partitioning.py:145] In /home/rosinality/t5x/flaxformer/flaxformer/architectures/t5/t5_architecture.py:623, activation_partitioning_dims was set, but it is deprecated and will be removed soon.
W0512 23:45:10.458003 139876724979712 activation_partitioning.py:145] In /home/rosinality/t5x/flaxformer/flaxformer/architectures/t5/t5_architecture.py:663, activation_partitioning_dims was set, but it is deprecated and will be removed soon.
W0512 23:45:10.459565 139666521167872 activation_partitioning.py:145] In /home/rosinality/t5x/flaxformer/flaxformer/architectures/t5/t5_architecture.py:623, activation_partitioning_dims was set, but it is deprecated and will be removed soon.
W0512 23:45:10.462883 139666521167872 activation_partitioning.py:145] In /home/rosinality/t5x/flaxformer/flaxformer/architectures/t5/t5_architecture.py:663, activation_partitioning_dims was set, but it is deprecated and will be removed soon.
W0512 23:45:10.449147 140643590531072 activation_partitioning.py:145] In /home/rosinality/t5x/flaxformer/flaxformer/architectures/t5/t5_architecture.py:623, activation_partitioning_dims was set, but it is deprecated and will be removed soon.
W0512 23:45:10.452290 140643590531072 activation_partitioning.py:145] In /home/rosinality/t5x/flaxformer/flaxformer/architectures/t5/t5_architecture.py:663, activation_partitioning_dims was set, but it is deprecated and will be removed soon.
I0512 23:45:10.479705 140143678494720 moe_layers.py:777] Selected group_size=4096 and num_groups=192 for input num_tokens=786432, max_group_size=4096, num_experts=8 and num_expert_replicas=1
W0512 23:45:10.465437 140643590531072 activation_partitioning.py:145] In /home/rosinality/t5x/flaxformer/flaxformer/architectures/t5/t5_architecture.py:678, activation_partitioning_dims was set, but it is deprecated and will be removed soon.
W0512 23:45:10.504108 139876724979712 activation_partitioning.py:145] In /home/rosinality/t5x/flaxformer/flaxformer/architectures/t5/t5_architecture.py:678, activation_partitioning_dims was set, but it is deprecated and will be removed soon.
W0512 23:45:10.508485 139876724979712 activation_partitioning.py:145] In /home/rosinality/t5x/flaxformer/flaxformer/architectures/t5/t5_architecture.py:520, activation_partitioning_dims was set, but it is deprecated and will be removed soon.
W0512 23:45:10.509098 139666521167872 activation_partitioning.py:145] In /home/rosinality/t5x/flaxformer/flaxformer/architectures/t5/t5_architecture.py:678, activation_partitioning_dims was set, but it is deprecated and will be removed soon.
W0512 23:45:10.511755 139876724979712 activation_partitioning.py:145] In /home/rosinality/t5x/flaxformer/flaxformer/architectures/t5/t5_architecture.py:586, activation_partitioning_dims was set, but it is deprecated and will be removed soon.
W0512 23:45:10.513494 139666521167872 activation_partitioning.py:145] In /home/rosinality/t5x/flaxformer/flaxformer/architectures/t5/t5_architecture.py:520, activation_partitioning_dims was set, but it is deprecated and will be removed soon.
W0512 23:45:10.516733 139666521167872 activation_partitioning.py:145] In /home/rosinality/t5x/flaxformer/flaxformer/architectures/t5/t5_architecture.py:586, activation_partitioning_dims was set, but it is deprecated and will be removed soon.
W0512 23:45:10.534060 139876724979712 activation_partitioning.py:145] In /home/rosinality/t5x/flaxformer/flaxformer/architectures/t5/t5_architecture.py:623, activation_partitioning_dims was set, but it is deprecated and will be removed soon.
W0512 23:45:10.537254 139876724979712 activation_partitioning.py:145] In /home/rosinality/t5x/flaxformer/flaxformer/architectures/t5/t5_architecture.py:663, activation_partitioning_dims was set, but it is deprecated and will be removed soon.
W0512 23:45:10.539006 139666521167872 activation_partitioning.py:145] In /home/rosinality/t5x/flaxformer/flaxformer/architectures/t5/t5_architecture.py:623, activation_partitioning_dims was set, but it is deprecated and will be removed soon.
W0512 23:45:10.542176 139666521167872 activation_partitioning.py:145] In /home/rosinality/t5x/flaxformer/flaxformer/architectures/t5/t5_architecture.py:663, activation_partitioning_dims was set, but it is deprecated and will be removed soon.
W0512 23:45:10.550674 139876724979712 activation_partitioning.py:145] In /home/rosinality/t5x/flaxformer/flaxformer/architectures/t5/t5_architecture.py:678, activation_partitioning_dims was set, but it is deprecated and will be removed soon.
W0512 23:45:10.555600 139666521167872 activation_partitioning.py:145] In /home/rosinality/t5x/flaxformer/flaxformer/architectures/t5/t5_architecture.py:678, activation_partitioning_dims was set, but it is deprecated and will be removed soon.
I0512 23:45:10.636502 140643590531072 moe_layers.py:777] Selected group_size=4096 and num_groups=192 for input num_tokens=786432, max_group_size=4096, num_experts=8 and num_expert_replicas=1
W0512 23:45:10.710356 139856202532864 activation_partitioning.py:145] In /home/rosinality/t5x/flaxformer/flaxformer/architectures/t5/t5_architecture.py:520, activation_partitioning_dims was set, but it is deprecated and will be removed soon.
W0512 23:45:10.717686 139856202532864 activation_partitioning.py:145] In /home/rosinality/t5x/flaxformer/flaxformer/architectures/t5/t5_architecture.py:586, activation_partitioning_dims was set, but it is deprecated and will be removed soon.
I0512 23:45:10.724429 139876724979712 moe_layers.py:777] Selected group_size=4096 and num_groups=192 for input num_tokens=786432, max_group_size=4096, num_experts=8 and num_expert_replicas=1
I0512 23:45:10.728415 139666521167872 moe_layers.py:777] Selected group_size=4096 and num_groups=192 for input num_tokens=786432, max_group_size=4096, num_experts=8 and num_expert_replicas=1
W0512 23:45:10.770332 139856202532864 activation_partitioning.py:145] In /home/rosinality/t5x/flaxformer/flaxformer/architectures/t5/t5_architecture.py:623, activation_partitioning_dims was set, but it is deprecated and will be removed soon.
W0512 23:45:10.773658 139856202532864 activation_partitioning.py:145] In /home/rosinality/t5x/flaxformer/flaxformer/architectures/t5/t5_architecture.py:663, activation_partitioning_dims was set, but it is deprecated and will be removed soon.
W0512 23:45:10.820003 139856202532864 activation_partitioning.py:145] In /home/rosinality/t5x/flaxformer/flaxformer/architectures/t5/t5_architecture.py:678, activation_partitioning_dims was set, but it is deprecated and will be removed soon.
W0512 23:45:10.824407 139856202532864 activation_partitioning.py:145] In /home/rosinality/t5x/flaxformer/flaxformer/architectures/t5/t5_architecture.py:520, activation_partitioning_dims was set, but it is deprecated and will be removed soon.
W0512 23:45:10.827689 139856202532864 activation_partitioning.py:145] In /home/rosinality/t5x/flaxformer/flaxformer/architectures/t5/t5_architecture.py:586, activation_partitioning_dims was set, but it is deprecated and will be removed soon.
W0512 23:45:10.850201 139856202532864 activation_partitioning.py:145] In /home/rosinality/t5x/flaxformer/flaxformer/architectures/t5/t5_architecture.py:623, activation_partitioning_dims was set, but it is deprecated and will be removed soon.
W0512 23:45:10.853375 139856202532864 activation_partitioning.py:145] In /home/rosinality/t5x/flaxformer/flaxformer/architectures/t5/t5_architecture.py:663, activation_partitioning_dims was set, but it is deprecated and will be removed soon.
W0512 23:45:10.836952 140180528838656 activation_partitioning.py:145] In /home/rosinality/t5x/flaxformer/flaxformer/architectures/t5/t5_architecture.py:520, activation_partitioning_dims was set, but it is deprecated and will be removed soon.
W0512 23:45:10.844208 140180528838656 activation_partitioning.py:145] In /home/rosinality/t5x/flaxformer/flaxformer/architectures/t5/t5_architecture.py:586, activation_partitioning_dims was set, but it is deprecated and will be removed soon.
W0512 23:45:10.866859 139856202532864 activation_partitioning.py:145] In /home/rosinality/t5x/flaxformer/flaxformer/architectures/t5/t5_architecture.py:678, activation_partitioning_dims was set, but it is deprecated and will be removed soon.
W0512 23:45:10.895867 140180528838656 activation_partitioning.py:145] In /home/rosinality/t5x/flaxformer/flaxformer/architectures/t5/t5_architecture.py:623, activation_partitioning_dims was set, but it is deprecated and will be removed soon.
W0512 23:45:10.899173 140180528838656 activation_partitioning.py:145] In /home/rosinality/t5x/flaxformer/flaxformer/architectures/t5/t5_architecture.py:663, activation_partitioning_dims was set, but it is deprecated and will be removed soon.
W0512 23:45:10.945067 140180528838656 activation_partitioning.py:145] In /home/rosinality/t5x/flaxformer/flaxformer/architectures/t5/t5_architecture.py:678, activation_partitioning_dims was set, but it is deprecated and will be removed soon.
W0512 23:45:10.949363 140180528838656 activation_partitioning.py:145] In /home/rosinality/t5x/flaxformer/flaxformer/architectures/t5/t5_architecture.py:520, activation_partitioning_dims was set, but it is deprecated and will be removed soon.
W0512 23:45:10.952545 140180528838656 activation_partitioning.py:145] In /home/rosinality/t5x/flaxformer/flaxformer/architectures/t5/t5_architecture.py:586, activation_partitioning_dims was set, but it is deprecated and will be removed soon.
W0512 23:45:10.981613 140193652815872 activation_partitioning.py:145] In /home/rosinality/t5x/flaxformer/flaxformer/architectures/t5/t5_architecture.py:520, activation_partitioning_dims was set, but it is deprecated and will be removed soon.
W0512 23:45:10.989391 140193652815872 activation_partitioning.py:145] In /home/rosinality/t5x/flaxformer/flaxformer/architectures/t5/t5_architecture.py:586, activation_partitioning_dims was set, but it is deprecated and will be removed soon.
W0512 23:45:10.974626 140180528838656 activation_partitioning.py:145] In /home/rosinality/t5x/flaxformer/flaxformer/architectures/t5/t5_architecture.py:623, activation_partitioning_dims was set, but it is deprecated and will be removed soon.
W0512 23:45:10.977785 140180528838656 activation_partitioning.py:145] In /home/rosinality/t5x/flaxformer/flaxformer/architectures/t5/t5_architecture.py:663, activation_partitioning_dims was set, but it is deprecated and will be removed soon.
W0512 23:45:10.991022 140180528838656 activation_partitioning.py:145] In /home/rosinality/t5x/flaxformer/flaxformer/architectures/t5/t5_architecture.py:678, activation_partitioning_dims was set, but it is deprecated and will be removed soon.
I0512 23:45:11.041828 139856202532864 moe_layers.py:777] Selected group_size=4096 and num_groups=192 for input num_tokens=786432, max_group_size=4096, num_experts=8 and num_expert_replicas=1
W0512 23:45:11.041859 140193652815872 activation_partitioning.py:145] In /home/rosinality/t5x/flaxformer/flaxformer/architectures/t5/t5_architecture.py:623, activation_partitioning_dims was set, but it is deprecated and will be removed soon.
W0512 23:45:11.045328 140193652815872 activation_partitioning.py:145] In /home/rosinality/t5x/flaxformer/flaxformer/architectures/t5/t5_architecture.py:663, activation_partitioning_dims was set, but it is deprecated and will be removed soon.
W0512 23:45:11.091332 140193652815872 activation_partitioning.py:145] In /home/rosinality/t5x/flaxformer/flaxformer/architectures/t5/t5_architecture.py:678, activation_partitioning_dims was set, but it is deprecated and will be removed soon.
W0512 23:45:11.095704 140193652815872 activation_partitioning.py:145] In /home/rosinality/t5x/flaxformer/flaxformer/architectures/t5/t5_architecture.py:520, activation_partitioning_dims was set, but it is deprecated and will be removed soon.
W0512 23:45:11.098933 140193652815872 activation_partitioning.py:145] In /home/rosinality/t5x/flaxformer/flaxformer/architectures/t5/t5_architecture.py:586, activation_partitioning_dims was set, but it is deprecated and will be removed soon.
W0512 23:45:11.121133 140193652815872 activation_partitioning.py:145] In /home/rosinality/t5x/flaxformer/flaxformer/architectures/t5/t5_architecture.py:623, activation_partitioning_dims was set, but it is deprecated and will be removed soon.
W0512 23:45:11.124273 140193652815872 activation_partitioning.py:145] In /home/rosinality/t5x/flaxformer/flaxformer/architectures/t5/t5_architecture.py:663, activation_partitioning_dims was set, but it is deprecated and will be removed soon.
I0512 23:45:11.128056 140143678494720 moe_layers.py:777] Selected group_size=4096 and num_groups=192 for input num_tokens=786432, max_group_size=4096, num_experts=8 and num_expert_replicas=1
W0512 23:45:11.137506 140193652815872 activation_partitioning.py:145] In /home/rosinality/t5x/flaxformer/flaxformer/architectures/t5/t5_architecture.py:678, activation_partitioning_dims was set, but it is deprecated and will be removed soon.
W0512 23:45:11.159780 140260698863616 activation_partitioning.py:145] In /home/rosinality/t5x/flaxformer/flaxformer/architectures/t5/t5_architecture.py:520, activation_partitioning_dims was set, but it is deprecated and will be removed soon.
I0512 23:45:11.162861 140180528838656 moe_layers.py:777] Selected group_size=4096 and num_groups=192 for input num_tokens=786432, max_group_size=4096, num_experts=8 and num_expert_replicas=1
W0512 23:45:11.167240 140260698863616 activation_partitioning.py:145] In /home/rosinality/t5x/flaxformer/flaxformer/architectures/t5/t5_architecture.py:586, activation_partitioning_dims was set, but it is deprecated and will be removed soon.
W0512 23:45:11.220258 140260698863616 activation_partitioning.py:145] In /home/rosinality/t5x/flaxformer/flaxformer/architectures/t5/t5_architecture.py:623, activation_partitioning_dims was set, but it is deprecated and will be removed soon.
W0512 23:45:11.223691 140260698863616 activation_partitioning.py:145] In /home/rosinality/t5x/flaxformer/flaxformer/architectures/t5/t5_architecture.py:663, activation_partitioning_dims was set, but it is deprecated and will be removed soon.
W0512 23:45:11.271250 140260698863616 activation_partitioning.py:145] In /home/rosinality/t5x/flaxformer/flaxformer/architectures/t5/t5_architecture.py:678, activation_partitioning_dims was set, but it is deprecated and will be removed soon.
W0512 23:45:11.275772 140260698863616 activation_partitioning.py:145] In /home/rosinality/t5x/flaxformer/flaxformer/architectures/t5/t5_architecture.py:520, activation_partitioning_dims was set, but it is deprecated and will be removed soon.
W0512 23:45:11.279076 140260698863616 activation_partitioning.py:145] In /home/rosinality/t5x/flaxformer/flaxformer/architectures/t5/t5_architecture.py:586, activation_partitioning_dims was set, but it is deprecated and will be removed soon.
I0512 23:45:11.310601 140193652815872 moe_layers.py:777] Selected group_size=4096 and num_groups=192 for input num_tokens=786432, max_group_size=4096, num_experts=8 and num_expert_replicas=1
W0512 23:45:11.303027 140260698863616 activation_partitioning.py:145] In /home/rosinality/t5x/flaxformer/flaxformer/architectures/t5/t5_architecture.py:623, activation_partitioning_dims was set, but it is deprecated and will be removed soon.
W0512 23:45:11.306390 140260698863616 activation_partitioning.py:145] In /home/rosinality/t5x/flaxformer/flaxformer/architectures/t5/t5_architecture.py:663, activation_partitioning_dims was set, but it is deprecated and will be removed soon.
I0512 23:45:11.309109 140643590531072 moe_layers.py:777] Selected group_size=4096 and num_groups=192 for input num_tokens=786432, max_group_size=4096, num_experts=8 and num_expert_replicas=1
W0512 23:45:11.320164 140260698863616 activation_partitioning.py:145] In /home/rosinality/t5x/flaxformer/flaxformer/architectures/t5/t5_architecture.py:678, activation_partitioning_dims was set, but it is deprecated and will be removed soon.
I0512 23:45:11.410415 139876724979712 moe_layers.py:777] Selected group_size=4096 and num_groups=192 for input num_tokens=786432, max_group_size=4096, num_experts=8 and num_expert_replicas=1
I0512 23:45:11.414438 139666521167872 moe_layers.py:777] Selected group_size=4096 and num_groups=192 for input num_tokens=786432, max_group_size=4096, num_experts=8 and num_expert_replicas=1
I0512 23:45:11.447642 140143678494720 moe_layers.py:777] Selected group_size=4096 and num_groups=192 for input num_tokens=786432, max_group_size=4096, num_experts=8 and num_expert_replicas=1
I0512 23:45:11.496805 140260698863616 moe_layers.py:777] Selected group_size=4096 and num_groups=192 for input num_tokens=786432, max_group_size=4096, num_experts=8 and num_expert_replicas=1
I0512 23:45:11.626632 140643590531072 moe_layers.py:777] Selected group_size=4096 and num_groups=192 for input num_tokens=786432, max_group_size=4096, num_experts=8 and num_expert_replicas=1
I0512 23:45:11.736059 139876724979712 moe_layers.py:777] Selected group_size=4096 and num_groups=192 for input num_tokens=786432, max_group_size=4096, num_experts=8 and num_expert_replicas=1
I0512 23:45:11.737822 139666521167872 moe_layers.py:777] Selected group_size=4096 and num_groups=192 for input num_tokens=786432, max_group_size=4096, num_experts=8 and num_expert_replicas=1
I0512 23:45:11.767570 140143678494720 moe_layers.py:777] Selected group_size=4096 and num_groups=192 for input num_tokens=786432, max_group_size=4096, num_experts=8 and num_expert_replicas=1
I0512 23:45:11.777515 139856202532864 moe_layers.py:777] Selected group_size=4096 and num_groups=192 for input num_tokens=786432, max_group_size=4096, num_experts=8 and num_expert_replicas=1
W0512 23:45:11.865373 140143678494720 adafactor.py:358] Since rank of parameter decoder/decoder_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:11.866397 140143678494720 adafactor.py:358] Since rank of parameter decoder/layers_0/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:11.867564 140143678494720 adafactor.py:358] Since rank of parameter decoder/layers_0/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:11.868733 140143678494720 adafactor.py:358] Since rank of parameter decoder/layers_0/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:11.869961 140143678494720 adafactor.py:358] Since rank of parameter decoder/layers_0/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:11.870903 140143678494720 adafactor.py:358] Since rank of parameter decoder/layers_0/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:11.871860 140143678494720 adafactor.py:358] Since rank of parameter decoder/layers_0/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:11.872977 140143678494720 adafactor.py:358] Since rank of parameter decoder/layers_0/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:11.874069 140143678494720 adafactor.py:358] Since rank of parameter decoder/layers_0/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:11.875247 140143678494720 adafactor.py:358] Since rank of parameter decoder/layers_0/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:11.876323 140143678494720 adafactor.py:358] Since rank of parameter decoder/layers_1/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:11.877432 140143678494720 adafactor.py:358] Since rank of parameter decoder/layers_1/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:11.878497 140143678494720 adafactor.py:358] Since rank of parameter decoder/layers_1/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:11.879642 140143678494720 adafactor.py:358] Since rank of parameter decoder/layers_1/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:11.880508 140143678494720 adafactor.py:358] Since rank of parameter decoder/layers_1/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:11.881397 140143678494720 adafactor.py:358] Since rank of parameter decoder/layers_1/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:11.882459 140143678494720 adafactor.py:358] Since rank of parameter decoder/layers_1/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:11.883580 140143678494720 adafactor.py:358] Since rank of parameter decoder/layers_1/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:11.884685 140143678494720 adafactor.py:358] Since rank of parameter decoder/layers_1/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:11.885757 140143678494720 adafactor.py:358] Since rank of parameter decoder/layers_10/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:11.886847 140143678494720 adafactor.py:358] Since rank of parameter decoder/layers_10/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:11.887996 140143678494720 adafactor.py:358] Since rank of parameter decoder/layers_10/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:11.889089 140143678494720 adafactor.py:358] Since rank of parameter decoder/layers_10/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:11.889981 140143678494720 adafactor.py:358] Since rank of parameter decoder/layers_10/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:11.890863 140143678494720 adafactor.py:358] Since rank of parameter decoder/layers_10/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:11.891992 140143678494720 adafactor.py:358] Since rank of parameter decoder/layers_10/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:11.893106 140143678494720 adafactor.py:358] Since rank of parameter decoder/layers_10/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:11.894168 140143678494720 adafactor.py:358] Since rank of parameter decoder/layers_10/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:11.895256 140143678494720 adafactor.py:358] Since rank of parameter decoder/layers_11/extra_mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:11.896395 140143678494720 adafactor.py:358] Since rank of parameter decoder/layers_11/extra_mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:11.897491 140143678494720 adafactor.py:358] Since rank of parameter decoder/layers_11/extra_mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:11.901535 140143678494720 adafactor.py:358] Since rank of parameter decoder/layers_11/mlp/router/router_weights/w/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>, <FactorDim.NONE: None>) is ignored.
W0512 23:45:11.902441 140143678494720 adafactor.py:358] Since rank of parameter decoder/layers_11/pre_extra_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:11.903300 140143678494720 adafactor.py:358] Since rank of parameter decoder/layers_11/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:11.904173 140143678494720 adafactor.py:358] Since rank of parameter decoder/layers_11/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:11.905157 140143678494720 adafactor.py:358] Since rank of parameter decoder/layers_11/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:11.906240 140143678494720 adafactor.py:358] Since rank of parameter decoder/layers_11/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:11.907314 140143678494720 adafactor.py:358] Since rank of parameter decoder/layers_11/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:11.908388 140143678494720 adafactor.py:358] Since rank of parameter decoder/layers_11/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:11.909603 140143678494720 adafactor.py:358] Since rank of parameter decoder/layers_12/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:11.910682 140143678494720 adafactor.py:358] Since rank of parameter decoder/layers_12/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
I0512 23:45:11.896192 140180528838656 moe_layers.py:777] Selected group_size=4096 and num_groups=192 for input num_tokens=786432, max_group_size=4096, num_experts=8 and num_expert_replicas=1
W0512 23:45:11.911749 140143678494720 adafactor.py:358] Since rank of parameter decoder/layers_12/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:11.912848 140143678494720 adafactor.py:358] Since rank of parameter decoder/layers_12/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:11.914519 140143678494720 adafactor.py:358] Since rank of parameter decoder/layers_12/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:11.915404 140143678494720 adafactor.py:358] Since rank of parameter decoder/layers_12/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:11.916479 140143678494720 adafactor.py:358] Since rank of parameter decoder/layers_12/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:11.917719 140143678494720 adafactor.py:358] Since rank of parameter decoder/layers_12/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:11.918848 140143678494720 adafactor.py:358] Since rank of parameter decoder/layers_12/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:11.919904 140143678494720 adafactor.py:358] Since rank of parameter decoder/layers_13/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:11.920994 140143678494720 adafactor.py:358] Since rank of parameter decoder/layers_13/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:11.922069 140143678494720 adafactor.py:358] Since rank of parameter decoder/layers_13/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:11.923182 140143678494720 adafactor.py:358] Since rank of parameter decoder/layers_13/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:11.924764 140143678494720 adafactor.py:358] Since rank of parameter decoder/layers_13/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:11.925694 140143678494720 adafactor.py:358] Since rank of parameter decoder/layers_13/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:11.926811 140143678494720 adafactor.py:358] Since rank of parameter decoder/layers_13/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:11.927976 140143678494720 adafactor.py:358] Since rank of parameter decoder/layers_13/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:11.929125 140143678494720 adafactor.py:358] Since rank of parameter decoder/layers_13/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:11.930436 140143678494720 adafactor.py:358] Since rank of parameter decoder/layers_14/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:11.931499 140143678494720 adafactor.py:358] Since rank of parameter decoder/layers_14/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:11.932650 140143678494720 adafactor.py:358] Since rank of parameter decoder/layers_14/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:11.933714 140143678494720 adafactor.py:358] Since rank of parameter decoder/layers_14/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:11.934622 140143678494720 adafactor.py:358] Since rank of parameter decoder/layers_14/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:11.935502 140143678494720 adafactor.py:358] Since rank of parameter decoder/layers_14/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:11.936585 140143678494720 adafactor.py:358] Since rank of parameter decoder/layers_14/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
I0512 23:45:11.943017 140643590531072 moe_layers.py:777] Selected group_size=4096 and num_groups=192 for input num_tokens=786432, max_group_size=4096, num_experts=8 and num_expert_replicas=1
W0512 23:45:11.937704 140143678494720 adafactor.py:358] Since rank of parameter decoder/layers_14/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:11.938775 140143678494720 adafactor.py:358] Since rank of parameter decoder/layers_14/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:11.939841 140143678494720 adafactor.py:358] Since rank of parameter decoder/layers_15/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:11.940925 140143678494720 adafactor.py:358] Since rank of parameter decoder/layers_15/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:11.942048 140143678494720 adafactor.py:358] Since rank of parameter decoder/layers_15/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:11.943105 140143678494720 adafactor.py:358] Since rank of parameter decoder/layers_15/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:11.943977 140143678494720 adafactor.py:358] Since rank of parameter decoder/layers_15/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:11.944869 140143678494720 adafactor.py:358] Since rank of parameter decoder/layers_15/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:11.945988 140143678494720 adafactor.py:358] Since rank of parameter decoder/layers_15/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:11.947047 140143678494720 adafactor.py:358] Since rank of parameter decoder/layers_15/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:11.948125 140143678494720 adafactor.py:358] Since rank of parameter decoder/layers_15/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:11.949221 140143678494720 adafactor.py:358] Since rank of parameter decoder/layers_16/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:11.950350 140143678494720 adafactor.py:358] Since rank of parameter decoder/layers_16/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:11.951404 140143678494720 adafactor.py:358] Since rank of parameter decoder/layers_16/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:11.952452 140143678494720 adafactor.py:358] Since rank of parameter decoder/layers_16/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:11.953356 140143678494720 adafactor.py:358] Since rank of parameter decoder/layers_16/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:11.954317 140143678494720 adafactor.py:358] Since rank of parameter decoder/layers_16/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:11.955389 140143678494720 adafactor.py:358] Since rank of parameter decoder/layers_16/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:11.956460 140143678494720 adafactor.py:358] Since rank of parameter decoder/layers_16/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:11.957553 140143678494720 adafactor.py:358] Since rank of parameter decoder/layers_16/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:11.958685 140143678494720 adafactor.py:358] Since rank of parameter decoder/layers_17/extra_mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:11.959748 140143678494720 adafactor.py:358] Since rank of parameter decoder/layers_17/extra_mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:11.960843 140143678494720 adafactor.py:358] Since rank of parameter decoder/layers_17/extra_mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:11.964835 140143678494720 adafactor.py:358] Since rank of parameter decoder/layers_17/mlp/router/router_weights/w/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>, <FactorDim.NONE: None>) is ignored.
W0512 23:45:11.965745 140143678494720 adafactor.py:358] Since rank of parameter decoder/layers_17/pre_extra_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:11.967077 140143678494720 adafactor.py:358] Since rank of parameter decoder/layers_17/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:11.967960 140143678494720 adafactor.py:358] Since rank of parameter decoder/layers_17/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:11.968877 140143678494720 adafactor.py:358] Since rank of parameter decoder/layers_17/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:11.969957 140143678494720 adafactor.py:358] Since rank of parameter decoder/layers_17/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:11.971046 140143678494720 adafactor.py:358] Since rank of parameter decoder/layers_17/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:11.972172 140143678494720 adafactor.py:358] Since rank of parameter decoder/layers_17/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:11.973262 140143678494720 adafactor.py:358] Since rank of parameter decoder/layers_18/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:11.974376 140143678494720 adafactor.py:358] Since rank of parameter decoder/layers_18/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:11.975437 140143678494720 adafactor.py:358] Since rank of parameter decoder/layers_18/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:11.976560 140143678494720 adafactor.py:358] Since rank of parameter decoder/layers_18/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:11.977435 140143678494720 adafactor.py:358] Since rank of parameter decoder/layers_18/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:11.978304 140143678494720 adafactor.py:358] Since rank of parameter decoder/layers_18/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:11.979369 140143678494720 adafactor.py:358] Since rank of parameter decoder/layers_18/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:11.980473 140143678494720 adafactor.py:358] Since rank of parameter decoder/layers_18/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:11.981564 140143678494720 adafactor.py:358] Since rank of parameter decoder/layers_18/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:11.982618 140143678494720 adafactor.py:358] Since rank of parameter decoder/layers_19/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:11.983690 140143678494720 adafactor.py:358] Since rank of parameter decoder/layers_19/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:11.984833 140143678494720 adafactor.py:358] Since rank of parameter decoder/layers_19/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:11.985895 140143678494720 adafactor.py:358] Since rank of parameter decoder/layers_19/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:11.986776 140143678494720 adafactor.py:358] Since rank of parameter decoder/layers_19/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:11.987657 140143678494720 adafactor.py:358] Since rank of parameter decoder/layers_19/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:11.988803 140143678494720 adafactor.py:358] Since rank of parameter decoder/layers_19/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:11.989879 140143678494720 adafactor.py:358] Since rank of parameter decoder/layers_19/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:11.990937 140143678494720 adafactor.py:358] Since rank of parameter decoder/layers_19/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:11.992012 140143678494720 adafactor.py:358] Since rank of parameter decoder/layers_2/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:11.993179 140143678494720 adafactor.py:358] Since rank of parameter decoder/layers_2/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:11.994280 140143678494720 adafactor.py:358] Since rank of parameter decoder/layers_2/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:11.995339 140143678494720 adafactor.py:358] Since rank of parameter decoder/layers_2/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:11.996193 140143678494720 adafactor.py:358] Since rank of parameter decoder/layers_2/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:11.997140 140143678494720 adafactor.py:358] Since rank of parameter decoder/layers_2/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:11.998194 140143678494720 adafactor.py:358] Since rank of parameter decoder/layers_2/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:11.999247 140143678494720 adafactor.py:358] Since rank of parameter decoder/layers_2/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.000306 140143678494720 adafactor.py:358] Since rank of parameter decoder/layers_2/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.001435 140143678494720 adafactor.py:358] Since rank of parameter decoder/layers_20/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.002498 140143678494720 adafactor.py:358] Since rank of parameter decoder/layers_20/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.003559 140143678494720 adafactor.py:358] Since rank of parameter decoder/layers_20/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:12.004648 140143678494720 adafactor.py:358] Since rank of parameter decoder/layers_20/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:12.005543 140143678494720 adafactor.py:358] Since rank of parameter decoder/layers_20/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:12.006468 140143678494720 adafactor.py:358] Since rank of parameter decoder/layers_20/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.007548 140143678494720 adafactor.py:358] Since rank of parameter decoder/layers_20/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:12.008624 140143678494720 adafactor.py:358] Since rank of parameter decoder/layers_20/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.009706 140143678494720 adafactor.py:358] Since rank of parameter decoder/layers_20/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.010806 140143678494720 adafactor.py:358] Since rank of parameter decoder/layers_21/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.011859 140143678494720 adafactor.py:358] Since rank of parameter decoder/layers_21/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.012955 140143678494720 adafactor.py:358] Since rank of parameter decoder/layers_21/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:12.014003 140143678494720 adafactor.py:358] Since rank of parameter decoder/layers_21/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:12.014951 140143678494720 adafactor.py:358] Since rank of parameter decoder/layers_21/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:12.015831 140143678494720 adafactor.py:358] Since rank of parameter decoder/layers_21/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.016917 140143678494720 adafactor.py:358] Since rank of parameter decoder/layers_21/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:12.017985 140143678494720 adafactor.py:358] Since rank of parameter decoder/layers_21/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.019498 140143678494720 adafactor.py:358] Since rank of parameter decoder/layers_21/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.020579 140143678494720 adafactor.py:358] Since rank of parameter decoder/layers_22/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.021644 140143678494720 adafactor.py:358] Since rank of parameter decoder/layers_22/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.022713 140143678494720 adafactor.py:358] Since rank of parameter decoder/layers_22/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:12.023823 140143678494720 adafactor.py:358] Since rank of parameter decoder/layers_22/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:12.024721 140143678494720 adafactor.py:358] Since rank of parameter decoder/layers_22/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:12.025606 140143678494720 adafactor.py:358] Since rank of parameter decoder/layers_22/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.026647 140143678494720 adafactor.py:358] Since rank of parameter decoder/layers_22/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:12.027768 140143678494720 adafactor.py:358] Since rank of parameter decoder/layers_22/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.028864 140143678494720 adafactor.py:358] Since rank of parameter decoder/layers_22/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.029916 140143678494720 adafactor.py:358] Since rank of parameter decoder/layers_23/extra_mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.030986 140143678494720 adafactor.py:358] Since rank of parameter decoder/layers_23/extra_mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
I0512 23:45:12.047479 140193652815872 moe_layers.py:777] Selected group_size=4096 and num_groups=192 for input num_tokens=786432, max_group_size=4096, num_experts=8 and num_expert_replicas=1
W0512 23:45:12.032096 140143678494720 adafactor.py:358] Since rank of parameter decoder/layers_23/extra_mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:12.036124 140143678494720 adafactor.py:358] Since rank of parameter decoder/layers_23/mlp/router/router_weights/w/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>, <FactorDim.NONE: None>) is ignored.
W0512 23:45:12.037031 140143678494720 adafactor.py:358] Since rank of parameter decoder/layers_23/pre_extra_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:12.037905 140143678494720 adafactor.py:358] Since rank of parameter decoder/layers_23/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:12.038770 140143678494720 adafactor.py:358] Since rank of parameter decoder/layers_23/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:12.039639 140143678494720 adafactor.py:358] Since rank of parameter decoder/layers_23/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.040781 140143678494720 adafactor.py:358] Since rank of parameter decoder/layers_23/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:12.041994 140143678494720 adafactor.py:358] Since rank of parameter decoder/layers_23/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.043047 140143678494720 adafactor.py:358] Since rank of parameter decoder/layers_23/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.044108 140143678494720 adafactor.py:358] Since rank of parameter decoder/layers_3/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.045263 140143678494720 adafactor.py:358] Since rank of parameter decoder/layers_3/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.046335 140143678494720 adafactor.py:358] Since rank of parameter decoder/layers_3/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:12.047395 140143678494720 adafactor.py:358] Since rank of parameter decoder/layers_3/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:12.048264 140143678494720 adafactor.py:358] Since rank of parameter decoder/layers_3/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:12.049226 140143678494720 adafactor.py:358] Since rank of parameter decoder/layers_3/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.050295 140143678494720 adafactor.py:358] Since rank of parameter decoder/layers_3/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:12.051364 140143678494720 adafactor.py:358] Since rank of parameter decoder/layers_3/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.052431 140143678494720 adafactor.py:358] Since rank of parameter decoder/layers_3/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.053575 140143678494720 adafactor.py:358] Since rank of parameter decoder/layers_4/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.054675 140143678494720 adafactor.py:358] Since rank of parameter decoder/layers_4/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.040063 140643590531072 adafactor.py:358] Since rank of parameter decoder/decoder_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:12.055739 140143678494720 adafactor.py:358] Since rank of parameter decoder/layers_4/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:12.041053 140643590531072 adafactor.py:358] Since rank of parameter decoder/layers_0/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.056830 140143678494720 adafactor.py:358] Since rank of parameter decoder/layers_4/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
I0512 23:45:12.059786 139666521167872 moe_layers.py:777] Selected group_size=4096 and num_groups=192 for input num_tokens=786432, max_group_size=4096, num_experts=8 and num_expert_replicas=1
W0512 23:45:12.042168 140643590531072 adafactor.py:358] Since rank of parameter decoder/layers_0/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.057758 140143678494720 adafactor.py:358] Since rank of parameter decoder/layers_4/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
I0512 23:45:12.060876 139876724979712 moe_layers.py:777] Selected group_size=4096 and num_groups=192 for input num_tokens=786432, max_group_size=4096, num_experts=8 and num_expert_replicas=1
W0512 23:45:12.058639 140143678494720 adafactor.py:358] Since rank of parameter decoder/layers_4/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.043267 140643590531072 adafactor.py:358] Since rank of parameter decoder/layers_0/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:12.059695 140143678494720 adafactor.py:358] Since rank of parameter decoder/layers_4/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:12.044467 140643590531072 adafactor.py:358] Since rank of parameter decoder/layers_0/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:12.045369 140643590531072 adafactor.py:358] Since rank of parameter decoder/layers_0/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:12.060785 140143678494720 adafactor.py:358] Since rank of parameter decoder/layers_4/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.046265 140643590531072 adafactor.py:358] Since rank of parameter decoder/layers_0/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.061893 140143678494720 adafactor.py:358] Since rank of parameter decoder/layers_4/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.047345 140643590531072 adafactor.py:358] Since rank of parameter decoder/layers_0/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:12.062957 140143678494720 adafactor.py:358] Since rank of parameter decoder/layers_5/extra_mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.048460 140643590531072 adafactor.py:358] Since rank of parameter decoder/layers_0/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.064007 140143678494720 adafactor.py:358] Since rank of parameter decoder/layers_5/extra_mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.049619 140643590531072 adafactor.py:358] Since rank of parameter decoder/layers_0/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.065097 140143678494720 adafactor.py:358] Since rank of parameter decoder/layers_5/extra_mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:12.050700 140643590531072 adafactor.py:358] Since rank of parameter decoder/layers_1/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.051816 140643590531072 adafactor.py:358] Since rank of parameter decoder/layers_1/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.052884 140643590531072 adafactor.py:358] Since rank of parameter decoder/layers_1/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:12.069108 140143678494720 adafactor.py:358] Since rank of parameter decoder/layers_5/mlp/router/router_weights/w/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>, <FactorDim.NONE: None>) is ignored.
W0512 23:45:12.054045 140643590531072 adafactor.py:358] Since rank of parameter decoder/layers_1/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:12.054916 140643590531072 adafactor.py:358] Since rank of parameter decoder/layers_1/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:12.070453 140143678494720 adafactor.py:358] Since rank of parameter decoder/layers_5/pre_extra_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:12.055856 140643590531072 adafactor.py:358] Since rank of parameter decoder/layers_1/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.071331 140143678494720 adafactor.py:358] Since rank of parameter decoder/layers_5/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:12.072196 140143678494720 adafactor.py:358] Since rank of parameter decoder/layers_5/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:12.056955 140643590531072 adafactor.py:358] Since rank of parameter decoder/layers_1/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:12.073094 140143678494720 adafactor.py:358] Since rank of parameter decoder/layers_5/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.058082 140643590531072 adafactor.py:358] Since rank of parameter decoder/layers_1/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.074156 140143678494720 adafactor.py:358] Since rank of parameter decoder/layers_5/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:12.059158 140643590531072 adafactor.py:358] Since rank of parameter decoder/layers_1/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.075287 140143678494720 adafactor.py:358] Since rank of parameter decoder/layers_5/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.060261 140643590531072 adafactor.py:358] Since rank of parameter decoder/layers_10/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.076359 140143678494720 adafactor.py:358] Since rank of parameter decoder/layers_5/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.061350 140643590531072 adafactor.py:358] Since rank of parameter decoder/layers_10/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.077445 140143678494720 adafactor.py:358] Since rank of parameter decoder/layers_6/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.062790 140643590531072 adafactor.py:358] Since rank of parameter decoder/layers_10/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:12.078497 140143678494720 adafactor.py:358] Since rank of parameter decoder/layers_6/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.063911 140643590531072 adafactor.py:358] Since rank of parameter decoder/layers_10/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:12.079598 140143678494720 adafactor.py:358] Since rank of parameter decoder/layers_6/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:12.064805 140643590531072 adafactor.py:358] Since rank of parameter decoder/layers_10/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:12.065682 140643590531072 adafactor.py:358] Since rank of parameter decoder/layers_10/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.080692 140143678494720 adafactor.py:358] Since rank of parameter decoder/layers_6/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:12.081567 140143678494720 adafactor.py:358] Since rank of parameter decoder/layers_6/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:12.066829 140643590531072 adafactor.py:358] Since rank of parameter decoder/layers_10/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:12.082441 140143678494720 adafactor.py:358] Since rank of parameter decoder/layers_6/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.067954 140643590531072 adafactor.py:358] Since rank of parameter decoder/layers_10/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.083547 140143678494720 adafactor.py:358] Since rank of parameter decoder/layers_6/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:12.069029 140643590531072 adafactor.py:358] Since rank of parameter decoder/layers_10/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.084638 140143678494720 adafactor.py:358] Since rank of parameter decoder/layers_6/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.070113 140643590531072 adafactor.py:358] Since rank of parameter decoder/layers_11/extra_mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.085718 140143678494720 adafactor.py:358] Since rank of parameter decoder/layers_6/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.071249 140643590531072 adafactor.py:358] Since rank of parameter decoder/layers_11/extra_mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.086784 140143678494720 adafactor.py:358] Since rank of parameter decoder/layers_7/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.072362 140643590531072 adafactor.py:358] Since rank of parameter decoder/layers_11/extra_mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:12.087882 140143678494720 adafactor.py:358] Since rank of parameter decoder/layers_7/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.088981 140143678494720 adafactor.py:358] Since rank of parameter decoder/layers_7/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:12.090035 140143678494720 adafactor.py:358] Since rank of parameter decoder/layers_7/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:12.090902 140143678494720 adafactor.py:358] Since rank of parameter decoder/layers_7/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:12.091819 140143678494720 adafactor.py:358] Since rank of parameter decoder/layers_7/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.076517 140643590531072 adafactor.py:358] Since rank of parameter decoder/layers_11/mlp/router/router_weights/w/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>, <FactorDim.NONE: None>) is ignored.
W0512 23:45:12.077413 140643590531072 adafactor.py:358] Since rank of parameter decoder/layers_11/pre_extra_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:12.092906 140143678494720 adafactor.py:358] Since rank of parameter decoder/layers_7/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:12.078290 140643590531072 adafactor.py:358] Since rank of parameter decoder/layers_11/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:12.093962 140143678494720 adafactor.py:358] Since rank of parameter decoder/layers_7/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.079180 140643590531072 adafactor.py:358] Since rank of parameter decoder/layers_11/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:12.095056 140143678494720 adafactor.py:358] Since rank of parameter decoder/layers_7/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.080157 140643590531072 adafactor.py:358] Since rank of parameter decoder/layers_11/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.096161 140143678494720 adafactor.py:358] Since rank of parameter decoder/layers_8/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.081245 140643590531072 adafactor.py:358] Since rank of parameter decoder/layers_11/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:12.097246 140143678494720 adafactor.py:358] Since rank of parameter decoder/layers_8/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.082335 140643590531072 adafactor.py:358] Since rank of parameter decoder/layers_11/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.098310 140143678494720 adafactor.py:358] Since rank of parameter decoder/layers_8/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:12.083411 140643590531072 adafactor.py:358] Since rank of parameter decoder/layers_11/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.099359 140143678494720 adafactor.py:358] Since rank of parameter decoder/layers_8/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:12.084643 140643590531072 adafactor.py:358] Since rank of parameter decoder/layers_12/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.100252 140143678494720 adafactor.py:358] Since rank of parameter decoder/layers_8/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:12.085753 140643590531072 adafactor.py:358] Since rank of parameter decoder/layers_12/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.101160 140143678494720 adafactor.py:358] Since rank of parameter decoder/layers_8/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.086823 140643590531072 adafactor.py:358] Since rank of parameter decoder/layers_12/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:12.102208 140143678494720 adafactor.py:358] Since rank of parameter decoder/layers_8/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:12.087939 140643590531072 adafactor.py:358] Since rank of parameter decoder/layers_12/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:12.103267 140143678494720 adafactor.py:358] Since rank of parameter decoder/layers_8/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.104388 140143678494720 adafactor.py:358] Since rank of parameter decoder/layers_8/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.089559 140643590531072 adafactor.py:358] Since rank of parameter decoder/layers_12/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:12.090447 140643590531072 adafactor.py:358] Since rank of parameter decoder/layers_12/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.105505 140143678494720 adafactor.py:358] Since rank of parameter decoder/layers_9/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.091537 140643590531072 adafactor.py:358] Since rank of parameter decoder/layers_12/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:12.106600 140143678494720 adafactor.py:358] Since rank of parameter decoder/layers_9/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
I0512 23:45:12.109773 139856202532864 moe_layers.py:777] Selected group_size=4096 and num_groups=192 for input num_tokens=786432, max_group_size=4096, num_experts=8 and num_expert_replicas=1
W0512 23:45:12.092649 140643590531072 adafactor.py:358] Since rank of parameter decoder/layers_12/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.107666 140143678494720 adafactor.py:358] Since rank of parameter decoder/layers_9/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:12.093782 140643590531072 adafactor.py:358] Since rank of parameter decoder/layers_12/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.108813 140143678494720 adafactor.py:358] Since rank of parameter decoder/layers_9/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:12.109698 140143678494720 adafactor.py:358] Since rank of parameter decoder/layers_9/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:12.094860 140643590531072 adafactor.py:358] Since rank of parameter decoder/layers_13/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.110565 140143678494720 adafactor.py:358] Since rank of parameter decoder/layers_9/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.096022 140643590531072 adafactor.py:358] Since rank of parameter decoder/layers_13/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.111617 140143678494720 adafactor.py:358] Since rank of parameter decoder/layers_9/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:12.097115 140643590531072 adafactor.py:358] Since rank of parameter decoder/layers_13/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:12.112698 140143678494720 adafactor.py:358] Since rank of parameter decoder/layers_9/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.098238 140643590531072 adafactor.py:358] Since rank of parameter decoder/layers_13/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:12.113805 140143678494720 adafactor.py:358] Since rank of parameter decoder/layers_9/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.099779 140643590531072 adafactor.py:358] Since rank of parameter decoder/layers_13/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:12.114894 140143678494720 adafactor.py:358] Since rank of parameter decoder/logits_dense/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.100658 140643590531072 adafactor.py:358] Since rank of parameter decoder/layers_13/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.115954 140143678494720 adafactor.py:358] Since rank of parameter token_embedder/embedding 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:12.101729 140643590531072 adafactor.py:358] Since rank of parameter decoder/layers_13/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:12.102852 140643590531072 adafactor.py:358] Since rank of parameter decoder/layers_13/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.103973 140643590531072 adafactor.py:358] Since rank of parameter decoder/layers_13/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.105052 140643590531072 adafactor.py:358] Since rank of parameter decoder/layers_14/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.106133 140643590531072 adafactor.py:358] Since rank of parameter decoder/layers_14/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.107265 140643590531072 adafactor.py:358] Since rank of parameter decoder/layers_14/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:12.108365 140643590531072 adafactor.py:358] Since rank of parameter decoder/layers_14/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:12.109260 140643590531072 adafactor.py:358] Since rank of parameter decoder/layers_14/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:12.110148 140643590531072 adafactor.py:358] Since rank of parameter decoder/layers_14/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.111218 140643590531072 adafactor.py:358] Since rank of parameter decoder/layers_14/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:12.112371 140643590531072 adafactor.py:358] Since rank of parameter decoder/layers_14/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.113460 140643590531072 adafactor.py:358] Since rank of parameter decoder/layers_14/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.114537 140643590531072 adafactor.py:358] Since rank of parameter decoder/layers_15/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.115679 140643590531072 adafactor.py:358] Since rank of parameter decoder/layers_15/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.116807 140643590531072 adafactor.py:358] Since rank of parameter decoder/layers_15/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:12.117882 140643590531072 adafactor.py:358] Since rank of parameter decoder/layers_15/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:12.159623 139666521167872 adafactor.py:358] Since rank of parameter decoder/decoder_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:12.160030 139876724979712 adafactor.py:358] Since rank of parameter decoder/decoder_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:12.160629 139666521167872 adafactor.py:358] Since rank of parameter decoder/layers_0/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.161041 139876724979712 adafactor.py:358] Since rank of parameter decoder/layers_0/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.161770 139666521167872 adafactor.py:358] Since rank of parameter decoder/layers_0/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.162177 139876724979712 adafactor.py:358] Since rank of parameter decoder/layers_0/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.162871 139666521167872 adafactor.py:358] Since rank of parameter decoder/layers_0/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:12.163306 139876724979712 adafactor.py:358] Since rank of parameter decoder/layers_0/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:12.164085 139666521167872 adafactor.py:358] Since rank of parameter decoder/layers_0/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:12.164547 139876724979712 adafactor.py:358] Since rank of parameter decoder/layers_0/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:12.165004 139666521167872 adafactor.py:358] Since rank of parameter decoder/layers_0/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:12.165453 139876724979712 adafactor.py:358] Since rank of parameter decoder/layers_0/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:12.165908 139666521167872 adafactor.py:358] Since rank of parameter decoder/layers_0/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.166364 139876724979712 adafactor.py:358] Since rank of parameter decoder/layers_0/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.167009 139666521167872 adafactor.py:358] Since rank of parameter decoder/layers_0/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:12.167524 139876724979712 adafactor.py:358] Since rank of parameter decoder/layers_0/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:12.168130 139666521167872 adafactor.py:358] Since rank of parameter decoder/layers_0/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.168644 139876724979712 adafactor.py:358] Since rank of parameter decoder/layers_0/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.169315 139666521167872 adafactor.py:358] Since rank of parameter decoder/layers_0/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.169830 139876724979712 adafactor.py:358] Since rank of parameter decoder/layers_0/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.170399 139666521167872 adafactor.py:358] Since rank of parameter decoder/layers_1/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.170940 139876724979712 adafactor.py:358] Since rank of parameter decoder/layers_1/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.171529 139666521167872 adafactor.py:358] Since rank of parameter decoder/layers_1/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.172060 139876724979712 adafactor.py:358] Since rank of parameter decoder/layers_1/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.172630 139666521167872 adafactor.py:358] Since rank of parameter decoder/layers_1/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:12.173147 139876724979712 adafactor.py:358] Since rank of parameter decoder/layers_1/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:12.173790 139666521167872 adafactor.py:358] Since rank of parameter decoder/layers_1/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:12.174314 139876724979712 adafactor.py:358] Since rank of parameter decoder/layers_1/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:12.174675 139666521167872 adafactor.py:358] Since rank of parameter decoder/layers_1/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:12.175233 139876724979712 adafactor.py:358] Since rank of parameter decoder/layers_1/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:12.175590 139666521167872 adafactor.py:358] Since rank of parameter decoder/layers_1/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.176124 139876724979712 adafactor.py:358] Since rank of parameter decoder/layers_1/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.176684 139666521167872 adafactor.py:358] Since rank of parameter decoder/layers_1/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:12.177244 139876724979712 adafactor.py:358] Since rank of parameter decoder/layers_1/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:12.177832 139666521167872 adafactor.py:358] Since rank of parameter decoder/layers_1/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.178390 139876724979712 adafactor.py:358] Since rank of parameter decoder/layers_1/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.178907 139666521167872 adafactor.py:358] Since rank of parameter decoder/layers_1/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.179515 139876724979712 adafactor.py:358] Since rank of parameter decoder/layers_1/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.180004 139666521167872 adafactor.py:358] Since rank of parameter decoder/layers_10/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.180614 139876724979712 adafactor.py:358] Since rank of parameter decoder/layers_10/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.181097 139666521167872 adafactor.py:358] Since rank of parameter decoder/layers_10/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.181712 139876724979712 adafactor.py:358] Since rank of parameter decoder/layers_10/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.182251 139666521167872 adafactor.py:358] Since rank of parameter decoder/layers_10/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:12.183175 139876724979712 adafactor.py:358] Since rank of parameter decoder/layers_10/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:12.183351 139666521167872 adafactor.py:358] Since rank of parameter decoder/layers_10/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:12.184320 139876724979712 adafactor.py:358] Since rank of parameter decoder/layers_10/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:12.184232 139666521167872 adafactor.py:358] Since rank of parameter decoder/layers_10/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:12.185211 139876724979712 adafactor.py:358] Since rank of parameter decoder/layers_10/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:12.185112 139666521167872 adafactor.py:358] Since rank of parameter decoder/layers_10/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.186105 139876724979712 adafactor.py:358] Since rank of parameter decoder/layers_10/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.186245 139666521167872 adafactor.py:358] Since rank of parameter decoder/layers_10/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:12.187280 139876724979712 adafactor.py:358] Since rank of parameter decoder/layers_10/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:12.187380 139666521167872 adafactor.py:358] Since rank of parameter decoder/layers_10/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.188389 139876724979712 adafactor.py:358] Since rank of parameter decoder/layers_10/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.188455 139666521167872 adafactor.py:358] Since rank of parameter decoder/layers_10/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.189524 139876724979712 adafactor.py:358] Since rank of parameter decoder/layers_10/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.189547 139666521167872 adafactor.py:358] Since rank of parameter decoder/layers_11/extra_mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.190631 139876724979712 adafactor.py:358] Since rank of parameter decoder/layers_11/extra_mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.190697 139666521167872 adafactor.py:358] Since rank of parameter decoder/layers_11/extra_mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.191799 139876724979712 adafactor.py:358] Since rank of parameter decoder/layers_11/extra_mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.191814 139666521167872 adafactor.py:358] Since rank of parameter decoder/layers_11/extra_mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:12.192901 139876724979712 adafactor.py:358] Since rank of parameter decoder/layers_11/extra_mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:12.195961 139666521167872 adafactor.py:358] Since rank of parameter decoder/layers_11/mlp/router/router_weights/w/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>, <FactorDim.NONE: None>) is ignored.
W0512 23:45:12.197054 139876724979712 adafactor.py:358] Since rank of parameter decoder/layers_11/mlp/router/router_weights/w/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>, <FactorDim.NONE: None>) is ignored.
W0512 23:45:12.196875 139666521167872 adafactor.py:358] Since rank of parameter decoder/layers_11/pre_extra_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:12.197983 139876724979712 adafactor.py:358] Since rank of parameter decoder/layers_11/pre_extra_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:12.197766 139666521167872 adafactor.py:358] Since rank of parameter decoder/layers_11/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:12.198653 139666521167872 adafactor.py:358] Since rank of parameter decoder/layers_11/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:12.198870 139876724979712 adafactor.py:358] Since rank of parameter decoder/layers_11/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:12.199802 139876724979712 adafactor.py:358] Since rank of parameter decoder/layers_11/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:12.199636 139666521167872 adafactor.py:358] Since rank of parameter decoder/layers_11/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.200787 139876724979712 adafactor.py:358] Since rank of parameter decoder/layers_11/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.200728 139666521167872 adafactor.py:358] Since rank of parameter decoder/layers_11/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:12.201881 139876724979712 adafactor.py:358] Since rank of parameter decoder/layers_11/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:12.201827 139666521167872 adafactor.py:358] Since rank of parameter decoder/layers_11/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.202985 139876724979712 adafactor.py:358] Since rank of parameter decoder/layers_11/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.202919 139666521167872 adafactor.py:358] Since rank of parameter decoder/layers_11/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.204159 139876724979712 adafactor.py:358] Since rank of parameter decoder/layers_11/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.204148 139666521167872 adafactor.py:358] Since rank of parameter decoder/layers_12/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.205371 139876724979712 adafactor.py:358] Since rank of parameter decoder/layers_12/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.205236 139666521167872 adafactor.py:358] Since rank of parameter decoder/layers_12/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.206473 139876724979712 adafactor.py:358] Since rank of parameter decoder/layers_12/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.206328 139666521167872 adafactor.py:358] Since rank of parameter decoder/layers_12/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:12.207621 139876724979712 adafactor.py:358] Since rank of parameter decoder/layers_12/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:12.207462 139666521167872 adafactor.py:358] Since rank of parameter decoder/layers_12/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:12.208727 139876724979712 adafactor.py:358] Since rank of parameter decoder/layers_12/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:12.209096 139666521167872 adafactor.py:358] Since rank of parameter decoder/layers_12/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:12.210397 139876724979712 adafactor.py:358] Since rank of parameter decoder/layers_12/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:12.209998 139666521167872 adafactor.py:358] Since rank of parameter decoder/layers_12/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.211348 139876724979712 adafactor.py:358] Since rank of parameter decoder/layers_12/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.211119 139666521167872 adafactor.py:358] Since rank of parameter decoder/layers_12/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:12.212450 139876724979712 adafactor.py:358] Since rank of parameter decoder/layers_12/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:12.212217 139666521167872 adafactor.py:358] Since rank of parameter decoder/layers_12/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.213544 139876724979712 adafactor.py:358] Since rank of parameter decoder/layers_12/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.213403 139666521167872 adafactor.py:358] Since rank of parameter decoder/layers_12/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.214705 139876724979712 adafactor.py:358] Since rank of parameter decoder/layers_12/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.214507 139666521167872 adafactor.py:358] Since rank of parameter decoder/layers_13/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.215820 139876724979712 adafactor.py:358] Since rank of parameter decoder/layers_13/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.215647 139666521167872 adafactor.py:358] Since rank of parameter decoder/layers_13/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.216924 139876724979712 adafactor.py:358] Since rank of parameter decoder/layers_13/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.216745 139666521167872 adafactor.py:358] Since rank of parameter decoder/layers_13/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:12.218033 139876724979712 adafactor.py:358] Since rank of parameter decoder/layers_13/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:12.217888 139666521167872 adafactor.py:358] Since rank of parameter decoder/layers_13/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:12.219202 139876724979712 adafactor.py:358] Since rank of parameter decoder/layers_13/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:12.219377 139666521167872 adafactor.py:358] Since rank of parameter decoder/layers_13/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:12.220278 139666521167872 adafactor.py:358] Since rank of parameter decoder/layers_13/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.220736 139876724979712 adafactor.py:358] Since rank of parameter decoder/layers_13/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:12.221647 139876724979712 adafactor.py:358] Since rank of parameter decoder/layers_13/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.221360 139666521167872 adafactor.py:358] Since rank of parameter decoder/layers_13/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:12.222738 139876724979712 adafactor.py:358] Since rank of parameter decoder/layers_13/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:12.222501 139666521167872 adafactor.py:358] Since rank of parameter decoder/layers_13/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.223920 139876724979712 adafactor.py:358] Since rank of parameter decoder/layers_13/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.223625 139666521167872 adafactor.py:358] Since rank of parameter decoder/layers_13/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.225055 139876724979712 adafactor.py:358] Since rank of parameter decoder/layers_13/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.224719 139666521167872 adafactor.py:358] Since rank of parameter decoder/layers_14/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.226154 139876724979712 adafactor.py:358] Since rank of parameter decoder/layers_14/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.225801 139666521167872 adafactor.py:358] Since rank of parameter decoder/layers_14/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.226946 139666521167872 adafactor.py:358] Since rank of parameter decoder/layers_14/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:12.227274 139876724979712 adafactor.py:358] Since rank of parameter decoder/layers_14/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.228417 139876724979712 adafactor.py:358] Since rank of parameter decoder/layers_14/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:12.228065 139666521167872 adafactor.py:358] Since rank of parameter decoder/layers_14/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:12.228949 139666521167872 adafactor.py:358] Since rank of parameter decoder/layers_14/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:12.229500 139876724979712 adafactor.py:358] Since rank of parameter decoder/layers_14/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:12.229834 139666521167872 adafactor.py:358] Since rank of parameter decoder/layers_14/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.230400 139876724979712 adafactor.py:358] Since rank of parameter decoder/layers_14/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:12.231353 139876724979712 adafactor.py:358] Since rank of parameter decoder/layers_14/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
I0512 23:45:12.220591 140180528838656 moe_layers.py:777] Selected group_size=4096 and num_groups=192 for input num_tokens=786432, max_group_size=4096, num_experts=8 and num_expert_replicas=1
W0512 23:45:12.118751 140643590531072 adafactor.py:358] Since rank of parameter decoder/layers_15/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:12.119691 140643590531072 adafactor.py:358] Since rank of parameter decoder/layers_15/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.120836 140643590531072 adafactor.py:358] Since rank of parameter decoder/layers_15/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:12.121911 140643590531072 adafactor.py:358] Since rank of parameter decoder/layers_15/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.123005 140643590531072 adafactor.py:358] Since rank of parameter decoder/layers_15/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.124111 140643590531072 adafactor.py:358] Since rank of parameter decoder/layers_16/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.125240 140643590531072 adafactor.py:358] Since rank of parameter decoder/layers_16/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.126322 140643590531072 adafactor.py:358] Since rank of parameter decoder/layers_16/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:12.127412 140643590531072 adafactor.py:358] Since rank of parameter decoder/layers_16/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:12.128331 140643590531072 adafactor.py:358] Since rank of parameter decoder/layers_16/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:12.129267 140643590531072 adafactor.py:358] Since rank of parameter decoder/layers_16/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.130342 140643590531072 adafactor.py:358] Since rank of parameter decoder/layers_16/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:12.131419 140643590531072 adafactor.py:358] Since rank of parameter decoder/layers_16/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.132534 140643590531072 adafactor.py:358] Since rank of parameter decoder/layers_16/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.133661 140643590531072 adafactor.py:358] Since rank of parameter decoder/layers_17/extra_mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.134726 140643590531072 adafactor.py:358] Since rank of parameter decoder/layers_17/extra_mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.135889 140643590531072 adafactor.py:358] Since rank of parameter decoder/layers_17/extra_mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:12.139976 140643590531072 adafactor.py:358] Since rank of parameter decoder/layers_17/mlp/router/router_weights/w/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>, <FactorDim.NONE: None>) is ignored.
W0512 23:45:12.140866 140643590531072 adafactor.py:358] Since rank of parameter decoder/layers_17/pre_extra_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:12.142201 140643590531072 adafactor.py:358] Since rank of parameter decoder/layers_17/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:12.143081 140643590531072 adafactor.py:358] Since rank of parameter decoder/layers_17/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:12.143994 140643590531072 adafactor.py:358] Since rank of parameter decoder/layers_17/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.145095 140643590531072 adafactor.py:358] Since rank of parameter decoder/layers_17/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:12.146174 140643590531072 adafactor.py:358] Since rank of parameter decoder/layers_17/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.147297 140643590531072 adafactor.py:358] Since rank of parameter decoder/layers_17/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.148412 140643590531072 adafactor.py:358] Since rank of parameter decoder/layers_18/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.149486 140643590531072 adafactor.py:358] Since rank of parameter decoder/layers_18/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.150558 140643590531072 adafactor.py:358] Since rank of parameter decoder/layers_18/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:12.151715 140643590531072 adafactor.py:358] Since rank of parameter decoder/layers_18/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:12.152593 140643590531072 adafactor.py:358] Since rank of parameter decoder/layers_18/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:12.153467 140643590531072 adafactor.py:358] Since rank of parameter decoder/layers_18/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.154550 140643590531072 adafactor.py:358] Since rank of parameter decoder/layers_18/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:12.155749 140643590531072 adafactor.py:358] Since rank of parameter decoder/layers_18/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.156846 140643590531072 adafactor.py:358] Since rank of parameter decoder/layers_18/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.157921 140643590531072 adafactor.py:358] Since rank of parameter decoder/layers_19/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.159001 140643590531072 adafactor.py:358] Since rank of parameter decoder/layers_19/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.160155 140643590531072 adafactor.py:358] Since rank of parameter decoder/layers_19/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:12.161225 140643590531072 adafactor.py:358] Since rank of parameter decoder/layers_19/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:12.162112 140643590531072 adafactor.py:358] Since rank of parameter decoder/layers_19/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:12.162991 140643590531072 adafactor.py:358] Since rank of parameter decoder/layers_19/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.164150 140643590531072 adafactor.py:358] Since rank of parameter decoder/layers_19/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:12.165234 140643590531072 adafactor.py:358] Since rank of parameter decoder/layers_19/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.166320 140643590531072 adafactor.py:358] Since rank of parameter decoder/layers_19/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.167416 140643590531072 adafactor.py:358] Since rank of parameter decoder/layers_2/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.168576 140643590531072 adafactor.py:358] Since rank of parameter decoder/layers_2/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.169648 140643590531072 adafactor.py:358] Since rank of parameter decoder/layers_2/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:12.170727 140643590531072 adafactor.py:358] Since rank of parameter decoder/layers_2/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:12.171625 140643590531072 adafactor.py:358] Since rank of parameter decoder/layers_2/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:12.172574 140643590531072 adafactor.py:358] Since rank of parameter decoder/layers_2/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.173648 140643590531072 adafactor.py:358] Since rank of parameter decoder/layers_2/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:12.174725 140643590531072 adafactor.py:358] Since rank of parameter decoder/layers_2/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.175858 140643590531072 adafactor.py:358] Since rank of parameter decoder/layers_2/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.176994 140643590531072 adafactor.py:358] Since rank of parameter decoder/layers_20/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.178076 140643590531072 adafactor.py:358] Since rank of parameter decoder/layers_20/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.179150 140643590531072 adafactor.py:358] Since rank of parameter decoder/layers_20/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:12.180260 140643590531072 adafactor.py:358] Since rank of parameter decoder/layers_20/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:12.181164 140643590531072 adafactor.py:358] Since rank of parameter decoder/layers_20/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:12.182104 140643590531072 adafactor.py:358] Since rank of parameter decoder/layers_20/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.183174 140643590531072 adafactor.py:358] Since rank of parameter decoder/layers_20/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:12.184285 140643590531072 adafactor.py:358] Since rank of parameter decoder/layers_20/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.185372 140643590531072 adafactor.py:358] Since rank of parameter decoder/layers_20/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.186491 140643590531072 adafactor.py:358] Since rank of parameter decoder/layers_21/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.187585 140643590531072 adafactor.py:358] Since rank of parameter decoder/layers_21/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.188659 140643590531072 adafactor.py:358] Since rank of parameter decoder/layers_21/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:12.189745 140643590531072 adafactor.py:358] Since rank of parameter decoder/layers_21/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:12.190665 140643590531072 adafactor.py:358] Since rank of parameter decoder/layers_21/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:12.191578 140643590531072 adafactor.py:358] Since rank of parameter decoder/layers_21/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.192667 140643590531072 adafactor.py:358] Since rank of parameter decoder/layers_21/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:12.193756 140643590531072 adafactor.py:358] Since rank of parameter decoder/layers_21/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.195351 140643590531072 adafactor.py:358] Since rank of parameter decoder/layers_21/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.196462 140643590531072 adafactor.py:358] Since rank of parameter decoder/layers_22/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.197538 140643590531072 adafactor.py:358] Since rank of parameter decoder/layers_22/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.198623 140643590531072 adafactor.py:358] Since rank of parameter decoder/layers_22/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:12.199770 140643590531072 adafactor.py:358] Since rank of parameter decoder/layers_22/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:12.200674 140643590531072 adafactor.py:358] Since rank of parameter decoder/layers_22/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:12.201797 140643590531072 adafactor.py:358] Since rank of parameter decoder/layers_22/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.202864 140643590531072 adafactor.py:358] Since rank of parameter decoder/layers_22/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:12.204052 140643590531072 adafactor.py:358] Since rank of parameter decoder/layers_22/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.205133 140643590531072 adafactor.py:358] Since rank of parameter decoder/layers_22/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.206206 140643590531072 adafactor.py:358] Since rank of parameter decoder/layers_23/extra_mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.207295 140643590531072 adafactor.py:358] Since rank of parameter decoder/layers_23/extra_mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.208450 140643590531072 adafactor.py:358] Since rank of parameter decoder/layers_23/extra_mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:12.212535 140643590531072 adafactor.py:358] Since rank of parameter decoder/layers_23/mlp/router/router_weights/w/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>, <FactorDim.NONE: None>) is ignored.
W0512 23:45:12.213434 140643590531072 adafactor.py:358] Since rank of parameter decoder/layers_23/pre_extra_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:12.214315 140643590531072 adafactor.py:358] Since rank of parameter decoder/layers_23/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:12.215239 140643590531072 adafactor.py:358] Since rank of parameter decoder/layers_23/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:12.216149 140643590531072 adafactor.py:358] Since rank of parameter decoder/layers_23/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.217275 140643590531072 adafactor.py:358] Since rank of parameter decoder/layers_23/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:12.218555 140643590531072 adafactor.py:358] Since rank of parameter decoder/layers_23/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.219663 140643590531072 adafactor.py:358] Since rank of parameter decoder/layers_23/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.220739 140643590531072 adafactor.py:358] Since rank of parameter decoder/layers_3/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.221860 140643590531072 adafactor.py:358] Since rank of parameter decoder/layers_3/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.222939 140643590531072 adafactor.py:358] Since rank of parameter decoder/layers_3/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:12.224035 140643590531072 adafactor.py:358] Since rank of parameter decoder/layers_3/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:12.224905 140643590531072 adafactor.py:358] Since rank of parameter decoder/layers_3/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:12.225834 140643590531072 adafactor.py:358] Since rank of parameter decoder/layers_3/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.226895 140643590531072 adafactor.py:358] Since rank of parameter decoder/layers_3/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:12.228013 140643590531072 adafactor.py:358] Since rank of parameter decoder/layers_3/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.229137 140643590531072 adafactor.py:358] Since rank of parameter decoder/layers_3/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.230264 140643590531072 adafactor.py:358] Since rank of parameter decoder/layers_4/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.231343 140643590531072 adafactor.py:358] Since rank of parameter decoder/layers_4/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.232448 140643590531072 adafactor.py:358] Since rank of parameter decoder/layers_4/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:12.233518 140643590531072 adafactor.py:358] Since rank of parameter decoder/layers_4/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:12.234447 140643590531072 adafactor.py:358] Since rank of parameter decoder/layers_4/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:12.235368 140643590531072 adafactor.py:358] Since rank of parameter decoder/layers_4/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.236464 140643590531072 adafactor.py:358] Since rank of parameter decoder/layers_4/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:12.237536 140643590531072 adafactor.py:358] Since rank of parameter decoder/layers_4/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.238663 140643590531072 adafactor.py:358] Since rank of parameter decoder/layers_4/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.239760 140643590531072 adafactor.py:358] Since rank of parameter decoder/layers_5/extra_mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.240825 140643590531072 adafactor.py:358] Since rank of parameter decoder/layers_5/extra_mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.241892 140643590531072 adafactor.py:358] Since rank of parameter decoder/layers_5/extra_mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
I0512 23:45:12.241427 140260698863616 moe_layers.py:777] Selected group_size=4096 and num_groups=192 for input num_tokens=786432, max_group_size=4096, num_experts=8 and num_expert_replicas=1
W0512 23:45:12.245963 140643590531072 adafactor.py:358] Since rank of parameter decoder/layers_5/mlp/router/router_weights/w/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>, <FactorDim.NONE: None>) is ignored.
W0512 23:45:12.247308 140643590531072 adafactor.py:358] Since rank of parameter decoder/layers_5/pre_extra_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:12.248231 140643590531072 adafactor.py:358] Since rank of parameter decoder/layers_5/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:12.249115 140643590531072 adafactor.py:358] Since rank of parameter decoder/layers_5/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:12.249994 140643590531072 adafactor.py:358] Since rank of parameter decoder/layers_5/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.251083 140643590531072 adafactor.py:358] Since rank of parameter decoder/layers_5/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:12.252241 140643590531072 adafactor.py:358] Since rank of parameter decoder/layers_5/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.253311 140643590531072 adafactor.py:358] Since rank of parameter decoder/layers_5/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.254366 140643590531072 adafactor.py:358] Since rank of parameter decoder/layers_6/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.255472 140643590531072 adafactor.py:358] Since rank of parameter decoder/layers_6/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.256605 140643590531072 adafactor.py:358] Since rank of parameter decoder/layers_6/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:12.257663 140643590531072 adafactor.py:358] Since rank of parameter decoder/layers_6/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:12.258546 140643590531072 adafactor.py:358] Since rank of parameter decoder/layers_6/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:12.259414 140643590531072 adafactor.py:358] Since rank of parameter decoder/layers_6/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.260557 140643590531072 adafactor.py:358] Since rank of parameter decoder/layers_6/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:12.261644 140643590531072 adafactor.py:358] Since rank of parameter decoder/layers_6/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.262710 140643590531072 adafactor.py:358] Since rank of parameter decoder/layers_6/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.263808 140643590531072 adafactor.py:358] Since rank of parameter decoder/layers_7/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.264922 140643590531072 adafactor.py:358] Since rank of parameter decoder/layers_7/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.265997 140643590531072 adafactor.py:358] Since rank of parameter decoder/layers_7/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:12.267074 140643590531072 adafactor.py:358] Since rank of parameter decoder/layers_7/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:12.267991 140643590531072 adafactor.py:358] Since rank of parameter decoder/layers_7/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:12.268908 140643590531072 adafactor.py:358] Since rank of parameter decoder/layers_7/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.269993 140643590531072 adafactor.py:358] Since rank of parameter decoder/layers_7/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:12.271078 140643590531072 adafactor.py:358] Since rank of parameter decoder/layers_7/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.272179 140643590531072 adafactor.py:358] Since rank of parameter decoder/layers_7/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.273312 140643590531072 adafactor.py:358] Since rank of parameter decoder/layers_8/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.274405 140643590531072 adafactor.py:358] Since rank of parameter decoder/layers_8/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.275511 140643590531072 adafactor.py:358] Since rank of parameter decoder/layers_8/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:12.276618 140643590531072 adafactor.py:358] Since rank of parameter decoder/layers_8/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:12.277541 140643590531072 adafactor.py:358] Since rank of parameter decoder/layers_8/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:12.278427 140643590531072 adafactor.py:358] Since rank of parameter decoder/layers_8/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.279492 140643590531072 adafactor.py:358] Since rank of parameter decoder/layers_8/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:12.280596 140643590531072 adafactor.py:358] Since rank of parameter decoder/layers_8/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.281720 140643590531072 adafactor.py:358] Since rank of parameter decoder/layers_8/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.282821 140643590531072 adafactor.py:358] Since rank of parameter decoder/layers_9/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.283926 140643590531072 adafactor.py:358] Since rank of parameter decoder/layers_9/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.285001 140643590531072 adafactor.py:358] Since rank of parameter decoder/layers_9/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:12.286117 140643590531072 adafactor.py:358] Since rank of parameter decoder/layers_9/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:12.287004 140643590531072 adafactor.py:358] Since rank of parameter decoder/layers_9/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:12.287905 140643590531072 adafactor.py:358] Since rank of parameter decoder/layers_9/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.288975 140643590531072 adafactor.py:358] Since rank of parameter decoder/layers_9/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:12.290047 140643590531072 adafactor.py:358] Since rank of parameter decoder/layers_9/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.291179 140643590531072 adafactor.py:358] Since rank of parameter decoder/layers_9/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.292275 140643590531072 adafactor.py:358] Since rank of parameter decoder/logits_dense/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.293351 140643590531072 adafactor.py:358] Since rank of parameter token_embedder/embedding 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:12.232437 139876724979712 adafactor.py:358] Since rank of parameter decoder/layers_14/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:12.233590 139876724979712 adafactor.py:358] Since rank of parameter decoder/layers_14/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.234683 139876724979712 adafactor.py:358] Since rank of parameter decoder/layers_14/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.235798 139876724979712 adafactor.py:358] Since rank of parameter decoder/layers_15/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.236905 139876724979712 adafactor.py:358] Since rank of parameter decoder/layers_15/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.238045 139876724979712 adafactor.py:358] Since rank of parameter decoder/layers_15/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:12.239157 139876724979712 adafactor.py:358] Since rank of parameter decoder/layers_15/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:12.240061 139876724979712 adafactor.py:358] Since rank of parameter decoder/layers_15/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:12.230915 139666521167872 adafactor.py:358] Since rank of parameter decoder/layers_14/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:12.232074 139666521167872 adafactor.py:358] Since rank of parameter decoder/layers_14/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.233175 139666521167872 adafactor.py:358] Since rank of parameter decoder/layers_14/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.234250 139666521167872 adafactor.py:358] Since rank of parameter decoder/layers_15/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.235360 139666521167872 adafactor.py:358] Since rank of parameter decoder/layers_15/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.236498 139666521167872 adafactor.py:358] Since rank of parameter decoder/layers_15/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:12.237587 139666521167872 adafactor.py:358] Since rank of parameter decoder/layers_15/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:12.238459 139666521167872 adafactor.py:358] Since rank of parameter decoder/layers_15/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:12.240958 139876724979712 adafactor.py:358] Since rank of parameter decoder/layers_15/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.242103 139876724979712 adafactor.py:358] Since rank of parameter decoder/layers_15/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:12.243228 139876724979712 adafactor.py:358] Since rank of parameter decoder/layers_15/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.244368 139876724979712 adafactor.py:358] Since rank of parameter decoder/layers_15/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.245455 139876724979712 adafactor.py:358] Since rank of parameter decoder/layers_16/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.246634 139876724979712 adafactor.py:358] Since rank of parameter decoder/layers_16/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.247776 139876724979712 adafactor.py:358] Since rank of parameter decoder/layers_16/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:12.248869 139876724979712 adafactor.py:358] Since rank of parameter decoder/layers_16/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:12.249774 139876724979712 adafactor.py:358] Since rank of parameter decoder/layers_16/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:12.239376 139666521167872 adafactor.py:358] Since rank of parameter decoder/layers_15/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.240557 139666521167872 adafactor.py:358] Since rank of parameter decoder/layers_15/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:12.241658 139666521167872 adafactor.py:358] Since rank of parameter decoder/layers_15/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.242733 139666521167872 adafactor.py:358] Since rank of parameter decoder/layers_15/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.243853 139666521167872 adafactor.py:358] Since rank of parameter decoder/layers_16/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.244992 139666521167872 adafactor.py:358] Since rank of parameter decoder/layers_16/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.246075 139666521167872 adafactor.py:358] Since rank of parameter decoder/layers_16/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:12.247195 139666521167872 adafactor.py:358] Since rank of parameter decoder/layers_16/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:12.248082 139666521167872 adafactor.py:358] Since rank of parameter decoder/layers_16/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:12.250726 139876724979712 adafactor.py:358] Since rank of parameter decoder/layers_16/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.251854 139876724979712 adafactor.py:358] Since rank of parameter decoder/layers_16/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:12.252965 139876724979712 adafactor.py:358] Since rank of parameter decoder/layers_16/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.254071 139876724979712 adafactor.py:358] Since rank of parameter decoder/layers_16/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.255258 139876724979712 adafactor.py:358] Since rank of parameter decoder/layers_17/extra_mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.256363 139876724979712 adafactor.py:358] Since rank of parameter decoder/layers_17/extra_mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.257463 139876724979712 adafactor.py:358] Since rank of parameter decoder/layers_17/extra_mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:12.261609 139876724979712 adafactor.py:358] Since rank of parameter decoder/layers_17/mlp/router/router_weights/w/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>, <FactorDim.NONE: None>) is ignored.
W0512 23:45:12.249015 139666521167872 adafactor.py:358] Since rank of parameter decoder/layers_16/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.250095 139666521167872 adafactor.py:358] Since rank of parameter decoder/layers_16/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:12.251210 139666521167872 adafactor.py:358] Since rank of parameter decoder/layers_16/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.252296 139666521167872 adafactor.py:358] Since rank of parameter decoder/layers_16/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.253481 139666521167872 adafactor.py:358] Since rank of parameter decoder/layers_17/extra_mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.254558 139666521167872 adafactor.py:358] Since rank of parameter decoder/layers_17/extra_mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.255676 139666521167872 adafactor.py:358] Since rank of parameter decoder/layers_17/extra_mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:12.259777 139666521167872 adafactor.py:358] Since rank of parameter decoder/layers_17/mlp/router/router_weights/w/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>, <FactorDim.NONE: None>) is ignored.
W0512 23:45:12.262521 139876724979712 adafactor.py:358] Since rank of parameter decoder/layers_17/pre_extra_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:12.263921 139876724979712 adafactor.py:358] Since rank of parameter decoder/layers_17/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:12.264851 139876724979712 adafactor.py:358] Since rank of parameter decoder/layers_17/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:12.265758 139876724979712 adafactor.py:358] Since rank of parameter decoder/layers_17/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.266862 139876724979712 adafactor.py:358] Since rank of parameter decoder/layers_17/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:12.267993 139876724979712 adafactor.py:358] Since rank of parameter decoder/layers_17/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.269136 139876724979712 adafactor.py:358] Since rank of parameter decoder/layers_17/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.270239 139876724979712 adafactor.py:358] Since rank of parameter decoder/layers_18/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.271379 139876724979712 adafactor.py:358] Since rank of parameter decoder/layers_18/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.260685 139666521167872 adafactor.py:358] Since rank of parameter decoder/layers_17/pre_extra_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:12.262006 139666521167872 adafactor.py:358] Since rank of parameter decoder/layers_17/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:12.262905 139666521167872 adafactor.py:358] Since rank of parameter decoder/layers_17/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:12.263826 139666521167872 adafactor.py:358] Since rank of parameter decoder/layers_17/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.264908 139666521167872 adafactor.py:358] Since rank of parameter decoder/layers_17/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:12.265986 139666521167872 adafactor.py:358] Since rank of parameter decoder/layers_17/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.267140 139666521167872 adafactor.py:358] Since rank of parameter decoder/layers_17/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.268223 139666521167872 adafactor.py:358] Since rank of parameter decoder/layers_18/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.269323 139666521167872 adafactor.py:358] Since rank of parameter decoder/layers_18/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.270395 139666521167872 adafactor.py:358] Since rank of parameter decoder/layers_18/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:12.271555 139666521167872 adafactor.py:358] Since rank of parameter decoder/layers_18/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:12.272470 139666521167872 adafactor.py:358] Since rank of parameter decoder/layers_18/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:12.273347 139666521167872 adafactor.py:358] Since rank of parameter decoder/layers_18/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.274428 139666521167872 adafactor.py:358] Since rank of parameter decoder/layers_18/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:12.275593 139666521167872 adafactor.py:358] Since rank of parameter decoder/layers_18/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.276679 139666521167872 adafactor.py:358] Since rank of parameter decoder/layers_18/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.277754 139666521167872 adafactor.py:358] Since rank of parameter decoder/layers_19/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.278839 139666521167872 adafactor.py:358] Since rank of parameter decoder/layers_19/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.272464 139876724979712 adafactor.py:358] Since rank of parameter decoder/layers_18/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:12.273614 139876724979712 adafactor.py:358] Since rank of parameter decoder/layers_18/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:12.274520 139876724979712 adafactor.py:358] Since rank of parameter decoder/layers_18/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:12.275618 139876724979712 adafactor.py:358] Since rank of parameter decoder/layers_18/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.276724 139876724979712 adafactor.py:358] Since rank of parameter decoder/layers_18/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:12.277880 139876724979712 adafactor.py:358] Since rank of parameter decoder/layers_18/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.278976 139876724979712 adafactor.py:358] Since rank of parameter decoder/layers_18/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.280094 139876724979712 adafactor.py:358] Since rank of parameter decoder/layers_19/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.281198 139876724979712 adafactor.py:358] Since rank of parameter decoder/layers_19/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.279995 139666521167872 adafactor.py:358] Since rank of parameter decoder/layers_19/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:12.281059 139666521167872 adafactor.py:358] Since rank of parameter decoder/layers_19/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:12.281959 139666521167872 adafactor.py:358] Since rank of parameter decoder/layers_19/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:12.282831 139666521167872 adafactor.py:358] Since rank of parameter decoder/layers_19/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.283991 139666521167872 adafactor.py:358] Since rank of parameter decoder/layers_19/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:12.285084 139666521167872 adafactor.py:358] Since rank of parameter decoder/layers_19/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.286166 139666521167872 adafactor.py:358] Since rank of parameter decoder/layers_19/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.287281 139666521167872 adafactor.py:358] Since rank of parameter decoder/layers_2/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.282344 139876724979712 adafactor.py:358] Since rank of parameter decoder/layers_19/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:12.283470 139876724979712 adafactor.py:358] Since rank of parameter decoder/layers_19/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:12.284425 139876724979712 adafactor.py:358] Since rank of parameter decoder/layers_19/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:12.285324 139876724979712 adafactor.py:358] Since rank of parameter decoder/layers_19/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.286472 139876724979712 adafactor.py:358] Since rank of parameter decoder/layers_19/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:12.287607 139876724979712 adafactor.py:358] Since rank of parameter decoder/layers_19/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.288712 139876724979712 adafactor.py:358] Since rank of parameter decoder/layers_19/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.289814 139876724979712 adafactor.py:358] Since rank of parameter decoder/layers_2/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.288415 139666521167872 adafactor.py:358] Since rank of parameter decoder/layers_2/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.289503 139666521167872 adafactor.py:358] Since rank of parameter decoder/layers_2/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:12.290583 139666521167872 adafactor.py:358] Since rank of parameter decoder/layers_2/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:12.291491 139666521167872 adafactor.py:358] Since rank of parameter decoder/layers_2/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:12.292433 139666521167872 adafactor.py:358] Since rank of parameter decoder/layers_2/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.293517 139666521167872 adafactor.py:358] Since rank of parameter decoder/layers_2/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:12.294593 139666521167872 adafactor.py:358] Since rank of parameter decoder/layers_2/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.295699 139666521167872 adafactor.py:358] Since rank of parameter decoder/layers_2/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.296823 139666521167872 adafactor.py:358] Since rank of parameter decoder/layers_20/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.291039 139876724979712 adafactor.py:358] Since rank of parameter decoder/layers_2/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.292254 139876724979712 adafactor.py:358] Since rank of parameter decoder/layers_2/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:12.293423 139876724979712 adafactor.py:358] Since rank of parameter decoder/layers_2/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:12.294379 139876724979712 adafactor.py:358] Since rank of parameter decoder/layers_2/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:12.295411 139876724979712 adafactor.py:358] Since rank of parameter decoder/layers_2/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.296577 139876724979712 adafactor.py:358] Since rank of parameter decoder/layers_2/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:12.297744 139876724979712 adafactor.py:358] Since rank of parameter decoder/layers_2/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.298902 139876724979712 adafactor.py:358] Since rank of parameter decoder/layers_2/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.300150 139876724979712 adafactor.py:358] Since rank of parameter decoder/layers_20/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.301357 139876724979712 adafactor.py:358] Since rank of parameter decoder/layers_20/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.302522 139876724979712 adafactor.py:358] Since rank of parameter decoder/layers_20/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:12.303732 139876724979712 adafactor.py:358] Since rank of parameter decoder/layers_20/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:12.304747 139876724979712 adafactor.py:358] Since rank of parameter decoder/layers_20/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:12.305764 139876724979712 adafactor.py:358] Since rank of parameter decoder/layers_20/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.306927 139876724979712 adafactor.py:358] Since rank of parameter decoder/layers_20/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:12.308135 139876724979712 adafactor.py:358] Since rank of parameter decoder/layers_20/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.309305 139876724979712 adafactor.py:358] Since rank of parameter decoder/layers_20/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.310523 139876724979712 adafactor.py:358] Since rank of parameter decoder/layers_21/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.297894 139666521167872 adafactor.py:358] Since rank of parameter decoder/layers_20/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.298964 139666521167872 adafactor.py:358] Since rank of parameter decoder/layers_20/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:12.300223 139666521167872 adafactor.py:358] Since rank of parameter decoder/layers_20/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:12.301122 139666521167872 adafactor.py:358] Since rank of parameter decoder/layers_20/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:12.302039 139666521167872 adafactor.py:358] Since rank of parameter decoder/layers_20/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.303153 139666521167872 adafactor.py:358] Since rank of parameter decoder/layers_20/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:12.304226 139666521167872 adafactor.py:358] Since rank of parameter decoder/layers_20/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.305304 139666521167872 adafactor.py:358] Since rank of parameter decoder/layers_20/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.306431 139666521167872 adafactor.py:358] Since rank of parameter decoder/layers_21/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.307554 139666521167872 adafactor.py:358] Since rank of parameter decoder/layers_21/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.308630 139666521167872 adafactor.py:358] Since rank of parameter decoder/layers_21/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:12.309713 139666521167872 adafactor.py:358] Since rank of parameter decoder/layers_21/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:12.310645 139666521167872 adafactor.py:358] Since rank of parameter decoder/layers_21/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:12.311561 139666521167872 adafactor.py:358] Since rank of parameter decoder/layers_21/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.312669 139666521167872 adafactor.py:358] Since rank of parameter decoder/layers_21/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:12.313751 139666521167872 adafactor.py:358] Since rank of parameter decoder/layers_21/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.315297 139666521167872 adafactor.py:358] Since rank of parameter decoder/layers_21/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.311746 139876724979712 adafactor.py:358] Since rank of parameter decoder/layers_21/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.312932 139876724979712 adafactor.py:358] Since rank of parameter decoder/layers_21/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:12.314102 139876724979712 adafactor.py:358] Since rank of parameter decoder/layers_21/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:12.315146 139876724979712 adafactor.py:358] Since rank of parameter decoder/layers_21/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:12.316123 139876724979712 adafactor.py:358] Since rank of parameter decoder/layers_21/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.317301 139876724979712 adafactor.py:358] Since rank of parameter decoder/layers_21/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:12.318486 139876724979712 adafactor.py:358] Since rank of parameter decoder/layers_21/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.320153 139876724979712 adafactor.py:358] Since rank of parameter decoder/layers_21/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.316383 139666521167872 adafactor.py:358] Since rank of parameter decoder/layers_22/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.317462 139666521167872 adafactor.py:358] Since rank of parameter decoder/layers_22/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.318556 139666521167872 adafactor.py:358] Since rank of parameter decoder/layers_22/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:12.319729 139666521167872 adafactor.py:358] Since rank of parameter decoder/layers_22/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:12.320603 139666521167872 adafactor.py:358] Since rank of parameter decoder/layers_22/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:12.321487 139666521167872 adafactor.py:358] Since rank of parameter decoder/layers_22/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.322555 139666521167872 adafactor.py:358] Since rank of parameter decoder/layers_22/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:12.323747 139666521167872 adafactor.py:358] Since rank of parameter decoder/layers_22/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.324832 139666521167872 adafactor.py:358] Since rank of parameter decoder/layers_22/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.321349 139876724979712 adafactor.py:358] Since rank of parameter decoder/layers_22/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.322520 139876724979712 adafactor.py:358] Since rank of parameter decoder/layers_22/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.323731 139876724979712 adafactor.py:358] Since rank of parameter decoder/layers_22/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:12.324904 139876724979712 adafactor.py:358] Since rank of parameter decoder/layers_22/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:12.325799 139876724979712 adafactor.py:358] Since rank of parameter decoder/layers_22/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:12.326695 139876724979712 adafactor.py:358] Since rank of parameter decoder/layers_22/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.327822 139876724979712 adafactor.py:358] Since rank of parameter decoder/layers_22/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:12.328979 139876724979712 adafactor.py:358] Since rank of parameter decoder/layers_22/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.330067 139876724979712 adafactor.py:358] Since rank of parameter decoder/layers_22/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.325917 139666521167872 adafactor.py:358] Since rank of parameter decoder/layers_23/extra_mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.326999 139666521167872 adafactor.py:358] Since rank of parameter decoder/layers_23/extra_mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.331197 139876724979712 adafactor.py:358] Since rank of parameter decoder/layers_23/extra_mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.332329 139876724979712 adafactor.py:358] Since rank of parameter decoder/layers_23/extra_mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.333470 139876724979712 adafactor.py:358] Since rank of parameter decoder/layers_23/extra_mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:12.337624 139876724979712 adafactor.py:358] Since rank of parameter decoder/layers_23/mlp/router/router_weights/w/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>, <FactorDim.NONE: None>) is ignored.
W0512 23:45:12.328164 139666521167872 adafactor.py:358] Since rank of parameter decoder/layers_23/extra_mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:12.332246 139666521167872 adafactor.py:358] Since rank of parameter decoder/layers_23/mlp/router/router_weights/w/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>, <FactorDim.NONE: None>) is ignored.
W0512 23:45:12.333177 139666521167872 adafactor.py:358] Since rank of parameter decoder/layers_23/pre_extra_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:12.334125 139666521167872 adafactor.py:358] Since rank of parameter decoder/layers_23/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:12.335030 139666521167872 adafactor.py:358] Since rank of parameter decoder/layers_23/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:12.335939 139666521167872 adafactor.py:358] Since rank of parameter decoder/layers_23/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.337083 139666521167872 adafactor.py:358] Since rank of parameter decoder/layers_23/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:12.338326 139666521167872 adafactor.py:358] Since rank of parameter decoder/layers_23/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.338517 139876724979712 adafactor.py:358] Since rank of parameter decoder/layers_23/pre_extra_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:12.339430 139876724979712 adafactor.py:358] Since rank of parameter decoder/layers_23/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:12.339427 139666521167872 adafactor.py:358] Since rank of parameter decoder/layers_23/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.340319 139876724979712 adafactor.py:358] Since rank of parameter decoder/layers_23/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:12.340500 139666521167872 adafactor.py:358] Since rank of parameter decoder/layers_3/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.341213 139876724979712 adafactor.py:358] Since rank of parameter decoder/layers_23/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.341629 139666521167872 adafactor.py:358] Since rank of parameter decoder/layers_3/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.342355 139876724979712 adafactor.py:358] Since rank of parameter decoder/layers_23/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:12.342683 139666521167872 adafactor.py:358] Since rank of parameter decoder/layers_3/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:12.343647 139876724979712 adafactor.py:358] Since rank of parameter decoder/layers_23/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.343818 139666521167872 adafactor.py:358] Since rank of parameter decoder/layers_3/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:12.344777 139876724979712 adafactor.py:358] Since rank of parameter decoder/layers_23/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.344703 139666521167872 adafactor.py:358] Since rank of parameter decoder/layers_3/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:12.345861 139876724979712 adafactor.py:358] Since rank of parameter decoder/layers_3/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.345629 139666521167872 adafactor.py:358] Since rank of parameter decoder/layers_3/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.347009 139876724979712 adafactor.py:358] Since rank of parameter decoder/layers_3/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.346698 139666521167872 adafactor.py:358] Since rank of parameter decoder/layers_3/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:12.348124 139876724979712 adafactor.py:358] Since rank of parameter decoder/layers_3/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:12.347810 139666521167872 adafactor.py:358] Since rank of parameter decoder/layers_3/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.349217 139876724979712 adafactor.py:358] Since rank of parameter decoder/layers_3/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:12.348878 139666521167872 adafactor.py:358] Since rank of parameter decoder/layers_3/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.350118 139876724979712 adafactor.py:358] Since rank of parameter decoder/layers_3/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:12.350007 139666521167872 adafactor.py:358] Since rank of parameter decoder/layers_4/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.351059 139876724979712 adafactor.py:358] Since rank of parameter decoder/layers_3/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.351072 139666521167872 adafactor.py:358] Since rank of parameter decoder/layers_4/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.352170 139876724979712 adafactor.py:358] Since rank of parameter decoder/layers_3/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:12.352149 139666521167872 adafactor.py:358] Since rank of parameter decoder/layers_4/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:12.353250 139876724979712 adafactor.py:358] Since rank of parameter decoder/layers_3/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.353249 139666521167872 adafactor.py:358] Since rank of parameter decoder/layers_4/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:12.354171 139666521167872 adafactor.py:358] Since rank of parameter decoder/layers_4/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:12.354343 139876724979712 adafactor.py:358] Since rank of parameter decoder/layers_3/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.355050 139666521167872 adafactor.py:358] Since rank of parameter decoder/layers_4/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.355523 139876724979712 adafactor.py:358] Since rank of parameter decoder/layers_4/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.356155 139666521167872 adafactor.py:358] Since rank of parameter decoder/layers_4/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:12.356620 139876724979712 adafactor.py:358] Since rank of parameter decoder/layers_4/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.357232 139666521167872 adafactor.py:358] Since rank of parameter decoder/layers_4/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.357708 139876724979712 adafactor.py:358] Since rank of parameter decoder/layers_4/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:12.358350 139666521167872 adafactor.py:358] Since rank of parameter decoder/layers_4/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.358783 139876724979712 adafactor.py:358] Since rank of parameter decoder/layers_4/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:12.359752 139876724979712 adafactor.py:358] Since rank of parameter decoder/layers_4/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:12.359456 139666521167872 adafactor.py:358] Since rank of parameter decoder/layers_5/extra_mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.360661 139876724979712 adafactor.py:358] Since rank of parameter decoder/layers_4/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.360527 139666521167872 adafactor.py:358] Since rank of parameter decoder/layers_5/extra_mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.361742 139876724979712 adafactor.py:358] Since rank of parameter decoder/layers_4/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:12.361600 139666521167872 adafactor.py:358] Since rank of parameter decoder/layers_5/extra_mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:12.362831 139876724979712 adafactor.py:358] Since rank of parameter decoder/layers_4/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.364044 139876724979712 adafactor.py:358] Since rank of parameter decoder/layers_4/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.365149 139876724979712 adafactor.py:358] Since rank of parameter decoder/layers_5/extra_mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.365659 139666521167872 adafactor.py:358] Since rank of parameter decoder/layers_5/mlp/router/router_weights/w/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>, <FactorDim.NONE: None>) is ignored.
W0512 23:45:12.366241 139876724979712 adafactor.py:358] Since rank of parameter decoder/layers_5/extra_mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.366994 139666521167872 adafactor.py:358] Since rank of parameter decoder/layers_5/pre_extra_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:12.367347 139876724979712 adafactor.py:358] Since rank of parameter decoder/layers_5/extra_mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:12.367906 139666521167872 adafactor.py:358] Since rank of parameter decoder/layers_5/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:12.368799 139666521167872 adafactor.py:358] Since rank of parameter decoder/layers_5/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:12.369690 139666521167872 adafactor.py:358] Since rank of parameter decoder/layers_5/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.370766 139666521167872 adafactor.py:358] Since rank of parameter decoder/layers_5/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:12.371441 139876724979712 adafactor.py:358] Since rank of parameter decoder/layers_5/mlp/router/router_weights/w/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>, <FactorDim.NONE: None>) is ignored.
W0512 23:45:12.371911 139666521167872 adafactor.py:358] Since rank of parameter decoder/layers_5/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.372805 139876724979712 adafactor.py:358] Since rank of parameter decoder/layers_5/pre_extra_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:12.373022 139666521167872 adafactor.py:358] Since rank of parameter decoder/layers_5/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.373705 139876724979712 adafactor.py:358] Since rank of parameter decoder/layers_5/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:12.374095 139666521167872 adafactor.py:358] Since rank of parameter decoder/layers_6/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.374610 139876724979712 adafactor.py:358] Since rank of parameter decoder/layers_5/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:12.375189 139666521167872 adafactor.py:358] Since rank of parameter decoder/layers_6/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.375523 139876724979712 adafactor.py:358] Since rank of parameter decoder/layers_5/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.376628 139876724979712 adafactor.py:358] Since rank of parameter decoder/layers_5/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:12.376310 139666521167872 adafactor.py:358] Since rank of parameter decoder/layers_6/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:12.377381 139666521167872 adafactor.py:358] Since rank of parameter decoder/layers_6/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:12.377766 139876724979712 adafactor.py:358] Since rank of parameter decoder/layers_5/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
I0512 23:45:12.377035 140193652815872 moe_layers.py:777] Selected group_size=4096 and num_groups=192 for input num_tokens=786432, max_group_size=4096, num_experts=8 and num_expert_replicas=1
W0512 23:45:12.378253 139666521167872 adafactor.py:358] Since rank of parameter decoder/layers_6/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:12.378850 139876724979712 adafactor.py:358] Since rank of parameter decoder/layers_5/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.379148 139666521167872 adafactor.py:358] Since rank of parameter decoder/layers_6/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.379960 139876724979712 adafactor.py:358] Since rank of parameter decoder/layers_6/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.380267 139666521167872 adafactor.py:358] Since rank of parameter decoder/layers_6/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:12.381046 139876724979712 adafactor.py:358] Since rank of parameter decoder/layers_6/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.381339 139666521167872 adafactor.py:358] Since rank of parameter decoder/layers_6/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.382179 139876724979712 adafactor.py:358] Since rank of parameter decoder/layers_6/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:12.382400 139666521167872 adafactor.py:358] Since rank of parameter decoder/layers_6/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.383276 139876724979712 adafactor.py:358] Since rank of parameter decoder/layers_6/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:12.383514 139666521167872 adafactor.py:358] Since rank of parameter decoder/layers_7/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.384212 139876724979712 adafactor.py:358] Since rank of parameter decoder/layers_6/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:12.385108 139876724979712 adafactor.py:358] Since rank of parameter decoder/layers_6/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.384638 139666521167872 adafactor.py:358] Since rank of parameter decoder/layers_7/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.386235 139876724979712 adafactor.py:358] Since rank of parameter decoder/layers_6/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:12.385714 139666521167872 adafactor.py:358] Since rank of parameter decoder/layers_7/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:12.386770 139666521167872 adafactor.py:358] Since rank of parameter decoder/layers_7/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:12.387365 139876724979712 adafactor.py:358] Since rank of parameter decoder/layers_6/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.387687 139666521167872 adafactor.py:358] Since rank of parameter decoder/layers_7/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:12.388435 139876724979712 adafactor.py:358] Since rank of parameter decoder/layers_6/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.388623 139666521167872 adafactor.py:358] Since rank of parameter decoder/layers_7/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.389511 139876724979712 adafactor.py:358] Since rank of parameter decoder/layers_7/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.389687 139666521167872 adafactor.py:358] Since rank of parameter decoder/layers_7/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:12.390641 139876724979712 adafactor.py:358] Since rank of parameter decoder/layers_7/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.390750 139666521167872 adafactor.py:358] Since rank of parameter decoder/layers_7/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.391758 139876724979712 adafactor.py:358] Since rank of parameter decoder/layers_7/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:12.391844 139666521167872 adafactor.py:358] Since rank of parameter decoder/layers_7/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.392829 139876724979712 adafactor.py:358] Since rank of parameter decoder/layers_7/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:12.392990 139666521167872 adafactor.py:358] Since rank of parameter decoder/layers_8/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.393713 139876724979712 adafactor.py:358] Since rank of parameter decoder/layers_7/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:12.394659 139876724979712 adafactor.py:358] Since rank of parameter decoder/layers_7/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.394063 139666521167872 adafactor.py:358] Since rank of parameter decoder/layers_8/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.395155 139666521167872 adafactor.py:358] Since rank of parameter decoder/layers_8/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:12.395758 139876724979712 adafactor.py:358] Since rank of parameter decoder/layers_7/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:12.396234 139666521167872 adafactor.py:358] Since rank of parameter decoder/layers_8/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:12.396839 139876724979712 adafactor.py:358] Since rank of parameter decoder/layers_7/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.397166 139666521167872 adafactor.py:358] Since rank of parameter decoder/layers_8/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:12.397921 139876724979712 adafactor.py:358] Since rank of parameter decoder/layers_7/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.398045 139666521167872 adafactor.py:358] Since rank of parameter decoder/layers_8/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.399049 139876724979712 adafactor.py:358] Since rank of parameter decoder/layers_8/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.399123 139666521167872 adafactor.py:358] Since rank of parameter decoder/layers_8/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:12.400168 139876724979712 adafactor.py:358] Since rank of parameter decoder/layers_8/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.400201 139666521167872 adafactor.py:358] Since rank of parameter decoder/layers_8/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.401248 139876724979712 adafactor.py:358] Since rank of parameter decoder/layers_8/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:12.401324 139666521167872 adafactor.py:358] Since rank of parameter decoder/layers_8/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.402334 139876724979712 adafactor.py:358] Since rank of parameter decoder/layers_8/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:12.402413 139666521167872 adafactor.py:358] Since rank of parameter decoder/layers_9/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.403298 139876724979712 adafactor.py:358] Since rank of parameter decoder/layers_8/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:12.403521 139666521167872 adafactor.py:358] Since rank of parameter decoder/layers_9/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.404232 139876724979712 adafactor.py:358] Since rank of parameter decoder/layers_8/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.404589 139666521167872 adafactor.py:358] Since rank of parameter decoder/layers_9/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:12.405319 139876724979712 adafactor.py:358] Since rank of parameter decoder/layers_8/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:12.405698 139666521167872 adafactor.py:358] Since rank of parameter decoder/layers_9/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:12.406388 139876724979712 adafactor.py:358] Since rank of parameter decoder/layers_8/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.406584 139666521167872 adafactor.py:358] Since rank of parameter decoder/layers_9/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:12.407555 139876724979712 adafactor.py:358] Since rank of parameter decoder/layers_8/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.407497 139666521167872 adafactor.py:358] Since rank of parameter decoder/layers_9/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.408661 139876724979712 adafactor.py:358] Since rank of parameter decoder/layers_9/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.408565 139666521167872 adafactor.py:358] Since rank of parameter decoder/layers_9/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:12.409630 139666521167872 adafactor.py:358] Since rank of parameter decoder/layers_9/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.409736 139876724979712 adafactor.py:358] Since rank of parameter decoder/layers_9/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.410739 139666521167872 adafactor.py:358] Since rank of parameter decoder/layers_9/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.410818 139876724979712 adafactor.py:358] Since rank of parameter decoder/layers_9/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:12.411835 139666521167872 adafactor.py:358] Since rank of parameter decoder/logits_dense/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.411963 139876724979712 adafactor.py:358] Since rank of parameter decoder/layers_9/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:12.412867 139876724979712 adafactor.py:358] Since rank of parameter decoder/layers_9/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:12.412954 139666521167872 adafactor.py:358] Since rank of parameter token_embedder/embedding 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:12.413773 139876724979712 adafactor.py:358] Since rank of parameter decoder/layers_9/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.414854 139876724979712 adafactor.py:358] Since rank of parameter decoder/layers_9/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:12.415974 139876724979712 adafactor.py:358] Since rank of parameter decoder/layers_9/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.417113 139876724979712 adafactor.py:358] Since rank of parameter decoder/layers_9/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.418185 139876724979712 adafactor.py:358] Since rank of parameter decoder/logits_dense/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.419308 139876724979712 adafactor.py:358] Since rank of parameter token_embedder/embedding 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
I0512 23:45:12.436828 139856202532864 moe_layers.py:777] Selected group_size=4096 and num_groups=192 for input num_tokens=786432, max_group_size=4096, num_experts=8 and num_expert_replicas=1
W0512 23:45:12.537305 139856202532864 adafactor.py:358] Since rank of parameter decoder/decoder_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:12.538368 139856202532864 adafactor.py:358] Since rank of parameter decoder/layers_0/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.539556 139856202532864 adafactor.py:358] Since rank of parameter decoder/layers_0/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.540653 139856202532864 adafactor.py:358] Since rank of parameter decoder/layers_0/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:12.541911 139856202532864 adafactor.py:358] Since rank of parameter decoder/layers_0/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:12.542811 139856202532864 adafactor.py:358] Since rank of parameter decoder/layers_0/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:12.543714 139856202532864 adafactor.py:358] Since rank of parameter decoder/layers_0/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.544821 139856202532864 adafactor.py:358] Since rank of parameter decoder/layers_0/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:12.545957 139856202532864 adafactor.py:358] Since rank of parameter decoder/layers_0/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.547125 139856202532864 adafactor.py:358] Since rank of parameter decoder/layers_0/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.548228 139856202532864 adafactor.py:358] Since rank of parameter decoder/layers_1/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.549323 139856202532864 adafactor.py:358] Since rank of parameter decoder/layers_1/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.550462 139856202532864 adafactor.py:358] Since rank of parameter decoder/layers_1/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:12.551638 139856202532864 adafactor.py:358] Since rank of parameter decoder/layers_1/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:12.552533 139856202532864 adafactor.py:358] Since rank of parameter decoder/layers_1/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:12.553418 139856202532864 adafactor.py:358] Since rank of parameter decoder/layers_1/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.554570 139856202532864 adafactor.py:358] Since rank of parameter decoder/layers_1/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:12.555734 139856202532864 adafactor.py:358] Since rank of parameter decoder/layers_1/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.556835 139856202532864 adafactor.py:358] Since rank of parameter decoder/layers_1/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.557966 139856202532864 adafactor.py:358] Since rank of parameter decoder/layers_10/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.559069 139856202532864 adafactor.py:358] Since rank of parameter decoder/layers_10/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.560561 139856202532864 adafactor.py:358] Since rank of parameter decoder/layers_10/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:12.561661 139856202532864 adafactor.py:358] Since rank of parameter decoder/layers_10/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:12.562619 139856202532864 adafactor.py:358] Since rank of parameter decoder/layers_10/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:12.563491 139856202532864 adafactor.py:358] Since rank of parameter decoder/layers_10/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.564787 139856202532864 adafactor.py:358] Since rank of parameter decoder/layers_10/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
I0512 23:45:12.546895 140180528838656 moe_layers.py:777] Selected group_size=4096 and num_groups=192 for input num_tokens=786432, max_group_size=4096, num_experts=8 and num_expert_replicas=1
W0512 23:45:12.566066 139856202532864 adafactor.py:358] Since rank of parameter decoder/layers_10/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.567160 139856202532864 adafactor.py:358] Since rank of parameter decoder/layers_10/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.568258 139856202532864 adafactor.py:358] Since rank of parameter decoder/layers_11/extra_mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.569413 139856202532864 adafactor.py:358] Since rank of parameter decoder/layers_11/extra_mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.570560 139856202532864 adafactor.py:358] Since rank of parameter decoder/layers_11/extra_mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:12.574788 139856202532864 adafactor.py:358] Since rank of parameter decoder/layers_11/mlp/router/router_weights/w/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>, <FactorDim.NONE: None>) is ignored.
W0512 23:45:12.575715 139856202532864 adafactor.py:358] Since rank of parameter decoder/layers_11/pre_extra_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:12.576613 139856202532864 adafactor.py:358] Since rank of parameter decoder/layers_11/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:12.577529 139856202532864 adafactor.py:358] Since rank of parameter decoder/layers_11/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:12.578554 139856202532864 adafactor.py:358] Since rank of parameter decoder/layers_11/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.579664 139856202532864 adafactor.py:358] Since rank of parameter decoder/layers_11/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:12.580770 139856202532864 adafactor.py:358] Since rank of parameter decoder/layers_11/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.581941 139856202532864 adafactor.py:358] Since rank of parameter decoder/layers_11/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.583152 139856202532864 adafactor.py:358] Since rank of parameter decoder/layers_12/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.584260 139856202532864 adafactor.py:358] Since rank of parameter decoder/layers_12/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.585353 139856202532864 adafactor.py:358] Since rank of parameter decoder/layers_12/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:12.586480 139856202532864 adafactor.py:358] Since rank of parameter decoder/layers_12/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:12.588156 139856202532864 adafactor.py:358] Since rank of parameter decoder/layers_12/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:12.589057 139856202532864 adafactor.py:358] Since rank of parameter decoder/layers_12/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.590173 139856202532864 adafactor.py:358] Since rank of parameter decoder/layers_12/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:12.591275 139856202532864 adafactor.py:358] Since rank of parameter decoder/layers_12/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.592449 139856202532864 adafactor.py:358] Since rank of parameter decoder/layers_12/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.593542 139856202532864 adafactor.py:358] Since rank of parameter decoder/layers_13/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
I0512 23:45:12.574436 140260698863616 moe_layers.py:777] Selected group_size=4096 and num_groups=192 for input num_tokens=786432, max_group_size=4096, num_experts=8 and num_expert_replicas=1
W0512 23:45:12.594652 139856202532864 adafactor.py:358] Since rank of parameter decoder/layers_13/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.595734 139856202532864 adafactor.py:358] Since rank of parameter decoder/layers_13/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:12.596893 139856202532864 adafactor.py:358] Since rank of parameter decoder/layers_13/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:12.598490 139856202532864 adafactor.py:358] Since rank of parameter decoder/layers_13/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:12.599394 139856202532864 adafactor.py:358] Since rank of parameter decoder/layers_13/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.600539 139856202532864 adafactor.py:358] Since rank of parameter decoder/layers_13/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:12.601686 139856202532864 adafactor.py:358] Since rank of parameter decoder/layers_13/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.602839 139856202532864 adafactor.py:358] Since rank of parameter decoder/layers_13/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.603930 139856202532864 adafactor.py:358] Since rank of parameter decoder/layers_14/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.605013 139856202532864 adafactor.py:358] Since rank of parameter decoder/layers_14/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.606188 139856202532864 adafactor.py:358] Since rank of parameter decoder/layers_14/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:12.607274 139856202532864 adafactor.py:358] Since rank of parameter decoder/layers_14/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:12.608170 139856202532864 adafactor.py:358] Since rank of parameter decoder/layers_14/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:12.646412 140180528838656 adafactor.py:358] Since rank of parameter decoder/decoder_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:12.647459 140180528838656 adafactor.py:358] Since rank of parameter decoder/layers_0/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.648624 140180528838656 adafactor.py:358] Since rank of parameter decoder/layers_0/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
I0512 23:45:12.664102 140143678494720 utils.py:952] Using latest T5X checkpoint.
W0512 23:45:12.649781 140180528838656 adafactor.py:358] Since rank of parameter decoder/layers_0/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:12.650981 140180528838656 adafactor.py:358] Since rank of parameter decoder/layers_0/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:12.651888 140180528838656 adafactor.py:358] Since rank of parameter decoder/layers_0/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:12.652796 140180528838656 adafactor.py:358] Since rank of parameter decoder/layers_0/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.653924 140180528838656 adafactor.py:358] Since rank of parameter decoder/layers_0/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:12.655042 140180528838656 adafactor.py:358] Since rank of parameter decoder/layers_0/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.656225 140180528838656 adafactor.py:358] Since rank of parameter decoder/layers_0/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.657353 140180528838656 adafactor.py:358] Since rank of parameter decoder/layers_1/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.658463 140180528838656 adafactor.py:358] Since rank of parameter decoder/layers_1/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.659547 140180528838656 adafactor.py:358] Since rank of parameter decoder/layers_1/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:12.660718 140180528838656 adafactor.py:358] Since rank of parameter decoder/layers_1/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:12.661643 140180528838656 adafactor.py:358] Since rank of parameter decoder/layers_1/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:12.662551 140180528838656 adafactor.py:358] Since rank of parameter decoder/layers_1/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.663652 140180528838656 adafactor.py:358] Since rank of parameter decoder/layers_1/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:12.664827 140180528838656 adafactor.py:358] Since rank of parameter decoder/layers_1/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.665952 140180528838656 adafactor.py:358] Since rank of parameter decoder/layers_1/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.667051 140180528838656 adafactor.py:358] Since rank of parameter decoder/layers_10/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.668156 140180528838656 adafactor.py:358] Since rank of parameter decoder/layers_10/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.669366 140180528838656 adafactor.py:358] Since rank of parameter decoder/layers_10/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:12.670475 140180528838656 adafactor.py:358] Since rank of parameter decoder/layers_10/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:12.671374 140180528838656 adafactor.py:358] Since rank of parameter decoder/layers_10/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:12.672262 140180528838656 adafactor.py:358] Since rank of parameter decoder/layers_10/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.673454 140180528838656 adafactor.py:358] Since rank of parameter decoder/layers_10/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:12.674565 140180528838656 adafactor.py:358] Since rank of parameter decoder/layers_10/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.675654 140180528838656 adafactor.py:358] Since rank of parameter decoder/layers_10/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.676754 140180528838656 adafactor.py:358] Since rank of parameter decoder/layers_11/extra_mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.677927 140180528838656 adafactor.py:358] Since rank of parameter decoder/layers_11/extra_mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.679027 140180528838656 adafactor.py:358] Since rank of parameter decoder/layers_11/extra_mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:12.683362 140180528838656 adafactor.py:358] Since rank of parameter decoder/layers_11/mlp/router/router_weights/w/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>, <FactorDim.NONE: None>) is ignored.
W0512 23:45:12.684285 140180528838656 adafactor.py:358] Since rank of parameter decoder/layers_11/pre_extra_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
I0512 23:45:12.701922 140193652815872 moe_layers.py:777] Selected group_size=4096 and num_groups=192 for input num_tokens=786432, max_group_size=4096, num_experts=8 and num_expert_replicas=1
W0512 23:45:12.685226 140180528838656 adafactor.py:358] Since rank of parameter decoder/layers_11/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:12.686114 140180528838656 adafactor.py:358] Since rank of parameter decoder/layers_11/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:12.687074 140180528838656 adafactor.py:358] Since rank of parameter decoder/layers_11/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.609069 139856202532864 adafactor.py:358] Since rank of parameter decoder/layers_14/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.610170 139856202532864 adafactor.py:358] Since rank of parameter decoder/layers_14/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:12.611307 139856202532864 adafactor.py:358] Since rank of parameter decoder/layers_14/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.612397 139856202532864 adafactor.py:358] Since rank of parameter decoder/layers_14/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.613489 139856202532864 adafactor.py:358] Since rank of parameter decoder/layers_15/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.614646 139856202532864 adafactor.py:358] Since rank of parameter decoder/layers_15/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.615814 139856202532864 adafactor.py:358] Since rank of parameter decoder/layers_15/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:12.616916 139856202532864 adafactor.py:358] Since rank of parameter decoder/layers_15/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:12.617845 139856202532864 adafactor.py:358] Since rank of parameter decoder/layers_15/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:12.618751 139856202532864 adafactor.py:358] Since rank of parameter decoder/layers_15/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.619909 139856202532864 adafactor.py:358] Since rank of parameter decoder/layers_15/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:12.621000 139856202532864 adafactor.py:358] Since rank of parameter decoder/layers_15/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.622318 139856202532864 adafactor.py:358] Since rank of parameter decoder/layers_15/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.623418 139856202532864 adafactor.py:358] Since rank of parameter decoder/layers_16/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.624585 139856202532864 adafactor.py:358] Since rank of parameter decoder/layers_16/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.625722 139856202532864 adafactor.py:358] Since rank of parameter decoder/layers_16/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:12.626818 139856202532864 adafactor.py:358] Since rank of parameter decoder/layers_16/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:12.688170 140180528838656 adafactor.py:358] Since rank of parameter decoder/layers_11/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:12.627729 139856202532864 adafactor.py:358] Since rank of parameter decoder/layers_16/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:12.628676 139856202532864 adafactor.py:358] Since rank of parameter decoder/layers_16/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.629799 139856202532864 adafactor.py:358] Since rank of parameter decoder/layers_16/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:12.630912 139856202532864 adafactor.py:358] Since rank of parameter decoder/layers_16/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.632017 139856202532864 adafactor.py:358] Since rank of parameter decoder/layers_16/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.633164 139856202532864 adafactor.py:358] Since rank of parameter decoder/layers_17/extra_mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.634286 139856202532864 adafactor.py:358] Since rank of parameter decoder/layers_17/extra_mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.635398 139856202532864 adafactor.py:358] Since rank of parameter decoder/layers_17/extra_mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:12.639564 139856202532864 adafactor.py:358] Since rank of parameter decoder/layers_17/mlp/router/router_weights/w/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>, <FactorDim.NONE: None>) is ignored.
W0512 23:45:12.640474 139856202532864 adafactor.py:358] Since rank of parameter decoder/layers_17/pre_extra_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:12.641900 139856202532864 adafactor.py:358] Since rank of parameter decoder/layers_17/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:12.642797 139856202532864 adafactor.py:358] Since rank of parameter decoder/layers_17/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:12.643696 139856202532864 adafactor.py:358] Since rank of parameter decoder/layers_17/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.644802 139856202532864 adafactor.py:358] Since rank of parameter decoder/layers_17/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:12.645941 139856202532864 adafactor.py:358] Since rank of parameter decoder/layers_17/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.647099 139856202532864 adafactor.py:358] Since rank of parameter decoder/layers_17/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.648195 139856202532864 adafactor.py:358] Since rank of parameter decoder/layers_18/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.689302 140180528838656 adafactor.py:358] Since rank of parameter decoder/layers_11/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.649292 139856202532864 adafactor.py:358] Since rank of parameter decoder/layers_18/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.650406 139856202532864 adafactor.py:358] Since rank of parameter decoder/layers_18/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:12.651559 139856202532864 adafactor.py:358] Since rank of parameter decoder/layers_18/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:12.652447 139856202532864 adafactor.py:358] Since rank of parameter decoder/layers_18/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:12.653338 139856202532864 adafactor.py:358] Since rank of parameter decoder/layers_18/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.654470 139856202532864 adafactor.py:358] Since rank of parameter decoder/layers_18/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:12.655620 139856202532864 adafactor.py:358] Since rank of parameter decoder/layers_18/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.656700 139856202532864 adafactor.py:358] Since rank of parameter decoder/layers_18/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.657839 139856202532864 adafactor.py:358] Since rank of parameter decoder/layers_19/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.690392 140180528838656 adafactor.py:358] Since rank of parameter decoder/layers_11/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.658953 139856202532864 adafactor.py:358] Since rank of parameter decoder/layers_19/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.660115 139856202532864 adafactor.py:358] Since rank of parameter decoder/layers_19/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:12.661213 139856202532864 adafactor.py:358] Since rank of parameter decoder/layers_19/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:12.662201 139856202532864 adafactor.py:358] Since rank of parameter decoder/layers_19/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:12.663080 139856202532864 adafactor.py:358] Since rank of parameter decoder/layers_19/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.664238 139856202532864 adafactor.py:358] Since rank of parameter decoder/layers_19/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:12.665328 139856202532864 adafactor.py:358] Since rank of parameter decoder/layers_19/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.666440 139856202532864 adafactor.py:358] Since rank of parameter decoder/layers_19/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.691604 140180528838656 adafactor.py:358] Since rank of parameter decoder/layers_12/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.667561 139856202532864 adafactor.py:358] Since rank of parameter decoder/layers_2/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.668700 139856202532864 adafactor.py:358] Since rank of parameter decoder/layers_2/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.669836 139856202532864 adafactor.py:358] Since rank of parameter decoder/layers_2/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:12.670940 139856202532864 adafactor.py:358] Since rank of parameter decoder/layers_2/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:12.671842 139856202532864 adafactor.py:358] Since rank of parameter decoder/layers_2/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:12.672775 139856202532864 adafactor.py:358] Since rank of parameter decoder/layers_2/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.673896 139856202532864 adafactor.py:358] Since rank of parameter decoder/layers_2/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:12.674987 139856202532864 adafactor.py:358] Since rank of parameter decoder/layers_2/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.676077 139856202532864 adafactor.py:358] Since rank of parameter decoder/layers_2/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.692710 140180528838656 adafactor.py:358] Since rank of parameter decoder/layers_12/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.677220 139856202532864 adafactor.py:358] Since rank of parameter decoder/layers_20/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.678346 139856202532864 adafactor.py:358] Since rank of parameter decoder/layers_20/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.679423 139856202532864 adafactor.py:358] Since rank of parameter decoder/layers_20/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:12.680528 139856202532864 adafactor.py:358] Since rank of parameter decoder/layers_20/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:12.681433 139856202532864 adafactor.py:358] Since rank of parameter decoder/layers_20/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:12.682439 139856202532864 adafactor.py:358] Since rank of parameter decoder/layers_20/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.683560 139856202532864 adafactor.py:358] Since rank of parameter decoder/layers_20/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:12.684654 139856202532864 adafactor.py:358] Since rank of parameter decoder/layers_20/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.685773 139856202532864 adafactor.py:358] Since rank of parameter decoder/layers_20/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.693829 140180528838656 adafactor.py:358] Since rank of parameter decoder/layers_12/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:12.686922 139856202532864 adafactor.py:358] Since rank of parameter decoder/layers_21/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.688021 139856202532864 adafactor.py:358] Since rank of parameter decoder/layers_21/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.689119 139856202532864 adafactor.py:358] Since rank of parameter decoder/layers_21/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:12.690258 139856202532864 adafactor.py:358] Since rank of parameter decoder/layers_21/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:12.691211 139856202532864 adafactor.py:358] Since rank of parameter decoder/layers_21/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:12.692117 139856202532864 adafactor.py:358] Since rank of parameter decoder/layers_21/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.693210 139856202532864 adafactor.py:358] Since rank of parameter decoder/layers_21/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:12.694340 139856202532864 adafactor.py:358] Since rank of parameter decoder/layers_21/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.695923 139856202532864 adafactor.py:358] Since rank of parameter decoder/layers_21/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.697013 139856202532864 adafactor.py:358] Since rank of parameter decoder/layers_22/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.698143 139856202532864 adafactor.py:358] Since rank of parameter decoder/layers_22/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.699253 139856202532864 adafactor.py:358] Since rank of parameter decoder/layers_22/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:12.700390 139856202532864 adafactor.py:358] Since rank of parameter decoder/layers_22/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:12.701292 139856202532864 adafactor.py:358] Since rank of parameter decoder/layers_22/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:12.702259 139856202532864 adafactor.py:358] Since rank of parameter decoder/layers_22/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.703336 139856202532864 adafactor.py:358] Since rank of parameter decoder/layers_22/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:12.704475 139856202532864 adafactor.py:358] Since rank of parameter decoder/layers_22/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.694934 140180528838656 adafactor.py:358] Since rank of parameter decoder/layers_12/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:12.705554 139856202532864 adafactor.py:358] Since rank of parameter decoder/layers_22/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.706652 139856202532864 adafactor.py:358] Since rank of parameter decoder/layers_23/extra_mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.707746 139856202532864 adafactor.py:358] Since rank of parameter decoder/layers_23/extra_mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.708892 139856202532864 adafactor.py:358] Since rank of parameter decoder/layers_23/extra_mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:12.712980 139856202532864 adafactor.py:358] Since rank of parameter decoder/layers_23/mlp/router/router_weights/w/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>, <FactorDim.NONE: None>) is ignored.
W0512 23:45:12.713920 139856202532864 adafactor.py:358] Since rank of parameter decoder/layers_23/pre_extra_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:12.696719 140180528838656 adafactor.py:358] Since rank of parameter decoder/layers_12/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:12.714803 139856202532864 adafactor.py:358] Since rank of parameter decoder/layers_23/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:12.715689 139856202532864 adafactor.py:358] Since rank of parameter decoder/layers_23/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:12.697666 140180528838656 adafactor.py:358] Since rank of parameter decoder/layers_12/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.698785 140180528838656 adafactor.py:358] Since rank of parameter decoder/layers_12/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:12.699882 140180528838656 adafactor.py:358] Since rank of parameter decoder/layers_12/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.716581 139856202532864 adafactor.py:358] Since rank of parameter decoder/layers_23/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.717747 139856202532864 adafactor.py:358] Since rank of parameter decoder/layers_23/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:12.719115 139856202532864 adafactor.py:358] Since rank of parameter decoder/layers_23/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.701062 140180528838656 adafactor.py:358] Since rank of parameter decoder/layers_12/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.720212 139856202532864 adafactor.py:358] Since rank of parameter decoder/layers_23/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.702157 140180528838656 adafactor.py:358] Since rank of parameter decoder/layers_13/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.721296 139856202532864 adafactor.py:358] Since rank of parameter decoder/layers_3/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.703266 140180528838656 adafactor.py:358] Since rank of parameter decoder/layers_13/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.704361 140180528838656 adafactor.py:358] Since rank of parameter decoder/layers_13/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:12.722495 139856202532864 adafactor.py:358] Since rank of parameter decoder/layers_3/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.723577 139856202532864 adafactor.py:358] Since rank of parameter decoder/layers_3/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:12.705577 140180528838656 adafactor.py:358] Since rank of parameter decoder/layers_13/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:12.724651 139856202532864 adafactor.py:358] Since rank of parameter decoder/layers_3/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:12.707235 140180528838656 adafactor.py:358] Since rank of parameter decoder/layers_13/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:12.725539 139856202532864 adafactor.py:358] Since rank of parameter decoder/layers_3/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:12.708119 140180528838656 adafactor.py:358] Since rank of parameter decoder/layers_13/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.726494 139856202532864 adafactor.py:358] Since rank of parameter decoder/layers_3/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.709237 140180528838656 adafactor.py:358] Since rank of parameter decoder/layers_13/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:12.727589 139856202532864 adafactor.py:358] Since rank of parameter decoder/layers_3/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:12.710384 140180528838656 adafactor.py:358] Since rank of parameter decoder/layers_13/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.728662 139856202532864 adafactor.py:358] Since rank of parameter decoder/layers_3/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.711486 140180528838656 adafactor.py:358] Since rank of parameter decoder/layers_13/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.729765 139856202532864 adafactor.py:358] Since rank of parameter decoder/layers_3/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.712581 140180528838656 adafactor.py:358] Since rank of parameter decoder/layers_14/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.730904 139856202532864 adafactor.py:358] Since rank of parameter decoder/layers_4/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.713703 140180528838656 adafactor.py:358] Since rank of parameter decoder/layers_14/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.731987 139856202532864 adafactor.py:358] Since rank of parameter decoder/layers_4/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.714848 140180528838656 adafactor.py:358] Since rank of parameter decoder/layers_14/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:12.733071 139856202532864 adafactor.py:358] Since rank of parameter decoder/layers_4/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:12.734184 139856202532864 adafactor.py:358] Since rank of parameter decoder/layers_4/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:12.715935 140180528838656 adafactor.py:358] Since rank of parameter decoder/layers_14/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:12.735127 139856202532864 adafactor.py:358] Since rank of parameter decoder/layers_4/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:12.736015 139856202532864 adafactor.py:358] Since rank of parameter decoder/layers_4/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.737100 139856202532864 adafactor.py:358] Since rank of parameter decoder/layers_4/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:12.738232 139856202532864 adafactor.py:358] Since rank of parameter decoder/layers_4/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.739381 139856202532864 adafactor.py:358] Since rank of parameter decoder/layers_4/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.740484 139856202532864 adafactor.py:358] Since rank of parameter decoder/layers_5/extra_mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.741572 139856202532864 adafactor.py:358] Since rank of parameter decoder/layers_5/extra_mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.742715 139856202532864 adafactor.py:358] Since rank of parameter decoder/layers_5/extra_mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:12.747247 139856202532864 adafactor.py:358] Since rank of parameter decoder/layers_5/mlp/router/router_weights/w/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>, <FactorDim.NONE: None>) is ignored.
W0512 23:45:12.748655 139856202532864 adafactor.py:358] Since rank of parameter decoder/layers_5/pre_extra_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:12.749571 139856202532864 adafactor.py:358] Since rank of parameter decoder/layers_5/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:12.716838 140180528838656 adafactor.py:358] Since rank of parameter decoder/layers_14/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:12.717758 140180528838656 adafactor.py:358] Since rank of parameter decoder/layers_14/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.718845 140180528838656 adafactor.py:358] Since rank of parameter decoder/layers_14/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:12.719984 140180528838656 adafactor.py:358] Since rank of parameter decoder/layers_14/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.721085 140180528838656 adafactor.py:358] Since rank of parameter decoder/layers_14/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.722168 140180528838656 adafactor.py:358] Since rank of parameter decoder/layers_15/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.723290 140180528838656 adafactor.py:358] Since rank of parameter decoder/layers_15/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.724447 140180528838656 adafactor.py:358] Since rank of parameter decoder/layers_15/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:12.750563 139856202532864 adafactor.py:358] Since rank of parameter decoder/layers_5/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:12.751764 139856202532864 adafactor.py:358] Since rank of parameter decoder/layers_5/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.752874 139856202532864 adafactor.py:358] Since rank of parameter decoder/layers_5/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:12.754046 139856202532864 adafactor.py:358] Since rank of parameter decoder/layers_5/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.755130 139856202532864 adafactor.py:358] Since rank of parameter decoder/layers_5/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.756196 139856202532864 adafactor.py:358] Since rank of parameter decoder/layers_6/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.757280 139856202532864 adafactor.py:358] Since rank of parameter decoder/layers_6/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.758454 139856202532864 adafactor.py:358] Since rank of parameter decoder/layers_6/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:12.759527 139856202532864 adafactor.py:358] Since rank of parameter decoder/layers_6/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:12.760415 139856202532864 adafactor.py:358] Since rank of parameter decoder/layers_6/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:12.761297 139856202532864 adafactor.py:358] Since rank of parameter decoder/layers_6/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.762474 139856202532864 adafactor.py:358] Since rank of parameter decoder/layers_6/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:12.763586 139856202532864 adafactor.py:358] Since rank of parameter decoder/layers_6/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.764662 139856202532864 adafactor.py:358] Since rank of parameter decoder/layers_6/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.765760 139856202532864 adafactor.py:358] Since rank of parameter decoder/layers_7/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.766996 139856202532864 adafactor.py:358] Since rank of parameter decoder/layers_7/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.768169 139856202532864 adafactor.py:358] Since rank of parameter decoder/layers_7/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:12.769250 139856202532864 adafactor.py:358] Since rank of parameter decoder/layers_7/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:12.770181 139856202532864 adafactor.py:358] Since rank of parameter decoder/layers_7/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:12.771115 139856202532864 adafactor.py:358] Since rank of parameter decoder/layers_7/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.772189 139856202532864 adafactor.py:358] Since rank of parameter decoder/layers_7/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:12.773265 139856202532864 adafactor.py:358] Since rank of parameter decoder/layers_7/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.774370 139856202532864 adafactor.py:358] Since rank of parameter decoder/layers_7/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.775502 139856202532864 adafactor.py:358] Since rank of parameter decoder/layers_8/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.776583 139856202532864 adafactor.py:358] Since rank of parameter decoder/layers_8/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.777660 139856202532864 adafactor.py:358] Since rank of parameter decoder/layers_8/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:12.778765 139856202532864 adafactor.py:358] Since rank of parameter decoder/layers_8/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:12.779696 139856202532864 adafactor.py:358] Since rank of parameter decoder/layers_8/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:12.780575 139856202532864 adafactor.py:358] Since rank of parameter decoder/layers_8/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.781629 139856202532864 adafactor.py:358] Since rank of parameter decoder/layers_8/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:12.782749 139856202532864 adafactor.py:358] Since rank of parameter decoder/layers_8/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.783868 139856202532864 adafactor.py:358] Since rank of parameter decoder/layers_8/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.784964 139856202532864 adafactor.py:358] Since rank of parameter decoder/layers_9/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.786078 139856202532864 adafactor.py:358] Since rank of parameter decoder/layers_9/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.787150 139856202532864 adafactor.py:358] Since rank of parameter decoder/layers_9/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:12.788264 139856202532864 adafactor.py:358] Since rank of parameter decoder/layers_9/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:12.789156 139856202532864 adafactor.py:358] Since rank of parameter decoder/layers_9/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:12.790057 139856202532864 adafactor.py:358] Since rank of parameter decoder/layers_9/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.791125 139856202532864 adafactor.py:358] Since rank of parameter decoder/layers_9/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:12.792189 139856202532864 adafactor.py:358] Since rank of parameter decoder/layers_9/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.793313 139856202532864 adafactor.py:358] Since rank of parameter decoder/layers_9/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.794421 139856202532864 adafactor.py:358] Since rank of parameter decoder/logits_dense/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.795495 139856202532864 adafactor.py:358] Since rank of parameter token_embedder/embedding 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:12.801466 140193652815872 adafactor.py:358] Since rank of parameter decoder/decoder_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:12.802521 140193652815872 adafactor.py:358] Since rank of parameter decoder/layers_0/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.803720 140193652815872 adafactor.py:358] Since rank of parameter decoder/layers_0/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.804855 140193652815872 adafactor.py:358] Since rank of parameter decoder/layers_0/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:12.806057 140193652815872 adafactor.py:358] Since rank of parameter decoder/layers_0/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:12.806960 140193652815872 adafactor.py:358] Since rank of parameter decoder/layers_0/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:12.807862 140193652815872 adafactor.py:358] Since rank of parameter decoder/layers_0/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.808997 140193652815872 adafactor.py:358] Since rank of parameter decoder/layers_0/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:12.810102 140193652815872 adafactor.py:358] Since rank of parameter decoder/layers_0/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.811279 140193652815872 adafactor.py:358] Since rank of parameter decoder/layers_0/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.812405 140193652815872 adafactor.py:358] Since rank of parameter decoder/layers_1/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.813498 140193652815872 adafactor.py:358] Since rank of parameter decoder/layers_1/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.814582 140193652815872 adafactor.py:358] Since rank of parameter decoder/layers_1/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:12.815744 140193652815872 adafactor.py:358] Since rank of parameter decoder/layers_1/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:12.816686 140193652815872 adafactor.py:358] Since rank of parameter decoder/layers_1/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:12.817581 140193652815872 adafactor.py:358] Since rank of parameter decoder/layers_1/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.818675 140193652815872 adafactor.py:358] Since rank of parameter decoder/layers_1/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:12.819831 140193652815872 adafactor.py:358] Since rank of parameter decoder/layers_1/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.820955 140193652815872 adafactor.py:358] Since rank of parameter decoder/layers_1/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.822056 140193652815872 adafactor.py:358] Since rank of parameter decoder/layers_10/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.823195 140193652815872 adafactor.py:358] Since rank of parameter decoder/layers_10/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.824862 140193652815872 adafactor.py:358] Since rank of parameter decoder/layers_10/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:12.825955 140193652815872 adafactor.py:358] Since rank of parameter decoder/layers_10/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:12.826852 140193652815872 adafactor.py:358] Since rank of parameter decoder/layers_10/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:12.827739 140193652815872 adafactor.py:358] Since rank of parameter decoder/layers_10/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.828923 140193652815872 adafactor.py:358] Since rank of parameter decoder/layers_10/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:12.830036 140193652815872 adafactor.py:358] Since rank of parameter decoder/layers_10/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.831124 140193652815872 adafactor.py:358] Since rank of parameter decoder/layers_10/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.832212 140193652815872 adafactor.py:358] Since rank of parameter decoder/layers_11/extra_mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.833400 140193652815872 adafactor.py:358] Since rank of parameter decoder/layers_11/extra_mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.834501 140193652815872 adafactor.py:358] Since rank of parameter decoder/layers_11/extra_mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:12.838667 140193652815872 adafactor.py:358] Since rank of parameter decoder/layers_11/mlp/router/router_weights/w/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>, <FactorDim.NONE: None>) is ignored.
W0512 23:45:12.839591 140193652815872 adafactor.py:358] Since rank of parameter decoder/layers_11/pre_extra_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:12.840505 140193652815872 adafactor.py:358] Since rank of parameter decoder/layers_11/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:12.841377 140193652815872 adafactor.py:358] Since rank of parameter decoder/layers_11/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:12.842350 140193652815872 adafactor.py:358] Since rank of parameter decoder/layers_11/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.843504 140193652815872 adafactor.py:358] Since rank of parameter decoder/layers_11/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:12.844642 140193652815872 adafactor.py:358] Since rank of parameter decoder/layers_11/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.845741 140193652815872 adafactor.py:358] Since rank of parameter decoder/layers_11/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.846984 140193652815872 adafactor.py:358] Since rank of parameter decoder/layers_12/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.848077 140193652815872 adafactor.py:358] Since rank of parameter decoder/layers_12/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.849190 140193652815872 adafactor.py:358] Since rank of parameter decoder/layers_12/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:12.850285 140193652815872 adafactor.py:358] Since rank of parameter decoder/layers_12/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:12.725566 140180528838656 adafactor.py:358] Since rank of parameter decoder/layers_15/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:12.726456 140180528838656 adafactor.py:358] Since rank of parameter decoder/layers_15/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:12.727361 140180528838656 adafactor.py:358] Since rank of parameter decoder/layers_15/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.728516 140180528838656 adafactor.py:358] Since rank of parameter decoder/layers_15/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:12.729637 140180528838656 adafactor.py:358] Since rank of parameter decoder/layers_15/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.730735 140180528838656 adafactor.py:358] Since rank of parameter decoder/layers_15/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.731827 140180528838656 adafactor.py:358] Since rank of parameter decoder/layers_16/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.732998 140180528838656 adafactor.py:358] Since rank of parameter decoder/layers_16/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.734137 140180528838656 adafactor.py:358] Since rank of parameter decoder/layers_16/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:12.735225 140180528838656 adafactor.py:358] Since rank of parameter decoder/layers_16/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:12.736129 140180528838656 adafactor.py:358] Since rank of parameter decoder/layers_16/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:12.737110 140180528838656 adafactor.py:358] Since rank of parameter decoder/layers_16/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.738187 140180528838656 adafactor.py:358] Since rank of parameter decoder/layers_16/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:12.739296 140180528838656 adafactor.py:358] Since rank of parameter decoder/layers_16/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.740391 140180528838656 adafactor.py:358] Since rank of parameter decoder/layers_16/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.741565 140180528838656 adafactor.py:358] Since rank of parameter decoder/layers_17/extra_mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.742662 140180528838656 adafactor.py:358] Since rank of parameter decoder/layers_17/extra_mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.743745 140180528838656 adafactor.py:358] Since rank of parameter decoder/layers_17/extra_mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:12.747899 140180528838656 adafactor.py:358] Since rank of parameter decoder/layers_17/mlp/router/router_weights/w/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>, <FactorDim.NONE: None>) is ignored.
W0512 23:45:12.748808 140180528838656 adafactor.py:358] Since rank of parameter decoder/layers_17/pre_extra_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:12.750180 140180528838656 adafactor.py:358] Since rank of parameter decoder/layers_17/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:12.751062 140180528838656 adafactor.py:358] Since rank of parameter decoder/layers_17/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:12.751973 140180528838656 adafactor.py:358] Since rank of parameter decoder/layers_17/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.753080 140180528838656 adafactor.py:358] Since rank of parameter decoder/layers_17/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:12.754172 140180528838656 adafactor.py:358] Since rank of parameter decoder/layers_17/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.755307 140180528838656 adafactor.py:358] Since rank of parameter decoder/layers_17/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.851959 140193652815872 adafactor.py:358] Since rank of parameter decoder/layers_12/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:12.756382 140180528838656 adafactor.py:358] Since rank of parameter decoder/layers_18/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.757516 140180528838656 adafactor.py:358] Since rank of parameter decoder/layers_18/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.758600 140180528838656 adafactor.py:358] Since rank of parameter decoder/layers_18/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:12.759739 140180528838656 adafactor.py:358] Since rank of parameter decoder/layers_18/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:12.760626 140180528838656 adafactor.py:358] Since rank of parameter decoder/layers_18/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:12.761542 140180528838656 adafactor.py:358] Since rank of parameter decoder/layers_18/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.762630 140180528838656 adafactor.py:358] Since rank of parameter decoder/layers_18/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:12.763762 140180528838656 adafactor.py:358] Since rank of parameter decoder/layers_18/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.764869 140180528838656 adafactor.py:358] Since rank of parameter decoder/layers_18/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.852893 140193652815872 adafactor.py:358] Since rank of parameter decoder/layers_12/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.765981 140180528838656 adafactor.py:358] Since rank of parameter decoder/layers_19/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.767055 140180528838656 adafactor.py:358] Since rank of parameter decoder/layers_19/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.768178 140180528838656 adafactor.py:358] Since rank of parameter decoder/layers_19/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:12.769302 140180528838656 adafactor.py:358] Since rank of parameter decoder/layers_19/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:12.770174 140180528838656 adafactor.py:358] Since rank of parameter decoder/layers_19/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:12.771066 140180528838656 adafactor.py:358] Since rank of parameter decoder/layers_19/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.772189 140180528838656 adafactor.py:358] Since rank of parameter decoder/layers_19/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:12.773314 140180528838656 adafactor.py:358] Since rank of parameter decoder/layers_19/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.853986 140193652815872 adafactor.py:358] Since rank of parameter decoder/layers_12/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:12.774383 140180528838656 adafactor.py:358] Since rank of parameter decoder/layers_19/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.775470 140180528838656 adafactor.py:358] Since rank of parameter decoder/layers_2/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.776600 140180528838656 adafactor.py:358] Since rank of parameter decoder/layers_2/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.777705 140180528838656 adafactor.py:358] Since rank of parameter decoder/layers_2/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:12.778780 140180528838656 adafactor.py:358] Since rank of parameter decoder/layers_2/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:12.779672 140180528838656 adafactor.py:358] Since rank of parameter decoder/layers_2/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:12.780601 140180528838656 adafactor.py:358] Since rank of parameter decoder/layers_2/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.781700 140180528838656 adafactor.py:358] Since rank of parameter decoder/layers_2/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:12.782783 140180528838656 adafactor.py:358] Since rank of parameter decoder/layers_2/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.783883 140180528838656 adafactor.py:358] Since rank of parameter decoder/layers_2/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.785068 140180528838656 adafactor.py:358] Since rank of parameter decoder/layers_20/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.786164 140180528838656 adafactor.py:358] Since rank of parameter decoder/layers_20/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.787250 140180528838656 adafactor.py:358] Since rank of parameter decoder/layers_20/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:12.788342 140180528838656 adafactor.py:358] Since rank of parameter decoder/layers_20/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:12.789278 140180528838656 adafactor.py:358] Since rank of parameter decoder/layers_20/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:12.790198 140180528838656 adafactor.py:358] Since rank of parameter decoder/layers_20/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.791276 140180528838656 adafactor.py:358] Since rank of parameter decoder/layers_20/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:12.792352 140180528838656 adafactor.py:358] Since rank of parameter decoder/layers_20/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.855094 140193652815872 adafactor.py:358] Since rank of parameter decoder/layers_12/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.856239 140193652815872 adafactor.py:358] Since rank of parameter decoder/layers_12/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.793467 140180528838656 adafactor.py:358] Since rank of parameter decoder/layers_20/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.794591 140180528838656 adafactor.py:358] Since rank of parameter decoder/layers_21/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.795679 140180528838656 adafactor.py:358] Since rank of parameter decoder/layers_21/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.796743 140180528838656 adafactor.py:358] Since rank of parameter decoder/layers_21/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:12.797836 140180528838656 adafactor.py:358] Since rank of parameter decoder/layers_21/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:12.798776 140180528838656 adafactor.py:358] Since rank of parameter decoder/layers_21/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:12.799663 140180528838656 adafactor.py:358] Since rank of parameter decoder/layers_21/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.800737 140180528838656 adafactor.py:358] Since rank of parameter decoder/layers_21/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:12.857344 140193652815872 adafactor.py:358] Since rank of parameter decoder/layers_13/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.801865 140180528838656 adafactor.py:358] Since rank of parameter decoder/layers_21/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.803415 140180528838656 adafactor.py:358] Since rank of parameter decoder/layers_21/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.804556 140180528838656 adafactor.py:358] Since rank of parameter decoder/layers_22/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.805690 140180528838656 adafactor.py:358] Since rank of parameter decoder/layers_22/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.806806 140180528838656 adafactor.py:358] Since rank of parameter decoder/layers_22/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:12.807934 140180528838656 adafactor.py:358] Since rank of parameter decoder/layers_22/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:12.808829 140180528838656 adafactor.py:358] Since rank of parameter decoder/layers_22/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:12.828628 140180528838656 adafactor.py:358] Since rank of parameter decoder/layers_22/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.829955 140180528838656 adafactor.py:358] Since rank of parameter decoder/layers_22/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:12.831158 140180528838656 adafactor.py:358] Since rank of parameter decoder/layers_22/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.832266 140180528838656 adafactor.py:358] Since rank of parameter decoder/layers_22/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.833400 140180528838656 adafactor.py:358] Since rank of parameter decoder/layers_23/extra_mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.834509 140180528838656 adafactor.py:358] Since rank of parameter decoder/layers_23/extra_mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.835651 140180528838656 adafactor.py:358] Since rank of parameter decoder/layers_23/extra_mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:12.839805 140180528838656 adafactor.py:358] Since rank of parameter decoder/layers_23/mlp/router/router_weights/w/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>, <FactorDim.NONE: None>) is ignored.
W0512 23:45:12.840720 140180528838656 adafactor.py:358] Since rank of parameter decoder/layers_23/pre_extra_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:12.858439 140193652815872 adafactor.py:358] Since rank of parameter decoder/layers_13/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.841671 140180528838656 adafactor.py:358] Since rank of parameter decoder/layers_23/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:12.859540 140193652815872 adafactor.py:358] Since rank of parameter decoder/layers_13/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:12.842575 140180528838656 adafactor.py:358] Since rank of parameter decoder/layers_23/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:12.843496 140180528838656 adafactor.py:358] Since rank of parameter decoder/layers_23/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.860721 140193652815872 adafactor.py:358] Since rank of parameter decoder/layers_13/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:12.844665 140180528838656 adafactor.py:358] Since rank of parameter decoder/layers_23/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:12.862359 140193652815872 adafactor.py:358] Since rank of parameter decoder/layers_13/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:12.845965 140180528838656 adafactor.py:358] Since rank of parameter decoder/layers_23/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.863288 140193652815872 adafactor.py:358] Since rank of parameter decoder/layers_13/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.847053 140180528838656 adafactor.py:358] Since rank of parameter decoder/layers_23/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.864387 140193652815872 adafactor.py:358] Since rank of parameter decoder/layers_13/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:12.848131 140180528838656 adafactor.py:358] Since rank of parameter decoder/layers_3/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.865543 140193652815872 adafactor.py:358] Since rank of parameter decoder/layers_13/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.849321 140180528838656 adafactor.py:358] Since rank of parameter decoder/layers_3/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.866634 140193652815872 adafactor.py:358] Since rank of parameter decoder/layers_13/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.850400 140180528838656 adafactor.py:358] Since rank of parameter decoder/layers_3/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:12.867726 140193652815872 adafactor.py:358] Since rank of parameter decoder/layers_14/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.851479 140180528838656 adafactor.py:358] Since rank of parameter decoder/layers_3/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:12.868831 140193652815872 adafactor.py:358] Since rank of parameter decoder/layers_14/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.852362 140180528838656 adafactor.py:358] Since rank of parameter decoder/layers_3/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:12.869971 140193652815872 adafactor.py:358] Since rank of parameter decoder/layers_14/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:12.853345 140180528838656 adafactor.py:358] Since rank of parameter decoder/layers_3/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.871056 140193652815872 adafactor.py:358] Since rank of parameter decoder/layers_14/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:12.854447 140180528838656 adafactor.py:358] Since rank of parameter decoder/layers_3/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:12.871952 140193652815872 adafactor.py:358] Since rank of parameter decoder/layers_14/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:12.855536 140180528838656 adafactor.py:358] Since rank of parameter decoder/layers_3/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.872881 140193652815872 adafactor.py:358] Since rank of parameter decoder/layers_14/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.856609 140180528838656 adafactor.py:358] Since rank of parameter decoder/layers_3/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.873966 140193652815872 adafactor.py:358] Since rank of parameter decoder/layers_14/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:12.857779 140180528838656 adafactor.py:358] Since rank of parameter decoder/layers_4/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.858854 140180528838656 adafactor.py:358] Since rank of parameter decoder/layers_4/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.859943 140180528838656 adafactor.py:358] Since rank of parameter decoder/layers_4/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:12.861015 140180528838656 adafactor.py:358] Since rank of parameter decoder/layers_4/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:12.861973 140180528838656 adafactor.py:358] Since rank of parameter decoder/layers_4/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:12.862852 140180528838656 adafactor.py:358] Since rank of parameter decoder/layers_4/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.863923 140180528838656 adafactor.py:358] Since rank of parameter decoder/layers_4/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:12.865067 140180528838656 adafactor.py:358] Since rank of parameter decoder/layers_4/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.866232 140180528838656 adafactor.py:358] Since rank of parameter decoder/layers_4/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.867305 140180528838656 adafactor.py:358] Since rank of parameter decoder/layers_5/extra_mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.868383 140180528838656 adafactor.py:358] Since rank of parameter decoder/layers_5/extra_mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
I0512 23:45:12.868736 140643590531072 utils.py:952] Using latest T5X checkpoint.
W0512 23:45:12.869490 140180528838656 adafactor.py:358] Since rank of parameter decoder/layers_5/extra_mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:12.873595 140180528838656 adafactor.py:358] Since rank of parameter decoder/layers_5/mlp/router/router_weights/w/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>, <FactorDim.NONE: None>) is ignored.
W0512 23:45:12.875002 140180528838656 adafactor.py:358] Since rank of parameter decoder/layers_5/pre_extra_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:12.875892 140180528838656 adafactor.py:358] Since rank of parameter decoder/layers_5/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:12.876785 140180528838656 adafactor.py:358] Since rank of parameter decoder/layers_5/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:12.877698 140180528838656 adafactor.py:358] Since rank of parameter decoder/layers_5/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.878785 140180528838656 adafactor.py:358] Since rank of parameter decoder/layers_5/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:12.879905 140180528838656 adafactor.py:358] Since rank of parameter decoder/layers_5/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.880987 140180528838656 adafactor.py:358] Since rank of parameter decoder/layers_5/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.882072 140180528838656 adafactor.py:358] Since rank of parameter decoder/layers_6/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.883132 140180528838656 adafactor.py:358] Since rank of parameter decoder/layers_6/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.884263 140180528838656 adafactor.py:358] Since rank of parameter decoder/layers_6/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:12.885388 140180528838656 adafactor.py:358] Since rank of parameter decoder/layers_6/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:12.886264 140180528838656 adafactor.py:358] Since rank of parameter decoder/layers_6/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:12.887132 140180528838656 adafactor.py:358] Since rank of parameter decoder/layers_6/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.888258 140180528838656 adafactor.py:358] Since rank of parameter decoder/layers_6/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:12.889357 140180528838656 adafactor.py:358] Since rank of parameter decoder/layers_6/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.890435 140180528838656 adafactor.py:358] Since rank of parameter decoder/layers_6/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.891505 140180528838656 adafactor.py:358] Since rank of parameter decoder/layers_7/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.892620 140180528838656 adafactor.py:358] Since rank of parameter decoder/layers_7/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.893735 140180528838656 adafactor.py:358] Since rank of parameter decoder/layers_7/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:12.894806 140180528838656 adafactor.py:358] Since rank of parameter decoder/layers_7/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:12.895687 140180528838656 adafactor.py:358] Since rank of parameter decoder/layers_7/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:12.896639 140180528838656 adafactor.py:358] Since rank of parameter decoder/layers_7/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.897745 140180528838656 adafactor.py:358] Since rank of parameter decoder/layers_7/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:12.898807 140180528838656 adafactor.py:358] Since rank of parameter decoder/layers_7/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.899904 140180528838656 adafactor.py:358] Since rank of parameter decoder/layers_7/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.901020 140180528838656 adafactor.py:358] Since rank of parameter decoder/layers_8/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.902127 140180528838656 adafactor.py:358] Since rank of parameter decoder/layers_8/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.903183 140180528838656 adafactor.py:358] Since rank of parameter decoder/layers_8/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:12.904274 140180528838656 adafactor.py:358] Since rank of parameter decoder/layers_8/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:12.905257 140180528838656 adafactor.py:358] Since rank of parameter decoder/layers_8/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:12.906136 140180528838656 adafactor.py:358] Since rank of parameter decoder/layers_8/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
I0512 23:45:12.904935 140260698863616 moe_layers.py:777] Selected group_size=4096 and num_groups=192 for input num_tokens=786432, max_group_size=4096, num_experts=8 and num_expert_replicas=1
W0512 23:45:12.907193 140180528838656 adafactor.py:358] Since rank of parameter decoder/layers_8/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:12.908274 140180528838656 adafactor.py:358] Since rank of parameter decoder/layers_8/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.909418 140180528838656 adafactor.py:358] Since rank of parameter decoder/layers_8/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.910539 140180528838656 adafactor.py:358] Since rank of parameter decoder/layers_9/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.911621 140180528838656 adafactor.py:358] Since rank of parameter decoder/layers_9/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.912694 140180528838656 adafactor.py:358] Since rank of parameter decoder/layers_9/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:12.913838 140180528838656 adafactor.py:358] Since rank of parameter decoder/layers_9/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:12.914743 140180528838656 adafactor.py:358] Since rank of parameter decoder/layers_9/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:12.915615 140180528838656 adafactor.py:358] Since rank of parameter decoder/layers_9/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.916685 140180528838656 adafactor.py:358] Since rank of parameter decoder/layers_9/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:12.917792 140180528838656 adafactor.py:358] Since rank of parameter decoder/layers_9/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.918931 140180528838656 adafactor.py:358] Since rank of parameter decoder/layers_9/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.920005 140180528838656 adafactor.py:358] Since rank of parameter decoder/logits_dense/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.921104 140180528838656 adafactor.py:358] Since rank of parameter token_embedder/embedding 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
I0512 23:45:12.965615 139876724979712 utils.py:952] Using latest T5X checkpoint.
W0512 23:45:12.875106 140193652815872 adafactor.py:358] Since rank of parameter decoder/layers_14/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.876180 140193652815872 adafactor.py:358] Since rank of parameter decoder/layers_14/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.877293 140193652815872 adafactor.py:358] Since rank of parameter decoder/layers_15/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.878387 140193652815872 adafactor.py:358] Since rank of parameter decoder/layers_15/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.879530 140193652815872 adafactor.py:358] Since rank of parameter decoder/layers_15/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:12.880650 140193652815872 adafactor.py:358] Since rank of parameter decoder/layers_15/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:12.881531 140193652815872 adafactor.py:358] Since rank of parameter decoder/layers_15/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:12.882408 140193652815872 adafactor.py:358] Since rank of parameter decoder/layers_15/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.883625 140193652815872 adafactor.py:358] Since rank of parameter decoder/layers_15/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:12.884738 140193652815872 adafactor.py:358] Since rank of parameter decoder/layers_15/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.885827 140193652815872 adafactor.py:358] Since rank of parameter decoder/layers_15/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.886918 140193652815872 adafactor.py:358] Since rank of parameter decoder/layers_16/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.888048 140193652815872 adafactor.py:358] Since rank of parameter decoder/layers_16/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.889160 140193652815872 adafactor.py:358] Since rank of parameter decoder/layers_16/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:12.890244 140193652815872 adafactor.py:358] Since rank of parameter decoder/layers_16/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:12.891151 140193652815872 adafactor.py:358] Since rank of parameter decoder/layers_16/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:12.892100 140193652815872 adafactor.py:358] Since rank of parameter decoder/layers_16/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.893212 140193652815872 adafactor.py:358] Since rank of parameter decoder/layers_16/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:12.894312 140193652815872 adafactor.py:358] Since rank of parameter decoder/layers_16/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.895396 140193652815872 adafactor.py:358] Since rank of parameter decoder/layers_16/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.896579 140193652815872 adafactor.py:358] Since rank of parameter decoder/layers_17/extra_mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.897665 140193652815872 adafactor.py:358] Since rank of parameter decoder/layers_17/extra_mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.898756 140193652815872 adafactor.py:358] Since rank of parameter decoder/layers_17/extra_mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:12.902898 140193652815872 adafactor.py:358] Since rank of parameter decoder/layers_17/mlp/router/router_weights/w/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>, <FactorDim.NONE: None>) is ignored.
W0512 23:45:12.903820 140193652815872 adafactor.py:358] Since rank of parameter decoder/layers_17/pre_extra_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:12.905194 140193652815872 adafactor.py:358] Since rank of parameter decoder/layers_17/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:12.906096 140193652815872 adafactor.py:358] Since rank of parameter decoder/layers_17/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:12.906990 140193652815872 adafactor.py:358] Since rank of parameter decoder/layers_17/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.908077 140193652815872 adafactor.py:358] Since rank of parameter decoder/layers_17/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:12.909186 140193652815872 adafactor.py:358] Since rank of parameter decoder/layers_17/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.910331 140193652815872 adafactor.py:358] Since rank of parameter decoder/layers_17/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.911418 140193652815872 adafactor.py:358] Since rank of parameter decoder/layers_18/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.912718 140193652815872 adafactor.py:358] Since rank of parameter decoder/layers_18/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.913807 140193652815872 adafactor.py:358] Since rank of parameter decoder/layers_18/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
I0512 23:45:12.974984 139666521167872 utils.py:952] Using latest T5X checkpoint.
W0512 23:45:12.914953 140193652815872 adafactor.py:358] Since rank of parameter decoder/layers_18/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:12.915847 140193652815872 adafactor.py:358] Since rank of parameter decoder/layers_18/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:12.916762 140193652815872 adafactor.py:358] Since rank of parameter decoder/layers_18/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.917841 140193652815872 adafactor.py:358] Since rank of parameter decoder/layers_18/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:12.918982 140193652815872 adafactor.py:358] Since rank of parameter decoder/layers_18/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.920098 140193652815872 adafactor.py:358] Since rank of parameter decoder/layers_18/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.921208 140193652815872 adafactor.py:358] Since rank of parameter decoder/layers_19/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.922304 140193652815872 adafactor.py:358] Since rank of parameter decoder/layers_19/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.923495 140193652815872 adafactor.py:358] Since rank of parameter decoder/layers_19/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:12.924602 140193652815872 adafactor.py:358] Since rank of parameter decoder/layers_19/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:12.925494 140193652815872 adafactor.py:358] Since rank of parameter decoder/layers_19/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:12.926372 140193652815872 adafactor.py:358] Since rank of parameter decoder/layers_19/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.927519 140193652815872 adafactor.py:358] Since rank of parameter decoder/layers_19/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:12.928651 140193652815872 adafactor.py:358] Since rank of parameter decoder/layers_19/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.929746 140193652815872 adafactor.py:358] Since rank of parameter decoder/layers_19/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.930832 140193652815872 adafactor.py:358] Since rank of parameter decoder/layers_2/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.931973 140193652815872 adafactor.py:358] Since rank of parameter decoder/layers_2/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.933102 140193652815872 adafactor.py:358] Since rank of parameter decoder/layers_2/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:12.934173 140193652815872 adafactor.py:358] Since rank of parameter decoder/layers_2/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:12.935044 140193652815872 adafactor.py:358] Since rank of parameter decoder/layers_2/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:12.935980 140193652815872 adafactor.py:358] Since rank of parameter decoder/layers_2/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.937091 140193652815872 adafactor.py:358] Since rank of parameter decoder/layers_2/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:12.938166 140193652815872 adafactor.py:358] Since rank of parameter decoder/layers_2/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.939258 140193652815872 adafactor.py:358] Since rank of parameter decoder/layers_2/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.940418 140193652815872 adafactor.py:358] Since rank of parameter decoder/layers_20/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.941518 140193652815872 adafactor.py:358] Since rank of parameter decoder/layers_20/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.942605 140193652815872 adafactor.py:358] Since rank of parameter decoder/layers_20/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:12.943719 140193652815872 adafactor.py:358] Since rank of parameter decoder/layers_20/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:12.944672 140193652815872 adafactor.py:358] Since rank of parameter decoder/layers_20/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:12.945617 140193652815872 adafactor.py:358] Since rank of parameter decoder/layers_20/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.946700 140193652815872 adafactor.py:358] Since rank of parameter decoder/layers_20/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:12.947783 140193652815872 adafactor.py:358] Since rank of parameter decoder/layers_20/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.948899 140193652815872 adafactor.py:358] Since rank of parameter decoder/layers_20/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.950031 140193652815872 adafactor.py:358] Since rank of parameter decoder/layers_21/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.951123 140193652815872 adafactor.py:358] Since rank of parameter decoder/layers_21/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.952205 140193652815872 adafactor.py:358] Since rank of parameter decoder/layers_21/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:12.953323 140193652815872 adafactor.py:358] Since rank of parameter decoder/layers_21/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:12.954258 140193652815872 adafactor.py:358] Since rank of parameter decoder/layers_21/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:12.955151 140193652815872 adafactor.py:358] Since rank of parameter decoder/layers_21/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.956245 140193652815872 adafactor.py:358] Since rank of parameter decoder/layers_21/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:12.957359 140193652815872 adafactor.py:358] Since rank of parameter decoder/layers_21/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.958923 140193652815872 adafactor.py:358] Since rank of parameter decoder/layers_21/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.960031 140193652815872 adafactor.py:358] Since rank of parameter decoder/layers_22/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.961139 140193652815872 adafactor.py:358] Since rank of parameter decoder/layers_22/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.962218 140193652815872 adafactor.py:358] Since rank of parameter decoder/layers_22/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:12.963383 140193652815872 adafactor.py:358] Since rank of parameter decoder/layers_22/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:12.964263 140193652815872 adafactor.py:358] Since rank of parameter decoder/layers_22/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:12.965179 140193652815872 adafactor.py:358] Since rank of parameter decoder/layers_22/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.966247 140193652815872 adafactor.py:358] Since rank of parameter decoder/layers_22/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:12.967401 140193652815872 adafactor.py:358] Since rank of parameter decoder/layers_22/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.968536 140193652815872 adafactor.py:358] Since rank of parameter decoder/layers_22/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.969624 140193652815872 adafactor.py:358] Since rank of parameter decoder/layers_23/extra_mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.970709 140193652815872 adafactor.py:358] Since rank of parameter decoder/layers_23/extra_mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.971845 140193652815872 adafactor.py:358] Since rank of parameter decoder/layers_23/extra_mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:12.975940 140193652815872 adafactor.py:358] Since rank of parameter decoder/layers_23/mlp/router/router_weights/w/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>, <FactorDim.NONE: None>) is ignored.
W0512 23:45:12.976873 140193652815872 adafactor.py:358] Since rank of parameter decoder/layers_23/pre_extra_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:12.977761 140193652815872 adafactor.py:358] Since rank of parameter decoder/layers_23/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:12.978641 140193652815872 adafactor.py:358] Since rank of parameter decoder/layers_23/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:12.979517 140193652815872 adafactor.py:358] Since rank of parameter decoder/layers_23/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.980685 140193652815872 adafactor.py:358] Since rank of parameter decoder/layers_23/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:12.982036 140193652815872 adafactor.py:358] Since rank of parameter decoder/layers_23/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.983151 140193652815872 adafactor.py:358] Since rank of parameter decoder/layers_23/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.984534 140193652815872 adafactor.py:358] Since rank of parameter decoder/layers_3/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.985658 140193652815872 adafactor.py:358] Since rank of parameter decoder/layers_3/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.986728 140193652815872 adafactor.py:358] Since rank of parameter decoder/layers_3/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:12.987801 140193652815872 adafactor.py:358] Since rank of parameter decoder/layers_3/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:12.988715 140193652815872 adafactor.py:358] Since rank of parameter decoder/layers_3/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:12.989653 140193652815872 adafactor.py:358] Since rank of parameter decoder/layers_3/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.990735 140193652815872 adafactor.py:358] Since rank of parameter decoder/layers_3/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:12.991806 140193652815872 adafactor.py:358] Since rank of parameter decoder/layers_3/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.992909 140193652815872 adafactor.py:358] Since rank of parameter decoder/layers_3/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.994042 140193652815872 adafactor.py:358] Since rank of parameter decoder/layers_4/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.995115 140193652815872 adafactor.py:358] Since rank of parameter decoder/layers_4/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:12.996207 140193652815872 adafactor.py:358] Since rank of parameter decoder/layers_4/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:12.997315 140193652815872 adafactor.py:358] Since rank of parameter decoder/layers_4/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:12.998239 140193652815872 adafactor.py:358] Since rank of parameter decoder/layers_4/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:12.999118 140193652815872 adafactor.py:358] Since rank of parameter decoder/layers_4/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:13.000190 140193652815872 adafactor.py:358] Since rank of parameter decoder/layers_4/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:13.001291 140193652815872 adafactor.py:358] Since rank of parameter decoder/layers_4/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:13.002423 140193652815872 adafactor.py:358] Since rank of parameter decoder/layers_4/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:13.003536 140193652815872 adafactor.py:358] Since rank of parameter decoder/layers_5/extra_mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:13.004631 140193652815872 adafactor.py:358] Since rank of parameter decoder/layers_5/extra_mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:13.005707 140193652815872 adafactor.py:358] Since rank of parameter decoder/layers_5/extra_mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:13.009767 140193652815872 adafactor.py:358] Since rank of parameter decoder/layers_5/mlp/router/router_weights/w/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>, <FactorDim.NONE: None>) is ignored.
W0512 23:45:13.011117 140193652815872 adafactor.py:358] Since rank of parameter decoder/layers_5/pre_extra_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:13.012003 140193652815872 adafactor.py:358] Since rank of parameter decoder/layers_5/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:13.012918 140193652815872 adafactor.py:358] Since rank of parameter decoder/layers_5/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:13.013807 140193652815872 adafactor.py:358] Since rank of parameter decoder/layers_5/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:13.014893 140193652815872 adafactor.py:358] Since rank of parameter decoder/layers_5/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:13.016010 140193652815872 adafactor.py:358] Since rank of parameter decoder/layers_5/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:13.017105 140193652815872 adafactor.py:358] Since rank of parameter decoder/layers_5/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:13.018169 140193652815872 adafactor.py:358] Since rank of parameter decoder/layers_6/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:13.019229 140193652815872 adafactor.py:358] Since rank of parameter decoder/layers_6/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:13.020375 140193652815872 adafactor.py:358] Since rank of parameter decoder/layers_6/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:13.021450 140193652815872 adafactor.py:358] Since rank of parameter decoder/layers_6/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:13.022312 140193652815872 adafactor.py:358] Since rank of parameter decoder/layers_6/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:13.023208 140193652815872 adafactor.py:358] Since rank of parameter decoder/layers_6/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:13.024362 140193652815872 adafactor.py:358] Since rank of parameter decoder/layers_6/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:13.006848 140260698863616 adafactor.py:358] Since rank of parameter decoder/decoder_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:13.025435 140193652815872 adafactor.py:358] Since rank of parameter decoder/layers_6/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:13.007928 140260698863616 adafactor.py:358] Since rank of parameter decoder/layers_0/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:13.026505 140193652815872 adafactor.py:358] Since rank of parameter decoder/layers_6/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:13.009107 140260698863616 adafactor.py:358] Since rank of parameter decoder/layers_0/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:13.027578 140193652815872 adafactor.py:358] Since rank of parameter decoder/layers_7/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:13.010243 140260698863616 adafactor.py:358] Since rank of parameter decoder/layers_0/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:13.028730 140193652815872 adafactor.py:358] Since rank of parameter decoder/layers_7/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:13.011508 140260698863616 adafactor.py:358] Since rank of parameter decoder/layers_0/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:13.029804 140193652815872 adafactor.py:358] Since rank of parameter decoder/layers_7/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:13.012447 140260698863616 adafactor.py:358] Since rank of parameter decoder/layers_0/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:13.030871 140193652815872 adafactor.py:358] Since rank of parameter decoder/layers_7/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:13.013392 140260698863616 adafactor.py:358] Since rank of parameter decoder/layers_0/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:13.031756 140193652815872 adafactor.py:358] Since rank of parameter decoder/layers_7/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:13.032715 140193652815872 adafactor.py:358] Since rank of parameter decoder/layers_7/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:13.014556 140260698863616 adafactor.py:358] Since rank of parameter decoder/layers_0/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:13.033784 140193652815872 adafactor.py:358] Since rank of parameter decoder/layers_7/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:13.015717 140260698863616 adafactor.py:358] Since rank of parameter decoder/layers_0/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:13.034854 140193652815872 adafactor.py:358] Since rank of parameter decoder/layers_7/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:13.016920 140260698863616 adafactor.py:358] Since rank of parameter decoder/layers_0/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:13.035935 140193652815872 adafactor.py:358] Since rank of parameter decoder/layers_7/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:13.018051 140260698863616 adafactor.py:358] Since rank of parameter decoder/layers_1/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:13.037084 140193652815872 adafactor.py:358] Since rank of parameter decoder/layers_8/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:13.019209 140260698863616 adafactor.py:358] Since rank of parameter decoder/layers_1/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:13.038148 140193652815872 adafactor.py:358] Since rank of parameter decoder/layers_8/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:13.020341 140260698863616 adafactor.py:358] Since rank of parameter decoder/layers_1/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:13.039207 140193652815872 adafactor.py:358] Since rank of parameter decoder/layers_8/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:13.021554 140260698863616 adafactor.py:358] Since rank of parameter decoder/layers_1/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:13.022475 140260698863616 adafactor.py:358] Since rank of parameter decoder/layers_1/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:13.040271 140193652815872 adafactor.py:358] Since rank of parameter decoder/layers_8/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:13.041218 140193652815872 adafactor.py:358] Since rank of parameter decoder/layers_8/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:13.023420 140260698863616 adafactor.py:358] Since rank of parameter decoder/layers_1/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:13.042118 140193652815872 adafactor.py:358] Since rank of parameter decoder/layers_8/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:13.024573 140260698863616 adafactor.py:358] Since rank of parameter decoder/layers_1/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:13.043231 140193652815872 adafactor.py:358] Since rank of parameter decoder/layers_8/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:13.025773 140260698863616 adafactor.py:358] Since rank of parameter decoder/layers_1/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:13.044341 140193652815872 adafactor.py:358] Since rank of parameter decoder/layers_8/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:13.026953 140260698863616 adafactor.py:358] Since rank of parameter decoder/layers_1/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:13.045469 140193652815872 adafactor.py:358] Since rank of parameter decoder/layers_8/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:13.028088 140260698863616 adafactor.py:358] Since rank of parameter decoder/layers_10/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:13.046606 140193652815872 adafactor.py:358] Since rank of parameter decoder/layers_9/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:13.029223 140260698863616 adafactor.py:358] Since rank of parameter decoder/layers_10/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:13.047668 140193652815872 adafactor.py:358] Since rank of parameter decoder/layers_9/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:13.030443 140260698863616 adafactor.py:358] Since rank of parameter decoder/layers_10/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:13.048777 140193652815872 adafactor.py:358] Since rank of parameter decoder/layers_9/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:13.031626 140260698863616 adafactor.py:358] Since rank of parameter decoder/layers_10/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:13.049899 140193652815872 adafactor.py:358] Since rank of parameter decoder/layers_9/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:13.050788 140193652815872 adafactor.py:358] Since rank of parameter decoder/layers_9/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:13.032570 140260698863616 adafactor.py:358] Since rank of parameter decoder/layers_10/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:13.051675 140193652815872 adafactor.py:358] Since rank of parameter decoder/layers_9/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:13.033496 140260698863616 adafactor.py:358] Since rank of parameter decoder/layers_10/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:13.052779 140193652815872 adafactor.py:358] Since rank of parameter decoder/layers_9/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:13.034748 140260698863616 adafactor.py:358] Since rank of parameter decoder/layers_10/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:13.053843 140193652815872 adafactor.py:358] Since rank of parameter decoder/layers_9/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:13.035905 140260698863616 adafactor.py:358] Since rank of parameter decoder/layers_10/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:13.054971 140193652815872 adafactor.py:358] Since rank of parameter decoder/layers_9/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:13.037042 140260698863616 adafactor.py:358] Since rank of parameter decoder/layers_10/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:13.056040 140193652815872 adafactor.py:358] Since rank of parameter decoder/logits_dense/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:13.038183 140260698863616 adafactor.py:358] Since rank of parameter decoder/layers_11/extra_mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:13.057135 140193652815872 adafactor.py:358] Since rank of parameter token_embedder/embedding 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:13.039407 140260698863616 adafactor.py:358] Since rank of parameter decoder/layers_11/extra_mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:13.040562 140260698863616 adafactor.py:358] Since rank of parameter decoder/layers_11/extra_mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:13.044874 140260698863616 adafactor.py:358] Since rank of parameter decoder/layers_11/mlp/router/router_weights/w/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>, <FactorDim.NONE: None>) is ignored.
W0512 23:45:13.045832 140260698863616 adafactor.py:358] Since rank of parameter decoder/layers_11/pre_extra_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:13.046789 140260698863616 adafactor.py:358] Since rank of parameter decoder/layers_11/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:13.047728 140260698863616 adafactor.py:358] Since rank of parameter decoder/layers_11/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:13.048750 140260698863616 adafactor.py:358] Since rank of parameter decoder/layers_11/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:13.049889 140260698863616 adafactor.py:358] Since rank of parameter decoder/layers_11/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:13.051077 140260698863616 adafactor.py:358] Since rank of parameter decoder/layers_11/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:13.052194 140260698863616 adafactor.py:358] Since rank of parameter decoder/layers_11/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:13.053458 140260698863616 adafactor.py:358] Since rank of parameter decoder/layers_12/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:13.054644 140260698863616 adafactor.py:358] Since rank of parameter decoder/layers_12/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:13.055813 140260698863616 adafactor.py:358] Since rank of parameter decoder/layers_12/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:13.056955 140260698863616 adafactor.py:358] Since rank of parameter decoder/layers_12/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:13.058661 140260698863616 adafactor.py:358] Since rank of parameter decoder/layers_12/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:13.059628 140260698863616 adafactor.py:358] Since rank of parameter decoder/layers_12/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:13.060762 140260698863616 adafactor.py:358] Since rank of parameter decoder/layers_12/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:13.061907 140260698863616 adafactor.py:358] Since rank of parameter decoder/layers_12/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:13.063135 140260698863616 adafactor.py:358] Since rank of parameter decoder/layers_12/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:13.064261 140260698863616 adafactor.py:358] Since rank of parameter decoder/layers_13/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:13.065399 140260698863616 adafactor.py:358] Since rank of parameter decoder/layers_13/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:13.066533 140260698863616 adafactor.py:358] Since rank of parameter decoder/layers_13/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:13.067737 140260698863616 adafactor.py:358] Since rank of parameter decoder/layers_13/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:13.069402 140260698863616 adafactor.py:358] Since rank of parameter decoder/layers_13/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:13.070337 140260698863616 adafactor.py:358] Since rank of parameter decoder/layers_13/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:13.071521 140260698863616 adafactor.py:358] Since rank of parameter decoder/layers_13/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:13.072708 140260698863616 adafactor.py:358] Since rank of parameter decoder/layers_13/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:13.073844 140260698863616 adafactor.py:358] Since rank of parameter decoder/layers_13/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:13.075046 140260698863616 adafactor.py:358] Since rank of parameter decoder/layers_14/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:13.076193 140260698863616 adafactor.py:358] Since rank of parameter decoder/layers_14/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:13.077369 140260698863616 adafactor.py:358] Since rank of parameter decoder/layers_14/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:13.078502 140260698863616 adafactor.py:358] Since rank of parameter decoder/layers_14/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:13.079463 140260698863616 adafactor.py:358] Since rank of parameter decoder/layers_14/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:13.080373 140260698863616 adafactor.py:358] Since rank of parameter decoder/layers_14/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:13.081510 140260698863616 adafactor.py:358] Since rank of parameter decoder/layers_14/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:13.082674 140260698863616 adafactor.py:358] Since rank of parameter decoder/layers_14/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:13.083820 140260698863616 adafactor.py:358] Since rank of parameter decoder/layers_14/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:13.084960 140260698863616 adafactor.py:358] Since rank of parameter decoder/layers_15/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:13.086092 140260698863616 adafactor.py:358] Since rank of parameter decoder/layers_15/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:13.087311 140260698863616 adafactor.py:358] Since rank of parameter decoder/layers_15/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:13.088447 140260698863616 adafactor.py:358] Since rank of parameter decoder/layers_15/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:13.089348 140260698863616 adafactor.py:358] Since rank of parameter decoder/layers_15/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:13.090277 140260698863616 adafactor.py:358] Since rank of parameter decoder/layers_15/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:13.091500 140260698863616 adafactor.py:358] Since rank of parameter decoder/layers_15/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:13.092632 140260698863616 adafactor.py:358] Since rank of parameter decoder/layers_15/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:13.093768 140260698863616 adafactor.py:358] Since rank of parameter decoder/layers_15/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:13.094954 140260698863616 adafactor.py:358] Since rank of parameter decoder/layers_16/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:13.096129 140260698863616 adafactor.py:358] Since rank of parameter decoder/layers_16/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:13.097281 140260698863616 adafactor.py:358] Since rank of parameter decoder/layers_16/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:13.098401 140260698863616 adafactor.py:358] Since rank of parameter decoder/layers_16/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:13.099362 140260698863616 adafactor.py:358] Since rank of parameter decoder/layers_16/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:13.100335 140260698863616 adafactor.py:358] Since rank of parameter decoder/layers_16/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:13.101473 140260698863616 adafactor.py:358] Since rank of parameter decoder/layers_16/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:13.102598 140260698863616 adafactor.py:358] Since rank of parameter decoder/layers_16/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:13.103760 140260698863616 adafactor.py:358] Since rank of parameter decoder/layers_16/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:13.104940 140260698863616 adafactor.py:358] Since rank of parameter decoder/layers_17/extra_mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:13.106064 140260698863616 adafactor.py:358] Since rank of parameter decoder/layers_17/extra_mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:13.107237 140260698863616 adafactor.py:358] Since rank of parameter decoder/layers_17/extra_mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:13.111503 140260698863616 adafactor.py:358] Since rank of parameter decoder/layers_17/mlp/router/router_weights/w/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>, <FactorDim.NONE: None>) is ignored.
W0512 23:45:13.112433 140260698863616 adafactor.py:358] Since rank of parameter decoder/layers_17/pre_extra_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:13.113826 140260698863616 adafactor.py:358] Since rank of parameter decoder/layers_17/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:13.114825 140260698863616 adafactor.py:358] Since rank of parameter decoder/layers_17/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:13.115765 140260698863616 adafactor.py:358] Since rank of parameter decoder/layers_17/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:13.116894 140260698863616 adafactor.py:358] Since rank of parameter decoder/layers_17/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:13.118117 140260698863616 adafactor.py:358] Since rank of parameter decoder/layers_17/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:13.119418 140260698863616 adafactor.py:358] Since rank of parameter decoder/layers_17/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:13.120644 140260698863616 adafactor.py:358] Since rank of parameter decoder/layers_18/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:13.121862 140260698863616 adafactor.py:358] Since rank of parameter decoder/layers_18/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:13.123111 140260698863616 adafactor.py:358] Since rank of parameter decoder/layers_18/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:13.124392 140260698863616 adafactor.py:358] Since rank of parameter decoder/layers_18/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:13.125374 140260698863616 adafactor.py:358] Since rank of parameter decoder/layers_18/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:13.126365 140260698863616 adafactor.py:358] Since rank of parameter decoder/layers_18/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:13.127624 140260698863616 adafactor.py:358] Since rank of parameter decoder/layers_18/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:13.128890 140260698863616 adafactor.py:358] Since rank of parameter decoder/layers_18/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:13.130095 140260698863616 adafactor.py:358] Since rank of parameter decoder/layers_18/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:13.131273 140260698863616 adafactor.py:358] Since rank of parameter decoder/layers_19/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:13.132399 140260698863616 adafactor.py:358] Since rank of parameter decoder/layers_19/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:13.133590 140260698863616 adafactor.py:358] Since rank of parameter decoder/layers_19/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:13.134790 140260698863616 adafactor.py:358] Since rank of parameter decoder/layers_19/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:13.135720 140260698863616 adafactor.py:358] Since rank of parameter decoder/layers_19/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:13.136637 140260698863616 adafactor.py:358] Since rank of parameter decoder/layers_19/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:13.137821 140260698863616 adafactor.py:358] Since rank of parameter decoder/layers_19/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:13.138991 140260698863616 adafactor.py:358] Since rank of parameter decoder/layers_19/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:13.140119 140260698863616 adafactor.py:358] Since rank of parameter decoder/layers_19/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:13.141268 140260698863616 adafactor.py:358] Since rank of parameter decoder/layers_2/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:13.142460 140260698863616 adafactor.py:358] Since rank of parameter decoder/layers_2/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:13.143622 140260698863616 adafactor.py:358] Since rank of parameter decoder/layers_2/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:13.144757 140260698863616 adafactor.py:358] Since rank of parameter decoder/layers_2/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:13.145678 140260698863616 adafactor.py:358] Since rank of parameter decoder/layers_2/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:13.146658 140260698863616 adafactor.py:358] Since rank of parameter decoder/layers_2/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:13.147822 140260698863616 adafactor.py:358] Since rank of parameter decoder/layers_2/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:13.148941 140260698863616 adafactor.py:358] Since rank of parameter decoder/layers_2/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:13.150060 140260698863616 adafactor.py:358] Since rank of parameter decoder/layers_2/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:13.151275 140260698863616 adafactor.py:358] Since rank of parameter decoder/layers_20/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:13.152405 140260698863616 adafactor.py:358] Since rank of parameter decoder/layers_20/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:13.153514 140260698863616 adafactor.py:358] Since rank of parameter decoder/layers_20/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:13.154685 140260698863616 adafactor.py:358] Since rank of parameter decoder/layers_20/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:13.155662 140260698863616 adafactor.py:358] Since rank of parameter decoder/layers_20/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:13.156638 140260698863616 adafactor.py:358] Since rank of parameter decoder/layers_20/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:13.157766 140260698863616 adafactor.py:358] Since rank of parameter decoder/layers_20/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:13.158905 140260698863616 adafactor.py:358] Since rank of parameter decoder/layers_20/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:13.160022 140260698863616 adafactor.py:358] Since rank of parameter decoder/layers_20/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:13.161199 140260698863616 adafactor.py:358] Since rank of parameter decoder/layers_21/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:13.162328 140260698863616 adafactor.py:358] Since rank of parameter decoder/layers_21/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:13.163476 140260698863616 adafactor.py:358] Since rank of parameter decoder/layers_21/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:13.164595 140260698863616 adafactor.py:358] Since rank of parameter decoder/layers_21/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:13.165568 140260698863616 adafactor.py:358] Since rank of parameter decoder/layers_21/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:13.166497 140260698863616 adafactor.py:358] Since rank of parameter decoder/layers_21/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:13.167667 140260698863616 adafactor.py:358] Since rank of parameter decoder/layers_21/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:13.168826 140260698863616 adafactor.py:358] Since rank of parameter decoder/layers_21/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:13.170404 140260698863616 adafactor.py:358] Since rank of parameter decoder/layers_21/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:13.171583 140260698863616 adafactor.py:358] Since rank of parameter decoder/layers_22/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:13.172703 140260698863616 adafactor.py:358] Since rank of parameter decoder/layers_22/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:13.173827 140260698863616 adafactor.py:358] Since rank of parameter decoder/layers_22/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:13.175068 140260698863616 adafactor.py:358] Since rank of parameter decoder/layers_22/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:13.176001 140260698863616 adafactor.py:358] Since rank of parameter decoder/layers_22/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:13.176918 140260698863616 adafactor.py:358] Since rank of parameter decoder/layers_22/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:13.178032 140260698863616 adafactor.py:358] Since rank of parameter decoder/layers_22/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:13.179250 140260698863616 adafactor.py:358] Since rank of parameter decoder/layers_22/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:13.180383 140260698863616 adafactor.py:358] Since rank of parameter decoder/layers_22/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:13.181509 140260698863616 adafactor.py:358] Since rank of parameter decoder/layers_23/extra_mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:13.182633 140260698863616 adafactor.py:358] Since rank of parameter decoder/layers_23/extra_mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:13.183836 140260698863616 adafactor.py:358] Since rank of parameter decoder/layers_23/extra_mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:13.188858 140260698863616 adafactor.py:358] Since rank of parameter decoder/layers_23/mlp/router/router_weights/w/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>, <FactorDim.NONE: None>) is ignored.
W0512 23:45:13.189817 140260698863616 adafactor.py:358] Since rank of parameter decoder/layers_23/pre_extra_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:13.190775 140260698863616 adafactor.py:358] Since rank of parameter decoder/layers_23/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:13.191687 140260698863616 adafactor.py:358] Since rank of parameter decoder/layers_23/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:13.192605 140260698863616 adafactor.py:358] Since rank of parameter decoder/layers_23/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:13.193776 140260698863616 adafactor.py:358] Since rank of parameter decoder/layers_23/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:13.195208 140260698863616 adafactor.py:358] Since rank of parameter decoder/layers_23/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:13.196338 140260698863616 adafactor.py:358] Since rank of parameter decoder/layers_23/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:13.197464 140260698863616 adafactor.py:358] Since rank of parameter decoder/layers_3/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:13.198623 140260698863616 adafactor.py:358] Since rank of parameter decoder/layers_3/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:13.199773 140260698863616 adafactor.py:358] Since rank of parameter decoder/layers_3/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:13.200888 140260698863616 adafactor.py:358] Since rank of parameter decoder/layers_3/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:13.201797 140260698863616 adafactor.py:358] Since rank of parameter decoder/layers_3/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:13.202801 140260698863616 adafactor.py:358] Since rank of parameter decoder/layers_3/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:13.203921 140260698863616 adafactor.py:358] Since rank of parameter decoder/layers_3/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:13.205044 140260698863616 adafactor.py:358] Since rank of parameter decoder/layers_3/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:13.206165 140260698863616 adafactor.py:358] Since rank of parameter decoder/layers_3/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:13.207384 140260698863616 adafactor.py:358] Since rank of parameter decoder/layers_4/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:13.208520 140260698863616 adafactor.py:358] Since rank of parameter decoder/layers_4/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:13.209641 140260698863616 adafactor.py:358] Since rank of parameter decoder/layers_4/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:13.210794 140260698863616 adafactor.py:358] Since rank of parameter decoder/layers_4/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:13.211772 140260698863616 adafactor.py:358] Since rank of parameter decoder/layers_4/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:13.212690 140260698863616 adafactor.py:358] Since rank of parameter decoder/layers_4/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:13.213801 140260698863616 adafactor.py:358] Since rank of parameter decoder/layers_4/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:13.214982 140260698863616 adafactor.py:358] Since rank of parameter decoder/layers_4/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:13.216153 140260698863616 adafactor.py:358] Since rank of parameter decoder/layers_4/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:13.217271 140260698863616 adafactor.py:358] Since rank of parameter decoder/layers_5/extra_mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:13.218390 140260698863616 adafactor.py:358] Since rank of parameter decoder/layers_5/extra_mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:13.219544 140260698863616 adafactor.py:358] Since rank of parameter decoder/layers_5/extra_mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:13.223769 140260698863616 adafactor.py:358] Since rank of parameter decoder/layers_5/mlp/router/router_weights/w/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>, <FactorDim.NONE: None>) is ignored.
W0512 23:45:13.225166 140260698863616 adafactor.py:358] Since rank of parameter decoder/layers_5/pre_extra_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:13.226098 140260698863616 adafactor.py:358] Since rank of parameter decoder/layers_5/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:13.227062 140260698863616 adafactor.py:358] Since rank of parameter decoder/layers_5/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:13.227980 140260698863616 adafactor.py:358] Since rank of parameter decoder/layers_5/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:13.229088 140260698863616 adafactor.py:358] Since rank of parameter decoder/layers_5/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:13.230241 140260698863616 adafactor.py:358] Since rank of parameter decoder/layers_5/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:13.231383 140260698863616 adafactor.py:358] Since rank of parameter decoder/layers_5/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:13.232486 140260698863616 adafactor.py:358] Since rank of parameter decoder/layers_6/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:13.233608 140260698863616 adafactor.py:358] Since rank of parameter decoder/layers_6/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:13.234825 140260698863616 adafactor.py:358] Since rank of parameter decoder/layers_6/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:13.235930 140260698863616 adafactor.py:358] Since rank of parameter decoder/layers_6/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:13.236835 140260698863616 adafactor.py:358] Since rank of parameter decoder/layers_6/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:13.237742 140260698863616 adafactor.py:358] Since rank of parameter decoder/layers_6/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:13.238912 140260698863616 adafactor.py:358] Since rank of parameter decoder/layers_6/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:13.240041 140260698863616 adafactor.py:358] Since rank of parameter decoder/layers_6/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:13.241146 140260698863616 adafactor.py:358] Since rank of parameter decoder/layers_6/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:13.242259 140260698863616 adafactor.py:358] Since rank of parameter decoder/layers_7/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:13.243454 140260698863616 adafactor.py:358] Since rank of parameter decoder/layers_7/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:13.244571 140260698863616 adafactor.py:358] Since rank of parameter decoder/layers_7/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:13.245709 140260698863616 adafactor.py:358] Since rank of parameter decoder/layers_7/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:13.246621 140260698863616 adafactor.py:358] Since rank of parameter decoder/layers_7/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:13.247617 140260698863616 adafactor.py:358] Since rank of parameter decoder/layers_7/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:13.248745 140260698863616 adafactor.py:358] Since rank of parameter decoder/layers_7/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:13.249877 140260698863616 adafactor.py:358] Since rank of parameter decoder/layers_7/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:13.251033 140260698863616 adafactor.py:358] Since rank of parameter decoder/layers_7/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:13.252212 140260698863616 adafactor.py:358] Since rank of parameter decoder/layers_8/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:13.253341 140260698863616 adafactor.py:358] Since rank of parameter decoder/layers_8/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:13.254483 140260698863616 adafactor.py:358] Since rank of parameter decoder/layers_8/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:13.255638 140260698863616 adafactor.py:358] Since rank of parameter decoder/layers_8/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:13.256603 140260698863616 adafactor.py:358] Since rank of parameter decoder/layers_8/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:13.257517 140260698863616 adafactor.py:358] Since rank of parameter decoder/layers_8/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:13.258622 140260698863616 adafactor.py:358] Since rank of parameter decoder/layers_8/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:13.259771 140260698863616 adafactor.py:358] Since rank of parameter decoder/layers_8/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:13.260921 140260698863616 adafactor.py:358] Since rank of parameter decoder/layers_8/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:13.262062 140260698863616 adafactor.py:358] Since rank of parameter decoder/layers_9/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:13.263495 140260698863616 adafactor.py:358] Since rank of parameter decoder/layers_9/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:13.264617 140260698863616 adafactor.py:358] Since rank of parameter decoder/layers_9/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:13.265775 140260698863616 adafactor.py:358] Since rank of parameter decoder/layers_9/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:13.266728 140260698863616 adafactor.py:358] Since rank of parameter decoder/layers_9/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:13.267654 140260698863616 adafactor.py:358] Since rank of parameter decoder/layers_9/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:13.268757 140260698863616 adafactor.py:358] Since rank of parameter decoder/layers_9/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:13.269862 140260698863616 adafactor.py:358] Since rank of parameter decoder/layers_9/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:13.271067 140260698863616 adafactor.py:358] Since rank of parameter decoder/layers_9/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:13.272172 140260698863616 adafactor.py:358] Since rank of parameter decoder/logits_dense/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:13.273287 140260698863616 adafactor.py:358] Since rank of parameter token_embedder/embedding 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
I0512 23:45:13.364804 139856202532864 utils.py:952] Using latest T5X checkpoint.
I0512 23:45:13.494462 140180528838656 utils.py:952] Using latest T5X checkpoint.
I0512 23:45:13.556386 140143678494720 utils.py:442] Initializing parameters from specific T5X checkpoint gs://rosinality-tpu-bucket/openmoe_8b_ul2/training/checkpoint_10000
I0512 23:45:13.640884 140193652815872 utils.py:952] Using latest T5X checkpoint.
I0512 23:45:13.749680 140643590531072 utils.py:442] Initializing parameters from specific T5X checkpoint gs://rosinality-tpu-bucket/openmoe_8b_ul2/training/checkpoint_10000
I0512 23:45:13.848048 139666521167872 utils.py:442] Initializing parameters from specific T5X checkpoint gs://rosinality-tpu-bucket/openmoe_8b_ul2/training/checkpoint_10000
I0512 23:45:13.850751 139876724979712 utils.py:442] Initializing parameters from specific T5X checkpoint gs://rosinality-tpu-bucket/openmoe_8b_ul2/training/checkpoint_10000
I0512 23:45:13.846920 140260698863616 utils.py:952] Using latest T5X checkpoint.
I0512 23:45:13.954388 140143678494720 checkpoints.py:1054] Restoring from checkpoint: gs://rosinality-tpu-bucket/openmoe_8b_ul2/training/checkpoint_10000/checkpoint
I0512 23:45:14.023684 140643590531072 checkpoints.py:1054] Restoring from checkpoint: gs://rosinality-tpu-bucket/openmoe_8b_ul2/training/checkpoint_10000/checkpoint
I0512 23:45:14.145240 139666521167872 checkpoints.py:1054] Restoring from checkpoint: gs://rosinality-tpu-bucket/openmoe_8b_ul2/training/checkpoint_10000/checkpoint
I0512 23:45:14.238188 139876724979712 checkpoints.py:1054] Restoring from checkpoint: gs://rosinality-tpu-bucket/openmoe_8b_ul2/training/checkpoint_10000/checkpoint
I0512 23:45:14.258839 139856202532864 utils.py:442] Initializing parameters from specific T5X checkpoint gs://rosinality-tpu-bucket/openmoe_8b_ul2/training/checkpoint_10000
I0512 23:45:14.324719 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/decoder_norm/scale/m
I0512 23:45:14.324866 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/decoder_norm/scale/v
I0512 23:45:14.324909 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/decoder_norm/scale/v_col
I0512 23:45:14.324945 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/decoder_norm/scale/v_row
I0512 23:45:14.324982 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_0/mlp/wi_0/kernel/m
I0512 23:45:14.325017 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_0/mlp/wi_0/kernel/v
I0512 23:45:14.325052 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_0/mlp/wi_0/kernel/v_col
I0512 23:45:14.325087 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_0/mlp/wi_0/kernel/v_row
I0512 23:45:14.325122 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_0/mlp/wi_1/kernel/m
I0512 23:45:14.325157 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_0/mlp/wi_1/kernel/v
I0512 23:45:14.325192 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_0/mlp/wi_1/kernel/v_col
I0512 23:45:14.325226 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_0/mlp/wi_1/kernel/v_row
I0512 23:45:14.325261 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_0/mlp/wo/kernel/m
I0512 23:45:14.325303 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_0/mlp/wo/kernel/v
I0512 23:45:14.325339 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_0/mlp/wo/kernel/v_col
I0512 23:45:14.325374 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_0/mlp/wo/kernel/v_row
I0512 23:45:14.325408 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_0/pre_mlp_layer_norm/scale/m
I0512 23:45:14.325444 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_0/pre_mlp_layer_norm/scale/v
I0512 23:45:14.325479 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_0/pre_mlp_layer_norm/scale/v_col
I0512 23:45:14.325513 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_0/pre_mlp_layer_norm/scale/v_row
I0512 23:45:14.325550 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_0/pre_self_attention_layer_norm/scale/m
I0512 23:45:14.325586 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_0/pre_self_attention_layer_norm/scale/v
I0512 23:45:14.325620 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_0/pre_self_attention_layer_norm/scale/v_col
I0512 23:45:14.325655 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_0/pre_self_attention_layer_norm/scale/v_row
I0512 23:45:14.325690 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_0/self_attention/key/kernel/m
I0512 23:45:14.325725 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_0/self_attention/key/kernel/v
I0512 23:45:14.325759 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_0/self_attention/key/kernel/v_col
I0512 23:45:14.325794 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_0/self_attention/key/kernel/v_row
I0512 23:45:14.325829 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_0/self_attention/out/kernel/m
I0512 23:45:14.325863 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_0/self_attention/out/kernel/v
I0512 23:45:14.325898 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_0/self_attention/out/kernel/v_col
I0512 23:45:14.325932 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_0/self_attention/out/kernel/v_row
I0512 23:45:14.325968 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_0/self_attention/query/kernel/m
I0512 23:45:14.326002 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_0/self_attention/query/kernel/v
I0512 23:45:14.326037 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_0/self_attention/query/kernel/v_col
I0512 23:45:14.326071 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_0/self_attention/query/kernel/v_row
I0512 23:45:14.326106 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_0/self_attention/value/kernel/m
I0512 23:45:14.326140 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_0/self_attention/value/kernel/v
I0512 23:45:14.326175 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_0/self_attention/value/kernel/v_col
I0512 23:45:14.326210 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_0/self_attention/value/kernel/v_row
I0512 23:45:14.326244 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_1/mlp/wi_0/kernel/m
I0512 23:45:14.326284 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_1/mlp/wi_0/kernel/v
I0512 23:45:14.326320 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_1/mlp/wi_0/kernel/v_col
I0512 23:45:14.326355 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_1/mlp/wi_0/kernel/v_row
I0512 23:45:14.326390 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_1/mlp/wi_1/kernel/m
I0512 23:45:14.326424 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_1/mlp/wi_1/kernel/v
I0512 23:45:14.326458 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_1/mlp/wi_1/kernel/v_col
I0512 23:45:14.326493 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_1/mlp/wi_1/kernel/v_row
I0512 23:45:14.326527 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_1/mlp/wo/kernel/m
I0512 23:45:14.326564 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_1/mlp/wo/kernel/v
I0512 23:45:14.326599 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_1/mlp/wo/kernel/v_col
I0512 23:45:14.326633 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_1/mlp/wo/kernel/v_row
I0512 23:45:14.326668 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_1/pre_mlp_layer_norm/scale/m
I0512 23:45:14.326703 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_1/pre_mlp_layer_norm/scale/v
I0512 23:45:14.326738 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_1/pre_mlp_layer_norm/scale/v_col
I0512 23:45:14.326772 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_1/pre_mlp_layer_norm/scale/v_row
I0512 23:45:14.326807 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_1/pre_self_attention_layer_norm/scale/m
I0512 23:45:14.326842 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_1/pre_self_attention_layer_norm/scale/v
I0512 23:45:14.326877 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_1/pre_self_attention_layer_norm/scale/v_col
I0512 23:45:14.326911 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_1/pre_self_attention_layer_norm/scale/v_row
I0512 23:45:14.326946 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_1/self_attention/key/kernel/m
I0512 23:45:14.326981 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_1/self_attention/key/kernel/v
I0512 23:45:14.327016 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_1/self_attention/key/kernel/v_col
I0512 23:45:14.327051 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_1/self_attention/key/kernel/v_row
I0512 23:45:14.327085 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_1/self_attention/out/kernel/m
I0512 23:45:14.327121 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_1/self_attention/out/kernel/v
I0512 23:45:14.327156 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_1/self_attention/out/kernel/v_col
I0512 23:45:14.327191 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_1/self_attention/out/kernel/v_row
I0512 23:45:14.327225 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_1/self_attention/query/kernel/m
I0512 23:45:14.327260 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_1/self_attention/query/kernel/v
I0512 23:45:14.327300 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_1/self_attention/query/kernel/v_col
I0512 23:45:14.327337 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_1/self_attention/query/kernel/v_row
I0512 23:45:14.327372 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_1/self_attention/value/kernel/m
I0512 23:45:14.327406 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_1/self_attention/value/kernel/v
I0512 23:45:14.327441 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_1/self_attention/value/kernel/v_col
I0512 23:45:14.327476 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_1/self_attention/value/kernel/v_row
I0512 23:45:14.327510 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_10/mlp/wi_0/kernel/m
I0512 23:45:14.327546 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_10/mlp/wi_0/kernel/v
I0512 23:45:14.327582 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_10/mlp/wi_0/kernel/v_col
I0512 23:45:14.327617 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_10/mlp/wi_0/kernel/v_row
I0512 23:45:14.327651 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_10/mlp/wi_1/kernel/m
I0512 23:45:14.327686 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_10/mlp/wi_1/kernel/v
I0512 23:45:14.327721 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_10/mlp/wi_1/kernel/v_col
I0512 23:45:14.327756 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_10/mlp/wi_1/kernel/v_row
I0512 23:45:14.327790 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_10/mlp/wo/kernel/m
I0512 23:45:14.327825 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_10/mlp/wo/kernel/v
I0512 23:45:14.327859 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_10/mlp/wo/kernel/v_col
I0512 23:45:14.327893 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_10/mlp/wo/kernel/v_row
I0512 23:45:14.327928 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_10/pre_mlp_layer_norm/scale/m
I0512 23:45:14.327963 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_10/pre_mlp_layer_norm/scale/v
I0512 23:45:14.327997 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_10/pre_mlp_layer_norm/scale/v_col
I0512 23:45:14.328032 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_10/pre_mlp_layer_norm/scale/v_row
I0512 23:45:14.328067 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_10/pre_self_attention_layer_norm/scale/m
I0512 23:45:14.328101 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_10/pre_self_attention_layer_norm/scale/v
I0512 23:45:14.328136 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_10/pre_self_attention_layer_norm/scale/v_col
I0512 23:45:14.328171 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_10/pre_self_attention_layer_norm/scale/v_row
I0512 23:45:14.328206 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_10/self_attention/key/kernel/m
I0512 23:45:14.328241 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_10/self_attention/key/kernel/v
I0512 23:45:14.328275 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_10/self_attention/key/kernel/v_col
I0512 23:45:14.328318 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_10/self_attention/key/kernel/v_row
I0512 23:45:14.328353 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_10/self_attention/out/kernel/m
I0512 23:45:14.328388 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_10/self_attention/out/kernel/v
I0512 23:45:14.328423 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_10/self_attention/out/kernel/v_col
I0512 23:45:14.328457 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_10/self_attention/out/kernel/v_row
I0512 23:45:14.328491 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_10/self_attention/query/kernel/m
I0512 23:45:14.328526 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_10/self_attention/query/kernel/v
I0512 23:45:14.328584 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_10/self_attention/query/kernel/v_col
I0512 23:45:14.328620 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_10/self_attention/query/kernel/v_row
I0512 23:45:14.328656 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_10/self_attention/value/kernel/m
I0512 23:45:14.328691 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_10/self_attention/value/kernel/v
I0512 23:45:14.328725 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_10/self_attention/value/kernel/v_col
I0512 23:45:14.328760 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_10/self_attention/value/kernel/v_row
I0512 23:45:14.328795 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/extra_mlp/wi_0/kernel/m
I0512 23:45:14.328830 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/extra_mlp/wi_0/kernel/v
I0512 23:45:14.328864 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/extra_mlp/wi_0/kernel/v_col
I0512 23:45:14.328899 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/extra_mlp/wi_0/kernel/v_row
I0512 23:45:14.328933 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/extra_mlp/wi_1/kernel/m
I0512 23:45:14.328968 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/extra_mlp/wi_1/kernel/v
I0512 23:45:14.329002 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/extra_mlp/wi_1/kernel/v_col
I0512 23:45:14.329037 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/extra_mlp/wi_1/kernel/v_row
I0512 23:45:14.329072 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/extra_mlp/wo/kernel/m
I0512 23:45:14.329107 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/extra_mlp/wo/kernel/v
I0512 23:45:14.329141 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/extra_mlp/wo/kernel/v_col
I0512 23:45:14.329176 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/extra_mlp/wo/kernel/v_row
I0512 23:45:14.329210 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/mlp/expert/wi_0/kernel/m
I0512 23:45:14.329245 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/mlp/expert/wi_0/kernel/v
I0512 23:45:14.329284 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/mlp/expert/wi_0/kernel/v_col
I0512 23:45:14.329320 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/mlp/expert/wi_0/kernel/v_row
I0512 23:45:14.329355 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/mlp/expert/wi_1/kernel/m
I0512 23:45:14.329390 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/mlp/expert/wi_1/kernel/v
I0512 23:45:14.329424 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/mlp/expert/wi_1/kernel/v_col
I0512 23:45:14.329459 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/mlp/expert/wi_1/kernel/v_row
I0512 23:45:14.329494 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/mlp/expert/wo/kernel/m
I0512 23:45:14.329529 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/mlp/expert/wo/kernel/v
I0512 23:45:14.329565 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/mlp/expert/wo/kernel/v_col
I0512 23:45:14.329600 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/mlp/expert/wo/kernel/v_row
I0512 23:45:14.329635 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/mlp/router/router_weights/w/kernel/m
I0512 23:45:14.329669 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/mlp/router/router_weights/w/kernel/v
I0512 23:45:14.329704 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/mlp/router/router_weights/w/kernel/v_col
I0512 23:45:14.329739 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/mlp/router/router_weights/w/kernel/v_row
I0512 23:45:14.329773 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/pre_extra_mlp_layer_norm/scale/m
I0512 23:45:14.329808 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/pre_extra_mlp_layer_norm/scale/v
I0512 23:45:14.329843 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/pre_extra_mlp_layer_norm/scale/v_col
I0512 23:45:14.329878 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/pre_extra_mlp_layer_norm/scale/v_row
I0512 23:45:14.329912 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/pre_mlp_layer_norm/scale/m
I0512 23:45:14.329947 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/pre_mlp_layer_norm/scale/v
I0512 23:45:14.329982 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/pre_mlp_layer_norm/scale/v_col
I0512 23:45:14.330016 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/pre_mlp_layer_norm/scale/v_row
I0512 23:45:14.330051 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/pre_self_attention_layer_norm/scale/m
I0512 23:45:14.330086 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/pre_self_attention_layer_norm/scale/v
I0512 23:45:14.330120 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/pre_self_attention_layer_norm/scale/v_col
I0512 23:45:14.330155 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/pre_self_attention_layer_norm/scale/v_row
I0512 23:45:14.330190 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/self_attention/key/kernel/m
I0512 23:45:14.330225 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/self_attention/key/kernel/v
I0512 23:45:14.330260 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/self_attention/key/kernel/v_col
I0512 23:45:14.330302 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/self_attention/key/kernel/v_row
I0512 23:45:14.330337 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/self_attention/out/kernel/m
I0512 23:45:14.330372 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/self_attention/out/kernel/v
I0512 23:45:14.330406 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/self_attention/out/kernel/v_col
I0512 23:45:14.330441 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/self_attention/out/kernel/v_row
I0512 23:45:14.330476 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/self_attention/query/kernel/m
I0512 23:45:14.330511 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/self_attention/query/kernel/v
I0512 23:45:14.330548 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/self_attention/query/kernel/v_col
I0512 23:45:14.330583 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/self_attention/query/kernel/v_row
I0512 23:45:14.330618 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/self_attention/value/kernel/m
I0512 23:45:14.330653 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/self_attention/value/kernel/v
I0512 23:45:14.330687 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/self_attention/value/kernel/v_col
I0512 23:45:14.330722 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/self_attention/value/kernel/v_row
I0512 23:45:14.330757 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_12/mlp/wi_0/kernel/m
I0512 23:45:14.330792 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_12/mlp/wi_0/kernel/v
I0512 23:45:14.330827 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_12/mlp/wi_0/kernel/v_col
I0512 23:45:14.330864 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_12/mlp/wi_0/kernel/v_row
I0512 23:45:14.330901 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_12/mlp/wi_1/kernel/m
I0512 23:45:14.330935 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_12/mlp/wi_1/kernel/v
I0512 23:45:14.330970 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_12/mlp/wi_1/kernel/v_col
I0512 23:45:14.331005 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_12/mlp/wi_1/kernel/v_row
I0512 23:45:14.331039 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_12/mlp/wo/kernel/m
I0512 23:45:14.331074 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_12/mlp/wo/kernel/v
I0512 23:45:14.331109 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_12/mlp/wo/kernel/v_col
I0512 23:45:14.331143 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_12/mlp/wo/kernel/v_row
I0512 23:45:14.331178 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_12/pre_mlp_layer_norm/scale/m
I0512 23:45:14.331212 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_12/pre_mlp_layer_norm/scale/v
I0512 23:45:14.331247 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_12/pre_mlp_layer_norm/scale/v_col
I0512 23:45:14.331287 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_12/pre_mlp_layer_norm/scale/v_row
I0512 23:45:14.331323 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_12/pre_self_attention_layer_norm/scale/m
I0512 23:45:14.331359 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_12/pre_self_attention_layer_norm/scale/v
I0512 23:45:14.331393 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_12/pre_self_attention_layer_norm/scale/v_col
I0512 23:45:14.331429 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_12/pre_self_attention_layer_norm/scale/v_row
I0512 23:45:14.331464 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_12/self_attention/key/kernel/m
I0512 23:45:14.331499 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_12/self_attention/key/kernel/v
I0512 23:45:14.331535 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_12/self_attention/key/kernel/v_col
I0512 23:45:14.331572 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_12/self_attention/key/kernel/v_row
I0512 23:45:14.331606 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_12/self_attention/out/kernel/m
I0512 23:45:14.331696 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_12/self_attention/out/kernel/v
I0512 23:45:14.331732 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_12/self_attention/out/kernel/v_col
I0512 23:45:14.331768 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_12/self_attention/out/kernel/v_row
I0512 23:45:14.331802 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_12/self_attention/query/kernel/m
I0512 23:45:14.331837 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_12/self_attention/query/kernel/v
I0512 23:45:14.331871 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_12/self_attention/query/kernel/v_col
I0512 23:45:14.331906 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_12/self_attention/query/kernel/v_row
I0512 23:45:14.331943 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_12/self_attention/value/kernel/m
I0512 23:45:14.331978 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_12/self_attention/value/kernel/v
I0512 23:45:14.332013 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_12/self_attention/value/kernel/v_col
I0512 23:45:14.332048 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_12/self_attention/value/kernel/v_row
I0512 23:45:14.332082 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_13/mlp/wi_0/kernel/m
I0512 23:45:14.332118 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_13/mlp/wi_0/kernel/v
I0512 23:45:14.332153 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_13/mlp/wi_0/kernel/v_col
I0512 23:45:14.332187 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_13/mlp/wi_0/kernel/v_row
I0512 23:45:14.332223 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_13/mlp/wi_1/kernel/m
I0512 23:45:14.332257 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_13/mlp/wi_1/kernel/v
I0512 23:45:14.332298 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_13/mlp/wi_1/kernel/v_col
I0512 23:45:14.332334 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_13/mlp/wi_1/kernel/v_row
I0512 23:45:14.332369 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_13/mlp/wo/kernel/m
I0512 23:45:14.332404 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_13/mlp/wo/kernel/v
I0512 23:45:14.332438 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_13/mlp/wo/kernel/v_col
I0512 23:45:14.332473 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_13/mlp/wo/kernel/v_row
I0512 23:45:14.332508 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_13/pre_mlp_layer_norm/scale/m
I0512 23:45:14.332563 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_13/pre_mlp_layer_norm/scale/v
I0512 23:45:14.332602 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_13/pre_mlp_layer_norm/scale/v_col
I0512 23:45:14.332637 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_13/pre_mlp_layer_norm/scale/v_row
I0512 23:45:14.332672 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_13/pre_self_attention_layer_norm/scale/m
I0512 23:45:14.332707 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_13/pre_self_attention_layer_norm/scale/v
I0512 23:45:14.332742 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_13/pre_self_attention_layer_norm/scale/v_col
I0512 23:45:14.332777 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_13/pre_self_attention_layer_norm/scale/v_row
I0512 23:45:14.332812 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_13/self_attention/key/kernel/m
I0512 23:45:14.332847 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_13/self_attention/key/kernel/v
I0512 23:45:14.332881 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_13/self_attention/key/kernel/v_col
I0512 23:45:14.332916 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_13/self_attention/key/kernel/v_row
I0512 23:45:14.332951 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_13/self_attention/out/kernel/m
I0512 23:45:14.332985 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_13/self_attention/out/kernel/v
I0512 23:45:14.333020 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_13/self_attention/out/kernel/v_col
I0512 23:45:14.333055 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_13/self_attention/out/kernel/v_row
I0512 23:45:14.333089 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_13/self_attention/query/kernel/m
I0512 23:45:14.333124 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_13/self_attention/query/kernel/v
I0512 23:45:14.333158 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_13/self_attention/query/kernel/v_col
I0512 23:45:14.333193 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_13/self_attention/query/kernel/v_row
I0512 23:45:14.333227 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_13/self_attention/value/kernel/m
I0512 23:45:14.333261 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_13/self_attention/value/kernel/v
I0512 23:45:14.333302 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_13/self_attention/value/kernel/v_col
I0512 23:45:14.333337 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_13/self_attention/value/kernel/v_row
I0512 23:45:14.333372 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_14/mlp/wi_0/kernel/m
I0512 23:45:14.333408 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_14/mlp/wi_0/kernel/v
I0512 23:45:14.333442 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_14/mlp/wi_0/kernel/v_col
I0512 23:45:14.333477 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_14/mlp/wi_0/kernel/v_row
I0512 23:45:14.333512 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_14/mlp/wi_1/kernel/m
I0512 23:45:14.333549 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_14/mlp/wi_1/kernel/v
I0512 23:45:14.333584 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_14/mlp/wi_1/kernel/v_col
I0512 23:45:14.333619 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_14/mlp/wi_1/kernel/v_row
I0512 23:45:14.333654 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_14/mlp/wo/kernel/m
I0512 23:45:14.333688 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_14/mlp/wo/kernel/v
I0512 23:45:14.333723 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_14/mlp/wo/kernel/v_col
I0512 23:45:14.333758 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_14/mlp/wo/kernel/v_row
I0512 23:45:14.333793 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_14/pre_mlp_layer_norm/scale/m
I0512 23:45:14.333827 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_14/pre_mlp_layer_norm/scale/v
I0512 23:45:14.333863 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_14/pre_mlp_layer_norm/scale/v_col
I0512 23:45:14.333898 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_14/pre_mlp_layer_norm/scale/v_row
I0512 23:45:14.333933 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_14/pre_self_attention_layer_norm/scale/m
I0512 23:45:14.333968 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_14/pre_self_attention_layer_norm/scale/v
I0512 23:45:14.334003 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_14/pre_self_attention_layer_norm/scale/v_col
I0512 23:45:14.334038 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_14/pre_self_attention_layer_norm/scale/v_row
I0512 23:45:14.334073 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_14/self_attention/key/kernel/m
I0512 23:45:14.334108 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_14/self_attention/key/kernel/v
I0512 23:45:14.334143 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_14/self_attention/key/kernel/v_col
I0512 23:45:14.334178 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_14/self_attention/key/kernel/v_row
I0512 23:45:14.334239 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_14/self_attention/out/kernel/m
I0512 23:45:14.334275 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_14/self_attention/out/kernel/v
I0512 23:45:14.334316 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_14/self_attention/out/kernel/v_col
I0512 23:45:14.334352 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_14/self_attention/out/kernel/v_row
I0512 23:45:14.334386 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_14/self_attention/query/kernel/m
I0512 23:45:14.334421 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_14/self_attention/query/kernel/v
I0512 23:45:14.334455 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_14/self_attention/query/kernel/v_col
I0512 23:45:14.334490 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_14/self_attention/query/kernel/v_row
I0512 23:45:14.334524 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_14/self_attention/value/kernel/m
I0512 23:45:14.334562 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_14/self_attention/value/kernel/v
I0512 23:45:14.334597 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_14/self_attention/value/kernel/v_col
I0512 23:45:14.334631 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_14/self_attention/value/kernel/v_row
I0512 23:45:14.334666 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_15/mlp/wi_0/kernel/m
I0512 23:45:14.334701 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_15/mlp/wi_0/kernel/v
I0512 23:45:14.334736 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_15/mlp/wi_0/kernel/v_col
I0512 23:45:14.334771 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_15/mlp/wi_0/kernel/v_row
I0512 23:45:14.334805 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_15/mlp/wi_1/kernel/m
I0512 23:45:14.334839 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_15/mlp/wi_1/kernel/v
I0512 23:45:14.334874 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_15/mlp/wi_1/kernel/v_col
I0512 23:45:14.334909 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_15/mlp/wi_1/kernel/v_row
I0512 23:45:14.334944 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_15/mlp/wo/kernel/m
I0512 23:45:14.334978 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_15/mlp/wo/kernel/v
I0512 23:45:14.335013 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_15/mlp/wo/kernel/v_col
I0512 23:45:14.335047 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_15/mlp/wo/kernel/v_row
I0512 23:45:14.335082 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_15/pre_mlp_layer_norm/scale/m
I0512 23:45:14.335117 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_15/pre_mlp_layer_norm/scale/v
I0512 23:45:14.335151 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_15/pre_mlp_layer_norm/scale/v_col
I0512 23:45:14.335186 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_15/pre_mlp_layer_norm/scale/v_row
I0512 23:45:14.335220 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_15/pre_self_attention_layer_norm/scale/m
I0512 23:45:14.335255 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_15/pre_self_attention_layer_norm/scale/v
I0512 23:45:14.335294 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_15/pre_self_attention_layer_norm/scale/v_col
I0512 23:45:14.335330 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_15/pre_self_attention_layer_norm/scale/v_row
I0512 23:45:14.335366 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_15/self_attention/key/kernel/m
I0512 23:45:14.335401 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_15/self_attention/key/kernel/v
I0512 23:45:14.335436 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_15/self_attention/key/kernel/v_col
I0512 23:45:14.335470 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_15/self_attention/key/kernel/v_row
I0512 23:45:14.335505 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_15/self_attention/out/kernel/m
I0512 23:45:14.335541 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_15/self_attention/out/kernel/v
I0512 23:45:14.335577 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_15/self_attention/out/kernel/v_col
I0512 23:45:14.335612 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_15/self_attention/out/kernel/v_row
I0512 23:45:14.335646 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_15/self_attention/query/kernel/m
I0512 23:45:14.335681 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_15/self_attention/query/kernel/v
I0512 23:45:14.335716 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_15/self_attention/query/kernel/v_col
I0512 23:45:14.335756 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_15/self_attention/query/kernel/v_row
I0512 23:45:14.335794 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_15/self_attention/value/kernel/m
I0512 23:45:14.335834 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_15/self_attention/value/kernel/v
I0512 23:45:14.335871 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_15/self_attention/value/kernel/v_col
I0512 23:45:14.335906 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_15/self_attention/value/kernel/v_row
I0512 23:45:14.335946 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_16/mlp/wi_0/kernel/m
I0512 23:45:14.335983 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_16/mlp/wi_0/kernel/v
I0512 23:45:14.336017 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_16/mlp/wi_0/kernel/v_col
I0512 23:45:14.336051 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_16/mlp/wi_0/kernel/v_row
I0512 23:45:14.336085 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_16/mlp/wi_1/kernel/m
I0512 23:45:14.336119 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_16/mlp/wi_1/kernel/v
I0512 23:45:14.336153 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_16/mlp/wi_1/kernel/v_col
I0512 23:45:14.336187 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_16/mlp/wi_1/kernel/v_row
I0512 23:45:14.336220 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_16/mlp/wo/kernel/m
I0512 23:45:14.336254 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_16/mlp/wo/kernel/v
I0512 23:45:14.336294 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_16/mlp/wo/kernel/v_col
I0512 23:45:14.336329 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_16/mlp/wo/kernel/v_row
I0512 23:45:14.336363 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_16/pre_mlp_layer_norm/scale/m
I0512 23:45:14.336396 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_16/pre_mlp_layer_norm/scale/v
I0512 23:45:14.336430 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_16/pre_mlp_layer_norm/scale/v_col
I0512 23:45:14.336465 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_16/pre_mlp_layer_norm/scale/v_row
I0512 23:45:14.336499 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_16/pre_self_attention_layer_norm/scale/m
I0512 23:45:14.336549 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_16/pre_self_attention_layer_norm/scale/v
I0512 23:45:14.336588 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_16/pre_self_attention_layer_norm/scale/v_col
I0512 23:45:14.336622 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_16/pre_self_attention_layer_norm/scale/v_row
I0512 23:45:14.336657 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_16/self_attention/key/kernel/m
I0512 23:45:14.336690 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_16/self_attention/key/kernel/v
I0512 23:45:14.336724 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_16/self_attention/key/kernel/v_col
I0512 23:45:14.336757 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_16/self_attention/key/kernel/v_row
I0512 23:45:14.336790 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_16/self_attention/out/kernel/m
I0512 23:45:14.336823 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_16/self_attention/out/kernel/v
I0512 23:45:14.336859 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_16/self_attention/out/kernel/v_col
I0512 23:45:14.336894 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_16/self_attention/out/kernel/v_row
I0512 23:45:14.336927 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_16/self_attention/query/kernel/m
I0512 23:45:14.336961 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_16/self_attention/query/kernel/v
I0512 23:45:14.336994 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_16/self_attention/query/kernel/v_col
I0512 23:45:14.337027 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_16/self_attention/query/kernel/v_row
I0512 23:45:14.337060 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_16/self_attention/value/kernel/m
I0512 23:45:14.337093 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_16/self_attention/value/kernel/v
I0512 23:45:14.337127 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_16/self_attention/value/kernel/v_col
I0512 23:45:14.337160 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_16/self_attention/value/kernel/v_row
I0512 23:45:14.337193 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/extra_mlp/wi_0/kernel/m
I0512 23:45:14.337226 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/extra_mlp/wi_0/kernel/v
I0512 23:45:14.337259 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/extra_mlp/wi_0/kernel/v_col
I0512 23:45:14.337300 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/extra_mlp/wi_0/kernel/v_row
I0512 23:45:14.337334 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/extra_mlp/wi_1/kernel/m
I0512 23:45:14.337367 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/extra_mlp/wi_1/kernel/v
I0512 23:45:14.337400 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/extra_mlp/wi_1/kernel/v_col
I0512 23:45:14.337433 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/extra_mlp/wi_1/kernel/v_row
I0512 23:45:14.337466 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/extra_mlp/wo/kernel/m
I0512 23:45:14.337499 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/extra_mlp/wo/kernel/v
I0512 23:45:14.337533 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/extra_mlp/wo/kernel/v_col
I0512 23:45:14.337568 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/extra_mlp/wo/kernel/v_row
I0512 23:45:14.337602 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/mlp/expert/wi_0/kernel/m
I0512 23:45:14.337635 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/mlp/expert/wi_0/kernel/v
I0512 23:45:14.337668 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/mlp/expert/wi_0/kernel/v_col
I0512 23:45:14.337702 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/mlp/expert/wi_0/kernel/v_row
I0512 23:45:14.337735 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/mlp/expert/wi_1/kernel/m
I0512 23:45:14.337768 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/mlp/expert/wi_1/kernel/v
I0512 23:45:14.337801 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/mlp/expert/wi_1/kernel/v_col
I0512 23:45:14.337833 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/mlp/expert/wi_1/kernel/v_row
I0512 23:45:14.337867 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/mlp/expert/wo/kernel/m
I0512 23:45:14.337899 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/mlp/expert/wo/kernel/v
I0512 23:45:14.337932 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/mlp/expert/wo/kernel/v_col
I0512 23:45:14.337965 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/mlp/expert/wo/kernel/v_row
I0512 23:45:14.337998 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/mlp/router/router_weights/w/kernel/m
I0512 23:45:14.338032 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/mlp/router/router_weights/w/kernel/v
I0512 23:45:14.338064 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/mlp/router/router_weights/w/kernel/v_col
I0512 23:45:14.338098 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/mlp/router/router_weights/w/kernel/v_row
I0512 23:45:14.338131 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/pre_extra_mlp_layer_norm/scale/m
I0512 23:45:14.338164 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/pre_extra_mlp_layer_norm/scale/v
I0512 23:45:14.338197 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/pre_extra_mlp_layer_norm/scale/v_col
I0512 23:45:14.338230 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/pre_extra_mlp_layer_norm/scale/v_row
I0512 23:45:14.338263 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/pre_mlp_layer_norm/scale/m
I0512 23:45:14.338302 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/pre_mlp_layer_norm/scale/v
I0512 23:45:14.338336 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/pre_mlp_layer_norm/scale/v_col
I0512 23:45:14.338369 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/pre_mlp_layer_norm/scale/v_row
I0512 23:45:14.338402 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/pre_self_attention_layer_norm/scale/m
I0512 23:45:14.338435 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/pre_self_attention_layer_norm/scale/v
I0512 23:45:14.338468 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/pre_self_attention_layer_norm/scale/v_col
I0512 23:45:14.338501 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/pre_self_attention_layer_norm/scale/v_row
I0512 23:45:14.338536 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/self_attention/key/kernel/m
I0512 23:45:14.338570 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/self_attention/key/kernel/v
I0512 23:45:14.338603 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/self_attention/key/kernel/v_col
I0512 23:45:14.338636 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/self_attention/key/kernel/v_row
I0512 23:45:14.338669 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/self_attention/out/kernel/m
I0512 23:45:14.338702 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/self_attention/out/kernel/v
I0512 23:45:14.338735 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/self_attention/out/kernel/v_col
I0512 23:45:14.338768 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/self_attention/out/kernel/v_row
I0512 23:45:14.338802 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/self_attention/query/kernel/m
I0512 23:45:14.338835 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/self_attention/query/kernel/v
I0512 23:45:14.338868 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/self_attention/query/kernel/v_col
I0512 23:45:14.338901 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/self_attention/query/kernel/v_row
I0512 23:45:14.338934 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/self_attention/value/kernel/m
I0512 23:45:14.338967 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/self_attention/value/kernel/v
I0512 23:45:14.339000 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/self_attention/value/kernel/v_col
I0512 23:45:14.339033 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/self_attention/value/kernel/v_row
I0512 23:45:14.339066 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_18/mlp/wi_0/kernel/m
I0512 23:45:14.339099 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_18/mlp/wi_0/kernel/v
I0512 23:45:14.339131 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_18/mlp/wi_0/kernel/v_col
I0512 23:45:14.339165 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_18/mlp/wi_0/kernel/v_row
I0512 23:45:14.339198 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_18/mlp/wi_1/kernel/m
I0512 23:45:14.339231 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_18/mlp/wi_1/kernel/v
I0512 23:45:14.339265 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_18/mlp/wi_1/kernel/v_col
I0512 23:45:14.339304 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_18/mlp/wi_1/kernel/v_row
I0512 23:45:14.339338 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_18/mlp/wo/kernel/m
I0512 23:45:14.339372 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_18/mlp/wo/kernel/v
I0512 23:45:14.339405 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_18/mlp/wo/kernel/v_col
I0512 23:45:14.339438 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_18/mlp/wo/kernel/v_row
I0512 23:45:14.339472 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_18/pre_mlp_layer_norm/scale/m
I0512 23:45:14.339505 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_18/pre_mlp_layer_norm/scale/v
I0512 23:45:14.339540 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_18/pre_mlp_layer_norm/scale/v_col
I0512 23:45:14.339575 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_18/pre_mlp_layer_norm/scale/v_row
I0512 23:45:14.339609 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_18/pre_self_attention_layer_norm/scale/m
I0512 23:45:14.339642 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_18/pre_self_attention_layer_norm/scale/v
I0512 23:45:14.339676 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_18/pre_self_attention_layer_norm/scale/v_col
I0512 23:45:14.339709 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_18/pre_self_attention_layer_norm/scale/v_row
I0512 23:45:14.339743 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_18/self_attention/key/kernel/m
I0512 23:45:14.339776 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_18/self_attention/key/kernel/v
I0512 23:45:14.339809 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_18/self_attention/key/kernel/v_col
I0512 23:45:14.339843 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_18/self_attention/key/kernel/v_row
I0512 23:45:14.339876 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_18/self_attention/out/kernel/m
I0512 23:45:14.339909 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_18/self_attention/out/kernel/v
I0512 23:45:14.339942 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_18/self_attention/out/kernel/v_col
I0512 23:45:14.339975 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_18/self_attention/out/kernel/v_row
I0512 23:45:14.340008 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_18/self_attention/query/kernel/m
I0512 23:45:14.340042 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_18/self_attention/query/kernel/v
I0512 23:45:14.340075 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_18/self_attention/query/kernel/v_col
I0512 23:45:14.340108 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_18/self_attention/query/kernel/v_row
I0512 23:45:14.340141 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_18/self_attention/value/kernel/m
I0512 23:45:14.340174 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_18/self_attention/value/kernel/v
I0512 23:45:14.340207 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_18/self_attention/value/kernel/v_col
I0512 23:45:14.340240 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_18/self_attention/value/kernel/v_row
I0512 23:45:14.340274 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_19/mlp/wi_0/kernel/m
I0512 23:45:14.340317 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_19/mlp/wi_0/kernel/v
I0512 23:45:14.340351 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_19/mlp/wi_0/kernel/v_col
I0512 23:45:14.340384 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_19/mlp/wi_0/kernel/v_row
I0512 23:45:14.340418 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_19/mlp/wi_1/kernel/m
I0512 23:45:14.340451 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_19/mlp/wi_1/kernel/v
I0512 23:45:14.340485 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_19/mlp/wi_1/kernel/v_col
I0512 23:45:14.340518 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_19/mlp/wi_1/kernel/v_row
I0512 23:45:14.340572 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_19/mlp/wo/kernel/m
I0512 23:45:14.340608 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_19/mlp/wo/kernel/v
I0512 23:45:14.340641 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_19/mlp/wo/kernel/v_col
I0512 23:45:14.340675 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_19/mlp/wo/kernel/v_row
I0512 23:45:14.340708 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_19/pre_mlp_layer_norm/scale/m
I0512 23:45:14.340742 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_19/pre_mlp_layer_norm/scale/v
I0512 23:45:14.340775 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_19/pre_mlp_layer_norm/scale/v_col
I0512 23:45:14.340809 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_19/pre_mlp_layer_norm/scale/v_row
I0512 23:45:14.340842 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_19/pre_self_attention_layer_norm/scale/m
I0512 23:45:14.340876 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_19/pre_self_attention_layer_norm/scale/v
I0512 23:45:14.340909 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_19/pre_self_attention_layer_norm/scale/v_col
I0512 23:45:14.340942 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_19/pre_self_attention_layer_norm/scale/v_row
I0512 23:45:14.340976 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_19/self_attention/key/kernel/m
I0512 23:45:14.341009 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_19/self_attention/key/kernel/v
I0512 23:45:14.341042 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_19/self_attention/key/kernel/v_col
I0512 23:45:14.341074 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_19/self_attention/key/kernel/v_row
I0512 23:45:14.341108 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_19/self_attention/out/kernel/m
I0512 23:45:14.341141 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_19/self_attention/out/kernel/v
I0512 23:45:14.341174 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_19/self_attention/out/kernel/v_col
I0512 23:45:14.341207 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_19/self_attention/out/kernel/v_row
I0512 23:45:14.341241 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_19/self_attention/query/kernel/m
I0512 23:45:14.341274 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_19/self_attention/query/kernel/v
I0512 23:45:14.341313 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_19/self_attention/query/kernel/v_col
I0512 23:45:14.341347 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_19/self_attention/query/kernel/v_row
I0512 23:45:14.341380 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_19/self_attention/value/kernel/m
I0512 23:45:14.341413 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_19/self_attention/value/kernel/v
I0512 23:45:14.341446 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_19/self_attention/value/kernel/v_col
I0512 23:45:14.341480 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_19/self_attention/value/kernel/v_row
I0512 23:45:14.341513 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_2/mlp/wi_0/kernel/m
I0512 23:45:14.341548 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_2/mlp/wi_0/kernel/v
I0512 23:45:14.341582 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_2/mlp/wi_0/kernel/v_col
I0512 23:45:14.341615 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_2/mlp/wi_0/kernel/v_row
I0512 23:45:14.341649 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_2/mlp/wi_1/kernel/m
I0512 23:45:14.341682 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_2/mlp/wi_1/kernel/v
I0512 23:45:14.341716 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_2/mlp/wi_1/kernel/v_col
I0512 23:45:14.341749 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_2/mlp/wi_1/kernel/v_row
I0512 23:45:14.341782 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_2/mlp/wo/kernel/m
I0512 23:45:14.341815 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_2/mlp/wo/kernel/v
I0512 23:45:14.341849 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_2/mlp/wo/kernel/v_col
I0512 23:45:14.341882 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_2/mlp/wo/kernel/v_row
I0512 23:45:14.341916 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_2/pre_mlp_layer_norm/scale/m
I0512 23:45:14.341949 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_2/pre_mlp_layer_norm/scale/v
I0512 23:45:14.341983 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_2/pre_mlp_layer_norm/scale/v_col
I0512 23:45:14.342015 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_2/pre_mlp_layer_norm/scale/v_row
I0512 23:45:14.342049 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_2/pre_self_attention_layer_norm/scale/m
I0512 23:45:14.342082 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_2/pre_self_attention_layer_norm/scale/v
I0512 23:45:14.342116 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_2/pre_self_attention_layer_norm/scale/v_col
I0512 23:45:14.342150 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_2/pre_self_attention_layer_norm/scale/v_row
I0512 23:45:14.342183 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_2/self_attention/key/kernel/m
I0512 23:45:14.342216 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_2/self_attention/key/kernel/v
I0512 23:45:14.342249 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_2/self_attention/key/kernel/v_col
I0512 23:45:14.342287 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_2/self_attention/key/kernel/v_row
I0512 23:45:14.342321 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_2/self_attention/out/kernel/m
I0512 23:45:14.342355 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_2/self_attention/out/kernel/v
I0512 23:45:14.342388 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_2/self_attention/out/kernel/v_col
I0512 23:45:14.342422 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_2/self_attention/out/kernel/v_row
I0512 23:45:14.342455 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_2/self_attention/query/kernel/m
I0512 23:45:14.342488 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_2/self_attention/query/kernel/v
I0512 23:45:14.342521 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_2/self_attention/query/kernel/v_col
I0512 23:45:14.342556 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_2/self_attention/query/kernel/v_row
I0512 23:45:14.342589 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_2/self_attention/value/kernel/m
I0512 23:45:14.342623 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_2/self_attention/value/kernel/v
I0512 23:45:14.342656 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_2/self_attention/value/kernel/v_col
I0512 23:45:14.342689 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_2/self_attention/value/kernel/v_row
I0512 23:45:14.342723 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_20/mlp/wi_0/kernel/m
I0512 23:45:14.342756 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_20/mlp/wi_0/kernel/v
I0512 23:45:14.342789 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_20/mlp/wi_0/kernel/v_col
I0512 23:45:14.342822 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_20/mlp/wi_0/kernel/v_row
I0512 23:45:14.342855 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_20/mlp/wi_1/kernel/m
I0512 23:45:14.342889 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_20/mlp/wi_1/kernel/v
I0512 23:45:14.342922 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_20/mlp/wi_1/kernel/v_col
I0512 23:45:14.342955 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_20/mlp/wi_1/kernel/v_row
I0512 23:45:14.342988 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_20/mlp/wo/kernel/m
I0512 23:45:14.343022 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_20/mlp/wo/kernel/v
I0512 23:45:14.343055 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_20/mlp/wo/kernel/v_col
I0512 23:45:14.343088 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_20/mlp/wo/kernel/v_row
I0512 23:45:14.343122 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_20/pre_mlp_layer_norm/scale/m
I0512 23:45:14.343155 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_20/pre_mlp_layer_norm/scale/v
I0512 23:45:14.343189 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_20/pre_mlp_layer_norm/scale/v_col
I0512 23:45:14.343222 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_20/pre_mlp_layer_norm/scale/v_row
I0512 23:45:14.343255 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_20/pre_self_attention_layer_norm/scale/m
I0512 23:45:14.343398 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_20/pre_self_attention_layer_norm/scale/v
I0512 23:45:14.343433 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_20/pre_self_attention_layer_norm/scale/v_col
I0512 23:45:14.343467 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_20/pre_self_attention_layer_norm/scale/v_row
I0512 23:45:14.343500 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_20/self_attention/key/kernel/m
I0512 23:45:14.343535 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_20/self_attention/key/kernel/v
I0512 23:45:14.343569 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_20/self_attention/key/kernel/v_col
I0512 23:45:14.343602 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_20/self_attention/key/kernel/v_row
I0512 23:45:14.343636 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_20/self_attention/out/kernel/m
I0512 23:45:14.343668 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_20/self_attention/out/kernel/v
I0512 23:45:14.343702 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_20/self_attention/out/kernel/v_col
I0512 23:45:14.343734 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_20/self_attention/out/kernel/v_row
I0512 23:45:14.343768 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_20/self_attention/query/kernel/m
I0512 23:45:14.343801 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_20/self_attention/query/kernel/v
I0512 23:45:14.343834 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_20/self_attention/query/kernel/v_col
I0512 23:45:14.343868 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_20/self_attention/query/kernel/v_row
I0512 23:45:14.343901 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_20/self_attention/value/kernel/m
I0512 23:45:14.343934 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_20/self_attention/value/kernel/v
I0512 23:45:14.343967 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_20/self_attention/value/kernel/v_col
I0512 23:45:14.344000 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_20/self_attention/value/kernel/v_row
I0512 23:45:14.344033 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_21/mlp/wi_0/kernel/m
I0512 23:45:14.344066 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_21/mlp/wi_0/kernel/v
I0512 23:45:14.344099 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_21/mlp/wi_0/kernel/v_col
I0512 23:45:14.344132 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_21/mlp/wi_0/kernel/v_row
I0512 23:45:14.344166 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_21/mlp/wi_1/kernel/m
I0512 23:45:14.344200 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_21/mlp/wi_1/kernel/v
I0512 23:45:14.344233 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_21/mlp/wi_1/kernel/v_col
I0512 23:45:14.344266 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_21/mlp/wi_1/kernel/v_row
I0512 23:45:14.344306 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_21/mlp/wo/kernel/m
I0512 23:45:14.344340 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_21/mlp/wo/kernel/v
I0512 23:45:14.344373 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_21/mlp/wo/kernel/v_col
I0512 23:45:14.344407 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_21/mlp/wo/kernel/v_row
I0512 23:45:14.344440 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_21/pre_mlp_layer_norm/scale/m
I0512 23:45:14.344474 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_21/pre_mlp_layer_norm/scale/v
I0512 23:45:14.344507 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_21/pre_mlp_layer_norm/scale/v_col
I0512 23:45:14.344559 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_21/pre_mlp_layer_norm/scale/v_row
I0512 23:45:14.344597 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_21/pre_self_attention_layer_norm/scale/m
I0512 23:45:14.344631 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_21/pre_self_attention_layer_norm/scale/v
I0512 23:45:14.344665 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_21/pre_self_attention_layer_norm/scale/v_col
I0512 23:45:14.344698 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_21/pre_self_attention_layer_norm/scale/v_row
I0512 23:45:14.344732 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_21/self_attention/key/kernel/m
I0512 23:45:14.344765 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_21/self_attention/key/kernel/v
I0512 23:45:14.344799 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_21/self_attention/key/kernel/v_col
I0512 23:45:14.344832 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_21/self_attention/key/kernel/v_row
I0512 23:45:14.344865 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_21/self_attention/out/kernel/m
I0512 23:45:14.344897 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_21/self_attention/out/kernel/v
I0512 23:45:14.344931 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_21/self_attention/out/kernel/v_col
I0512 23:45:14.344964 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_21/self_attention/out/kernel/v_row
I0512 23:45:14.344997 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_21/self_attention/query/kernel/m
I0512 23:45:14.345030 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_21/self_attention/query/kernel/v
I0512 23:45:14.345063 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_21/self_attention/query/kernel/v_col
I0512 23:45:14.345095 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_21/self_attention/query/kernel/v_row
I0512 23:45:14.345129 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_21/self_attention/value/kernel/m
I0512 23:45:14.345161 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_21/self_attention/value/kernel/v
I0512 23:45:14.345194 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_21/self_attention/value/kernel/v_col
I0512 23:45:14.345227 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_21/self_attention/value/kernel/v_row
I0512 23:45:14.345260 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_22/mlp/wi_0/kernel/m
I0512 23:45:14.345300 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_22/mlp/wi_0/kernel/v
I0512 23:45:14.345335 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_22/mlp/wi_0/kernel/v_col
I0512 23:45:14.345368 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_22/mlp/wi_0/kernel/v_row
I0512 23:45:14.345401 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_22/mlp/wi_1/kernel/m
I0512 23:45:14.345434 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_22/mlp/wi_1/kernel/v
I0512 23:45:14.345467 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_22/mlp/wi_1/kernel/v_col
I0512 23:45:14.345501 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_22/mlp/wi_1/kernel/v_row
I0512 23:45:14.345535 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_22/mlp/wo/kernel/m
I0512 23:45:14.345570 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_22/mlp/wo/kernel/v
I0512 23:45:14.345603 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_22/mlp/wo/kernel/v_col
I0512 23:45:14.345637 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_22/mlp/wo/kernel/v_row
I0512 23:45:14.345670 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_22/pre_mlp_layer_norm/scale/m
I0512 23:45:14.345703 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_22/pre_mlp_layer_norm/scale/v
I0512 23:45:14.345736 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_22/pre_mlp_layer_norm/scale/v_col
I0512 23:45:14.345769 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_22/pre_mlp_layer_norm/scale/v_row
I0512 23:45:14.345803 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_22/pre_self_attention_layer_norm/scale/m
I0512 23:45:14.345836 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_22/pre_self_attention_layer_norm/scale/v
I0512 23:45:14.345870 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_22/pre_self_attention_layer_norm/scale/v_col
I0512 23:45:14.345903 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_22/pre_self_attention_layer_norm/scale/v_row
I0512 23:45:14.345936 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_22/self_attention/key/kernel/m
I0512 23:45:14.345969 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_22/self_attention/key/kernel/v
I0512 23:45:14.346002 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_22/self_attention/key/kernel/v_col
I0512 23:45:14.346035 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_22/self_attention/key/kernel/v_row
I0512 23:45:14.346068 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_22/self_attention/out/kernel/m
I0512 23:45:14.346101 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_22/self_attention/out/kernel/v
I0512 23:45:14.346135 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_22/self_attention/out/kernel/v_col
I0512 23:45:14.346168 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_22/self_attention/out/kernel/v_row
I0512 23:45:14.346201 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_22/self_attention/query/kernel/m
I0512 23:45:14.346234 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_22/self_attention/query/kernel/v
I0512 23:45:14.346267 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_22/self_attention/query/kernel/v_col
I0512 23:45:14.346306 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_22/self_attention/query/kernel/v_row
I0512 23:45:14.346340 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_22/self_attention/value/kernel/m
I0512 23:45:14.346374 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_22/self_attention/value/kernel/v
I0512 23:45:14.346407 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_22/self_attention/value/kernel/v_col
I0512 23:45:14.346439 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_22/self_attention/value/kernel/v_row
I0512 23:45:14.346473 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/extra_mlp/wi_0/kernel/m
I0512 23:45:14.346505 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/extra_mlp/wi_0/kernel/v
I0512 23:45:14.346539 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/extra_mlp/wi_0/kernel/v_col
I0512 23:45:14.346574 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/extra_mlp/wi_0/kernel/v_row
I0512 23:45:14.346607 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/extra_mlp/wi_1/kernel/m
I0512 23:45:14.346640 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/extra_mlp/wi_1/kernel/v
I0512 23:45:14.346673 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/extra_mlp/wi_1/kernel/v_col
I0512 23:45:14.346706 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/extra_mlp/wi_1/kernel/v_row
I0512 23:45:14.346739 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/extra_mlp/wo/kernel/m
I0512 23:45:14.346772 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/extra_mlp/wo/kernel/v
I0512 23:45:14.346805 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/extra_mlp/wo/kernel/v_col
I0512 23:45:14.346837 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/extra_mlp/wo/kernel/v_row
I0512 23:45:14.346870 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/mlp/expert/wi_0/kernel/m
I0512 23:45:14.346903 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/mlp/expert/wi_0/kernel/v
I0512 23:45:14.346936 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/mlp/expert/wi_0/kernel/v_col
I0512 23:45:14.346969 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/mlp/expert/wi_0/kernel/v_row
I0512 23:45:14.347002 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/mlp/expert/wi_1/kernel/m
I0512 23:45:14.347035 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/mlp/expert/wi_1/kernel/v
I0512 23:45:14.347068 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/mlp/expert/wi_1/kernel/v_col
I0512 23:45:14.347101 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/mlp/expert/wi_1/kernel/v_row
I0512 23:45:14.347133 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/mlp/expert/wo/kernel/m
I0512 23:45:14.347166 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/mlp/expert/wo/kernel/v
I0512 23:45:14.347199 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/mlp/expert/wo/kernel/v_col
I0512 23:45:14.347232 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/mlp/expert/wo/kernel/v_row
I0512 23:45:14.347265 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/mlp/router/router_weights/w/kernel/m
I0512 23:45:14.347304 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/mlp/router/router_weights/w/kernel/v
I0512 23:45:14.347338 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/mlp/router/router_weights/w/kernel/v_col
I0512 23:45:14.347372 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/mlp/router/router_weights/w/kernel/v_row
I0512 23:45:14.347404 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/pre_extra_mlp_layer_norm/scale/m
I0512 23:45:14.347438 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/pre_extra_mlp_layer_norm/scale/v
I0512 23:45:14.347470 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/pre_extra_mlp_layer_norm/scale/v_col
I0512 23:45:14.347503 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/pre_extra_mlp_layer_norm/scale/v_row
I0512 23:45:14.347538 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/pre_mlp_layer_norm/scale/m
I0512 23:45:14.347573 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/pre_mlp_layer_norm/scale/v
I0512 23:45:14.347606 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/pre_mlp_layer_norm/scale/v_col
I0512 23:45:14.347639 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/pre_mlp_layer_norm/scale/v_row
I0512 23:45:14.347672 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/pre_self_attention_layer_norm/scale/m
I0512 23:45:14.347706 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/pre_self_attention_layer_norm/scale/v
I0512 23:45:14.347739 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/pre_self_attention_layer_norm/scale/v_col
I0512 23:45:14.347772 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/pre_self_attention_layer_norm/scale/v_row
I0512 23:45:14.347805 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/self_attention/key/kernel/m
I0512 23:45:14.347839 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/self_attention/key/kernel/v
I0512 23:45:14.347872 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/self_attention/key/kernel/v_col
I0512 23:45:14.347905 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/self_attention/key/kernel/v_row
I0512 23:45:14.347938 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/self_attention/out/kernel/m
I0512 23:45:14.347971 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/self_attention/out/kernel/v
I0512 23:45:14.348005 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/self_attention/out/kernel/v_col
I0512 23:45:14.348038 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/self_attention/out/kernel/v_row
I0512 23:45:14.348072 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/self_attention/query/kernel/m
I0512 23:45:14.348105 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/self_attention/query/kernel/v
I0512 23:45:14.348138 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/self_attention/query/kernel/v_col
I0512 23:45:14.348171 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/self_attention/query/kernel/v_row
I0512 23:45:14.348204 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/self_attention/value/kernel/m
I0512 23:45:14.348237 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/self_attention/value/kernel/v
I0512 23:45:14.348270 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/self_attention/value/kernel/v_col
I0512 23:45:14.348309 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/self_attention/value/kernel/v_row
I0512 23:45:14.348343 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_3/mlp/wi_0/kernel/m
I0512 23:45:14.348376 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_3/mlp/wi_0/kernel/v
I0512 23:45:14.348409 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_3/mlp/wi_0/kernel/v_col
I0512 23:45:14.348442 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_3/mlp/wi_0/kernel/v_row
I0512 23:45:14.348476 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_3/mlp/wi_1/kernel/m
I0512 23:45:14.348509 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_3/mlp/wi_1/kernel/v
I0512 23:45:14.348562 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_3/mlp/wi_1/kernel/v_col
I0512 23:45:14.348598 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_3/mlp/wi_1/kernel/v_row
I0512 23:45:14.348632 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_3/mlp/wo/kernel/m
I0512 23:45:14.348665 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_3/mlp/wo/kernel/v
I0512 23:45:14.348698 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_3/mlp/wo/kernel/v_col
I0512 23:45:14.348731 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_3/mlp/wo/kernel/v_row
I0512 23:45:14.348764 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_3/pre_mlp_layer_norm/scale/m
I0512 23:45:14.348798 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_3/pre_mlp_layer_norm/scale/v
I0512 23:45:14.348831 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_3/pre_mlp_layer_norm/scale/v_col
I0512 23:45:14.348864 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_3/pre_mlp_layer_norm/scale/v_row
I0512 23:45:14.348897 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_3/pre_self_attention_layer_norm/scale/m
I0512 23:45:14.348930 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_3/pre_self_attention_layer_norm/scale/v
I0512 23:45:14.348963 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_3/pre_self_attention_layer_norm/scale/v_col
I0512 23:45:14.348996 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_3/pre_self_attention_layer_norm/scale/v_row
I0512 23:45:14.349029 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_3/self_attention/key/kernel/m
I0512 23:45:14.349062 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_3/self_attention/key/kernel/v
I0512 23:45:14.349096 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_3/self_attention/key/kernel/v_col
I0512 23:45:14.349129 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_3/self_attention/key/kernel/v_row
I0512 23:45:14.349162 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_3/self_attention/out/kernel/m
I0512 23:45:14.349195 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_3/self_attention/out/kernel/v
I0512 23:45:14.349228 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_3/self_attention/out/kernel/v_col
I0512 23:45:14.349261 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_3/self_attention/out/kernel/v_row
I0512 23:45:14.349300 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_3/self_attention/query/kernel/m
I0512 23:45:14.349334 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_3/self_attention/query/kernel/v
I0512 23:45:14.349367 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_3/self_attention/query/kernel/v_col
I0512 23:45:14.349400 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_3/self_attention/query/kernel/v_row
I0512 23:45:14.349433 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_3/self_attention/value/kernel/m
I0512 23:45:14.349466 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_3/self_attention/value/kernel/v
I0512 23:45:14.349499 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_3/self_attention/value/kernel/v_col
I0512 23:45:14.349533 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_3/self_attention/value/kernel/v_row
I0512 23:45:14.349567 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_4/mlp/wi_0/kernel/m
I0512 23:45:14.349601 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_4/mlp/wi_0/kernel/v
I0512 23:45:14.349633 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_4/mlp/wi_0/kernel/v_col
I0512 23:45:14.349667 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_4/mlp/wi_0/kernel/v_row
I0512 23:45:14.349700 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_4/mlp/wi_1/kernel/m
I0512 23:45:14.349733 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_4/mlp/wi_1/kernel/v
I0512 23:45:14.349766 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_4/mlp/wi_1/kernel/v_col
I0512 23:45:14.349799 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_4/mlp/wi_1/kernel/v_row
I0512 23:45:14.349833 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_4/mlp/wo/kernel/m
I0512 23:45:14.349866 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_4/mlp/wo/kernel/v
I0512 23:45:14.349899 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_4/mlp/wo/kernel/v_col
I0512 23:45:14.349932 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_4/mlp/wo/kernel/v_row
I0512 23:45:14.349966 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_4/pre_mlp_layer_norm/scale/m
I0512 23:45:14.349998 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_4/pre_mlp_layer_norm/scale/v
I0512 23:45:14.350032 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_4/pre_mlp_layer_norm/scale/v_col
I0512 23:45:14.350065 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_4/pre_mlp_layer_norm/scale/v_row
I0512 23:45:14.350098 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_4/pre_self_attention_layer_norm/scale/m
I0512 23:45:14.350132 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_4/pre_self_attention_layer_norm/scale/v
I0512 23:45:14.350165 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_4/pre_self_attention_layer_norm/scale/v_col
I0512 23:45:14.350198 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_4/pre_self_attention_layer_norm/scale/v_row
I0512 23:45:14.350230 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_4/self_attention/key/kernel/m
I0512 23:45:14.350263 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_4/self_attention/key/kernel/v
I0512 23:45:14.350303 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_4/self_attention/key/kernel/v_col
I0512 23:45:14.350337 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_4/self_attention/key/kernel/v_row
I0512 23:45:14.350370 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_4/self_attention/out/kernel/m
I0512 23:45:14.350403 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_4/self_attention/out/kernel/v
I0512 23:45:14.350436 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_4/self_attention/out/kernel/v_col
I0512 23:45:14.350469 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_4/self_attention/out/kernel/v_row
I0512 23:45:14.350502 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_4/self_attention/query/kernel/m
I0512 23:45:14.350535 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_4/self_attention/query/kernel/v
I0512 23:45:14.350569 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_4/self_attention/query/kernel/v_col
I0512 23:45:14.350603 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_4/self_attention/query/kernel/v_row
I0512 23:45:14.350636 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_4/self_attention/value/kernel/m
I0512 23:45:14.350669 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_4/self_attention/value/kernel/v
I0512 23:45:14.350702 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_4/self_attention/value/kernel/v_col
I0512 23:45:14.350735 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_4/self_attention/value/kernel/v_row
I0512 23:45:14.350768 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/extra_mlp/wi_0/kernel/m
I0512 23:45:14.350801 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/extra_mlp/wi_0/kernel/v
I0512 23:45:14.350834 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/extra_mlp/wi_0/kernel/v_col
I0512 23:45:14.350867 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/extra_mlp/wi_0/kernel/v_row
I0512 23:45:14.350899 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/extra_mlp/wi_1/kernel/m
I0512 23:45:14.350933 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/extra_mlp/wi_1/kernel/v
I0512 23:45:14.350965 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/extra_mlp/wi_1/kernel/v_col
I0512 23:45:14.350998 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/extra_mlp/wi_1/kernel/v_row
I0512 23:45:14.351032 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/extra_mlp/wo/kernel/m
I0512 23:45:14.351065 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/extra_mlp/wo/kernel/v
I0512 23:45:14.351099 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/extra_mlp/wo/kernel/v_col
I0512 23:45:14.351133 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/extra_mlp/wo/kernel/v_row
I0512 23:45:14.351166 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/mlp/expert/wi_0/kernel/m
I0512 23:45:14.351199 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/mlp/expert/wi_0/kernel/v
I0512 23:45:14.351233 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/mlp/expert/wi_0/kernel/v_col
I0512 23:45:14.351266 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/mlp/expert/wi_0/kernel/v_row
I0512 23:45:14.351305 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/mlp/expert/wi_1/kernel/m
I0512 23:45:14.351338 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/mlp/expert/wi_1/kernel/v
I0512 23:45:14.351372 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/mlp/expert/wi_1/kernel/v_col
I0512 23:45:14.351405 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/mlp/expert/wi_1/kernel/v_row
I0512 23:45:14.351438 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/mlp/expert/wo/kernel/m
I0512 23:45:14.351471 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/mlp/expert/wo/kernel/v
I0512 23:45:14.351504 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/mlp/expert/wo/kernel/v_col
I0512 23:45:14.351538 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/mlp/expert/wo/kernel/v_row
I0512 23:45:14.351573 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/mlp/router/router_weights/w/kernel/m
I0512 23:45:14.351606 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/mlp/router/router_weights/w/kernel/v
I0512 23:45:14.351639 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/mlp/router/router_weights/w/kernel/v_col
I0512 23:45:14.351672 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/mlp/router/router_weights/w/kernel/v_row
I0512 23:45:14.351706 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/pre_extra_mlp_layer_norm/scale/m
I0512 23:45:14.351739 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/pre_extra_mlp_layer_norm/scale/v
I0512 23:45:14.351772 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/pre_extra_mlp_layer_norm/scale/v_col
I0512 23:45:14.351804 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/pre_extra_mlp_layer_norm/scale/v_row
I0512 23:45:14.351837 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/pre_mlp_layer_norm/scale/m
I0512 23:45:14.351871 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/pre_mlp_layer_norm/scale/v
I0512 23:45:14.351904 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/pre_mlp_layer_norm/scale/v_col
I0512 23:45:14.351937 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/pre_mlp_layer_norm/scale/v_row
I0512 23:45:14.351971 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/pre_self_attention_layer_norm/scale/m
I0512 23:45:14.352004 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/pre_self_attention_layer_norm/scale/v
I0512 23:45:14.352037 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/pre_self_attention_layer_norm/scale/v_col
I0512 23:45:14.352070 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/pre_self_attention_layer_norm/scale/v_row
I0512 23:45:14.352103 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/self_attention/key/kernel/m
I0512 23:45:14.352136 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/self_attention/key/kernel/v
I0512 23:45:14.352169 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/self_attention/key/kernel/v_col
I0512 23:45:14.352202 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/self_attention/key/kernel/v_row
I0512 23:45:14.352236 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/self_attention/out/kernel/m
I0512 23:45:14.352269 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/self_attention/out/kernel/v
I0512 23:45:14.352309 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/self_attention/out/kernel/v_col
I0512 23:45:14.352342 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/self_attention/out/kernel/v_row
I0512 23:45:14.352375 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/self_attention/query/kernel/m
I0512 23:45:14.352408 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/self_attention/query/kernel/v
I0512 23:45:14.352441 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/self_attention/query/kernel/v_col
I0512 23:45:14.352474 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/self_attention/query/kernel/v_row
I0512 23:45:14.352508 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/self_attention/value/kernel/m
I0512 23:45:14.352560 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/self_attention/value/kernel/v
I0512 23:45:14.352597 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/self_attention/value/kernel/v_col
I0512 23:45:14.352631 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/self_attention/value/kernel/v_row
I0512 23:45:14.352664 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_6/mlp/wi_0/kernel/m
I0512 23:45:14.352697 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_6/mlp/wi_0/kernel/v
I0512 23:45:14.352730 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_6/mlp/wi_0/kernel/v_col
I0512 23:45:14.352763 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_6/mlp/wi_0/kernel/v_row
I0512 23:45:14.352797 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_6/mlp/wi_1/kernel/m
I0512 23:45:14.352830 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_6/mlp/wi_1/kernel/v
I0512 23:45:14.352864 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_6/mlp/wi_1/kernel/v_col
I0512 23:45:14.352897 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_6/mlp/wi_1/kernel/v_row
I0512 23:45:14.352930 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_6/mlp/wo/kernel/m
I0512 23:45:14.352963 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_6/mlp/wo/kernel/v
I0512 23:45:14.352996 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_6/mlp/wo/kernel/v_col
I0512 23:45:14.353029 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_6/mlp/wo/kernel/v_row
I0512 23:45:14.353063 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_6/pre_mlp_layer_norm/scale/m
I0512 23:45:14.353096 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_6/pre_mlp_layer_norm/scale/v
I0512 23:45:14.353129 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_6/pre_mlp_layer_norm/scale/v_col
I0512 23:45:14.353162 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_6/pre_mlp_layer_norm/scale/v_row
I0512 23:45:14.353196 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_6/pre_self_attention_layer_norm/scale/m
I0512 23:45:14.353229 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_6/pre_self_attention_layer_norm/scale/v
I0512 23:45:14.353262 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_6/pre_self_attention_layer_norm/scale/v_col
I0512 23:45:14.353302 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_6/pre_self_attention_layer_norm/scale/v_row
I0512 23:45:14.353336 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_6/self_attention/key/kernel/m
I0512 23:45:14.353370 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_6/self_attention/key/kernel/v
I0512 23:45:14.353403 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_6/self_attention/key/kernel/v_col
I0512 23:45:14.353436 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_6/self_attention/key/kernel/v_row
I0512 23:45:14.353469 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_6/self_attention/out/kernel/m
I0512 23:45:14.353502 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_6/self_attention/out/kernel/v
I0512 23:45:14.353536 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_6/self_attention/out/kernel/v_col
I0512 23:45:14.353570 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_6/self_attention/out/kernel/v_row
I0512 23:45:14.353603 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_6/self_attention/query/kernel/m
I0512 23:45:14.353636 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_6/self_attention/query/kernel/v
I0512 23:45:14.353669 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_6/self_attention/query/kernel/v_col
I0512 23:45:14.353702 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_6/self_attention/query/kernel/v_row
I0512 23:45:14.353735 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_6/self_attention/value/kernel/m
I0512 23:45:14.353768 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_6/self_attention/value/kernel/v
I0512 23:45:14.353801 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_6/self_attention/value/kernel/v_col
I0512 23:45:14.353834 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_6/self_attention/value/kernel/v_row
I0512 23:45:14.353867 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_7/mlp/wi_0/kernel/m
I0512 23:45:14.353900 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_7/mlp/wi_0/kernel/v
I0512 23:45:14.353934 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_7/mlp/wi_0/kernel/v_col
I0512 23:45:14.353966 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_7/mlp/wi_0/kernel/v_row
I0512 23:45:14.354000 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_7/mlp/wi_1/kernel/m
I0512 23:45:14.354033 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_7/mlp/wi_1/kernel/v
I0512 23:45:14.354066 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_7/mlp/wi_1/kernel/v_col
I0512 23:45:14.354099 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_7/mlp/wi_1/kernel/v_row
I0512 23:45:14.354132 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_7/mlp/wo/kernel/m
I0512 23:45:14.354166 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_7/mlp/wo/kernel/v
I0512 23:45:14.354219 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_7/mlp/wo/kernel/v_col
I0512 23:45:14.354254 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_7/mlp/wo/kernel/v_row
I0512 23:45:14.354293 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_7/pre_mlp_layer_norm/scale/m
I0512 23:45:14.354327 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_7/pre_mlp_layer_norm/scale/v
I0512 23:45:14.354361 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_7/pre_mlp_layer_norm/scale/v_col
I0512 23:45:14.354394 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_7/pre_mlp_layer_norm/scale/v_row
I0512 23:45:14.354428 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_7/pre_self_attention_layer_norm/scale/m
I0512 23:45:14.354461 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_7/pre_self_attention_layer_norm/scale/v
I0512 23:45:14.354494 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_7/pre_self_attention_layer_norm/scale/v_col
I0512 23:45:14.354527 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_7/pre_self_attention_layer_norm/scale/v_row
I0512 23:45:14.354563 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_7/self_attention/key/kernel/m
I0512 23:45:14.354596 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_7/self_attention/key/kernel/v
I0512 23:45:14.354629 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_7/self_attention/key/kernel/v_col
I0512 23:45:14.354662 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_7/self_attention/key/kernel/v_row
I0512 23:45:14.354695 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_7/self_attention/out/kernel/m
I0512 23:45:14.354728 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_7/self_attention/out/kernel/v
I0512 23:45:14.354761 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_7/self_attention/out/kernel/v_col
I0512 23:45:14.354794 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_7/self_attention/out/kernel/v_row
I0512 23:45:14.354827 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_7/self_attention/query/kernel/m
I0512 23:45:14.354860 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_7/self_attention/query/kernel/v
I0512 23:45:14.354893 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_7/self_attention/query/kernel/v_col
I0512 23:45:14.354926 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_7/self_attention/query/kernel/v_row
I0512 23:45:14.354959 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_7/self_attention/value/kernel/m
I0512 23:45:14.354992 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_7/self_attention/value/kernel/v
I0512 23:45:14.355025 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_7/self_attention/value/kernel/v_col
I0512 23:45:14.355057 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_7/self_attention/value/kernel/v_row
I0512 23:45:14.355090 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_8/mlp/wi_0/kernel/m
I0512 23:45:14.355123 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_8/mlp/wi_0/kernel/v
I0512 23:45:14.355156 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_8/mlp/wi_0/kernel/v_col
I0512 23:45:14.355189 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_8/mlp/wi_0/kernel/v_row
I0512 23:45:14.355222 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_8/mlp/wi_1/kernel/m
I0512 23:45:14.355256 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_8/mlp/wi_1/kernel/v
I0512 23:45:14.355294 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_8/mlp/wi_1/kernel/v_col
I0512 23:45:14.355328 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_8/mlp/wi_1/kernel/v_row
I0512 23:45:14.355361 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_8/mlp/wo/kernel/m
I0512 23:45:14.355395 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_8/mlp/wo/kernel/v
I0512 23:45:14.355428 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_8/mlp/wo/kernel/v_col
I0512 23:45:14.355461 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_8/mlp/wo/kernel/v_row
I0512 23:45:14.355494 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_8/pre_mlp_layer_norm/scale/m
I0512 23:45:14.355527 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_8/pre_mlp_layer_norm/scale/v
I0512 23:45:14.355562 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_8/pre_mlp_layer_norm/scale/v_col
I0512 23:45:14.355596 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_8/pre_mlp_layer_norm/scale/v_row
I0512 23:45:14.355629 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_8/pre_self_attention_layer_norm/scale/m
I0512 23:45:14.355662 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_8/pre_self_attention_layer_norm/scale/v
I0512 23:45:14.355695 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_8/pre_self_attention_layer_norm/scale/v_col
I0512 23:45:14.355728 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_8/pre_self_attention_layer_norm/scale/v_row
I0512 23:45:14.355761 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_8/self_attention/key/kernel/m
I0512 23:45:14.355794 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_8/self_attention/key/kernel/v
I0512 23:45:14.355827 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_8/self_attention/key/kernel/v_col
I0512 23:45:14.355862 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_8/self_attention/key/kernel/v_row
I0512 23:45:14.355897 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_8/self_attention/out/kernel/m
I0512 23:45:14.355930 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_8/self_attention/out/kernel/v
I0512 23:45:14.355963 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_8/self_attention/out/kernel/v_col
I0512 23:45:14.355997 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_8/self_attention/out/kernel/v_row
I0512 23:45:14.356030 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_8/self_attention/query/kernel/m
I0512 23:45:14.356063 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_8/self_attention/query/kernel/v
I0512 23:45:14.356096 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_8/self_attention/query/kernel/v_col
I0512 23:45:14.356129 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_8/self_attention/query/kernel/v_row
I0512 23:45:14.356162 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_8/self_attention/value/kernel/m
I0512 23:45:14.356195 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_8/self_attention/value/kernel/v
I0512 23:45:14.356228 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_8/self_attention/value/kernel/v_col
I0512 23:45:14.356261 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_8/self_attention/value/kernel/v_row
I0512 23:45:14.356300 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_9/mlp/wi_0/kernel/m
I0512 23:45:14.356333 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_9/mlp/wi_0/kernel/v
I0512 23:45:14.356367 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_9/mlp/wi_0/kernel/v_col
I0512 23:45:14.356400 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_9/mlp/wi_0/kernel/v_row
I0512 23:45:14.356434 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_9/mlp/wi_1/kernel/m
I0512 23:45:14.356467 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_9/mlp/wi_1/kernel/v
I0512 23:45:14.356500 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_9/mlp/wi_1/kernel/v_col
I0512 23:45:14.356556 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_9/mlp/wi_1/kernel/v_row
I0512 23:45:14.356595 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_9/mlp/wo/kernel/m
I0512 23:45:14.356629 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_9/mlp/wo/kernel/v
I0512 23:45:14.356663 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_9/mlp/wo/kernel/v_col
I0512 23:45:14.356696 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_9/mlp/wo/kernel/v_row
I0512 23:45:14.356729 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_9/pre_mlp_layer_norm/scale/m
I0512 23:45:14.356762 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_9/pre_mlp_layer_norm/scale/v
I0512 23:45:14.356795 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_9/pre_mlp_layer_norm/scale/v_col
I0512 23:45:14.356829 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_9/pre_mlp_layer_norm/scale/v_row
I0512 23:45:14.356862 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_9/pre_self_attention_layer_norm/scale/m
I0512 23:45:14.356895 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_9/pre_self_attention_layer_norm/scale/v
I0512 23:45:14.356928 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_9/pre_self_attention_layer_norm/scale/v_col
I0512 23:45:14.356961 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_9/pre_self_attention_layer_norm/scale/v_row
I0512 23:45:14.356994 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_9/self_attention/key/kernel/m
I0512 23:45:14.357028 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_9/self_attention/key/kernel/v
I0512 23:45:14.357061 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_9/self_attention/key/kernel/v_col
I0512 23:45:14.357094 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_9/self_attention/key/kernel/v_row
I0512 23:45:14.357127 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_9/self_attention/out/kernel/m
I0512 23:45:14.357160 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_9/self_attention/out/kernel/v
I0512 23:45:14.357193 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_9/self_attention/out/kernel/v_col
I0512 23:45:14.357226 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_9/self_attention/out/kernel/v_row
I0512 23:45:14.357259 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_9/self_attention/query/kernel/m
I0512 23:45:14.357297 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_9/self_attention/query/kernel/v
I0512 23:45:14.357331 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_9/self_attention/query/kernel/v_col
I0512 23:45:14.357364 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_9/self_attention/query/kernel/v_row
I0512 23:45:14.357397 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_9/self_attention/value/kernel/m
I0512 23:45:14.357430 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_9/self_attention/value/kernel/v
I0512 23:45:14.357464 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_9/self_attention/value/kernel/v_col
I0512 23:45:14.357497 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_9/self_attention/value/kernel/v_row
I0512 23:45:14.357530 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/logits_dense/kernel/m
I0512 23:45:14.357566 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/logits_dense/kernel/v
I0512 23:45:14.357599 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/logits_dense/kernel/v_col
I0512 23:45:14.357633 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/logits_dense/kernel/v_row
I0512 23:45:14.357666 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/token_embedder/embedding/m
I0512 23:45:14.357700 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/token_embedder/embedding/v
I0512 23:45:14.357734 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/token_embedder/embedding/v_col
I0512 23:45:14.357767 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/param_states/token_embedder/embedding/v_row
I0512 23:45:14.357801 140143678494720 checkpoints.py:1100] Restoring key from ckpt: state/step
I0512 23:45:14.357835 140143678494720 checkpoints.py:1100] Restoring key from ckpt: target/decoder/decoder_norm/scale
I0512 23:45:14.357868 140143678494720 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_0/mlp/wi_0/kernel
I0512 23:45:14.357901 140143678494720 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_0/mlp/wi_1/kernel
I0512 23:45:14.357934 140143678494720 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_0/mlp/wo/kernel
I0512 23:45:14.357968 140143678494720 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_0/pre_mlp_layer_norm/scale
I0512 23:45:14.358001 140143678494720 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_0/pre_self_attention_layer_norm/scale
I0512 23:45:14.358034 140143678494720 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_0/self_attention/key/kernel
I0512 23:45:14.358068 140143678494720 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_0/self_attention/out/kernel
I0512 23:45:14.358101 140143678494720 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_0/self_attention/query/kernel
I0512 23:45:14.358134 140143678494720 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_0/self_attention/value/kernel
I0512 23:45:14.358167 140143678494720 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_1/mlp/wi_0/kernel
I0512 23:45:14.358200 140143678494720 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_1/mlp/wi_1/kernel
I0512 23:45:14.358233 140143678494720 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_1/mlp/wo/kernel
I0512 23:45:14.358266 140143678494720 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_1/pre_mlp_layer_norm/scale
I0512 23:45:14.358306 140143678494720 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_1/pre_self_attention_layer_norm/scale
I0512 23:45:14.358340 140143678494720 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_1/self_attention/key/kernel
I0512 23:45:14.358373 140143678494720 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_1/self_attention/out/kernel
I0512 23:45:14.358406 140143678494720 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_1/self_attention/query/kernel
I0512 23:45:14.358439 140143678494720 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_1/self_attention/value/kernel
I0512 23:45:14.358473 140143678494720 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_10/mlp/wi_0/kernel
I0512 23:45:14.358506 140143678494720 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_10/mlp/wi_1/kernel
I0512 23:45:14.358540 140143678494720 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_10/mlp/wo/kernel
I0512 23:45:14.358574 140143678494720 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_10/pre_mlp_layer_norm/scale
I0512 23:45:14.358607 140143678494720 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_10/pre_self_attention_layer_norm/scale
I0512 23:45:14.358641 140143678494720 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_10/self_attention/key/kernel
I0512 23:45:14.358674 140143678494720 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_10/self_attention/out/kernel
I0512 23:45:14.358707 140143678494720 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_10/self_attention/query/kernel
I0512 23:45:14.358740 140143678494720 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_10/self_attention/value/kernel
I0512 23:45:14.358773 140143678494720 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_11/extra_mlp/wi_0/kernel
I0512 23:45:14.358807 140143678494720 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_11/extra_mlp/wi_1/kernel
I0512 23:45:14.358839 140143678494720 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_11/extra_mlp/wo/kernel
I0512 23:45:14.358877 140143678494720 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_11/mlp/expert/wi_0/kernel
I0512 23:45:14.358910 140143678494720 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_11/mlp/expert/wi_1/kernel
I0512 23:45:14.358943 140143678494720 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_11/mlp/expert/wo/kernel
I0512 23:45:14.358976 140143678494720 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_11/mlp/router/router_weights/w/kernel
I0512 23:45:14.359009 140143678494720 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_11/pre_extra_mlp_layer_norm/scale
I0512 23:45:14.359043 140143678494720 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_11/pre_mlp_layer_norm/scale
I0512 23:45:14.359076 140143678494720 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_11/pre_self_attention_layer_norm/scale
I0512 23:45:14.359109 140143678494720 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_11/self_attention/key/kernel
I0512 23:45:14.359143 140143678494720 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_11/self_attention/out/kernel
I0512 23:45:14.359176 140143678494720 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_11/self_attention/query/kernel
I0512 23:45:14.359209 140143678494720 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_11/self_attention/value/kernel
I0512 23:45:14.359242 140143678494720 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_12/mlp/wi_0/kernel
I0512 23:45:14.359276 140143678494720 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_12/mlp/wi_1/kernel
I0512 23:45:14.359315 140143678494720 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_12/mlp/wo/kernel
I0512 23:45:14.359349 140143678494720 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_12/pre_mlp_layer_norm/scale
I0512 23:45:14.359382 140143678494720 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_12/pre_self_attention_layer_norm/scale
I0512 23:45:14.359416 140143678494720 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_12/self_attention/key/kernel
I0512 23:45:14.359449 140143678494720 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_12/self_attention/out/kernel
I0512 23:45:14.359482 140143678494720 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_12/self_attention/query/kernel
I0512 23:45:14.359515 140143678494720 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_12/self_attention/value/kernel
I0512 23:45:14.359550 140143678494720 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_13/mlp/wi_0/kernel
I0512 23:45:14.359584 140143678494720 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_13/mlp/wi_1/kernel
I0512 23:45:14.359618 140143678494720 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_13/mlp/wo/kernel
I0512 23:45:14.359651 140143678494720 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_13/pre_mlp_layer_norm/scale
I0512 23:45:14.359684 140143678494720 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_13/pre_self_attention_layer_norm/scale
I0512 23:45:14.359718 140143678494720 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_13/self_attention/key/kernel
I0512 23:45:14.359751 140143678494720 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_13/self_attention/out/kernel
I0512 23:45:14.359784 140143678494720 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_13/self_attention/query/kernel
I0512 23:45:14.359818 140143678494720 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_13/self_attention/value/kernel
I0512 23:45:14.359851 140143678494720 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_14/mlp/wi_0/kernel
I0512 23:45:14.359885 140143678494720 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_14/mlp/wi_1/kernel
I0512 23:45:14.359918 140143678494720 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_14/mlp/wo/kernel
I0512 23:45:14.359952 140143678494720 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_14/pre_mlp_layer_norm/scale
I0512 23:45:14.359985 140143678494720 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_14/pre_self_attention_layer_norm/scale
I0512 23:45:14.360018 140143678494720 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_14/self_attention/key/kernel
I0512 23:45:14.360052 140143678494720 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_14/self_attention/out/kernel
I0512 23:45:14.360085 140143678494720 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_14/self_attention/query/kernel
I0512 23:45:14.360118 140143678494720 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_14/self_attention/value/kernel
I0512 23:45:14.360151 140143678494720 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_15/mlp/wi_0/kernel
I0512 23:45:14.360185 140143678494720 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_15/mlp/wi_1/kernel
I0512 23:45:14.360218 140143678494720 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_15/mlp/wo/kernel
I0512 23:45:14.360251 140143678494720 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_15/pre_mlp_layer_norm/scale
I0512 23:45:14.360289 140143678494720 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_15/pre_self_attention_layer_norm/scale
I0512 23:45:14.360323 140143678494720 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_15/self_attention/key/kernel
I0512 23:45:14.360357 140143678494720 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_15/self_attention/out/kernel
I0512 23:45:14.360390 140143678494720 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_15/self_attention/query/kernel
I0512 23:45:14.360424 140143678494720 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_15/self_attention/value/kernel
I0512 23:45:14.360456 140143678494720 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_16/mlp/wi_0/kernel
I0512 23:45:14.360487 140143678494720 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_16/mlp/wi_1/kernel
I0512 23:45:14.360518 140143678494720 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_16/mlp/wo/kernel
I0512 23:45:14.360569 140143678494720 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_16/pre_mlp_layer_norm/scale
I0512 23:45:14.360603 140143678494720 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_16/pre_self_attention_layer_norm/scale
I0512 23:45:14.360635 140143678494720 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_16/self_attention/key/kernel
I0512 23:45:14.360666 140143678494720 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_16/self_attention/out/kernel
I0512 23:45:14.360697 140143678494720 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_16/self_attention/query/kernel
I0512 23:45:14.360728 140143678494720 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_16/self_attention/value/kernel
I0512 23:45:14.360760 140143678494720 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_17/extra_mlp/wi_0/kernel
I0512 23:45:14.360791 140143678494720 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_17/extra_mlp/wi_1/kernel
I0512 23:45:14.360822 140143678494720 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_17/extra_mlp/wo/kernel
I0512 23:45:14.360853 140143678494720 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_17/mlp/expert/wi_0/kernel
I0512 23:45:14.360885 140143678494720 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_17/mlp/expert/wi_1/kernel
I0512 23:45:14.360916 140143678494720 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_17/mlp/expert/wo/kernel
I0512 23:45:14.360947 140143678494720 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_17/mlp/router/router_weights/w/kernel
I0512 23:45:14.360978 140143678494720 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_17/pre_extra_mlp_layer_norm/scale
I0512 23:45:14.361009 140143678494720 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_17/pre_mlp_layer_norm/scale
I0512 23:45:14.361040 140143678494720 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_17/pre_self_attention_layer_norm/scale
I0512 23:45:14.361071 140143678494720 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_17/self_attention/key/kernel
I0512 23:45:14.361103 140143678494720 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_17/self_attention/out/kernel
I0512 23:45:14.361134 140143678494720 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_17/self_attention/query/kernel
I0512 23:45:14.361165 140143678494720 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_17/self_attention/value/kernel
I0512 23:45:14.361197 140143678494720 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_18/mlp/wi_0/kernel
I0512 23:45:14.361228 140143678494720 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_18/mlp/wi_1/kernel
I0512 23:45:14.361259 140143678494720 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_18/mlp/wo/kernel
I0512 23:45:14.361295 140143678494720 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_18/pre_mlp_layer_norm/scale
I0512 23:45:14.361328 140143678494720 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_18/pre_self_attention_layer_norm/scale
I0512 23:45:14.361359 140143678494720 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_18/self_attention/key/kernel
I0512 23:45:14.361390 140143678494720 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_18/self_attention/out/kernel
I0512 23:45:14.361421 140143678494720 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_18/self_attention/query/kernel
I0512 23:45:14.361453 140143678494720 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_18/self_attention/value/kernel
I0512 23:45:14.361484 140143678494720 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_19/mlp/wi_0/kernel
I0512 23:45:14.361515 140143678494720 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_19/mlp/wi_1/kernel
I0512 23:45:14.361548 140143678494720 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_19/mlp/wo/kernel
I0512 23:45:14.361581 140143678494720 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_19/pre_mlp_layer_norm/scale
I0512 23:45:14.361612 140143678494720 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_19/pre_self_attention_layer_norm/scale
I0512 23:45:14.361643 140143678494720 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_19/self_attention/key/kernel
I0512 23:45:14.361675 140143678494720 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_19/self_attention/out/kernel
I0512 23:45:14.361706 140143678494720 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_19/self_attention/query/kernel
I0512 23:45:14.361737 140143678494720 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_19/self_attention/value/kernel
I0512 23:45:14.361768 140143678494720 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_2/mlp/wi_0/kernel
I0512 23:45:14.361799 140143678494720 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_2/mlp/wi_1/kernel
I0512 23:45:14.361829 140143678494720 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_2/mlp/wo/kernel
I0512 23:45:14.361860 140143678494720 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_2/pre_mlp_layer_norm/scale
I0512 23:45:14.361891 140143678494720 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_2/pre_self_attention_layer_norm/scale
I0512 23:45:14.361922 140143678494720 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_2/self_attention/key/kernel
I0512 23:45:14.361953 140143678494720 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_2/self_attention/out/kernel
I0512 23:45:14.361985 140143678494720 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_2/self_attention/query/kernel
I0512 23:45:14.362016 140143678494720 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_2/self_attention/value/kernel
I0512 23:45:14.362047 140143678494720 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_20/mlp/wi_0/kernel
I0512 23:45:14.362078 140143678494720 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_20/mlp/wi_1/kernel
I0512 23:45:14.362109 140143678494720 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_20/mlp/wo/kernel
I0512 23:45:14.362140 140143678494720 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_20/pre_mlp_layer_norm/scale
I0512 23:45:14.362172 140143678494720 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_20/pre_self_attention_layer_norm/scale
I0512 23:45:14.362203 140143678494720 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_20/self_attention/key/kernel
I0512 23:45:14.362234 140143678494720 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_20/self_attention/out/kernel
I0512 23:45:14.362265 140143678494720 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_20/self_attention/query/kernel
I0512 23:45:14.362303 140143678494720 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_20/self_attention/value/kernel
I0512 23:45:14.362335 140143678494720 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_21/mlp/wi_0/kernel
I0512 23:45:14.362366 140143678494720 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_21/mlp/wi_1/kernel
I0512 23:45:14.362398 140143678494720 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_21/mlp/wo/kernel
I0512 23:45:14.362429 140143678494720 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_21/pre_mlp_layer_norm/scale
I0512 23:45:14.362461 140143678494720 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_21/pre_self_attention_layer_norm/scale
I0512 23:45:14.362492 140143678494720 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_21/self_attention/key/kernel
I0512 23:45:14.362523 140143678494720 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_21/self_attention/out/kernel
I0512 23:45:14.362556 140143678494720 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_21/self_attention/query/kernel
I0512 23:45:14.362589 140143678494720 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_21/self_attention/value/kernel
I0512 23:45:14.362620 140143678494720 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_22/mlp/wi_0/kernel
I0512 23:45:14.362652 140143678494720 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_22/mlp/wi_1/kernel
I0512 23:45:14.362683 140143678494720 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_22/mlp/wo/kernel
I0512 23:45:14.362714 140143678494720 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_22/pre_mlp_layer_norm/scale
I0512 23:45:14.362745 140143678494720 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_22/pre_self_attention_layer_norm/scale
I0512 23:45:14.362777 140143678494720 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_22/self_attention/key/kernel
I0512 23:45:14.362808 140143678494720 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_22/self_attention/out/kernel
I0512 23:45:14.362839 140143678494720 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_22/self_attention/query/kernel
I0512 23:45:14.362870 140143678494720 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_22/self_attention/value/kernel
I0512 23:45:14.362901 140143678494720 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_23/extra_mlp/wi_0/kernel
I0512 23:45:14.362933 140143678494720 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_23/extra_mlp/wi_1/kernel
I0512 23:45:14.362964 140143678494720 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_23/extra_mlp/wo/kernel
I0512 23:45:14.362994 140143678494720 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_23/mlp/expert/wi_0/kernel
I0512 23:45:14.363026 140143678494720 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_23/mlp/expert/wi_1/kernel
I0512 23:45:14.363057 140143678494720 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_23/mlp/expert/wo/kernel
I0512 23:45:14.363089 140143678494720 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_23/mlp/router/router_weights/w/kernel
I0512 23:45:14.363120 140143678494720 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_23/pre_extra_mlp_layer_norm/scale
I0512 23:45:14.363151 140143678494720 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_23/pre_mlp_layer_norm/scale
I0512 23:45:14.363182 140143678494720 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_23/pre_self_attention_layer_norm/scale
I0512 23:45:14.363213 140143678494720 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_23/self_attention/key/kernel
I0512 23:45:14.363244 140143678494720 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_23/self_attention/out/kernel
I0512 23:45:14.363275 140143678494720 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_23/self_attention/query/kernel
I0512 23:45:14.363313 140143678494720 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_23/self_attention/value/kernel
I0512 23:45:14.363345 140143678494720 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_3/mlp/wi_0/kernel
I0512 23:45:14.363376 140143678494720 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_3/mlp/wi_1/kernel
I0512 23:45:14.363407 140143678494720 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_3/mlp/wo/kernel
I0512 23:45:14.363438 140143678494720 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_3/pre_mlp_layer_norm/scale
I0512 23:45:14.363469 140143678494720 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_3/pre_self_attention_layer_norm/scale
I0512 23:45:14.363500 140143678494720 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_3/self_attention/key/kernel
I0512 23:45:14.363532 140143678494720 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_3/self_attention/out/kernel
I0512 23:45:14.363566 140143678494720 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_3/self_attention/query/kernel
I0512 23:45:14.363597 140143678494720 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_3/self_attention/value/kernel
I0512 23:45:14.363629 140143678494720 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_4/mlp/wi_0/kernel
I0512 23:45:14.363660 140143678494720 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_4/mlp/wi_1/kernel
I0512 23:45:14.363691 140143678494720 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_4/mlp/wo/kernel
I0512 23:45:14.363722 140143678494720 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_4/pre_mlp_layer_norm/scale
I0512 23:45:14.363752 140143678494720 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_4/pre_self_attention_layer_norm/scale
I0512 23:45:14.363783 140143678494720 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_4/self_attention/key/kernel
I0512 23:45:14.363815 140143678494720 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_4/self_attention/out/kernel
I0512 23:45:14.363845 140143678494720 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_4/self_attention/query/kernel
I0512 23:45:14.363877 140143678494720 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_4/self_attention/value/kernel
I0512 23:45:14.363908 140143678494720 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_5/extra_mlp/wi_0/kernel
I0512 23:45:14.363939 140143678494720 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_5/extra_mlp/wi_1/kernel
I0512 23:45:14.363970 140143678494720 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_5/extra_mlp/wo/kernel
I0512 23:45:14.364001 140143678494720 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_5/mlp/expert/wi_0/kernel
I0512 23:45:14.364032 140143678494720 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_5/mlp/expert/wi_1/kernel
I0512 23:45:14.364063 140143678494720 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_5/mlp/expert/wo/kernel
I0512 23:45:14.364095 140143678494720 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_5/mlp/router/router_weights/w/kernel
I0512 23:45:14.364126 140143678494720 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_5/pre_extra_mlp_layer_norm/scale
I0512 23:45:14.364157 140143678494720 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_5/pre_mlp_layer_norm/scale
I0512 23:45:14.364188 140143678494720 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_5/pre_self_attention_layer_norm/scale
I0512 23:45:14.364219 140143678494720 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_5/self_attention/key/kernel
I0512 23:45:14.364250 140143678494720 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_5/self_attention/out/kernel
I0512 23:45:14.364286 140143678494720 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_5/self_attention/query/kernel
I0512 23:45:14.364319 140143678494720 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_5/self_attention/value/kernel
I0512 23:45:14.364350 140143678494720 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_6/mlp/wi_0/kernel
I0512 23:45:14.364382 140143678494720 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_6/mlp/wi_1/kernel
I0512 23:45:14.364413 140143678494720 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_6/mlp/wo/kernel
I0512 23:45:14.364444 140143678494720 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_6/pre_mlp_layer_norm/scale
I0512 23:45:14.364475 140143678494720 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_6/pre_self_attention_layer_norm/scale
I0512 23:45:14.364506 140143678494720 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_6/self_attention/key/kernel
I0512 23:45:14.364559 140143678494720 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_6/self_attention/out/kernel
I0512 23:45:14.364596 140143678494720 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_6/self_attention/query/kernel
I0512 23:45:14.364628 140143678494720 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_6/self_attention/value/kernel
I0512 23:45:14.364659 140143678494720 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_7/mlp/wi_0/kernel
I0512 23:45:14.364691 140143678494720 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_7/mlp/wi_1/kernel
I0512 23:45:14.364722 140143678494720 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_7/mlp/wo/kernel
I0512 23:45:14.364753 140143678494720 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_7/pre_mlp_layer_norm/scale
I0512 23:45:14.364784 140143678494720 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_7/pre_self_attention_layer_norm/scale
I0512 23:45:14.364815 140143678494720 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_7/self_attention/key/kernel
I0512 23:45:14.364847 140143678494720 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_7/self_attention/out/kernel
I0512 23:45:14.364882 140143678494720 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_7/self_attention/query/kernel
I0512 23:45:14.364913 140143678494720 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_7/self_attention/value/kernel
I0512 23:45:14.364945 140143678494720 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_8/mlp/wi_0/kernel
I0512 23:45:14.364976 140143678494720 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_8/mlp/wi_1/kernel
I0512 23:45:14.365007 140143678494720 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_8/mlp/wo/kernel
I0512 23:45:14.365038 140143678494720 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_8/pre_mlp_layer_norm/scale
I0512 23:45:14.365069 140143678494720 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_8/pre_self_attention_layer_norm/scale
I0512 23:45:14.365100 140143678494720 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_8/self_attention/key/kernel
I0512 23:45:14.365132 140143678494720 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_8/self_attention/out/kernel
I0512 23:45:14.365163 140143678494720 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_8/self_attention/query/kernel
I0512 23:45:14.365194 140143678494720 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_8/self_attention/value/kernel
I0512 23:45:14.365225 140143678494720 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_9/mlp/wi_0/kernel
I0512 23:45:14.365256 140143678494720 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_9/mlp/wi_1/kernel
I0512 23:45:14.365293 140143678494720 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_9/mlp/wo/kernel
I0512 23:45:14.365325 140143678494720 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_9/pre_mlp_layer_norm/scale
I0512 23:45:14.365357 140143678494720 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_9/pre_self_attention_layer_norm/scale
I0512 23:45:14.365388 140143678494720 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_9/self_attention/key/kernel
I0512 23:45:14.365419 140143678494720 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_9/self_attention/out/kernel
I0512 23:45:14.365450 140143678494720 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_9/self_attention/query/kernel
I0512 23:45:14.365481 140143678494720 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_9/self_attention/value/kernel
I0512 23:45:14.365512 140143678494720 checkpoints.py:1100] Restoring key from ckpt: target/decoder/logits_dense/kernel
I0512 23:45:14.365545 140143678494720 checkpoints.py:1100] Restoring key from ckpt: target/token_embedder/embedding
I0512 23:45:14.379584 140180528838656 utils.py:442] Initializing parameters from specific T5X checkpoint gs://rosinality-tpu-bucket/openmoe_8b_ul2/training/checkpoint_10000
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
I0000 00:00:1715557514.396215  406878 gcs_resource.cc:109] Using default AdmissionQueue with limit 32
I0000 00:00:1715557514.402655  409055 google_auth_provider.cc:180] Running on GCE, using service account 1081612546678-compute@developer.gserviceaccount.com
I0512 23:45:14.406875 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/decoder_norm/scale/m
I0512 23:45:14.407057 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/decoder_norm/scale/v
I0512 23:45:14.407097 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/decoder_norm/scale/v_col
I0512 23:45:14.407130 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/decoder_norm/scale/v_row
I0512 23:45:14.407163 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_0/mlp/wi_0/kernel/m
I0512 23:45:14.407195 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_0/mlp/wi_0/kernel/v
I0512 23:45:14.407227 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_0/mlp/wi_0/kernel/v_col
I0512 23:45:14.407259 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_0/mlp/wi_0/kernel/v_row
I0512 23:45:14.407291 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_0/mlp/wi_1/kernel/m
I0512 23:45:14.407322 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_0/mlp/wi_1/kernel/v
I0512 23:45:14.407354 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_0/mlp/wi_1/kernel/v_col
I0512 23:45:14.407385 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_0/mlp/wi_1/kernel/v_row
I0512 23:45:14.407416 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_0/mlp/wo/kernel/m
I0512 23:45:14.407447 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_0/mlp/wo/kernel/v
I0512 23:45:14.407479 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_0/mlp/wo/kernel/v_col
I0512 23:45:14.407510 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_0/mlp/wo/kernel/v_row
I0512 23:45:14.407541 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_0/pre_mlp_layer_norm/scale/m
I0512 23:45:14.407596 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_0/pre_mlp_layer_norm/scale/v
I0512 23:45:14.407634 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_0/pre_mlp_layer_norm/scale/v_col
I0512 23:45:14.407672 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_0/pre_mlp_layer_norm/scale/v_row
I0512 23:45:14.407737 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_0/pre_self_attention_layer_norm/scale/m
I0512 23:45:14.407777 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_0/pre_self_attention_layer_norm/scale/v
I0512 23:45:14.407809 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_0/pre_self_attention_layer_norm/scale/v_col
I0512 23:45:14.407842 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_0/pre_self_attention_layer_norm/scale/v_row
I0512 23:45:14.407874 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_0/self_attention/key/kernel/m
I0512 23:45:14.407906 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_0/self_attention/key/kernel/v
I0512 23:45:14.407939 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_0/self_attention/key/kernel/v_col
I0512 23:45:14.407973 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_0/self_attention/key/kernel/v_row
I0512 23:45:14.408005 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_0/self_attention/out/kernel/m
I0512 23:45:14.408036 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_0/self_attention/out/kernel/v
I0512 23:45:14.408068 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_0/self_attention/out/kernel/v_col
I0512 23:45:14.408099 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_0/self_attention/out/kernel/v_row
I0512 23:45:14.408131 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_0/self_attention/query/kernel/m
I0512 23:45:14.408162 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_0/self_attention/query/kernel/v
I0512 23:45:14.408194 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_0/self_attention/query/kernel/v_col
I0512 23:45:14.408225 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_0/self_attention/query/kernel/v_row
I0512 23:45:14.408256 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_0/self_attention/value/kernel/m
I0512 23:45:14.408288 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_0/self_attention/value/kernel/v
I0512 23:45:14.408319 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_0/self_attention/value/kernel/v_col
I0512 23:45:14.408350 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_0/self_attention/value/kernel/v_row
I0512 23:45:14.408382 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_1/mlp/wi_0/kernel/m
I0512 23:45:14.408413 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_1/mlp/wi_0/kernel/v
I0512 23:45:14.408445 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_1/mlp/wi_0/kernel/v_col
I0512 23:45:14.408476 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_1/mlp/wi_0/kernel/v_row
I0512 23:45:14.408508 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_1/mlp/wi_1/kernel/m
I0512 23:45:14.408540 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_1/mlp/wi_1/kernel/v
I0512 23:45:14.408571 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_1/mlp/wi_1/kernel/v_col
I0512 23:45:14.408602 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_1/mlp/wi_1/kernel/v_row
I0512 23:45:14.408633 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_1/mlp/wo/kernel/m
I0512 23:45:14.408665 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_1/mlp/wo/kernel/v
I0512 23:45:14.408712 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_1/mlp/wo/kernel/v_col
I0512 23:45:14.408745 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_1/mlp/wo/kernel/v_row
I0512 23:45:14.408777 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_1/pre_mlp_layer_norm/scale/m
I0512 23:45:14.408809 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_1/pre_mlp_layer_norm/scale/v
I0512 23:45:14.408840 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_1/pre_mlp_layer_norm/scale/v_col
I0512 23:45:14.408872 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_1/pre_mlp_layer_norm/scale/v_row
I0512 23:45:14.408903 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_1/pre_self_attention_layer_norm/scale/m
I0512 23:45:14.408936 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_1/pre_self_attention_layer_norm/scale/v
I0512 23:45:14.408970 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_1/pre_self_attention_layer_norm/scale/v_col
I0512 23:45:14.409002 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_1/pre_self_attention_layer_norm/scale/v_row
I0512 23:45:14.409034 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_1/self_attention/key/kernel/m
I0512 23:45:14.409065 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_1/self_attention/key/kernel/v
I0512 23:45:14.409097 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_1/self_attention/key/kernel/v_col
I0512 23:45:14.409128 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_1/self_attention/key/kernel/v_row
I0512 23:45:14.409160 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_1/self_attention/out/kernel/m
I0512 23:45:14.409191 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_1/self_attention/out/kernel/v
I0512 23:45:14.409223 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_1/self_attention/out/kernel/v_col
I0512 23:45:14.409255 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_1/self_attention/out/kernel/v_row
I0512 23:45:14.409286 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_1/self_attention/query/kernel/m
I0512 23:45:14.409317 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_1/self_attention/query/kernel/v
I0512 23:45:14.409349 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_1/self_attention/query/kernel/v_col
I0512 23:45:14.409380 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_1/self_attention/query/kernel/v_row
I0512 23:45:14.409412 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_1/self_attention/value/kernel/m
I0512 23:45:14.409444 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_1/self_attention/value/kernel/v
I0512 23:45:14.409475 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_1/self_attention/value/kernel/v_col
I0512 23:45:14.409506 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_1/self_attention/value/kernel/v_row
I0512 23:45:14.409538 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_10/mlp/wi_0/kernel/m
I0512 23:45:14.409569 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_10/mlp/wi_0/kernel/v
I0512 23:45:14.409601 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_10/mlp/wi_0/kernel/v_col
I0512 23:45:14.409632 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_10/mlp/wi_0/kernel/v_row
I0512 23:45:14.409664 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_10/mlp/wi_1/kernel/m
I0512 23:45:14.409701 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_10/mlp/wi_1/kernel/v
I0512 23:45:14.409734 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_10/mlp/wi_1/kernel/v_col
I0512 23:45:14.409765 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_10/mlp/wi_1/kernel/v_row
I0512 23:45:14.409797 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_10/mlp/wo/kernel/m
I0512 23:45:14.409828 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_10/mlp/wo/kernel/v
I0512 23:45:14.409859 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_10/mlp/wo/kernel/v_col
I0512 23:45:14.409891 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_10/mlp/wo/kernel/v_row
I0512 23:45:14.409922 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_10/pre_mlp_layer_norm/scale/m
I0512 23:45:14.409955 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_10/pre_mlp_layer_norm/scale/v
I0512 23:45:14.409988 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_10/pre_mlp_layer_norm/scale/v_col
I0512 23:45:14.410019 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_10/pre_mlp_layer_norm/scale/v_row
I0512 23:45:14.410051 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_10/pre_self_attention_layer_norm/scale/m
I0512 23:45:14.410082 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_10/pre_self_attention_layer_norm/scale/v
I0512 23:45:14.410114 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_10/pre_self_attention_layer_norm/scale/v_col
I0512 23:45:14.410145 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_10/pre_self_attention_layer_norm/scale/v_row
I0512 23:45:14.410177 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_10/self_attention/key/kernel/m
I0512 23:45:14.410209 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_10/self_attention/key/kernel/v
I0512 23:45:14.410241 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_10/self_attention/key/kernel/v_col
I0512 23:45:14.410272 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_10/self_attention/key/kernel/v_row
I0512 23:45:14.410304 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_10/self_attention/out/kernel/m
I0512 23:45:14.410335 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_10/self_attention/out/kernel/v
I0512 23:45:14.410367 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_10/self_attention/out/kernel/v_col
I0512 23:45:14.410398 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_10/self_attention/out/kernel/v_row
I0512 23:45:14.410429 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_10/self_attention/query/kernel/m
I0512 23:45:14.410461 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_10/self_attention/query/kernel/v
I0512 23:45:14.410492 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_10/self_attention/query/kernel/v_col
I0512 23:45:14.410523 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_10/self_attention/query/kernel/v_row
I0512 23:45:14.410555 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_10/self_attention/value/kernel/m
I0512 23:45:14.410586 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_10/self_attention/value/kernel/v
I0512 23:45:14.410618 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_10/self_attention/value/kernel/v_col
I0512 23:45:14.410649 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_10/self_attention/value/kernel/v_row
I0512 23:45:14.410686 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/extra_mlp/wi_0/kernel/m
I0512 23:45:14.410720 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/extra_mlp/wi_0/kernel/v
I0512 23:45:14.410752 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/extra_mlp/wi_0/kernel/v_col
I0512 23:45:14.410783 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/extra_mlp/wi_0/kernel/v_row
I0512 23:45:14.410815 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/extra_mlp/wi_1/kernel/m
I0512 23:45:14.410846 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/extra_mlp/wi_1/kernel/v
I0512 23:45:14.410878 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/extra_mlp/wi_1/kernel/v_col
I0512 23:45:14.410909 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/extra_mlp/wi_1/kernel/v_row
I0512 23:45:14.410942 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/extra_mlp/wo/kernel/m
I0512 23:45:14.410975 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/extra_mlp/wo/kernel/v
I0512 23:45:14.411007 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/extra_mlp/wo/kernel/v_col
I0512 23:45:14.411038 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/extra_mlp/wo/kernel/v_row
I0512 23:45:14.411070 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/mlp/expert/wi_0/kernel/m
I0512 23:45:14.411102 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/mlp/expert/wi_0/kernel/v
I0512 23:45:14.411133 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/mlp/expert/wi_0/kernel/v_col
I0512 23:45:14.411164 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/mlp/expert/wi_0/kernel/v_row
I0512 23:45:14.411196 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/mlp/expert/wi_1/kernel/m
I0512 23:45:14.411227 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/mlp/expert/wi_1/kernel/v
I0512 23:45:14.411259 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/mlp/expert/wi_1/kernel/v_col
I0512 23:45:14.411290 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/mlp/expert/wi_1/kernel/v_row
I0512 23:45:14.411322 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/mlp/expert/wo/kernel/m
I0512 23:45:14.411353 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/mlp/expert/wo/kernel/v
I0512 23:45:14.411385 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/mlp/expert/wo/kernel/v_col
I0512 23:45:14.411417 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/mlp/expert/wo/kernel/v_row
I0512 23:45:14.411448 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/mlp/router/router_weights/w/kernel/m
I0512 23:45:14.411479 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/mlp/router/router_weights/w/kernel/v
I0512 23:45:14.411511 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/mlp/router/router_weights/w/kernel/v_col
I0512 23:45:14.411543 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/mlp/router/router_weights/w/kernel/v_row
I0512 23:45:14.411596 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/pre_extra_mlp_layer_norm/scale/m
I0512 23:45:14.411632 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/pre_extra_mlp_layer_norm/scale/v
I0512 23:45:14.411664 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/pre_extra_mlp_layer_norm/scale/v_col
I0512 23:45:14.411701 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/pre_extra_mlp_layer_norm/scale/v_row
I0512 23:45:14.411734 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/pre_mlp_layer_norm/scale/m
I0512 23:45:14.411766 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/pre_mlp_layer_norm/scale/v
I0512 23:45:14.411798 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/pre_mlp_layer_norm/scale/v_col
I0512 23:45:14.411829 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/pre_mlp_layer_norm/scale/v_row
I0512 23:45:14.411860 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/pre_self_attention_layer_norm/scale/m
I0512 23:45:14.411892 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/pre_self_attention_layer_norm/scale/v
I0512 23:45:14.411923 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/pre_self_attention_layer_norm/scale/v_col
I0512 23:45:14.411958 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/pre_self_attention_layer_norm/scale/v_row
I0512 23:45:14.411990 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/self_attention/key/kernel/m
I0512 23:45:14.412022 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/self_attention/key/kernel/v
I0512 23:45:14.412053 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/self_attention/key/kernel/v_col
I0512 23:45:14.412085 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/self_attention/key/kernel/v_row
I0512 23:45:14.412116 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/self_attention/out/kernel/m
I0512 23:45:14.412148 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/self_attention/out/kernel/v
I0512 23:45:14.412179 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/self_attention/out/kernel/v_col
I0512 23:45:14.412211 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/self_attention/out/kernel/v_row
I0512 23:45:14.412242 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/self_attention/query/kernel/m
I0512 23:45:14.412274 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/self_attention/query/kernel/v
I0512 23:45:14.412306 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/self_attention/query/kernel/v_col
I0512 23:45:14.412338 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/self_attention/query/kernel/v_row
I0512 23:45:14.412369 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/self_attention/value/kernel/m
I0512 23:45:14.412400 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/self_attention/value/kernel/v
I0512 23:45:14.412432 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/self_attention/value/kernel/v_col
I0512 23:45:14.412463 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/self_attention/value/kernel/v_row
I0512 23:45:14.412494 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_12/mlp/wi_0/kernel/m
I0512 23:45:14.412526 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_12/mlp/wi_0/kernel/v
I0512 23:45:14.412557 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_12/mlp/wi_0/kernel/v_col
I0512 23:45:14.412589 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_12/mlp/wi_0/kernel/v_row
I0512 23:45:14.412620 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_12/mlp/wi_1/kernel/m
I0512 23:45:14.412651 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_12/mlp/wi_1/kernel/v
I0512 23:45:14.412689 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_12/mlp/wi_1/kernel/v_col
I0512 23:45:14.412722 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_12/mlp/wi_1/kernel/v_row
I0512 23:45:14.412753 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_12/mlp/wo/kernel/m
I0512 23:45:14.412785 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_12/mlp/wo/kernel/v
I0512 23:45:14.412816 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_12/mlp/wo/kernel/v_col
I0512 23:45:14.412847 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_12/mlp/wo/kernel/v_row
I0512 23:45:14.412878 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_12/pre_mlp_layer_norm/scale/m
I0512 23:45:14.412910 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_12/pre_mlp_layer_norm/scale/v
I0512 23:45:14.412943 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_12/pre_mlp_layer_norm/scale/v_col
I0512 23:45:14.412976 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_12/pre_mlp_layer_norm/scale/v_row
I0512 23:45:14.413008 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_12/pre_self_attention_layer_norm/scale/m
I0512 23:45:14.413039 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_12/pre_self_attention_layer_norm/scale/v
I0512 23:45:14.413071 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_12/pre_self_attention_layer_norm/scale/v_col
I0512 23:45:14.413102 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_12/pre_self_attention_layer_norm/scale/v_row
I0512 23:45:14.413133 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_12/self_attention/key/kernel/m
I0512 23:45:14.413165 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_12/self_attention/key/kernel/v
I0512 23:45:14.413196 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_12/self_attention/key/kernel/v_col
I0512 23:45:14.413227 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_12/self_attention/key/kernel/v_row
I0512 23:45:14.413259 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_12/self_attention/out/kernel/m
I0512 23:45:14.413290 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_12/self_attention/out/kernel/v
I0512 23:45:14.413321 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_12/self_attention/out/kernel/v_col
I0512 23:45:14.413352 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_12/self_attention/out/kernel/v_row
I0512 23:45:14.413383 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_12/self_attention/query/kernel/m
I0512 23:45:14.413415 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_12/self_attention/query/kernel/v
I0512 23:45:14.413446 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_12/self_attention/query/kernel/v_col
I0512 23:45:14.413477 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_12/self_attention/query/kernel/v_row
I0512 23:45:14.413510 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_12/self_attention/value/kernel/m
I0512 23:45:14.413542 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_12/self_attention/value/kernel/v
I0512 23:45:14.413573 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_12/self_attention/value/kernel/v_col
I0512 23:45:14.413604 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_12/self_attention/value/kernel/v_row
I0512 23:45:14.413635 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_13/mlp/wi_0/kernel/m
I0512 23:45:14.413666 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_13/mlp/wi_0/kernel/v
I0512 23:45:14.413703 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_13/mlp/wi_0/kernel/v_col
I0512 23:45:14.413735 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_13/mlp/wi_0/kernel/v_row
I0512 23:45:14.413766 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_13/mlp/wi_1/kernel/m
I0512 23:45:14.413797 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_13/mlp/wi_1/kernel/v
I0512 23:45:14.413828 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_13/mlp/wi_1/kernel/v_col
I0512 23:45:14.413859 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_13/mlp/wi_1/kernel/v_row
I0512 23:45:14.413890 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_13/mlp/wo/kernel/m
I0512 23:45:14.413921 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_13/mlp/wo/kernel/v
I0512 23:45:14.413953 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_13/mlp/wo/kernel/v_col
I0512 23:45:14.413985 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_13/mlp/wo/kernel/v_row
I0512 23:45:14.414016 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_13/pre_mlp_layer_norm/scale/m
I0512 23:45:14.414047 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_13/pre_mlp_layer_norm/scale/v
I0512 23:45:14.414077 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_13/pre_mlp_layer_norm/scale/v_col
I0512 23:45:14.414109 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_13/pre_mlp_layer_norm/scale/v_row
I0512 23:45:14.414139 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_13/pre_self_attention_layer_norm/scale/m
I0512 23:45:14.414171 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_13/pre_self_attention_layer_norm/scale/v
I0512 23:45:14.414202 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_13/pre_self_attention_layer_norm/scale/v_col
I0512 23:45:14.414232 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_13/pre_self_attention_layer_norm/scale/v_row
I0512 23:45:14.414264 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_13/self_attention/key/kernel/m
I0512 23:45:14.414295 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_13/self_attention/key/kernel/v
I0512 23:45:14.414326 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_13/self_attention/key/kernel/v_col
I0512 23:45:14.414356 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_13/self_attention/key/kernel/v_row
I0512 23:45:14.414387 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_13/self_attention/out/kernel/m
I0512 23:45:14.414418 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_13/self_attention/out/kernel/v
I0512 23:45:14.414449 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_13/self_attention/out/kernel/v_col
I0512 23:45:14.414480 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_13/self_attention/out/kernel/v_row
I0512 23:45:14.414511 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_13/self_attention/query/kernel/m
I0512 23:45:14.414542 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_13/self_attention/query/kernel/v
I0512 23:45:14.414573 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_13/self_attention/query/kernel/v_col
I0512 23:45:14.414603 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_13/self_attention/query/kernel/v_row
I0512 23:45:14.414634 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_13/self_attention/value/kernel/m
I0512 23:45:14.414666 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_13/self_attention/value/kernel/v
I0512 23:45:14.414703 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_13/self_attention/value/kernel/v_col
I0512 23:45:14.414735 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_13/self_attention/value/kernel/v_row
I0512 23:45:14.414766 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_14/mlp/wi_0/kernel/m
I0512 23:45:14.414797 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_14/mlp/wi_0/kernel/v
I0512 23:45:14.414828 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_14/mlp/wi_0/kernel/v_col
I0512 23:45:14.414859 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_14/mlp/wi_0/kernel/v_row
I0512 23:45:14.414890 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_14/mlp/wi_1/kernel/m
I0512 23:45:14.414921 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_14/mlp/wi_1/kernel/v
I0512 23:45:14.414954 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_14/mlp/wi_1/kernel/v_col
I0512 23:45:14.414985 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_14/mlp/wi_1/kernel/v_row
I0512 23:45:14.415016 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_14/mlp/wo/kernel/m
I0512 23:45:14.415047 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_14/mlp/wo/kernel/v
I0512 23:45:14.415078 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_14/mlp/wo/kernel/v_col
I0512 23:45:14.415141 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_14/mlp/wo/kernel/v_row
I0512 23:45:14.415173 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_14/pre_mlp_layer_norm/scale/m
I0512 23:45:14.415204 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_14/pre_mlp_layer_norm/scale/v
I0512 23:45:14.415235 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_14/pre_mlp_layer_norm/scale/v_col
I0512 23:45:14.415266 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_14/pre_mlp_layer_norm/scale/v_row
I0512 23:45:14.415297 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_14/pre_self_attention_layer_norm/scale/m
I0512 23:45:14.415328 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_14/pre_self_attention_layer_norm/scale/v
I0512 23:45:14.415359 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_14/pre_self_attention_layer_norm/scale/v_col
I0512 23:45:14.415390 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_14/pre_self_attention_layer_norm/scale/v_row
I0512 23:45:14.415422 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_14/self_attention/key/kernel/m
I0512 23:45:14.415453 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_14/self_attention/key/kernel/v
I0512 23:45:14.415484 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_14/self_attention/key/kernel/v_col
I0512 23:45:14.415515 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_14/self_attention/key/kernel/v_row
I0512 23:45:14.415547 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_14/self_attention/out/kernel/m
I0512 23:45:14.415600 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_14/self_attention/out/kernel/v
I0512 23:45:14.415633 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_14/self_attention/out/kernel/v_col
I0512 23:45:14.415664 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_14/self_attention/out/kernel/v_row
I0512 23:45:14.415701 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_14/self_attention/query/kernel/m
I0512 23:45:14.415733 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_14/self_attention/query/kernel/v
I0512 23:45:14.415764 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_14/self_attention/query/kernel/v_col
I0512 23:45:14.415795 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_14/self_attention/query/kernel/v_row
I0512 23:45:14.415826 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_14/self_attention/value/kernel/m
I0512 23:45:14.415857 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_14/self_attention/value/kernel/v
I0512 23:45:14.415888 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_14/self_attention/value/kernel/v_col
I0512 23:45:14.415919 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_14/self_attention/value/kernel/v_row
I0512 23:45:14.415952 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_15/mlp/wi_0/kernel/m
I0512 23:45:14.415985 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_15/mlp/wi_0/kernel/v
I0512 23:45:14.416016 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_15/mlp/wi_0/kernel/v_col
I0512 23:45:14.416047 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_15/mlp/wi_0/kernel/v_row
I0512 23:45:14.416078 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_15/mlp/wi_1/kernel/m
I0512 23:45:14.416109 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_15/mlp/wi_1/kernel/v
I0512 23:45:14.416140 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_15/mlp/wi_1/kernel/v_col
I0512 23:45:14.416171 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_15/mlp/wi_1/kernel/v_row
I0512 23:45:14.416201 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_15/mlp/wo/kernel/m
I0512 23:45:14.416232 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_15/mlp/wo/kernel/v
I0512 23:45:14.416263 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_15/mlp/wo/kernel/v_col
I0512 23:45:14.416294 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_15/mlp/wo/kernel/v_row
I0512 23:45:14.416324 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_15/pre_mlp_layer_norm/scale/m
I0512 23:45:14.416355 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_15/pre_mlp_layer_norm/scale/v
I0512 23:45:14.416386 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_15/pre_mlp_layer_norm/scale/v_col
I0512 23:45:14.416417 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_15/pre_mlp_layer_norm/scale/v_row
I0512 23:45:14.416448 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_15/pre_self_attention_layer_norm/scale/m
I0512 23:45:14.416479 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_15/pre_self_attention_layer_norm/scale/v
I0512 23:45:14.416509 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_15/pre_self_attention_layer_norm/scale/v_col
I0512 23:45:14.416540 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_15/pre_self_attention_layer_norm/scale/v_row
I0512 23:45:14.416571 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_15/self_attention/key/kernel/m
I0512 23:45:14.416602 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_15/self_attention/key/kernel/v
I0512 23:45:14.416633 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_15/self_attention/key/kernel/v_col
I0512 23:45:14.416664 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_15/self_attention/key/kernel/v_row
I0512 23:45:14.416701 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_15/self_attention/out/kernel/m
I0512 23:45:14.416733 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_15/self_attention/out/kernel/v
I0512 23:45:14.416764 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_15/self_attention/out/kernel/v_col
I0512 23:45:14.416795 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_15/self_attention/out/kernel/v_row
I0512 23:45:14.416827 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_15/self_attention/query/kernel/m
I0512 23:45:14.416862 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_15/self_attention/query/kernel/v
I0512 23:45:14.416893 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_15/self_attention/query/kernel/v_col
I0512 23:45:14.416928 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_15/self_attention/query/kernel/v_row
I0512 23:45:14.416965 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_15/self_attention/value/kernel/m
I0512 23:45:14.417002 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_15/self_attention/value/kernel/v
I0512 23:45:14.417035 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_15/self_attention/value/kernel/v_col
I0512 23:45:14.417066 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_15/self_attention/value/kernel/v_row
I0512 23:45:14.417103 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_16/mlp/wi_0/kernel/m
I0512 23:45:14.417136 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_16/mlp/wi_0/kernel/v
I0512 23:45:14.417166 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_16/mlp/wi_0/kernel/v_col
I0512 23:45:14.417196 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_16/mlp/wi_0/kernel/v_row
I0512 23:45:14.417227 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_16/mlp/wi_1/kernel/m
I0512 23:45:14.417257 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_16/mlp/wi_1/kernel/v
I0512 23:45:14.417286 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_16/mlp/wi_1/kernel/v_col
I0512 23:45:14.417316 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_16/mlp/wi_1/kernel/v_row
I0512 23:45:14.417346 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_16/mlp/wo/kernel/m
I0512 23:45:14.417376 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_16/mlp/wo/kernel/v
I0512 23:45:14.417405 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_16/mlp/wo/kernel/v_col
I0512 23:45:14.417435 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_16/mlp/wo/kernel/v_row
I0512 23:45:14.417464 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_16/pre_mlp_layer_norm/scale/m
I0512 23:45:14.417494 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_16/pre_mlp_layer_norm/scale/v
I0512 23:45:14.417523 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_16/pre_mlp_layer_norm/scale/v_col
I0512 23:45:14.417554 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_16/pre_mlp_layer_norm/scale/v_row
I0512 23:45:14.417585 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_16/pre_self_attention_layer_norm/scale/m
I0512 23:45:14.417615 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_16/pre_self_attention_layer_norm/scale/v
I0512 23:45:14.417645 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_16/pre_self_attention_layer_norm/scale/v_col
I0512 23:45:14.417675 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_16/pre_self_attention_layer_norm/scale/v_row
I0512 23:45:14.417713 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_16/self_attention/key/kernel/m
I0512 23:45:14.417743 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_16/self_attention/key/kernel/v
I0512 23:45:14.417772 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_16/self_attention/key/kernel/v_col
I0512 23:45:14.417802 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_16/self_attention/key/kernel/v_row
I0512 23:45:14.417832 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_16/self_attention/out/kernel/m
I0512 23:45:14.417861 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_16/self_attention/out/kernel/v
I0512 23:45:14.417891 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_16/self_attention/out/kernel/v_col
I0512 23:45:14.417920 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_16/self_attention/out/kernel/v_row
I0512 23:45:14.417953 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_16/self_attention/query/kernel/m
I0512 23:45:14.417989 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_16/self_attention/query/kernel/v
I0512 23:45:14.418021 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_16/self_attention/query/kernel/v_col
I0512 23:45:14.418052 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_16/self_attention/query/kernel/v_row
I0512 23:45:14.418081 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_16/self_attention/value/kernel/m
I0512 23:45:14.418111 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_16/self_attention/value/kernel/v
I0512 23:45:14.418140 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_16/self_attention/value/kernel/v_col
I0512 23:45:14.418169 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_16/self_attention/value/kernel/v_row
I0512 23:45:14.418199 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/extra_mlp/wi_0/kernel/m
I0512 23:45:14.418228 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/extra_mlp/wi_0/kernel/v
I0512 23:45:14.418258 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/extra_mlp/wi_0/kernel/v_col
I0512 23:45:14.418287 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/extra_mlp/wi_0/kernel/v_row
I0512 23:45:14.418317 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/extra_mlp/wi_1/kernel/m
I0512 23:45:14.418347 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/extra_mlp/wi_1/kernel/v
I0512 23:45:14.418376 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/extra_mlp/wi_1/kernel/v_col
I0512 23:45:14.418405 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/extra_mlp/wi_1/kernel/v_row
I0512 23:45:14.418435 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/extra_mlp/wo/kernel/m
I0512 23:45:14.418465 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/extra_mlp/wo/kernel/v
I0512 23:45:14.418494 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/extra_mlp/wo/kernel/v_col
I0512 23:45:14.418524 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/extra_mlp/wo/kernel/v_row
I0512 23:45:14.418553 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/mlp/expert/wi_0/kernel/m
I0512 23:45:14.418583 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/mlp/expert/wi_0/kernel/v
I0512 23:45:14.418612 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/mlp/expert/wi_0/kernel/v_col
I0512 23:45:14.418642 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/mlp/expert/wi_0/kernel/v_row
I0512 23:45:14.418671 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/mlp/expert/wi_1/kernel/m
I0512 23:45:14.418708 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/mlp/expert/wi_1/kernel/v
I0512 23:45:14.418738 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/mlp/expert/wi_1/kernel/v_col
I0512 23:45:14.418768 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/mlp/expert/wi_1/kernel/v_row
I0512 23:45:14.418797 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/mlp/expert/wo/kernel/m
I0512 23:45:14.418827 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/mlp/expert/wo/kernel/v
I0512 23:45:14.418856 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/mlp/expert/wo/kernel/v_col
I0512 23:45:14.418885 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/mlp/expert/wo/kernel/v_row
I0512 23:45:14.418915 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/mlp/router/router_weights/w/kernel/m
I0512 23:45:14.418946 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/mlp/router/router_weights/w/kernel/v
I0512 23:45:14.418977 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/mlp/router/router_weights/w/kernel/v_col
I0512 23:45:14.419007 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/mlp/router/router_weights/w/kernel/v_row
I0512 23:45:14.419037 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/pre_extra_mlp_layer_norm/scale/m
I0512 23:45:14.419067 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/pre_extra_mlp_layer_norm/scale/v
I0512 23:45:14.419096 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/pre_extra_mlp_layer_norm/scale/v_col
I0512 23:45:14.419126 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/pre_extra_mlp_layer_norm/scale/v_row
I0512 23:45:14.419155 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/pre_mlp_layer_norm/scale/m
I0512 23:45:14.419184 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/pre_mlp_layer_norm/scale/v
I0512 23:45:14.419214 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/pre_mlp_layer_norm/scale/v_col
I0512 23:45:14.419244 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/pre_mlp_layer_norm/scale/v_row
I0512 23:45:14.419273 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/pre_self_attention_layer_norm/scale/m
I0512 23:45:14.419302 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/pre_self_attention_layer_norm/scale/v
I0512 23:45:14.419332 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/pre_self_attention_layer_norm/scale/v_col
I0512 23:45:14.419362 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/pre_self_attention_layer_norm/scale/v_row
I0512 23:45:14.419391 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/self_attention/key/kernel/m
I0512 23:45:14.419421 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/self_attention/key/kernel/v
I0512 23:45:14.419451 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/self_attention/key/kernel/v_col
I0512 23:45:14.419480 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/self_attention/key/kernel/v_row
I0512 23:45:14.419509 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/self_attention/out/kernel/m
I0512 23:45:14.419538 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/self_attention/out/kernel/v
I0512 23:45:14.419593 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/self_attention/out/kernel/v_col
I0512 23:45:14.419629 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/self_attention/out/kernel/v_row
I0512 23:45:14.419659 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/self_attention/query/kernel/m
I0512 23:45:14.419694 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/self_attention/query/kernel/v
I0512 23:45:14.419725 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/self_attention/query/kernel/v_col
I0512 23:45:14.419755 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/self_attention/query/kernel/v_row
I0512 23:45:14.419784 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/self_attention/value/kernel/m
I0512 23:45:14.419814 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/self_attention/value/kernel/v
I0512 23:45:14.419843 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/self_attention/value/kernel/v_col
I0512 23:45:14.419872 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/self_attention/value/kernel/v_row
I0512 23:45:14.419901 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_18/mlp/wi_0/kernel/m
I0512 23:45:14.419931 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_18/mlp/wi_0/kernel/v
I0512 23:45:14.419963 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_18/mlp/wi_0/kernel/v_col
I0512 23:45:14.419993 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_18/mlp/wi_0/kernel/v_row
I0512 23:45:14.420022 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_18/mlp/wi_1/kernel/m
I0512 23:45:14.420052 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_18/mlp/wi_1/kernel/v
I0512 23:45:14.420081 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_18/mlp/wi_1/kernel/v_col
I0512 23:45:14.420110 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_18/mlp/wi_1/kernel/v_row
I0512 23:45:14.420139 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_18/mlp/wo/kernel/m
I0512 23:45:14.420169 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_18/mlp/wo/kernel/v
I0512 23:45:14.420198 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_18/mlp/wo/kernel/v_col
I0512 23:45:14.420228 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_18/mlp/wo/kernel/v_row
I0512 23:45:14.420258 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_18/pre_mlp_layer_norm/scale/m
I0512 23:45:14.420287 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_18/pre_mlp_layer_norm/scale/v
I0512 23:45:14.420317 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_18/pre_mlp_layer_norm/scale/v_col
I0512 23:45:14.420346 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_18/pre_mlp_layer_norm/scale/v_row
I0512 23:45:14.420375 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_18/pre_self_attention_layer_norm/scale/m
I0512 23:45:14.420404 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_18/pre_self_attention_layer_norm/scale/v
I0512 23:45:14.420434 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_18/pre_self_attention_layer_norm/scale/v_col
I0512 23:45:14.420463 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_18/pre_self_attention_layer_norm/scale/v_row
I0512 23:45:14.420493 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_18/self_attention/key/kernel/m
I0512 23:45:14.420522 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_18/self_attention/key/kernel/v
I0512 23:45:14.420552 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_18/self_attention/key/kernel/v_col
I0512 23:45:14.420581 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_18/self_attention/key/kernel/v_row
I0512 23:45:14.420610 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_18/self_attention/out/kernel/m
I0512 23:45:14.420639 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_18/self_attention/out/kernel/v
I0512 23:45:14.420668 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_18/self_attention/out/kernel/v_col
I0512 23:45:14.420704 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_18/self_attention/out/kernel/v_row
I0512 23:45:14.420734 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_18/self_attention/query/kernel/m
I0512 23:45:14.420764 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_18/self_attention/query/kernel/v
I0512 23:45:14.420793 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_18/self_attention/query/kernel/v_col
I0512 23:45:14.420823 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_18/self_attention/query/kernel/v_row
I0512 23:45:14.420857 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_18/self_attention/value/kernel/m
I0512 23:45:14.420886 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_18/self_attention/value/kernel/v
I0512 23:45:14.420916 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_18/self_attention/value/kernel/v_col
I0512 23:45:14.420947 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_18/self_attention/value/kernel/v_row
I0512 23:45:14.420977 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_19/mlp/wi_0/kernel/m
I0512 23:45:14.421007 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_19/mlp/wi_0/kernel/v
I0512 23:45:14.421037 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_19/mlp/wi_0/kernel/v_col
I0512 23:45:14.421066 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_19/mlp/wi_0/kernel/v_row
I0512 23:45:14.421095 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_19/mlp/wi_1/kernel/m
I0512 23:45:14.421125 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_19/mlp/wi_1/kernel/v
I0512 23:45:14.421154 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_19/mlp/wi_1/kernel/v_col
I0512 23:45:14.421183 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_19/mlp/wi_1/kernel/v_row
I0512 23:45:14.421212 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_19/mlp/wo/kernel/m
I0512 23:45:14.421242 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_19/mlp/wo/kernel/v
I0512 23:45:14.421272 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_19/mlp/wo/kernel/v_col
I0512 23:45:14.421301 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_19/mlp/wo/kernel/v_row
I0512 23:45:14.421330 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_19/pre_mlp_layer_norm/scale/m
I0512 23:45:14.421360 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_19/pre_mlp_layer_norm/scale/v
I0512 23:45:14.421390 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_19/pre_mlp_layer_norm/scale/v_col
I0512 23:45:14.421419 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_19/pre_mlp_layer_norm/scale/v_row
I0512 23:45:14.421449 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_19/pre_self_attention_layer_norm/scale/m
I0512 23:45:14.421478 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_19/pre_self_attention_layer_norm/scale/v
I0512 23:45:14.421508 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_19/pre_self_attention_layer_norm/scale/v_col
I0512 23:45:14.421538 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_19/pre_self_attention_layer_norm/scale/v_row
I0512 23:45:14.421567 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_19/self_attention/key/kernel/m
I0512 23:45:14.421597 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_19/self_attention/key/kernel/v
I0512 23:45:14.421626 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_19/self_attention/key/kernel/v_col
I0512 23:45:14.421656 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_19/self_attention/key/kernel/v_row
I0512 23:45:14.421690 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_19/self_attention/out/kernel/m
I0512 23:45:14.421720 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_19/self_attention/out/kernel/v
I0512 23:45:14.421750 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_19/self_attention/out/kernel/v_col
I0512 23:45:14.421779 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_19/self_attention/out/kernel/v_row
I0512 23:45:14.421809 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_19/self_attention/query/kernel/m
I0512 23:45:14.421838 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_19/self_attention/query/kernel/v
I0512 23:45:14.421868 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_19/self_attention/query/kernel/v_col
I0512 23:45:14.421897 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_19/self_attention/query/kernel/v_row
I0512 23:45:14.421927 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_19/self_attention/value/kernel/m
I0512 23:45:14.421958 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_19/self_attention/value/kernel/v
I0512 23:45:14.421989 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_19/self_attention/value/kernel/v_col
I0512 23:45:14.422019 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_19/self_attention/value/kernel/v_row
I0512 23:45:14.422048 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_2/mlp/wi_0/kernel/m
I0512 23:45:14.422078 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_2/mlp/wi_0/kernel/v
I0512 23:45:14.422107 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_2/mlp/wi_0/kernel/v_col
I0512 23:45:14.422137 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_2/mlp/wi_0/kernel/v_row
I0512 23:45:14.422166 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_2/mlp/wi_1/kernel/m
I0512 23:45:14.422195 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_2/mlp/wi_1/kernel/v
I0512 23:45:14.422225 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_2/mlp/wi_1/kernel/v_col
I0512 23:45:14.422254 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_2/mlp/wi_1/kernel/v_row
I0512 23:45:14.422284 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_2/mlp/wo/kernel/m
I0512 23:45:14.422313 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_2/mlp/wo/kernel/v
I0512 23:45:14.422343 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_2/mlp/wo/kernel/v_col
I0512 23:45:14.422372 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_2/mlp/wo/kernel/v_row
I0512 23:45:14.422401 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_2/pre_mlp_layer_norm/scale/m
I0512 23:45:14.422430 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_2/pre_mlp_layer_norm/scale/v
I0512 23:45:14.422460 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_2/pre_mlp_layer_norm/scale/v_col
I0512 23:45:14.422489 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_2/pre_mlp_layer_norm/scale/v_row
I0512 23:45:14.422519 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_2/pre_self_attention_layer_norm/scale/m
I0512 23:45:14.422549 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_2/pre_self_attention_layer_norm/scale/v
I0512 23:45:14.422578 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_2/pre_self_attention_layer_norm/scale/v_col
I0512 23:45:14.422608 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_2/pre_self_attention_layer_norm/scale/v_row
I0512 23:45:14.422638 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_2/self_attention/key/kernel/m
I0512 23:45:14.422667 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_2/self_attention/key/kernel/v
I0512 23:45:14.422703 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_2/self_attention/key/kernel/v_col
I0512 23:45:14.422733 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_2/self_attention/key/kernel/v_row
I0512 23:45:14.422763 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_2/self_attention/out/kernel/m
I0512 23:45:14.422792 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_2/self_attention/out/kernel/v
I0512 23:45:14.422821 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_2/self_attention/out/kernel/v_col
I0512 23:45:14.422851 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_2/self_attention/out/kernel/v_row
I0512 23:45:14.422881 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_2/self_attention/query/kernel/m
I0512 23:45:14.422910 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_2/self_attention/query/kernel/v
I0512 23:45:14.422941 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_2/self_attention/query/kernel/v_col
I0512 23:45:14.422972 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_2/self_attention/query/kernel/v_row
I0512 23:45:14.423002 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_2/self_attention/value/kernel/m
I0512 23:45:14.423032 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_2/self_attention/value/kernel/v
I0512 23:45:14.423061 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_2/self_attention/value/kernel/v_col
I0512 23:45:14.423091 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_2/self_attention/value/kernel/v_row
I0512 23:45:14.423121 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_20/mlp/wi_0/kernel/m
I0512 23:45:14.423150 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_20/mlp/wi_0/kernel/v
I0512 23:45:14.423180 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_20/mlp/wi_0/kernel/v_col
I0512 23:45:14.423209 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_20/mlp/wi_0/kernel/v_row
I0512 23:45:14.423239 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_20/mlp/wi_1/kernel/m
I0512 23:45:14.423268 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_20/mlp/wi_1/kernel/v
I0512 23:45:14.423298 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_20/mlp/wi_1/kernel/v_col
I0512 23:45:14.423327 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_20/mlp/wi_1/kernel/v_row
I0512 23:45:14.423356 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_20/mlp/wo/kernel/m
I0512 23:45:14.423385 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_20/mlp/wo/kernel/v
I0512 23:45:14.423414 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_20/mlp/wo/kernel/v_col
I0512 23:45:14.423444 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_20/mlp/wo/kernel/v_row
I0512 23:45:14.423473 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_20/pre_mlp_layer_norm/scale/m
I0512 23:45:14.423503 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_20/pre_mlp_layer_norm/scale/v
I0512 23:45:14.423532 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_20/pre_mlp_layer_norm/scale/v_col
I0512 23:45:14.423578 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_20/pre_mlp_layer_norm/scale/v_row
I0512 23:45:14.423614 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_20/pre_self_attention_layer_norm/scale/m
I0512 23:45:14.423645 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_20/pre_self_attention_layer_norm/scale/v
I0512 23:45:14.423675 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_20/pre_self_attention_layer_norm/scale/v_col
I0512 23:45:14.423711 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_20/pre_self_attention_layer_norm/scale/v_row
I0512 23:45:14.423740 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_20/self_attention/key/kernel/m
I0512 23:45:14.423770 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_20/self_attention/key/kernel/v
I0512 23:45:14.423800 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_20/self_attention/key/kernel/v_col
I0512 23:45:14.423829 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_20/self_attention/key/kernel/v_row
I0512 23:45:14.423858 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_20/self_attention/out/kernel/m
I0512 23:45:14.423888 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_20/self_attention/out/kernel/v
I0512 23:45:14.423917 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_20/self_attention/out/kernel/v_col
I0512 23:45:14.423949 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_20/self_attention/out/kernel/v_row
I0512 23:45:14.423979 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_20/self_attention/query/kernel/m
I0512 23:45:14.424009 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_20/self_attention/query/kernel/v
I0512 23:45:14.424039 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_20/self_attention/query/kernel/v_col
I0512 23:45:14.424068 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_20/self_attention/query/kernel/v_row
I0512 23:45:14.424097 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_20/self_attention/value/kernel/m
I0512 23:45:14.424127 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_20/self_attention/value/kernel/v
I0512 23:45:14.424156 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_20/self_attention/value/kernel/v_col
I0512 23:45:14.424186 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_20/self_attention/value/kernel/v_row
I0512 23:45:14.424215 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_21/mlp/wi_0/kernel/m
I0512 23:45:14.424245 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_21/mlp/wi_0/kernel/v
I0512 23:45:14.424274 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_21/mlp/wi_0/kernel/v_col
I0512 23:45:14.424304 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_21/mlp/wi_0/kernel/v_row
I0512 23:45:14.424333 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_21/mlp/wi_1/kernel/m
I0512 23:45:14.424363 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_21/mlp/wi_1/kernel/v
I0512 23:45:14.424392 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_21/mlp/wi_1/kernel/v_col
I0512 23:45:14.424421 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_21/mlp/wi_1/kernel/v_row
I0512 23:45:14.424450 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_21/mlp/wo/kernel/m
I0512 23:45:14.424479 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_21/mlp/wo/kernel/v
I0512 23:45:14.424509 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_21/mlp/wo/kernel/v_col
I0512 23:45:14.424538 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_21/mlp/wo/kernel/v_row
I0512 23:45:14.424567 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_21/pre_mlp_layer_norm/scale/m
I0512 23:45:14.424597 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_21/pre_mlp_layer_norm/scale/v
I0512 23:45:14.424626 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_21/pre_mlp_layer_norm/scale/v_col
I0512 23:45:14.424656 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_21/pre_mlp_layer_norm/scale/v_row
I0512 23:45:14.424690 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_21/pre_self_attention_layer_norm/scale/m
I0512 23:45:14.424721 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_21/pre_self_attention_layer_norm/scale/v
I0512 23:45:14.424751 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_21/pre_self_attention_layer_norm/scale/v_col
I0512 23:45:14.424780 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_21/pre_self_attention_layer_norm/scale/v_row
I0512 23:45:14.424810 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_21/self_attention/key/kernel/m
I0512 23:45:14.424840 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_21/self_attention/key/kernel/v
I0512 23:45:14.424869 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_21/self_attention/key/kernel/v_col
I0512 23:45:14.424899 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_21/self_attention/key/kernel/v_row
I0512 23:45:14.424928 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_21/self_attention/out/kernel/m
I0512 23:45:14.424959 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_21/self_attention/out/kernel/v
I0512 23:45:14.424989 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_21/self_attention/out/kernel/v_col
I0512 23:45:14.425019 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_21/self_attention/out/kernel/v_row
I0512 23:45:14.425048 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_21/self_attention/query/kernel/m
I0512 23:45:14.425077 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_21/self_attention/query/kernel/v
I0512 23:45:14.425107 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_21/self_attention/query/kernel/v_col
I0512 23:45:14.425136 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_21/self_attention/query/kernel/v_row
I0512 23:45:14.425165 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_21/self_attention/value/kernel/m
I0512 23:45:14.425194 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_21/self_attention/value/kernel/v
I0512 23:45:14.425224 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_21/self_attention/value/kernel/v_col
I0512 23:45:14.425253 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_21/self_attention/value/kernel/v_row
I0512 23:45:14.425282 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_22/mlp/wi_0/kernel/m
I0512 23:45:14.425312 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_22/mlp/wi_0/kernel/v
I0512 23:45:14.425341 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_22/mlp/wi_0/kernel/v_col
I0512 23:45:14.425371 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_22/mlp/wi_0/kernel/v_row
I0512 23:45:14.425400 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_22/mlp/wi_1/kernel/m
I0512 23:45:14.425429 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_22/mlp/wi_1/kernel/v
I0512 23:45:14.425458 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_22/mlp/wi_1/kernel/v_col
I0512 23:45:14.425487 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_22/mlp/wi_1/kernel/v_row
I0512 23:45:14.425516 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_22/mlp/wo/kernel/m
I0512 23:45:14.425546 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_22/mlp/wo/kernel/v
I0512 23:45:14.425575 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_22/mlp/wo/kernel/v_col
I0512 23:45:14.425604 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_22/mlp/wo/kernel/v_row
I0512 23:45:14.425634 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_22/pre_mlp_layer_norm/scale/m
I0512 23:45:14.425663 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_22/pre_mlp_layer_norm/scale/v
I0512 23:45:14.425698 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_22/pre_mlp_layer_norm/scale/v_col
I0512 23:45:14.425729 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_22/pre_mlp_layer_norm/scale/v_row
I0512 23:45:14.425758 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_22/pre_self_attention_layer_norm/scale/m
I0512 23:45:14.425788 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_22/pre_self_attention_layer_norm/scale/v
I0512 23:45:14.425817 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_22/pre_self_attention_layer_norm/scale/v_col
I0512 23:45:14.425847 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_22/pre_self_attention_layer_norm/scale/v_row
I0512 23:45:14.425876 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_22/self_attention/key/kernel/m
I0512 23:45:14.425906 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_22/self_attention/key/kernel/v
I0512 23:45:14.425937 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_22/self_attention/key/kernel/v_col
I0512 23:45:14.425968 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_22/self_attention/key/kernel/v_row
I0512 23:45:14.425997 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_22/self_attention/out/kernel/m
I0512 23:45:14.426027 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_22/self_attention/out/kernel/v
I0512 23:45:14.426057 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_22/self_attention/out/kernel/v_col
I0512 23:45:14.426086 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_22/self_attention/out/kernel/v_row
I0512 23:45:14.426115 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_22/self_attention/query/kernel/m
I0512 23:45:14.426145 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_22/self_attention/query/kernel/v
I0512 23:45:14.426175 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_22/self_attention/query/kernel/v_col
I0512 23:45:14.426204 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_22/self_attention/query/kernel/v_row
I0512 23:45:14.426233 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_22/self_attention/value/kernel/m
I0512 23:45:14.426262 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_22/self_attention/value/kernel/v
I0512 23:45:14.426291 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_22/self_attention/value/kernel/v_col
I0512 23:45:14.426321 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_22/self_attention/value/kernel/v_row
I0512 23:45:14.426351 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/extra_mlp/wi_0/kernel/m
I0512 23:45:14.426380 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/extra_mlp/wi_0/kernel/v
I0512 23:45:14.426409 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/extra_mlp/wi_0/kernel/v_col
I0512 23:45:14.426439 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/extra_mlp/wi_0/kernel/v_row
I0512 23:45:14.426468 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/extra_mlp/wi_1/kernel/m
I0512 23:45:14.426497 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/extra_mlp/wi_1/kernel/v
I0512 23:45:14.426527 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/extra_mlp/wi_1/kernel/v_col
I0512 23:45:14.426556 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/extra_mlp/wi_1/kernel/v_row
I0512 23:45:14.426585 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/extra_mlp/wo/kernel/m
I0512 23:45:14.426614 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/extra_mlp/wo/kernel/v
I0512 23:45:14.426644 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/extra_mlp/wo/kernel/v_col
I0512 23:45:14.426673 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/extra_mlp/wo/kernel/v_row
I0512 23:45:14.426709 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/mlp/expert/wi_0/kernel/m
I0512 23:45:14.426739 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/mlp/expert/wi_0/kernel/v
I0512 23:45:14.426768 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/mlp/expert/wi_0/kernel/v_col
I0512 23:45:14.426798 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/mlp/expert/wi_0/kernel/v_row
I0512 23:45:14.426827 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/mlp/expert/wi_1/kernel/m
I0512 23:45:14.426857 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/mlp/expert/wi_1/kernel/v
I0512 23:45:14.426886 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/mlp/expert/wi_1/kernel/v_col
I0512 23:45:14.426915 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/mlp/expert/wi_1/kernel/v_row
I0512 23:45:14.426946 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/mlp/expert/wo/kernel/m
I0512 23:45:14.426976 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/mlp/expert/wo/kernel/v
I0512 23:45:14.427006 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/mlp/expert/wo/kernel/v_col
I0512 23:45:14.427035 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/mlp/expert/wo/kernel/v_row
I0512 23:45:14.427065 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/mlp/router/router_weights/w/kernel/m
I0512 23:45:14.427094 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/mlp/router/router_weights/w/kernel/v
I0512 23:45:14.427124 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/mlp/router/router_weights/w/kernel/v_col
I0512 23:45:14.427154 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/mlp/router/router_weights/w/kernel/v_row
I0512 23:45:14.427183 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/pre_extra_mlp_layer_norm/scale/m
I0512 23:45:14.427213 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/pre_extra_mlp_layer_norm/scale/v
I0512 23:45:14.427242 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/pre_extra_mlp_layer_norm/scale/v_col
I0512 23:45:14.427272 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/pre_extra_mlp_layer_norm/scale/v_row
I0512 23:45:14.427301 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/pre_mlp_layer_norm/scale/m
I0512 23:45:14.427330 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/pre_mlp_layer_norm/scale/v
I0512 23:45:14.427360 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/pre_mlp_layer_norm/scale/v_col
I0512 23:45:14.427390 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/pre_mlp_layer_norm/scale/v_row
I0512 23:45:14.427419 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/pre_self_attention_layer_norm/scale/m
I0512 23:45:14.427448 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/pre_self_attention_layer_norm/scale/v
I0512 23:45:14.427478 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/pre_self_attention_layer_norm/scale/v_col
I0512 23:45:14.427507 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/pre_self_attention_layer_norm/scale/v_row
I0512 23:45:14.427537 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/self_attention/key/kernel/m
I0512 23:45:14.427586 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/self_attention/key/kernel/v
I0512 23:45:14.427622 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/self_attention/key/kernel/v_col
I0512 23:45:14.427652 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/self_attention/key/kernel/v_row
I0512 23:45:14.427687 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/self_attention/out/kernel/m
I0512 23:45:14.427718 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/self_attention/out/kernel/v
I0512 23:45:14.427748 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/self_attention/out/kernel/v_col
I0512 23:45:14.427777 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/self_attention/out/kernel/v_row
I0512 23:45:14.427807 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/self_attention/query/kernel/m
I0512 23:45:14.427836 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/self_attention/query/kernel/v
I0512 23:45:14.427866 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/self_attention/query/kernel/v_col
I0512 23:45:14.427895 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/self_attention/query/kernel/v_row
I0512 23:45:14.427924 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/self_attention/value/kernel/m
I0512 23:45:14.427956 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/self_attention/value/kernel/v
I0512 23:45:14.427986 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/self_attention/value/kernel/v_col
I0512 23:45:14.428016 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/self_attention/value/kernel/v_row
I0512 23:45:14.428045 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_3/mlp/wi_0/kernel/m
I0512 23:45:14.428075 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_3/mlp/wi_0/kernel/v
I0512 23:45:14.428104 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_3/mlp/wi_0/kernel/v_col
I0512 23:45:14.428133 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_3/mlp/wi_0/kernel/v_row
I0512 23:45:14.428163 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_3/mlp/wi_1/kernel/m
I0512 23:45:14.428192 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_3/mlp/wi_1/kernel/v
I0512 23:45:14.428221 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_3/mlp/wi_1/kernel/v_col
I0512 23:45:14.428250 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_3/mlp/wi_1/kernel/v_row
I0512 23:45:14.428279 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_3/mlp/wo/kernel/m
I0512 23:45:14.428308 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_3/mlp/wo/kernel/v
I0512 23:45:14.428338 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_3/mlp/wo/kernel/v_col
I0512 23:45:14.428367 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_3/mlp/wo/kernel/v_row
I0512 23:45:14.428396 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_3/pre_mlp_layer_norm/scale/m
I0512 23:45:14.428425 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_3/pre_mlp_layer_norm/scale/v
I0512 23:45:14.428454 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_3/pre_mlp_layer_norm/scale/v_col
I0512 23:45:14.428484 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_3/pre_mlp_layer_norm/scale/v_row
I0512 23:45:14.428513 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_3/pre_self_attention_layer_norm/scale/m
I0512 23:45:14.428543 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_3/pre_self_attention_layer_norm/scale/v
I0512 23:45:14.428572 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_3/pre_self_attention_layer_norm/scale/v_col
I0512 23:45:14.428602 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_3/pre_self_attention_layer_norm/scale/v_row
I0512 23:45:14.428631 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_3/self_attention/key/kernel/m
I0512 23:45:14.428661 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_3/self_attention/key/kernel/v
I0512 23:45:14.428696 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_3/self_attention/key/kernel/v_col
I0512 23:45:14.428727 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_3/self_attention/key/kernel/v_row
I0512 23:45:14.428756 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_3/self_attention/out/kernel/m
I0512 23:45:14.428785 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_3/self_attention/out/kernel/v
I0512 23:45:14.428815 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_3/self_attention/out/kernel/v_col
I0512 23:45:14.428844 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_3/self_attention/out/kernel/v_row
I0512 23:45:14.428874 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_3/self_attention/query/kernel/m
I0512 23:45:14.428903 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_3/self_attention/query/kernel/v
I0512 23:45:14.428933 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_3/self_attention/query/kernel/v_col
I0512 23:45:14.428965 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_3/self_attention/query/kernel/v_row
I0512 23:45:14.428995 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_3/self_attention/value/kernel/m
I0512 23:45:14.429024 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_3/self_attention/value/kernel/v
I0512 23:45:14.429054 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_3/self_attention/value/kernel/v_col
I0512 23:45:14.429083 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_3/self_attention/value/kernel/v_row
I0512 23:45:14.429112 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_4/mlp/wi_0/kernel/m
I0512 23:45:14.429141 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_4/mlp/wi_0/kernel/v
I0512 23:45:14.429171 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_4/mlp/wi_0/kernel/v_col
I0512 23:45:14.429201 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_4/mlp/wi_0/kernel/v_row
I0512 23:45:14.429230 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_4/mlp/wi_1/kernel/m
I0512 23:45:14.429260 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_4/mlp/wi_1/kernel/v
I0512 23:45:14.429289 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_4/mlp/wi_1/kernel/v_col
I0512 23:45:14.429318 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_4/mlp/wi_1/kernel/v_row
I0512 23:45:14.429348 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_4/mlp/wo/kernel/m
I0512 23:45:14.429377 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_4/mlp/wo/kernel/v
I0512 23:45:14.429406 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_4/mlp/wo/kernel/v_col
I0512 23:45:14.429436 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_4/mlp/wo/kernel/v_row
I0512 23:45:14.429465 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_4/pre_mlp_layer_norm/scale/m
I0512 23:45:14.429495 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_4/pre_mlp_layer_norm/scale/v
I0512 23:45:14.429524 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_4/pre_mlp_layer_norm/scale/v_col
I0512 23:45:14.429554 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_4/pre_mlp_layer_norm/scale/v_row
I0512 23:45:14.429583 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_4/pre_self_attention_layer_norm/scale/m
I0512 23:45:14.429613 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_4/pre_self_attention_layer_norm/scale/v
I0512 23:45:14.429642 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_4/pre_self_attention_layer_norm/scale/v_col
I0512 23:45:14.429672 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_4/pre_self_attention_layer_norm/scale/v_row
I0512 23:45:14.429708 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_4/self_attention/key/kernel/m
I0512 23:45:14.429738 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_4/self_attention/key/kernel/v
I0512 23:45:14.429768 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_4/self_attention/key/kernel/v_col
I0512 23:45:14.429797 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_4/self_attention/key/kernel/v_row
I0512 23:45:14.429826 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_4/self_attention/out/kernel/m
I0512 23:45:14.429856 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_4/self_attention/out/kernel/v
I0512 23:45:14.429885 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_4/self_attention/out/kernel/v_col
I0512 23:45:14.429914 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_4/self_attention/out/kernel/v_row
I0512 23:45:14.429945 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_4/self_attention/query/kernel/m
I0512 23:45:14.429976 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_4/self_attention/query/kernel/v
I0512 23:45:14.430005 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_4/self_attention/query/kernel/v_col
I0512 23:45:14.430034 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_4/self_attention/query/kernel/v_row
I0512 23:45:14.430064 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_4/self_attention/value/kernel/m
I0512 23:45:14.430093 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_4/self_attention/value/kernel/v
I0512 23:45:14.430123 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_4/self_attention/value/kernel/v_col
I0512 23:45:14.430152 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_4/self_attention/value/kernel/v_row
I0512 23:45:14.430181 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/extra_mlp/wi_0/kernel/m
I0512 23:45:14.430211 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/extra_mlp/wi_0/kernel/v
I0512 23:45:14.430240 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/extra_mlp/wi_0/kernel/v_col
I0512 23:45:14.430269 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/extra_mlp/wi_0/kernel/v_row
I0512 23:45:14.430299 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/extra_mlp/wi_1/kernel/m
I0512 23:45:14.430328 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/extra_mlp/wi_1/kernel/v
I0512 23:45:14.430356 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/extra_mlp/wi_1/kernel/v_col
I0512 23:45:14.430385 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/extra_mlp/wi_1/kernel/v_row
I0512 23:45:14.430414 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/extra_mlp/wo/kernel/m
I0512 23:45:14.430444 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/extra_mlp/wo/kernel/v
I0512 23:45:14.430472 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/extra_mlp/wo/kernel/v_col
I0512 23:45:14.430502 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/extra_mlp/wo/kernel/v_row
I0512 23:45:14.430531 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/mlp/expert/wi_0/kernel/m
I0512 23:45:14.430560 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/mlp/expert/wi_0/kernel/v
I0512 23:45:14.430590 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/mlp/expert/wi_0/kernel/v_col
I0512 23:45:14.430619 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/mlp/expert/wi_0/kernel/v_row
I0512 23:45:14.430649 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/mlp/expert/wi_1/kernel/m
I0512 23:45:14.430683 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/mlp/expert/wi_1/kernel/v
I0512 23:45:14.430714 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/mlp/expert/wi_1/kernel/v_col
I0512 23:45:14.430743 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/mlp/expert/wi_1/kernel/v_row
I0512 23:45:14.430773 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/mlp/expert/wo/kernel/m
I0512 23:45:14.430803 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/mlp/expert/wo/kernel/v
I0512 23:45:14.430832 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/mlp/expert/wo/kernel/v_col
I0512 23:45:14.430861 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/mlp/expert/wo/kernel/v_row
I0512 23:45:14.430891 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/mlp/router/router_weights/w/kernel/m
I0512 23:45:14.430921 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/mlp/router/router_weights/w/kernel/v
I0512 23:45:14.430952 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/mlp/router/router_weights/w/kernel/v_col
I0512 23:45:14.430982 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/mlp/router/router_weights/w/kernel/v_row
I0512 23:45:14.431012 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/pre_extra_mlp_layer_norm/scale/m
I0512 23:45:14.431042 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/pre_extra_mlp_layer_norm/scale/v
I0512 23:45:14.431071 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/pre_extra_mlp_layer_norm/scale/v_col
I0512 23:45:14.431101 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/pre_extra_mlp_layer_norm/scale/v_row
I0512 23:45:14.431130 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/pre_mlp_layer_norm/scale/m
I0512 23:45:14.431159 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/pre_mlp_layer_norm/scale/v
I0512 23:45:14.431189 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/pre_mlp_layer_norm/scale/v_col
I0512 23:45:14.431218 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/pre_mlp_layer_norm/scale/v_row
I0512 23:45:14.431248 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/pre_self_attention_layer_norm/scale/m
I0512 23:45:14.431277 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/pre_self_attention_layer_norm/scale/v
I0512 23:45:14.431307 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/pre_self_attention_layer_norm/scale/v_col
I0512 23:45:14.431336 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/pre_self_attention_layer_norm/scale/v_row
I0512 23:45:14.431365 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/self_attention/key/kernel/m
I0512 23:45:14.431395 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/self_attention/key/kernel/v
I0512 23:45:14.431425 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/self_attention/key/kernel/v_col
I0512 23:45:14.431454 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/self_attention/key/kernel/v_row
I0512 23:45:14.431483 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/self_attention/out/kernel/m
I0512 23:45:14.431513 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/self_attention/out/kernel/v
I0512 23:45:14.431542 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/self_attention/out/kernel/v_col
I0512 23:45:14.431593 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/self_attention/out/kernel/v_row
I0512 23:45:14.431627 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/self_attention/query/kernel/m
I0512 23:45:14.431657 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/self_attention/query/kernel/v
I0512 23:45:14.431692 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/self_attention/query/kernel/v_col
I0512 23:45:14.431722 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/self_attention/query/kernel/v_row
I0512 23:45:14.431752 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/self_attention/value/kernel/m
I0512 23:45:14.431781 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/self_attention/value/kernel/v
I0512 23:45:14.431811 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/self_attention/value/kernel/v_col
I0512 23:45:14.431840 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/self_attention/value/kernel/v_row
I0512 23:45:14.431870 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_6/mlp/wi_0/kernel/m
I0512 23:45:14.431899 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_6/mlp/wi_0/kernel/v
I0512 23:45:14.431929 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_6/mlp/wi_0/kernel/v_col
I0512 23:45:14.431961 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_6/mlp/wi_0/kernel/v_row
I0512 23:45:14.431991 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_6/mlp/wi_1/kernel/m
I0512 23:45:14.432020 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_6/mlp/wi_1/kernel/v
I0512 23:45:14.432049 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_6/mlp/wi_1/kernel/v_col
I0512 23:45:14.432078 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_6/mlp/wi_1/kernel/v_row
I0512 23:45:14.432108 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_6/mlp/wo/kernel/m
I0512 23:45:14.432137 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_6/mlp/wo/kernel/v
I0512 23:45:14.432166 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_6/mlp/wo/kernel/v_col
I0512 23:45:14.432195 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_6/mlp/wo/kernel/v_row
I0512 23:45:14.432224 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_6/pre_mlp_layer_norm/scale/m
I0512 23:45:14.432253 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_6/pre_mlp_layer_norm/scale/v
I0512 23:45:14.432283 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_6/pre_mlp_layer_norm/scale/v_col
I0512 23:45:14.432313 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_6/pre_mlp_layer_norm/scale/v_row
I0512 23:45:14.432342 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_6/pre_self_attention_layer_norm/scale/m
I0512 23:45:14.432371 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_6/pre_self_attention_layer_norm/scale/v
I0512 23:45:14.432400 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_6/pre_self_attention_layer_norm/scale/v_col
I0512 23:45:14.432430 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_6/pre_self_attention_layer_norm/scale/v_row
I0512 23:45:14.432460 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_6/self_attention/key/kernel/m
I0512 23:45:14.432489 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_6/self_attention/key/kernel/v
I0512 23:45:14.432519 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_6/self_attention/key/kernel/v_col
I0512 23:45:14.432548 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_6/self_attention/key/kernel/v_row
I0512 23:45:14.432578 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_6/self_attention/out/kernel/m
I0512 23:45:14.432607 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_6/self_attention/out/kernel/v
I0512 23:45:14.432636 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_6/self_attention/out/kernel/v_col
I0512 23:45:14.432665 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_6/self_attention/out/kernel/v_row
I0512 23:45:14.432700 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_6/self_attention/query/kernel/m
I0512 23:45:14.432731 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_6/self_attention/query/kernel/v
I0512 23:45:14.432760 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_6/self_attention/query/kernel/v_col
I0512 23:45:14.432789 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_6/self_attention/query/kernel/v_row
I0512 23:45:14.432818 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_6/self_attention/value/kernel/m
I0512 23:45:14.432847 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_6/self_attention/value/kernel/v
I0512 23:45:14.432877 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_6/self_attention/value/kernel/v_col
I0512 23:45:14.432906 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_6/self_attention/value/kernel/v_row
I0512 23:45:14.432936 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_7/mlp/wi_0/kernel/m
I0512 23:45:14.432968 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_7/mlp/wi_0/kernel/v
I0512 23:45:14.432997 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_7/mlp/wi_0/kernel/v_col
I0512 23:45:14.433027 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_7/mlp/wi_0/kernel/v_row
I0512 23:45:14.433056 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_7/mlp/wi_1/kernel/m
I0512 23:45:14.433085 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_7/mlp/wi_1/kernel/v
I0512 23:45:14.433114 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_7/mlp/wi_1/kernel/v_col
I0512 23:45:14.433143 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_7/mlp/wi_1/kernel/v_row
I0512 23:45:14.433172 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_7/mlp/wo/kernel/m
I0512 23:45:14.433202 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_7/mlp/wo/kernel/v
I0512 23:45:14.433231 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_7/mlp/wo/kernel/v_col
I0512 23:45:14.433261 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_7/mlp/wo/kernel/v_row
I0512 23:45:14.433290 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_7/pre_mlp_layer_norm/scale/m
I0512 23:45:14.433319 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_7/pre_mlp_layer_norm/scale/v
I0512 23:45:14.433349 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_7/pre_mlp_layer_norm/scale/v_col
I0512 23:45:14.433378 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_7/pre_mlp_layer_norm/scale/v_row
I0512 23:45:14.433408 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_7/pre_self_attention_layer_norm/scale/m
I0512 23:45:14.433437 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_7/pre_self_attention_layer_norm/scale/v
I0512 23:45:14.433467 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_7/pre_self_attention_layer_norm/scale/v_col
I0512 23:45:14.433496 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_7/pre_self_attention_layer_norm/scale/v_row
I0512 23:45:14.433526 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_7/self_attention/key/kernel/m
I0512 23:45:14.433555 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_7/self_attention/key/kernel/v
I0512 23:45:14.433585 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_7/self_attention/key/kernel/v_col
I0512 23:45:14.433614 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_7/self_attention/key/kernel/v_row
I0512 23:45:14.433643 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_7/self_attention/out/kernel/m
I0512 23:45:14.433672 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_7/self_attention/out/kernel/v
I0512 23:45:14.433712 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_7/self_attention/out/kernel/v_col
I0512 23:45:14.433742 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_7/self_attention/out/kernel/v_row
I0512 23:45:14.433772 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_7/self_attention/query/kernel/m
I0512 23:45:14.433801 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_7/self_attention/query/kernel/v
I0512 23:45:14.433830 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_7/self_attention/query/kernel/v_col
I0512 23:45:14.433859 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_7/self_attention/query/kernel/v_row
I0512 23:45:14.433888 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_7/self_attention/value/kernel/m
I0512 23:45:14.433917 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_7/self_attention/value/kernel/v
I0512 23:45:14.433948 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_7/self_attention/value/kernel/v_col
I0512 23:45:14.433978 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_7/self_attention/value/kernel/v_row
I0512 23:45:14.434008 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_8/mlp/wi_0/kernel/m
I0512 23:45:14.434037 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_8/mlp/wi_0/kernel/v
I0512 23:45:14.434066 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_8/mlp/wi_0/kernel/v_col
I0512 23:45:14.434096 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_8/mlp/wi_0/kernel/v_row
I0512 23:45:14.434125 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_8/mlp/wi_1/kernel/m
I0512 23:45:14.434154 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_8/mlp/wi_1/kernel/v
I0512 23:45:14.434183 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_8/mlp/wi_1/kernel/v_col
I0512 23:45:14.434212 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_8/mlp/wi_1/kernel/v_row
I0512 23:45:14.434242 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_8/mlp/wo/kernel/m
I0512 23:45:14.434271 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_8/mlp/wo/kernel/v
I0512 23:45:14.434299 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_8/mlp/wo/kernel/v_col
I0512 23:45:14.434328 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_8/mlp/wo/kernel/v_row
I0512 23:45:14.434358 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_8/pre_mlp_layer_norm/scale/m
I0512 23:45:14.434387 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_8/pre_mlp_layer_norm/scale/v
I0512 23:45:14.434416 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_8/pre_mlp_layer_norm/scale/v_col
I0512 23:45:14.434445 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_8/pre_mlp_layer_norm/scale/v_row
I0512 23:45:14.434475 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_8/pre_self_attention_layer_norm/scale/m
I0512 23:45:14.434504 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_8/pre_self_attention_layer_norm/scale/v
I0512 23:45:14.434533 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_8/pre_self_attention_layer_norm/scale/v_col
I0512 23:45:14.434563 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_8/pre_self_attention_layer_norm/scale/v_row
I0512 23:45:14.434592 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_8/self_attention/key/kernel/m
I0512 23:45:14.434621 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_8/self_attention/key/kernel/v
I0512 23:45:14.434651 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_8/self_attention/key/kernel/v_col
I0512 23:45:14.434684 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_8/self_attention/key/kernel/v_row
I0512 23:45:14.434715 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_8/self_attention/out/kernel/m
I0512 23:45:14.434744 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_8/self_attention/out/kernel/v
I0512 23:45:14.434774 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_8/self_attention/out/kernel/v_col
I0512 23:45:14.434803 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_8/self_attention/out/kernel/v_row
I0512 23:45:14.434833 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_8/self_attention/query/kernel/m
I0512 23:45:14.434862 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_8/self_attention/query/kernel/v
I0512 23:45:14.434892 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_8/self_attention/query/kernel/v_col
I0512 23:45:14.434921 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_8/self_attention/query/kernel/v_row
I0512 23:45:14.434952 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_8/self_attention/value/kernel/m
I0512 23:45:14.434982 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_8/self_attention/value/kernel/v
I0512 23:45:14.435011 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_8/self_attention/value/kernel/v_col
I0512 23:45:14.435040 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_8/self_attention/value/kernel/v_row
I0512 23:45:14.435070 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_9/mlp/wi_0/kernel/m
I0512 23:45:14.435131 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_9/mlp/wi_0/kernel/v
I0512 23:45:14.435162 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_9/mlp/wi_0/kernel/v_col
I0512 23:45:14.435192 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_9/mlp/wi_0/kernel/v_row
I0512 23:45:14.435221 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_9/mlp/wi_1/kernel/m
I0512 23:45:14.435250 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_9/mlp/wi_1/kernel/v
I0512 23:45:14.435280 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_9/mlp/wi_1/kernel/v_col
I0512 23:45:14.435309 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_9/mlp/wi_1/kernel/v_row
I0512 23:45:14.435338 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_9/mlp/wo/kernel/m
I0512 23:45:14.435367 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_9/mlp/wo/kernel/v
I0512 23:45:14.435396 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_9/mlp/wo/kernel/v_col
I0512 23:45:14.435425 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_9/mlp/wo/kernel/v_row
I0512 23:45:14.435455 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_9/pre_mlp_layer_norm/scale/m
I0512 23:45:14.435484 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_9/pre_mlp_layer_norm/scale/v
I0512 23:45:14.435513 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_9/pre_mlp_layer_norm/scale/v_col
I0512 23:45:14.435543 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_9/pre_mlp_layer_norm/scale/v_row
I0512 23:45:14.435592 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_9/pre_self_attention_layer_norm/scale/m
I0512 23:45:14.435625 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_9/pre_self_attention_layer_norm/scale/v
I0512 23:45:14.435655 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_9/pre_self_attention_layer_norm/scale/v_col
I0512 23:45:14.435690 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_9/pre_self_attention_layer_norm/scale/v_row
I0512 23:45:14.435721 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_9/self_attention/key/kernel/m
I0512 23:45:14.435751 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_9/self_attention/key/kernel/v
I0512 23:45:14.435781 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_9/self_attention/key/kernel/v_col
I0512 23:45:14.435810 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_9/self_attention/key/kernel/v_row
I0512 23:45:14.435839 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_9/self_attention/out/kernel/m
I0512 23:45:14.435868 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_9/self_attention/out/kernel/v
I0512 23:45:14.435898 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_9/self_attention/out/kernel/v_col
I0512 23:45:14.435927 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_9/self_attention/out/kernel/v_row
I0512 23:45:14.435959 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_9/self_attention/query/kernel/m
I0512 23:45:14.435989 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_9/self_attention/query/kernel/v
I0512 23:45:14.436018 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_9/self_attention/query/kernel/v_col
I0512 23:45:14.436048 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_9/self_attention/query/kernel/v_row
I0512 23:45:14.436077 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_9/self_attention/value/kernel/m
I0512 23:45:14.436106 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_9/self_attention/value/kernel/v
I0512 23:45:14.436135 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_9/self_attention/value/kernel/v_col
I0512 23:45:14.436164 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_9/self_attention/value/kernel/v_row
I0512 23:45:14.436193 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/logits_dense/kernel/m
I0512 23:45:14.436223 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/logits_dense/kernel/v
I0512 23:45:14.436252 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/logits_dense/kernel/v_col
I0512 23:45:14.436281 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/logits_dense/kernel/v_row
I0512 23:45:14.436310 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/token_embedder/embedding/m
I0512 23:45:14.436339 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/token_embedder/embedding/v
I0512 23:45:14.436368 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/token_embedder/embedding/v_col
I0512 23:45:14.436398 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/param_states/token_embedder/embedding/v_row
I0512 23:45:14.436427 140643590531072 checkpoints.py:1100] Restoring key from ckpt: state/step
I0512 23:45:14.436456 140643590531072 checkpoints.py:1100] Restoring key from ckpt: target/decoder/decoder_norm/scale
I0512 23:45:14.436487 140643590531072 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_0/mlp/wi_0/kernel
I0512 23:45:14.436516 140643590531072 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_0/mlp/wi_1/kernel
I0512 23:45:14.436545 140643590531072 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_0/mlp/wo/kernel
I0512 23:45:14.436574 140643590531072 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_0/pre_mlp_layer_norm/scale
I0512 23:45:14.436604 140643590531072 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_0/pre_self_attention_layer_norm/scale
I0512 23:45:14.436633 140643590531072 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_0/self_attention/key/kernel
I0512 23:45:14.436662 140643590531072 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_0/self_attention/out/kernel
I0512 23:45:14.436697 140643590531072 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_0/self_attention/query/kernel
I0512 23:45:14.436727 140643590531072 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_0/self_attention/value/kernel
I0512 23:45:14.436757 140643590531072 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_1/mlp/wi_0/kernel
I0512 23:45:14.436786 140643590531072 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_1/mlp/wi_1/kernel
I0512 23:45:14.436814 140643590531072 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_1/mlp/wo/kernel
I0512 23:45:14.436844 140643590531072 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_1/pre_mlp_layer_norm/scale
I0512 23:45:14.436872 140643590531072 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_1/pre_self_attention_layer_norm/scale
I0512 23:45:14.436902 140643590531072 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_1/self_attention/key/kernel
I0512 23:45:14.436932 140643590531072 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_1/self_attention/out/kernel
I0512 23:45:14.436963 140643590531072 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_1/self_attention/query/kernel
I0512 23:45:14.436992 140643590531072 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_1/self_attention/value/kernel
I0512 23:45:14.437021 140643590531072 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_10/mlp/wi_0/kernel
I0512 23:45:14.437051 140643590531072 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_10/mlp/wi_1/kernel
I0512 23:45:14.437080 140643590531072 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_10/mlp/wo/kernel
I0512 23:45:14.437109 140643590531072 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_10/pre_mlp_layer_norm/scale
I0512 23:45:14.437138 140643590531072 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_10/pre_self_attention_layer_norm/scale
I0512 23:45:14.437167 140643590531072 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_10/self_attention/key/kernel
I0512 23:45:14.437196 140643590531072 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_10/self_attention/out/kernel
I0512 23:45:14.437226 140643590531072 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_10/self_attention/query/kernel
I0512 23:45:14.437255 140643590531072 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_10/self_attention/value/kernel
I0512 23:45:14.437283 140643590531072 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_11/extra_mlp/wi_0/kernel
I0512 23:45:14.437313 140643590531072 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_11/extra_mlp/wi_1/kernel
I0512 23:45:14.437341 140643590531072 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_11/extra_mlp/wo/kernel
I0512 23:45:14.437371 140643590531072 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_11/mlp/expert/wi_0/kernel
I0512 23:45:14.437400 140643590531072 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_11/mlp/expert/wi_1/kernel
I0512 23:45:14.437429 140643590531072 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_11/mlp/expert/wo/kernel
I0512 23:45:14.437458 140643590531072 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_11/mlp/router/router_weights/w/kernel
I0512 23:45:14.437488 140643590531072 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_11/pre_extra_mlp_layer_norm/scale
I0512 23:45:14.437517 140643590531072 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_11/pre_mlp_layer_norm/scale
I0512 23:45:14.437546 140643590531072 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_11/pre_self_attention_layer_norm/scale
I0512 23:45:14.437575 140643590531072 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_11/self_attention/key/kernel
I0512 23:45:14.437604 140643590531072 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_11/self_attention/out/kernel
I0512 23:45:14.437633 140643590531072 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_11/self_attention/query/kernel
I0512 23:45:14.437663 140643590531072 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_11/self_attention/value/kernel
I0512 23:45:14.437698 140643590531072 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_12/mlp/wi_0/kernel
I0512 23:45:14.437729 140643590531072 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_12/mlp/wi_1/kernel
I0512 23:45:14.437758 140643590531072 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_12/mlp/wo/kernel
I0512 23:45:14.437787 140643590531072 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_12/pre_mlp_layer_norm/scale
I0512 23:45:14.437816 140643590531072 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_12/pre_self_attention_layer_norm/scale
I0512 23:45:14.437845 140643590531072 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_12/self_attention/key/kernel
I0512 23:45:14.437875 140643590531072 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_12/self_attention/out/kernel
I0512 23:45:14.437903 140643590531072 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_12/self_attention/query/kernel
I0512 23:45:14.437933 140643590531072 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_12/self_attention/value/kernel
I0512 23:45:14.437964 140643590531072 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_13/mlp/wi_0/kernel
I0512 23:45:14.437994 140643590531072 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_13/mlp/wi_1/kernel
I0512 23:45:14.438024 140643590531072 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_13/mlp/wo/kernel
I0512 23:45:14.438053 140643590531072 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_13/pre_mlp_layer_norm/scale
I0512 23:45:14.438082 140643590531072 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_13/pre_self_attention_layer_norm/scale
I0512 23:45:14.438111 140643590531072 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_13/self_attention/key/kernel
I0512 23:45:14.438140 140643590531072 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_13/self_attention/out/kernel
I0512 23:45:14.438169 140643590531072 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_13/self_attention/query/kernel
I0512 23:45:14.438198 140643590531072 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_13/self_attention/value/kernel
I0512 23:45:14.438227 140643590531072 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_14/mlp/wi_0/kernel
I0512 23:45:14.438257 140643590531072 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_14/mlp/wi_1/kernel
I0512 23:45:14.438286 140643590531072 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_14/mlp/wo/kernel
I0512 23:45:14.438316 140643590531072 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_14/pre_mlp_layer_norm/scale
I0512 23:45:14.438345 140643590531072 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_14/pre_self_attention_layer_norm/scale
I0512 23:45:14.438374 140643590531072 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_14/self_attention/key/kernel
I0512 23:45:14.438404 140643590531072 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_14/self_attention/out/kernel
I0512 23:45:14.438433 140643590531072 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_14/self_attention/query/kernel
I0512 23:45:14.438462 140643590531072 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_14/self_attention/value/kernel
I0512 23:45:14.438491 140643590531072 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_15/mlp/wi_0/kernel
I0512 23:45:14.438521 140643590531072 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_15/mlp/wi_1/kernel
I0512 23:45:14.438550 140643590531072 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_15/mlp/wo/kernel
I0512 23:45:14.438579 140643590531072 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_15/pre_mlp_layer_norm/scale
I0512 23:45:14.438608 140643590531072 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_15/pre_self_attention_layer_norm/scale
I0512 23:45:14.438637 140643590531072 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_15/self_attention/key/kernel
I0512 23:45:14.438667 140643590531072 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_15/self_attention/out/kernel
I0512 23:45:14.438701 140643590531072 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_15/self_attention/query/kernel
I0512 23:45:14.438732 140643590531072 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_15/self_attention/value/kernel
I0512 23:45:14.438761 140643590531072 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_16/mlp/wi_0/kernel
I0512 23:45:14.438790 140643590531072 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_16/mlp/wi_1/kernel
I0512 23:45:14.438820 140643590531072 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_16/mlp/wo/kernel
I0512 23:45:14.438849 140643590531072 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_16/pre_mlp_layer_norm/scale
I0512 23:45:14.438878 140643590531072 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_16/pre_self_attention_layer_norm/scale
I0512 23:45:14.438907 140643590531072 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_16/self_attention/key/kernel
I0512 23:45:14.438938 140643590531072 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_16/self_attention/out/kernel
I0512 23:45:14.438968 140643590531072 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_16/self_attention/query/kernel
I0512 23:45:14.438998 140643590531072 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_16/self_attention/value/kernel
I0512 23:45:14.439028 140643590531072 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_17/extra_mlp/wi_0/kernel
I0512 23:45:14.439057 140643590531072 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_17/extra_mlp/wi_1/kernel
I0512 23:45:14.439086 140643590531072 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_17/extra_mlp/wo/kernel
I0512 23:45:14.439115 140643590531072 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_17/mlp/expert/wi_0/kernel
I0512 23:45:14.439144 140643590531072 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_17/mlp/expert/wi_1/kernel
I0512 23:45:14.439173 140643590531072 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_17/mlp/expert/wo/kernel
I0512 23:45:14.439202 140643590531072 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_17/mlp/router/router_weights/w/kernel
I0512 23:45:14.439232 140643590531072 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_17/pre_extra_mlp_layer_norm/scale
I0512 23:45:14.439260 140643590531072 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_17/pre_mlp_layer_norm/scale
I0512 23:45:14.439290 140643590531072 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_17/pre_self_attention_layer_norm/scale
I0512 23:45:14.439319 140643590531072 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_17/self_attention/key/kernel
I0512 23:45:14.439348 140643590531072 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_17/self_attention/out/kernel
I0512 23:45:14.439378 140643590531072 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_17/self_attention/query/kernel
I0512 23:45:14.439407 140643590531072 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_17/self_attention/value/kernel
I0512 23:45:14.439437 140643590531072 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_18/mlp/wi_0/kernel
I0512 23:45:14.439465 140643590531072 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_18/mlp/wi_1/kernel
I0512 23:45:14.439494 140643590531072 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_18/mlp/wo/kernel
I0512 23:45:14.439524 140643590531072 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_18/pre_mlp_layer_norm/scale
I0512 23:45:14.439553 140643590531072 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_18/pre_self_attention_layer_norm/scale
I0512 23:45:14.439609 140643590531072 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_18/self_attention/key/kernel
I0512 23:45:14.439641 140643590531072 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_18/self_attention/out/kernel
I0512 23:45:14.439671 140643590531072 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_18/self_attention/query/kernel
I0512 23:45:14.439709 140643590531072 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_18/self_attention/value/kernel
I0512 23:45:14.439739 140643590531072 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_19/mlp/wi_0/kernel
I0512 23:45:14.439769 140643590531072 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_19/mlp/wi_1/kernel
I0512 23:45:14.439798 140643590531072 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_19/mlp/wo/kernel
I0512 23:45:14.439827 140643590531072 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_19/pre_mlp_layer_norm/scale
I0512 23:45:14.439861 140643590531072 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_19/pre_self_attention_layer_norm/scale
I0512 23:45:14.439890 140643590531072 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_19/self_attention/key/kernel
I0512 23:45:14.439920 140643590531072 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_19/self_attention/out/kernel
I0512 23:45:14.439952 140643590531072 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_19/self_attention/query/kernel
I0512 23:45:14.439982 140643590531072 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_19/self_attention/value/kernel
I0512 23:45:14.440011 140643590531072 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_2/mlp/wi_0/kernel
I0512 23:45:14.440041 140643590531072 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_2/mlp/wi_1/kernel
I0512 23:45:14.440070 140643590531072 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_2/mlp/wo/kernel
I0512 23:45:14.440099 140643590531072 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_2/pre_mlp_layer_norm/scale
I0512 23:45:14.440128 140643590531072 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_2/pre_self_attention_layer_norm/scale
I0512 23:45:14.440157 140643590531072 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_2/self_attention/key/kernel
I0512 23:45:14.440186 140643590531072 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_2/self_attention/out/kernel
I0512 23:45:14.440216 140643590531072 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_2/self_attention/query/kernel
I0512 23:45:14.440244 140643590531072 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_2/self_attention/value/kernel
I0512 23:45:14.440274 140643590531072 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_20/mlp/wi_0/kernel
I0512 23:45:14.440303 140643590531072 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_20/mlp/wi_1/kernel
I0512 23:45:14.440332 140643590531072 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_20/mlp/wo/kernel
I0512 23:45:14.440361 140643590531072 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_20/pre_mlp_layer_norm/scale
I0512 23:45:14.440390 140643590531072 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_20/pre_self_attention_layer_norm/scale
I0512 23:45:14.440420 140643590531072 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_20/self_attention/key/kernel
I0512 23:45:14.440449 140643590531072 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_20/self_attention/out/kernel
I0512 23:45:14.440478 140643590531072 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_20/self_attention/query/kernel
I0512 23:45:14.440508 140643590531072 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_20/self_attention/value/kernel
I0512 23:45:14.440537 140643590531072 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_21/mlp/wi_0/kernel
I0512 23:45:14.440566 140643590531072 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_21/mlp/wi_1/kernel
I0512 23:45:14.440595 140643590531072 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_21/mlp/wo/kernel
I0512 23:45:14.440624 140643590531072 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_21/pre_mlp_layer_norm/scale
I0512 23:45:14.440653 140643590531072 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_21/pre_self_attention_layer_norm/scale
I0512 23:45:14.440689 140643590531072 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_21/self_attention/key/kernel
I0512 23:45:14.440720 140643590531072 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_21/self_attention/out/kernel
I0512 23:45:14.440749 140643590531072 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_21/self_attention/query/kernel
I0512 23:45:14.440778 140643590531072 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_21/self_attention/value/kernel
I0512 23:45:14.440807 140643590531072 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_22/mlp/wi_0/kernel
I0512 23:45:14.440836 140643590531072 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_22/mlp/wi_1/kernel
I0512 23:45:14.440866 140643590531072 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_22/mlp/wo/kernel
I0512 23:45:14.440895 140643590531072 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_22/pre_mlp_layer_norm/scale
I0512 23:45:14.440924 140643590531072 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_22/pre_self_attention_layer_norm/scale
I0512 23:45:14.440955 140643590531072 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_22/self_attention/key/kernel
I0512 23:45:14.440985 140643590531072 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_22/self_attention/out/kernel
I0512 23:45:14.441015 140643590531072 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_22/self_attention/query/kernel
I0512 23:45:14.441044 140643590531072 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_22/self_attention/value/kernel
I0512 23:45:14.441073 140643590531072 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_23/extra_mlp/wi_0/kernel
I0512 23:45:14.441102 140643590531072 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_23/extra_mlp/wi_1/kernel
I0512 23:45:14.441132 140643590531072 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_23/extra_mlp/wo/kernel
I0512 23:45:14.441161 140643590531072 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_23/mlp/expert/wi_0/kernel
I0512 23:45:14.441190 140643590531072 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_23/mlp/expert/wi_1/kernel
I0512 23:45:14.441219 140643590531072 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_23/mlp/expert/wo/kernel
I0512 23:45:14.441249 140643590531072 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_23/mlp/router/router_weights/w/kernel
I0512 23:45:14.441278 140643590531072 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_23/pre_extra_mlp_layer_norm/scale
I0512 23:45:14.441307 140643590531072 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_23/pre_mlp_layer_norm/scale
I0512 23:45:14.441336 140643590531072 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_23/pre_self_attention_layer_norm/scale
I0512 23:45:14.441365 140643590531072 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_23/self_attention/key/kernel
I0512 23:45:14.441395 140643590531072 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_23/self_attention/out/kernel
I0512 23:45:14.441423 140643590531072 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_23/self_attention/query/kernel
I0512 23:45:14.441453 140643590531072 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_23/self_attention/value/kernel
I0512 23:45:14.441483 140643590531072 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_3/mlp/wi_0/kernel
I0512 23:45:14.441512 140643590531072 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_3/mlp/wi_1/kernel
I0512 23:45:14.441541 140643590531072 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_3/mlp/wo/kernel
I0512 23:45:14.441570 140643590531072 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_3/pre_mlp_layer_norm/scale
I0512 23:45:14.441600 140643590531072 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_3/pre_self_attention_layer_norm/scale
I0512 23:45:14.441629 140643590531072 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_3/self_attention/key/kernel
I0512 23:45:14.441658 140643590531072 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_3/self_attention/out/kernel
I0512 23:45:14.441694 140643590531072 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_3/self_attention/query/kernel
I0512 23:45:14.441724 140643590531072 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_3/self_attention/value/kernel
I0512 23:45:14.441754 140643590531072 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_4/mlp/wi_0/kernel
I0512 23:45:14.441783 140643590531072 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_4/mlp/wi_1/kernel
I0512 23:45:14.441812 140643590531072 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_4/mlp/wo/kernel
I0512 23:45:14.441841 140643590531072 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_4/pre_mlp_layer_norm/scale
I0512 23:45:14.441870 140643590531072 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_4/pre_self_attention_layer_norm/scale
I0512 23:45:14.441899 140643590531072 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_4/self_attention/key/kernel
I0512 23:45:14.441929 140643590531072 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_4/self_attention/out/kernel
I0512 23:45:14.441960 140643590531072 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_4/self_attention/query/kernel
I0512 23:45:14.441990 140643590531072 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_4/self_attention/value/kernel
I0512 23:45:14.442019 140643590531072 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_5/extra_mlp/wi_0/kernel
I0512 23:45:14.442048 140643590531072 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_5/extra_mlp/wi_1/kernel
I0512 23:45:14.442077 140643590531072 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_5/extra_mlp/wo/kernel
I0512 23:45:14.442106 140643590531072 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_5/mlp/expert/wi_0/kernel
I0512 23:45:14.442135 140643590531072 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_5/mlp/expert/wi_1/kernel
I0512 23:45:14.442164 140643590531072 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_5/mlp/expert/wo/kernel
I0512 23:45:14.442193 140643590531072 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_5/mlp/router/router_weights/w/kernel
I0512 23:45:14.442222 140643590531072 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_5/pre_extra_mlp_layer_norm/scale
I0512 23:45:14.442251 140643590531072 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_5/pre_mlp_layer_norm/scale
I0512 23:45:14.442280 140643590531072 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_5/pre_self_attention_layer_norm/scale
I0512 23:45:14.442309 140643590531072 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_5/self_attention/key/kernel
I0512 23:45:14.442339 140643590531072 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_5/self_attention/out/kernel
I0512 23:45:14.442368 140643590531072 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_5/self_attention/query/kernel
I0512 23:45:14.442398 140643590531072 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_5/self_attention/value/kernel
I0512 23:45:14.442427 140643590531072 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_6/mlp/wi_0/kernel
I0512 23:45:14.442456 140643590531072 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_6/mlp/wi_1/kernel
I0512 23:45:14.442485 140643590531072 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_6/mlp/wo/kernel
I0512 23:45:14.442514 140643590531072 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_6/pre_mlp_layer_norm/scale
I0512 23:45:14.442543 140643590531072 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_6/pre_self_attention_layer_norm/scale
I0512 23:45:14.442573 140643590531072 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_6/self_attention/key/kernel
I0512 23:45:14.442602 140643590531072 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_6/self_attention/out/kernel
I0512 23:45:14.442631 140643590531072 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_6/self_attention/query/kernel
I0512 23:45:14.442660 140643590531072 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_6/self_attention/value/kernel
I0512 23:45:14.442695 140643590531072 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_7/mlp/wi_0/kernel
I0512 23:45:14.442725 140643590531072 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_7/mlp/wi_1/kernel
I0512 23:45:14.442754 140643590531072 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_7/mlp/wo/kernel
I0512 23:45:14.442784 140643590531072 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_7/pre_mlp_layer_norm/scale
I0512 23:45:14.442814 140643590531072 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_7/pre_self_attention_layer_norm/scale
I0512 23:45:14.442846 140643590531072 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_7/self_attention/key/kernel
I0512 23:45:14.442878 140643590531072 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_7/self_attention/out/kernel
I0512 23:45:14.442907 140643590531072 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_7/self_attention/query/kernel
I0512 23:45:14.442937 140643590531072 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_7/self_attention/value/kernel
I0512 23:45:14.442968 140643590531072 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_8/mlp/wi_0/kernel
I0512 23:45:14.442997 140643590531072 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_8/mlp/wi_1/kernel
I0512 23:45:14.443026 140643590531072 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_8/mlp/wo/kernel
I0512 23:45:14.443055 140643590531072 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_8/pre_mlp_layer_norm/scale
I0512 23:45:14.443084 140643590531072 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_8/pre_self_attention_layer_norm/scale
I0512 23:45:14.443114 140643590531072 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_8/self_attention/key/kernel
I0512 23:45:14.443143 140643590531072 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_8/self_attention/out/kernel
I0512 23:45:14.443172 140643590531072 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_8/self_attention/query/kernel
I0512 23:45:14.443201 140643590531072 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_8/self_attention/value/kernel
I0512 23:45:14.443230 140643590531072 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_9/mlp/wi_0/kernel
I0512 23:45:14.443259 140643590531072 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_9/mlp/wi_1/kernel
I0512 23:45:14.443288 140643590531072 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_9/mlp/wo/kernel
I0512 23:45:14.443317 140643590531072 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_9/pre_mlp_layer_norm/scale
I0512 23:45:14.443346 140643590531072 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_9/pre_self_attention_layer_norm/scale
I0512 23:45:14.443375 140643590531072 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_9/self_attention/key/kernel
I0512 23:45:14.443404 140643590531072 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_9/self_attention/out/kernel
I0512 23:45:14.443433 140643590531072 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_9/self_attention/query/kernel
I0512 23:45:14.443462 140643590531072 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_9/self_attention/value/kernel
I0512 23:45:14.443491 140643590531072 checkpoints.py:1100] Restoring key from ckpt: target/decoder/logits_dense/kernel
I0512 23:45:14.443521 140643590531072 checkpoints.py:1100] Restoring key from ckpt: target/token_embedder/embedding
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
I0000 00:00:1715557514.475173  370028 gcs_resource.cc:109] Using default AdmissionQueue with limit 32
I0000 00:00:1715557514.483396  372195 google_auth_provider.cc:180] Running on GCE, using service account 1081612546678-compute@developer.gserviceaccount.com
I0512 23:45:14.515603 140193652815872 utils.py:442] Initializing parameters from specific T5X checkpoint gs://rosinality-tpu-bucket/openmoe_8b_ul2/training/checkpoint_10000
I0512 23:45:14.554101 139856202532864 checkpoints.py:1054] Restoring from checkpoint: gs://rosinality-tpu-bucket/openmoe_8b_ul2/training/checkpoint_10000/checkpoint
I0512 23:45:14.620584 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/decoder_norm/scale/m
I0512 23:45:14.620768 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/decoder_norm/scale/v
I0512 23:45:14.620810 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/decoder_norm/scale/v_col
I0512 23:45:14.620847 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/decoder_norm/scale/v_row
I0512 23:45:14.620880 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_0/mlp/wi_0/kernel/m
I0512 23:45:14.620912 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_0/mlp/wi_0/kernel/v
I0512 23:45:14.620951 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_0/mlp/wi_0/kernel/v_col
I0512 23:45:14.620987 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_0/mlp/wi_0/kernel/v_row
I0512 23:45:14.621019 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_0/mlp/wi_1/kernel/m
I0512 23:45:14.621052 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_0/mlp/wi_1/kernel/v
I0512 23:45:14.621083 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_0/mlp/wi_1/kernel/v_col
I0512 23:45:14.621115 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_0/mlp/wi_1/kernel/v_row
I0512 23:45:14.621147 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_0/mlp/wo/kernel/m
I0512 23:45:14.621178 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_0/mlp/wo/kernel/v
I0512 23:45:14.621210 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_0/mlp/wo/kernel/v_col
I0512 23:45:14.621243 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_0/mlp/wo/kernel/v_row
I0512 23:45:14.621276 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_0/pre_mlp_layer_norm/scale/m
I0512 23:45:14.621308 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_0/pre_mlp_layer_norm/scale/v
I0512 23:45:14.621339 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_0/pre_mlp_layer_norm/scale/v_col
I0512 23:45:14.621371 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_0/pre_mlp_layer_norm/scale/v_row
I0512 23:45:14.621402 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_0/pre_self_attention_layer_norm/scale/m
I0512 23:45:14.621433 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_0/pre_self_attention_layer_norm/scale/v
I0512 23:45:14.621464 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_0/pre_self_attention_layer_norm/scale/v_col
I0512 23:45:14.621503 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_0/pre_self_attention_layer_norm/scale/v_row
I0512 23:45:14.621536 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_0/self_attention/key/kernel/m
I0512 23:45:14.621567 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_0/self_attention/key/kernel/v
I0512 23:45:14.621598 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_0/self_attention/key/kernel/v_col
I0512 23:45:14.621630 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_0/self_attention/key/kernel/v_row
I0512 23:45:14.621661 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_0/self_attention/out/kernel/m
I0512 23:45:14.621693 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_0/self_attention/out/kernel/v
I0512 23:45:14.621725 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_0/self_attention/out/kernel/v_col
I0512 23:45:14.621756 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_0/self_attention/out/kernel/v_row
I0512 23:45:14.621787 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_0/self_attention/query/kernel/m
I0512 23:45:14.621818 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_0/self_attention/query/kernel/v
I0512 23:45:14.621850 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_0/self_attention/query/kernel/v_col
I0512 23:45:14.621881 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_0/self_attention/query/kernel/v_row
I0512 23:45:14.621912 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_0/self_attention/value/kernel/m
I0512 23:45:14.621943 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_0/self_attention/value/kernel/v
I0512 23:45:14.621974 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_0/self_attention/value/kernel/v_col
I0512 23:45:14.622005 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_0/self_attention/value/kernel/v_row
I0512 23:45:14.622036 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_1/mlp/wi_0/kernel/m
I0512 23:45:14.622067 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_1/mlp/wi_0/kernel/v
I0512 23:45:14.622098 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_1/mlp/wi_0/kernel/v_col
I0512 23:45:14.622130 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_1/mlp/wi_0/kernel/v_row
I0512 23:45:14.622161 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_1/mlp/wi_1/kernel/m
I0512 23:45:14.622192 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_1/mlp/wi_1/kernel/v
I0512 23:45:14.622223 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_1/mlp/wi_1/kernel/v_col
I0512 23:45:14.622257 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_1/mlp/wi_1/kernel/v_row
I0512 23:45:14.622289 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_1/mlp/wo/kernel/m
I0512 23:45:14.622320 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_1/mlp/wo/kernel/v
I0512 23:45:14.622351 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_1/mlp/wo/kernel/v_col
I0512 23:45:14.622382 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_1/mlp/wo/kernel/v_row
I0512 23:45:14.622414 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_1/pre_mlp_layer_norm/scale/m
I0512 23:45:14.622445 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_1/pre_mlp_layer_norm/scale/v
I0512 23:45:14.622477 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_1/pre_mlp_layer_norm/scale/v_col
I0512 23:45:14.622514 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_1/pre_mlp_layer_norm/scale/v_row
I0512 23:45:14.622547 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_1/pre_self_attention_layer_norm/scale/m
I0512 23:45:14.622578 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_1/pre_self_attention_layer_norm/scale/v
I0512 23:45:14.622609 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_1/pre_self_attention_layer_norm/scale/v_col
I0512 23:45:14.622641 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_1/pre_self_attention_layer_norm/scale/v_row
I0512 23:45:14.622672 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_1/self_attention/key/kernel/m
I0512 23:45:14.622704 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_1/self_attention/key/kernel/v
I0512 23:45:14.622735 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_1/self_attention/key/kernel/v_col
I0512 23:45:14.622766 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_1/self_attention/key/kernel/v_row
I0512 23:45:14.622797 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_1/self_attention/out/kernel/m
I0512 23:45:14.622828 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_1/self_attention/out/kernel/v
I0512 23:45:14.622859 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_1/self_attention/out/kernel/v_col
I0512 23:45:14.622890 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_1/self_attention/out/kernel/v_row
I0512 23:45:14.622921 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_1/self_attention/query/kernel/m
I0512 23:45:14.622952 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_1/self_attention/query/kernel/v
I0512 23:45:14.622983 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_1/self_attention/query/kernel/v_col
I0512 23:45:14.623014 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_1/self_attention/query/kernel/v_row
I0512 23:45:14.623045 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_1/self_attention/value/kernel/m
I0512 23:45:14.623076 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_1/self_attention/value/kernel/v
I0512 23:45:14.623131 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_1/self_attention/value/kernel/v_col
I0512 23:45:14.623165 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_1/self_attention/value/kernel/v_row
I0512 23:45:14.623196 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_10/mlp/wi_0/kernel/m
I0512 23:45:14.623227 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_10/mlp/wi_0/kernel/v
I0512 23:45:14.623261 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_10/mlp/wi_0/kernel/v_col
I0512 23:45:14.623293 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_10/mlp/wi_0/kernel/v_row
I0512 23:45:14.623325 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_10/mlp/wi_1/kernel/m
I0512 23:45:14.623357 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_10/mlp/wi_1/kernel/v
I0512 23:45:14.623388 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_10/mlp/wi_1/kernel/v_col
I0512 23:45:14.623419 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_10/mlp/wi_1/kernel/v_row
I0512 23:45:14.623450 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_10/mlp/wo/kernel/m
I0512 23:45:14.623486 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_10/mlp/wo/kernel/v
I0512 23:45:14.623520 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_10/mlp/wo/kernel/v_col
I0512 23:45:14.623552 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_10/mlp/wo/kernel/v_row
I0512 23:45:14.623583 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_10/pre_mlp_layer_norm/scale/m
I0512 23:45:14.623614 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_10/pre_mlp_layer_norm/scale/v
I0512 23:45:14.623646 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_10/pre_mlp_layer_norm/scale/v_col
I0512 23:45:14.623677 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_10/pre_mlp_layer_norm/scale/v_row
I0512 23:45:14.623708 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_10/pre_self_attention_layer_norm/scale/m
I0512 23:45:14.623739 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_10/pre_self_attention_layer_norm/scale/v
I0512 23:45:14.623771 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_10/pre_self_attention_layer_norm/scale/v_col
I0512 23:45:14.623802 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_10/pre_self_attention_layer_norm/scale/v_row
I0512 23:45:14.623833 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_10/self_attention/key/kernel/m
I0512 23:45:14.623864 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_10/self_attention/key/kernel/v
I0512 23:45:14.623896 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_10/self_attention/key/kernel/v_col
I0512 23:45:14.623927 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_10/self_attention/key/kernel/v_row
I0512 23:45:14.624000 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_10/self_attention/out/kernel/m
I0512 23:45:14.624032 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_10/self_attention/out/kernel/v
I0512 23:45:14.624063 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_10/self_attention/out/kernel/v_col
I0512 23:45:14.624095 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_10/self_attention/out/kernel/v_row
I0512 23:45:14.624126 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_10/self_attention/query/kernel/m
I0512 23:45:14.624157 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_10/self_attention/query/kernel/v
I0512 23:45:14.624188 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_10/self_attention/query/kernel/v_col
I0512 23:45:14.624220 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_10/self_attention/query/kernel/v_row
I0512 23:45:14.624253 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_10/self_attention/value/kernel/m
I0512 23:45:14.624285 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_10/self_attention/value/kernel/v
I0512 23:45:14.624316 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_10/self_attention/value/kernel/v_col
I0512 23:45:14.624347 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_10/self_attention/value/kernel/v_row
I0512 23:45:14.624378 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/extra_mlp/wi_0/kernel/m
I0512 23:45:14.624410 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/extra_mlp/wi_0/kernel/v
I0512 23:45:14.624441 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/extra_mlp/wi_0/kernel/v_col
I0512 23:45:14.624472 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/extra_mlp/wi_0/kernel/v_row
I0512 23:45:14.624509 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/extra_mlp/wi_1/kernel/m
I0512 23:45:14.624541 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/extra_mlp/wi_1/kernel/v
I0512 23:45:14.624572 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/extra_mlp/wi_1/kernel/v_col
I0512 23:45:14.624603 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/extra_mlp/wi_1/kernel/v_row
I0512 23:45:14.624634 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/extra_mlp/wo/kernel/m
I0512 23:45:14.624666 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/extra_mlp/wo/kernel/v
I0512 23:45:14.624697 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/extra_mlp/wo/kernel/v_col
I0512 23:45:14.624729 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/extra_mlp/wo/kernel/v_row
I0512 23:45:14.624760 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/mlp/expert/wi_0/kernel/m
I0512 23:45:14.624792 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/mlp/expert/wi_0/kernel/v
I0512 23:45:14.624823 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/mlp/expert/wi_0/kernel/v_col
I0512 23:45:14.624854 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/mlp/expert/wi_0/kernel/v_row
I0512 23:45:14.624885 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/mlp/expert/wi_1/kernel/m
I0512 23:45:14.624917 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/mlp/expert/wi_1/kernel/v
I0512 23:45:14.624948 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/mlp/expert/wi_1/kernel/v_col
I0512 23:45:14.624979 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/mlp/expert/wi_1/kernel/v_row
I0512 23:45:14.625010 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/mlp/expert/wo/kernel/m
I0512 23:45:14.625041 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/mlp/expert/wo/kernel/v
I0512 23:45:14.625072 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/mlp/expert/wo/kernel/v_col
I0512 23:45:14.625103 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/mlp/expert/wo/kernel/v_row
I0512 23:45:14.625134 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/mlp/router/router_weights/w/kernel/m
I0512 23:45:14.625166 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/mlp/router/router_weights/w/kernel/v
I0512 23:45:14.625197 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/mlp/router/router_weights/w/kernel/v_col
I0512 23:45:14.625229 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/mlp/router/router_weights/w/kernel/v_row
I0512 23:45:14.625262 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/pre_extra_mlp_layer_norm/scale/m
I0512 23:45:14.625294 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/pre_extra_mlp_layer_norm/scale/v
I0512 23:45:14.625325 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/pre_extra_mlp_layer_norm/scale/v_col
I0512 23:45:14.625356 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/pre_extra_mlp_layer_norm/scale/v_row
I0512 23:45:14.625388 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/pre_mlp_layer_norm/scale/m
I0512 23:45:14.625419 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/pre_mlp_layer_norm/scale/v
I0512 23:45:14.625449 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/pre_mlp_layer_norm/scale/v_col
I0512 23:45:14.625481 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/pre_mlp_layer_norm/scale/v_row
I0512 23:45:14.625519 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/pre_self_attention_layer_norm/scale/m
I0512 23:45:14.625550 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/pre_self_attention_layer_norm/scale/v
I0512 23:45:14.625581 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/pre_self_attention_layer_norm/scale/v_col
I0512 23:45:14.625612 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/pre_self_attention_layer_norm/scale/v_row
I0512 23:45:14.625643 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/self_attention/key/kernel/m
I0512 23:45:14.625674 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/self_attention/key/kernel/v
I0512 23:45:14.625706 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/self_attention/key/kernel/v_col
I0512 23:45:14.625737 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/self_attention/key/kernel/v_row
I0512 23:45:14.625768 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/self_attention/out/kernel/m
I0512 23:45:14.625799 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/self_attention/out/kernel/v
I0512 23:45:14.625830 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/self_attention/out/kernel/v_col
I0512 23:45:14.625861 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/self_attention/out/kernel/v_row
I0512 23:45:14.625892 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/self_attention/query/kernel/m
I0512 23:45:14.625924 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/self_attention/query/kernel/v
I0512 23:45:14.625955 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/self_attention/query/kernel/v_col
I0512 23:45:14.625986 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/self_attention/query/kernel/v_row
I0512 23:45:14.626017 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/self_attention/value/kernel/m
I0512 23:45:14.626049 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/self_attention/value/kernel/v
I0512 23:45:14.626080 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/self_attention/value/kernel/v_col
I0512 23:45:14.626111 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/self_attention/value/kernel/v_row
I0512 23:45:14.626142 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_12/mlp/wi_0/kernel/m
I0512 23:45:14.626172 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_12/mlp/wi_0/kernel/v
I0512 23:45:14.626203 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_12/mlp/wi_0/kernel/v_col
I0512 23:45:14.626235 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_12/mlp/wi_0/kernel/v_row
I0512 23:45:14.626268 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_12/mlp/wi_1/kernel/m
I0512 23:45:14.626300 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_12/mlp/wi_1/kernel/v
I0512 23:45:14.626332 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_12/mlp/wi_1/kernel/v_col
I0512 23:45:14.626364 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_12/mlp/wi_1/kernel/v_row
I0512 23:45:14.626395 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_12/mlp/wo/kernel/m
I0512 23:45:14.626427 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_12/mlp/wo/kernel/v
I0512 23:45:14.626458 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_12/mlp/wo/kernel/v_col
I0512 23:45:14.626494 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_12/mlp/wo/kernel/v_row
I0512 23:45:14.626527 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_12/pre_mlp_layer_norm/scale/m
I0512 23:45:14.626558 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_12/pre_mlp_layer_norm/scale/v
I0512 23:45:14.626588 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_12/pre_mlp_layer_norm/scale/v_col
I0512 23:45:14.626618 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_12/pre_mlp_layer_norm/scale/v_row
I0512 23:45:14.626648 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_12/pre_self_attention_layer_norm/scale/m
I0512 23:45:14.626679 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_12/pre_self_attention_layer_norm/scale/v
I0512 23:45:14.626709 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_12/pre_self_attention_layer_norm/scale/v_col
I0512 23:45:14.626740 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_12/pre_self_attention_layer_norm/scale/v_row
I0512 23:45:14.626771 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_12/self_attention/key/kernel/m
I0512 23:45:14.626802 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_12/self_attention/key/kernel/v
I0512 23:45:14.626832 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_12/self_attention/key/kernel/v_col
I0512 23:45:14.626862 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_12/self_attention/key/kernel/v_row
I0512 23:45:14.626892 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_12/self_attention/out/kernel/m
I0512 23:45:14.626922 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_12/self_attention/out/kernel/v
I0512 23:45:14.626952 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_12/self_attention/out/kernel/v_col
I0512 23:45:14.626983 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_12/self_attention/out/kernel/v_row
I0512 23:45:14.627013 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_12/self_attention/query/kernel/m
I0512 23:45:14.627044 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_12/self_attention/query/kernel/v
I0512 23:45:14.627074 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_12/self_attention/query/kernel/v_col
I0512 23:45:14.627130 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_12/self_attention/query/kernel/v_row
I0512 23:45:14.627165 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_12/self_attention/value/kernel/m
I0512 23:45:14.627197 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_12/self_attention/value/kernel/v
I0512 23:45:14.627228 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_12/self_attention/value/kernel/v_col
I0512 23:45:14.627261 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_12/self_attention/value/kernel/v_row
I0512 23:45:14.627292 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_13/mlp/wi_0/kernel/m
I0512 23:45:14.627324 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_13/mlp/wi_0/kernel/v
I0512 23:45:14.627355 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_13/mlp/wi_0/kernel/v_col
I0512 23:45:14.627386 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_13/mlp/wi_0/kernel/v_row
I0512 23:45:14.627418 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_13/mlp/wi_1/kernel/m
I0512 23:45:14.627449 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_13/mlp/wi_1/kernel/v
I0512 23:45:14.627480 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_13/mlp/wi_1/kernel/v_col
I0512 23:45:14.627518 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_13/mlp/wi_1/kernel/v_row
I0512 23:45:14.627550 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_13/mlp/wo/kernel/m
I0512 23:45:14.627580 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_13/mlp/wo/kernel/v
I0512 23:45:14.627612 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_13/mlp/wo/kernel/v_col
I0512 23:45:14.627643 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_13/mlp/wo/kernel/v_row
I0512 23:45:14.627674 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_13/pre_mlp_layer_norm/scale/m
I0512 23:45:14.627705 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_13/pre_mlp_layer_norm/scale/v
I0512 23:45:14.627736 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_13/pre_mlp_layer_norm/scale/v_col
I0512 23:45:14.627767 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_13/pre_mlp_layer_norm/scale/v_row
I0512 23:45:14.627798 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_13/pre_self_attention_layer_norm/scale/m
I0512 23:45:14.627829 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_13/pre_self_attention_layer_norm/scale/v
I0512 23:45:14.627859 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_13/pre_self_attention_layer_norm/scale/v_col
I0512 23:45:14.627890 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_13/pre_self_attention_layer_norm/scale/v_row
I0512 23:45:14.627921 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_13/self_attention/key/kernel/m
I0512 23:45:14.627952 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_13/self_attention/key/kernel/v
I0512 23:45:14.627983 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_13/self_attention/key/kernel/v_col
I0512 23:45:14.628013 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_13/self_attention/key/kernel/v_row
I0512 23:45:14.628044 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_13/self_attention/out/kernel/m
I0512 23:45:14.628075 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_13/self_attention/out/kernel/v
I0512 23:45:14.628106 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_13/self_attention/out/kernel/v_col
I0512 23:45:14.628137 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_13/self_attention/out/kernel/v_row
I0512 23:45:14.628167 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_13/self_attention/query/kernel/m
I0512 23:45:14.628198 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_13/self_attention/query/kernel/v
I0512 23:45:14.628230 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_13/self_attention/query/kernel/v_col
I0512 23:45:14.628263 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_13/self_attention/query/kernel/v_row
I0512 23:45:14.628294 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_13/self_attention/value/kernel/m
I0512 23:45:14.628325 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_13/self_attention/value/kernel/v
I0512 23:45:14.628356 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_13/self_attention/value/kernel/v_col
I0512 23:45:14.628387 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_13/self_attention/value/kernel/v_row
I0512 23:45:14.628418 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_14/mlp/wi_0/kernel/m
I0512 23:45:14.628450 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_14/mlp/wi_0/kernel/v
I0512 23:45:14.628480 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_14/mlp/wi_0/kernel/v_col
I0512 23:45:14.628518 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_14/mlp/wi_0/kernel/v_row
I0512 23:45:14.628549 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_14/mlp/wi_1/kernel/m
I0512 23:45:14.628581 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_14/mlp/wi_1/kernel/v
I0512 23:45:14.628612 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_14/mlp/wi_1/kernel/v_col
I0512 23:45:14.628643 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_14/mlp/wi_1/kernel/v_row
I0512 23:45:14.628674 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_14/mlp/wo/kernel/m
I0512 23:45:14.628705 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_14/mlp/wo/kernel/v
I0512 23:45:14.628735 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_14/mlp/wo/kernel/v_col
I0512 23:45:14.628766 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_14/mlp/wo/kernel/v_row
I0512 23:45:14.628797 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_14/pre_mlp_layer_norm/scale/m
I0512 23:45:14.628828 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_14/pre_mlp_layer_norm/scale/v
I0512 23:45:14.628859 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_14/pre_mlp_layer_norm/scale/v_col
I0512 23:45:14.628890 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_14/pre_mlp_layer_norm/scale/v_row
I0512 23:45:14.628921 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_14/pre_self_attention_layer_norm/scale/m
I0512 23:45:14.628952 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_14/pre_self_attention_layer_norm/scale/v
I0512 23:45:14.628983 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_14/pre_self_attention_layer_norm/scale/v_col
I0512 23:45:14.629014 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_14/pre_self_attention_layer_norm/scale/v_row
I0512 23:45:14.629045 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_14/self_attention/key/kernel/m
I0512 23:45:14.629076 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_14/self_attention/key/kernel/v
I0512 23:45:14.629106 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_14/self_attention/key/kernel/v_col
I0512 23:45:14.629137 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_14/self_attention/key/kernel/v_row
I0512 23:45:14.629167 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_14/self_attention/out/kernel/m
I0512 23:45:14.629198 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_14/self_attention/out/kernel/v
I0512 23:45:14.629229 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_14/self_attention/out/kernel/v_col
I0512 23:45:14.629262 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_14/self_attention/out/kernel/v_row
I0512 23:45:14.629294 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_14/self_attention/query/kernel/m
I0512 23:45:14.629325 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_14/self_attention/query/kernel/v
I0512 23:45:14.629355 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_14/self_attention/query/kernel/v_col
I0512 23:45:14.629386 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_14/self_attention/query/kernel/v_row
I0512 23:45:14.629417 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_14/self_attention/value/kernel/m
I0512 23:45:14.629448 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_14/self_attention/value/kernel/v
I0512 23:45:14.629479 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_14/self_attention/value/kernel/v_col
I0512 23:45:14.629516 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_14/self_attention/value/kernel/v_row
I0512 23:45:14.629547 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_15/mlp/wi_0/kernel/m
I0512 23:45:14.629578 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_15/mlp/wi_0/kernel/v
I0512 23:45:14.629609 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_15/mlp/wi_0/kernel/v_col
I0512 23:45:14.629640 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_15/mlp/wi_0/kernel/v_row
I0512 23:45:14.629670 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_15/mlp/wi_1/kernel/m
I0512 23:45:14.629701 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_15/mlp/wi_1/kernel/v
I0512 23:45:14.629732 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_15/mlp/wi_1/kernel/v_col
I0512 23:45:14.629763 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_15/mlp/wi_1/kernel/v_row
I0512 23:45:14.629794 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_15/mlp/wo/kernel/m
I0512 23:45:14.629824 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_15/mlp/wo/kernel/v
I0512 23:45:14.629855 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_15/mlp/wo/kernel/v_col
I0512 23:45:14.629886 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_15/mlp/wo/kernel/v_row
I0512 23:45:14.629916 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_15/pre_mlp_layer_norm/scale/m
I0512 23:45:14.629947 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_15/pre_mlp_layer_norm/scale/v
I0512 23:45:14.629978 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_15/pre_mlp_layer_norm/scale/v_col
I0512 23:45:14.630009 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_15/pre_mlp_layer_norm/scale/v_row
I0512 23:45:14.630040 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_15/pre_self_attention_layer_norm/scale/m
I0512 23:45:14.630070 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_15/pre_self_attention_layer_norm/scale/v
I0512 23:45:14.630101 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_15/pre_self_attention_layer_norm/scale/v_col
I0512 23:45:14.630132 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_15/pre_self_attention_layer_norm/scale/v_row
I0512 23:45:14.630163 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_15/self_attention/key/kernel/m
I0512 23:45:14.630193 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_15/self_attention/key/kernel/v
I0512 23:45:14.630224 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_15/self_attention/key/kernel/v_col
I0512 23:45:14.630257 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_15/self_attention/key/kernel/v_row
I0512 23:45:14.630288 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_15/self_attention/out/kernel/m
I0512 23:45:14.630319 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_15/self_attention/out/kernel/v
I0512 23:45:14.630350 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_15/self_attention/out/kernel/v_col
I0512 23:45:14.630381 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_15/self_attention/out/kernel/v_row
I0512 23:45:14.630412 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_15/self_attention/query/kernel/m
I0512 23:45:14.630443 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_15/self_attention/query/kernel/v
I0512 23:45:14.630473 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_15/self_attention/query/kernel/v_col
I0512 23:45:14.630514 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_15/self_attention/query/kernel/v_row
I0512 23:45:14.630551 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_15/self_attention/value/kernel/m
I0512 23:45:14.630588 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_15/self_attention/value/kernel/v
I0512 23:45:14.630621 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_15/self_attention/value/kernel/v_col
I0512 23:45:14.630652 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_15/self_attention/value/kernel/v_row
I0512 23:45:14.630688 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_16/mlp/wi_0/kernel/m
I0512 23:45:14.630721 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_16/mlp/wi_0/kernel/v
I0512 23:45:14.630753 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_16/mlp/wi_0/kernel/v_col
I0512 23:45:14.630783 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_16/mlp/wi_0/kernel/v_row
I0512 23:45:14.630813 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_16/mlp/wi_1/kernel/m
I0512 23:45:14.630843 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_16/mlp/wi_1/kernel/v
I0512 23:45:14.630873 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_16/mlp/wi_1/kernel/v_col
I0512 23:45:14.630903 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_16/mlp/wi_1/kernel/v_row
I0512 23:45:14.630933 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_16/mlp/wo/kernel/m
I0512 23:45:14.630963 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_16/mlp/wo/kernel/v
I0512 23:45:14.630992 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_16/mlp/wo/kernel/v_col
I0512 23:45:14.631021 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_16/mlp/wo/kernel/v_row
I0512 23:45:14.631051 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_16/pre_mlp_layer_norm/scale/m
I0512 23:45:14.631102 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_16/pre_mlp_layer_norm/scale/v
I0512 23:45:14.631139 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_16/pre_mlp_layer_norm/scale/v_col
I0512 23:45:14.631172 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_16/pre_mlp_layer_norm/scale/v_row
I0512 23:45:14.631203 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_16/pre_self_attention_layer_norm/scale/m
I0512 23:45:14.631235 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_16/pre_self_attention_layer_norm/scale/v
I0512 23:45:14.631267 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_16/pre_self_attention_layer_norm/scale/v_col
I0512 23:45:14.631298 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_16/pre_self_attention_layer_norm/scale/v_row
I0512 23:45:14.631328 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_16/self_attention/key/kernel/m
I0512 23:45:14.631358 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_16/self_attention/key/kernel/v
I0512 23:45:14.631388 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_16/self_attention/key/kernel/v_col
I0512 23:45:14.631418 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_16/self_attention/key/kernel/v_row
I0512 23:45:14.631448 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_16/self_attention/out/kernel/m
I0512 23:45:14.631477 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_16/self_attention/out/kernel/v
I0512 23:45:14.631517 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_16/self_attention/out/kernel/v_col
I0512 23:45:14.631547 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_16/self_attention/out/kernel/v_row
I0512 23:45:14.631576 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_16/self_attention/query/kernel/m
I0512 23:45:14.631606 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_16/self_attention/query/kernel/v
I0512 23:45:14.631636 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_16/self_attention/query/kernel/v_col
I0512 23:45:14.631665 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_16/self_attention/query/kernel/v_row
I0512 23:45:14.631696 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_16/self_attention/value/kernel/m
I0512 23:45:14.631729 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_16/self_attention/value/kernel/v
I0512 23:45:14.631759 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_16/self_attention/value/kernel/v_col
I0512 23:45:14.631788 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_16/self_attention/value/kernel/v_row
I0512 23:45:14.631818 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/extra_mlp/wi_0/kernel/m
I0512 23:45:14.631848 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/extra_mlp/wi_0/kernel/v
I0512 23:45:14.631877 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/extra_mlp/wi_0/kernel/v_col
I0512 23:45:14.631907 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/extra_mlp/wi_0/kernel/v_row
I0512 23:45:14.631937 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/extra_mlp/wi_1/kernel/m
I0512 23:45:14.631966 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/extra_mlp/wi_1/kernel/v
I0512 23:45:14.631996 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/extra_mlp/wi_1/kernel/v_col
I0512 23:45:14.632026 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/extra_mlp/wi_1/kernel/v_row
I0512 23:45:14.632055 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/extra_mlp/wo/kernel/m
I0512 23:45:14.632085 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/extra_mlp/wo/kernel/v
I0512 23:45:14.632114 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/extra_mlp/wo/kernel/v_col
I0512 23:45:14.632144 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/extra_mlp/wo/kernel/v_row
I0512 23:45:14.632174 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/mlp/expert/wi_0/kernel/m
I0512 23:45:14.632203 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/mlp/expert/wi_0/kernel/v
I0512 23:45:14.632234 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/mlp/expert/wi_0/kernel/v_col
I0512 23:45:14.632266 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/mlp/expert/wi_0/kernel/v_row
I0512 23:45:14.632296 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/mlp/expert/wi_1/kernel/m
I0512 23:45:14.632326 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/mlp/expert/wi_1/kernel/v
I0512 23:45:14.632355 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/mlp/expert/wi_1/kernel/v_col
I0512 23:45:14.632385 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/mlp/expert/wi_1/kernel/v_row
I0512 23:45:14.632415 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/mlp/expert/wo/kernel/m
I0512 23:45:14.632445 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/mlp/expert/wo/kernel/v
I0512 23:45:14.632474 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/mlp/expert/wo/kernel/v_col
I0512 23:45:14.632510 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/mlp/expert/wo/kernel/v_row
I0512 23:45:14.632540 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/mlp/router/router_weights/w/kernel/m
I0512 23:45:14.632570 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/mlp/router/router_weights/w/kernel/v
I0512 23:45:14.632599 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/mlp/router/router_weights/w/kernel/v_col
I0512 23:45:14.632629 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/mlp/router/router_weights/w/kernel/v_row
I0512 23:45:14.632659 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/pre_extra_mlp_layer_norm/scale/m
I0512 23:45:14.632689 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/pre_extra_mlp_layer_norm/scale/v
I0512 23:45:14.632718 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/pre_extra_mlp_layer_norm/scale/v_col
I0512 23:45:14.632748 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/pre_extra_mlp_layer_norm/scale/v_row
I0512 23:45:14.632778 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/pre_mlp_layer_norm/scale/m
I0512 23:45:14.632850 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/decoder_norm/scale/m
I0512 23:45:14.633010 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/decoder_norm/scale/v
I0512 23:45:14.633052 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/decoder_norm/scale/v_col
I0512 23:45:14.632808 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/pre_mlp_layer_norm/scale/v
I0512 23:45:14.632837 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/pre_mlp_layer_norm/scale/v_col
I0512 23:45:14.632867 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/pre_mlp_layer_norm/scale/v_row
I0512 23:45:14.632896 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/pre_self_attention_layer_norm/scale/m
I0512 23:45:14.632926 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/pre_self_attention_layer_norm/scale/v
I0512 23:45:14.632955 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/pre_self_attention_layer_norm/scale/v_col
I0512 23:45:14.632985 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/pre_self_attention_layer_norm/scale/v_row
I0512 23:45:14.633015 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/self_attention/key/kernel/m
I0512 23:45:14.633044 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/self_attention/key/kernel/v
I0512 23:45:14.633074 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/self_attention/key/kernel/v_col
I0512 23:45:14.633104 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/self_attention/key/kernel/v_row
I0512 23:45:14.633133 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/self_attention/out/kernel/m
I0512 23:45:14.633163 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/self_attention/out/kernel/v
I0512 23:45:14.633193 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/self_attention/out/kernel/v_col
I0512 23:45:14.633222 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/self_attention/out/kernel/v_row
I0512 23:45:14.633088 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/decoder_norm/scale/v_row
I0512 23:45:14.633131 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_0/mlp/wi_0/kernel/m
I0512 23:45:14.633167 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_0/mlp/wi_0/kernel/v
I0512 23:45:14.633201 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_0/mlp/wi_0/kernel/v_col
I0512 23:45:14.633241 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_0/mlp/wi_0/kernel/v_row
I0512 23:45:14.633276 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_0/mlp/wi_1/kernel/m
I0512 23:45:14.633311 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_0/mlp/wi_1/kernel/v
I0512 23:45:14.633345 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_0/mlp/wi_1/kernel/v_col
I0512 23:45:14.633380 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_0/mlp/wi_1/kernel/v_row
I0512 23:45:14.633254 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/self_attention/query/kernel/m
I0512 23:45:14.633284 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/self_attention/query/kernel/v
I0512 23:45:14.633314 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/self_attention/query/kernel/v_col
I0512 23:45:14.633343 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/self_attention/query/kernel/v_row
I0512 23:45:14.633373 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/self_attention/value/kernel/m
I0512 23:45:14.633403 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/self_attention/value/kernel/v
I0512 23:45:14.633432 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/self_attention/value/kernel/v_col
I0512 23:45:14.633462 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/self_attention/value/kernel/v_row
I0512 23:45:14.633496 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_18/mlp/wi_0/kernel/m
I0512 23:45:14.633527 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_18/mlp/wi_0/kernel/v
I0512 23:45:14.633558 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_18/mlp/wi_0/kernel/v_col
I0512 23:45:14.633587 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_18/mlp/wi_0/kernel/v_row
I0512 23:45:14.633617 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_18/mlp/wi_1/kernel/m
I0512 23:45:14.633647 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_18/mlp/wi_1/kernel/v
I0512 23:45:14.633414 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_0/mlp/wo/kernel/m
I0512 23:45:14.633448 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_0/mlp/wo/kernel/v
I0512 23:45:14.633485 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_0/mlp/wo/kernel/v_col
I0512 23:45:14.633520 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_0/mlp/wo/kernel/v_row
I0512 23:45:14.633554 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_0/pre_mlp_layer_norm/scale/m
I0512 23:45:14.633588 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_0/pre_mlp_layer_norm/scale/v
I0512 23:45:14.633623 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_0/pre_mlp_layer_norm/scale/v_col
I0512 23:45:14.633656 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_0/pre_mlp_layer_norm/scale/v_row
I0512 23:45:14.633691 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_0/pre_self_attention_layer_norm/scale/m
I0512 23:45:14.633725 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_0/pre_self_attention_layer_norm/scale/v
I0512 23:45:14.633759 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_0/pre_self_attention_layer_norm/scale/v_col
I0512 23:45:14.633793 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_0/pre_self_attention_layer_norm/scale/v_row
I0512 23:45:14.633828 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_0/self_attention/key/kernel/m
I0512 23:45:14.633862 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_0/self_attention/key/kernel/v
I0512 23:45:14.633897 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_0/self_attention/key/kernel/v_col
I0512 23:45:14.633676 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_18/mlp/wi_1/kernel/v_col
I0512 23:45:14.633706 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_18/mlp/wi_1/kernel/v_row
I0512 23:45:14.633736 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_18/mlp/wo/kernel/m
I0512 23:45:14.633766 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_18/mlp/wo/kernel/v
I0512 23:45:14.633796 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_18/mlp/wo/kernel/v_col
I0512 23:45:14.633825 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_18/mlp/wo/kernel/v_row
I0512 23:45:14.633855 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_18/pre_mlp_layer_norm/scale/m
I0512 23:45:14.633885 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_18/pre_mlp_layer_norm/scale/v
I0512 23:45:14.633914 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_18/pre_mlp_layer_norm/scale/v_col
I0512 23:45:14.633944 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_18/pre_mlp_layer_norm/scale/v_row
I0512 23:45:14.633974 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_18/pre_self_attention_layer_norm/scale/m
I0512 23:45:14.634003 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_18/pre_self_attention_layer_norm/scale/v
I0512 23:45:14.634033 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_18/pre_self_attention_layer_norm/scale/v_col
I0512 23:45:14.634062 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_18/pre_self_attention_layer_norm/scale/v_row
I0512 23:45:14.634092 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_18/self_attention/key/kernel/m
I0512 23:45:14.633930 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_0/self_attention/key/kernel/v_row
I0512 23:45:14.633965 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_0/self_attention/out/kernel/m
I0512 23:45:14.633999 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_0/self_attention/out/kernel/v
I0512 23:45:14.634033 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_0/self_attention/out/kernel/v_col
I0512 23:45:14.634067 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_0/self_attention/out/kernel/v_row
I0512 23:45:14.634101 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_0/self_attention/query/kernel/m
I0512 23:45:14.634135 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_0/self_attention/query/kernel/v
I0512 23:45:14.634169 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_0/self_attention/query/kernel/v_col
I0512 23:45:14.634203 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_0/self_attention/query/kernel/v_row
I0512 23:45:14.634245 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_0/self_attention/value/kernel/m
I0512 23:45:14.634279 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_0/self_attention/value/kernel/v
I0512 23:45:14.634314 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_0/self_attention/value/kernel/v_col
I0512 23:45:14.634347 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_0/self_attention/value/kernel/v_row
I0512 23:45:14.634122 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_18/self_attention/key/kernel/v
I0512 23:45:14.634151 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_18/self_attention/key/kernel/v_col
I0512 23:45:14.634181 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_18/self_attention/key/kernel/v_row
I0512 23:45:14.634210 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_18/self_attention/out/kernel/m
I0512 23:45:14.634241 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_18/self_attention/out/kernel/v
I0512 23:45:14.634272 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_18/self_attention/out/kernel/v_col
I0512 23:45:14.634302 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_18/self_attention/out/kernel/v_row
I0512 23:45:14.634332 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_18/self_attention/query/kernel/m
I0512 23:45:14.634361 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_18/self_attention/query/kernel/v
I0512 23:45:14.634391 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_18/self_attention/query/kernel/v_col
I0512 23:45:14.634421 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_18/self_attention/query/kernel/v_row
I0512 23:45:14.634450 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_18/self_attention/value/kernel/m
I0512 23:45:14.634480 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_18/self_attention/value/kernel/v
I0512 23:45:14.634516 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_18/self_attention/value/kernel/v_col
I0512 23:45:14.634545 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_18/self_attention/value/kernel/v_row
I0512 23:45:14.634382 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_1/mlp/wi_0/kernel/m
I0512 23:45:14.634416 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_1/mlp/wi_0/kernel/v
I0512 23:45:14.634450 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_1/mlp/wi_0/kernel/v_col
I0512 23:45:14.634486 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_1/mlp/wi_0/kernel/v_row
I0512 23:45:14.634521 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_1/mlp/wi_1/kernel/m
I0512 23:45:14.634555 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_1/mlp/wi_1/kernel/v
I0512 23:45:14.634589 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_1/mlp/wi_1/kernel/v_col
I0512 23:45:14.634623 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_1/mlp/wi_1/kernel/v_row
I0512 23:45:14.634575 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_19/mlp/wi_0/kernel/m
I0512 23:45:14.634605 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_19/mlp/wi_0/kernel/v
I0512 23:45:14.634634 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_19/mlp/wi_0/kernel/v_col
I0512 23:45:14.634664 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_19/mlp/wi_0/kernel/v_row
I0512 23:45:14.634696 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_19/mlp/wi_1/kernel/m
I0512 23:45:14.634728 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_19/mlp/wi_1/kernel/v
I0512 23:45:14.634758 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_19/mlp/wi_1/kernel/v_col
I0512 23:45:14.634788 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_19/mlp/wi_1/kernel/v_row
I0512 23:45:14.634818 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_19/mlp/wo/kernel/m
I0512 23:45:14.634847 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_19/mlp/wo/kernel/v
I0512 23:45:14.634877 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_19/mlp/wo/kernel/v_col
I0512 23:45:14.634907 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_19/mlp/wo/kernel/v_row
I0512 23:45:14.634936 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_19/pre_mlp_layer_norm/scale/m
I0512 23:45:14.634966 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_19/pre_mlp_layer_norm/scale/v
I0512 23:45:14.634996 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_19/pre_mlp_layer_norm/scale/v_col
I0512 23:45:14.634657 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_1/mlp/wo/kernel/m
I0512 23:45:14.634691 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_1/mlp/wo/kernel/v
I0512 23:45:14.634725 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_1/mlp/wo/kernel/v_col
I0512 23:45:14.634759 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_1/mlp/wo/kernel/v_row
I0512 23:45:14.634794 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_1/pre_mlp_layer_norm/scale/m
I0512 23:45:14.634828 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_1/pre_mlp_layer_norm/scale/v
I0512 23:45:14.634862 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_1/pre_mlp_layer_norm/scale/v_col
I0512 23:45:14.634896 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_1/pre_mlp_layer_norm/scale/v_row
I0512 23:45:14.634930 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_1/pre_self_attention_layer_norm/scale/m
I0512 23:45:14.634964 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_1/pre_self_attention_layer_norm/scale/v
I0512 23:45:14.634998 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_1/pre_self_attention_layer_norm/scale/v_col
I0512 23:45:14.635033 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_1/pre_self_attention_layer_norm/scale/v_row
I0512 23:45:14.635067 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_1/self_attention/key/kernel/m
I0512 23:45:14.635123 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_1/self_attention/key/kernel/v
I0512 23:45:14.635160 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_1/self_attention/key/kernel/v_col
I0512 23:45:14.635194 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_1/self_attention/key/kernel/v_row
I0512 23:45:14.635026 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_19/pre_mlp_layer_norm/scale/v_row
I0512 23:45:14.635055 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_19/pre_self_attention_layer_norm/scale/m
I0512 23:45:14.635102 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_19/pre_self_attention_layer_norm/scale/v
I0512 23:45:14.635138 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_19/pre_self_attention_layer_norm/scale/v_col
I0512 23:45:14.635169 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_19/pre_self_attention_layer_norm/scale/v_row
I0512 23:45:14.635199 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_19/self_attention/key/kernel/m
I0512 23:45:14.635229 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_19/self_attention/key/kernel/v
I0512 23:45:14.635262 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_19/self_attention/key/kernel/v_col
I0512 23:45:14.635292 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_19/self_attention/key/kernel/v_row
I0512 23:45:14.635321 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_19/self_attention/out/kernel/m
I0512 23:45:14.635351 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_19/self_attention/out/kernel/v
I0512 23:45:14.635381 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_19/self_attention/out/kernel/v_col
I0512 23:45:14.635411 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_19/self_attention/out/kernel/v_row
I0512 23:45:14.635440 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_19/self_attention/query/kernel/m
I0512 23:45:14.635234 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_1/self_attention/out/kernel/m
I0512 23:45:14.635269 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_1/self_attention/out/kernel/v
I0512 23:45:14.635304 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_1/self_attention/out/kernel/v_col
I0512 23:45:14.635338 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_1/self_attention/out/kernel/v_row
I0512 23:45:14.635371 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_1/self_attention/query/kernel/m
I0512 23:45:14.635405 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_1/self_attention/query/kernel/v
I0512 23:45:14.635439 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_1/self_attention/query/kernel/v_col
I0512 23:45:14.635475 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_1/self_attention/query/kernel/v_row
I0512 23:45:14.635510 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_1/self_attention/value/kernel/m
I0512 23:45:14.635470 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_19/self_attention/query/kernel/v
I0512 23:45:14.635505 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_19/self_attention/query/kernel/v_col
I0512 23:45:14.635535 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_19/self_attention/query/kernel/v_row
I0512 23:45:14.635565 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_19/self_attention/value/kernel/m
I0512 23:45:14.635595 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_19/self_attention/value/kernel/v
I0512 23:45:14.635624 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_19/self_attention/value/kernel/v_col
I0512 23:45:14.635654 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_19/self_attention/value/kernel/v_row
I0512 23:45:14.635684 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_2/mlp/wi_0/kernel/m
I0512 23:45:14.635713 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_2/mlp/wi_0/kernel/v
I0512 23:45:14.635743 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_2/mlp/wi_0/kernel/v_col
I0512 23:45:14.635773 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_2/mlp/wi_0/kernel/v_row
I0512 23:45:14.635803 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_2/mlp/wi_1/kernel/m
I0512 23:45:14.635832 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_2/mlp/wi_1/kernel/v
I0512 23:45:14.635862 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_2/mlp/wi_1/kernel/v_col
I0512 23:45:14.635892 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_2/mlp/wi_1/kernel/v_row
I0512 23:45:14.635545 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_1/self_attention/value/kernel/v
I0512 23:45:14.635579 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_1/self_attention/value/kernel/v_col
I0512 23:45:14.635613 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_1/self_attention/value/kernel/v_row
I0512 23:45:14.635648 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_10/mlp/wi_0/kernel/m
I0512 23:45:14.635682 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_10/mlp/wi_0/kernel/v
I0512 23:45:14.635716 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_10/mlp/wi_0/kernel/v_col
I0512 23:45:14.635750 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_10/mlp/wi_0/kernel/v_row
I0512 23:45:14.635784 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_10/mlp/wi_1/kernel/m
I0512 23:45:14.635818 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_10/mlp/wi_1/kernel/v
I0512 23:45:14.635853 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_10/mlp/wi_1/kernel/v_col
I0512 23:45:14.635921 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_2/mlp/wo/kernel/m
I0512 23:45:14.635951 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_2/mlp/wo/kernel/v
I0512 23:45:14.635981 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_2/mlp/wo/kernel/v_col
I0512 23:45:14.636011 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_2/mlp/wo/kernel/v_row
I0512 23:45:14.636040 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_2/pre_mlp_layer_norm/scale/m
I0512 23:45:14.636070 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_2/pre_mlp_layer_norm/scale/v
I0512 23:45:14.636100 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_2/pre_mlp_layer_norm/scale/v_col
I0512 23:45:14.636129 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_2/pre_mlp_layer_norm/scale/v_row
I0512 23:45:14.636159 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_2/pre_self_attention_layer_norm/scale/m
I0512 23:45:14.636188 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_2/pre_self_attention_layer_norm/scale/v
I0512 23:45:14.636218 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_2/pre_self_attention_layer_norm/scale/v_col
I0512 23:45:14.636250 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_2/pre_self_attention_layer_norm/scale/v_row
I0512 23:45:14.636281 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_2/self_attention/key/kernel/m
I0512 23:45:14.636311 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_2/self_attention/key/kernel/v
I0512 23:45:14.636341 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_2/self_attention/key/kernel/v_col
I0512 23:45:14.635887 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_10/mlp/wi_1/kernel/v_row
I0512 23:45:14.635921 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_10/mlp/wo/kernel/m
I0512 23:45:14.635954 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_10/mlp/wo/kernel/v
I0512 23:45:14.635988 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_10/mlp/wo/kernel/v_col
I0512 23:45:14.636022 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_10/mlp/wo/kernel/v_row
I0512 23:45:14.636056 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_10/pre_mlp_layer_norm/scale/m
I0512 23:45:14.636090 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_10/pre_mlp_layer_norm/scale/v
I0512 23:45:14.636123 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_10/pre_mlp_layer_norm/scale/v_col
I0512 23:45:14.636155 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_10/pre_mlp_layer_norm/scale/v_row
I0512 23:45:14.636187 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_10/pre_self_attention_layer_norm/scale/m
I0512 23:45:14.636370 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_2/self_attention/key/kernel/v_row
I0512 23:45:14.636400 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_2/self_attention/out/kernel/m
I0512 23:45:14.636429 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_2/self_attention/out/kernel/v
I0512 23:45:14.636459 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_2/self_attention/out/kernel/v_col
I0512 23:45:14.636494 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_2/self_attention/out/kernel/v_row
I0512 23:45:14.636526 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_2/self_attention/query/kernel/m
I0512 23:45:14.636556 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_2/self_attention/query/kernel/v
I0512 23:45:14.636585 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_2/self_attention/query/kernel/v_col
I0512 23:45:14.636615 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_2/self_attention/query/kernel/v_row
I0512 23:45:14.636645 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_2/self_attention/value/kernel/m
I0512 23:45:14.636674 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_2/self_attention/value/kernel/v
I0512 23:45:14.636704 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_2/self_attention/value/kernel/v_col
I0512 23:45:14.636734 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_2/self_attention/value/kernel/v_row
I0512 23:45:14.636763 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_20/mlp/wi_0/kernel/m
I0512 23:45:14.636792 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_20/mlp/wi_0/kernel/v
I0512 23:45:14.636225 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_10/pre_self_attention_layer_norm/scale/v
I0512 23:45:14.636259 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_10/pre_self_attention_layer_norm/scale/v_col
I0512 23:45:14.636291 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_10/pre_self_attention_layer_norm/scale/v_row
I0512 23:45:14.636323 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_10/self_attention/key/kernel/m
I0512 23:45:14.636355 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_10/self_attention/key/kernel/v
I0512 23:45:14.636387 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_10/self_attention/key/kernel/v_col
I0512 23:45:14.636419 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_10/self_attention/key/kernel/v_row
I0512 23:45:14.636451 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_10/self_attention/out/kernel/m
I0512 23:45:14.636485 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_10/self_attention/out/kernel/v
I0512 23:45:14.636518 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_10/self_attention/out/kernel/v_col
I0512 23:45:14.636549 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_10/self_attention/out/kernel/v_row
I0512 23:45:14.636581 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_10/self_attention/query/kernel/m
I0512 23:45:14.636613 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_10/self_attention/query/kernel/v
I0512 23:45:14.636646 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_10/self_attention/query/kernel/v_col
I0512 23:45:14.636677 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_10/self_attention/query/kernel/v_row
I0512 23:45:14.636709 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_10/self_attention/value/kernel/m
I0512 23:45:14.636741 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_10/self_attention/value/kernel/v
I0512 23:45:14.636772 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_10/self_attention/value/kernel/v_col
I0512 23:45:14.636804 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_10/self_attention/value/kernel/v_row
I0512 23:45:14.636836 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/extra_mlp/wi_0/kernel/m
I0512 23:45:14.636868 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/extra_mlp/wi_0/kernel/v
I0512 23:45:14.636822 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_20/mlp/wi_0/kernel/v_col
I0512 23:45:14.636852 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_20/mlp/wi_0/kernel/v_row
I0512 23:45:14.636881 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_20/mlp/wi_1/kernel/m
I0512 23:45:14.636911 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_20/mlp/wi_1/kernel/v
I0512 23:45:14.636940 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_20/mlp/wi_1/kernel/v_col
I0512 23:45:14.636970 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_20/mlp/wi_1/kernel/v_row
I0512 23:45:14.637000 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_20/mlp/wo/kernel/m
I0512 23:45:14.637030 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_20/mlp/wo/kernel/v
I0512 23:45:14.637059 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_20/mlp/wo/kernel/v_col
I0512 23:45:14.637089 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_20/mlp/wo/kernel/v_row
I0512 23:45:14.637119 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_20/pre_mlp_layer_norm/scale/m
I0512 23:45:14.637148 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_20/pre_mlp_layer_norm/scale/v
I0512 23:45:14.637178 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_20/pre_mlp_layer_norm/scale/v_col
I0512 23:45:14.637208 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_20/pre_mlp_layer_norm/scale/v_row
I0512 23:45:14.637238 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_20/pre_self_attention_layer_norm/scale/m
I0512 23:45:14.636900 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/extra_mlp/wi_0/kernel/v_col
I0512 23:45:14.636932 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/extra_mlp/wi_0/kernel/v_row
I0512 23:45:14.636963 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/extra_mlp/wi_1/kernel/m
I0512 23:45:14.636995 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/extra_mlp/wi_1/kernel/v
I0512 23:45:14.637027 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/extra_mlp/wi_1/kernel/v_col
I0512 23:45:14.637059 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/extra_mlp/wi_1/kernel/v_row
I0512 23:45:14.637091 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/extra_mlp/wo/kernel/m
I0512 23:45:14.637122 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/extra_mlp/wo/kernel/v
I0512 23:45:14.637154 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/extra_mlp/wo/kernel/v_col
I0512 23:45:14.637186 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/extra_mlp/wo/kernel/v_row
I0512 23:45:14.637222 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/mlp/expert/wi_0/kernel/m
I0512 23:45:14.637255 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/mlp/expert/wi_0/kernel/v
I0512 23:45:14.637270 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_20/pre_self_attention_layer_norm/scale/v
I0512 23:45:14.637300 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_20/pre_self_attention_layer_norm/scale/v_col
I0512 23:45:14.637331 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_20/pre_self_attention_layer_norm/scale/v_row
I0512 23:45:14.637361 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_20/self_attention/key/kernel/m
I0512 23:45:14.637390 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_20/self_attention/key/kernel/v
I0512 23:45:14.637420 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_20/self_attention/key/kernel/v_col
I0512 23:45:14.637450 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_20/self_attention/key/kernel/v_row
I0512 23:45:14.637480 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_20/self_attention/out/kernel/m
I0512 23:45:14.637516 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_20/self_attention/out/kernel/v
I0512 23:45:14.637546 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_20/self_attention/out/kernel/v_col
I0512 23:45:14.637575 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_20/self_attention/out/kernel/v_row
I0512 23:45:14.637605 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_20/self_attention/query/kernel/m
I0512 23:45:14.637635 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_20/self_attention/query/kernel/v
I0512 23:45:14.637665 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_20/self_attention/query/kernel/v_col
I0512 23:45:14.637287 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/mlp/expert/wi_0/kernel/v_col
I0512 23:45:14.637319 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/mlp/expert/wi_0/kernel/v_row
I0512 23:45:14.637351 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/mlp/expert/wi_1/kernel/m
I0512 23:45:14.637383 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/mlp/expert/wi_1/kernel/v
I0512 23:45:14.637415 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/mlp/expert/wi_1/kernel/v_col
I0512 23:45:14.637447 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/mlp/expert/wi_1/kernel/v_row
I0512 23:45:14.637480 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/mlp/expert/wo/kernel/m
I0512 23:45:14.637513 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/mlp/expert/wo/kernel/v
I0512 23:45:14.637545 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/mlp/expert/wo/kernel/v_col
I0512 23:45:14.637577 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/mlp/expert/wo/kernel/v_row
I0512 23:45:14.637609 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/mlp/router/router_weights/w/kernel/m
I0512 23:45:14.637641 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/mlp/router/router_weights/w/kernel/v
I0512 23:45:14.637696 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_20/self_attention/query/kernel/v_row
I0512 23:45:14.637729 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_20/self_attention/value/kernel/m
I0512 23:45:14.637759 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_20/self_attention/value/kernel/v
I0512 23:45:14.637789 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_20/self_attention/value/kernel/v_col
I0512 23:45:14.637818 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_20/self_attention/value/kernel/v_row
I0512 23:45:14.637848 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_21/mlp/wi_0/kernel/m
I0512 23:45:14.637877 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_21/mlp/wi_0/kernel/v
I0512 23:45:14.637907 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_21/mlp/wi_0/kernel/v_col
I0512 23:45:14.637937 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_21/mlp/wi_0/kernel/v_row
I0512 23:45:14.637967 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_21/mlp/wi_1/kernel/m
I0512 23:45:14.637996 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_21/mlp/wi_1/kernel/v
I0512 23:45:14.638025 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_21/mlp/wi_1/kernel/v_col
I0512 23:45:14.638055 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_21/mlp/wi_1/kernel/v_row
I0512 23:45:14.638085 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_21/mlp/wo/kernel/m
I0512 23:45:14.638115 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_21/mlp/wo/kernel/v
I0512 23:45:14.637672 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/mlp/router/router_weights/w/kernel/v_col
I0512 23:45:14.637704 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/mlp/router/router_weights/w/kernel/v_row
I0512 23:45:14.637736 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/pre_extra_mlp_layer_norm/scale/m
I0512 23:45:14.637768 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/pre_extra_mlp_layer_norm/scale/v
I0512 23:45:14.637800 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/pre_extra_mlp_layer_norm/scale/v_col
I0512 23:45:14.637831 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/pre_extra_mlp_layer_norm/scale/v_row
I0512 23:45:14.637863 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/pre_mlp_layer_norm/scale/m
I0512 23:45:14.637895 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/pre_mlp_layer_norm/scale/v
I0512 23:45:14.637927 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/pre_mlp_layer_norm/scale/v_col
I0512 23:45:14.637959 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/pre_mlp_layer_norm/scale/v_row
I0512 23:45:14.637990 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/pre_self_attention_layer_norm/scale/m
I0512 23:45:14.638022 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/pre_self_attention_layer_norm/scale/v
I0512 23:45:14.638054 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/pre_self_attention_layer_norm/scale/v_col
I0512 23:45:14.638085 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/pre_self_attention_layer_norm/scale/v_row
I0512 23:45:14.638117 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/self_attention/key/kernel/m
I0512 23:45:14.638149 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/self_attention/key/kernel/v
I0512 23:45:14.638181 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/self_attention/key/kernel/v_col
I0512 23:45:14.638219 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/self_attention/key/kernel/v_row
I0512 23:45:14.638252 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/self_attention/out/kernel/m
I0512 23:45:14.638285 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/self_attention/out/kernel/v
I0512 23:45:14.638316 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/self_attention/out/kernel/v_col
I0512 23:45:14.638348 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/self_attention/out/kernel/v_row
I0512 23:45:14.638380 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/self_attention/query/kernel/m
I0512 23:45:14.638412 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/self_attention/query/kernel/v
I0512 23:45:14.638443 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/self_attention/query/kernel/v_col
I0512 23:45:14.638144 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_21/mlp/wo/kernel/v_col
I0512 23:45:14.638174 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_21/mlp/wo/kernel/v_row
I0512 23:45:14.638203 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_21/pre_mlp_layer_norm/scale/m
I0512 23:45:14.638234 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_21/pre_mlp_layer_norm/scale/v
I0512 23:45:14.638267 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_21/pre_mlp_layer_norm/scale/v_col
I0512 23:45:14.638297 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_21/pre_mlp_layer_norm/scale/v_row
I0512 23:45:14.638327 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_21/pre_self_attention_layer_norm/scale/m
I0512 23:45:14.638357 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_21/pre_self_attention_layer_norm/scale/v
I0512 23:45:14.638387 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_21/pre_self_attention_layer_norm/scale/v_col
I0512 23:45:14.638417 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_21/pre_self_attention_layer_norm/scale/v_row
I0512 23:45:14.638447 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_21/self_attention/key/kernel/m
I0512 23:45:14.638477 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_21/self_attention/key/kernel/v
I0512 23:45:14.638513 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_21/self_attention/key/kernel/v_col
I0512 23:45:14.638543 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_21/self_attention/key/kernel/v_row
I0512 23:45:14.638573 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_21/self_attention/out/kernel/m
I0512 23:45:14.638602 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_21/self_attention/out/kernel/v
I0512 23:45:14.638631 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_21/self_attention/out/kernel/v_col
I0512 23:45:14.638660 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_21/self_attention/out/kernel/v_row
I0512 23:45:14.638689 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_21/self_attention/query/kernel/m
I0512 23:45:14.638719 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_21/self_attention/query/kernel/v
I0512 23:45:14.638748 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_21/self_attention/query/kernel/v_col
I0512 23:45:14.638778 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_21/self_attention/query/kernel/v_row
I0512 23:45:14.638808 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_21/self_attention/value/kernel/m
I0512 23:45:14.638837 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_21/self_attention/value/kernel/v
I0512 23:45:14.638866 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_21/self_attention/value/kernel/v_col
I0512 23:45:14.638896 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_21/self_attention/value/kernel/v_row
I0512 23:45:14.638925 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_22/mlp/wi_0/kernel/m
I0512 23:45:14.638955 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_22/mlp/wi_0/kernel/v
I0512 23:45:14.638984 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_22/mlp/wi_0/kernel/v_col
I0512 23:45:14.639014 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_22/mlp/wi_0/kernel/v_row
I0512 23:45:14.638477 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/self_attention/query/kernel/v_row
I0512 23:45:14.638510 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/self_attention/value/kernel/m
I0512 23:45:14.638542 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/self_attention/value/kernel/v
I0512 23:45:14.638574 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/self_attention/value/kernel/v_col
I0512 23:45:14.638605 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/self_attention/value/kernel/v_row
I0512 23:45:14.638637 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_12/mlp/wi_0/kernel/m
I0512 23:45:14.638669 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_12/mlp/wi_0/kernel/v
I0512 23:45:14.638700 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_12/mlp/wi_0/kernel/v_col
I0512 23:45:14.638733 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_12/mlp/wi_0/kernel/v_row
I0512 23:45:14.638765 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_12/mlp/wi_1/kernel/m
I0512 23:45:14.638796 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_12/mlp/wi_1/kernel/v
I0512 23:45:14.638829 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_12/mlp/wi_1/kernel/v_col
I0512 23:45:14.638861 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_12/mlp/wi_1/kernel/v_row
I0512 23:45:14.638893 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_12/mlp/wo/kernel/m
I0512 23:45:14.639043 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_22/mlp/wi_1/kernel/m
I0512 23:45:14.639072 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_22/mlp/wi_1/kernel/v
I0512 23:45:14.639126 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_22/mlp/wi_1/kernel/v_col
I0512 23:45:14.639158 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_22/mlp/wi_1/kernel/v_row
I0512 23:45:14.639188 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_22/mlp/wo/kernel/m
I0512 23:45:14.639217 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_22/mlp/wo/kernel/v
I0512 23:45:14.639248 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_22/mlp/wo/kernel/v_col
I0512 23:45:14.639279 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_22/mlp/wo/kernel/v_row
I0512 23:45:14.639309 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_22/pre_mlp_layer_norm/scale/m
I0512 23:45:14.639338 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_22/pre_mlp_layer_norm/scale/v
I0512 23:45:14.639368 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_22/pre_mlp_layer_norm/scale/v_col
I0512 23:45:14.639397 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_22/pre_mlp_layer_norm/scale/v_row
I0512 23:45:14.639427 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_22/pre_self_attention_layer_norm/scale/m
I0512 23:45:14.639456 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_22/pre_self_attention_layer_norm/scale/v
I0512 23:45:14.638924 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_12/mlp/wo/kernel/v
I0512 23:45:14.638955 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_12/mlp/wo/kernel/v_col
I0512 23:45:14.638987 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_12/mlp/wo/kernel/v_row
I0512 23:45:14.639018 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_12/pre_mlp_layer_norm/scale/m
I0512 23:45:14.639049 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_12/pre_mlp_layer_norm/scale/v
I0512 23:45:14.639081 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_12/pre_mlp_layer_norm/scale/v_col
I0512 23:45:14.639136 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_12/pre_mlp_layer_norm/scale/v_row
I0512 23:45:14.639168 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_12/pre_self_attention_layer_norm/scale/m
I0512 23:45:14.639200 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_12/pre_self_attention_layer_norm/scale/v
I0512 23:45:14.639240 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_12/pre_self_attention_layer_norm/scale/v_col
I0512 23:45:14.639272 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_12/pre_self_attention_layer_norm/scale/v_row
I0512 23:45:14.639304 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_12/self_attention/key/kernel/m
I0512 23:45:14.639336 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_12/self_attention/key/kernel/v
I0512 23:45:14.639367 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_12/self_attention/key/kernel/v_col
I0512 23:45:14.639490 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_22/pre_self_attention_layer_norm/scale/v_col
I0512 23:45:14.639522 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_22/pre_self_attention_layer_norm/scale/v_row
I0512 23:45:14.639552 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_22/self_attention/key/kernel/m
I0512 23:45:14.639581 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_22/self_attention/key/kernel/v
I0512 23:45:14.639610 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_22/self_attention/key/kernel/v_col
I0512 23:45:14.639640 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_22/self_attention/key/kernel/v_row
I0512 23:45:14.639669 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_22/self_attention/out/kernel/m
I0512 23:45:14.639698 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_22/self_attention/out/kernel/v
I0512 23:45:14.639728 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_22/self_attention/out/kernel/v_col
I0512 23:45:14.639757 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_22/self_attention/out/kernel/v_row
I0512 23:45:14.639786 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_22/self_attention/query/kernel/m
I0512 23:45:14.639815 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_22/self_attention/query/kernel/v
I0512 23:45:14.639844 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_22/self_attention/query/kernel/v_col
I0512 23:45:14.639873 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_22/self_attention/query/kernel/v_row
I0512 23:45:14.639903 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_22/self_attention/value/kernel/m
I0512 23:45:14.639398 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_12/self_attention/key/kernel/v_row
I0512 23:45:14.639430 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_12/self_attention/out/kernel/m
I0512 23:45:14.639461 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_12/self_attention/out/kernel/v
I0512 23:45:14.639497 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_12/self_attention/out/kernel/v_col
I0512 23:45:14.639529 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_12/self_attention/out/kernel/v_row
I0512 23:45:14.639561 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_12/self_attention/query/kernel/m
I0512 23:45:14.639592 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_12/self_attention/query/kernel/v
I0512 23:45:14.639624 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_12/self_attention/query/kernel/v_col
I0512 23:45:14.639655 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_12/self_attention/query/kernel/v_row
I0512 23:45:14.639688 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_12/self_attention/value/kernel/m
I0512 23:45:14.639721 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_12/self_attention/value/kernel/v
I0512 23:45:14.639753 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_12/self_attention/value/kernel/v_col
I0512 23:45:14.639785 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_12/self_attention/value/kernel/v_row
I0512 23:45:14.639816 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_13/mlp/wi_0/kernel/m
I0512 23:45:14.639932 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_22/self_attention/value/kernel/v
I0512 23:45:14.639961 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_22/self_attention/value/kernel/v_col
I0512 23:45:14.639991 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_22/self_attention/value/kernel/v_row
I0512 23:45:14.640020 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/extra_mlp/wi_0/kernel/m
I0512 23:45:14.640049 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/extra_mlp/wi_0/kernel/v
I0512 23:45:14.640079 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/extra_mlp/wi_0/kernel/v_col
I0512 23:45:14.640108 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/extra_mlp/wi_0/kernel/v_row
I0512 23:45:14.640137 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/extra_mlp/wi_1/kernel/m
I0512 23:45:14.640166 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/extra_mlp/wi_1/kernel/v
I0512 23:45:14.640196 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/extra_mlp/wi_1/kernel/v_col
I0512 23:45:14.640225 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/extra_mlp/wi_1/kernel/v_row
I0512 23:45:14.640256 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/extra_mlp/wo/kernel/m
I0512 23:45:14.640286 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/extra_mlp/wo/kernel/v
I0512 23:45:14.640316 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/extra_mlp/wo/kernel/v_col
I0512 23:45:14.640345 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/extra_mlp/wo/kernel/v_row
I0512 23:45:14.639853 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_13/mlp/wi_0/kernel/v
I0512 23:45:14.639884 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_13/mlp/wi_0/kernel/v_col
I0512 23:45:14.639916 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_13/mlp/wi_0/kernel/v_row
I0512 23:45:14.639948 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_13/mlp/wi_1/kernel/m
I0512 23:45:14.639980 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_13/mlp/wi_1/kernel/v
I0512 23:45:14.640012 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_13/mlp/wi_1/kernel/v_col
I0512 23:45:14.640043 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_13/mlp/wi_1/kernel/v_row
I0512 23:45:14.640075 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_13/mlp/wo/kernel/m
I0512 23:45:14.640107 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_13/mlp/wo/kernel/v
I0512 23:45:14.640139 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_13/mlp/wo/kernel/v_col
I0512 23:45:14.640170 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_13/mlp/wo/kernel/v_row
I0512 23:45:14.640202 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_13/pre_mlp_layer_norm/scale/m
I0512 23:45:14.640241 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_13/pre_mlp_layer_norm/scale/v
I0512 23:45:14.640274 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_13/pre_mlp_layer_norm/scale/v_col
I0512 23:45:14.640305 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_13/pre_mlp_layer_norm/scale/v_row
I0512 23:45:14.640375 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/mlp/expert/wi_0/kernel/m
I0512 23:45:14.640404 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/mlp/expert/wi_0/kernel/v
I0512 23:45:14.640433 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/mlp/expert/wi_0/kernel/v_col
I0512 23:45:14.640462 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/mlp/expert/wi_0/kernel/v_row
I0512 23:45:14.640497 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/mlp/expert/wi_1/kernel/m
I0512 23:45:14.640528 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/mlp/expert/wi_1/kernel/v
I0512 23:45:14.640558 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/mlp/expert/wi_1/kernel/v_col
I0512 23:45:14.640587 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/mlp/expert/wi_1/kernel/v_row
I0512 23:45:14.640616 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/mlp/expert/wo/kernel/m
I0512 23:45:14.640645 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/mlp/expert/wo/kernel/v
I0512 23:45:14.640675 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/mlp/expert/wo/kernel/v_col
I0512 23:45:14.640707 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/mlp/expert/wo/kernel/v_row
I0512 23:45:14.640738 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/mlp/router/router_weights/w/kernel/m
I0512 23:45:14.640768 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/mlp/router/router_weights/w/kernel/v
I0512 23:45:14.640797 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/mlp/router/router_weights/w/kernel/v_col
I0512 23:45:14.640827 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/mlp/router/router_weights/w/kernel/v_row
I0512 23:45:14.640856 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/pre_extra_mlp_layer_norm/scale/m
I0512 23:45:14.640886 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/pre_extra_mlp_layer_norm/scale/v
I0512 23:45:14.640915 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/pre_extra_mlp_layer_norm/scale/v_col
I0512 23:45:14.640944 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/pre_extra_mlp_layer_norm/scale/v_row
I0512 23:45:14.640973 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/pre_mlp_layer_norm/scale/m
I0512 23:45:14.641003 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/pre_mlp_layer_norm/scale/v
I0512 23:45:14.641032 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/pre_mlp_layer_norm/scale/v_col
I0512 23:45:14.641061 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/pre_mlp_layer_norm/scale/v_row
I0512 23:45:14.641091 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/pre_self_attention_layer_norm/scale/m
I0512 23:45:14.641120 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/pre_self_attention_layer_norm/scale/v
I0512 23:45:14.640337 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_13/pre_self_attention_layer_norm/scale/m
I0512 23:45:14.640368 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_13/pre_self_attention_layer_norm/scale/v
I0512 23:45:14.640400 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_13/pre_self_attention_layer_norm/scale/v_col
I0512 23:45:14.640432 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_13/pre_self_attention_layer_norm/scale/v_row
I0512 23:45:14.640464 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_13/self_attention/key/kernel/m
I0512 23:45:14.640498 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_13/self_attention/key/kernel/v
I0512 23:45:14.640531 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_13/self_attention/key/kernel/v_col
I0512 23:45:14.640563 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_13/self_attention/key/kernel/v_row
I0512 23:45:14.640594 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_13/self_attention/out/kernel/m
I0512 23:45:14.640626 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_13/self_attention/out/kernel/v
I0512 23:45:14.640658 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_13/self_attention/out/kernel/v_col
I0512 23:45:14.640689 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_13/self_attention/out/kernel/v_row
I0512 23:45:14.640721 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_13/self_attention/query/kernel/m
I0512 23:45:14.640753 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_13/self_attention/query/kernel/v
I0512 23:45:14.641149 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/pre_self_attention_layer_norm/scale/v_col
I0512 23:45:14.641185 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/pre_self_attention_layer_norm/scale/v_row
I0512 23:45:14.641217 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/self_attention/key/kernel/m
I0512 23:45:14.641248 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/self_attention/key/kernel/v
I0512 23:45:14.641279 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/self_attention/key/kernel/v_col
I0512 23:45:14.641309 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/self_attention/key/kernel/v_row
I0512 23:45:14.641338 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/self_attention/out/kernel/m
I0512 23:45:14.641368 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/self_attention/out/kernel/v
I0512 23:45:14.641397 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/self_attention/out/kernel/v_col
I0512 23:45:14.641427 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/self_attention/out/kernel/v_row
I0512 23:45:14.641456 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/self_attention/query/kernel/m
I0512 23:45:14.641490 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/self_attention/query/kernel/v
I0512 23:45:14.640785 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_13/self_attention/query/kernel/v_col
I0512 23:45:14.640817 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_13/self_attention/query/kernel/v_row
I0512 23:45:14.640848 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_13/self_attention/value/kernel/m
I0512 23:45:14.640880 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_13/self_attention/value/kernel/v
I0512 23:45:14.640911 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_13/self_attention/value/kernel/v_col
I0512 23:45:14.640943 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_13/self_attention/value/kernel/v_row
I0512 23:45:14.640974 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_14/mlp/wi_0/kernel/m
I0512 23:45:14.641006 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_14/mlp/wi_0/kernel/v
I0512 23:45:14.641038 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_14/mlp/wi_0/kernel/v_col
I0512 23:45:14.641070 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_14/mlp/wi_0/kernel/v_row
I0512 23:45:14.641102 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_14/mlp/wi_1/kernel/m
I0512 23:45:14.641134 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_14/mlp/wi_1/kernel/v
I0512 23:45:14.641165 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_14/mlp/wi_1/kernel/v_col
I0512 23:45:14.641197 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_14/mlp/wi_1/kernel/v_row
I0512 23:45:14.641235 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_14/mlp/wo/kernel/m
I0512 23:45:14.641522 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/self_attention/query/kernel/v_col
I0512 23:45:14.641551 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/self_attention/query/kernel/v_row
I0512 23:45:14.641581 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/self_attention/value/kernel/m
I0512 23:45:14.641610 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/self_attention/value/kernel/v
I0512 23:45:14.641639 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/self_attention/value/kernel/v_col
I0512 23:45:14.641669 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/self_attention/value/kernel/v_row
I0512 23:45:14.641698 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_3/mlp/wi_0/kernel/m
I0512 23:45:14.641728 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_3/mlp/wi_0/kernel/v
I0512 23:45:14.641757 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_3/mlp/wi_0/kernel/v_col
I0512 23:45:14.641787 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_3/mlp/wi_0/kernel/v_row
I0512 23:45:14.641817 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_3/mlp/wi_1/kernel/m
I0512 23:45:14.641846 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_3/mlp/wi_1/kernel/v
I0512 23:45:14.641875 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_3/mlp/wi_1/kernel/v_col
I0512 23:45:14.641272 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_14/mlp/wo/kernel/v
I0512 23:45:14.641308 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_14/mlp/wo/kernel/v_col
I0512 23:45:14.641341 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_14/mlp/wo/kernel/v_row
I0512 23:45:14.641373 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_14/pre_mlp_layer_norm/scale/m
I0512 23:45:14.641404 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_14/pre_mlp_layer_norm/scale/v
I0512 23:45:14.641436 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_14/pre_mlp_layer_norm/scale/v_col
I0512 23:45:14.641469 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_14/pre_mlp_layer_norm/scale/v_row
I0512 23:45:14.641503 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_14/pre_self_attention_layer_norm/scale/m
I0512 23:45:14.641535 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_14/pre_self_attention_layer_norm/scale/v
I0512 23:45:14.641567 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_14/pre_self_attention_layer_norm/scale/v_col
I0512 23:45:14.641599 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_14/pre_self_attention_layer_norm/scale/v_row
I0512 23:45:14.641631 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_14/self_attention/key/kernel/m
I0512 23:45:14.641663 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_14/self_attention/key/kernel/v
I0512 23:45:14.641695 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_14/self_attention/key/kernel/v_col
I0512 23:45:14.641904 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_3/mlp/wi_1/kernel/v_row
I0512 23:45:14.641933 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_3/mlp/wo/kernel/m
I0512 23:45:14.641963 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_3/mlp/wo/kernel/v
I0512 23:45:14.641992 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_3/mlp/wo/kernel/v_col
I0512 23:45:14.642021 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_3/mlp/wo/kernel/v_row
I0512 23:45:14.642050 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_3/pre_mlp_layer_norm/scale/m
I0512 23:45:14.642080 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_3/pre_mlp_layer_norm/scale/v
I0512 23:45:14.642109 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_3/pre_mlp_layer_norm/scale/v_col
I0512 23:45:14.642138 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_3/pre_mlp_layer_norm/scale/v_row
I0512 23:45:14.642168 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_3/pre_self_attention_layer_norm/scale/m
I0512 23:45:14.642197 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_3/pre_self_attention_layer_norm/scale/v
I0512 23:45:14.642226 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_3/pre_self_attention_layer_norm/scale/v_col
I0512 23:45:14.642258 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_3/pre_self_attention_layer_norm/scale/v_row
I0512 23:45:14.641726 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_14/self_attention/key/kernel/v_row
I0512 23:45:14.641758 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_14/self_attention/out/kernel/m
I0512 23:45:14.641790 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_14/self_attention/out/kernel/v
I0512 23:45:14.641822 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_14/self_attention/out/kernel/v_col
I0512 23:45:14.641854 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_14/self_attention/out/kernel/v_row
I0512 23:45:14.641885 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_14/self_attention/query/kernel/m
I0512 23:45:14.641917 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_14/self_attention/query/kernel/v
I0512 23:45:14.641949 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_14/self_attention/query/kernel/v_col
I0512 23:45:14.641981 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_14/self_attention/query/kernel/v_row
I0512 23:45:14.642013 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_14/self_attention/value/kernel/m
I0512 23:45:14.642044 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_14/self_attention/value/kernel/v
I0512 23:45:14.642076 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_14/self_attention/value/kernel/v_col
I0512 23:45:14.642107 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_14/self_attention/value/kernel/v_row
I0512 23:45:14.642139 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_15/mlp/wi_0/kernel/m
I0512 23:45:14.642171 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_15/mlp/wi_0/kernel/v
I0512 23:45:14.642288 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_3/self_attention/key/kernel/m
I0512 23:45:14.642317 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_3/self_attention/key/kernel/v
I0512 23:45:14.642347 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_3/self_attention/key/kernel/v_col
I0512 23:45:14.642376 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_3/self_attention/key/kernel/v_row
I0512 23:45:14.642405 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_3/self_attention/out/kernel/m
I0512 23:45:14.642434 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_3/self_attention/out/kernel/v
I0512 23:45:14.642464 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_3/self_attention/out/kernel/v_col
I0512 23:45:14.642498 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_3/self_attention/out/kernel/v_row
I0512 23:45:14.642529 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_3/self_attention/query/kernel/m
I0512 23:45:14.642559 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_3/self_attention/query/kernel/v
I0512 23:45:14.642588 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_3/self_attention/query/kernel/v_col
I0512 23:45:14.642617 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_3/self_attention/query/kernel/v_row
I0512 23:45:14.642647 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_3/self_attention/value/kernel/m
I0512 23:45:14.642202 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_15/mlp/wi_0/kernel/v_col
I0512 23:45:14.642241 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_15/mlp/wi_0/kernel/v_row
I0512 23:45:14.642274 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_15/mlp/wi_1/kernel/m
I0512 23:45:14.642306 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_15/mlp/wi_1/kernel/v
I0512 23:45:14.642337 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_15/mlp/wi_1/kernel/v_col
I0512 23:45:14.642369 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_15/mlp/wi_1/kernel/v_row
I0512 23:45:14.642401 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_15/mlp/wo/kernel/m
I0512 23:45:14.642432 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_15/mlp/wo/kernel/v
I0512 23:45:14.642465 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_15/mlp/wo/kernel/v_col
I0512 23:45:14.642499 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_15/mlp/wo/kernel/v_row
I0512 23:45:14.642531 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_15/pre_mlp_layer_norm/scale/m
I0512 23:45:14.642563 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_15/pre_mlp_layer_norm/scale/v
I0512 23:45:14.642595 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_15/pre_mlp_layer_norm/scale/v_col
I0512 23:45:14.642626 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_15/pre_mlp_layer_norm/scale/v_row
I0512 23:45:14.642658 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_15/pre_self_attention_layer_norm/scale/m
I0512 23:45:14.642676 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_3/self_attention/value/kernel/v
I0512 23:45:14.642705 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_3/self_attention/value/kernel/v_col
I0512 23:45:14.642734 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_3/self_attention/value/kernel/v_row
I0512 23:45:14.642764 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_4/mlp/wi_0/kernel/m
I0512 23:45:14.642793 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_4/mlp/wi_0/kernel/v
I0512 23:45:14.642822 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_4/mlp/wi_0/kernel/v_col
I0512 23:45:14.642851 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_4/mlp/wi_0/kernel/v_row
I0512 23:45:14.642881 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_4/mlp/wi_1/kernel/m
I0512 23:45:14.642910 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_4/mlp/wi_1/kernel/v
I0512 23:45:14.642939 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_4/mlp/wi_1/kernel/v_col
I0512 23:45:14.642969 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_4/mlp/wi_1/kernel/v_row
I0512 23:45:14.642998 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_4/mlp/wo/kernel/m
I0512 23:45:14.643028 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_4/mlp/wo/kernel/v
I0512 23:45:14.643057 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_4/mlp/wo/kernel/v_col
I0512 23:45:14.643104 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_4/mlp/wo/kernel/v_row
I0512 23:45:14.643139 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_4/pre_mlp_layer_norm/scale/m
I0512 23:45:14.643170 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_4/pre_mlp_layer_norm/scale/v
I0512 23:45:14.643200 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_4/pre_mlp_layer_norm/scale/v_col
I0512 23:45:14.643229 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_4/pre_mlp_layer_norm/scale/v_row
I0512 23:45:14.643261 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_4/pre_self_attention_layer_norm/scale/m
I0512 23:45:14.643291 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_4/pre_self_attention_layer_norm/scale/v
I0512 23:45:14.643321 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_4/pre_self_attention_layer_norm/scale/v_col
I0512 23:45:14.643351 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_4/pre_self_attention_layer_norm/scale/v_row
I0512 23:45:14.643380 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_4/self_attention/key/kernel/m
I0512 23:45:14.643409 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_4/self_attention/key/kernel/v
I0512 23:45:14.643439 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_4/self_attention/key/kernel/v_col
I0512 23:45:14.643468 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_4/self_attention/key/kernel/v_row
I0512 23:45:14.643503 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_4/self_attention/out/kernel/m
I0512 23:45:14.642689 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_15/pre_self_attention_layer_norm/scale/v
I0512 23:45:14.642721 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_15/pre_self_attention_layer_norm/scale/v_col
I0512 23:45:14.642753 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_15/pre_self_attention_layer_norm/scale/v_row
I0512 23:45:14.642784 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_15/self_attention/key/kernel/m
I0512 23:45:14.642816 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_15/self_attention/key/kernel/v
I0512 23:45:14.642848 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_15/self_attention/key/kernel/v_col
I0512 23:45:14.642879 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_15/self_attention/key/kernel/v_row
I0512 23:45:14.642911 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_15/self_attention/out/kernel/m
I0512 23:45:14.642943 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_15/self_attention/out/kernel/v
I0512 23:45:14.642975 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_15/self_attention/out/kernel/v_col
I0512 23:45:14.643006 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_15/self_attention/out/kernel/v_row
I0512 23:45:14.643038 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_15/self_attention/query/kernel/m
I0512 23:45:14.643069 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_15/self_attention/query/kernel/v
I0512 23:45:14.643139 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_15/self_attention/query/kernel/v_col
I0512 23:45:14.643178 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_15/self_attention/query/kernel/v_row
I0512 23:45:14.643533 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_4/self_attention/out/kernel/v
I0512 23:45:14.643563 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_4/self_attention/out/kernel/v_col
I0512 23:45:14.643592 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_4/self_attention/out/kernel/v_row
I0512 23:45:14.643621 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_4/self_attention/query/kernel/m
I0512 23:45:14.643651 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_4/self_attention/query/kernel/v
I0512 23:45:14.643680 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_4/self_attention/query/kernel/v_col
I0512 23:45:14.643712 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_4/self_attention/query/kernel/v_row
I0512 23:45:14.643743 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_4/self_attention/value/kernel/m
I0512 23:45:14.643771 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_4/self_attention/value/kernel/v
I0512 23:45:14.643800 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_4/self_attention/value/kernel/v_col
I0512 23:45:14.643830 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_4/self_attention/value/kernel/v_row
I0512 23:45:14.643859 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/extra_mlp/wi_0/kernel/m
I0512 23:45:14.643888 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/extra_mlp/wi_0/kernel/v
I0512 23:45:14.643917 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/extra_mlp/wi_0/kernel/v_col
I0512 23:45:14.643218 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_15/self_attention/value/kernel/m
I0512 23:45:14.643257 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_15/self_attention/value/kernel/v
I0512 23:45:14.643291 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_15/self_attention/value/kernel/v_col
I0512 23:45:14.643324 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_15/self_attention/value/kernel/v_row
I0512 23:45:14.643362 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_16/mlp/wi_0/kernel/m
I0512 23:45:14.643395 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_16/mlp/wi_0/kernel/v
I0512 23:45:14.643427 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_16/mlp/wi_0/kernel/v_col
I0512 23:45:14.643459 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_16/mlp/wi_0/kernel/v_row
I0512 23:45:14.643493 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_16/mlp/wi_1/kernel/m
I0512 23:45:14.643525 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_16/mlp/wi_1/kernel/v
I0512 23:45:14.643556 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_16/mlp/wi_1/kernel/v_col
I0512 23:45:14.643589 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_16/mlp/wi_1/kernel/v_row
I0512 23:45:14.643621 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_16/mlp/wo/kernel/m
I0512 23:45:14.643653 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_16/mlp/wo/kernel/v
I0512 23:45:14.643684 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_16/mlp/wo/kernel/v_col
I0512 23:45:14.643978 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/extra_mlp/wi_0/kernel/v_row
I0512 23:45:14.644009 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/extra_mlp/wi_1/kernel/m
I0512 23:45:14.644039 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/extra_mlp/wi_1/kernel/v
I0512 23:45:14.644068 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/extra_mlp/wi_1/kernel/v_col
I0512 23:45:14.644098 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/extra_mlp/wi_1/kernel/v_row
I0512 23:45:14.644127 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/extra_mlp/wo/kernel/m
I0512 23:45:14.644156 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/extra_mlp/wo/kernel/v
I0512 23:45:14.644186 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/extra_mlp/wo/kernel/v_col
I0512 23:45:14.644215 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/extra_mlp/wo/kernel/v_row
I0512 23:45:14.644246 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/mlp/expert/wi_0/kernel/m
I0512 23:45:14.644277 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/mlp/expert/wi_0/kernel/v
I0512 23:45:14.644306 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/mlp/expert/wi_0/kernel/v_col
I0512 23:45:14.644336 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/mlp/expert/wi_0/kernel/v_row
I0512 23:45:14.644365 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/mlp/expert/wi_1/kernel/m
I0512 23:45:14.644395 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/mlp/expert/wi_1/kernel/v
I0512 23:45:14.643716 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_16/mlp/wo/kernel/v_row
I0512 23:45:14.643747 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_16/pre_mlp_layer_norm/scale/m
I0512 23:45:14.643779 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_16/pre_mlp_layer_norm/scale/v
I0512 23:45:14.643810 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_16/pre_mlp_layer_norm/scale/v_col
I0512 23:45:14.643844 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_16/pre_mlp_layer_norm/scale/v_row
I0512 23:45:14.643876 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_16/pre_self_attention_layer_norm/scale/m
I0512 23:45:14.643908 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_16/pre_self_attention_layer_norm/scale/v
I0512 23:45:14.643940 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_16/pre_self_attention_layer_norm/scale/v_col
I0512 23:45:14.643971 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_16/pre_self_attention_layer_norm/scale/v_row
I0512 23:45:14.644003 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_16/self_attention/key/kernel/m
I0512 23:45:14.644034 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_16/self_attention/key/kernel/v
I0512 23:45:14.644065 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_16/self_attention/key/kernel/v_col
I0512 23:45:14.644096 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_16/self_attention/key/kernel/v_row
I0512 23:45:14.644128 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_16/self_attention/out/kernel/m
I0512 23:45:14.644424 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/mlp/expert/wi_1/kernel/v_col
I0512 23:45:14.644454 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/mlp/expert/wi_1/kernel/v_row
I0512 23:45:14.644488 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/mlp/expert/wo/kernel/m
I0512 23:45:14.644519 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/mlp/expert/wo/kernel/v
I0512 23:45:14.644549 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/mlp/expert/wo/kernel/v_col
I0512 23:45:14.644578 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/mlp/expert/wo/kernel/v_row
I0512 23:45:14.644607 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/mlp/router/router_weights/w/kernel/m
I0512 23:45:14.644636 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/mlp/router/router_weights/w/kernel/v
I0512 23:45:14.644666 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/mlp/router/router_weights/w/kernel/v_col
I0512 23:45:14.644695 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/mlp/router/router_weights/w/kernel/v_row
I0512 23:45:14.644725 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/pre_extra_mlp_layer_norm/scale/m
I0512 23:45:14.644754 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/pre_extra_mlp_layer_norm/scale/v
I0512 23:45:14.644784 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/pre_extra_mlp_layer_norm/scale/v_col
I0512 23:45:14.644813 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/pre_extra_mlp_layer_norm/scale/v_row
I0512 23:45:14.644842 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/pre_mlp_layer_norm/scale/m
I0512 23:45:14.644159 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_16/self_attention/out/kernel/v
I0512 23:45:14.644190 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_16/self_attention/out/kernel/v_col
I0512 23:45:14.644227 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_16/self_attention/out/kernel/v_row
I0512 23:45:14.644260 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_16/self_attention/query/kernel/m
I0512 23:45:14.644291 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_16/self_attention/query/kernel/v
I0512 23:45:14.644322 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_16/self_attention/query/kernel/v_col
I0512 23:45:14.644353 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_16/self_attention/query/kernel/v_row
I0512 23:45:14.644384 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_16/self_attention/value/kernel/m
I0512 23:45:14.644416 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_16/self_attention/value/kernel/v
I0512 23:45:14.644447 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_16/self_attention/value/kernel/v_col
I0512 23:45:14.644479 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_16/self_attention/value/kernel/v_row
I0512 23:45:14.644511 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/extra_mlp/wi_0/kernel/m
I0512 23:45:14.644543 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/extra_mlp/wi_0/kernel/v
I0512 23:45:14.644574 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/extra_mlp/wi_0/kernel/v_col
I0512 23:45:14.644605 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/extra_mlp/wi_0/kernel/v_row
I0512 23:45:14.644872 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/pre_mlp_layer_norm/scale/v
I0512 23:45:14.644901 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/pre_mlp_layer_norm/scale/v_col
I0512 23:45:14.644931 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/pre_mlp_layer_norm/scale/v_row
I0512 23:45:14.644960 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/pre_self_attention_layer_norm/scale/m
I0512 23:45:14.644989 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/pre_self_attention_layer_norm/scale/v
I0512 23:45:14.645018 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/pre_self_attention_layer_norm/scale/v_col
I0512 23:45:14.645048 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/pre_self_attention_layer_norm/scale/v_row
I0512 23:45:14.645077 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/self_attention/key/kernel/m
I0512 23:45:14.645107 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/self_attention/key/kernel/v
I0512 23:45:14.645136 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/self_attention/key/kernel/v_col
I0512 23:45:14.645166 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/self_attention/key/kernel/v_row
I0512 23:45:14.645195 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/self_attention/out/kernel/m
I0512 23:45:14.645224 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/self_attention/out/kernel/v
I0512 23:45:14.645256 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/self_attention/out/kernel/v_col
I0512 23:45:14.644637 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/extra_mlp/wi_1/kernel/m
I0512 23:45:14.644668 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/extra_mlp/wi_1/kernel/v
I0512 23:45:14.644699 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/extra_mlp/wi_1/kernel/v_col
I0512 23:45:14.644730 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/extra_mlp/wi_1/kernel/v_row
I0512 23:45:14.644761 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/extra_mlp/wo/kernel/m
I0512 23:45:14.644792 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/extra_mlp/wo/kernel/v
I0512 23:45:14.644824 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/extra_mlp/wo/kernel/v_col
I0512 23:45:14.644855 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/extra_mlp/wo/kernel/v_row
I0512 23:45:14.644886 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/mlp/expert/wi_0/kernel/m
I0512 23:45:14.644917 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/mlp/expert/wi_0/kernel/v
I0512 23:45:14.644949 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/mlp/expert/wi_0/kernel/v_col
I0512 23:45:14.644980 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/mlp/expert/wi_0/kernel/v_row
I0512 23:45:14.645011 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/mlp/expert/wi_1/kernel/m
I0512 23:45:14.645042 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/mlp/expert/wi_1/kernel/v
I0512 23:45:14.645073 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/mlp/expert/wi_1/kernel/v_col
I0512 23:45:14.645286 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/self_attention/out/kernel/v_row
I0512 23:45:14.645315 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/self_attention/query/kernel/m
I0512 23:45:14.645345 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/self_attention/query/kernel/v
I0512 23:45:14.645374 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/self_attention/query/kernel/v_col
I0512 23:45:14.645403 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/self_attention/query/kernel/v_row
I0512 23:45:14.645433 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/self_attention/value/kernel/m
I0512 23:45:14.645462 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/self_attention/value/kernel/v
I0512 23:45:14.645496 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/self_attention/value/kernel/v_col
I0512 23:45:14.645527 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/self_attention/value/kernel/v_row
I0512 23:45:14.645556 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_6/mlp/wi_0/kernel/m
I0512 23:45:14.645586 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_6/mlp/wi_0/kernel/v
I0512 23:45:14.645614 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_6/mlp/wi_0/kernel/v_col
I0512 23:45:14.645644 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_6/mlp/wi_0/kernel/v_row
I0512 23:45:14.645673 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_6/mlp/wi_1/kernel/m
I0512 23:45:14.645703 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_6/mlp/wi_1/kernel/v
I0512 23:45:14.645105 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/mlp/expert/wi_1/kernel/v_row
I0512 23:45:14.645136 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/mlp/expert/wo/kernel/m
I0512 23:45:14.645167 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/mlp/expert/wo/kernel/v
I0512 23:45:14.645199 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/mlp/expert/wo/kernel/v_col
I0512 23:45:14.645236 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/mlp/expert/wo/kernel/v_row
I0512 23:45:14.645268 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/mlp/router/router_weights/w/kernel/m
I0512 23:45:14.645300 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/mlp/router/router_weights/w/kernel/v
I0512 23:45:14.645331 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/mlp/router/router_weights/w/kernel/v_col
I0512 23:45:14.645362 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/mlp/router/router_weights/w/kernel/v_row
I0512 23:45:14.645393 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/pre_extra_mlp_layer_norm/scale/m
I0512 23:45:14.645424 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/pre_extra_mlp_layer_norm/scale/v
I0512 23:45:14.645456 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/pre_extra_mlp_layer_norm/scale/v_col
I0512 23:45:14.645489 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/pre_extra_mlp_layer_norm/scale/v_row
I0512 23:45:14.645521 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/pre_mlp_layer_norm/scale/m
I0512 23:45:14.645732 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_6/mlp/wi_1/kernel/v_col
I0512 23:45:14.645761 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_6/mlp/wi_1/kernel/v_row
I0512 23:45:14.645791 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_6/mlp/wo/kernel/m
I0512 23:45:14.645820 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_6/mlp/wo/kernel/v
I0512 23:45:14.645849 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_6/mlp/wo/kernel/v_col
I0512 23:45:14.645879 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_6/mlp/wo/kernel/v_row
I0512 23:45:14.645908 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_6/pre_mlp_layer_norm/scale/m
I0512 23:45:14.645937 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_6/pre_mlp_layer_norm/scale/v
I0512 23:45:14.645966 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_6/pre_mlp_layer_norm/scale/v_col
I0512 23:45:14.645996 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_6/pre_mlp_layer_norm/scale/v_row
I0512 23:45:14.646025 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_6/pre_self_attention_layer_norm/scale/m
I0512 23:45:14.646054 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_6/pre_self_attention_layer_norm/scale/v
I0512 23:45:14.646084 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_6/pre_self_attention_layer_norm/scale/v_col
I0512 23:45:14.646113 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_6/pre_self_attention_layer_norm/scale/v_row
I0512 23:45:14.646143 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_6/self_attention/key/kernel/m
I0512 23:45:14.645552 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/pre_mlp_layer_norm/scale/v
I0512 23:45:14.645583 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/pre_mlp_layer_norm/scale/v_col
I0512 23:45:14.645615 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/pre_mlp_layer_norm/scale/v_row
I0512 23:45:14.645646 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/pre_self_attention_layer_norm/scale/m
I0512 23:45:14.645677 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/pre_self_attention_layer_norm/scale/v
I0512 23:45:14.645708 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/pre_self_attention_layer_norm/scale/v_col
I0512 23:45:14.645739 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/pre_self_attention_layer_norm/scale/v_row
I0512 23:45:14.645770 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/self_attention/key/kernel/m
I0512 23:45:14.645801 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/self_attention/key/kernel/v
I0512 23:45:14.645834 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/self_attention/key/kernel/v_col
I0512 23:45:14.645869 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/self_attention/key/kernel/v_row
I0512 23:45:14.645900 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/self_attention/out/kernel/m
I0512 23:45:14.645931 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/self_attention/out/kernel/v
I0512 23:45:14.645962 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/self_attention/out/kernel/v_col
I0512 23:45:14.645993 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/self_attention/out/kernel/v_row
I0512 23:45:14.646172 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_6/self_attention/key/kernel/v
I0512 23:45:14.646201 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_6/self_attention/key/kernel/v_col
I0512 23:45:14.646231 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_6/self_attention/key/kernel/v_row
I0512 23:45:14.646262 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_6/self_attention/out/kernel/m
I0512 23:45:14.646292 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_6/self_attention/out/kernel/v
I0512 23:45:14.646321 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_6/self_attention/out/kernel/v_col
I0512 23:45:14.646350 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_6/self_attention/out/kernel/v_row
I0512 23:45:14.646380 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_6/self_attention/query/kernel/m
I0512 23:45:14.646409 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_6/self_attention/query/kernel/v
I0512 23:45:14.646439 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_6/self_attention/query/kernel/v_col
I0512 23:45:14.646468 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_6/self_attention/query/kernel/v_row
I0512 23:45:14.646502 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_6/self_attention/value/kernel/m
I0512 23:45:14.646533 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_6/self_attention/value/kernel/v
I0512 23:45:14.646563 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_6/self_attention/value/kernel/v_col
I0512 23:45:14.646592 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_6/self_attention/value/kernel/v_row
I0512 23:45:14.646024 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/self_attention/query/kernel/m
I0512 23:45:14.646055 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/self_attention/query/kernel/v
I0512 23:45:14.646086 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/self_attention/query/kernel/v_col
I0512 23:45:14.646117 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/self_attention/query/kernel/v_row
I0512 23:45:14.646148 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/self_attention/value/kernel/m
I0512 23:45:14.646179 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/self_attention/value/kernel/v
I0512 23:45:14.646210 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/self_attention/value/kernel/v_col
I0512 23:45:14.646248 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/self_attention/value/kernel/v_row
I0512 23:45:14.646280 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_18/mlp/wi_0/kernel/m
I0512 23:45:14.646311 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_18/mlp/wi_0/kernel/v
I0512 23:45:14.646342 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_18/mlp/wi_0/kernel/v_col
I0512 23:45:14.646373 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_18/mlp/wi_0/kernel/v_row
I0512 23:45:14.646405 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_18/mlp/wi_1/kernel/m
I0512 23:45:14.646436 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_18/mlp/wi_1/kernel/v
I0512 23:45:14.646621 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_7/mlp/wi_0/kernel/m
I0512 23:45:14.646651 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_7/mlp/wi_0/kernel/v
I0512 23:45:14.646680 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_7/mlp/wi_0/kernel/v_col
I0512 23:45:14.646709 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_7/mlp/wi_0/kernel/v_row
I0512 23:45:14.646739 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_7/mlp/wi_1/kernel/m
I0512 23:45:14.646769 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_7/mlp/wi_1/kernel/v
I0512 23:45:14.646798 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_7/mlp/wi_1/kernel/v_col
I0512 23:45:14.646827 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_7/mlp/wi_1/kernel/v_row
I0512 23:45:14.646857 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_7/mlp/wo/kernel/m
I0512 23:45:14.646886 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_7/mlp/wo/kernel/v
I0512 23:45:14.646915 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_7/mlp/wo/kernel/v_col
I0512 23:45:14.646944 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_7/mlp/wo/kernel/v_row
I0512 23:45:14.646974 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_7/pre_mlp_layer_norm/scale/m
I0512 23:45:14.647004 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_7/pre_mlp_layer_norm/scale/v
I0512 23:45:14.647033 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_7/pre_mlp_layer_norm/scale/v_col
I0512 23:45:14.646469 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_18/mlp/wi_1/kernel/v_col
I0512 23:45:14.646502 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_18/mlp/wi_1/kernel/v_row
I0512 23:45:14.646534 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_18/mlp/wo/kernel/m
I0512 23:45:14.646565 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_18/mlp/wo/kernel/v
I0512 23:45:14.646597 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_18/mlp/wo/kernel/v_col
I0512 23:45:14.646628 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_18/mlp/wo/kernel/v_row
I0512 23:45:14.646660 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_18/pre_mlp_layer_norm/scale/m
I0512 23:45:14.646691 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_18/pre_mlp_layer_norm/scale/v
I0512 23:45:14.646722 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_18/pre_mlp_layer_norm/scale/v_col
I0512 23:45:14.646753 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_18/pre_mlp_layer_norm/scale/v_row
I0512 23:45:14.646785 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_18/pre_self_attention_layer_norm/scale/m
I0512 23:45:14.646816 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_18/pre_self_attention_layer_norm/scale/v
I0512 23:45:14.646847 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_18/pre_self_attention_layer_norm/scale/v_col
I0512 23:45:14.646879 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_18/pre_self_attention_layer_norm/scale/v_row
I0512 23:45:14.646910 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_18/self_attention/key/kernel/m
I0512 23:45:14.647062 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_7/pre_mlp_layer_norm/scale/v_row
I0512 23:45:14.647113 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_7/pre_self_attention_layer_norm/scale/m
I0512 23:45:14.647147 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_7/pre_self_attention_layer_norm/scale/v
I0512 23:45:14.647177 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_7/pre_self_attention_layer_norm/scale/v_col
I0512 23:45:14.647207 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_7/pre_self_attention_layer_norm/scale/v_row
I0512 23:45:14.647237 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_7/self_attention/key/kernel/m
I0512 23:45:14.647268 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_7/self_attention/key/kernel/v
I0512 23:45:14.647298 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_7/self_attention/key/kernel/v_col
I0512 23:45:14.647327 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_7/self_attention/key/kernel/v_row
I0512 23:45:14.647356 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_7/self_attention/out/kernel/m
I0512 23:45:14.647386 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_7/self_attention/out/kernel/v
I0512 23:45:14.647415 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_7/self_attention/out/kernel/v_col
I0512 23:45:14.647445 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_7/self_attention/out/kernel/v_row
I0512 23:45:14.647475 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_7/self_attention/query/kernel/m
I0512 23:45:14.647510 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_7/self_attention/query/kernel/v
I0512 23:45:14.646942 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_18/self_attention/key/kernel/v
I0512 23:45:14.646973 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_18/self_attention/key/kernel/v_col
I0512 23:45:14.647004 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_18/self_attention/key/kernel/v_row
I0512 23:45:14.647036 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_18/self_attention/out/kernel/m
I0512 23:45:14.647067 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_18/self_attention/out/kernel/v
I0512 23:45:14.647121 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_18/self_attention/out/kernel/v_col
I0512 23:45:14.647155 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_18/self_attention/out/kernel/v_row
I0512 23:45:14.647187 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_18/self_attention/query/kernel/m
I0512 23:45:14.647223 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_18/self_attention/query/kernel/v
I0512 23:45:14.647255 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_18/self_attention/query/kernel/v_col
I0512 23:45:14.647287 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_18/self_attention/query/kernel/v_row
I0512 23:45:14.647318 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_18/self_attention/value/kernel/m
I0512 23:45:14.647349 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_18/self_attention/value/kernel/v
I0512 23:45:14.647380 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_18/self_attention/value/kernel/v_col
I0512 23:45:14.647411 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_18/self_attention/value/kernel/v_row
I0512 23:45:14.647540 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_7/self_attention/query/kernel/v_col
I0512 23:45:14.647569 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_7/self_attention/query/kernel/v_row
I0512 23:45:14.647598 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_7/self_attention/value/kernel/m
I0512 23:45:14.647628 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_7/self_attention/value/kernel/v
I0512 23:45:14.647657 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_7/self_attention/value/kernel/v_col
I0512 23:45:14.647686 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_7/self_attention/value/kernel/v_row
I0512 23:45:14.647716 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_8/mlp/wi_0/kernel/m
I0512 23:45:14.647745 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_8/mlp/wi_0/kernel/v
I0512 23:45:14.647774 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_8/mlp/wi_0/kernel/v_col
I0512 23:45:14.647804 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_8/mlp/wi_0/kernel/v_row
I0512 23:45:14.647833 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_8/mlp/wi_1/kernel/m
I0512 23:45:14.647862 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_8/mlp/wi_1/kernel/v
I0512 23:45:14.647891 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_8/mlp/wi_1/kernel/v_col
I0512 23:45:14.647921 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_8/mlp/wi_1/kernel/v_row
I0512 23:45:14.647950 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_8/mlp/wo/kernel/m
I0512 23:45:14.647442 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_19/mlp/wi_0/kernel/m
I0512 23:45:14.647475 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_19/mlp/wi_0/kernel/v
I0512 23:45:14.647507 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_19/mlp/wi_0/kernel/v_col
I0512 23:45:14.647539 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_19/mlp/wi_0/kernel/v_row
I0512 23:45:14.647570 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_19/mlp/wi_1/kernel/m
I0512 23:45:14.647601 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_19/mlp/wi_1/kernel/v
I0512 23:45:14.647633 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_19/mlp/wi_1/kernel/v_col
I0512 23:45:14.647664 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_19/mlp/wi_1/kernel/v_row
I0512 23:45:14.647695 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_19/mlp/wo/kernel/m
I0512 23:45:14.647726 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_19/mlp/wo/kernel/v
I0512 23:45:14.647758 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_19/mlp/wo/kernel/v_col
I0512 23:45:14.647789 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_19/mlp/wo/kernel/v_row
I0512 23:45:14.647820 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_19/pre_mlp_layer_norm/scale/m
I0512 23:45:14.647851 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_19/pre_mlp_layer_norm/scale/v
I0512 23:45:14.647883 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_19/pre_mlp_layer_norm/scale/v_col
I0512 23:45:14.647979 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_8/mlp/wo/kernel/v
I0512 23:45:14.648009 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_8/mlp/wo/kernel/v_col
I0512 23:45:14.648038 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_8/mlp/wo/kernel/v_row
I0512 23:45:14.648068 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_8/pre_mlp_layer_norm/scale/m
I0512 23:45:14.648097 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_8/pre_mlp_layer_norm/scale/v
I0512 23:45:14.648127 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_8/pre_mlp_layer_norm/scale/v_col
I0512 23:45:14.648156 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_8/pre_mlp_layer_norm/scale/v_row
I0512 23:45:14.648185 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_8/pre_self_attention_layer_norm/scale/m
I0512 23:45:14.648215 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_8/pre_self_attention_layer_norm/scale/v
I0512 23:45:14.648246 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_8/pre_self_attention_layer_norm/scale/v_col
I0512 23:45:14.648277 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_8/pre_self_attention_layer_norm/scale/v_row
I0512 23:45:14.648306 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_8/self_attention/key/kernel/m
I0512 23:45:14.648336 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_8/self_attention/key/kernel/v
I0512 23:45:14.648365 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_8/self_attention/key/kernel/v_col
I0512 23:45:14.648395 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_8/self_attention/key/kernel/v_row
I0512 23:45:14.647914 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_19/pre_mlp_layer_norm/scale/v_row
I0512 23:45:14.647945 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_19/pre_self_attention_layer_norm/scale/m
I0512 23:45:14.647976 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_19/pre_self_attention_layer_norm/scale/v
I0512 23:45:14.648007 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_19/pre_self_attention_layer_norm/scale/v_col
I0512 23:45:14.648038 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_19/pre_self_attention_layer_norm/scale/v_row
I0512 23:45:14.648069 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_19/self_attention/key/kernel/m
I0512 23:45:14.648101 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_19/self_attention/key/kernel/v
I0512 23:45:14.648132 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_19/self_attention/key/kernel/v_col
I0512 23:45:14.648163 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_19/self_attention/key/kernel/v_row
I0512 23:45:14.648194 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_19/self_attention/out/kernel/m
I0512 23:45:14.648232 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_19/self_attention/out/kernel/v
I0512 23:45:14.648264 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_19/self_attention/out/kernel/v_col
I0512 23:45:14.648296 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_19/self_attention/out/kernel/v_row
I0512 23:45:14.648327 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_19/self_attention/query/kernel/m
I0512 23:45:14.648424 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_8/self_attention/out/kernel/m
I0512 23:45:14.648453 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_8/self_attention/out/kernel/v
I0512 23:45:14.648487 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_8/self_attention/out/kernel/v_col
I0512 23:45:14.648518 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_8/self_attention/out/kernel/v_row
I0512 23:45:14.648548 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_8/self_attention/query/kernel/m
I0512 23:45:14.648577 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_8/self_attention/query/kernel/v
I0512 23:45:14.648607 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_8/self_attention/query/kernel/v_col
I0512 23:45:14.648636 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_8/self_attention/query/kernel/v_row
I0512 23:45:14.648665 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_8/self_attention/value/kernel/m
I0512 23:45:14.648694 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_8/self_attention/value/kernel/v
I0512 23:45:14.648723 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_8/self_attention/value/kernel/v_col
I0512 23:45:14.648752 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_8/self_attention/value/kernel/v_row
I0512 23:45:14.648782 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_9/mlp/wi_0/kernel/m
I0512 23:45:14.648811 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_9/mlp/wi_0/kernel/v
I0512 23:45:14.648841 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_9/mlp/wi_0/kernel/v_col
I0512 23:45:14.648358 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_19/self_attention/query/kernel/v
I0512 23:45:14.648389 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_19/self_attention/query/kernel/v_col
I0512 23:45:14.648420 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_19/self_attention/query/kernel/v_row
I0512 23:45:14.648451 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_19/self_attention/value/kernel/m
I0512 23:45:14.648484 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_19/self_attention/value/kernel/v
I0512 23:45:14.648516 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_19/self_attention/value/kernel/v_col
I0512 23:45:14.648547 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_19/self_attention/value/kernel/v_row
I0512 23:45:14.648578 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_2/mlp/wi_0/kernel/m
I0512 23:45:14.648609 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_2/mlp/wi_0/kernel/v
I0512 23:45:14.648640 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_2/mlp/wi_0/kernel/v_col
I0512 23:45:14.648672 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_2/mlp/wi_0/kernel/v_row
I0512 23:45:14.648703 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_2/mlp/wi_1/kernel/m
I0512 23:45:14.648734 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_2/mlp/wi_1/kernel/v
I0512 23:45:14.648766 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_2/mlp/wi_1/kernel/v_col
I0512 23:45:14.648797 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_2/mlp/wi_1/kernel/v_row
I0512 23:45:14.648870 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_9/mlp/wi_0/kernel/v_row
I0512 23:45:14.648899 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_9/mlp/wi_1/kernel/m
I0512 23:45:14.648929 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_9/mlp/wi_1/kernel/v
I0512 23:45:14.648958 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_9/mlp/wi_1/kernel/v_col
I0512 23:45:14.648988 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_9/mlp/wi_1/kernel/v_row
I0512 23:45:14.649017 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_9/mlp/wo/kernel/m
I0512 23:45:14.649047 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_9/mlp/wo/kernel/v
I0512 23:45:14.649076 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_9/mlp/wo/kernel/v_col
I0512 23:45:14.649106 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_9/mlp/wo/kernel/v_row
I0512 23:45:14.649135 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_9/pre_mlp_layer_norm/scale/m
I0512 23:45:14.649165 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_9/pre_mlp_layer_norm/scale/v
I0512 23:45:14.649194 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_9/pre_mlp_layer_norm/scale/v_col
I0512 23:45:14.649224 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_9/pre_mlp_layer_norm/scale/v_row
I0512 23:45:14.649256 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_9/pre_self_attention_layer_norm/scale/m
I0512 23:45:14.649286 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_9/pre_self_attention_layer_norm/scale/v
I0512 23:45:14.649315 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_9/pre_self_attention_layer_norm/scale/v_col
I0512 23:45:14.649345 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_9/pre_self_attention_layer_norm/scale/v_row
I0512 23:45:14.649374 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_9/self_attention/key/kernel/m
I0512 23:45:14.649404 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_9/self_attention/key/kernel/v
I0512 23:45:14.649434 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_9/self_attention/key/kernel/v_col
I0512 23:45:14.649463 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_9/self_attention/key/kernel/v_row
I0512 23:45:14.649497 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_9/self_attention/out/kernel/m
I0512 23:45:14.649528 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_9/self_attention/out/kernel/v
I0512 23:45:14.649557 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_9/self_attention/out/kernel/v_col
I0512 23:45:14.649586 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_9/self_attention/out/kernel/v_row
I0512 23:45:14.649615 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_9/self_attention/query/kernel/m
I0512 23:45:14.649645 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_9/self_attention/query/kernel/v
I0512 23:45:14.649674 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_9/self_attention/query/kernel/v_col
I0512 23:45:14.649703 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_9/self_attention/query/kernel/v_row
I0512 23:45:14.648827 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_2/mlp/wo/kernel/m
I0512 23:45:14.648858 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_2/mlp/wo/kernel/v
I0512 23:45:14.648889 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_2/mlp/wo/kernel/v_col
I0512 23:45:14.648920 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_2/mlp/wo/kernel/v_row
I0512 23:45:14.648951 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_2/pre_mlp_layer_norm/scale/m
I0512 23:45:14.648982 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_2/pre_mlp_layer_norm/scale/v
I0512 23:45:14.649013 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_2/pre_mlp_layer_norm/scale/v_col
I0512 23:45:14.649044 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_2/pre_mlp_layer_norm/scale/v_row
I0512 23:45:14.649075 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_2/pre_self_attention_layer_norm/scale/m
I0512 23:45:14.649106 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_2/pre_self_attention_layer_norm/scale/v
I0512 23:45:14.649138 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_2/pre_self_attention_layer_norm/scale/v_col
I0512 23:45:14.649169 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_2/pre_self_attention_layer_norm/scale/v_row
I0512 23:45:14.649201 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_2/self_attention/key/kernel/m
I0512 23:45:14.649239 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_2/self_attention/key/kernel/v
I0512 23:45:14.649271 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_2/self_attention/key/kernel/v_col
I0512 23:45:14.649733 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_9/self_attention/value/kernel/m
I0512 23:45:14.649762 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_9/self_attention/value/kernel/v
I0512 23:45:14.649791 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_9/self_attention/value/kernel/v_col
I0512 23:45:14.649821 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_9/self_attention/value/kernel/v_row
I0512 23:45:14.649850 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/logits_dense/kernel/m
I0512 23:45:14.649879 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/logits_dense/kernel/v
I0512 23:45:14.649908 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/logits_dense/kernel/v_col
I0512 23:45:14.649938 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/logits_dense/kernel/v_row
I0512 23:45:14.649967 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/token_embedder/embedding/m
I0512 23:45:14.649996 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/token_embedder/embedding/v
I0512 23:45:14.650026 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/token_embedder/embedding/v_col
I0512 23:45:14.650055 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/param_states/token_embedder/embedding/v_row
I0512 23:45:14.650084 139876724979712 checkpoints.py:1100] Restoring key from ckpt: state/step
I0512 23:45:14.650115 139876724979712 checkpoints.py:1100] Restoring key from ckpt: target/decoder/decoder_norm/scale
I0512 23:45:14.650145 139876724979712 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_0/mlp/wi_0/kernel
I0512 23:45:14.650174 139876724979712 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_0/mlp/wi_1/kernel
I0512 23:45:14.650204 139876724979712 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_0/mlp/wo/kernel
I0512 23:45:14.649302 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_2/self_attention/key/kernel/v_row
I0512 23:45:14.649333 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_2/self_attention/out/kernel/m
I0512 23:45:14.649365 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_2/self_attention/out/kernel/v
I0512 23:45:14.649396 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_2/self_attention/out/kernel/v_col
I0512 23:45:14.649427 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_2/self_attention/out/kernel/v_row
I0512 23:45:14.649458 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_2/self_attention/query/kernel/m
I0512 23:45:14.649492 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_2/self_attention/query/kernel/v
I0512 23:45:14.649524 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_2/self_attention/query/kernel/v_col
I0512 23:45:14.649555 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_2/self_attention/query/kernel/v_row
I0512 23:45:14.649586 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_2/self_attention/value/kernel/m
I0512 23:45:14.649617 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_2/self_attention/value/kernel/v
I0512 23:45:14.649648 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_2/self_attention/value/kernel/v_col
I0512 23:45:14.649679 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_2/self_attention/value/kernel/v_row
I0512 23:45:14.649710 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_20/mlp/wi_0/kernel/m
I0512 23:45:14.649741 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_20/mlp/wi_0/kernel/v
I0512 23:45:14.650234 139876724979712 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_0/pre_mlp_layer_norm/scale
I0512 23:45:14.650265 139876724979712 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_0/pre_self_attention_layer_norm/scale
I0512 23:45:14.650295 139876724979712 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_0/self_attention/key/kernel
I0512 23:45:14.650325 139876724979712 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_0/self_attention/out/kernel
I0512 23:45:14.650354 139876724979712 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_0/self_attention/query/kernel
I0512 23:45:14.650384 139876724979712 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_0/self_attention/value/kernel
I0512 23:45:14.650413 139876724979712 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_1/mlp/wi_0/kernel
I0512 23:45:14.650443 139876724979712 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_1/mlp/wi_1/kernel
I0512 23:45:14.650472 139876724979712 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_1/mlp/wo/kernel
I0512 23:45:14.650507 139876724979712 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_1/pre_mlp_layer_norm/scale
I0512 23:45:14.650537 139876724979712 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_1/pre_self_attention_layer_norm/scale
I0512 23:45:14.650567 139876724979712 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_1/self_attention/key/kernel
I0512 23:45:14.650597 139876724979712 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_1/self_attention/out/kernel
I0512 23:45:14.650626 139876724979712 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_1/self_attention/query/kernel
I0512 23:45:14.650655 139876724979712 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_1/self_attention/value/kernel
I0512 23:45:14.650685 139876724979712 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_10/mlp/wi_0/kernel
I0512 23:45:14.649773 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_20/mlp/wi_0/kernel/v_col
I0512 23:45:14.649804 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_20/mlp/wi_0/kernel/v_row
I0512 23:45:14.649835 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_20/mlp/wi_1/kernel/m
I0512 23:45:14.649866 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_20/mlp/wi_1/kernel/v
I0512 23:45:14.649898 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_20/mlp/wi_1/kernel/v_col
I0512 23:45:14.649929 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_20/mlp/wi_1/kernel/v_row
I0512 23:45:14.649960 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_20/mlp/wo/kernel/m
I0512 23:45:14.649991 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_20/mlp/wo/kernel/v
I0512 23:45:14.650022 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_20/mlp/wo/kernel/v_col
I0512 23:45:14.650053 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_20/mlp/wo/kernel/v_row
I0512 23:45:14.650084 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_20/pre_mlp_layer_norm/scale/m
I0512 23:45:14.650115 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_20/pre_mlp_layer_norm/scale/v
I0512 23:45:14.650147 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_20/pre_mlp_layer_norm/scale/v_col
I0512 23:45:14.650178 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_20/pre_mlp_layer_norm/scale/v_row
I0512 23:45:14.650209 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_20/pre_self_attention_layer_norm/scale/m
I0512 23:45:14.650714 139876724979712 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_10/mlp/wi_1/kernel
I0512 23:45:14.650744 139876724979712 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_10/mlp/wo/kernel
I0512 23:45:14.650773 139876724979712 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_10/pre_mlp_layer_norm/scale
I0512 23:45:14.650803 139876724979712 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_10/pre_self_attention_layer_norm/scale
I0512 23:45:14.650832 139876724979712 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_10/self_attention/key/kernel
I0512 23:45:14.650861 139876724979712 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_10/self_attention/out/kernel
I0512 23:45:14.650890 139876724979712 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_10/self_attention/query/kernel
I0512 23:45:14.650920 139876724979712 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_10/self_attention/value/kernel
I0512 23:45:14.650949 139876724979712 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_11/extra_mlp/wi_0/kernel
I0512 23:45:14.650979 139876724979712 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_11/extra_mlp/wi_1/kernel
I0512 23:45:14.651008 139876724979712 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_11/extra_mlp/wo/kernel
I0512 23:45:14.651037 139876724979712 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_11/mlp/expert/wi_0/kernel
I0512 23:45:14.651066 139876724979712 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_11/mlp/expert/wi_1/kernel
I0512 23:45:14.651125 139876724979712 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_11/mlp/expert/wo/kernel
I0512 23:45:14.651159 139876724979712 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_11/mlp/router/router_weights/w/kernel
I0512 23:45:14.651190 139876724979712 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_11/pre_extra_mlp_layer_norm/scale
I0512 23:45:14.650247 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_20/pre_self_attention_layer_norm/scale/v
I0512 23:45:14.650279 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_20/pre_self_attention_layer_norm/scale/v_col
I0512 23:45:14.650310 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_20/pre_self_attention_layer_norm/scale/v_row
I0512 23:45:14.650341 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_20/self_attention/key/kernel/m
I0512 23:45:14.650373 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_20/self_attention/key/kernel/v
I0512 23:45:14.650404 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_20/self_attention/key/kernel/v_col
I0512 23:45:14.650435 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_20/self_attention/key/kernel/v_row
I0512 23:45:14.650467 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_20/self_attention/out/kernel/m
I0512 23:45:14.650500 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_20/self_attention/out/kernel/v
I0512 23:45:14.650531 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_20/self_attention/out/kernel/v_col
I0512 23:45:14.650562 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_20/self_attention/out/kernel/v_row
I0512 23:45:14.650593 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_20/self_attention/query/kernel/m
I0512 23:45:14.650624 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_20/self_attention/query/kernel/v
I0512 23:45:14.650655 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_20/self_attention/query/kernel/v_col
I0512 23:45:14.651220 139876724979712 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_11/pre_mlp_layer_norm/scale
I0512 23:45:14.651251 139876724979712 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_11/pre_self_attention_layer_norm/scale
I0512 23:45:14.651283 139876724979712 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_11/self_attention/key/kernel
I0512 23:45:14.651314 139876724979712 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_11/self_attention/out/kernel
I0512 23:45:14.651344 139876724979712 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_11/self_attention/query/kernel
I0512 23:45:14.651373 139876724979712 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_11/self_attention/value/kernel
I0512 23:45:14.651403 139876724979712 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_12/mlp/wi_0/kernel
I0512 23:45:14.651432 139876724979712 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_12/mlp/wi_1/kernel
I0512 23:45:14.651462 139876724979712 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_12/mlp/wo/kernel
I0512 23:45:14.651497 139876724979712 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_12/pre_mlp_layer_norm/scale
I0512 23:45:14.651529 139876724979712 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_12/pre_self_attention_layer_norm/scale
I0512 23:45:14.651558 139876724979712 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_12/self_attention/key/kernel
I0512 23:45:14.651588 139876724979712 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_12/self_attention/out/kernel
I0512 23:45:14.651617 139876724979712 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_12/self_attention/query/kernel
I0512 23:45:14.651646 139876724979712 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_12/self_attention/value/kernel
I0512 23:45:14.651676 139876724979712 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_13/mlp/wi_0/kernel
I0512 23:45:14.650686 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_20/self_attention/query/kernel/v_row
I0512 23:45:14.650717 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_20/self_attention/value/kernel/m
I0512 23:45:14.650748 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_20/self_attention/value/kernel/v
I0512 23:45:14.650779 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_20/self_attention/value/kernel/v_col
I0512 23:45:14.650810 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_20/self_attention/value/kernel/v_row
I0512 23:45:14.650840 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_21/mlp/wi_0/kernel/m
I0512 23:45:14.650871 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_21/mlp/wi_0/kernel/v
I0512 23:45:14.650903 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_21/mlp/wi_0/kernel/v_col
I0512 23:45:14.650934 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_21/mlp/wi_0/kernel/v_row
I0512 23:45:14.650965 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_21/mlp/wi_1/kernel/m
I0512 23:45:14.650996 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_21/mlp/wi_1/kernel/v
I0512 23:45:14.651027 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_21/mlp/wi_1/kernel/v_col
I0512 23:45:14.651058 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_21/mlp/wi_1/kernel/v_row
I0512 23:45:14.651108 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_21/mlp/wo/kernel/m
I0512 23:45:14.651145 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_21/mlp/wo/kernel/v
I0512 23:45:14.651705 139876724979712 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_13/mlp/wi_1/kernel
I0512 23:45:14.651735 139876724979712 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_13/mlp/wo/kernel
I0512 23:45:14.651764 139876724979712 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_13/pre_mlp_layer_norm/scale
I0512 23:45:14.651794 139876724979712 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_13/pre_self_attention_layer_norm/scale
I0512 23:45:14.651823 139876724979712 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_13/self_attention/key/kernel
I0512 23:45:14.651853 139876724979712 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_13/self_attention/out/kernel
I0512 23:45:14.651882 139876724979712 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_13/self_attention/query/kernel
I0512 23:45:14.651911 139876724979712 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_13/self_attention/value/kernel
I0512 23:45:14.651940 139876724979712 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_14/mlp/wi_0/kernel
I0512 23:45:14.651970 139876724979712 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_14/mlp/wi_1/kernel
I0512 23:45:14.651999 139876724979712 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_14/mlp/wo/kernel
I0512 23:45:14.652029 139876724979712 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_14/pre_mlp_layer_norm/scale
I0512 23:45:14.652058 139876724979712 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_14/pre_self_attention_layer_norm/scale
I0512 23:45:14.652088 139876724979712 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_14/self_attention/key/kernel
I0512 23:45:14.652117 139876724979712 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_14/self_attention/out/kernel
I0512 23:45:14.652146 139876724979712 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_14/self_attention/query/kernel
I0512 23:45:14.651177 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_21/mlp/wo/kernel/v_col
I0512 23:45:14.651208 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_21/mlp/wo/kernel/v_row
I0512 23:45:14.651245 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_21/pre_mlp_layer_norm/scale/m
I0512 23:45:14.651277 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_21/pre_mlp_layer_norm/scale/v
I0512 23:45:14.651308 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_21/pre_mlp_layer_norm/scale/v_col
I0512 23:45:14.651339 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_21/pre_mlp_layer_norm/scale/v_row
I0512 23:45:14.651370 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_21/pre_self_attention_layer_norm/scale/m
I0512 23:45:14.651401 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_21/pre_self_attention_layer_norm/scale/v
I0512 23:45:14.651433 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_21/pre_self_attention_layer_norm/scale/v_col
I0512 23:45:14.651465 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_21/pre_self_attention_layer_norm/scale/v_row
I0512 23:45:14.651499 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_21/self_attention/key/kernel/m
I0512 23:45:14.651531 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_21/self_attention/key/kernel/v
I0512 23:45:14.651562 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_21/self_attention/key/kernel/v_col
I0512 23:45:14.651594 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_21/self_attention/key/kernel/v_row
I0512 23:45:14.651625 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_21/self_attention/out/kernel/m
I0512 23:45:14.652175 139876724979712 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_14/self_attention/value/kernel
I0512 23:45:14.652204 139876724979712 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_15/mlp/wi_0/kernel
I0512 23:45:14.652234 139876724979712 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_15/mlp/wi_1/kernel
I0512 23:45:14.652266 139876724979712 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_15/mlp/wo/kernel
I0512 23:45:14.652295 139876724979712 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_15/pre_mlp_layer_norm/scale
I0512 23:45:14.652325 139876724979712 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_15/pre_self_attention_layer_norm/scale
I0512 23:45:14.652354 139876724979712 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_15/self_attention/key/kernel
I0512 23:45:14.652384 139876724979712 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_15/self_attention/out/kernel
I0512 23:45:14.652413 139876724979712 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_15/self_attention/query/kernel
I0512 23:45:14.652442 139876724979712 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_15/self_attention/value/kernel
I0512 23:45:14.652471 139876724979712 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_16/mlp/wi_0/kernel
I0512 23:45:14.652507 139876724979712 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_16/mlp/wi_1/kernel
I0512 23:45:14.652537 139876724979712 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_16/mlp/wo/kernel
I0512 23:45:14.652566 139876724979712 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_16/pre_mlp_layer_norm/scale
I0512 23:45:14.652595 139876724979712 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_16/pre_self_attention_layer_norm/scale
I0512 23:45:14.652625 139876724979712 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_16/self_attention/key/kernel
I0512 23:45:14.651656 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_21/self_attention/out/kernel/v
I0512 23:45:14.651687 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_21/self_attention/out/kernel/v_col
I0512 23:45:14.651718 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_21/self_attention/out/kernel/v_row
I0512 23:45:14.651749 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_21/self_attention/query/kernel/m
I0512 23:45:14.651780 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_21/self_attention/query/kernel/v
I0512 23:45:14.651812 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_21/self_attention/query/kernel/v_col
I0512 23:45:14.651843 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_21/self_attention/query/kernel/v_row
I0512 23:45:14.651874 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_21/self_attention/value/kernel/m
I0512 23:45:14.651905 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_21/self_attention/value/kernel/v
I0512 23:45:14.651936 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_21/self_attention/value/kernel/v_col
I0512 23:45:14.651967 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_21/self_attention/value/kernel/v_row
I0512 23:45:14.651998 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_22/mlp/wi_0/kernel/m
I0512 23:45:14.652029 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_22/mlp/wi_0/kernel/v
I0512 23:45:14.652060 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_22/mlp/wi_0/kernel/v_col
I0512 23:45:14.652092 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_22/mlp/wi_0/kernel/v_row
I0512 23:45:14.652123 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_22/mlp/wi_1/kernel/m
I0512 23:45:14.652154 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_22/mlp/wi_1/kernel/v
I0512 23:45:14.652186 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_22/mlp/wi_1/kernel/v_col
I0512 23:45:14.652222 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_22/mlp/wi_1/kernel/v_row
I0512 23:45:14.652255 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_22/mlp/wo/kernel/m
I0512 23:45:14.652286 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_22/mlp/wo/kernel/v
I0512 23:45:14.652317 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_22/mlp/wo/kernel/v_col
I0512 23:45:14.652371 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_22/mlp/wo/kernel/v_row
I0512 23:45:14.652404 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_22/pre_mlp_layer_norm/scale/m
I0512 23:45:14.652436 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_22/pre_mlp_layer_norm/scale/v
I0512 23:45:14.652468 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_22/pre_mlp_layer_norm/scale/v_col
I0512 23:45:14.652501 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_22/pre_mlp_layer_norm/scale/v_row
I0512 23:45:14.652532 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_22/pre_self_attention_layer_norm/scale/m
I0512 23:45:14.652564 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_22/pre_self_attention_layer_norm/scale/v
I0512 23:45:14.652654 139876724979712 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_16/self_attention/out/kernel
I0512 23:45:14.652683 139876724979712 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_16/self_attention/query/kernel
I0512 23:45:14.652713 139876724979712 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_16/self_attention/value/kernel
I0512 23:45:14.652742 139876724979712 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_17/extra_mlp/wi_0/kernel
I0512 23:45:14.652771 139876724979712 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_17/extra_mlp/wi_1/kernel
I0512 23:45:14.652800 139876724979712 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_17/extra_mlp/wo/kernel
I0512 23:45:14.652829 139876724979712 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_17/mlp/expert/wi_0/kernel
I0512 23:45:14.652859 139876724979712 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_17/mlp/expert/wi_1/kernel
I0512 23:45:14.652888 139876724979712 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_17/mlp/expert/wo/kernel
I0512 23:45:14.652917 139876724979712 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_17/mlp/router/router_weights/w/kernel
I0512 23:45:14.652946 139876724979712 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_17/pre_extra_mlp_layer_norm/scale
I0512 23:45:14.652976 139876724979712 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_17/pre_mlp_layer_norm/scale
I0512 23:45:14.653005 139876724979712 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_17/pre_self_attention_layer_norm/scale
I0512 23:45:14.653035 139876724979712 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_17/self_attention/key/kernel
I0512 23:45:14.653064 139876724979712 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_17/self_attention/out/kernel
I0512 23:45:14.653093 139876724979712 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_17/self_attention/query/kernel
I0512 23:45:14.652595 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_22/pre_self_attention_layer_norm/scale/v_col
I0512 23:45:14.652626 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_22/pre_self_attention_layer_norm/scale/v_row
I0512 23:45:14.652657 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_22/self_attention/key/kernel/m
I0512 23:45:14.652688 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_22/self_attention/key/kernel/v
I0512 23:45:14.652719 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_22/self_attention/key/kernel/v_col
I0512 23:45:14.652751 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_22/self_attention/key/kernel/v_row
I0512 23:45:14.652781 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_22/self_attention/out/kernel/m
I0512 23:45:14.652812 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_22/self_attention/out/kernel/v
I0512 23:45:14.652844 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_22/self_attention/out/kernel/v_col
I0512 23:45:14.652875 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_22/self_attention/out/kernel/v_row
I0512 23:45:14.652906 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_22/self_attention/query/kernel/m
I0512 23:45:14.652937 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_22/self_attention/query/kernel/v
I0512 23:45:14.652968 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_22/self_attention/query/kernel/v_col
I0512 23:45:14.653006 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_22/self_attention/query/kernel/v_row
I0512 23:45:14.653037 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_22/self_attention/value/kernel/m
I0512 23:45:14.653123 139876724979712 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_17/self_attention/value/kernel
I0512 23:45:14.653152 139876724979712 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_18/mlp/wi_0/kernel
I0512 23:45:14.653182 139876724979712 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_18/mlp/wi_1/kernel
I0512 23:45:14.653211 139876724979712 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_18/mlp/wo/kernel
I0512 23:45:14.653241 139876724979712 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_18/pre_mlp_layer_norm/scale
I0512 23:45:14.653273 139876724979712 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_18/pre_self_attention_layer_norm/scale
I0512 23:45:14.653303 139876724979712 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_18/self_attention/key/kernel
I0512 23:45:14.653332 139876724979712 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_18/self_attention/out/kernel
I0512 23:45:14.653361 139876724979712 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_18/self_attention/query/kernel
I0512 23:45:14.653391 139876724979712 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_18/self_attention/value/kernel
I0512 23:45:14.653420 139876724979712 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_19/mlp/wi_0/kernel
I0512 23:45:14.653450 139876724979712 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_19/mlp/wi_1/kernel
I0512 23:45:14.653479 139876724979712 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_19/mlp/wo/kernel
I0512 23:45:14.653515 139876724979712 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_19/pre_mlp_layer_norm/scale
I0512 23:45:14.653545 139876724979712 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_19/pre_self_attention_layer_norm/scale
I0512 23:45:14.653574 139876724979712 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_19/self_attention/key/kernel
I0512 23:45:14.653069 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_22/self_attention/value/kernel/v
I0512 23:45:14.653100 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_22/self_attention/value/kernel/v_col
I0512 23:45:14.653131 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_22/self_attention/value/kernel/v_row
I0512 23:45:14.653162 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/extra_mlp/wi_0/kernel/m
I0512 23:45:14.653192 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/extra_mlp/wi_0/kernel/v
I0512 23:45:14.653230 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/extra_mlp/wi_0/kernel/v_col
I0512 23:45:14.653262 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/extra_mlp/wi_0/kernel/v_row
I0512 23:45:14.653293 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/extra_mlp/wi_1/kernel/m
I0512 23:45:14.653324 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/extra_mlp/wi_1/kernel/v
I0512 23:45:14.653355 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/extra_mlp/wi_1/kernel/v_col
I0512 23:45:14.653386 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/extra_mlp/wi_1/kernel/v_row
I0512 23:45:14.653417 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/extra_mlp/wo/kernel/m
I0512 23:45:14.653448 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/extra_mlp/wo/kernel/v
I0512 23:45:14.653481 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/extra_mlp/wo/kernel/v_col
I0512 23:45:14.653513 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/extra_mlp/wo/kernel/v_row
I0512 23:45:14.653544 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/mlp/expert/wi_0/kernel/m
I0512 23:45:14.653575 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/mlp/expert/wi_0/kernel/v
I0512 23:45:14.653606 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/mlp/expert/wi_0/kernel/v_col
I0512 23:45:14.653637 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/mlp/expert/wi_0/kernel/v_row
I0512 23:45:14.653668 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/mlp/expert/wi_1/kernel/m
I0512 23:45:14.653604 139876724979712 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_19/self_attention/out/kernel
I0512 23:45:14.653633 139876724979712 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_19/self_attention/query/kernel
I0512 23:45:14.653663 139876724979712 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_19/self_attention/value/kernel
I0512 23:45:14.653693 139876724979712 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_2/mlp/wi_0/kernel
I0512 23:45:14.653726 139876724979712 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_2/mlp/wi_1/kernel
I0512 23:45:14.653762 139876724979712 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_2/mlp/wo/kernel
I0512 23:45:14.653793 139876724979712 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_2/pre_mlp_layer_norm/scale
I0512 23:45:14.653822 139876724979712 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_2/pre_self_attention_layer_norm/scale
I0512 23:45:14.653852 139876724979712 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_2/self_attention/key/kernel
I0512 23:45:14.653882 139876724979712 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_2/self_attention/out/kernel
I0512 23:45:14.653911 139876724979712 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_2/self_attention/query/kernel
I0512 23:45:14.653940 139876724979712 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_2/self_attention/value/kernel
I0512 23:45:14.653970 139876724979712 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_20/mlp/wi_0/kernel
I0512 23:45:14.653999 139876724979712 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_20/mlp/wi_1/kernel
I0512 23:45:14.654029 139876724979712 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_20/mlp/wo/kernel
I0512 23:45:14.654058 139876724979712 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_20/pre_mlp_layer_norm/scale
I0512 23:45:14.653699 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/mlp/expert/wi_1/kernel/v
I0512 23:45:14.653730 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/mlp/expert/wi_1/kernel/v_col
I0512 23:45:14.653760 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/mlp/expert/wi_1/kernel/v_row
I0512 23:45:14.653791 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/mlp/expert/wo/kernel/m
I0512 23:45:14.653822 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/mlp/expert/wo/kernel/v
I0512 23:45:14.653853 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/mlp/expert/wo/kernel/v_col
I0512 23:45:14.653884 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/mlp/expert/wo/kernel/v_row
I0512 23:45:14.653915 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/mlp/router/router_weights/w/kernel/m
I0512 23:45:14.653946 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/mlp/router/router_weights/w/kernel/v
I0512 23:45:14.653977 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/mlp/router/router_weights/w/kernel/v_col
I0512 23:45:14.654008 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/mlp/router/router_weights/w/kernel/v_row
I0512 23:45:14.654039 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/pre_extra_mlp_layer_norm/scale/m
I0512 23:45:14.654070 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/pre_extra_mlp_layer_norm/scale/v
I0512 23:45:14.654101 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/pre_extra_mlp_layer_norm/scale/v_col
I0512 23:45:14.654132 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/pre_extra_mlp_layer_norm/scale/v_row
I0512 23:45:14.654163 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/pre_mlp_layer_norm/scale/m
I0512 23:45:14.654194 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/pre_mlp_layer_norm/scale/v
I0512 23:45:14.654234 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/pre_mlp_layer_norm/scale/v_col
I0512 23:45:14.654270 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/pre_mlp_layer_norm/scale/v_row
I0512 23:45:14.654302 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/pre_self_attention_layer_norm/scale/m
I0512 23:45:14.654087 139876724979712 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_20/pre_self_attention_layer_norm/scale
I0512 23:45:14.654117 139876724979712 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_20/self_attention/key/kernel
I0512 23:45:14.654147 139876724979712 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_20/self_attention/out/kernel
I0512 23:45:14.654176 139876724979712 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_20/self_attention/query/kernel
I0512 23:45:14.654206 139876724979712 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_20/self_attention/value/kernel
I0512 23:45:14.654235 139876724979712 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_21/mlp/wi_0/kernel
I0512 23:45:14.654267 139876724979712 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_21/mlp/wi_1/kernel
I0512 23:45:14.654297 139876724979712 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_21/mlp/wo/kernel
I0512 23:45:14.654327 139876724979712 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_21/pre_mlp_layer_norm/scale
I0512 23:45:14.654356 139876724979712 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_21/pre_self_attention_layer_norm/scale
I0512 23:45:14.654386 139876724979712 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_21/self_attention/key/kernel
I0512 23:45:14.654415 139876724979712 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_21/self_attention/out/kernel
I0512 23:45:14.654445 139876724979712 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_21/self_attention/query/kernel
I0512 23:45:14.654474 139876724979712 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_21/self_attention/value/kernel
I0512 23:45:14.654509 139876724979712 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_22/mlp/wi_0/kernel
I0512 23:45:14.654540 139876724979712 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_22/mlp/wi_1/kernel
I0512 23:45:14.654569 139876724979712 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_22/mlp/wo/kernel
I0512 23:45:14.654334 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/pre_self_attention_layer_norm/scale/v
I0512 23:45:14.654364 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/pre_self_attention_layer_norm/scale/v_col
I0512 23:45:14.654396 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/pre_self_attention_layer_norm/scale/v_row
I0512 23:45:14.654428 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/self_attention/key/kernel/m
I0512 23:45:14.654459 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/self_attention/key/kernel/v
I0512 23:45:14.654492 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/self_attention/key/kernel/v_col
I0512 23:45:14.654524 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/self_attention/key/kernel/v_row
I0512 23:45:14.654555 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/self_attention/out/kernel/m
I0512 23:45:14.654586 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/self_attention/out/kernel/v
I0512 23:45:14.654617 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/self_attention/out/kernel/v_col
I0512 23:45:14.654648 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/self_attention/out/kernel/v_row
I0512 23:45:14.654598 139876724979712 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_22/pre_mlp_layer_norm/scale
I0512 23:45:14.654628 139876724979712 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_22/pre_self_attention_layer_norm/scale
I0512 23:45:14.654658 139876724979712 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_22/self_attention/key/kernel
I0512 23:45:14.654687 139876724979712 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_22/self_attention/out/kernel
I0512 23:45:14.654716 139876724979712 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_22/self_attention/query/kernel
I0512 23:45:14.654745 139876724979712 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_22/self_attention/value/kernel
I0512 23:45:14.654775 139876724979712 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_23/extra_mlp/wi_0/kernel
I0512 23:45:14.654804 139876724979712 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_23/extra_mlp/wi_1/kernel
I0512 23:45:14.654833 139876724979712 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_23/extra_mlp/wo/kernel
I0512 23:45:14.654862 139876724979712 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_23/mlp/expert/wi_0/kernel
I0512 23:45:14.654892 139876724979712 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_23/mlp/expert/wi_1/kernel
I0512 23:45:14.654921 139876724979712 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_23/mlp/expert/wo/kernel
I0512 23:45:14.654951 139876724979712 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_23/mlp/router/router_weights/w/kernel
I0512 23:45:14.654980 139876724979712 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_23/pre_extra_mlp_layer_norm/scale
I0512 23:45:14.655010 139876724979712 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_23/pre_mlp_layer_norm/scale
I0512 23:45:14.654679 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/self_attention/query/kernel/m
I0512 23:45:14.654710 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/self_attention/query/kernel/v
I0512 23:45:14.654741 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/self_attention/query/kernel/v_col
I0512 23:45:14.654772 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/self_attention/query/kernel/v_row
I0512 23:45:14.654803 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/self_attention/value/kernel/m
I0512 23:45:14.654834 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/self_attention/value/kernel/v
I0512 23:45:14.654865 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/self_attention/value/kernel/v_col
I0512 23:45:14.654896 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/self_attention/value/kernel/v_row
I0512 23:45:14.654927 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_3/mlp/wi_0/kernel/m
I0512 23:45:14.654958 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_3/mlp/wi_0/kernel/v
I0512 23:45:14.654990 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_3/mlp/wi_0/kernel/v_col
I0512 23:45:14.655020 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_3/mlp/wi_0/kernel/v_row
I0512 23:45:14.655039 139876724979712 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_23/pre_self_attention_layer_norm/scale
I0512 23:45:14.655069 139876724979712 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_23/self_attention/key/kernel
I0512 23:45:14.655120 139876724979712 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_23/self_attention/out/kernel
I0512 23:45:14.655152 139876724979712 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_23/self_attention/query/kernel
I0512 23:45:14.655182 139876724979712 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_23/self_attention/value/kernel
I0512 23:45:14.655211 139876724979712 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_3/mlp/wi_0/kernel
I0512 23:45:14.655242 139876724979712 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_3/mlp/wi_1/kernel
I0512 23:45:14.655274 139876724979712 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_3/mlp/wo/kernel
I0512 23:45:14.655304 139876724979712 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_3/pre_mlp_layer_norm/scale
I0512 23:45:14.655333 139876724979712 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_3/pre_self_attention_layer_norm/scale
I0512 23:45:14.655363 139876724979712 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_3/self_attention/key/kernel
I0512 23:45:14.655392 139876724979712 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_3/self_attention/out/kernel
I0512 23:45:14.655421 139876724979712 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_3/self_attention/query/kernel
I0512 23:45:14.655451 139876724979712 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_3/self_attention/value/kernel
I0512 23:45:14.655480 139876724979712 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_4/mlp/wi_0/kernel
I0512 23:45:14.655516 139876724979712 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_4/mlp/wi_1/kernel
I0512 23:45:14.655546 139876724979712 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_4/mlp/wo/kernel
I0512 23:45:14.655052 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_3/mlp/wi_1/kernel/m
I0512 23:45:14.655104 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_3/mlp/wi_1/kernel/v
I0512 23:45:14.655142 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_3/mlp/wi_1/kernel/v_col
I0512 23:45:14.655174 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_3/mlp/wi_1/kernel/v_row
I0512 23:45:14.655206 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_3/mlp/wo/kernel/m
I0512 23:45:14.655247 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_3/mlp/wo/kernel/v
I0512 23:45:14.655278 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_3/mlp/wo/kernel/v_col
I0512 23:45:14.655310 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_3/mlp/wo/kernel/v_row
I0512 23:45:14.655341 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_3/pre_mlp_layer_norm/scale/m
I0512 23:45:14.655372 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_3/pre_mlp_layer_norm/scale/v
I0512 23:45:14.655403 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_3/pre_mlp_layer_norm/scale/v_col
I0512 23:45:14.655434 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_3/pre_mlp_layer_norm/scale/v_row
I0512 23:45:14.655466 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_3/pre_self_attention_layer_norm/scale/m
I0512 23:45:14.655575 139876724979712 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_4/pre_mlp_layer_norm/scale
I0512 23:45:14.655605 139876724979712 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_4/pre_self_attention_layer_norm/scale
I0512 23:45:14.655634 139876724979712 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_4/self_attention/key/kernel
I0512 23:45:14.655663 139876724979712 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_4/self_attention/out/kernel
I0512 23:45:14.655693 139876724979712 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_4/self_attention/query/kernel
I0512 23:45:14.655722 139876724979712 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_4/self_attention/value/kernel
I0512 23:45:14.655751 139876724979712 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_5/extra_mlp/wi_0/kernel
I0512 23:45:14.655781 139876724979712 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_5/extra_mlp/wi_1/kernel
I0512 23:45:14.655810 139876724979712 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_5/extra_mlp/wo/kernel
I0512 23:45:14.655839 139876724979712 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_5/mlp/expert/wi_0/kernel
I0512 23:45:14.655869 139876724979712 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_5/mlp/expert/wi_1/kernel
I0512 23:45:14.655898 139876724979712 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_5/mlp/expert/wo/kernel
I0512 23:45:14.655927 139876724979712 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_5/mlp/router/router_weights/w/kernel
I0512 23:45:14.655956 139876724979712 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_5/pre_extra_mlp_layer_norm/scale
I0512 23:45:14.655986 139876724979712 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_5/pre_mlp_layer_norm/scale
I0512 23:45:14.656015 139876724979712 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_5/pre_self_attention_layer_norm/scale
I0512 23:45:14.655499 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_3/pre_self_attention_layer_norm/scale/v
I0512 23:45:14.655530 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_3/pre_self_attention_layer_norm/scale/v_col
I0512 23:45:14.655561 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_3/pre_self_attention_layer_norm/scale/v_row
I0512 23:45:14.655593 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_3/self_attention/key/kernel/m
I0512 23:45:14.655624 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_3/self_attention/key/kernel/v
I0512 23:45:14.655655 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_3/self_attention/key/kernel/v_col
I0512 23:45:14.655686 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_3/self_attention/key/kernel/v_row
I0512 23:45:14.655717 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_3/self_attention/out/kernel/m
I0512 23:45:14.655749 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_3/self_attention/out/kernel/v
I0512 23:45:14.655780 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_3/self_attention/out/kernel/v_col
I0512 23:45:14.655810 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_3/self_attention/out/kernel/v_row
I0512 23:45:14.655842 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_3/self_attention/query/kernel/m
I0512 23:45:14.655873 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_3/self_attention/query/kernel/v
I0512 23:45:14.656045 139876724979712 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_5/self_attention/key/kernel
I0512 23:45:14.656075 139876724979712 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_5/self_attention/out/kernel
I0512 23:45:14.656104 139876724979712 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_5/self_attention/query/kernel
I0512 23:45:14.656133 139876724979712 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_5/self_attention/value/kernel
I0512 23:45:14.656162 139876724979712 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_6/mlp/wi_0/kernel
I0512 23:45:14.656192 139876724979712 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_6/mlp/wi_1/kernel
I0512 23:45:14.656222 139876724979712 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_6/mlp/wo/kernel
I0512 23:45:14.656253 139876724979712 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_6/pre_mlp_layer_norm/scale
I0512 23:45:14.656283 139876724979712 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_6/pre_self_attention_layer_norm/scale
I0512 23:45:14.656313 139876724979712 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_6/self_attention/key/kernel
I0512 23:45:14.656342 139876724979712 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_6/self_attention/out/kernel
I0512 23:45:14.656372 139876724979712 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_6/self_attention/query/kernel
I0512 23:45:14.656401 139876724979712 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_6/self_attention/value/kernel
I0512 23:45:14.656431 139876724979712 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_7/mlp/wi_0/kernel
I0512 23:45:14.656460 139876724979712 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_7/mlp/wi_1/kernel
I0512 23:45:14.656494 139876724979712 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_7/mlp/wo/kernel
I0512 23:45:14.655904 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_3/self_attention/query/kernel/v_col
I0512 23:45:14.655935 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_3/self_attention/query/kernel/v_row
I0512 23:45:14.655966 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_3/self_attention/value/kernel/m
I0512 23:45:14.655997 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_3/self_attention/value/kernel/v
I0512 23:45:14.656028 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_3/self_attention/value/kernel/v_col
I0512 23:45:14.656059 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_3/self_attention/value/kernel/v_row
I0512 23:45:14.656090 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_4/mlp/wi_0/kernel/m
I0512 23:45:14.656121 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_4/mlp/wi_0/kernel/v
I0512 23:45:14.656153 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_4/mlp/wi_0/kernel/v_col
I0512 23:45:14.656184 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_4/mlp/wi_0/kernel/v_row
I0512 23:45:14.656221 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_4/mlp/wi_1/kernel/m
I0512 23:45:14.656254 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_4/mlp/wi_1/kernel/v
I0512 23:45:14.656285 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_4/mlp/wi_1/kernel/v_col
I0512 23:45:14.656316 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_4/mlp/wi_1/kernel/v_row
I0512 23:45:14.656525 139876724979712 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_7/pre_mlp_layer_norm/scale
I0512 23:45:14.656555 139876724979712 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_7/pre_self_attention_layer_norm/scale
I0512 23:45:14.656584 139876724979712 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_7/self_attention/key/kernel
I0512 23:45:14.656613 139876724979712 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_7/self_attention/out/kernel
I0512 23:45:14.656643 139876724979712 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_7/self_attention/query/kernel
I0512 23:45:14.656672 139876724979712 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_7/self_attention/value/kernel
I0512 23:45:14.656704 139876724979712 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_8/mlp/wi_0/kernel
I0512 23:45:14.656736 139876724979712 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_8/mlp/wi_1/kernel
I0512 23:45:14.656348 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_4/mlp/wo/kernel/m
I0512 23:45:14.656379 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_4/mlp/wo/kernel/v
I0512 23:45:14.656410 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_4/mlp/wo/kernel/v_col
I0512 23:45:14.656441 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_4/mlp/wo/kernel/v_row
I0512 23:45:14.656474 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_4/pre_mlp_layer_norm/scale/m
I0512 23:45:14.656506 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_4/pre_mlp_layer_norm/scale/v
I0512 23:45:14.656538 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_4/pre_mlp_layer_norm/scale/v_col
I0512 23:45:14.656569 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_4/pre_mlp_layer_norm/scale/v_row
I0512 23:45:14.656600 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_4/pre_self_attention_layer_norm/scale/m
I0512 23:45:14.656631 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_4/pre_self_attention_layer_norm/scale/v
I0512 23:45:14.656662 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_4/pre_self_attention_layer_norm/scale/v_col
I0512 23:45:14.656693 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_4/pre_self_attention_layer_norm/scale/v_row
I0512 23:45:14.656725 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_4/self_attention/key/kernel/m
I0512 23:45:14.656756 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_4/self_attention/key/kernel/v
I0512 23:45:14.656766 139876724979712 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_8/mlp/wo/kernel
I0512 23:45:14.656795 139876724979712 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_8/pre_mlp_layer_norm/scale
I0512 23:45:14.656825 139876724979712 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_8/pre_self_attention_layer_norm/scale
I0512 23:45:14.656854 139876724979712 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_8/self_attention/key/kernel
I0512 23:45:14.656883 139876724979712 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_8/self_attention/out/kernel
I0512 23:45:14.656913 139876724979712 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_8/self_attention/query/kernel
I0512 23:45:14.656942 139876724979712 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_8/self_attention/value/kernel
I0512 23:45:14.656972 139876724979712 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_9/mlp/wi_0/kernel
I0512 23:45:14.656787 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_4/self_attention/key/kernel/v_col
I0512 23:45:14.656819 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_4/self_attention/key/kernel/v_row
I0512 23:45:14.656850 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_4/self_attention/out/kernel/m
I0512 23:45:14.656881 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_4/self_attention/out/kernel/v
I0512 23:45:14.656912 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_4/self_attention/out/kernel/v_col
I0512 23:45:14.656943 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_4/self_attention/out/kernel/v_row
I0512 23:45:14.656975 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_4/self_attention/query/kernel/m
I0512 23:45:14.657006 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_4/self_attention/query/kernel/v
I0512 23:45:14.657037 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_4/self_attention/query/kernel/v_col
I0512 23:45:14.657068 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_4/self_attention/query/kernel/v_row
I0512 23:45:14.657099 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_4/self_attention/value/kernel/m
I0512 23:45:14.657130 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_4/self_attention/value/kernel/v
I0512 23:45:14.657161 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_4/self_attention/value/kernel/v_col
I0512 23:45:14.657192 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_4/self_attention/value/kernel/v_row
I0512 23:45:14.657228 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/extra_mlp/wi_0/kernel/m
I0512 23:45:14.657261 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/extra_mlp/wi_0/kernel/v
I0512 23:45:14.657292 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/extra_mlp/wi_0/kernel/v_col
I0512 23:45:14.657323 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/extra_mlp/wi_0/kernel/v_row
I0512 23:45:14.657354 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/extra_mlp/wi_1/kernel/m
I0512 23:45:14.657385 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/extra_mlp/wi_1/kernel/v
I0512 23:45:14.657416 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/extra_mlp/wi_1/kernel/v_col
I0512 23:45:14.657447 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/extra_mlp/wi_1/kernel/v_row
I0512 23:45:14.657479 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/extra_mlp/wo/kernel/m
I0512 23:45:14.657511 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/extra_mlp/wo/kernel/v
I0512 23:45:14.657543 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/extra_mlp/wo/kernel/v_col
I0512 23:45:14.657574 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/extra_mlp/wo/kernel/v_row
I0512 23:45:14.657605 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/mlp/expert/wi_0/kernel/m
I0512 23:45:14.657636 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/mlp/expert/wi_0/kernel/v
I0512 23:45:14.657667 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/mlp/expert/wi_0/kernel/v_col
I0512 23:45:14.657001 139876724979712 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_9/mlp/wi_1/kernel
I0512 23:45:14.657031 139876724979712 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_9/mlp/wo/kernel
I0512 23:45:14.657060 139876724979712 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_9/pre_mlp_layer_norm/scale
I0512 23:45:14.657089 139876724979712 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_9/pre_self_attention_layer_norm/scale
I0512 23:45:14.657119 139876724979712 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_9/self_attention/key/kernel
I0512 23:45:14.657148 139876724979712 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_9/self_attention/out/kernel
I0512 23:45:14.657178 139876724979712 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_9/self_attention/query/kernel
I0512 23:45:14.657207 139876724979712 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_9/self_attention/value/kernel
I0512 23:45:14.657237 139876724979712 checkpoints.py:1100] Restoring key from ckpt: target/decoder/logits_dense/kernel
I0512 23:45:14.657269 139876724979712 checkpoints.py:1100] Restoring key from ckpt: target/token_embedder/embedding
I0512 23:45:14.657698 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/mlp/expert/wi_0/kernel/v_row
I0512 23:45:14.657729 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/mlp/expert/wi_1/kernel/m
I0512 23:45:14.657759 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/mlp/expert/wi_1/kernel/v
I0512 23:45:14.657791 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/mlp/expert/wi_1/kernel/v_col
I0512 23:45:14.657822 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/mlp/expert/wi_1/kernel/v_row
I0512 23:45:14.657853 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/mlp/expert/wo/kernel/m
I0512 23:45:14.657884 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/mlp/expert/wo/kernel/v
I0512 23:45:14.657915 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/mlp/expert/wo/kernel/v_col
I0512 23:45:14.657946 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/mlp/expert/wo/kernel/v_row
I0512 23:45:14.657977 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/mlp/router/router_weights/w/kernel/m
I0512 23:45:14.658008 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/mlp/router/router_weights/w/kernel/v
I0512 23:45:14.658039 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/mlp/router/router_weights/w/kernel/v_col
I0512 23:45:14.658070 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/mlp/router/router_weights/w/kernel/v_row
I0512 23:45:14.658101 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/pre_extra_mlp_layer_norm/scale/m
I0512 23:45:14.658132 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/pre_extra_mlp_layer_norm/scale/v
I0512 23:45:14.658163 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/pre_extra_mlp_layer_norm/scale/v_col
I0512 23:45:14.658194 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/pre_extra_mlp_layer_norm/scale/v_row
I0512 23:45:14.658231 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/pre_mlp_layer_norm/scale/m
I0512 23:45:14.658263 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/pre_mlp_layer_norm/scale/v
I0512 23:45:14.658295 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/pre_mlp_layer_norm/scale/v_col
I0512 23:45:14.658325 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/pre_mlp_layer_norm/scale/v_row
I0512 23:45:14.658356 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/pre_self_attention_layer_norm/scale/m
I0512 23:45:14.658387 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/pre_self_attention_layer_norm/scale/v
I0512 23:45:14.658418 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/pre_self_attention_layer_norm/scale/v_col
I0512 23:45:14.658449 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/pre_self_attention_layer_norm/scale/v_row
I0512 23:45:14.658483 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/self_attention/key/kernel/m
I0512 23:45:14.658515 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/self_attention/key/kernel/v
I0512 23:45:14.658546 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/self_attention/key/kernel/v_col
I0512 23:45:14.658577 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/self_attention/key/kernel/v_row
I0512 23:45:14.658608 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/self_attention/out/kernel/m
I0512 23:45:14.658639 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/self_attention/out/kernel/v
I0512 23:45:14.658670 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/self_attention/out/kernel/v_col
I0512 23:45:14.658700 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/self_attention/out/kernel/v_row
I0512 23:45:14.658731 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/self_attention/query/kernel/m
I0512 23:45:14.658763 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/self_attention/query/kernel/v
I0512 23:45:14.658793 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/self_attention/query/kernel/v_col
I0512 23:45:14.658825 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/self_attention/query/kernel/v_row
I0512 23:45:14.658856 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/self_attention/value/kernel/m
I0512 23:45:14.658887 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/self_attention/value/kernel/v
I0512 23:45:14.658918 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/self_attention/value/kernel/v_col
I0512 23:45:14.658949 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/self_attention/value/kernel/v_row
I0512 23:45:14.658980 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_6/mlp/wi_0/kernel/m
I0512 23:45:14.659011 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_6/mlp/wi_0/kernel/v
I0512 23:45:14.659042 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_6/mlp/wi_0/kernel/v_col
I0512 23:45:14.659073 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_6/mlp/wi_0/kernel/v_row
I0512 23:45:14.659126 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_6/mlp/wi_1/kernel/m
I0512 23:45:14.659159 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_6/mlp/wi_1/kernel/v
I0512 23:45:14.659191 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_6/mlp/wi_1/kernel/v_col
I0512 23:45:14.659227 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_6/mlp/wi_1/kernel/v_row
I0512 23:45:14.659259 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_6/mlp/wo/kernel/m
I0512 23:45:14.659291 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_6/mlp/wo/kernel/v
I0512 23:45:14.659322 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_6/mlp/wo/kernel/v_col
I0512 23:45:14.659353 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_6/mlp/wo/kernel/v_row
I0512 23:45:14.659384 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_6/pre_mlp_layer_norm/scale/m
I0512 23:45:14.659415 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_6/pre_mlp_layer_norm/scale/v
I0512 23:45:14.659446 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_6/pre_mlp_layer_norm/scale/v_col
I0512 23:45:14.659478 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_6/pre_mlp_layer_norm/scale/v_row
I0512 23:45:14.659510 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_6/pre_self_attention_layer_norm/scale/m
I0512 23:45:14.659542 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_6/pre_self_attention_layer_norm/scale/v
I0512 23:45:14.659573 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_6/pre_self_attention_layer_norm/scale/v_col
I0512 23:45:14.659604 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_6/pre_self_attention_layer_norm/scale/v_row
I0512 23:45:14.659635 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_6/self_attention/key/kernel/m
I0512 23:45:14.659667 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_6/self_attention/key/kernel/v
I0512 23:45:14.659698 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_6/self_attention/key/kernel/v_col
I0512 23:45:14.659729 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_6/self_attention/key/kernel/v_row
I0512 23:45:14.659759 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_6/self_attention/out/kernel/m
I0512 23:45:14.659790 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_6/self_attention/out/kernel/v
I0512 23:45:14.659821 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_6/self_attention/out/kernel/v_col
I0512 23:45:14.659852 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_6/self_attention/out/kernel/v_row
I0512 23:45:14.659883 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_6/self_attention/query/kernel/m
I0512 23:45:14.659914 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_6/self_attention/query/kernel/v
I0512 23:45:14.659945 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_6/self_attention/query/kernel/v_col
I0512 23:45:14.659976 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_6/self_attention/query/kernel/v_row
I0512 23:45:14.660007 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_6/self_attention/value/kernel/m
I0512 23:45:14.660038 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_6/self_attention/value/kernel/v
I0512 23:45:14.660069 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_6/self_attention/value/kernel/v_col
I0512 23:45:14.660099 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_6/self_attention/value/kernel/v_row
I0512 23:45:14.660130 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_7/mlp/wi_0/kernel/m
I0512 23:45:14.660161 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_7/mlp/wi_0/kernel/v
I0512 23:45:14.660192 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_7/mlp/wi_0/kernel/v_col
I0512 23:45:14.660230 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_7/mlp/wi_0/kernel/v_row
I0512 23:45:14.660262 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_7/mlp/wi_1/kernel/m
I0512 23:45:14.660293 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_7/mlp/wi_1/kernel/v
I0512 23:45:14.660325 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_7/mlp/wi_1/kernel/v_col
I0512 23:45:14.660356 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_7/mlp/wi_1/kernel/v_row
I0512 23:45:14.660387 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_7/mlp/wo/kernel/m
I0512 23:45:14.660418 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_7/mlp/wo/kernel/v
I0512 23:45:14.660449 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_7/mlp/wo/kernel/v_col
I0512 23:45:14.660482 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_7/mlp/wo/kernel/v_row
I0512 23:45:14.660514 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_7/pre_mlp_layer_norm/scale/m
I0512 23:45:14.660545 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_7/pre_mlp_layer_norm/scale/v
I0512 23:45:14.660576 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_7/pre_mlp_layer_norm/scale/v_col
I0512 23:45:14.660607 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_7/pre_mlp_layer_norm/scale/v_row
I0512 23:45:14.660638 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_7/pre_self_attention_layer_norm/scale/m
I0512 23:45:14.660669 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_7/pre_self_attention_layer_norm/scale/v
I0512 23:45:14.660701 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_7/pre_self_attention_layer_norm/scale/v_col
I0512 23:45:14.660731 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_7/pre_self_attention_layer_norm/scale/v_row
I0512 23:45:14.660763 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_7/self_attention/key/kernel/m
I0512 23:45:14.660794 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_7/self_attention/key/kernel/v
I0512 23:45:14.660825 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_7/self_attention/key/kernel/v_col
I0512 23:45:14.660856 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_7/self_attention/key/kernel/v_row
I0512 23:45:14.660887 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_7/self_attention/out/kernel/m
I0512 23:45:14.660918 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_7/self_attention/out/kernel/v
I0512 23:45:14.660949 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_7/self_attention/out/kernel/v_col
I0512 23:45:14.660980 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_7/self_attention/out/kernel/v_row
I0512 23:45:14.661011 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_7/self_attention/query/kernel/m
I0512 23:45:14.661042 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_7/self_attention/query/kernel/v
I0512 23:45:14.661073 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_7/self_attention/query/kernel/v_col
I0512 23:45:14.661104 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_7/self_attention/query/kernel/v_row
I0512 23:45:14.661135 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_7/self_attention/value/kernel/m
I0512 23:45:14.661166 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_7/self_attention/value/kernel/v
I0512 23:45:14.661197 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_7/self_attention/value/kernel/v_col
I0512 23:45:14.661235 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_7/self_attention/value/kernel/v_row
I0512 23:45:14.661267 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_8/mlp/wi_0/kernel/m
I0512 23:45:14.661298 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_8/mlp/wi_0/kernel/v
I0512 23:45:14.661329 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_8/mlp/wi_0/kernel/v_col
I0512 23:45:14.661360 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_8/mlp/wi_0/kernel/v_row
I0512 23:45:14.661391 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_8/mlp/wi_1/kernel/m
I0512 23:45:14.661422 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_8/mlp/wi_1/kernel/v
I0512 23:45:14.661453 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_8/mlp/wi_1/kernel/v_col
I0512 23:45:14.661486 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_8/mlp/wi_1/kernel/v_row
I0512 23:45:14.661518 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_8/mlp/wo/kernel/m
I0512 23:45:14.661550 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_8/mlp/wo/kernel/v
I0512 23:45:14.661581 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_8/mlp/wo/kernel/v_col
I0512 23:45:14.661612 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_8/mlp/wo/kernel/v_row
I0512 23:45:14.661643 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_8/pre_mlp_layer_norm/scale/m
I0512 23:45:14.661674 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_8/pre_mlp_layer_norm/scale/v
I0512 23:45:14.661705 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_8/pre_mlp_layer_norm/scale/v_col
I0512 23:45:14.661736 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_8/pre_mlp_layer_norm/scale/v_row
I0512 23:45:14.661767 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_8/pre_self_attention_layer_norm/scale/m
I0512 23:45:14.661798 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_8/pre_self_attention_layer_norm/scale/v
I0512 23:45:14.661829 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_8/pre_self_attention_layer_norm/scale/v_col
I0512 23:45:14.661860 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_8/pre_self_attention_layer_norm/scale/v_row
I0512 23:45:14.661891 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_8/self_attention/key/kernel/m
I0512 23:45:14.661922 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_8/self_attention/key/kernel/v
I0512 23:45:14.661953 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_8/self_attention/key/kernel/v_col
I0512 23:45:14.661984 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_8/self_attention/key/kernel/v_row
I0512 23:45:14.662015 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_8/self_attention/out/kernel/m
I0512 23:45:14.662046 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_8/self_attention/out/kernel/v
I0512 23:45:14.662077 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_8/self_attention/out/kernel/v_col
I0512 23:45:14.662109 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_8/self_attention/out/kernel/v_row
I0512 23:45:14.662140 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_8/self_attention/query/kernel/m
I0512 23:45:14.662171 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_8/self_attention/query/kernel/v
I0512 23:45:14.662203 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_8/self_attention/query/kernel/v_col
I0512 23:45:14.662240 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_8/self_attention/query/kernel/v_row
I0512 23:45:14.662271 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_8/self_attention/value/kernel/m
I0512 23:45:14.662302 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_8/self_attention/value/kernel/v
I0512 23:45:14.662332 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_8/self_attention/value/kernel/v_col
I0512 23:45:14.662363 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_8/self_attention/value/kernel/v_row
I0512 23:45:14.662393 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_9/mlp/wi_0/kernel/m
I0512 23:45:14.662424 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_9/mlp/wi_0/kernel/v
I0512 23:45:14.662455 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_9/mlp/wi_0/kernel/v_col
I0512 23:45:14.662488 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_9/mlp/wi_0/kernel/v_row
I0512 23:45:14.662519 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_9/mlp/wi_1/kernel/m
I0512 23:45:14.662550 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_9/mlp/wi_1/kernel/v
I0512 23:45:14.662581 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_9/mlp/wi_1/kernel/v_col
I0512 23:45:14.662611 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_9/mlp/wi_1/kernel/v_row
I0512 23:45:14.662642 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_9/mlp/wo/kernel/m
I0512 23:45:14.662673 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_9/mlp/wo/kernel/v
I0512 23:45:14.662704 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_9/mlp/wo/kernel/v_col
I0512 23:45:14.662734 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_9/mlp/wo/kernel/v_row
I0512 23:45:14.662765 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_9/pre_mlp_layer_norm/scale/m
I0512 23:45:14.662796 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_9/pre_mlp_layer_norm/scale/v
I0512 23:45:14.662826 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_9/pre_mlp_layer_norm/scale/v_col
I0512 23:45:14.662857 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_9/pre_mlp_layer_norm/scale/v_row
I0512 23:45:14.662888 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_9/pre_self_attention_layer_norm/scale/m
I0512 23:45:14.662918 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_9/pre_self_attention_layer_norm/scale/v
I0512 23:45:14.662949 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_9/pre_self_attention_layer_norm/scale/v_col
I0512 23:45:14.662979 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_9/pre_self_attention_layer_norm/scale/v_row
I0512 23:45:14.663010 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_9/self_attention/key/kernel/m
I0512 23:45:14.663040 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_9/self_attention/key/kernel/v
I0512 23:45:14.663071 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_9/self_attention/key/kernel/v_col
I0512 23:45:14.663125 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_9/self_attention/key/kernel/v_row
I0512 23:45:14.663159 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_9/self_attention/out/kernel/m
I0512 23:45:14.663190 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_9/self_attention/out/kernel/v
I0512 23:45:14.663225 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_9/self_attention/out/kernel/v_col
I0512 23:45:14.663257 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_9/self_attention/out/kernel/v_row
I0512 23:45:14.663288 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_9/self_attention/query/kernel/m
I0512 23:45:14.663319 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_9/self_attention/query/kernel/v
I0512 23:45:14.663349 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_9/self_attention/query/kernel/v_col
I0512 23:45:14.663380 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_9/self_attention/query/kernel/v_row
I0512 23:45:14.663410 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_9/self_attention/value/kernel/m
I0512 23:45:14.663441 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_9/self_attention/value/kernel/v
I0512 23:45:14.663473 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_9/self_attention/value/kernel/v_col
I0512 23:45:14.663505 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_9/self_attention/value/kernel/v_row
I0512 23:45:14.663535 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/logits_dense/kernel/m
I0512 23:45:14.663566 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/logits_dense/kernel/v
I0512 23:45:14.663597 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/logits_dense/kernel/v_col
I0512 23:45:14.663628 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/logits_dense/kernel/v_row
I0512 23:45:14.663658 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/token_embedder/embedding/m
I0512 23:45:14.663689 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/token_embedder/embedding/v
I0512 23:45:14.663720 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/token_embedder/embedding/v_col
I0512 23:45:14.663750 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/token_embedder/embedding/v_row
I0512 23:45:14.663781 139666521167872 checkpoints.py:1100] Restoring key from ckpt: state/step
I0512 23:45:14.663812 139666521167872 checkpoints.py:1100] Restoring key from ckpt: target/decoder/decoder_norm/scale
I0512 23:45:14.663844 139666521167872 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_0/mlp/wi_0/kernel
I0512 23:45:14.663875 139666521167872 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_0/mlp/wi_1/kernel
I0512 23:45:14.663906 139666521167872 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_0/mlp/wo/kernel
I0512 23:45:14.663937 139666521167872 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_0/pre_mlp_layer_norm/scale
I0512 23:45:14.663968 139666521167872 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_0/pre_self_attention_layer_norm/scale
I0512 23:45:14.663999 139666521167872 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_0/self_attention/key/kernel
I0512 23:45:14.664029 139666521167872 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_0/self_attention/out/kernel
I0512 23:45:14.664060 139666521167872 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_0/self_attention/query/kernel
I0512 23:45:14.664091 139666521167872 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_0/self_attention/value/kernel
I0512 23:45:14.664121 139666521167872 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_1/mlp/wi_0/kernel
I0512 23:45:14.664152 139666521167872 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_1/mlp/wi_1/kernel
I0512 23:45:14.664183 139666521167872 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_1/mlp/wo/kernel
I0512 23:45:14.664220 139666521167872 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_1/pre_mlp_layer_norm/scale
I0512 23:45:14.664252 139666521167872 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_1/pre_self_attention_layer_norm/scale
I0512 23:45:14.664283 139666521167872 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_1/self_attention/key/kernel
I0512 23:45:14.664314 139666521167872 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_1/self_attention/out/kernel
I0512 23:45:14.664344 139666521167872 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_1/self_attention/query/kernel
I0512 23:45:14.664375 139666521167872 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_1/self_attention/value/kernel
I0512 23:45:14.664406 139666521167872 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_10/mlp/wi_0/kernel
I0512 23:45:14.664437 139666521167872 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_10/mlp/wi_1/kernel
I0512 23:45:14.664468 139666521167872 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_10/mlp/wo/kernel
I0512 23:45:14.664500 139666521167872 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_10/pre_mlp_layer_norm/scale
I0512 23:45:14.664531 139666521167872 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_10/pre_self_attention_layer_norm/scale
I0512 23:45:14.664562 139666521167872 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_10/self_attention/key/kernel
I0512 23:45:14.664593 139666521167872 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_10/self_attention/out/kernel
I0512 23:45:14.664623 139666521167872 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_10/self_attention/query/kernel
I0512 23:45:14.664654 139666521167872 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_10/self_attention/value/kernel
I0512 23:45:14.664685 139666521167872 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_11/extra_mlp/wi_0/kernel
I0512 23:45:14.664716 139666521167872 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_11/extra_mlp/wi_1/kernel
I0512 23:45:14.664747 139666521167872 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_11/extra_mlp/wo/kernel
I0512 23:45:14.664777 139666521167872 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_11/mlp/expert/wi_0/kernel
I0512 23:45:14.664808 139666521167872 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_11/mlp/expert/wi_1/kernel
I0512 23:45:14.664839 139666521167872 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_11/mlp/expert/wo/kernel
I0512 23:45:14.664869 139666521167872 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_11/mlp/router/router_weights/w/kernel
I0512 23:45:14.664900 139666521167872 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_11/pre_extra_mlp_layer_norm/scale
I0512 23:45:14.664930 139666521167872 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_11/pre_mlp_layer_norm/scale
I0512 23:45:14.664961 139666521167872 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_11/pre_self_attention_layer_norm/scale
I0512 23:45:14.664992 139666521167872 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_11/self_attention/key/kernel
I0512 23:45:14.665022 139666521167872 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_11/self_attention/out/kernel
I0512 23:45:14.665052 139666521167872 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_11/self_attention/query/kernel
I0512 23:45:14.665083 139666521167872 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_11/self_attention/value/kernel
I0512 23:45:14.665113 139666521167872 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_12/mlp/wi_0/kernel
I0512 23:45:14.665143 139666521167872 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_12/mlp/wi_1/kernel
I0512 23:45:14.665174 139666521167872 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_12/mlp/wo/kernel
I0512 23:45:14.665205 139666521167872 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_12/pre_mlp_layer_norm/scale
I0512 23:45:14.665242 139666521167872 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_12/pre_self_attention_layer_norm/scale
I0512 23:45:14.665273 139666521167872 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_12/self_attention/key/kernel
I0512 23:45:14.665304 139666521167872 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_12/self_attention/out/kernel
I0512 23:45:14.665334 139666521167872 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_12/self_attention/query/kernel
I0512 23:45:14.665365 139666521167872 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_12/self_attention/value/kernel
I0512 23:45:14.665396 139666521167872 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_13/mlp/wi_0/kernel
I0512 23:45:14.665426 139666521167872 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_13/mlp/wi_1/kernel
I0512 23:45:14.665457 139666521167872 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_13/mlp/wo/kernel
I0512 23:45:14.665490 139666521167872 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_13/pre_mlp_layer_norm/scale
I0512 23:45:14.665522 139666521167872 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_13/pre_self_attention_layer_norm/scale
I0512 23:45:14.665553 139666521167872 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_13/self_attention/key/kernel
I0512 23:45:14.665583 139666521167872 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_13/self_attention/out/kernel
I0512 23:45:14.665614 139666521167872 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_13/self_attention/query/kernel
I0512 23:45:14.665645 139666521167872 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_13/self_attention/value/kernel
I0512 23:45:14.665675 139666521167872 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_14/mlp/wi_0/kernel
I0512 23:45:14.665706 139666521167872 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_14/mlp/wi_1/kernel
I0512 23:45:14.665737 139666521167872 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_14/mlp/wo/kernel
I0512 23:45:14.665767 139666521167872 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_14/pre_mlp_layer_norm/scale
I0512 23:45:14.665798 139666521167872 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_14/pre_self_attention_layer_norm/scale
I0512 23:45:14.665829 139666521167872 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_14/self_attention/key/kernel
I0512 23:45:14.665859 139666521167872 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_14/self_attention/out/kernel
I0512 23:45:14.665890 139666521167872 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_14/self_attention/query/kernel
I0512 23:45:14.665920 139666521167872 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_14/self_attention/value/kernel
I0512 23:45:14.665951 139666521167872 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_15/mlp/wi_0/kernel
I0512 23:45:14.665982 139666521167872 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_15/mlp/wi_1/kernel
I0512 23:45:14.666013 139666521167872 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_15/mlp/wo/kernel
I0512 23:45:14.666043 139666521167872 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_15/pre_mlp_layer_norm/scale
I0512 23:45:14.666074 139666521167872 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_15/pre_self_attention_layer_norm/scale
I0512 23:45:14.666105 139666521167872 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_15/self_attention/key/kernel
I0512 23:45:14.666135 139666521167872 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_15/self_attention/out/kernel
I0512 23:45:14.666166 139666521167872 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_15/self_attention/query/kernel
I0512 23:45:14.666197 139666521167872 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_15/self_attention/value/kernel
I0512 23:45:14.666234 139666521167872 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_16/mlp/wi_0/kernel
I0512 23:45:14.666266 139666521167872 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_16/mlp/wi_1/kernel
I0512 23:45:14.666297 139666521167872 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_16/mlp/wo/kernel
I0512 23:45:14.666328 139666521167872 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_16/pre_mlp_layer_norm/scale
I0512 23:45:14.666358 139666521167872 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_16/pre_self_attention_layer_norm/scale
I0512 23:45:14.666389 139666521167872 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_16/self_attention/key/kernel
I0512 23:45:14.666420 139666521167872 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_16/self_attention/out/kernel
I0512 23:45:14.666451 139666521167872 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_16/self_attention/query/kernel
I0512 23:45:14.666483 139666521167872 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_16/self_attention/value/kernel
I0512 23:45:14.666515 139666521167872 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_17/extra_mlp/wi_0/kernel
I0512 23:45:14.666546 139666521167872 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_17/extra_mlp/wi_1/kernel
I0512 23:45:14.666577 139666521167872 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_17/extra_mlp/wo/kernel
I0512 23:45:14.666607 139666521167872 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_17/mlp/expert/wi_0/kernel
I0512 23:45:14.666638 139666521167872 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_17/mlp/expert/wi_1/kernel
I0512 23:45:14.666668 139666521167872 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_17/mlp/expert/wo/kernel
I0512 23:45:14.666699 139666521167872 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_17/mlp/router/router_weights/w/kernel
I0512 23:45:14.666730 139666521167872 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_17/pre_extra_mlp_layer_norm/scale
I0512 23:45:14.666760 139666521167872 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_17/pre_mlp_layer_norm/scale
I0512 23:45:14.666791 139666521167872 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_17/pre_self_attention_layer_norm/scale
I0512 23:45:14.666822 139666521167872 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_17/self_attention/key/kernel
I0512 23:45:14.666852 139666521167872 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_17/self_attention/out/kernel
I0512 23:45:14.666882 139666521167872 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_17/self_attention/query/kernel
I0512 23:45:14.666913 139666521167872 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_17/self_attention/value/kernel
I0512 23:45:14.666944 139666521167872 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_18/mlp/wi_0/kernel
I0512 23:45:14.666975 139666521167872 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_18/mlp/wi_1/kernel
I0512 23:45:14.667006 139666521167872 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_18/mlp/wo/kernel
I0512 23:45:14.667037 139666521167872 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_18/pre_mlp_layer_norm/scale
I0512 23:45:14.667068 139666521167872 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_18/pre_self_attention_layer_norm/scale
I0512 23:45:14.667121 139666521167872 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_18/self_attention/key/kernel
I0512 23:45:14.667155 139666521167872 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_18/self_attention/out/kernel
I0512 23:45:14.667187 139666521167872 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_18/self_attention/query/kernel
I0512 23:45:14.667222 139666521167872 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_18/self_attention/value/kernel
I0512 23:45:14.667255 139666521167872 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_19/mlp/wi_0/kernel
I0512 23:45:14.667286 139666521167872 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_19/mlp/wi_1/kernel
I0512 23:45:14.667317 139666521167872 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_19/mlp/wo/kernel
I0512 23:45:14.667347 139666521167872 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_19/pre_mlp_layer_norm/scale
I0512 23:45:14.667378 139666521167872 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_19/pre_self_attention_layer_norm/scale
I0512 23:45:14.667409 139666521167872 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_19/self_attention/key/kernel
I0512 23:45:14.667440 139666521167872 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_19/self_attention/out/kernel
I0512 23:45:14.667472 139666521167872 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_19/self_attention/query/kernel
I0512 23:45:14.667505 139666521167872 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_19/self_attention/value/kernel
I0512 23:45:14.667536 139666521167872 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_2/mlp/wi_0/kernel
I0512 23:45:14.667567 139666521167872 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_2/mlp/wi_1/kernel
I0512 23:45:14.667598 139666521167872 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_2/mlp/wo/kernel
I0512 23:45:14.667629 139666521167872 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_2/pre_mlp_layer_norm/scale
I0512 23:45:14.667659 139666521167872 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_2/pre_self_attention_layer_norm/scale
I0512 23:45:14.667690 139666521167872 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_2/self_attention/key/kernel
I0512 23:45:14.667721 139666521167872 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_2/self_attention/out/kernel
I0512 23:45:14.667752 139666521167872 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_2/self_attention/query/kernel
I0512 23:45:14.667782 139666521167872 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_2/self_attention/value/kernel
I0512 23:45:14.667813 139666521167872 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_20/mlp/wi_0/kernel
I0512 23:45:14.667844 139666521167872 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_20/mlp/wi_1/kernel
I0512 23:45:14.667875 139666521167872 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_20/mlp/wo/kernel
I0512 23:45:14.667906 139666521167872 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_20/pre_mlp_layer_norm/scale
I0512 23:45:14.667937 139666521167872 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_20/pre_self_attention_layer_norm/scale
I0512 23:45:14.667968 139666521167872 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_20/self_attention/key/kernel
I0512 23:45:14.667999 139666521167872 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_20/self_attention/out/kernel
I0512 23:45:14.668029 139666521167872 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_20/self_attention/query/kernel
I0512 23:45:14.668060 139666521167872 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_20/self_attention/value/kernel
I0512 23:45:14.668091 139666521167872 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_21/mlp/wi_0/kernel
I0512 23:45:14.668122 139666521167872 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_21/mlp/wi_1/kernel
I0512 23:45:14.668153 139666521167872 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_21/mlp/wo/kernel
I0512 23:45:14.668184 139666521167872 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_21/pre_mlp_layer_norm/scale
I0512 23:45:14.668221 139666521167872 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_21/pre_self_attention_layer_norm/scale
I0512 23:45:14.668254 139666521167872 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_21/self_attention/key/kernel
I0512 23:45:14.668285 139666521167872 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_21/self_attention/out/kernel
I0512 23:45:14.668316 139666521167872 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_21/self_attention/query/kernel
I0512 23:45:14.668347 139666521167872 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_21/self_attention/value/kernel
I0512 23:45:14.668377 139666521167872 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_22/mlp/wi_0/kernel
I0512 23:45:14.668408 139666521167872 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_22/mlp/wi_1/kernel
I0512 23:45:14.668440 139666521167872 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_22/mlp/wo/kernel
I0512 23:45:14.668472 139666521167872 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_22/pre_mlp_layer_norm/scale
I0512 23:45:14.668504 139666521167872 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_22/pre_self_attention_layer_norm/scale
I0512 23:45:14.668535 139666521167872 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_22/self_attention/key/kernel
I0512 23:45:14.668566 139666521167872 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_22/self_attention/out/kernel
I0512 23:45:14.668597 139666521167872 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_22/self_attention/query/kernel
I0512 23:45:14.668627 139666521167872 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_22/self_attention/value/kernel
I0512 23:45:14.668658 139666521167872 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_23/extra_mlp/wi_0/kernel
I0512 23:45:14.668689 139666521167872 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_23/extra_mlp/wi_1/kernel
I0512 23:45:14.668720 139666521167872 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_23/extra_mlp/wo/kernel
I0512 23:45:14.668750 139666521167872 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_23/mlp/expert/wi_0/kernel
I0512 23:45:14.668781 139666521167872 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_23/mlp/expert/wi_1/kernel
I0512 23:45:14.668812 139666521167872 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_23/mlp/expert/wo/kernel
I0512 23:45:14.668843 139666521167872 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_23/mlp/router/router_weights/w/kernel
I0512 23:45:14.668874 139666521167872 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_23/pre_extra_mlp_layer_norm/scale
I0512 23:45:14.668905 139666521167872 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_23/pre_mlp_layer_norm/scale
I0512 23:45:14.668935 139666521167872 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_23/pre_self_attention_layer_norm/scale
I0512 23:45:14.668966 139666521167872 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_23/self_attention/key/kernel
I0512 23:45:14.668997 139666521167872 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_23/self_attention/out/kernel
I0512 23:45:14.669027 139666521167872 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_23/self_attention/query/kernel
I0512 23:45:14.669059 139666521167872 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_23/self_attention/value/kernel
I0512 23:45:14.669089 139666521167872 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_3/mlp/wi_0/kernel
I0512 23:45:14.669120 139666521167872 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_3/mlp/wi_1/kernel
I0512 23:45:14.669151 139666521167872 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_3/mlp/wo/kernel
I0512 23:45:14.669182 139666521167872 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_3/pre_mlp_layer_norm/scale
I0512 23:45:14.669218 139666521167872 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_3/pre_self_attention_layer_norm/scale
I0512 23:45:14.669251 139666521167872 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_3/self_attention/key/kernel
I0512 23:45:14.669282 139666521167872 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_3/self_attention/out/kernel
I0512 23:45:14.669313 139666521167872 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_3/self_attention/query/kernel
I0512 23:45:14.669344 139666521167872 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_3/self_attention/value/kernel
I0512 23:45:14.669375 139666521167872 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_4/mlp/wi_0/kernel
I0512 23:45:14.669406 139666521167872 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_4/mlp/wi_1/kernel
I0512 23:45:14.669437 139666521167872 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_4/mlp/wo/kernel
I0512 23:45:14.669469 139666521167872 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_4/pre_mlp_layer_norm/scale
I0512 23:45:14.669502 139666521167872 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_4/pre_self_attention_layer_norm/scale
I0512 23:45:14.669533 139666521167872 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_4/self_attention/key/kernel
I0512 23:45:14.669564 139666521167872 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_4/self_attention/out/kernel
I0512 23:45:14.669594 139666521167872 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_4/self_attention/query/kernel
I0512 23:45:14.669625 139666521167872 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_4/self_attention/value/kernel
I0512 23:45:14.669656 139666521167872 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_5/extra_mlp/wi_0/kernel
I0512 23:45:14.669687 139666521167872 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_5/extra_mlp/wi_1/kernel
I0512 23:45:14.669718 139666521167872 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_5/extra_mlp/wo/kernel
I0512 23:45:14.669749 139666521167872 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_5/mlp/expert/wi_0/kernel
I0512 23:45:14.669779 139666521167872 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_5/mlp/expert/wi_1/kernel
I0512 23:45:14.669810 139666521167872 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_5/mlp/expert/wo/kernel
I0512 23:45:14.669840 139666521167872 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_5/mlp/router/router_weights/w/kernel
I0512 23:45:14.669871 139666521167872 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_5/pre_extra_mlp_layer_norm/scale
I0512 23:45:14.669902 139666521167872 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_5/pre_mlp_layer_norm/scale
I0512 23:45:14.669933 139666521167872 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_5/pre_self_attention_layer_norm/scale
I0512 23:45:14.669964 139666521167872 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_5/self_attention/key/kernel
I0512 23:45:14.669995 139666521167872 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_5/self_attention/out/kernel
I0512 23:45:14.670025 139666521167872 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_5/self_attention/query/kernel
I0512 23:45:14.670056 139666521167872 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_5/self_attention/value/kernel
I0512 23:45:14.670087 139666521167872 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_6/mlp/wi_0/kernel
I0512 23:45:14.670117 139666521167872 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_6/mlp/wi_1/kernel
I0512 23:45:14.670148 139666521167872 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_6/mlp/wo/kernel
I0512 23:45:14.670179 139666521167872 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_6/pre_mlp_layer_norm/scale
I0512 23:45:14.670215 139666521167872 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_6/pre_self_attention_layer_norm/scale
I0512 23:45:14.670248 139666521167872 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_6/self_attention/key/kernel
I0512 23:45:14.670279 139666521167872 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_6/self_attention/out/kernel
I0512 23:45:14.670309 139666521167872 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_6/self_attention/query/kernel
I0512 23:45:14.670340 139666521167872 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_6/self_attention/value/kernel
I0512 23:45:14.670371 139666521167872 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_7/mlp/wi_0/kernel
I0512 23:45:14.670402 139666521167872 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_7/mlp/wi_1/kernel
I0512 23:45:14.670433 139666521167872 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_7/mlp/wo/kernel
I0512 23:45:14.670465 139666521167872 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_7/pre_mlp_layer_norm/scale
I0512 23:45:14.670499 139666521167872 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_7/pre_self_attention_layer_norm/scale
I0512 23:45:14.670530 139666521167872 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_7/self_attention/key/kernel
I0512 23:45:14.670561 139666521167872 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_7/self_attention/out/kernel
I0512 23:45:14.670592 139666521167872 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_7/self_attention/query/kernel
I0512 23:45:14.670623 139666521167872 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_7/self_attention/value/kernel
I0512 23:45:14.670654 139666521167872 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_8/mlp/wi_0/kernel
I0512 23:45:14.670684 139666521167872 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_8/mlp/wi_1/kernel
I0512 23:45:14.670715 139666521167872 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_8/mlp/wo/kernel
I0512 23:45:14.670746 139666521167872 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_8/pre_mlp_layer_norm/scale
I0512 23:45:14.670777 139666521167872 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_8/pre_self_attention_layer_norm/scale
I0512 23:45:14.670808 139666521167872 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_8/self_attention/key/kernel
I0512 23:45:14.670839 139666521167872 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_8/self_attention/out/kernel
I0512 23:45:14.670870 139666521167872 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_8/self_attention/query/kernel
I0512 23:45:14.670901 139666521167872 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_8/self_attention/value/kernel
I0512 23:45:14.670931 139666521167872 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_9/mlp/wi_0/kernel
I0512 23:45:14.670962 139666521167872 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_9/mlp/wi_1/kernel
I0512 23:45:14.670993 139666521167872 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_9/mlp/wo/kernel
I0512 23:45:14.671024 139666521167872 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_9/pre_mlp_layer_norm/scale
I0512 23:45:14.671055 139666521167872 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_9/pre_self_attention_layer_norm/scale
I0512 23:45:14.671107 139666521167872 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_9/self_attention/key/kernel
I0512 23:45:14.671144 139666521167872 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_9/self_attention/out/kernel
I0512 23:45:14.671175 139666521167872 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_9/self_attention/query/kernel
I0512 23:45:14.671206 139666521167872 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_9/self_attention/value/kernel
I0512 23:45:14.671243 139666521167872 checkpoints.py:1100] Restoring key from ckpt: target/decoder/logits_dense/kernel
I0512 23:45:14.671275 139666521167872 checkpoints.py:1100] Restoring key from ckpt: target/token_embedder/embedding
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
I0000 00:00:1715557514.688639  368289 gcs_resource.cc:109] Using default AdmissionQueue with limit 32
I0000 00:00:1715557514.695827  370458 google_auth_provider.cc:180] Running on GCE, using service account 1081612546678-compute@developer.gserviceaccount.com
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
I0000 00:00:1715557514.702114  368954 gcs_resource.cc:109] Using default AdmissionQueue with limit 32
I0000 00:00:1715557514.706613  371119 google_auth_provider.cc:180] Running on GCE, using service account 1081612546678-compute@developer.gserviceaccount.com
I0512 23:45:14.690128 140180528838656 checkpoints.py:1054] Restoring from checkpoint: gs://rosinality-tpu-bucket/openmoe_8b_ul2/training/checkpoint_10000/checkpoint
I0512 23:45:14.765251 140260698863616 utils.py:442] Initializing parameters from specific T5X checkpoint gs://rosinality-tpu-bucket/openmoe_8b_ul2/training/checkpoint_10000
I0512 23:45:14.833081 140193652815872 checkpoints.py:1054] Restoring from checkpoint: gs://rosinality-tpu-bucket/openmoe_8b_ul2/training/checkpoint_10000/checkpoint
I0512 23:45:14.997537 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/decoder_norm/scale/m
I0512 23:45:14.997753 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/decoder_norm/scale/v
I0512 23:45:14.997800 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/decoder_norm/scale/v_col
I0512 23:45:14.997837 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/decoder_norm/scale/v_row
I0512 23:45:14.997871 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_0/mlp/wi_0/kernel/m
I0512 23:45:14.997904 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_0/mlp/wi_0/kernel/v
I0512 23:45:14.997937 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_0/mlp/wi_0/kernel/v_col
I0512 23:45:14.997970 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_0/mlp/wi_0/kernel/v_row
I0512 23:45:14.998003 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_0/mlp/wi_1/kernel/m
I0512 23:45:14.998036 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_0/mlp/wi_1/kernel/v
I0512 23:45:14.998068 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_0/mlp/wi_1/kernel/v_col
I0512 23:45:14.998100 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_0/mlp/wi_1/kernel/v_row
I0512 23:45:14.998133 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_0/mlp/wo/kernel/m
I0512 23:45:14.998166 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_0/mlp/wo/kernel/v
I0512 23:45:14.998198 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_0/mlp/wo/kernel/v_col
I0512 23:45:14.998230 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_0/mlp/wo/kernel/v_row
I0512 23:45:14.998263 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_0/pre_mlp_layer_norm/scale/m
I0512 23:45:14.998296 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_0/pre_mlp_layer_norm/scale/v
I0512 23:45:14.998330 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_0/pre_mlp_layer_norm/scale/v_col
I0512 23:45:14.998363 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_0/pre_mlp_layer_norm/scale/v_row
I0512 23:45:14.998396 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_0/pre_self_attention_layer_norm/scale/m
I0512 23:45:14.998428 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_0/pre_self_attention_layer_norm/scale/v
I0512 23:45:14.998461 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_0/pre_self_attention_layer_norm/scale/v_col
I0512 23:45:14.998493 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_0/pre_self_attention_layer_norm/scale/v_row
I0512 23:45:14.998525 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_0/self_attention/key/kernel/m
I0512 23:45:14.998558 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_0/self_attention/key/kernel/v
I0512 23:45:14.998597 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_0/self_attention/key/kernel/v_col
I0512 23:45:14.998631 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_0/self_attention/key/kernel/v_row
I0512 23:45:14.998664 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_0/self_attention/out/kernel/m
I0512 23:45:14.998697 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_0/self_attention/out/kernel/v
I0512 23:45:14.998729 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_0/self_attention/out/kernel/v_col
I0512 23:45:14.998762 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_0/self_attention/out/kernel/v_row
I0512 23:45:14.998794 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_0/self_attention/query/kernel/m
I0512 23:45:14.998826 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_0/self_attention/query/kernel/v
I0512 23:45:14.998861 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_0/self_attention/query/kernel/v_col
I0512 23:45:14.998894 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_0/self_attention/query/kernel/v_row
I0512 23:45:14.998927 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_0/self_attention/value/kernel/m
I0512 23:45:14.998959 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_0/self_attention/value/kernel/v
I0512 23:45:14.998992 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_0/self_attention/value/kernel/v_col
I0512 23:45:14.999024 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_0/self_attention/value/kernel/v_row
I0512 23:45:14.999056 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_1/mlp/wi_0/kernel/m
I0512 23:45:14.999089 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_1/mlp/wi_0/kernel/v
I0512 23:45:14.999121 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_1/mlp/wi_0/kernel/v_col
I0512 23:45:14.999153 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_1/mlp/wi_0/kernel/v_row
I0512 23:45:14.999186 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_1/mlp/wi_1/kernel/m
I0512 23:45:14.999218 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_1/mlp/wi_1/kernel/v
I0512 23:45:14.999251 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_1/mlp/wi_1/kernel/v_col
I0512 23:45:14.999283 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_1/mlp/wi_1/kernel/v_row
I0512 23:45:14.999315 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_1/mlp/wo/kernel/m
I0512 23:45:14.999347 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_1/mlp/wo/kernel/v
I0512 23:45:14.999380 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_1/mlp/wo/kernel/v_col
I0512 23:45:14.999412 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_1/mlp/wo/kernel/v_row
I0512 23:45:14.999445 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_1/pre_mlp_layer_norm/scale/m
I0512 23:45:14.999478 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_1/pre_mlp_layer_norm/scale/v
I0512 23:45:14.999511 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_1/pre_mlp_layer_norm/scale/v_col
I0512 23:45:14.999542 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_1/pre_mlp_layer_norm/scale/v_row
I0512 23:45:14.999575 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_1/pre_self_attention_layer_norm/scale/m
I0512 23:45:14.999615 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_1/pre_self_attention_layer_norm/scale/v
I0512 23:45:14.999648 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_1/pre_self_attention_layer_norm/scale/v_col
I0512 23:45:14.999680 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_1/pre_self_attention_layer_norm/scale/v_row
I0512 23:45:14.999713 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_1/self_attention/key/kernel/m
I0512 23:45:14.999745 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_1/self_attention/key/kernel/v
I0512 23:45:14.999777 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_1/self_attention/key/kernel/v_col
I0512 23:45:14.999810 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_1/self_attention/key/kernel/v_row
I0512 23:45:14.999844 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_1/self_attention/out/kernel/m
I0512 23:45:14.999877 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_1/self_attention/out/kernel/v
I0512 23:45:14.999910 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_1/self_attention/out/kernel/v_col
I0512 23:45:14.999943 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_1/self_attention/out/kernel/v_row
I0512 23:45:14.999975 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_1/self_attention/query/kernel/m
I0512 23:45:15.000007 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_1/self_attention/query/kernel/v
I0512 23:45:15.000040 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_1/self_attention/query/kernel/v_col
I0512 23:45:15.000073 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_1/self_attention/query/kernel/v_row
I0512 23:45:15.000105 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_1/self_attention/value/kernel/m
I0512 23:45:15.000138 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_1/self_attention/value/kernel/v
I0512 23:45:15.000170 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_1/self_attention/value/kernel/v_col
I0512 23:45:15.000202 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_1/self_attention/value/kernel/v_row
I0512 23:45:15.000236 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_10/mlp/wi_0/kernel/m
I0512 23:45:15.000268 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_10/mlp/wi_0/kernel/v
I0512 23:45:15.000301 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_10/mlp/wi_0/kernel/v_col
I0512 23:45:15.000334 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_10/mlp/wi_0/kernel/v_row
I0512 23:45:15.000367 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_10/mlp/wi_1/kernel/m
I0512 23:45:15.000399 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_10/mlp/wi_1/kernel/v
I0512 23:45:15.000432 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_10/mlp/wi_1/kernel/v_col
I0512 23:45:15.000464 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_10/mlp/wi_1/kernel/v_row
I0512 23:45:15.000497 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_10/mlp/wo/kernel/m
I0512 23:45:15.000530 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_10/mlp/wo/kernel/v
I0512 23:45:15.000562 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_10/mlp/wo/kernel/v_col
I0512 23:45:15.000601 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_10/mlp/wo/kernel/v_row
I0512 23:45:15.000634 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_10/pre_mlp_layer_norm/scale/m
I0512 23:45:15.000667 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_10/pre_mlp_layer_norm/scale/v
I0512 23:45:15.000699 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_10/pre_mlp_layer_norm/scale/v_col
I0512 23:45:15.000732 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_10/pre_mlp_layer_norm/scale/v_row
I0512 23:45:15.000764 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_10/pre_self_attention_layer_norm/scale/m
I0512 23:45:15.000796 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_10/pre_self_attention_layer_norm/scale/v
I0512 23:45:15.000829 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_10/pre_self_attention_layer_norm/scale/v_col
I0512 23:45:15.000863 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_10/pre_self_attention_layer_norm/scale/v_row
I0512 23:45:15.000896 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_10/self_attention/key/kernel/m
I0512 23:45:15.000929 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_10/self_attention/key/kernel/v
I0512 23:45:15.000961 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_10/self_attention/key/kernel/v_col
I0512 23:45:15.000993 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_10/self_attention/key/kernel/v_row
I0512 23:45:15.001026 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_10/self_attention/out/kernel/m
I0512 23:45:15.001058 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_10/self_attention/out/kernel/v
I0512 23:45:15.001091 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_10/self_attention/out/kernel/v_col
I0512 23:45:15.001123 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_10/self_attention/out/kernel/v_row
I0512 23:45:15.001156 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_10/self_attention/query/kernel/m
I0512 23:45:15.001188 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_10/self_attention/query/kernel/v
I0512 23:45:15.001221 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_10/self_attention/query/kernel/v_col
I0512 23:45:15.001253 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_10/self_attention/query/kernel/v_row
I0512 23:45:15.001286 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_10/self_attention/value/kernel/m
I0512 23:45:15.001318 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_10/self_attention/value/kernel/v
I0512 23:45:15.001350 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_10/self_attention/value/kernel/v_col
I0512 23:45:15.001382 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_10/self_attention/value/kernel/v_row
I0512 23:45:15.001415 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/extra_mlp/wi_0/kernel/m
I0512 23:45:15.001447 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/extra_mlp/wi_0/kernel/v
I0512 23:45:15.001479 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/extra_mlp/wi_0/kernel/v_col
I0512 23:45:15.001512 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/extra_mlp/wi_0/kernel/v_row
I0512 23:45:15.001544 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/extra_mlp/wi_1/kernel/m
I0512 23:45:15.001583 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/extra_mlp/wi_1/kernel/v
I0512 23:45:15.001617 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/extra_mlp/wi_1/kernel/v_col
I0512 23:45:15.001649 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/extra_mlp/wi_1/kernel/v_row
I0512 23:45:15.001682 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/extra_mlp/wo/kernel/m
I0512 23:45:15.001740 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/extra_mlp/wo/kernel/v
I0512 23:45:15.001811 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/extra_mlp/wo/kernel/v_col
I0512 23:45:15.001848 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/extra_mlp/wo/kernel/v_row
I0512 23:45:15.001882 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/mlp/expert/wi_0/kernel/m
I0512 23:45:15.001914 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/mlp/expert/wi_0/kernel/v
I0512 23:45:15.001947 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/mlp/expert/wi_0/kernel/v_col
I0512 23:45:15.001980 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/mlp/expert/wi_0/kernel/v_row
I0512 23:45:15.002013 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/mlp/expert/wi_1/kernel/m
I0512 23:45:15.002045 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/mlp/expert/wi_1/kernel/v
I0512 23:45:15.002078 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/mlp/expert/wi_1/kernel/v_col
I0512 23:45:15.002110 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/mlp/expert/wi_1/kernel/v_row
I0512 23:45:15.002143 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/mlp/expert/wo/kernel/m
I0512 23:45:15.002176 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/mlp/expert/wo/kernel/v
I0512 23:45:15.002208 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/mlp/expert/wo/kernel/v_col
I0512 23:45:15.002241 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/mlp/expert/wo/kernel/v_row
I0512 23:45:15.002273 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/mlp/router/router_weights/w/kernel/m
I0512 23:45:15.002305 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/mlp/router/router_weights/w/kernel/v
I0512 23:45:15.002337 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/mlp/router/router_weights/w/kernel/v_col
I0512 23:45:15.002370 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/mlp/router/router_weights/w/kernel/v_row
I0512 23:45:15.002402 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/pre_extra_mlp_layer_norm/scale/m
I0512 23:45:15.002439 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/pre_extra_mlp_layer_norm/scale/v
I0512 23:45:15.002472 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/pre_extra_mlp_layer_norm/scale/v_col
I0512 23:45:15.002505 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/pre_extra_mlp_layer_norm/scale/v_row
I0512 23:45:15.002537 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/pre_mlp_layer_norm/scale/m
I0512 23:45:15.002569 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/pre_mlp_layer_norm/scale/v
I0512 23:45:15.002609 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/pre_mlp_layer_norm/scale/v_col
I0512 23:45:15.002642 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/pre_mlp_layer_norm/scale/v_row
I0512 23:45:15.002674 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/pre_self_attention_layer_norm/scale/m
I0512 23:45:15.002707 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/pre_self_attention_layer_norm/scale/v
I0512 23:45:15.002739 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/pre_self_attention_layer_norm/scale/v_col
I0512 23:45:15.002772 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/pre_self_attention_layer_norm/scale/v_row
I0512 23:45:15.002804 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/self_attention/key/kernel/m
I0512 23:45:15.002838 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/self_attention/key/kernel/v
I0512 23:45:15.002871 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/self_attention/key/kernel/v_col
I0512 23:45:15.002905 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/self_attention/key/kernel/v_row
I0512 23:45:15.002937 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/self_attention/out/kernel/m
I0512 23:45:15.002970 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/self_attention/out/kernel/v
I0512 23:45:15.003002 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/self_attention/out/kernel/v_col
I0512 23:45:15.003035 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/self_attention/out/kernel/v_row
I0512 23:45:15.003067 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/self_attention/query/kernel/m
I0512 23:45:15.003099 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/self_attention/query/kernel/v
I0512 23:45:15.003131 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/self_attention/query/kernel/v_col
I0512 23:45:15.003164 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/self_attention/query/kernel/v_row
I0512 23:45:15.003196 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/self_attention/value/kernel/m
I0512 23:45:15.003228 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/self_attention/value/kernel/v
I0512 23:45:15.003261 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/self_attention/value/kernel/v_col
I0512 23:45:15.003293 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/self_attention/value/kernel/v_row
I0512 23:45:15.003325 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_12/mlp/wi_0/kernel/m
I0512 23:45:15.003358 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_12/mlp/wi_0/kernel/v
I0512 23:45:15.003391 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_12/mlp/wi_0/kernel/v_col
I0512 23:45:15.003423 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_12/mlp/wi_0/kernel/v_row
I0512 23:45:15.003456 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_12/mlp/wi_1/kernel/m
I0512 23:45:15.003488 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_12/mlp/wi_1/kernel/v
I0512 23:45:15.003522 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_12/mlp/wi_1/kernel/v_col
I0512 23:45:15.003555 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_12/mlp/wi_1/kernel/v_row
I0512 23:45:15.003592 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_12/mlp/wo/kernel/m
I0512 23:45:15.003627 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_12/mlp/wo/kernel/v
I0512 23:45:15.003659 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_12/mlp/wo/kernel/v_col
I0512 23:45:15.003692 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_12/mlp/wo/kernel/v_row
I0512 23:45:15.003724 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_12/pre_mlp_layer_norm/scale/m
I0512 23:45:15.003756 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_12/pre_mlp_layer_norm/scale/v
I0512 23:45:15.003788 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_12/pre_mlp_layer_norm/scale/v_col
I0512 23:45:15.003821 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_12/pre_mlp_layer_norm/scale/v_row
I0512 23:45:15.003855 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_12/pre_self_attention_layer_norm/scale/m
I0512 23:45:15.003889 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_12/pre_self_attention_layer_norm/scale/v
I0512 23:45:15.003921 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_12/pre_self_attention_layer_norm/scale/v_col
I0512 23:45:15.003954 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_12/pre_self_attention_layer_norm/scale/v_row
I0512 23:45:15.003987 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_12/self_attention/key/kernel/m
I0512 23:45:15.004019 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_12/self_attention/key/kernel/v
I0512 23:45:15.004051 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_12/self_attention/key/kernel/v_col
I0512 23:45:15.004083 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_12/self_attention/key/kernel/v_row
I0512 23:45:15.004115 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_12/self_attention/out/kernel/m
I0512 23:45:15.004148 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_12/self_attention/out/kernel/v
I0512 23:45:15.004180 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_12/self_attention/out/kernel/v_col
I0512 23:45:15.004212 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_12/self_attention/out/kernel/v_row
I0512 23:45:15.004245 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_12/self_attention/query/kernel/m
I0512 23:45:15.004276 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_12/self_attention/query/kernel/v
I0512 23:45:15.004309 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_12/self_attention/query/kernel/v_col
I0512 23:45:15.004341 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_12/self_attention/query/kernel/v_row
I0512 23:45:15.004375 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_12/self_attention/value/kernel/m
I0512 23:45:15.004408 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_12/self_attention/value/kernel/v
I0512 23:45:15.004440 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_12/self_attention/value/kernel/v_col
I0512 23:45:15.004472 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_12/self_attention/value/kernel/v_row
I0512 23:45:15.004505 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_13/mlp/wi_0/kernel/m
I0512 23:45:15.004537 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_13/mlp/wi_0/kernel/v
I0512 23:45:15.004570 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_13/mlp/wi_0/kernel/v_col
I0512 23:45:15.004609 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_13/mlp/wi_0/kernel/v_row
I0512 23:45:15.004642 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_13/mlp/wi_1/kernel/m
I0512 23:45:15.004673 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_13/mlp/wi_1/kernel/v
I0512 23:45:15.004706 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_13/mlp/wi_1/kernel/v_col
I0512 23:45:15.004738 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_13/mlp/wi_1/kernel/v_row
I0512 23:45:15.004770 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_13/mlp/wo/kernel/m
I0512 23:45:15.004803 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_13/mlp/wo/kernel/v
I0512 23:45:15.004836 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_13/mlp/wo/kernel/v_col
I0512 23:45:15.004870 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_13/mlp/wo/kernel/v_row
I0512 23:45:15.004903 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_13/pre_mlp_layer_norm/scale/m
I0512 23:45:15.004935 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_13/pre_mlp_layer_norm/scale/v
I0512 23:45:15.004967 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_13/pre_mlp_layer_norm/scale/v_col
I0512 23:45:15.004999 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_13/pre_mlp_layer_norm/scale/v_row
I0512 23:45:15.005032 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_13/pre_self_attention_layer_norm/scale/m
I0512 23:45:15.005064 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_13/pre_self_attention_layer_norm/scale/v
I0512 23:45:15.005096 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_13/pre_self_attention_layer_norm/scale/v_col
I0512 23:45:15.005128 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_13/pre_self_attention_layer_norm/scale/v_row
I0512 23:45:15.005160 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_13/self_attention/key/kernel/m
I0512 23:45:15.005192 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_13/self_attention/key/kernel/v
I0512 23:45:15.005224 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_13/self_attention/key/kernel/v_col
I0512 23:45:15.005256 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_13/self_attention/key/kernel/v_row
I0512 23:45:15.005289 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_13/self_attention/out/kernel/m
I0512 23:45:15.005321 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_13/self_attention/out/kernel/v
I0512 23:45:15.005353 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_13/self_attention/out/kernel/v_col
I0512 23:45:15.005385 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_13/self_attention/out/kernel/v_row
I0512 23:45:15.005418 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_13/self_attention/query/kernel/m
I0512 23:45:15.005450 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_13/self_attention/query/kernel/v
I0512 23:45:15.005481 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_13/self_attention/query/kernel/v_col
I0512 23:45:15.005514 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_13/self_attention/query/kernel/v_row
I0512 23:45:15.005546 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_13/self_attention/value/kernel/m
I0512 23:45:15.005584 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_13/self_attention/value/kernel/v
I0512 23:45:15.005618 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_13/self_attention/value/kernel/v_col
I0512 23:45:15.005650 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_13/self_attention/value/kernel/v_row
I0512 23:45:15.005682 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_14/mlp/wi_0/kernel/m
I0512 23:45:15.005734 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_14/mlp/wi_0/kernel/v
I0512 23:45:15.005771 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_14/mlp/wi_0/kernel/v_col
I0512 23:45:15.005804 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_14/mlp/wi_0/kernel/v_row
I0512 23:45:15.005838 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_14/mlp/wi_1/kernel/m
I0512 23:45:15.005872 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_14/mlp/wi_1/kernel/v
I0512 23:45:15.005904 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_14/mlp/wi_1/kernel/v_col
I0512 23:45:15.005937 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_14/mlp/wi_1/kernel/v_row
I0512 23:45:15.005969 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_14/mlp/wo/kernel/m
I0512 23:45:15.006002 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_14/mlp/wo/kernel/v
I0512 23:45:15.006034 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_14/mlp/wo/kernel/v_col
I0512 23:45:15.006066 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_14/mlp/wo/kernel/v_row
I0512 23:45:15.006098 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_14/pre_mlp_layer_norm/scale/m
I0512 23:45:15.006131 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_14/pre_mlp_layer_norm/scale/v
I0512 23:45:15.006163 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_14/pre_mlp_layer_norm/scale/v_col
I0512 23:45:15.006196 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_14/pre_mlp_layer_norm/scale/v_row
I0512 23:45:15.006228 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_14/pre_self_attention_layer_norm/scale/m
I0512 23:45:15.006261 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_14/pre_self_attention_layer_norm/scale/v
I0512 23:45:15.006293 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_14/pre_self_attention_layer_norm/scale/v_col
I0512 23:45:15.006325 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_14/pre_self_attention_layer_norm/scale/v_row
I0512 23:45:15.006358 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_14/self_attention/key/kernel/m
I0512 23:45:15.006391 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_14/self_attention/key/kernel/v
I0512 23:45:15.006423 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_14/self_attention/key/kernel/v_col
I0512 23:45:15.006455 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_14/self_attention/key/kernel/v_row
I0512 23:45:15.006487 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_14/self_attention/out/kernel/m
I0512 23:45:15.006520 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_14/self_attention/out/kernel/v
I0512 23:45:15.006552 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_14/self_attention/out/kernel/v_col
I0512 23:45:15.006590 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_14/self_attention/out/kernel/v_row
I0512 23:45:15.006624 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_14/self_attention/query/kernel/m
I0512 23:45:15.006657 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_14/self_attention/query/kernel/v
I0512 23:45:15.006689 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_14/self_attention/query/kernel/v_col
I0512 23:45:15.006721 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_14/self_attention/query/kernel/v_row
I0512 23:45:15.006753 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_14/self_attention/value/kernel/m
I0512 23:45:15.006785 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_14/self_attention/value/kernel/v
I0512 23:45:15.006817 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_14/self_attention/value/kernel/v_col
I0512 23:45:15.006852 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_14/self_attention/value/kernel/v_row
I0512 23:45:15.006885 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_15/mlp/wi_0/kernel/m
I0512 23:45:15.006917 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_15/mlp/wi_0/kernel/v
I0512 23:45:15.006950 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_15/mlp/wi_0/kernel/v_col
I0512 23:45:15.006982 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_15/mlp/wi_0/kernel/v_row
I0512 23:45:15.007014 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_15/mlp/wi_1/kernel/m
I0512 23:45:15.007047 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_15/mlp/wi_1/kernel/v
I0512 23:45:15.007079 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_15/mlp/wi_1/kernel/v_col
I0512 23:45:15.007112 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_15/mlp/wi_1/kernel/v_row
I0512 23:45:15.007144 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_15/mlp/wo/kernel/m
I0512 23:45:15.007176 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_15/mlp/wo/kernel/v
I0512 23:45:15.007209 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_15/mlp/wo/kernel/v_col
I0512 23:45:15.007241 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_15/mlp/wo/kernel/v_row
I0512 23:45:15.007273 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_15/pre_mlp_layer_norm/scale/m
I0512 23:45:15.007306 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_15/pre_mlp_layer_norm/scale/v
I0512 23:45:15.007338 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_15/pre_mlp_layer_norm/scale/v_col
I0512 23:45:15.007370 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_15/pre_mlp_layer_norm/scale/v_row
I0512 23:45:15.007402 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_15/pre_self_attention_layer_norm/scale/m
I0512 23:45:15.007435 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_15/pre_self_attention_layer_norm/scale/v
I0512 23:45:15.007467 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_15/pre_self_attention_layer_norm/scale/v_col
I0512 23:45:15.007499 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_15/pre_self_attention_layer_norm/scale/v_row
I0512 23:45:15.007532 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_15/self_attention/key/kernel/m
I0512 23:45:15.007564 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_15/self_attention/key/kernel/v
I0512 23:45:15.007602 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_15/self_attention/key/kernel/v_col
I0512 23:45:15.007635 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_15/self_attention/key/kernel/v_row
I0512 23:45:15.007668 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_15/self_attention/out/kernel/m
I0512 23:45:15.007699 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_15/self_attention/out/kernel/v
I0512 23:45:15.007731 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_15/self_attention/out/kernel/v_col
I0512 23:45:15.007764 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_15/self_attention/out/kernel/v_row
I0512 23:45:15.007796 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_15/self_attention/query/kernel/m
I0512 23:45:15.007828 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_15/self_attention/query/kernel/v
I0512 23:45:15.007863 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_15/self_attention/query/kernel/v_col
I0512 23:45:15.007899 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_15/self_attention/query/kernel/v_row
I0512 23:45:15.007935 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_15/self_attention/value/kernel/m
I0512 23:45:15.007973 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_15/self_attention/value/kernel/v
I0512 23:45:15.008008 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_15/self_attention/value/kernel/v_col
I0512 23:45:15.008040 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_15/self_attention/value/kernel/v_row
I0512 23:45:15.008079 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_16/mlp/wi_0/kernel/m
I0512 23:45:15.008113 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_16/mlp/wi_0/kernel/v
I0512 23:45:15.008146 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_16/mlp/wi_0/kernel/v_col
I0512 23:45:15.008178 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_16/mlp/wi_0/kernel/v_row
I0512 23:45:15.008210 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_16/mlp/wi_1/kernel/m
I0512 23:45:15.008241 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_16/mlp/wi_1/kernel/v
I0512 23:45:15.008273 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_16/mlp/wi_1/kernel/v_col
I0512 23:45:15.008305 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_16/mlp/wi_1/kernel/v_row
I0512 23:45:15.008336 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_16/mlp/wo/kernel/m
I0512 23:45:15.008368 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_16/mlp/wo/kernel/v
I0512 23:45:15.008399 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_16/mlp/wo/kernel/v_col
I0512 23:45:15.008431 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_16/mlp/wo/kernel/v_row
I0512 23:45:15.008463 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_16/pre_mlp_layer_norm/scale/m
I0512 23:45:15.008494 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_16/pre_mlp_layer_norm/scale/v
I0512 23:45:15.008525 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_16/pre_mlp_layer_norm/scale/v_col
I0512 23:45:15.008563 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_16/pre_mlp_layer_norm/scale/v_row
I0512 23:45:15.008602 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_16/pre_self_attention_layer_norm/scale/m
I0512 23:45:15.008636 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_16/pre_self_attention_layer_norm/scale/v
I0512 23:45:15.008667 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_16/pre_self_attention_layer_norm/scale/v_col
I0512 23:45:15.008700 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_16/pre_self_attention_layer_norm/scale/v_row
I0512 23:45:15.008731 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_16/self_attention/key/kernel/m
I0512 23:45:15.008763 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_16/self_attention/key/kernel/v
I0512 23:45:15.008794 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_16/self_attention/key/kernel/v_col
I0512 23:45:15.008826 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_16/self_attention/key/kernel/v_row
I0512 23:45:15.008860 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_16/self_attention/out/kernel/m
I0512 23:45:15.008892 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_16/self_attention/out/kernel/v
I0512 23:45:15.008924 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_16/self_attention/out/kernel/v_col
I0512 23:45:15.008956 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_16/self_attention/out/kernel/v_row
I0512 23:45:15.008987 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_16/self_attention/query/kernel/m
I0512 23:45:15.009019 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_16/self_attention/query/kernel/v
I0512 23:45:15.009050 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_16/self_attention/query/kernel/v_col
I0512 23:45:15.009082 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_16/self_attention/query/kernel/v_row
I0512 23:45:15.009113 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_16/self_attention/value/kernel/m
I0512 23:45:15.009145 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_16/self_attention/value/kernel/v
I0512 23:45:15.009176 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_16/self_attention/value/kernel/v_col
I0512 23:45:15.009208 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_16/self_attention/value/kernel/v_row
I0512 23:45:15.009240 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/extra_mlp/wi_0/kernel/m
I0512 23:45:15.009271 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/extra_mlp/wi_0/kernel/v
I0512 23:45:15.009303 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/extra_mlp/wi_0/kernel/v_col
I0512 23:45:15.009334 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/extra_mlp/wi_0/kernel/v_row
I0512 23:45:15.009366 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/extra_mlp/wi_1/kernel/m
I0512 23:45:15.009397 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/extra_mlp/wi_1/kernel/v
I0512 23:45:15.009429 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/extra_mlp/wi_1/kernel/v_col
I0512 23:45:15.009460 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/extra_mlp/wi_1/kernel/v_row
I0512 23:45:15.009492 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/extra_mlp/wo/kernel/m
I0512 23:45:15.009523 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/extra_mlp/wo/kernel/v
I0512 23:45:15.009555 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/extra_mlp/wo/kernel/v_col
I0512 23:45:15.009592 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/extra_mlp/wo/kernel/v_row
I0512 23:45:15.009625 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/mlp/expert/wi_0/kernel/m
I0512 23:45:15.009657 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/mlp/expert/wi_0/kernel/v
I0512 23:45:15.009688 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/mlp/expert/wi_0/kernel/v_col
I0512 23:45:15.009749 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/mlp/expert/wi_0/kernel/v_row
I0512 23:45:15.009784 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/mlp/expert/wi_1/kernel/m
I0512 23:45:15.009816 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/mlp/expert/wi_1/kernel/v
I0512 23:45:15.009849 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/mlp/expert/wi_1/kernel/v_col
I0512 23:45:15.009882 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/mlp/expert/wi_1/kernel/v_row
I0512 23:45:15.009913 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/mlp/expert/wo/kernel/m
I0512 23:45:15.009945 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/mlp/expert/wo/kernel/v
I0512 23:45:15.009976 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/mlp/expert/wo/kernel/v_col
I0512 23:45:15.010008 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/mlp/expert/wo/kernel/v_row
I0512 23:45:15.010039 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/mlp/router/router_weights/w/kernel/m
I0512 23:45:15.010071 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/mlp/router/router_weights/w/kernel/v
I0512 23:45:15.010102 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/mlp/router/router_weights/w/kernel/v_col
I0512 23:45:15.010134 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/mlp/router/router_weights/w/kernel/v_row
I0512 23:45:15.010166 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/pre_extra_mlp_layer_norm/scale/m
I0512 23:45:15.010198 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/pre_extra_mlp_layer_norm/scale/v
I0512 23:45:15.010229 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/pre_extra_mlp_layer_norm/scale/v_col
I0512 23:45:15.010260 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/pre_extra_mlp_layer_norm/scale/v_row
I0512 23:45:15.010291 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/pre_mlp_layer_norm/scale/m
I0512 23:45:15.010323 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/pre_mlp_layer_norm/scale/v
I0512 23:45:15.010354 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/pre_mlp_layer_norm/scale/v_col
I0512 23:45:15.010385 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/pre_mlp_layer_norm/scale/v_row
I0512 23:45:15.010417 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/pre_self_attention_layer_norm/scale/m
I0512 23:45:15.010448 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/pre_self_attention_layer_norm/scale/v
I0512 23:45:15.010480 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/pre_self_attention_layer_norm/scale/v_col
I0512 23:45:15.010511 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/pre_self_attention_layer_norm/scale/v_row
I0512 23:45:15.010543 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/self_attention/key/kernel/m
I0512 23:45:15.010574 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/self_attention/key/kernel/v
I0512 23:45:15.010614 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/self_attention/key/kernel/v_col
I0512 23:45:15.010646 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/self_attention/key/kernel/v_row
I0512 23:45:15.010677 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/self_attention/out/kernel/m
I0512 23:45:15.010708 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/self_attention/out/kernel/v
I0512 23:45:15.010739 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/self_attention/out/kernel/v_col
I0512 23:45:15.010771 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/self_attention/out/kernel/v_row
I0512 23:45:15.010802 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/self_attention/query/kernel/m
I0512 23:45:15.010834 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/self_attention/query/kernel/v
I0512 23:45:15.010866 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/self_attention/query/kernel/v_col
I0512 23:45:15.010898 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/self_attention/query/kernel/v_row
I0512 23:45:15.010928 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/self_attention/value/kernel/m
I0512 23:45:15.010960 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/self_attention/value/kernel/v
I0512 23:45:15.010991 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/self_attention/value/kernel/v_col
I0512 23:45:15.011022 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/self_attention/value/kernel/v_row
I0512 23:45:15.011053 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_18/mlp/wi_0/kernel/m
I0512 23:45:15.011084 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_18/mlp/wi_0/kernel/v
I0512 23:45:15.011116 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_18/mlp/wi_0/kernel/v_col
I0512 23:45:15.011147 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_18/mlp/wi_0/kernel/v_row
I0512 23:45:15.011178 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_18/mlp/wi_1/kernel/m
I0512 23:45:15.011209 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_18/mlp/wi_1/kernel/v
I0512 23:45:15.011240 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_18/mlp/wi_1/kernel/v_col
I0512 23:45:15.011271 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_18/mlp/wi_1/kernel/v_row
I0512 23:45:15.011302 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_18/mlp/wo/kernel/m
I0512 23:45:15.011333 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_18/mlp/wo/kernel/v
I0512 23:45:15.011363 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_18/mlp/wo/kernel/v_col
I0512 23:45:15.011394 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_18/mlp/wo/kernel/v_row
I0512 23:45:15.011425 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_18/pre_mlp_layer_norm/scale/m
I0512 23:45:15.011456 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_18/pre_mlp_layer_norm/scale/v
I0512 23:45:15.011487 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_18/pre_mlp_layer_norm/scale/v_col
I0512 23:45:15.011518 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_18/pre_mlp_layer_norm/scale/v_row
I0512 23:45:15.011549 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_18/pre_self_attention_layer_norm/scale/m
I0512 23:45:15.011585 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_18/pre_self_attention_layer_norm/scale/v
I0512 23:45:15.011618 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_18/pre_self_attention_layer_norm/scale/v_col
I0512 23:45:15.011650 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_18/pre_self_attention_layer_norm/scale/v_row
I0512 23:45:15.011681 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_18/self_attention/key/kernel/m
I0512 23:45:15.011713 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_18/self_attention/key/kernel/v
I0512 23:45:15.011744 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_18/self_attention/key/kernel/v_col
I0512 23:45:15.011775 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_18/self_attention/key/kernel/v_row
I0512 23:45:15.011806 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_18/self_attention/out/kernel/m
I0512 23:45:15.011838 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_18/self_attention/out/kernel/v
I0512 23:45:15.011870 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_18/self_attention/out/kernel/v_col
I0512 23:45:15.011901 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_18/self_attention/out/kernel/v_row
I0512 23:45:15.011932 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_18/self_attention/query/kernel/m
I0512 23:45:15.011963 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_18/self_attention/query/kernel/v
I0512 23:45:15.011994 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_18/self_attention/query/kernel/v_col
I0512 23:45:15.012026 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_18/self_attention/query/kernel/v_row
I0512 23:45:15.012057 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_18/self_attention/value/kernel/m
I0512 23:45:15.012088 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_18/self_attention/value/kernel/v
I0512 23:45:15.012119 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_18/self_attention/value/kernel/v_col
I0512 23:45:15.012150 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_18/self_attention/value/kernel/v_row
I0512 23:45:15.012181 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_19/mlp/wi_0/kernel/m
I0512 23:45:15.012212 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_19/mlp/wi_0/kernel/v
I0512 23:45:15.012243 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_19/mlp/wi_0/kernel/v_col
I0512 23:45:15.012274 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_19/mlp/wi_0/kernel/v_row
I0512 23:45:15.012305 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_19/mlp/wi_1/kernel/m
I0512 23:45:15.012336 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_19/mlp/wi_1/kernel/v
I0512 23:45:15.012367 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_19/mlp/wi_1/kernel/v_col
I0512 23:45:15.012397 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_19/mlp/wi_1/kernel/v_row
I0512 23:45:15.012428 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_19/mlp/wo/kernel/m
I0512 23:45:15.012458 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_19/mlp/wo/kernel/v
I0512 23:45:15.012489 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_19/mlp/wo/kernel/v_col
I0512 23:45:15.012520 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_19/mlp/wo/kernel/v_row
I0512 23:45:15.012551 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_19/pre_mlp_layer_norm/scale/m
I0512 23:45:15.012588 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_19/pre_mlp_layer_norm/scale/v
I0512 23:45:15.012620 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_19/pre_mlp_layer_norm/scale/v_col
I0512 23:45:15.012652 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_19/pre_mlp_layer_norm/scale/v_row
I0512 23:45:15.012683 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_19/pre_self_attention_layer_norm/scale/m
I0512 23:45:15.012714 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_19/pre_self_attention_layer_norm/scale/v
I0512 23:45:15.012745 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_19/pre_self_attention_layer_norm/scale/v_col
I0512 23:45:15.012776 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_19/pre_self_attention_layer_norm/scale/v_row
I0512 23:45:15.012808 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_19/self_attention/key/kernel/m
I0512 23:45:15.012840 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_19/self_attention/key/kernel/v
I0512 23:45:15.012873 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_19/self_attention/key/kernel/v_col
I0512 23:45:15.012904 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_19/self_attention/key/kernel/v_row
I0512 23:45:15.012935 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_19/self_attention/out/kernel/m
I0512 23:45:15.012966 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_19/self_attention/out/kernel/v
I0512 23:45:15.012997 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_19/self_attention/out/kernel/v_col
I0512 23:45:15.013028 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_19/self_attention/out/kernel/v_row
I0512 23:45:15.013060 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_19/self_attention/query/kernel/m
I0512 23:45:15.013091 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_19/self_attention/query/kernel/v
I0512 23:45:15.013122 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_19/self_attention/query/kernel/v_col
I0512 23:45:15.013153 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_19/self_attention/query/kernel/v_row
I0512 23:45:15.013184 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_19/self_attention/value/kernel/m
I0512 23:45:15.013215 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_19/self_attention/value/kernel/v
I0512 23:45:15.013246 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_19/self_attention/value/kernel/v_col
I0512 23:45:15.013277 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_19/self_attention/value/kernel/v_row
I0512 23:45:15.013308 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_2/mlp/wi_0/kernel/m
I0512 23:45:15.013339 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_2/mlp/wi_0/kernel/v
I0512 23:45:15.013370 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_2/mlp/wi_0/kernel/v_col
I0512 23:45:15.013402 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_2/mlp/wi_0/kernel/v_row
I0512 23:45:15.013432 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_2/mlp/wi_1/kernel/m
I0512 23:45:15.013463 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_2/mlp/wi_1/kernel/v
I0512 23:45:15.013494 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_2/mlp/wi_1/kernel/v_col
I0512 23:45:15.013525 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_2/mlp/wi_1/kernel/v_row
I0512 23:45:15.013556 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_2/mlp/wo/kernel/m
I0512 23:45:15.013592 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_2/mlp/wo/kernel/v
I0512 23:45:15.013624 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_2/mlp/wo/kernel/v_col
I0512 23:45:15.013656 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_2/mlp/wo/kernel/v_row
I0512 23:45:15.013687 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_2/pre_mlp_layer_norm/scale/m
I0512 23:45:15.013740 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_2/pre_mlp_layer_norm/scale/v
I0512 23:45:15.013775 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_2/pre_mlp_layer_norm/scale/v_col
I0512 23:45:15.013806 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_2/pre_mlp_layer_norm/scale/v_row
I0512 23:45:15.013839 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_2/pre_self_attention_layer_norm/scale/m
I0512 23:45:15.013871 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_2/pre_self_attention_layer_norm/scale/v
I0512 23:45:15.013903 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_2/pre_self_attention_layer_norm/scale/v_col
I0512 23:45:15.013934 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_2/pre_self_attention_layer_norm/scale/v_row
I0512 23:45:15.013966 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_2/self_attention/key/kernel/m
I0512 23:45:15.013997 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_2/self_attention/key/kernel/v
I0512 23:45:15.014028 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_2/self_attention/key/kernel/v_col
I0512 23:45:15.014059 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_2/self_attention/key/kernel/v_row
I0512 23:45:15.014090 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_2/self_attention/out/kernel/m
I0512 23:45:15.014122 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_2/self_attention/out/kernel/v
I0512 23:45:15.014153 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_2/self_attention/out/kernel/v_col
I0512 23:45:15.014184 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_2/self_attention/out/kernel/v_row
I0512 23:45:15.014215 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_2/self_attention/query/kernel/m
I0512 23:45:15.014246 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_2/self_attention/query/kernel/v
I0512 23:45:15.014278 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_2/self_attention/query/kernel/v_col
I0512 23:45:15.014309 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_2/self_attention/query/kernel/v_row
I0512 23:45:15.014340 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_2/self_attention/value/kernel/m
I0512 23:45:15.014371 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_2/self_attention/value/kernel/v
I0512 23:45:15.014402 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_2/self_attention/value/kernel/v_col
I0512 23:45:15.014433 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_2/self_attention/value/kernel/v_row
I0512 23:45:15.014464 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_20/mlp/wi_0/kernel/m
I0512 23:45:15.014495 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_20/mlp/wi_0/kernel/v
I0512 23:45:15.014526 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_20/mlp/wi_0/kernel/v_col
I0512 23:45:15.014557 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_20/mlp/wi_0/kernel/v_row
I0512 23:45:15.014595 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_20/mlp/wi_1/kernel/m
I0512 23:45:15.014627 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_20/mlp/wi_1/kernel/v
I0512 23:45:15.014658 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_20/mlp/wi_1/kernel/v_col
I0512 23:45:15.014690 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_20/mlp/wi_1/kernel/v_row
I0512 23:45:15.014720 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_20/mlp/wo/kernel/m
I0512 23:45:15.014751 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_20/mlp/wo/kernel/v
I0512 23:45:15.014782 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_20/mlp/wo/kernel/v_col
I0512 23:45:15.014813 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_20/mlp/wo/kernel/v_row
I0512 23:45:15.014846 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_20/pre_mlp_layer_norm/scale/m
I0512 23:45:15.014878 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_20/pre_mlp_layer_norm/scale/v
I0512 23:45:15.014909 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_20/pre_mlp_layer_norm/scale/v_col
I0512 23:45:15.014941 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_20/pre_mlp_layer_norm/scale/v_row
I0512 23:45:15.014972 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_20/pre_self_attention_layer_norm/scale/m
I0512 23:45:15.015003 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_20/pre_self_attention_layer_norm/scale/v
I0512 23:45:15.015035 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_20/pre_self_attention_layer_norm/scale/v_col
I0512 23:45:15.015066 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_20/pre_self_attention_layer_norm/scale/v_row
I0512 23:45:15.015098 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_20/self_attention/key/kernel/m
I0512 23:45:15.015129 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_20/self_attention/key/kernel/v
I0512 23:45:15.015160 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_20/self_attention/key/kernel/v_col
I0512 23:45:15.015191 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_20/self_attention/key/kernel/v_row
I0512 23:45:15.015222 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_20/self_attention/out/kernel/m
I0512 23:45:15.015253 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_20/self_attention/out/kernel/v
I0512 23:45:15.015285 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_20/self_attention/out/kernel/v_col
I0512 23:45:15.015316 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_20/self_attention/out/kernel/v_row
I0512 23:45:15.015347 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_20/self_attention/query/kernel/m
I0512 23:45:15.015378 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_20/self_attention/query/kernel/v
I0512 23:45:15.015409 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_20/self_attention/query/kernel/v_col
I0512 23:45:15.015445 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_20/self_attention/query/kernel/v_row
I0512 23:45:15.015476 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_20/self_attention/value/kernel/m
I0512 23:45:15.015507 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_20/self_attention/value/kernel/v
I0512 23:45:15.015538 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_20/self_attention/value/kernel/v_col
I0512 23:45:15.015569 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_20/self_attention/value/kernel/v_row
I0512 23:45:15.015607 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_21/mlp/wi_0/kernel/m
I0512 23:45:15.015639 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_21/mlp/wi_0/kernel/v
I0512 23:45:15.015670 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_21/mlp/wi_0/kernel/v_col
I0512 23:45:15.015702 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_21/mlp/wi_0/kernel/v_row
I0512 23:45:15.015732 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_21/mlp/wi_1/kernel/m
I0512 23:45:15.015763 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_21/mlp/wi_1/kernel/v
I0512 23:45:15.015794 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_21/mlp/wi_1/kernel/v_col
I0512 23:45:15.015825 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_21/mlp/wi_1/kernel/v_row
I0512 23:45:15.015858 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_21/mlp/wo/kernel/m
I0512 23:45:15.015890 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_21/mlp/wo/kernel/v
I0512 23:45:15.015921 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_21/mlp/wo/kernel/v_col
I0512 23:45:15.015951 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_21/mlp/wo/kernel/v_row
I0512 23:45:15.015982 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_21/pre_mlp_layer_norm/scale/m
I0512 23:45:15.016013 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_21/pre_mlp_layer_norm/scale/v
I0512 23:45:15.016044 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_21/pre_mlp_layer_norm/scale/v_col
I0512 23:45:15.016075 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_21/pre_mlp_layer_norm/scale/v_row
I0512 23:45:15.016106 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_21/pre_self_attention_layer_norm/scale/m
I0512 23:45:15.016138 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_21/pre_self_attention_layer_norm/scale/v
I0512 23:45:15.016169 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_21/pre_self_attention_layer_norm/scale/v_col
I0512 23:45:15.016200 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_21/pre_self_attention_layer_norm/scale/v_row
I0512 23:45:15.016232 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_21/self_attention/key/kernel/m
I0512 23:45:15.016264 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_21/self_attention/key/kernel/v
I0512 23:45:15.016295 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_21/self_attention/key/kernel/v_col
I0512 23:45:15.016326 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_21/self_attention/key/kernel/v_row
I0512 23:45:15.016357 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_21/self_attention/out/kernel/m
I0512 23:45:15.016387 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_21/self_attention/out/kernel/v
I0512 23:45:15.016418 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_21/self_attention/out/kernel/v_col
I0512 23:45:15.016450 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_21/self_attention/out/kernel/v_row
I0512 23:45:15.016480 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_21/self_attention/query/kernel/m
I0512 23:45:15.016511 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_21/self_attention/query/kernel/v
I0512 23:45:15.016542 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_21/self_attention/query/kernel/v_col
I0512 23:45:15.016573 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_21/self_attention/query/kernel/v_row
I0512 23:45:15.016611 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_21/self_attention/value/kernel/m
I0512 23:45:15.016642 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_21/self_attention/value/kernel/v
I0512 23:45:15.016674 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_21/self_attention/value/kernel/v_col
I0512 23:45:15.016705 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_21/self_attention/value/kernel/v_row
I0512 23:45:15.016736 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_22/mlp/wi_0/kernel/m
I0512 23:45:15.016766 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_22/mlp/wi_0/kernel/v
I0512 23:45:15.016798 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_22/mlp/wi_0/kernel/v_col
I0512 23:45:15.016829 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_22/mlp/wi_0/kernel/v_row
I0512 23:45:15.016862 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_22/mlp/wi_1/kernel/m
I0512 23:45:15.016894 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_22/mlp/wi_1/kernel/v
I0512 23:45:15.016925 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_22/mlp/wi_1/kernel/v_col
I0512 23:45:15.016955 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_22/mlp/wi_1/kernel/v_row
I0512 23:45:15.016986 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_22/mlp/wo/kernel/m
I0512 23:45:15.017017 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_22/mlp/wo/kernel/v
I0512 23:45:15.017048 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_22/mlp/wo/kernel/v_col
I0512 23:45:15.017079 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_22/mlp/wo/kernel/v_row
I0512 23:45:15.017111 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_22/pre_mlp_layer_norm/scale/m
I0512 23:45:15.017142 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_22/pre_mlp_layer_norm/scale/v
I0512 23:45:15.017173 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_22/pre_mlp_layer_norm/scale/v_col
I0512 23:45:15.017204 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_22/pre_mlp_layer_norm/scale/v_row
I0512 23:45:15.017234 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_22/pre_self_attention_layer_norm/scale/m
I0512 23:45:15.017265 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_22/pre_self_attention_layer_norm/scale/v
I0512 23:45:15.017296 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_22/pre_self_attention_layer_norm/scale/v_col
I0512 23:45:15.017327 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_22/pre_self_attention_layer_norm/scale/v_row
I0512 23:45:15.017358 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_22/self_attention/key/kernel/m
I0512 23:45:15.017389 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_22/self_attention/key/kernel/v
I0512 23:45:15.017420 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_22/self_attention/key/kernel/v_col
I0512 23:45:15.017451 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_22/self_attention/key/kernel/v_row
I0512 23:45:15.017482 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_22/self_attention/out/kernel/m
I0512 23:45:15.017513 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_22/self_attention/out/kernel/v
I0512 23:45:15.017543 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_22/self_attention/out/kernel/v_col
I0512 23:45:15.017574 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_22/self_attention/out/kernel/v_row
I0512 23:45:15.017612 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_22/self_attention/query/kernel/m
I0512 23:45:15.017643 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_22/self_attention/query/kernel/v
I0512 23:45:15.017674 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_22/self_attention/query/kernel/v_col
I0512 23:45:15.017723 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_22/self_attention/query/kernel/v_row
I0512 23:45:15.017760 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_22/self_attention/value/kernel/m
I0512 23:45:15.017792 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_22/self_attention/value/kernel/v
I0512 23:45:15.017823 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_22/self_attention/value/kernel/v_col
I0512 23:45:15.017856 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_22/self_attention/value/kernel/v_row
I0512 23:45:15.017888 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/extra_mlp/wi_0/kernel/m
I0512 23:45:15.017919 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/extra_mlp/wi_0/kernel/v
I0512 23:45:15.017949 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/extra_mlp/wi_0/kernel/v_col
I0512 23:45:15.017980 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/extra_mlp/wi_0/kernel/v_row
I0512 23:45:15.018011 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/extra_mlp/wi_1/kernel/m
I0512 23:45:15.018042 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/extra_mlp/wi_1/kernel/v
I0512 23:45:15.018073 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/extra_mlp/wi_1/kernel/v_col
I0512 23:45:15.018104 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/extra_mlp/wi_1/kernel/v_row
I0512 23:45:15.018135 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/extra_mlp/wo/kernel/m
I0512 23:45:15.018166 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/extra_mlp/wo/kernel/v
I0512 23:45:15.018197 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/extra_mlp/wo/kernel/v_col
I0512 23:45:15.018228 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/extra_mlp/wo/kernel/v_row
I0512 23:45:15.018259 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/mlp/expert/wi_0/kernel/m
I0512 23:45:15.018289 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/mlp/expert/wi_0/kernel/v
I0512 23:45:15.018320 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/mlp/expert/wi_0/kernel/v_col
I0512 23:45:15.018351 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/mlp/expert/wi_0/kernel/v_row
I0512 23:45:15.018382 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/mlp/expert/wi_1/kernel/m
I0512 23:45:15.018412 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/mlp/expert/wi_1/kernel/v
I0512 23:45:15.018448 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/mlp/expert/wi_1/kernel/v_col
I0512 23:45:15.018478 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/mlp/expert/wi_1/kernel/v_row
I0512 23:45:15.018509 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/mlp/expert/wo/kernel/m
I0512 23:45:15.018539 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/mlp/expert/wo/kernel/v
I0512 23:45:15.018570 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/mlp/expert/wo/kernel/v_col
I0512 23:45:15.018608 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/mlp/expert/wo/kernel/v_row
I0512 23:45:15.018640 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/mlp/router/router_weights/w/kernel/m
I0512 23:45:15.018671 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/mlp/router/router_weights/w/kernel/v
I0512 23:45:15.018702 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/mlp/router/router_weights/w/kernel/v_col
I0512 23:45:15.018733 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/mlp/router/router_weights/w/kernel/v_row
I0512 23:45:15.018764 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/pre_extra_mlp_layer_norm/scale/m
I0512 23:45:15.018795 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/pre_extra_mlp_layer_norm/scale/v
I0512 23:45:15.018825 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/pre_extra_mlp_layer_norm/scale/v_col
I0512 23:45:15.018858 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/pre_extra_mlp_layer_norm/scale/v_row
I0512 23:45:15.018889 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/pre_mlp_layer_norm/scale/m
I0512 23:45:15.018920 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/pre_mlp_layer_norm/scale/v
I0512 23:45:15.018951 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/pre_mlp_layer_norm/scale/v_col
I0512 23:45:15.018982 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/pre_mlp_layer_norm/scale/v_row
I0512 23:45:15.019012 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/pre_self_attention_layer_norm/scale/m
I0512 23:45:15.019043 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/pre_self_attention_layer_norm/scale/v
I0512 23:45:15.019074 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/pre_self_attention_layer_norm/scale/v_col
I0512 23:45:15.019105 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/pre_self_attention_layer_norm/scale/v_row
I0512 23:45:15.019136 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/self_attention/key/kernel/m
I0512 23:45:15.019166 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/self_attention/key/kernel/v
I0512 23:45:15.019197 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/self_attention/key/kernel/v_col
I0512 23:45:15.019228 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/self_attention/key/kernel/v_row
I0512 23:45:15.019259 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/self_attention/out/kernel/m
I0512 23:45:15.019289 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/self_attention/out/kernel/v
I0512 23:45:15.019320 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/self_attention/out/kernel/v_col
I0512 23:45:15.019351 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/self_attention/out/kernel/v_row
I0512 23:45:15.019381 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/self_attention/query/kernel/m
I0512 23:45:15.019412 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/self_attention/query/kernel/v
I0512 23:45:15.019443 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/self_attention/query/kernel/v_col
I0512 23:45:15.019474 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/self_attention/query/kernel/v_row
I0512 23:45:15.019504 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/self_attention/value/kernel/m
I0512 23:45:15.019535 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/self_attention/value/kernel/v
I0512 23:45:15.019565 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/self_attention/value/kernel/v_col
I0512 23:45:15.019606 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/self_attention/value/kernel/v_row
I0512 23:45:15.019638 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_3/mlp/wi_0/kernel/m
I0512 23:45:15.019668 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_3/mlp/wi_0/kernel/v
I0512 23:45:15.019700 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_3/mlp/wi_0/kernel/v_col
I0512 23:45:15.019730 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_3/mlp/wi_0/kernel/v_row
I0512 23:45:15.019761 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_3/mlp/wi_1/kernel/m
I0512 23:45:15.019792 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_3/mlp/wi_1/kernel/v
I0512 23:45:15.019823 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_3/mlp/wi_1/kernel/v_col
I0512 23:45:15.019856 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_3/mlp/wi_1/kernel/v_row
I0512 23:45:15.019887 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_3/mlp/wo/kernel/m
I0512 23:45:15.019918 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_3/mlp/wo/kernel/v
I0512 23:45:15.019949 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_3/mlp/wo/kernel/v_col
I0512 23:45:15.019980 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_3/mlp/wo/kernel/v_row
I0512 23:45:15.020011 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_3/pre_mlp_layer_norm/scale/m
I0512 23:45:15.020042 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_3/pre_mlp_layer_norm/scale/v
I0512 23:45:15.020073 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_3/pre_mlp_layer_norm/scale/v_col
I0512 23:45:15.020103 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_3/pre_mlp_layer_norm/scale/v_row
I0512 23:45:15.020134 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_3/pre_self_attention_layer_norm/scale/m
I0512 23:45:15.020165 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_3/pre_self_attention_layer_norm/scale/v
I0512 23:45:15.020196 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_3/pre_self_attention_layer_norm/scale/v_col
I0512 23:45:15.020227 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_3/pre_self_attention_layer_norm/scale/v_row
I0512 23:45:15.020258 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_3/self_attention/key/kernel/m
I0512 23:45:15.020289 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_3/self_attention/key/kernel/v
I0512 23:45:15.020320 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_3/self_attention/key/kernel/v_col
I0512 23:45:15.020350 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_3/self_attention/key/kernel/v_row
I0512 23:45:15.020381 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_3/self_attention/out/kernel/m
I0512 23:45:15.020412 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_3/self_attention/out/kernel/v
I0512 23:45:15.020443 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_3/self_attention/out/kernel/v_col
I0512 23:45:15.020473 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_3/self_attention/out/kernel/v_row
I0512 23:45:15.020504 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_3/self_attention/query/kernel/m
I0512 23:45:15.020534 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_3/self_attention/query/kernel/v
I0512 23:45:15.020565 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_3/self_attention/query/kernel/v_col
I0512 23:45:15.020602 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_3/self_attention/query/kernel/v_row
I0512 23:45:15.020634 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_3/self_attention/value/kernel/m
I0512 23:45:15.020664 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_3/self_attention/value/kernel/v
I0512 23:45:15.020695 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_3/self_attention/value/kernel/v_col
I0512 23:45:15.020726 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_3/self_attention/value/kernel/v_row
I0512 23:45:15.020756 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_4/mlp/wi_0/kernel/m
I0512 23:45:15.020787 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_4/mlp/wi_0/kernel/v
I0512 23:45:15.020818 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_4/mlp/wi_0/kernel/v_col
I0512 23:45:15.020851 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_4/mlp/wi_0/kernel/v_row
I0512 23:45:15.020883 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_4/mlp/wi_1/kernel/m
I0512 23:45:15.020914 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_4/mlp/wi_1/kernel/v
I0512 23:45:15.020945 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_4/mlp/wi_1/kernel/v_col
I0512 23:45:15.020976 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_4/mlp/wi_1/kernel/v_row
I0512 23:45:15.021006 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_4/mlp/wo/kernel/m
I0512 23:45:15.021037 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_4/mlp/wo/kernel/v
I0512 23:45:15.021068 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_4/mlp/wo/kernel/v_col
I0512 23:45:15.021099 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_4/mlp/wo/kernel/v_row
I0512 23:45:15.021130 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_4/pre_mlp_layer_norm/scale/m
I0512 23:45:15.021160 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_4/pre_mlp_layer_norm/scale/v
I0512 23:45:15.021191 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_4/pre_mlp_layer_norm/scale/v_col
I0512 23:45:15.021222 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_4/pre_mlp_layer_norm/scale/v_row
I0512 23:45:15.021253 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_4/pre_self_attention_layer_norm/scale/m
I0512 23:45:15.021283 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_4/pre_self_attention_layer_norm/scale/v
I0512 23:45:15.021314 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_4/pre_self_attention_layer_norm/scale/v_col
I0512 23:45:15.021345 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_4/pre_self_attention_layer_norm/scale/v_row
I0512 23:45:15.021375 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_4/self_attention/key/kernel/m
I0512 23:45:15.021406 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_4/self_attention/key/kernel/v
I0512 23:45:15.021437 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_4/self_attention/key/kernel/v_col
I0512 23:45:15.021467 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_4/self_attention/key/kernel/v_row
I0512 23:45:15.021498 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_4/self_attention/out/kernel/m
I0512 23:45:15.021529 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_4/self_attention/out/kernel/v
I0512 23:45:15.021560 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_4/self_attention/out/kernel/v_col
I0512 23:45:15.021597 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_4/self_attention/out/kernel/v_row
I0512 23:45:15.021628 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_4/self_attention/query/kernel/m
I0512 23:45:15.021659 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_4/self_attention/query/kernel/v
I0512 23:45:15.021690 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_4/self_attention/query/kernel/v_col
I0512 23:45:15.021742 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_4/self_attention/query/kernel/v_row
I0512 23:45:15.021806 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_4/self_attention/value/kernel/m
I0512 23:45:15.021840 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_4/self_attention/value/kernel/v
I0512 23:45:15.021872 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_4/self_attention/value/kernel/v_col
I0512 23:45:15.021903 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_4/self_attention/value/kernel/v_row
I0512 23:45:15.021934 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/extra_mlp/wi_0/kernel/m
I0512 23:45:15.021965 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/extra_mlp/wi_0/kernel/v
I0512 23:45:15.021996 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/extra_mlp/wi_0/kernel/v_col
I0512 23:45:15.022027 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/extra_mlp/wi_0/kernel/v_row
I0512 23:45:15.022058 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/extra_mlp/wi_1/kernel/m
I0512 23:45:15.022089 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/extra_mlp/wi_1/kernel/v
I0512 23:45:15.022119 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/extra_mlp/wi_1/kernel/v_col
I0512 23:45:15.022150 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/extra_mlp/wi_1/kernel/v_row
I0512 23:45:15.022181 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/extra_mlp/wo/kernel/m
I0512 23:45:15.022212 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/extra_mlp/wo/kernel/v
I0512 23:45:15.022243 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/extra_mlp/wo/kernel/v_col
I0512 23:45:15.022274 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/extra_mlp/wo/kernel/v_row
I0512 23:45:15.022305 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/mlp/expert/wi_0/kernel/m
I0512 23:45:15.022335 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/mlp/expert/wi_0/kernel/v
I0512 23:45:15.022366 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/mlp/expert/wi_0/kernel/v_col
I0512 23:45:15.022397 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/mlp/expert/wi_0/kernel/v_row
I0512 23:45:15.022428 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/mlp/expert/wi_1/kernel/m
I0512 23:45:15.022458 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/mlp/expert/wi_1/kernel/v
I0512 23:45:15.022489 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/mlp/expert/wi_1/kernel/v_col
I0512 23:45:15.022520 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/mlp/expert/wi_1/kernel/v_row
I0512 23:45:15.022551 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/mlp/expert/wo/kernel/m
I0512 23:45:15.022587 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/mlp/expert/wo/kernel/v
I0512 23:45:15.022620 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/mlp/expert/wo/kernel/v_col
I0512 23:45:15.022651 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/mlp/expert/wo/kernel/v_row
I0512 23:45:15.022682 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/mlp/router/router_weights/w/kernel/m
I0512 23:45:15.022713 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/mlp/router/router_weights/w/kernel/v
I0512 23:45:15.022743 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/mlp/router/router_weights/w/kernel/v_col
I0512 23:45:15.022774 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/mlp/router/router_weights/w/kernel/v_row
I0512 23:45:15.022805 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/pre_extra_mlp_layer_norm/scale/m
I0512 23:45:15.022837 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/pre_extra_mlp_layer_norm/scale/v
I0512 23:45:15.022869 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/pre_extra_mlp_layer_norm/scale/v_col
I0512 23:45:15.022900 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/pre_extra_mlp_layer_norm/scale/v_row
I0512 23:45:15.022931 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/pre_mlp_layer_norm/scale/m
I0512 23:45:15.022961 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/pre_mlp_layer_norm/scale/v
I0512 23:45:15.022992 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/pre_mlp_layer_norm/scale/v_col
I0512 23:45:15.023023 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/pre_mlp_layer_norm/scale/v_row
I0512 23:45:15.023054 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/pre_self_attention_layer_norm/scale/m
I0512 23:45:15.023084 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/pre_self_attention_layer_norm/scale/v
I0512 23:45:15.023115 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/pre_self_attention_layer_norm/scale/v_col
I0512 23:45:15.023146 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/pre_self_attention_layer_norm/scale/v_row
I0512 23:45:15.023177 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/self_attention/key/kernel/m
I0512 23:45:15.023207 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/self_attention/key/kernel/v
I0512 23:45:15.023238 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/self_attention/key/kernel/v_col
I0512 23:45:15.023269 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/self_attention/key/kernel/v_row
I0512 23:45:15.023299 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/self_attention/out/kernel/m
I0512 23:45:15.023330 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/self_attention/out/kernel/v
I0512 23:45:15.023361 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/self_attention/out/kernel/v_col
I0512 23:45:15.023391 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/self_attention/out/kernel/v_row
I0512 23:45:15.023422 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/self_attention/query/kernel/m
I0512 23:45:15.023453 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/self_attention/query/kernel/v
I0512 23:45:15.023484 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/self_attention/query/kernel/v_col
I0512 23:45:15.023514 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/self_attention/query/kernel/v_row
I0512 23:45:15.023544 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/self_attention/value/kernel/m
I0512 23:45:15.023575 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/self_attention/value/kernel/v
I0512 23:45:15.023612 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/self_attention/value/kernel/v_col
I0512 23:45:15.023643 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/self_attention/value/kernel/v_row
I0512 23:45:15.023673 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_6/mlp/wi_0/kernel/m
I0512 23:45:15.023704 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_6/mlp/wi_0/kernel/v
I0512 23:45:15.023735 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_6/mlp/wi_0/kernel/v_col
I0512 23:45:15.023766 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_6/mlp/wi_0/kernel/v_row
I0512 23:45:15.023797 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_6/mlp/wi_1/kernel/m
I0512 23:45:15.023827 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_6/mlp/wi_1/kernel/v
I0512 23:45:15.023860 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_6/mlp/wi_1/kernel/v_col
I0512 23:45:15.023892 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_6/mlp/wi_1/kernel/v_row
I0512 23:45:15.023923 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_6/mlp/wo/kernel/m
I0512 23:45:15.023953 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_6/mlp/wo/kernel/v
I0512 23:45:15.023985 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_6/mlp/wo/kernel/v_col
I0512 23:45:15.024016 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_6/mlp/wo/kernel/v_row
I0512 23:45:15.024047 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_6/pre_mlp_layer_norm/scale/m
I0512 23:45:15.024077 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_6/pre_mlp_layer_norm/scale/v
I0512 23:45:15.024108 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_6/pre_mlp_layer_norm/scale/v_col
I0512 23:45:15.024139 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_6/pre_mlp_layer_norm/scale/v_row
I0512 23:45:15.024170 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_6/pre_self_attention_layer_norm/scale/m
I0512 23:45:15.024201 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_6/pre_self_attention_layer_norm/scale/v
I0512 23:45:15.024232 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_6/pre_self_attention_layer_norm/scale/v_col
I0512 23:45:15.024262 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_6/pre_self_attention_layer_norm/scale/v_row
I0512 23:45:15.024293 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_6/self_attention/key/kernel/m
I0512 23:45:15.024324 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_6/self_attention/key/kernel/v
I0512 23:45:15.024355 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_6/self_attention/key/kernel/v_col
I0512 23:45:15.024386 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_6/self_attention/key/kernel/v_row
I0512 23:45:15.024417 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_6/self_attention/out/kernel/m
I0512 23:45:15.024447 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_6/self_attention/out/kernel/v
I0512 23:45:15.024478 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_6/self_attention/out/kernel/v_col
I0512 23:45:15.024509 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_6/self_attention/out/kernel/v_row
I0512 23:45:15.024539 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_6/self_attention/query/kernel/m
I0512 23:45:15.024570 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_6/self_attention/query/kernel/v
I0512 23:45:15.024607 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_6/self_attention/query/kernel/v_col
I0512 23:45:15.024638 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_6/self_attention/query/kernel/v_row
I0512 23:45:15.024669 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_6/self_attention/value/kernel/m
I0512 23:45:15.024700 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_6/self_attention/value/kernel/v
I0512 23:45:15.024731 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_6/self_attention/value/kernel/v_col
I0512 23:45:15.024761 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_6/self_attention/value/kernel/v_row
I0512 23:45:15.024792 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_7/mlp/wi_0/kernel/m
I0512 23:45:15.024823 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_7/mlp/wi_0/kernel/v
I0512 23:45:15.024856 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_7/mlp/wi_0/kernel/v_col
I0512 23:45:15.024888 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_7/mlp/wi_0/kernel/v_row
I0512 23:45:15.024918 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_7/mlp/wi_1/kernel/m
I0512 23:45:15.024949 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_7/mlp/wi_1/kernel/v
I0512 23:45:15.024980 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_7/mlp/wi_1/kernel/v_col
I0512 23:45:15.025011 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_7/mlp/wi_1/kernel/v_row
I0512 23:45:15.025042 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_7/mlp/wo/kernel/m
I0512 23:45:15.025072 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_7/mlp/wo/kernel/v
I0512 23:45:15.025103 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_7/mlp/wo/kernel/v_col
I0512 23:45:15.025134 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_7/mlp/wo/kernel/v_row
I0512 23:45:15.025165 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_7/pre_mlp_layer_norm/scale/m
I0512 23:45:15.025196 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_7/pre_mlp_layer_norm/scale/v
I0512 23:45:15.025227 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_7/pre_mlp_layer_norm/scale/v_col
I0512 23:45:15.025258 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_7/pre_mlp_layer_norm/scale/v_row
I0512 23:45:15.025289 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_7/pre_self_attention_layer_norm/scale/m
I0512 23:45:15.025320 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_7/pre_self_attention_layer_norm/scale/v
I0512 23:45:15.025350 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_7/pre_self_attention_layer_norm/scale/v_col
I0512 23:45:15.025382 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_7/pre_self_attention_layer_norm/scale/v_row
I0512 23:45:15.025413 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_7/self_attention/key/kernel/m
I0512 23:45:15.025444 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_7/self_attention/key/kernel/v
I0512 23:45:15.025474 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_7/self_attention/key/kernel/v_col
I0512 23:45:15.025505 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_7/self_attention/key/kernel/v_row
I0512 23:45:15.025536 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_7/self_attention/out/kernel/m
I0512 23:45:15.025567 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_7/self_attention/out/kernel/v
I0512 23:45:15.025604 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_7/self_attention/out/kernel/v_col
I0512 23:45:15.025636 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_7/self_attention/out/kernel/v_row
I0512 23:45:15.025666 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_7/self_attention/query/kernel/m
I0512 23:45:15.025697 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_7/self_attention/query/kernel/v
I0512 23:45:15.025751 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_7/self_attention/query/kernel/v_col
I0512 23:45:15.025784 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_7/self_attention/query/kernel/v_row
I0512 23:45:15.025814 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_7/self_attention/value/kernel/m
I0512 23:45:15.025847 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_7/self_attention/value/kernel/v
I0512 23:45:15.025879 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_7/self_attention/value/kernel/v_col
I0512 23:45:15.025909 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_7/self_attention/value/kernel/v_row
I0512 23:45:15.025940 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_8/mlp/wi_0/kernel/m
I0512 23:45:15.025971 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_8/mlp/wi_0/kernel/v
I0512 23:45:15.026002 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_8/mlp/wi_0/kernel/v_col
I0512 23:45:15.026033 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_8/mlp/wi_0/kernel/v_row
I0512 23:45:15.026065 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_8/mlp/wi_1/kernel/m
I0512 23:45:15.026096 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_8/mlp/wi_1/kernel/v
I0512 23:45:15.026127 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_8/mlp/wi_1/kernel/v_col
I0512 23:45:15.026158 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_8/mlp/wi_1/kernel/v_row
I0512 23:45:15.026189 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_8/mlp/wo/kernel/m
I0512 23:45:15.026220 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_8/mlp/wo/kernel/v
I0512 23:45:15.026251 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_8/mlp/wo/kernel/v_col
I0512 23:45:15.026282 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_8/mlp/wo/kernel/v_row
I0512 23:45:15.026313 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_8/pre_mlp_layer_norm/scale/m
I0512 23:45:15.026344 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_8/pre_mlp_layer_norm/scale/v
I0512 23:45:15.026375 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_8/pre_mlp_layer_norm/scale/v_col
I0512 23:45:15.026405 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_8/pre_mlp_layer_norm/scale/v_row
I0512 23:45:15.026436 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_8/pre_self_attention_layer_norm/scale/m
I0512 23:45:15.026467 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_8/pre_self_attention_layer_norm/scale/v
I0512 23:45:15.026498 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_8/pre_self_attention_layer_norm/scale/v_col
I0512 23:45:15.026529 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_8/pre_self_attention_layer_norm/scale/v_row
I0512 23:45:15.026560 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_8/self_attention/key/kernel/m
I0512 23:45:15.026597 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_8/self_attention/key/kernel/v
I0512 23:45:15.026629 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_8/self_attention/key/kernel/v_col
I0512 23:45:15.026660 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_8/self_attention/key/kernel/v_row
I0512 23:45:15.026691 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_8/self_attention/out/kernel/m
I0512 23:45:15.026722 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_8/self_attention/out/kernel/v
I0512 23:45:15.026753 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_8/self_attention/out/kernel/v_col
I0512 23:45:15.026783 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_8/self_attention/out/kernel/v_row
I0512 23:45:15.026814 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_8/self_attention/query/kernel/m
I0512 23:45:15.026846 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_8/self_attention/query/kernel/v
I0512 23:45:15.026879 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_8/self_attention/query/kernel/v_col
I0512 23:45:15.026910 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_8/self_attention/query/kernel/v_row
I0512 23:45:15.026941 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_8/self_attention/value/kernel/m
I0512 23:45:15.026972 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_8/self_attention/value/kernel/v
I0512 23:45:15.027003 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_8/self_attention/value/kernel/v_col
I0512 23:45:15.027033 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_8/self_attention/value/kernel/v_row
I0512 23:45:15.027064 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_9/mlp/wi_0/kernel/m
I0512 23:45:15.027095 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_9/mlp/wi_0/kernel/v
I0512 23:45:15.027126 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_9/mlp/wi_0/kernel/v_col
I0512 23:45:15.027158 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_9/mlp/wi_0/kernel/v_row
I0512 23:45:15.027189 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_9/mlp/wi_1/kernel/m
I0512 23:45:15.027219 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_9/mlp/wi_1/kernel/v
I0512 23:45:15.027250 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_9/mlp/wi_1/kernel/v_col
I0512 23:45:15.027281 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_9/mlp/wi_1/kernel/v_row
I0512 23:45:15.027312 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_9/mlp/wo/kernel/m
I0512 23:45:15.027343 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_9/mlp/wo/kernel/v
I0512 23:45:15.027374 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_9/mlp/wo/kernel/v_col
I0512 23:45:15.027405 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_9/mlp/wo/kernel/v_row
I0512 23:45:15.027440 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_9/pre_mlp_layer_norm/scale/m
I0512 23:45:15.027471 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_9/pre_mlp_layer_norm/scale/v
I0512 23:45:15.027503 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_9/pre_mlp_layer_norm/scale/v_col
I0512 23:45:15.027533 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_9/pre_mlp_layer_norm/scale/v_row
I0512 23:45:15.027564 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_9/pre_self_attention_layer_norm/scale/m
I0512 23:45:15.027602 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_9/pre_self_attention_layer_norm/scale/v
I0512 23:45:15.027634 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_9/pre_self_attention_layer_norm/scale/v_col
I0512 23:45:15.027665 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_9/pre_self_attention_layer_norm/scale/v_row
I0512 23:45:15.027696 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_9/self_attention/key/kernel/m
I0512 23:45:15.027727 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_9/self_attention/key/kernel/v
I0512 23:45:15.027758 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_9/self_attention/key/kernel/v_col
I0512 23:45:15.027789 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_9/self_attention/key/kernel/v_row
I0512 23:45:15.027819 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_9/self_attention/out/kernel/m
I0512 23:45:15.027852 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_9/self_attention/out/kernel/v
I0512 23:45:15.027884 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_9/self_attention/out/kernel/v_col
I0512 23:45:15.027915 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_9/self_attention/out/kernel/v_row
I0512 23:45:15.027946 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_9/self_attention/query/kernel/m
I0512 23:45:15.027976 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_9/self_attention/query/kernel/v
I0512 23:45:15.028007 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_9/self_attention/query/kernel/v_col
I0512 23:45:15.028038 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_9/self_attention/query/kernel/v_row
I0512 23:45:15.028068 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_9/self_attention/value/kernel/m
I0512 23:45:15.028099 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_9/self_attention/value/kernel/v
I0512 23:45:15.028130 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_9/self_attention/value/kernel/v_col
I0512 23:45:15.028160 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_9/self_attention/value/kernel/v_row
I0512 23:45:15.028191 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/logits_dense/kernel/m
I0512 23:45:15.028221 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/logits_dense/kernel/v
I0512 23:45:15.028252 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/logits_dense/kernel/v_col
I0512 23:45:15.028283 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/logits_dense/kernel/v_row
I0512 23:45:15.028314 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/token_embedder/embedding/m
I0512 23:45:15.028345 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/token_embedder/embedding/v
I0512 23:45:15.028377 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/token_embedder/embedding/v_col
I0512 23:45:15.028408 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/param_states/token_embedder/embedding/v_row
I0512 23:45:15.028438 139856202532864 checkpoints.py:1100] Restoring key from ckpt: state/step
I0512 23:45:15.028470 139856202532864 checkpoints.py:1100] Restoring key from ckpt: target/decoder/decoder_norm/scale
I0512 23:45:15.028501 139856202532864 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_0/mlp/wi_0/kernel
I0512 23:45:15.028532 139856202532864 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_0/mlp/wi_1/kernel
I0512 23:45:15.028563 139856202532864 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_0/mlp/wo/kernel
I0512 23:45:15.028601 139856202532864 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_0/pre_mlp_layer_norm/scale
I0512 23:45:15.028633 139856202532864 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_0/pre_self_attention_layer_norm/scale
I0512 23:45:15.028664 139856202532864 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_0/self_attention/key/kernel
I0512 23:45:15.028695 139856202532864 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_0/self_attention/out/kernel
I0512 23:45:15.028726 139856202532864 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_0/self_attention/query/kernel
I0512 23:45:15.028757 139856202532864 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_0/self_attention/value/kernel
I0512 23:45:15.028788 139856202532864 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_1/mlp/wi_0/kernel
I0512 23:45:15.028819 139856202532864 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_1/mlp/wi_1/kernel
I0512 23:45:15.028852 139856202532864 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_1/mlp/wo/kernel
I0512 23:45:15.028884 139856202532864 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_1/pre_mlp_layer_norm/scale
I0512 23:45:15.028914 139856202532864 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_1/pre_self_attention_layer_norm/scale
I0512 23:45:15.028946 139856202532864 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_1/self_attention/key/kernel
I0512 23:45:15.028976 139856202532864 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_1/self_attention/out/kernel
I0512 23:45:15.029008 139856202532864 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_1/self_attention/query/kernel
I0512 23:45:15.029038 139856202532864 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_1/self_attention/value/kernel
I0512 23:45:15.029069 139856202532864 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_10/mlp/wi_0/kernel
I0512 23:45:15.029100 139856202532864 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_10/mlp/wi_1/kernel
I0512 23:45:15.029131 139856202532864 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_10/mlp/wo/kernel
I0512 23:45:15.029163 139856202532864 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_10/pre_mlp_layer_norm/scale
I0512 23:45:15.029193 139856202532864 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_10/pre_self_attention_layer_norm/scale
I0512 23:45:15.029224 139856202532864 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_10/self_attention/key/kernel
I0512 23:45:15.029255 139856202532864 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_10/self_attention/out/kernel
I0512 23:45:15.029286 139856202532864 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_10/self_attention/query/kernel
I0512 23:45:15.029317 139856202532864 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_10/self_attention/value/kernel
I0512 23:45:15.029348 139856202532864 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_11/extra_mlp/wi_0/kernel
I0512 23:45:15.029379 139856202532864 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_11/extra_mlp/wi_1/kernel
I0512 23:45:15.029409 139856202532864 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_11/extra_mlp/wo/kernel
I0512 23:45:15.029440 139856202532864 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_11/mlp/expert/wi_0/kernel
I0512 23:45:15.029471 139856202532864 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_11/mlp/expert/wi_1/kernel
I0512 23:45:15.029502 139856202532864 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_11/mlp/expert/wo/kernel
I0512 23:45:15.029533 139856202532864 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_11/mlp/router/router_weights/w/kernel
I0512 23:45:15.029564 139856202532864 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_11/pre_extra_mlp_layer_norm/scale
I0512 23:45:15.029600 139856202532864 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_11/pre_mlp_layer_norm/scale
I0512 23:45:15.029632 139856202532864 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_11/pre_self_attention_layer_norm/scale
I0512 23:45:15.029663 139856202532864 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_11/self_attention/key/kernel
I0512 23:45:15.029694 139856202532864 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_11/self_attention/out/kernel
I0512 23:45:15.029746 139856202532864 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_11/self_attention/query/kernel
I0512 23:45:15.029779 139856202532864 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_11/self_attention/value/kernel
I0512 23:45:15.029810 139856202532864 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_12/mlp/wi_0/kernel
I0512 23:45:15.029843 139856202532864 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_12/mlp/wi_1/kernel
I0512 23:45:15.029875 139856202532864 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_12/mlp/wo/kernel
I0512 23:45:15.029906 139856202532864 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_12/pre_mlp_layer_norm/scale
I0512 23:45:15.029937 139856202532864 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_12/pre_self_attention_layer_norm/scale
I0512 23:45:15.029968 139856202532864 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_12/self_attention/key/kernel
I0512 23:45:15.029999 139856202532864 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_12/self_attention/out/kernel
I0512 23:45:15.030031 139856202532864 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_12/self_attention/query/kernel
I0512 23:45:15.030061 139856202532864 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_12/self_attention/value/kernel
I0512 23:45:15.030092 139856202532864 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_13/mlp/wi_0/kernel
I0512 23:45:15.030123 139856202532864 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_13/mlp/wi_1/kernel
I0512 23:45:15.030154 139856202532864 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_13/mlp/wo/kernel
I0512 23:45:15.030185 139856202532864 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_13/pre_mlp_layer_norm/scale
I0512 23:45:15.030216 139856202532864 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_13/pre_self_attention_layer_norm/scale
I0512 23:45:15.030247 139856202532864 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_13/self_attention/key/kernel
I0512 23:45:15.030278 139856202532864 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_13/self_attention/out/kernel
I0512 23:45:15.030308 139856202532864 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_13/self_attention/query/kernel
I0512 23:45:15.030339 139856202532864 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_13/self_attention/value/kernel
I0512 23:45:15.030370 139856202532864 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_14/mlp/wi_0/kernel
I0512 23:45:15.030401 139856202532864 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_14/mlp/wi_1/kernel
I0512 23:45:15.030436 139856202532864 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_14/mlp/wo/kernel
I0512 23:45:15.030468 139856202532864 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_14/pre_mlp_layer_norm/scale
I0512 23:45:15.030499 139856202532864 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_14/pre_self_attention_layer_norm/scale
I0512 23:45:15.030530 139856202532864 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_14/self_attention/key/kernel
I0512 23:45:15.030561 139856202532864 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_14/self_attention/out/kernel
I0512 23:45:15.030598 139856202532864 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_14/self_attention/query/kernel
I0512 23:45:15.030630 139856202532864 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_14/self_attention/value/kernel
I0512 23:45:15.030661 139856202532864 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_15/mlp/wi_0/kernel
I0512 23:45:15.030692 139856202532864 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_15/mlp/wi_1/kernel
I0512 23:45:15.030723 139856202532864 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_15/mlp/wo/kernel
I0512 23:45:15.030754 139856202532864 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_15/pre_mlp_layer_norm/scale
I0512 23:45:15.030785 139856202532864 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_15/pre_self_attention_layer_norm/scale
I0512 23:45:15.030816 139856202532864 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_15/self_attention/key/kernel
I0512 23:45:15.030848 139856202532864 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_15/self_attention/out/kernel
I0512 23:45:15.030880 139856202532864 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_15/self_attention/query/kernel
I0512 23:45:15.030911 139856202532864 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_15/self_attention/value/kernel
I0512 23:45:15.030943 139856202532864 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_16/mlp/wi_0/kernel
I0512 23:45:15.030974 139856202532864 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_16/mlp/wi_1/kernel
I0512 23:45:15.031005 139856202532864 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_16/mlp/wo/kernel
I0512 23:45:15.031036 139856202532864 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_16/pre_mlp_layer_norm/scale
I0512 23:45:15.031067 139856202532864 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_16/pre_self_attention_layer_norm/scale
I0512 23:45:15.031098 139856202532864 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_16/self_attention/key/kernel
I0512 23:45:15.031129 139856202532864 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_16/self_attention/out/kernel
I0512 23:45:15.031160 139856202532864 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_16/self_attention/query/kernel
I0512 23:45:15.031191 139856202532864 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_16/self_attention/value/kernel
I0512 23:45:15.031222 139856202532864 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_17/extra_mlp/wi_0/kernel
I0512 23:45:15.031253 139856202532864 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_17/extra_mlp/wi_1/kernel
I0512 23:45:15.031284 139856202532864 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_17/extra_mlp/wo/kernel
I0512 23:45:15.031315 139856202532864 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_17/mlp/expert/wi_0/kernel
I0512 23:45:15.031346 139856202532864 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_17/mlp/expert/wi_1/kernel
I0512 23:45:15.031377 139856202532864 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_17/mlp/expert/wo/kernel
I0512 23:45:15.031408 139856202532864 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_17/mlp/router/router_weights/w/kernel
I0512 23:45:15.031439 139856202532864 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_17/pre_extra_mlp_layer_norm/scale
I0512 23:45:15.031470 139856202532864 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_17/pre_mlp_layer_norm/scale
I0512 23:45:15.031501 139856202532864 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_17/pre_self_attention_layer_norm/scale
I0512 23:45:15.031533 139856202532864 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_17/self_attention/key/kernel
I0512 23:45:15.031563 139856202532864 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_17/self_attention/out/kernel
I0512 23:45:15.031600 139856202532864 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_17/self_attention/query/kernel
I0512 23:45:15.031632 139856202532864 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_17/self_attention/value/kernel
I0512 23:45:15.031663 139856202532864 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_18/mlp/wi_0/kernel
I0512 23:45:15.031694 139856202532864 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_18/mlp/wi_1/kernel
I0512 23:45:15.031725 139856202532864 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_18/mlp/wo/kernel
I0512 23:45:15.031757 139856202532864 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_18/pre_mlp_layer_norm/scale
I0512 23:45:15.031788 139856202532864 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_18/pre_self_attention_layer_norm/scale
I0512 23:45:15.031819 139856202532864 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_18/self_attention/key/kernel
I0512 23:45:15.031852 139856202532864 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_18/self_attention/out/kernel
I0512 23:45:15.031884 139856202532864 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_18/self_attention/query/kernel
I0512 23:45:15.031915 139856202532864 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_18/self_attention/value/kernel
I0512 23:45:15.031945 139856202532864 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_19/mlp/wi_0/kernel
I0512 23:45:15.031977 139856202532864 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_19/mlp/wi_1/kernel
I0512 23:45:15.032007 139856202532864 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_19/mlp/wo/kernel
I0512 23:45:15.032039 139856202532864 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_19/pre_mlp_layer_norm/scale
I0512 23:45:15.032070 139856202532864 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_19/pre_self_attention_layer_norm/scale
I0512 23:45:15.032101 139856202532864 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_19/self_attention/key/kernel
I0512 23:45:15.032132 139856202532864 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_19/self_attention/out/kernel
I0512 23:45:15.032163 139856202532864 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_19/self_attention/query/kernel
I0512 23:45:15.032194 139856202532864 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_19/self_attention/value/kernel
I0512 23:45:15.032225 139856202532864 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_2/mlp/wi_0/kernel
I0512 23:45:15.032256 139856202532864 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_2/mlp/wi_1/kernel
I0512 23:45:15.032287 139856202532864 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_2/mlp/wo/kernel
I0512 23:45:15.032318 139856202532864 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_2/pre_mlp_layer_norm/scale
I0512 23:45:15.032349 139856202532864 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_2/pre_self_attention_layer_norm/scale
I0512 23:45:15.032380 139856202532864 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_2/self_attention/key/kernel
I0512 23:45:15.032411 139856202532864 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_2/self_attention/out/kernel
I0512 23:45:15.032442 139856202532864 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_2/self_attention/query/kernel
I0512 23:45:15.032473 139856202532864 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_2/self_attention/value/kernel
I0512 23:45:15.032503 139856202532864 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_20/mlp/wi_0/kernel
I0512 23:45:15.032535 139856202532864 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_20/mlp/wi_1/kernel
I0512 23:45:15.032566 139856202532864 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_20/mlp/wo/kernel
I0512 23:45:15.032603 139856202532864 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_20/pre_mlp_layer_norm/scale
I0512 23:45:15.032634 139856202532864 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_20/pre_self_attention_layer_norm/scale
I0512 23:45:15.032666 139856202532864 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_20/self_attention/key/kernel
I0512 23:45:15.032697 139856202532864 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_20/self_attention/out/kernel
I0512 23:45:15.032727 139856202532864 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_20/self_attention/query/kernel
I0512 23:45:15.032758 139856202532864 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_20/self_attention/value/kernel
I0512 23:45:15.032789 139856202532864 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_21/mlp/wi_0/kernel
I0512 23:45:15.032820 139856202532864 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_21/mlp/wi_1/kernel
I0512 23:45:15.032854 139856202532864 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_21/mlp/wo/kernel
I0512 23:45:15.032886 139856202532864 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_21/pre_mlp_layer_norm/scale
I0512 23:45:15.032917 139856202532864 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_21/pre_self_attention_layer_norm/scale
I0512 23:45:15.032948 139856202532864 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_21/self_attention/key/kernel
I0512 23:45:15.032979 139856202532864 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_21/self_attention/out/kernel
I0512 23:45:15.033009 139856202532864 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_21/self_attention/query/kernel
I0512 23:45:15.033040 139856202532864 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_21/self_attention/value/kernel
I0512 23:45:15.033071 139856202532864 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_22/mlp/wi_0/kernel
I0512 23:45:15.033102 139856202532864 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_22/mlp/wi_1/kernel
I0512 23:45:15.033133 139856202532864 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_22/mlp/wo/kernel
I0512 23:45:15.033164 139856202532864 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_22/pre_mlp_layer_norm/scale
I0512 23:45:15.033195 139856202532864 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_22/pre_self_attention_layer_norm/scale
I0512 23:45:15.033226 139856202532864 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_22/self_attention/key/kernel
I0512 23:45:15.033257 139856202532864 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_22/self_attention/out/kernel
I0512 23:45:15.033288 139856202532864 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_22/self_attention/query/kernel
I0512 23:45:15.033318 139856202532864 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_22/self_attention/value/kernel
I0512 23:45:15.033349 139856202532864 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_23/extra_mlp/wi_0/kernel
I0512 23:45:15.033380 139856202532864 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_23/extra_mlp/wi_1/kernel
I0512 23:45:15.033411 139856202532864 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_23/extra_mlp/wo/kernel
I0512 23:45:15.033442 139856202532864 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_23/mlp/expert/wi_0/kernel
I0512 23:45:15.033473 139856202532864 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_23/mlp/expert/wi_1/kernel
I0512 23:45:15.033504 139856202532864 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_23/mlp/expert/wo/kernel
I0512 23:45:15.033535 139856202532864 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_23/mlp/router/router_weights/w/kernel
I0512 23:45:15.033566 139856202532864 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_23/pre_extra_mlp_layer_norm/scale
I0512 23:45:15.033604 139856202532864 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_23/pre_mlp_layer_norm/scale
I0512 23:45:15.033636 139856202532864 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_23/pre_self_attention_layer_norm/scale
I0512 23:45:15.033667 139856202532864 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_23/self_attention/key/kernel
I0512 23:45:15.033698 139856202532864 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_23/self_attention/out/kernel
I0512 23:45:15.033755 139856202532864 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_23/self_attention/query/kernel
I0512 23:45:15.033787 139856202532864 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_23/self_attention/value/kernel
I0512 23:45:15.033818 139856202532864 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_3/mlp/wi_0/kernel
I0512 23:45:15.033851 139856202532864 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_3/mlp/wi_1/kernel
I0512 23:45:15.033883 139856202532864 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_3/mlp/wo/kernel
I0512 23:45:15.033914 139856202532864 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_3/pre_mlp_layer_norm/scale
I0512 23:45:15.033945 139856202532864 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_3/pre_self_attention_layer_norm/scale
I0512 23:45:15.033975 139856202532864 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_3/self_attention/key/kernel
I0512 23:45:15.034006 139856202532864 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_3/self_attention/out/kernel
I0512 23:45:15.034037 139856202532864 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_3/self_attention/query/kernel
I0512 23:45:15.034068 139856202532864 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_3/self_attention/value/kernel
I0512 23:45:15.034099 139856202532864 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_4/mlp/wi_0/kernel
I0512 23:45:15.034130 139856202532864 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_4/mlp/wi_1/kernel
I0512 23:45:15.034161 139856202532864 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_4/mlp/wo/kernel
I0512 23:45:15.034192 139856202532864 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_4/pre_mlp_layer_norm/scale
I0512 23:45:15.034223 139856202532864 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_4/pre_self_attention_layer_norm/scale
I0512 23:45:15.034254 139856202532864 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_4/self_attention/key/kernel
I0512 23:45:15.034285 139856202532864 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_4/self_attention/out/kernel
I0512 23:45:15.034316 139856202532864 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_4/self_attention/query/kernel
I0512 23:45:15.034347 139856202532864 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_4/self_attention/value/kernel
I0512 23:45:15.034377 139856202532864 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_5/extra_mlp/wi_0/kernel
I0512 23:45:15.034408 139856202532864 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_5/extra_mlp/wi_1/kernel
I0512 23:45:15.034439 139856202532864 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_5/extra_mlp/wo/kernel
I0512 23:45:15.034470 139856202532864 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_5/mlp/expert/wi_0/kernel
I0512 23:45:15.034501 139856202532864 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_5/mlp/expert/wi_1/kernel
I0512 23:45:15.034532 139856202532864 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_5/mlp/expert/wo/kernel
I0512 23:45:15.034563 139856202532864 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_5/mlp/router/router_weights/w/kernel
I0512 23:45:15.034600 139856202532864 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_5/pre_extra_mlp_layer_norm/scale
I0512 23:45:15.034632 139856202532864 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_5/pre_mlp_layer_norm/scale
I0512 23:45:15.034663 139856202532864 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_5/pre_self_attention_layer_norm/scale
I0512 23:45:15.034694 139856202532864 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_5/self_attention/key/kernel
I0512 23:45:15.034725 139856202532864 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_5/self_attention/out/kernel
I0512 23:45:15.034756 139856202532864 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_5/self_attention/query/kernel
I0512 23:45:15.034786 139856202532864 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_5/self_attention/value/kernel
I0512 23:45:15.034817 139856202532864 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_6/mlp/wi_0/kernel
I0512 23:45:15.034850 139856202532864 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_6/mlp/wi_1/kernel
I0512 23:45:15.034882 139856202532864 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_6/mlp/wo/kernel
I0512 23:45:15.034913 139856202532864 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_6/pre_mlp_layer_norm/scale
I0512 23:45:15.034944 139856202532864 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_6/pre_self_attention_layer_norm/scale
I0512 23:45:15.034974 139856202532864 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_6/self_attention/key/kernel
I0512 23:45:15.035005 139856202532864 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_6/self_attention/out/kernel
I0512 23:45:15.035036 139856202532864 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_6/self_attention/query/kernel
I0512 23:45:15.035067 139856202532864 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_6/self_attention/value/kernel
I0512 23:45:15.035098 139856202532864 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_7/mlp/wi_0/kernel
I0512 23:45:15.035129 139856202532864 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_7/mlp/wi_1/kernel
I0512 23:45:15.035160 139856202532864 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_7/mlp/wo/kernel
I0512 23:45:15.035191 139856202532864 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_7/pre_mlp_layer_norm/scale
I0512 23:45:15.035221 139856202532864 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_7/pre_self_attention_layer_norm/scale
I0512 23:45:15.035252 139856202532864 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_7/self_attention/key/kernel
I0512 23:45:15.035282 139856202532864 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_7/self_attention/out/kernel
I0512 23:45:15.035313 139856202532864 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_7/self_attention/query/kernel
I0512 23:45:15.035344 139856202532864 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_7/self_attention/value/kernel
I0512 23:45:15.035375 139856202532864 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_8/mlp/wi_0/kernel
I0512 23:45:15.035406 139856202532864 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_8/mlp/wi_1/kernel
I0512 23:45:15.035437 139856202532864 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_8/mlp/wo/kernel
I0512 23:45:15.035468 139856202532864 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_8/pre_mlp_layer_norm/scale
I0512 23:45:15.035499 139856202532864 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_8/pre_self_attention_layer_norm/scale
I0512 23:45:15.035530 139856202532864 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_8/self_attention/key/kernel
I0512 23:45:15.035560 139856202532864 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_8/self_attention/out/kernel
I0512 23:45:15.035597 139856202532864 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_8/self_attention/query/kernel
I0512 23:45:15.035629 139856202532864 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_8/self_attention/value/kernel
I0512 23:45:15.035660 139856202532864 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_9/mlp/wi_0/kernel
I0512 23:45:15.035691 139856202532864 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_9/mlp/wi_1/kernel
I0512 23:45:15.035722 139856202532864 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_9/mlp/wo/kernel
I0512 23:45:15.035754 139856202532864 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_9/pre_mlp_layer_norm/scale
I0512 23:45:15.035784 139856202532864 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_9/pre_self_attention_layer_norm/scale
I0512 23:45:15.035815 139856202532864 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_9/self_attention/key/kernel
I0512 23:45:15.035847 139856202532864 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_9/self_attention/out/kernel
I0512 23:45:15.035879 139856202532864 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_9/self_attention/query/kernel
I0512 23:45:15.035910 139856202532864 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_9/self_attention/value/kernel
I0512 23:45:15.035941 139856202532864 checkpoints.py:1100] Restoring key from ckpt: target/decoder/logits_dense/kernel
I0512 23:45:15.035972 139856202532864 checkpoints.py:1100] Restoring key from ckpt: target/token_embedder/embedding
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
I0000 00:00:1715557515.067413  393698 gcs_resource.cc:109] Using default AdmissionQueue with limit 32
I0000 00:00:1715557515.075030  395877 google_auth_provider.cc:180] Running on GCE, using service account 1081612546678-compute@developer.gserviceaccount.com
I0512 23:45:15.097474 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/decoder_norm/scale/m
I0512 23:45:15.097669 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/decoder_norm/scale/v
I0512 23:45:15.097712 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/decoder_norm/scale/v_col
I0512 23:45:15.097744 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/decoder_norm/scale/v_row
I0512 23:45:15.097775 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_0/mlp/wi_0/kernel/m
I0512 23:45:15.097807 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_0/mlp/wi_0/kernel/v
I0512 23:45:15.097837 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_0/mlp/wi_0/kernel/v_col
I0512 23:45:15.097868 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_0/mlp/wi_0/kernel/v_row
I0512 23:45:15.097898 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_0/mlp/wi_1/kernel/m
I0512 23:45:15.097929 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_0/mlp/wi_1/kernel/v
I0512 23:45:15.097959 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_0/mlp/wi_1/kernel/v_col
I0512 23:45:15.097989 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_0/mlp/wi_1/kernel/v_row
I0512 23:45:15.098019 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_0/mlp/wo/kernel/m
I0512 23:45:15.098050 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_0/mlp/wo/kernel/v
I0512 23:45:15.098080 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_0/mlp/wo/kernel/v_col
I0512 23:45:15.098110 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_0/mlp/wo/kernel/v_row
I0512 23:45:15.098141 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_0/pre_mlp_layer_norm/scale/m
I0512 23:45:15.098172 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_0/pre_mlp_layer_norm/scale/v
I0512 23:45:15.098209 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_0/pre_mlp_layer_norm/scale/v_col
I0512 23:45:15.098242 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_0/pre_mlp_layer_norm/scale/v_row
I0512 23:45:15.098274 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_0/pre_self_attention_layer_norm/scale/m
I0512 23:45:15.098304 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_0/pre_self_attention_layer_norm/scale/v
I0512 23:45:15.098335 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_0/pre_self_attention_layer_norm/scale/v_col
I0512 23:45:15.098366 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_0/pre_self_attention_layer_norm/scale/v_row
I0512 23:45:15.098396 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_0/self_attention/key/kernel/m
I0512 23:45:15.098426 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_0/self_attention/key/kernel/v
I0512 23:45:15.098458 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_0/self_attention/key/kernel/v_col
I0512 23:45:15.098490 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_0/self_attention/key/kernel/v_row
I0512 23:45:15.098521 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_0/self_attention/out/kernel/m
I0512 23:45:15.098551 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_0/self_attention/out/kernel/v
I0512 23:45:15.098582 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_0/self_attention/out/kernel/v_col
I0512 23:45:15.098613 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_0/self_attention/out/kernel/v_row
I0512 23:45:15.098643 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_0/self_attention/query/kernel/m
I0512 23:45:15.098673 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_0/self_attention/query/kernel/v
I0512 23:45:15.098704 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_0/self_attention/query/kernel/v_col
I0512 23:45:15.098734 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_0/self_attention/query/kernel/v_row
I0512 23:45:15.098764 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_0/self_attention/value/kernel/m
I0512 23:45:15.098794 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_0/self_attention/value/kernel/v
I0512 23:45:15.098824 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_0/self_attention/value/kernel/v_col
I0512 23:45:15.098855 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_0/self_attention/value/kernel/v_row
I0512 23:45:15.098885 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_1/mlp/wi_0/kernel/m
I0512 23:45:15.098915 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_1/mlp/wi_0/kernel/v
I0512 23:45:15.098946 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_1/mlp/wi_0/kernel/v_col
I0512 23:45:15.098975 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_1/mlp/wi_0/kernel/v_row
I0512 23:45:15.099006 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_1/mlp/wi_1/kernel/m
I0512 23:45:15.099036 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_1/mlp/wi_1/kernel/v
I0512 23:45:15.099066 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_1/mlp/wi_1/kernel/v_col
I0512 23:45:15.099096 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_1/mlp/wi_1/kernel/v_row
I0512 23:45:15.099127 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_1/mlp/wo/kernel/m
I0512 23:45:15.099157 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_1/mlp/wo/kernel/v
I0512 23:45:15.099187 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_1/mlp/wo/kernel/v_col
I0512 23:45:15.099223 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_1/mlp/wo/kernel/v_row
I0512 23:45:15.099255 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_1/pre_mlp_layer_norm/scale/m
I0512 23:45:15.099286 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_1/pre_mlp_layer_norm/scale/v
I0512 23:45:15.099316 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_1/pre_mlp_layer_norm/scale/v_col
I0512 23:45:15.099347 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_1/pre_mlp_layer_norm/scale/v_row
I0512 23:45:15.099377 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_1/pre_self_attention_layer_norm/scale/m
I0512 23:45:15.099407 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_1/pre_self_attention_layer_norm/scale/v
I0512 23:45:15.099438 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_1/pre_self_attention_layer_norm/scale/v_col
I0512 23:45:15.099470 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_1/pre_self_attention_layer_norm/scale/v_row
I0512 23:45:15.099502 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_1/self_attention/key/kernel/m
I0512 23:45:15.099533 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_1/self_attention/key/kernel/v
I0512 23:45:15.099564 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_1/self_attention/key/kernel/v_col
I0512 23:45:15.099594 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_1/self_attention/key/kernel/v_row
I0512 23:45:15.099625 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_1/self_attention/out/kernel/m
I0512 23:45:15.099655 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_1/self_attention/out/kernel/v
I0512 23:45:15.099686 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_1/self_attention/out/kernel/v_col
I0512 23:45:15.099716 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_1/self_attention/out/kernel/v_row
I0512 23:45:15.099746 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_1/self_attention/query/kernel/m
I0512 23:45:15.099777 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_1/self_attention/query/kernel/v
I0512 23:45:15.099807 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_1/self_attention/query/kernel/v_col
I0512 23:45:15.099837 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_1/self_attention/query/kernel/v_row
I0512 23:45:15.099867 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_1/self_attention/value/kernel/m
I0512 23:45:15.099898 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_1/self_attention/value/kernel/v
I0512 23:45:15.099928 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_1/self_attention/value/kernel/v_col
I0512 23:45:15.099957 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_1/self_attention/value/kernel/v_row
I0512 23:45:15.099988 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_10/mlp/wi_0/kernel/m
I0512 23:45:15.100018 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_10/mlp/wi_0/kernel/v
I0512 23:45:15.100048 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_10/mlp/wi_0/kernel/v_col
I0512 23:45:15.100079 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_10/mlp/wi_0/kernel/v_row
I0512 23:45:15.100109 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_10/mlp/wi_1/kernel/m
I0512 23:45:15.100139 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_10/mlp/wi_1/kernel/v
I0512 23:45:15.100169 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_10/mlp/wi_1/kernel/v_col
I0512 23:45:15.100204 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_10/mlp/wi_1/kernel/v_row
I0512 23:45:15.100237 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_10/mlp/wo/kernel/m
I0512 23:45:15.100267 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_10/mlp/wo/kernel/v
I0512 23:45:15.100297 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_10/mlp/wo/kernel/v_col
I0512 23:45:15.100328 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_10/mlp/wo/kernel/v_row
I0512 23:45:15.100358 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_10/pre_mlp_layer_norm/scale/m
I0512 23:45:15.100389 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_10/pre_mlp_layer_norm/scale/v
I0512 23:45:15.100419 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_10/pre_mlp_layer_norm/scale/v_col
I0512 23:45:15.100450 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_10/pre_mlp_layer_norm/scale/v_row
I0512 23:45:15.100483 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_10/pre_self_attention_layer_norm/scale/m
I0512 23:45:15.100514 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_10/pre_self_attention_layer_norm/scale/v
I0512 23:45:15.100544 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_10/pre_self_attention_layer_norm/scale/v_col
I0512 23:45:15.100574 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_10/pre_self_attention_layer_norm/scale/v_row
I0512 23:45:15.100605 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_10/self_attention/key/kernel/m
I0512 23:45:15.100636 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_10/self_attention/key/kernel/v
I0512 23:45:15.100666 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_10/self_attention/key/kernel/v_col
I0512 23:45:15.100697 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_10/self_attention/key/kernel/v_row
I0512 23:45:15.100727 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_10/self_attention/out/kernel/m
I0512 23:45:15.100757 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_10/self_attention/out/kernel/v
I0512 23:45:15.100788 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_10/self_attention/out/kernel/v_col
I0512 23:45:15.100818 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_10/self_attention/out/kernel/v_row
I0512 23:45:15.100849 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_10/self_attention/query/kernel/m
I0512 23:45:15.100879 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_10/self_attention/query/kernel/v
I0512 23:45:15.100909 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_10/self_attention/query/kernel/v_col
I0512 23:45:15.100939 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_10/self_attention/query/kernel/v_row
I0512 23:45:15.100970 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_10/self_attention/value/kernel/m
I0512 23:45:15.101000 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_10/self_attention/value/kernel/v
I0512 23:45:15.101050 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_10/self_attention/value/kernel/v_col
I0512 23:45:15.101087 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_10/self_attention/value/kernel/v_row
I0512 23:45:15.101118 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/extra_mlp/wi_0/kernel/m
I0512 23:45:15.101148 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/extra_mlp/wi_0/kernel/v
I0512 23:45:15.101178 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/extra_mlp/wi_0/kernel/v_col
I0512 23:45:15.101214 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/extra_mlp/wi_0/kernel/v_row
I0512 23:45:15.101246 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/extra_mlp/wi_1/kernel/m
I0512 23:45:15.101277 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/extra_mlp/wi_1/kernel/v
I0512 23:45:15.101307 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/extra_mlp/wi_1/kernel/v_col
I0512 23:45:15.101338 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/extra_mlp/wi_1/kernel/v_row
I0512 23:45:15.101368 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/extra_mlp/wo/kernel/m
I0512 23:45:15.101399 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/extra_mlp/wo/kernel/v
I0512 23:45:15.101429 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/extra_mlp/wo/kernel/v_col
I0512 23:45:15.101462 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/extra_mlp/wo/kernel/v_row
I0512 23:45:15.101493 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/mlp/expert/wi_0/kernel/m
I0512 23:45:15.101524 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/mlp/expert/wi_0/kernel/v
I0512 23:45:15.101554 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/mlp/expert/wi_0/kernel/v_col
I0512 23:45:15.101585 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/mlp/expert/wi_0/kernel/v_row
I0512 23:45:15.101615 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/mlp/expert/wi_1/kernel/m
I0512 23:45:15.101645 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/mlp/expert/wi_1/kernel/v
I0512 23:45:15.101675 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/mlp/expert/wi_1/kernel/v_col
I0512 23:45:15.101706 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/mlp/expert/wi_1/kernel/v_row
I0512 23:45:15.101736 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/mlp/expert/wo/kernel/m
I0512 23:45:15.101766 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/mlp/expert/wo/kernel/v
I0512 23:45:15.101797 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/mlp/expert/wo/kernel/v_col
I0512 23:45:15.101827 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/mlp/expert/wo/kernel/v_row
I0512 23:45:15.101858 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/mlp/router/router_weights/w/kernel/m
I0512 23:45:15.101888 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/mlp/router/router_weights/w/kernel/v
I0512 23:45:15.101918 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/mlp/router/router_weights/w/kernel/v_col
I0512 23:45:15.101949 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/mlp/router/router_weights/w/kernel/v_row
I0512 23:45:15.101979 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/pre_extra_mlp_layer_norm/scale/m
I0512 23:45:15.102010 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/pre_extra_mlp_layer_norm/scale/v
I0512 23:45:15.102040 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/pre_extra_mlp_layer_norm/scale/v_col
I0512 23:45:15.102071 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/pre_extra_mlp_layer_norm/scale/v_row
I0512 23:45:15.102101 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/pre_mlp_layer_norm/scale/m
I0512 23:45:15.102131 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/pre_mlp_layer_norm/scale/v
I0512 23:45:15.102161 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/pre_mlp_layer_norm/scale/v_col
I0512 23:45:15.102191 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/pre_mlp_layer_norm/scale/v_row
I0512 23:45:15.102227 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/pre_self_attention_layer_norm/scale/m
I0512 23:45:15.102258 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/pre_self_attention_layer_norm/scale/v
I0512 23:45:15.102289 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/pre_self_attention_layer_norm/scale/v_col
I0512 23:45:15.102319 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/pre_self_attention_layer_norm/scale/v_row
I0512 23:45:15.102349 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/self_attention/key/kernel/m
I0512 23:45:15.102380 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/self_attention/key/kernel/v
I0512 23:45:15.102410 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/self_attention/key/kernel/v_col
I0512 23:45:15.102441 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/self_attention/key/kernel/v_row
I0512 23:45:15.102473 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/self_attention/out/kernel/m
I0512 23:45:15.102504 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/self_attention/out/kernel/v
I0512 23:45:15.102535 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/self_attention/out/kernel/v_col
I0512 23:45:15.102565 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/self_attention/out/kernel/v_row
I0512 23:45:15.102595 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/self_attention/query/kernel/m
I0512 23:45:15.102626 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/self_attention/query/kernel/v
I0512 23:45:15.102656 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/self_attention/query/kernel/v_col
I0512 23:45:15.102686 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/self_attention/query/kernel/v_row
I0512 23:45:15.102716 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/self_attention/value/kernel/m
I0512 23:45:15.102746 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/self_attention/value/kernel/v
I0512 23:45:15.102777 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/self_attention/value/kernel/v_col
I0512 23:45:15.102807 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/self_attention/value/kernel/v_row
I0512 23:45:15.102837 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_12/mlp/wi_0/kernel/m
I0512 23:45:15.102868 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_12/mlp/wi_0/kernel/v
I0512 23:45:15.102898 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_12/mlp/wi_0/kernel/v_col
I0512 23:45:15.102928 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_12/mlp/wi_0/kernel/v_row
I0512 23:45:15.102958 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_12/mlp/wi_1/kernel/m
I0512 23:45:15.102988 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_12/mlp/wi_1/kernel/v
I0512 23:45:15.103019 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_12/mlp/wi_1/kernel/v_col
I0512 23:45:15.103050 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_12/mlp/wi_1/kernel/v_row
I0512 23:45:15.103080 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_12/mlp/wo/kernel/m
I0512 23:45:15.103111 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_12/mlp/wo/kernel/v
I0512 23:45:15.103141 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_12/mlp/wo/kernel/v_col
I0512 23:45:15.103171 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_12/mlp/wo/kernel/v_row
I0512 23:45:15.103206 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_12/pre_mlp_layer_norm/scale/m
I0512 23:45:15.103238 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_12/pre_mlp_layer_norm/scale/v
I0512 23:45:15.103269 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_12/pre_mlp_layer_norm/scale/v_col
I0512 23:45:15.103300 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_12/pre_mlp_layer_norm/scale/v_row
I0512 23:45:15.103330 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_12/pre_self_attention_layer_norm/scale/m
I0512 23:45:15.103361 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_12/pre_self_attention_layer_norm/scale/v
I0512 23:45:15.103391 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_12/pre_self_attention_layer_norm/scale/v_col
I0512 23:45:15.103421 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_12/pre_self_attention_layer_norm/scale/v_row
I0512 23:45:15.103452 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_12/self_attention/key/kernel/m
I0512 23:45:15.103485 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_12/self_attention/key/kernel/v
I0512 23:45:15.103516 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_12/self_attention/key/kernel/v_col
I0512 23:45:15.103546 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_12/self_attention/key/kernel/v_row
I0512 23:45:15.103577 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_12/self_attention/out/kernel/m
I0512 23:45:15.103607 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_12/self_attention/out/kernel/v
I0512 23:45:15.103637 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_12/self_attention/out/kernel/v_col
I0512 23:45:15.103667 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_12/self_attention/out/kernel/v_row
I0512 23:45:15.103698 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_12/self_attention/query/kernel/m
I0512 23:45:15.103728 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_12/self_attention/query/kernel/v
I0512 23:45:15.103758 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_12/self_attention/query/kernel/v_col
I0512 23:45:15.103789 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_12/self_attention/query/kernel/v_row
I0512 23:45:15.103819 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_12/self_attention/value/kernel/m
I0512 23:45:15.103851 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_12/self_attention/value/kernel/v
I0512 23:45:15.103882 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_12/self_attention/value/kernel/v_col
I0512 23:45:15.103913 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_12/self_attention/value/kernel/v_row
I0512 23:45:15.103942 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_13/mlp/wi_0/kernel/m
I0512 23:45:15.103973 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_13/mlp/wi_0/kernel/v
I0512 23:45:15.104003 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_13/mlp/wi_0/kernel/v_col
I0512 23:45:15.104034 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_13/mlp/wi_0/kernel/v_row
I0512 23:45:15.104064 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_13/mlp/wi_1/kernel/m
I0512 23:45:15.104095 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_13/mlp/wi_1/kernel/v
I0512 23:45:15.104125 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_13/mlp/wi_1/kernel/v_col
I0512 23:45:15.104155 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_13/mlp/wi_1/kernel/v_row
I0512 23:45:15.104185 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_13/mlp/wo/kernel/m
I0512 23:45:15.104221 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_13/mlp/wo/kernel/v
I0512 23:45:15.104252 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_13/mlp/wo/kernel/v_col
I0512 23:45:15.104283 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_13/mlp/wo/kernel/v_row
I0512 23:45:15.104313 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_13/pre_mlp_layer_norm/scale/m
I0512 23:45:15.104344 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_13/pre_mlp_layer_norm/scale/v
I0512 23:45:15.104374 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_13/pre_mlp_layer_norm/scale/v_col
I0512 23:45:15.104430 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_13/pre_mlp_layer_norm/scale/v_row
I0512 23:45:15.104466 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_13/pre_self_attention_layer_norm/scale/m
I0512 23:45:15.104498 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_13/pre_self_attention_layer_norm/scale/v
I0512 23:45:15.104528 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_13/pre_self_attention_layer_norm/scale/v_col
I0512 23:45:15.104559 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_13/pre_self_attention_layer_norm/scale/v_row
I0512 23:45:15.104589 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_13/self_attention/key/kernel/m
I0512 23:45:15.104619 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_13/self_attention/key/kernel/v
I0512 23:45:15.104650 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_13/self_attention/key/kernel/v_col
I0512 23:45:15.104680 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_13/self_attention/key/kernel/v_row
I0512 23:45:15.104710 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_13/self_attention/out/kernel/m
I0512 23:45:15.104740 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_13/self_attention/out/kernel/v
I0512 23:45:15.104771 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_13/self_attention/out/kernel/v_col
I0512 23:45:15.104801 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_13/self_attention/out/kernel/v_row
I0512 23:45:15.104831 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_13/self_attention/query/kernel/m
I0512 23:45:15.104861 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_13/self_attention/query/kernel/v
I0512 23:45:15.104892 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_13/self_attention/query/kernel/v_col
I0512 23:45:15.104921 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_13/self_attention/query/kernel/v_row
I0512 23:45:15.104951 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_13/self_attention/value/kernel/m
I0512 23:45:15.104981 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_13/self_attention/value/kernel/v
I0512 23:45:15.105011 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_13/self_attention/value/kernel/v_col
I0512 23:45:15.105065 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_13/self_attention/value/kernel/v_row
I0512 23:45:15.105100 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_14/mlp/wi_0/kernel/m
I0512 23:45:15.105131 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_14/mlp/wi_0/kernel/v
I0512 23:45:15.105161 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_14/mlp/wi_0/kernel/v_col
I0512 23:45:15.105192 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_14/mlp/wi_0/kernel/v_row
I0512 23:45:15.105228 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_14/mlp/wi_1/kernel/m
I0512 23:45:15.105259 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_14/mlp/wi_1/kernel/v
I0512 23:45:15.105290 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_14/mlp/wi_1/kernel/v_col
I0512 23:45:15.105321 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_14/mlp/wi_1/kernel/v_row
I0512 23:45:15.105351 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_14/mlp/wo/kernel/m
I0512 23:45:15.105382 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_14/mlp/wo/kernel/v
I0512 23:45:15.105412 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_14/mlp/wo/kernel/v_col
I0512 23:45:15.105442 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_14/mlp/wo/kernel/v_row
I0512 23:45:15.105475 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_14/pre_mlp_layer_norm/scale/m
I0512 23:45:15.105506 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_14/pre_mlp_layer_norm/scale/v
I0512 23:45:15.105537 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_14/pre_mlp_layer_norm/scale/v_col
I0512 23:45:15.105567 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_14/pre_mlp_layer_norm/scale/v_row
I0512 23:45:15.105598 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_14/pre_self_attention_layer_norm/scale/m
I0512 23:45:15.105628 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_14/pre_self_attention_layer_norm/scale/v
I0512 23:45:15.105659 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_14/pre_self_attention_layer_norm/scale/v_col
I0512 23:45:15.105689 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_14/pre_self_attention_layer_norm/scale/v_row
I0512 23:45:15.105719 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_14/self_attention/key/kernel/m
I0512 23:45:15.105750 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_14/self_attention/key/kernel/v
I0512 23:45:15.105780 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_14/self_attention/key/kernel/v_col
I0512 23:45:15.105810 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_14/self_attention/key/kernel/v_row
I0512 23:45:15.105840 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_14/self_attention/out/kernel/m
I0512 23:45:15.105870 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_14/self_attention/out/kernel/v
I0512 23:45:15.105900 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_14/self_attention/out/kernel/v_col
I0512 23:45:15.105931 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_14/self_attention/out/kernel/v_row
I0512 23:45:15.105961 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_14/self_attention/query/kernel/m
I0512 23:45:15.105991 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_14/self_attention/query/kernel/v
I0512 23:45:15.106021 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_14/self_attention/query/kernel/v_col
I0512 23:45:15.106051 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_14/self_attention/query/kernel/v_row
I0512 23:45:15.106081 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_14/self_attention/value/kernel/m
I0512 23:45:15.106112 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_14/self_attention/value/kernel/v
I0512 23:45:15.106142 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_14/self_attention/value/kernel/v_col
I0512 23:45:15.106171 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_14/self_attention/value/kernel/v_row
I0512 23:45:15.106206 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_15/mlp/wi_0/kernel/m
I0512 23:45:15.106239 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_15/mlp/wi_0/kernel/v
I0512 23:45:15.106269 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_15/mlp/wi_0/kernel/v_col
I0512 23:45:15.106300 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_15/mlp/wi_0/kernel/v_row
I0512 23:45:15.106330 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_15/mlp/wi_1/kernel/m
I0512 23:45:15.106360 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_15/mlp/wi_1/kernel/v
I0512 23:45:15.106390 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_15/mlp/wi_1/kernel/v_col
I0512 23:45:15.106420 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_15/mlp/wi_1/kernel/v_row
I0512 23:45:15.106451 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_15/mlp/wo/kernel/m
I0512 23:45:15.106484 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_15/mlp/wo/kernel/v
I0512 23:45:15.106514 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_15/mlp/wo/kernel/v_col
I0512 23:45:15.106545 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_15/mlp/wo/kernel/v_row
I0512 23:45:15.106575 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_15/pre_mlp_layer_norm/scale/m
I0512 23:45:15.106606 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_15/pre_mlp_layer_norm/scale/v
I0512 23:45:15.106636 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_15/pre_mlp_layer_norm/scale/v_col
I0512 23:45:15.106666 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_15/pre_mlp_layer_norm/scale/v_row
I0512 23:45:15.106696 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_15/pre_self_attention_layer_norm/scale/m
I0512 23:45:15.106726 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_15/pre_self_attention_layer_norm/scale/v
I0512 23:45:15.106756 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_15/pre_self_attention_layer_norm/scale/v_col
I0512 23:45:15.106786 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_15/pre_self_attention_layer_norm/scale/v_row
I0512 23:45:15.106817 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_15/self_attention/key/kernel/m
I0512 23:45:15.106847 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_15/self_attention/key/kernel/v
I0512 23:45:15.106877 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_15/self_attention/key/kernel/v_col
I0512 23:45:15.106907 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_15/self_attention/key/kernel/v_row
I0512 23:45:15.106937 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_15/self_attention/out/kernel/m
I0512 23:45:15.106968 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_15/self_attention/out/kernel/v
I0512 23:45:15.106998 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_15/self_attention/out/kernel/v_col
I0512 23:45:15.107028 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_15/self_attention/out/kernel/v_row
I0512 23:45:15.107058 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_15/self_attention/query/kernel/m
I0512 23:45:15.107088 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_15/self_attention/query/kernel/v
I0512 23:45:15.107118 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_15/self_attention/query/kernel/v_col
I0512 23:45:15.107148 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_15/self_attention/query/kernel/v_row
I0512 23:45:15.107182 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_15/self_attention/value/kernel/m
I0512 23:45:15.107222 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_15/self_attention/value/kernel/v
I0512 23:45:15.107259 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_15/self_attention/value/kernel/v_col
I0512 23:45:15.107291 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_15/self_attention/value/kernel/v_row
I0512 23:45:15.107322 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_16/mlp/wi_0/kernel/m
I0512 23:45:15.107359 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_16/mlp/wi_0/kernel/v
I0512 23:45:15.107392 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_16/mlp/wi_0/kernel/v_col
I0512 23:45:15.107422 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_16/mlp/wi_0/kernel/v_row
I0512 23:45:15.107453 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_16/mlp/wi_1/kernel/m
I0512 23:45:15.107486 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_16/mlp/wi_1/kernel/v
I0512 23:45:15.107516 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_16/mlp/wi_1/kernel/v_col
I0512 23:45:15.107546 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_16/mlp/wi_1/kernel/v_row
I0512 23:45:15.107576 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_16/mlp/wo/kernel/m
I0512 23:45:15.107606 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_16/mlp/wo/kernel/v
I0512 23:45:15.107636 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_16/mlp/wo/kernel/v_col
I0512 23:45:15.107666 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_16/mlp/wo/kernel/v_row
I0512 23:45:15.107695 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_16/pre_mlp_layer_norm/scale/m
I0512 23:45:15.107725 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_16/pre_mlp_layer_norm/scale/v
I0512 23:45:15.107754 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_16/pre_mlp_layer_norm/scale/v_col
I0512 23:45:15.107784 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_16/pre_mlp_layer_norm/scale/v_row
I0512 23:45:15.107815 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_16/pre_self_attention_layer_norm/scale/m
I0512 23:45:15.107845 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_16/pre_self_attention_layer_norm/scale/v
I0512 23:45:15.107875 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_16/pre_self_attention_layer_norm/scale/v_col
I0512 23:45:15.107905 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_16/pre_self_attention_layer_norm/scale/v_row
I0512 23:45:15.107935 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_16/self_attention/key/kernel/m
I0512 23:45:15.107965 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_16/self_attention/key/kernel/v
I0512 23:45:15.107995 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_16/self_attention/key/kernel/v_col
I0512 23:45:15.108025 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_16/self_attention/key/kernel/v_row
I0512 23:45:15.108055 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_16/self_attention/out/kernel/m
I0512 23:45:15.108084 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_16/self_attention/out/kernel/v
I0512 23:45:15.108114 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_16/self_attention/out/kernel/v_col
I0512 23:45:15.108143 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_16/self_attention/out/kernel/v_row
I0512 23:45:15.108173 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_16/self_attention/query/kernel/m
I0512 23:45:15.108208 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_16/self_attention/query/kernel/v
I0512 23:45:15.108240 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_16/self_attention/query/kernel/v_col
I0512 23:45:15.108270 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_16/self_attention/query/kernel/v_row
I0512 23:45:15.108300 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_16/self_attention/value/kernel/m
I0512 23:45:15.108330 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_16/self_attention/value/kernel/v
I0512 23:45:15.108360 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_16/self_attention/value/kernel/v_col
I0512 23:45:15.108389 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_16/self_attention/value/kernel/v_row
I0512 23:45:15.108419 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/extra_mlp/wi_0/kernel/m
I0512 23:45:15.108449 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/extra_mlp/wi_0/kernel/v
I0512 23:45:15.108481 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/extra_mlp/wi_0/kernel/v_col
I0512 23:45:15.108511 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/extra_mlp/wi_0/kernel/v_row
I0512 23:45:15.108541 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/extra_mlp/wi_1/kernel/m
I0512 23:45:15.108570 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/extra_mlp/wi_1/kernel/v
I0512 23:45:15.108599 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/extra_mlp/wi_1/kernel/v_col
I0512 23:45:15.108629 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/extra_mlp/wi_1/kernel/v_row
I0512 23:45:15.108658 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/extra_mlp/wo/kernel/m
I0512 23:45:15.108688 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/extra_mlp/wo/kernel/v
I0512 23:45:15.108717 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/extra_mlp/wo/kernel/v_col
I0512 23:45:15.108747 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/extra_mlp/wo/kernel/v_row
I0512 23:45:15.108776 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/mlp/expert/wi_0/kernel/m
I0512 23:45:15.108805 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/mlp/expert/wi_0/kernel/v
I0512 23:45:15.108834 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/mlp/expert/wi_0/kernel/v_col
I0512 23:45:15.108864 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/mlp/expert/wi_0/kernel/v_row
I0512 23:45:15.108893 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/mlp/expert/wi_1/kernel/m
I0512 23:45:15.108923 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/mlp/expert/wi_1/kernel/v
I0512 23:45:15.108952 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/mlp/expert/wi_1/kernel/v_col
I0512 23:45:15.108982 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/mlp/expert/wi_1/kernel/v_row
I0512 23:45:15.109011 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/mlp/expert/wo/kernel/m
I0512 23:45:15.109061 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/mlp/expert/wo/kernel/v
I0512 23:45:15.109094 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/mlp/expert/wo/kernel/v_col
I0512 23:45:15.109125 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/mlp/expert/wo/kernel/v_row
I0512 23:45:15.109155 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/mlp/router/router_weights/w/kernel/m
I0512 23:45:15.109184 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/mlp/router/router_weights/w/kernel/v
I0512 23:45:15.109221 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/mlp/router/router_weights/w/kernel/v_col
I0512 23:45:15.109252 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/mlp/router/router_weights/w/kernel/v_row
I0512 23:45:15.109283 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/pre_extra_mlp_layer_norm/scale/m
I0512 23:45:15.109313 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/pre_extra_mlp_layer_norm/scale/v
I0512 23:45:15.109342 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/pre_extra_mlp_layer_norm/scale/v_col
I0512 23:45:15.109372 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/pre_extra_mlp_layer_norm/scale/v_row
I0512 23:45:15.109401 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/pre_mlp_layer_norm/scale/m
I0512 23:45:15.109431 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/pre_mlp_layer_norm/scale/v
I0512 23:45:15.109462 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/pre_mlp_layer_norm/scale/v_col
I0512 23:45:15.109493 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/pre_mlp_layer_norm/scale/v_row
I0512 23:45:15.109523 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/pre_self_attention_layer_norm/scale/m
I0512 23:45:15.109553 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/pre_self_attention_layer_norm/scale/v
I0512 23:45:15.109582 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/pre_self_attention_layer_norm/scale/v_col
I0512 23:45:15.109612 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/pre_self_attention_layer_norm/scale/v_row
I0512 23:45:15.109642 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/self_attention/key/kernel/m
I0512 23:45:15.109671 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/self_attention/key/kernel/v
I0512 23:45:15.109701 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/self_attention/key/kernel/v_col
I0512 23:45:15.109731 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/self_attention/key/kernel/v_row
I0512 23:45:15.109760 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/self_attention/out/kernel/m
I0512 23:45:15.109791 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/self_attention/out/kernel/v
I0512 23:45:15.109820 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/self_attention/out/kernel/v_col
I0512 23:45:15.109850 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/self_attention/out/kernel/v_row
I0512 23:45:15.109879 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/self_attention/query/kernel/m
I0512 23:45:15.109909 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/self_attention/query/kernel/v
I0512 23:45:15.109938 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/self_attention/query/kernel/v_col
I0512 23:45:15.109968 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/self_attention/query/kernel/v_row
I0512 23:45:15.109998 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/self_attention/value/kernel/m
I0512 23:45:15.110027 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/self_attention/value/kernel/v
I0512 23:45:15.110057 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/self_attention/value/kernel/v_col
I0512 23:45:15.110086 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/self_attention/value/kernel/v_row
I0512 23:45:15.110116 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_18/mlp/wi_0/kernel/m
I0512 23:45:15.110145 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_18/mlp/wi_0/kernel/v
I0512 23:45:15.110176 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_18/mlp/wi_0/kernel/v_col
I0512 23:45:15.110210 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_18/mlp/wi_0/kernel/v_row
I0512 23:45:15.110242 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_18/mlp/wi_1/kernel/m
I0512 23:45:15.110271 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_18/mlp/wi_1/kernel/v
I0512 23:45:15.110301 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_18/mlp/wi_1/kernel/v_col
I0512 23:45:15.110331 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_18/mlp/wi_1/kernel/v_row
I0512 23:45:15.110361 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_18/mlp/wo/kernel/m
I0512 23:45:15.110390 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_18/mlp/wo/kernel/v
I0512 23:45:15.110419 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_18/mlp/wo/kernel/v_col
I0512 23:45:15.110449 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_18/mlp/wo/kernel/v_row
I0512 23:45:15.110481 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_18/pre_mlp_layer_norm/scale/m
I0512 23:45:15.110511 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_18/pre_mlp_layer_norm/scale/v
I0512 23:45:15.110541 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_18/pre_mlp_layer_norm/scale/v_col
I0512 23:45:15.110570 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_18/pre_mlp_layer_norm/scale/v_row
I0512 23:45:15.110600 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_18/pre_self_attention_layer_norm/scale/m
I0512 23:45:15.110630 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_18/pre_self_attention_layer_norm/scale/v
I0512 23:45:15.110660 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_18/pre_self_attention_layer_norm/scale/v_col
I0512 23:45:15.110693 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_18/pre_self_attention_layer_norm/scale/v_row
I0512 23:45:15.110728 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_18/self_attention/key/kernel/m
I0512 23:45:15.110758 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_18/self_attention/key/kernel/v
I0512 23:45:15.110788 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_18/self_attention/key/kernel/v_col
I0512 23:45:15.110818 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_18/self_attention/key/kernel/v_row
I0512 23:45:15.110847 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_18/self_attention/out/kernel/m
I0512 23:45:15.110877 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_18/self_attention/out/kernel/v
I0512 23:45:15.110906 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_18/self_attention/out/kernel/v_col
I0512 23:45:15.110936 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_18/self_attention/out/kernel/v_row
I0512 23:45:15.110965 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_18/self_attention/query/kernel/m
I0512 23:45:15.110995 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_18/self_attention/query/kernel/v
I0512 23:45:15.111025 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_18/self_attention/query/kernel/v_col
I0512 23:45:15.111054 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_18/self_attention/query/kernel/v_row
I0512 23:45:15.111084 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_18/self_attention/value/kernel/m
I0512 23:45:15.111113 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_18/self_attention/value/kernel/v
I0512 23:45:15.111143 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_18/self_attention/value/kernel/v_col
I0512 23:45:15.111172 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_18/self_attention/value/kernel/v_row
I0512 23:45:15.111207 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_19/mlp/wi_0/kernel/m
I0512 23:45:15.111239 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_19/mlp/wi_0/kernel/v
I0512 23:45:15.111269 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_19/mlp/wi_0/kernel/v_col
I0512 23:45:15.111299 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_19/mlp/wi_0/kernel/v_row
I0512 23:45:15.111329 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_19/mlp/wi_1/kernel/m
I0512 23:45:15.111358 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_19/mlp/wi_1/kernel/v
I0512 23:45:15.111387 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_19/mlp/wi_1/kernel/v_col
I0512 23:45:15.111417 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_19/mlp/wi_1/kernel/v_row
I0512 23:45:15.111446 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_19/mlp/wo/kernel/m
I0512 23:45:15.111478 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_19/mlp/wo/kernel/v
I0512 23:45:15.111509 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_19/mlp/wo/kernel/v_col
I0512 23:45:15.111539 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_19/mlp/wo/kernel/v_row
I0512 23:45:15.111568 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_19/pre_mlp_layer_norm/scale/m
I0512 23:45:15.111598 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_19/pre_mlp_layer_norm/scale/v
I0512 23:45:15.111627 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_19/pre_mlp_layer_norm/scale/v_col
I0512 23:45:15.111658 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_19/pre_mlp_layer_norm/scale/v_row
I0512 23:45:15.111687 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_19/pre_self_attention_layer_norm/scale/m
I0512 23:45:15.111717 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_19/pre_self_attention_layer_norm/scale/v
I0512 23:45:15.111747 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_19/pre_self_attention_layer_norm/scale/v_col
I0512 23:45:15.111777 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_19/pre_self_attention_layer_norm/scale/v_row
I0512 23:45:15.111806 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_19/self_attention/key/kernel/m
I0512 23:45:15.111836 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_19/self_attention/key/kernel/v
I0512 23:45:15.111865 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_19/self_attention/key/kernel/v_col
I0512 23:45:15.111895 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_19/self_attention/key/kernel/v_row
I0512 23:45:15.111925 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_19/self_attention/out/kernel/m
I0512 23:45:15.111954 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_19/self_attention/out/kernel/v
I0512 23:45:15.111984 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_19/self_attention/out/kernel/v_col
I0512 23:45:15.112014 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_19/self_attention/out/kernel/v_row
I0512 23:45:15.112044 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_19/self_attention/query/kernel/m
I0512 23:45:15.112073 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_19/self_attention/query/kernel/v
I0512 23:45:15.112103 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_19/self_attention/query/kernel/v_col
I0512 23:45:15.112133 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_19/self_attention/query/kernel/v_row
I0512 23:45:15.112162 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_19/self_attention/value/kernel/m
I0512 23:45:15.112192 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_19/self_attention/value/kernel/v
I0512 23:45:15.112229 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_19/self_attention/value/kernel/v_col
I0512 23:45:15.112259 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_19/self_attention/value/kernel/v_row
I0512 23:45:15.112289 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_2/mlp/wi_0/kernel/m
I0512 23:45:15.112318 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_2/mlp/wi_0/kernel/v
I0512 23:45:15.112348 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_2/mlp/wi_0/kernel/v_col
I0512 23:45:15.112378 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_2/mlp/wi_0/kernel/v_row
I0512 23:45:15.112408 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_2/mlp/wi_1/kernel/m
I0512 23:45:15.112437 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_2/mlp/wi_1/kernel/v
I0512 23:45:15.112469 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_2/mlp/wi_1/kernel/v_col
I0512 23:45:15.112499 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_2/mlp/wi_1/kernel/v_row
I0512 23:45:15.112529 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_2/mlp/wo/kernel/m
I0512 23:45:15.112559 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_2/mlp/wo/kernel/v
I0512 23:45:15.112589 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_2/mlp/wo/kernel/v_col
I0512 23:45:15.112618 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_2/mlp/wo/kernel/v_row
I0512 23:45:15.112648 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_2/pre_mlp_layer_norm/scale/m
I0512 23:45:15.112678 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_2/pre_mlp_layer_norm/scale/v
I0512 23:45:15.112708 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_2/pre_mlp_layer_norm/scale/v_col
I0512 23:45:15.112738 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_2/pre_mlp_layer_norm/scale/v_row
I0512 23:45:15.112768 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_2/pre_self_attention_layer_norm/scale/m
I0512 23:45:15.112798 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_2/pre_self_attention_layer_norm/scale/v
I0512 23:45:15.112828 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_2/pre_self_attention_layer_norm/scale/v_col
I0512 23:45:15.112858 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_2/pre_self_attention_layer_norm/scale/v_row
I0512 23:45:15.112888 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_2/self_attention/key/kernel/m
I0512 23:45:15.112918 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_2/self_attention/key/kernel/v
I0512 23:45:15.112948 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_2/self_attention/key/kernel/v_col
I0512 23:45:15.112978 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_2/self_attention/key/kernel/v_row
I0512 23:45:15.113008 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_2/self_attention/out/kernel/m
I0512 23:45:15.113057 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_2/self_attention/out/kernel/v
I0512 23:45:15.113091 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_2/self_attention/out/kernel/v_col
I0512 23:45:15.113121 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_2/self_attention/out/kernel/v_row
I0512 23:45:15.113151 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_2/self_attention/query/kernel/m
I0512 23:45:15.113181 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_2/self_attention/query/kernel/v
I0512 23:45:15.113217 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_2/self_attention/query/kernel/v_col
I0512 23:45:15.113248 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_2/self_attention/query/kernel/v_row
I0512 23:45:15.113278 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_2/self_attention/value/kernel/m
I0512 23:45:15.113308 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_2/self_attention/value/kernel/v
I0512 23:45:15.113337 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_2/self_attention/value/kernel/v_col
I0512 23:45:15.113367 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_2/self_attention/value/kernel/v_row
I0512 23:45:15.113397 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_20/mlp/wi_0/kernel/m
I0512 23:45:15.113427 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_20/mlp/wi_0/kernel/v
I0512 23:45:15.113458 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_20/mlp/wi_0/kernel/v_col
I0512 23:45:15.113489 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_20/mlp/wi_0/kernel/v_row
I0512 23:45:15.113520 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_20/mlp/wi_1/kernel/m
I0512 23:45:15.113549 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_20/mlp/wi_1/kernel/v
I0512 23:45:15.113579 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_20/mlp/wi_1/kernel/v_col
I0512 23:45:15.113609 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_20/mlp/wi_1/kernel/v_row
I0512 23:45:15.113638 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_20/mlp/wo/kernel/m
I0512 23:45:15.113668 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_20/mlp/wo/kernel/v
I0512 23:45:15.113698 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_20/mlp/wo/kernel/v_col
I0512 23:45:15.113727 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_20/mlp/wo/kernel/v_row
I0512 23:45:15.113756 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_20/pre_mlp_layer_norm/scale/m
I0512 23:45:15.113786 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_20/pre_mlp_layer_norm/scale/v
I0512 23:45:15.113816 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_20/pre_mlp_layer_norm/scale/v_col
I0512 23:45:15.113846 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_20/pre_mlp_layer_norm/scale/v_row
I0512 23:45:15.113876 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_20/pre_self_attention_layer_norm/scale/m
I0512 23:45:15.113906 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_20/pre_self_attention_layer_norm/scale/v
I0512 23:45:15.113936 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_20/pre_self_attention_layer_norm/scale/v_col
I0512 23:45:15.113965 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_20/pre_self_attention_layer_norm/scale/v_row
I0512 23:45:15.113996 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_20/self_attention/key/kernel/m
I0512 23:45:15.114025 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_20/self_attention/key/kernel/v
I0512 23:45:15.114055 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_20/self_attention/key/kernel/v_col
I0512 23:45:15.114084 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_20/self_attention/key/kernel/v_row
I0512 23:45:15.114114 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_20/self_attention/out/kernel/m
I0512 23:45:15.114144 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_20/self_attention/out/kernel/v
I0512 23:45:15.114174 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_20/self_attention/out/kernel/v_col
I0512 23:45:15.114208 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_20/self_attention/out/kernel/v_row
I0512 23:45:15.114240 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_20/self_attention/query/kernel/m
I0512 23:45:15.114270 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_20/self_attention/query/kernel/v
I0512 23:45:15.114300 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_20/self_attention/query/kernel/v_col
I0512 23:45:15.114329 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_20/self_attention/query/kernel/v_row
I0512 23:45:15.114359 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_20/self_attention/value/kernel/m
I0512 23:45:15.114389 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_20/self_attention/value/kernel/v
I0512 23:45:15.114419 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_20/self_attention/value/kernel/v_col
I0512 23:45:15.114449 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_20/self_attention/value/kernel/v_row
I0512 23:45:15.114481 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_21/mlp/wi_0/kernel/m
I0512 23:45:15.114511 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_21/mlp/wi_0/kernel/v
I0512 23:45:15.114542 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_21/mlp/wi_0/kernel/v_col
I0512 23:45:15.114571 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_21/mlp/wi_0/kernel/v_row
I0512 23:45:15.114601 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_21/mlp/wi_1/kernel/m
I0512 23:45:15.114631 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_21/mlp/wi_1/kernel/v
I0512 23:45:15.114660 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_21/mlp/wi_1/kernel/v_col
I0512 23:45:15.114690 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_21/mlp/wi_1/kernel/v_row
I0512 23:45:15.114720 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_21/mlp/wo/kernel/m
I0512 23:45:15.114750 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_21/mlp/wo/kernel/v
I0512 23:45:15.114779 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_21/mlp/wo/kernel/v_col
I0512 23:45:15.114809 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_21/mlp/wo/kernel/v_row
I0512 23:45:15.114839 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_21/pre_mlp_layer_norm/scale/m
I0512 23:45:15.114868 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_21/pre_mlp_layer_norm/scale/v
I0512 23:45:15.114898 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_21/pre_mlp_layer_norm/scale/v_col
I0512 23:45:15.114928 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_21/pre_mlp_layer_norm/scale/v_row
I0512 23:45:15.114958 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_21/pre_self_attention_layer_norm/scale/m
I0512 23:45:15.114988 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_21/pre_self_attention_layer_norm/scale/v
I0512 23:45:15.115018 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_21/pre_self_attention_layer_norm/scale/v_col
I0512 23:45:15.115048 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_21/pre_self_attention_layer_norm/scale/v_row
I0512 23:45:15.115078 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_21/self_attention/key/kernel/m
I0512 23:45:15.115108 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_21/self_attention/key/kernel/v
I0512 23:45:15.115138 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_21/self_attention/key/kernel/v_col
I0512 23:45:15.115168 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_21/self_attention/key/kernel/v_row
I0512 23:45:15.115197 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_21/self_attention/out/kernel/m
I0512 23:45:15.115233 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_21/self_attention/out/kernel/v
I0512 23:45:15.115263 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_21/self_attention/out/kernel/v_col
I0512 23:45:15.115293 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_21/self_attention/out/kernel/v_row
I0512 23:45:15.115323 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_21/self_attention/query/kernel/m
I0512 23:45:15.115353 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_21/self_attention/query/kernel/v
I0512 23:45:15.115382 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_21/self_attention/query/kernel/v_col
I0512 23:45:15.115412 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_21/self_attention/query/kernel/v_row
I0512 23:45:15.115442 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_21/self_attention/value/kernel/m
I0512 23:45:15.115473 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_21/self_attention/value/kernel/v
I0512 23:45:15.115503 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_21/self_attention/value/kernel/v_col
I0512 23:45:15.115533 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_21/self_attention/value/kernel/v_row
I0512 23:45:15.115562 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_22/mlp/wi_0/kernel/m
I0512 23:45:15.115592 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_22/mlp/wi_0/kernel/v
I0512 23:45:15.115622 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_22/mlp/wi_0/kernel/v_col
I0512 23:45:15.115651 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_22/mlp/wi_0/kernel/v_row
I0512 23:45:15.115681 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_22/mlp/wi_1/kernel/m
I0512 23:45:15.115710 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_22/mlp/wi_1/kernel/v
I0512 23:45:15.115740 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_22/mlp/wi_1/kernel/v_col
I0512 23:45:15.115770 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_22/mlp/wi_1/kernel/v_row
I0512 23:45:15.115800 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_22/mlp/wo/kernel/m
I0512 23:45:15.115829 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_22/mlp/wo/kernel/v
I0512 23:45:15.115858 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_22/mlp/wo/kernel/v_col
I0512 23:45:15.115888 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_22/mlp/wo/kernel/v_row
I0512 23:45:15.115917 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_22/pre_mlp_layer_norm/scale/m
I0512 23:45:15.115947 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_22/pre_mlp_layer_norm/scale/v
I0512 23:45:15.115977 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_22/pre_mlp_layer_norm/scale/v_col
I0512 23:45:15.116006 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_22/pre_mlp_layer_norm/scale/v_row
I0512 23:45:15.116036 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_22/pre_self_attention_layer_norm/scale/m
I0512 23:45:15.116065 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_22/pre_self_attention_layer_norm/scale/v
I0512 23:45:15.116095 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_22/pre_self_attention_layer_norm/scale/v_col
I0512 23:45:15.116125 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_22/pre_self_attention_layer_norm/scale/v_row
I0512 23:45:15.116155 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_22/self_attention/key/kernel/m
I0512 23:45:15.116185 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_22/self_attention/key/kernel/v
I0512 23:45:15.116226 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_22/self_attention/key/kernel/v_col
I0512 23:45:15.116257 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_22/self_attention/key/kernel/v_row
I0512 23:45:15.116287 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_22/self_attention/out/kernel/m
I0512 23:45:15.116317 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_22/self_attention/out/kernel/v
I0512 23:45:15.116347 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_22/self_attention/out/kernel/v_col
I0512 23:45:15.116377 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_22/self_attention/out/kernel/v_row
I0512 23:45:15.116407 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_22/self_attention/query/kernel/m
I0512 23:45:15.116437 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_22/self_attention/query/kernel/v
I0512 23:45:15.116470 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_22/self_attention/query/kernel/v_col
I0512 23:45:15.116502 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_22/self_attention/query/kernel/v_row
I0512 23:45:15.116531 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_22/self_attention/value/kernel/m
I0512 23:45:15.116561 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_22/self_attention/value/kernel/v
I0512 23:45:15.116591 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_22/self_attention/value/kernel/v_col
I0512 23:45:15.116620 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_22/self_attention/value/kernel/v_row
I0512 23:45:15.116650 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/extra_mlp/wi_0/kernel/m
I0512 23:45:15.116680 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/extra_mlp/wi_0/kernel/v
I0512 23:45:15.116709 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/extra_mlp/wi_0/kernel/v_col
I0512 23:45:15.116739 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/extra_mlp/wi_0/kernel/v_row
I0512 23:45:15.116769 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/extra_mlp/wi_1/kernel/m
I0512 23:45:15.116799 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/extra_mlp/wi_1/kernel/v
I0512 23:45:15.116829 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/extra_mlp/wi_1/kernel/v_col
I0512 23:45:15.116858 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/extra_mlp/wi_1/kernel/v_row
I0512 23:45:15.116888 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/extra_mlp/wo/kernel/m
I0512 23:45:15.116917 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/extra_mlp/wo/kernel/v
I0512 23:45:15.116947 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/extra_mlp/wo/kernel/v_col
I0512 23:45:15.116976 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/extra_mlp/wo/kernel/v_row
I0512 23:45:15.117006 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/mlp/expert/wi_0/kernel/m
I0512 23:45:15.117054 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/mlp/expert/wi_0/kernel/v
I0512 23:45:15.117088 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/mlp/expert/wi_0/kernel/v_col
I0512 23:45:15.117119 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/mlp/expert/wi_0/kernel/v_row
I0512 23:45:15.117149 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/mlp/expert/wi_1/kernel/m
I0512 23:45:15.117178 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/mlp/expert/wi_1/kernel/v
I0512 23:45:15.117214 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/mlp/expert/wi_1/kernel/v_col
I0512 23:45:15.117245 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/mlp/expert/wi_1/kernel/v_row
I0512 23:45:15.117276 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/mlp/expert/wo/kernel/m
I0512 23:45:15.117305 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/mlp/expert/wo/kernel/v
I0512 23:45:15.117335 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/mlp/expert/wo/kernel/v_col
I0512 23:45:15.117364 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/mlp/expert/wo/kernel/v_row
I0512 23:45:15.117394 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/mlp/router/router_weights/w/kernel/m
I0512 23:45:15.117424 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/mlp/router/router_weights/w/kernel/v
I0512 23:45:15.117455 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/mlp/router/router_weights/w/kernel/v_col
I0512 23:45:15.117487 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/mlp/router/router_weights/w/kernel/v_row
I0512 23:45:15.117518 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/pre_extra_mlp_layer_norm/scale/m
I0512 23:45:15.117547 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/pre_extra_mlp_layer_norm/scale/v
I0512 23:45:15.117577 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/pre_extra_mlp_layer_norm/scale/v_col
I0512 23:45:15.117607 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/pre_extra_mlp_layer_norm/scale/v_row
I0512 23:45:15.117637 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/pre_mlp_layer_norm/scale/m
I0512 23:45:15.117666 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/pre_mlp_layer_norm/scale/v
I0512 23:45:15.117696 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/pre_mlp_layer_norm/scale/v_col
I0512 23:45:15.117725 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/pre_mlp_layer_norm/scale/v_row
I0512 23:45:15.117755 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/pre_self_attention_layer_norm/scale/m
I0512 23:45:15.117785 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/pre_self_attention_layer_norm/scale/v
I0512 23:45:15.117815 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/pre_self_attention_layer_norm/scale/v_col
I0512 23:45:15.117845 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/pre_self_attention_layer_norm/scale/v_row
I0512 23:45:15.117875 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/self_attention/key/kernel/m
I0512 23:45:15.117905 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/self_attention/key/kernel/v
I0512 23:45:15.117935 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/self_attention/key/kernel/v_col
I0512 23:45:15.117964 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/self_attention/key/kernel/v_row
I0512 23:45:15.117994 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/self_attention/out/kernel/m
I0512 23:45:15.118023 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/self_attention/out/kernel/v
I0512 23:45:15.118053 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/self_attention/out/kernel/v_col
I0512 23:45:15.118083 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/self_attention/out/kernel/v_row
I0512 23:45:15.118113 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/self_attention/query/kernel/m
I0512 23:45:15.118142 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/self_attention/query/kernel/v
I0512 23:45:15.118172 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/self_attention/query/kernel/v_col
I0512 23:45:15.118207 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/self_attention/query/kernel/v_row
I0512 23:45:15.118239 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/self_attention/value/kernel/m
I0512 23:45:15.118269 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/self_attention/value/kernel/v
I0512 23:45:15.118299 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/self_attention/value/kernel/v_col
I0512 23:45:15.118328 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/self_attention/value/kernel/v_row
I0512 23:45:15.118358 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_3/mlp/wi_0/kernel/m
I0512 23:45:15.118388 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_3/mlp/wi_0/kernel/v
I0512 23:45:15.118418 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_3/mlp/wi_0/kernel/v_col
I0512 23:45:15.118448 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_3/mlp/wi_0/kernel/v_row
I0512 23:45:15.118480 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_3/mlp/wi_1/kernel/m
I0512 23:45:15.118510 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_3/mlp/wi_1/kernel/v
I0512 23:45:15.118540 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_3/mlp/wi_1/kernel/v_col
I0512 23:45:15.118570 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_3/mlp/wi_1/kernel/v_row
I0512 23:45:15.118599 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_3/mlp/wo/kernel/m
I0512 23:45:15.118629 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_3/mlp/wo/kernel/v
I0512 23:45:15.118658 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_3/mlp/wo/kernel/v_col
I0512 23:45:15.118688 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_3/mlp/wo/kernel/v_row
I0512 23:45:15.118717 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_3/pre_mlp_layer_norm/scale/m
I0512 23:45:15.118747 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_3/pre_mlp_layer_norm/scale/v
I0512 23:45:15.118776 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_3/pre_mlp_layer_norm/scale/v_col
I0512 23:45:15.118806 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_3/pre_mlp_layer_norm/scale/v_row
I0512 23:45:15.118835 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_3/pre_self_attention_layer_norm/scale/m
I0512 23:45:15.118865 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_3/pre_self_attention_layer_norm/scale/v
I0512 23:45:15.118895 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_3/pre_self_attention_layer_norm/scale/v_col
I0512 23:45:15.118925 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_3/pre_self_attention_layer_norm/scale/v_row
I0512 23:45:15.118955 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_3/self_attention/key/kernel/m
I0512 23:45:15.118984 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_3/self_attention/key/kernel/v
I0512 23:45:15.119014 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_3/self_attention/key/kernel/v_col
I0512 23:45:15.119043 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_3/self_attention/key/kernel/v_row
I0512 23:45:15.119073 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_3/self_attention/out/kernel/m
I0512 23:45:15.119103 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_3/self_attention/out/kernel/v
I0512 23:45:15.119133 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_3/self_attention/out/kernel/v_col
I0512 23:45:15.119162 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_3/self_attention/out/kernel/v_row
I0512 23:45:15.119192 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_3/self_attention/query/kernel/m
I0512 23:45:15.119228 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_3/self_attention/query/kernel/v
I0512 23:45:15.119258 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_3/self_attention/query/kernel/v_col
I0512 23:45:15.119288 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_3/self_attention/query/kernel/v_row
I0512 23:45:15.119318 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_3/self_attention/value/kernel/m
I0512 23:45:15.119348 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_3/self_attention/value/kernel/v
I0512 23:45:15.119377 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_3/self_attention/value/kernel/v_col
I0512 23:45:15.119407 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_3/self_attention/value/kernel/v_row
I0512 23:45:15.119436 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_4/mlp/wi_0/kernel/m
I0512 23:45:15.119467 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_4/mlp/wi_0/kernel/v
I0512 23:45:15.119498 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_4/mlp/wi_0/kernel/v_col
I0512 23:45:15.119528 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_4/mlp/wi_0/kernel/v_row
I0512 23:45:15.119558 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_4/mlp/wi_1/kernel/m
I0512 23:45:15.119587 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_4/mlp/wi_1/kernel/v
I0512 23:45:15.119617 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_4/mlp/wi_1/kernel/v_col
I0512 23:45:15.119647 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_4/mlp/wi_1/kernel/v_row
I0512 23:45:15.119676 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_4/mlp/wo/kernel/m
I0512 23:45:15.119708 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_4/mlp/wo/kernel/v
I0512 23:45:15.119740 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_4/mlp/wo/kernel/v_col
I0512 23:45:15.119770 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_4/mlp/wo/kernel/v_row
I0512 23:45:15.119799 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_4/pre_mlp_layer_norm/scale/m
I0512 23:45:15.119828 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_4/pre_mlp_layer_norm/scale/v
I0512 23:45:15.119858 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_4/pre_mlp_layer_norm/scale/v_col
I0512 23:45:15.119887 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_4/pre_mlp_layer_norm/scale/v_row
I0512 23:45:15.119917 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_4/pre_self_attention_layer_norm/scale/m
I0512 23:45:15.119946 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_4/pre_self_attention_layer_norm/scale/v
I0512 23:45:15.119976 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_4/pre_self_attention_layer_norm/scale/v_col
I0512 23:45:15.120006 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_4/pre_self_attention_layer_norm/scale/v_row
I0512 23:45:15.120036 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_4/self_attention/key/kernel/m
I0512 23:45:15.120065 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_4/self_attention/key/kernel/v
I0512 23:45:15.120095 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_4/self_attention/key/kernel/v_col
I0512 23:45:15.120124 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_4/self_attention/key/kernel/v_row
I0512 23:45:15.120154 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_4/self_attention/out/kernel/m
I0512 23:45:15.120184 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_4/self_attention/out/kernel/v
I0512 23:45:15.120219 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_4/self_attention/out/kernel/v_col
I0512 23:45:15.120250 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_4/self_attention/out/kernel/v_row
I0512 23:45:15.120280 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_4/self_attention/query/kernel/m
I0512 23:45:15.120309 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_4/self_attention/query/kernel/v
I0512 23:45:15.120339 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_4/self_attention/query/kernel/v_col
I0512 23:45:15.120368 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_4/self_attention/query/kernel/v_row
I0512 23:45:15.120398 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_4/self_attention/value/kernel/m
I0512 23:45:15.120428 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_4/self_attention/value/kernel/v
I0512 23:45:15.120459 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_4/self_attention/value/kernel/v_col
I0512 23:45:15.120490 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_4/self_attention/value/kernel/v_row
I0512 23:45:15.120519 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/extra_mlp/wi_0/kernel/m
I0512 23:45:15.120549 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/extra_mlp/wi_0/kernel/v
I0512 23:45:15.120579 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/extra_mlp/wi_0/kernel/v_col
I0512 23:45:15.120608 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/extra_mlp/wi_0/kernel/v_row
I0512 23:45:15.120638 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/extra_mlp/wi_1/kernel/m
I0512 23:45:15.120667 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/extra_mlp/wi_1/kernel/v
I0512 23:45:15.120697 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/extra_mlp/wi_1/kernel/v_col
I0512 23:45:15.120727 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/extra_mlp/wi_1/kernel/v_row
I0512 23:45:15.120756 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/extra_mlp/wo/kernel/m
I0512 23:45:15.120786 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/extra_mlp/wo/kernel/v
I0512 23:45:15.120816 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/extra_mlp/wo/kernel/v_col
I0512 23:45:15.120845 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/extra_mlp/wo/kernel/v_row
I0512 23:45:15.120875 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/mlp/expert/wi_0/kernel/m
I0512 23:45:15.120905 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/mlp/expert/wi_0/kernel/v
I0512 23:45:15.120934 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/mlp/expert/wi_0/kernel/v_col
I0512 23:45:15.120963 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/mlp/expert/wi_0/kernel/v_row
I0512 23:45:15.120993 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/mlp/expert/wi_1/kernel/m
I0512 23:45:15.121023 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/mlp/expert/wi_1/kernel/v
I0512 23:45:15.121077 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/mlp/expert/wi_1/kernel/v_col
I0512 23:45:15.121108 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/mlp/expert/wi_1/kernel/v_row
I0512 23:45:15.121137 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/mlp/expert/wo/kernel/m
I0512 23:45:15.121167 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/mlp/expert/wo/kernel/v
I0512 23:45:15.121196 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/mlp/expert/wo/kernel/v_col
I0512 23:45:15.121232 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/mlp/expert/wo/kernel/v_row
I0512 23:45:15.121262 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/mlp/router/router_weights/w/kernel/m
I0512 23:45:15.121292 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/mlp/router/router_weights/w/kernel/v
I0512 23:45:15.121322 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/mlp/router/router_weights/w/kernel/v_col
I0512 23:45:15.121352 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/mlp/router/router_weights/w/kernel/v_row
I0512 23:45:15.121382 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/pre_extra_mlp_layer_norm/scale/m
I0512 23:45:15.121411 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/pre_extra_mlp_layer_norm/scale/v
I0512 23:45:15.121441 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/pre_extra_mlp_layer_norm/scale/v_col
I0512 23:45:15.121473 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/pre_extra_mlp_layer_norm/scale/v_row
I0512 23:45:15.121504 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/pre_mlp_layer_norm/scale/m
I0512 23:45:15.121534 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/pre_mlp_layer_norm/scale/v
I0512 23:45:15.121563 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/pre_mlp_layer_norm/scale/v_col
I0512 23:45:15.121593 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/pre_mlp_layer_norm/scale/v_row
I0512 23:45:15.121623 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/pre_self_attention_layer_norm/scale/m
I0512 23:45:15.121653 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/pre_self_attention_layer_norm/scale/v
I0512 23:45:15.121688 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/pre_self_attention_layer_norm/scale/v_col
I0512 23:45:15.121719 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/pre_self_attention_layer_norm/scale/v_row
I0512 23:45:15.121749 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/self_attention/key/kernel/m
I0512 23:45:15.121779 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/self_attention/key/kernel/v
I0512 23:45:15.121809 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/self_attention/key/kernel/v_col
I0512 23:45:15.121838 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/self_attention/key/kernel/v_row
I0512 23:45:15.121868 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/self_attention/out/kernel/m
I0512 23:45:15.121898 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/self_attention/out/kernel/v
I0512 23:45:15.121927 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/self_attention/out/kernel/v_col
I0512 23:45:15.121957 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/self_attention/out/kernel/v_row
I0512 23:45:15.121987 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/self_attention/query/kernel/m
I0512 23:45:15.122016 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/self_attention/query/kernel/v
I0512 23:45:15.122046 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/self_attention/query/kernel/v_col
I0512 23:45:15.122076 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/self_attention/query/kernel/v_row
I0512 23:45:15.122105 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/self_attention/value/kernel/m
I0512 23:45:15.122135 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/self_attention/value/kernel/v
I0512 23:45:15.122164 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/self_attention/value/kernel/v_col
I0512 23:45:15.122194 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/self_attention/value/kernel/v_row
I0512 23:45:15.122229 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_6/mlp/wi_0/kernel/m
I0512 23:45:15.122260 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_6/mlp/wi_0/kernel/v
I0512 23:45:15.122289 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_6/mlp/wi_0/kernel/v_col
I0512 23:45:15.122319 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_6/mlp/wi_0/kernel/v_row
I0512 23:45:15.122349 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_6/mlp/wi_1/kernel/m
I0512 23:45:15.122378 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_6/mlp/wi_1/kernel/v
I0512 23:45:15.122408 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_6/mlp/wi_1/kernel/v_col
I0512 23:45:15.122437 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_6/mlp/wi_1/kernel/v_row
I0512 23:45:15.122469 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_6/mlp/wo/kernel/m
I0512 23:45:15.122499 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_6/mlp/wo/kernel/v
I0512 23:45:15.122529 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_6/mlp/wo/kernel/v_col
I0512 23:45:15.122559 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_6/mlp/wo/kernel/v_row
I0512 23:45:15.122588 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_6/pre_mlp_layer_norm/scale/m
I0512 23:45:15.122618 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_6/pre_mlp_layer_norm/scale/v
I0512 23:45:15.122647 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_6/pre_mlp_layer_norm/scale/v_col
I0512 23:45:15.122677 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_6/pre_mlp_layer_norm/scale/v_row
I0512 23:45:15.122706 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_6/pre_self_attention_layer_norm/scale/m
I0512 23:45:15.122736 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_6/pre_self_attention_layer_norm/scale/v
I0512 23:45:15.122766 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_6/pre_self_attention_layer_norm/scale/v_col
I0512 23:45:15.122796 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_6/pre_self_attention_layer_norm/scale/v_row
I0512 23:45:15.122825 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_6/self_attention/key/kernel/m
I0512 23:45:15.122855 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_6/self_attention/key/kernel/v
I0512 23:45:15.122885 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_6/self_attention/key/kernel/v_col
I0512 23:45:15.122914 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_6/self_attention/key/kernel/v_row
I0512 23:45:15.122944 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_6/self_attention/out/kernel/m
I0512 23:45:15.122973 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_6/self_attention/out/kernel/v
I0512 23:45:15.123003 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_6/self_attention/out/kernel/v_col
I0512 23:45:15.123033 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_6/self_attention/out/kernel/v_row
I0512 23:45:15.123063 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_6/self_attention/query/kernel/m
I0512 23:45:15.123092 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_6/self_attention/query/kernel/v
I0512 23:45:15.123122 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_6/self_attention/query/kernel/v_col
I0512 23:45:15.123152 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_6/self_attention/query/kernel/v_row
I0512 23:45:15.123181 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_6/self_attention/value/kernel/m
I0512 23:45:15.123216 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_6/self_attention/value/kernel/v
I0512 23:45:15.123247 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_6/self_attention/value/kernel/v_col
I0512 23:45:15.123277 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_6/self_attention/value/kernel/v_row
I0512 23:45:15.123307 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_7/mlp/wi_0/kernel/m
I0512 23:45:15.123337 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_7/mlp/wi_0/kernel/v
I0512 23:45:15.123366 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_7/mlp/wi_0/kernel/v_col
I0512 23:45:15.123396 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_7/mlp/wi_0/kernel/v_row
I0512 23:45:15.123425 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_7/mlp/wi_1/kernel/m
I0512 23:45:15.123456 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_7/mlp/wi_1/kernel/v
I0512 23:45:15.123487 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_7/mlp/wi_1/kernel/v_col
I0512 23:45:15.123517 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_7/mlp/wi_1/kernel/v_row
I0512 23:45:15.123546 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_7/mlp/wo/kernel/m
I0512 23:45:15.123576 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_7/mlp/wo/kernel/v
I0512 23:45:15.123605 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_7/mlp/wo/kernel/v_col
I0512 23:45:15.123634 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_7/mlp/wo/kernel/v_row
I0512 23:45:15.123663 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_7/pre_mlp_layer_norm/scale/m
I0512 23:45:15.123693 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_7/pre_mlp_layer_norm/scale/v
I0512 23:45:15.123727 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_7/pre_mlp_layer_norm/scale/v_col
I0512 23:45:15.123757 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_7/pre_mlp_layer_norm/scale/v_row
I0512 23:45:15.123786 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_7/pre_self_attention_layer_norm/scale/m
I0512 23:45:15.123816 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_7/pre_self_attention_layer_norm/scale/v
I0512 23:45:15.123845 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_7/pre_self_attention_layer_norm/scale/v_col
I0512 23:45:15.123875 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_7/pre_self_attention_layer_norm/scale/v_row
I0512 23:45:15.123905 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_7/self_attention/key/kernel/m
I0512 23:45:15.123935 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_7/self_attention/key/kernel/v
I0512 23:45:15.123965 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_7/self_attention/key/kernel/v_col
I0512 23:45:15.123994 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_7/self_attention/key/kernel/v_row
I0512 23:45:15.124024 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_7/self_attention/out/kernel/m
I0512 23:45:15.124053 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_7/self_attention/out/kernel/v
I0512 23:45:15.124083 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_7/self_attention/out/kernel/v_col
I0512 23:45:15.124113 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_7/self_attention/out/kernel/v_row
I0512 23:45:15.124142 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_7/self_attention/query/kernel/m
I0512 23:45:15.124172 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_7/self_attention/query/kernel/v
I0512 23:45:15.124206 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_7/self_attention/query/kernel/v_col
I0512 23:45:15.124238 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_7/self_attention/query/kernel/v_row
I0512 23:45:15.124268 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_7/self_attention/value/kernel/m
I0512 23:45:15.124297 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_7/self_attention/value/kernel/v
I0512 23:45:15.124327 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_7/self_attention/value/kernel/v_col
I0512 23:45:15.124357 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_7/self_attention/value/kernel/v_row
I0512 23:45:15.124386 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_8/mlp/wi_0/kernel/m
I0512 23:45:15.124442 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_8/mlp/wi_0/kernel/v
I0512 23:45:15.124475 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_8/mlp/wi_0/kernel/v_col
I0512 23:45:15.124505 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_8/mlp/wi_0/kernel/v_row
I0512 23:45:15.124535 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_8/mlp/wi_1/kernel/m
I0512 23:45:15.124564 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_8/mlp/wi_1/kernel/v
I0512 23:45:15.124594 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_8/mlp/wi_1/kernel/v_col
I0512 23:45:15.124623 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_8/mlp/wi_1/kernel/v_row
I0512 23:45:15.124652 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_8/mlp/wo/kernel/m
I0512 23:45:15.124681 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_8/mlp/wo/kernel/v
I0512 23:45:15.124711 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_8/mlp/wo/kernel/v_col
I0512 23:45:15.124741 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_8/mlp/wo/kernel/v_row
I0512 23:45:15.124770 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_8/pre_mlp_layer_norm/scale/m
I0512 23:45:15.124800 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_8/pre_mlp_layer_norm/scale/v
I0512 23:45:15.124830 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_8/pre_mlp_layer_norm/scale/v_col
I0512 23:45:15.124859 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_8/pre_mlp_layer_norm/scale/v_row
I0512 23:45:15.124889 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_8/pre_self_attention_layer_norm/scale/m
I0512 23:45:15.124918 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_8/pre_self_attention_layer_norm/scale/v
I0512 23:45:15.124948 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_8/pre_self_attention_layer_norm/scale/v_col
I0512 23:45:15.124978 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_8/pre_self_attention_layer_norm/scale/v_row
I0512 23:45:15.125008 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_8/self_attention/key/kernel/m
I0512 23:45:15.125060 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_8/self_attention/key/kernel/v
I0512 23:45:15.125094 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_8/self_attention/key/kernel/v_col
I0512 23:45:15.125124 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_8/self_attention/key/kernel/v_row
I0512 23:45:15.125154 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_8/self_attention/out/kernel/m
I0512 23:45:15.125184 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_8/self_attention/out/kernel/v
I0512 23:45:15.125220 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_8/self_attention/out/kernel/v_col
I0512 23:45:15.125251 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_8/self_attention/out/kernel/v_row
I0512 23:45:15.125281 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_8/self_attention/query/kernel/m
I0512 23:45:15.125310 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_8/self_attention/query/kernel/v
I0512 23:45:15.125340 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_8/self_attention/query/kernel/v_col
I0512 23:45:15.125369 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_8/self_attention/query/kernel/v_row
I0512 23:45:15.125399 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_8/self_attention/value/kernel/m
I0512 23:45:15.125428 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_8/self_attention/value/kernel/v
I0512 23:45:15.125459 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_8/self_attention/value/kernel/v_col
I0512 23:45:15.125490 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_8/self_attention/value/kernel/v_row
I0512 23:45:15.125520 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_9/mlp/wi_0/kernel/m
I0512 23:45:15.125550 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_9/mlp/wi_0/kernel/v
I0512 23:45:15.125580 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_9/mlp/wi_0/kernel/v_col
I0512 23:45:15.125610 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_9/mlp/wi_0/kernel/v_row
I0512 23:45:15.125639 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_9/mlp/wi_1/kernel/m
I0512 23:45:15.125668 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_9/mlp/wi_1/kernel/v
I0512 23:45:15.125698 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_9/mlp/wi_1/kernel/v_col
I0512 23:45:15.125727 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_9/mlp/wi_1/kernel/v_row
I0512 23:45:15.125756 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_9/mlp/wo/kernel/m
I0512 23:45:15.125786 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_9/mlp/wo/kernel/v
I0512 23:45:15.125815 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_9/mlp/wo/kernel/v_col
I0512 23:45:15.125844 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_9/mlp/wo/kernel/v_row
I0512 23:45:15.125874 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_9/pre_mlp_layer_norm/scale/m
I0512 23:45:15.125903 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_9/pre_mlp_layer_norm/scale/v
I0512 23:45:15.125933 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_9/pre_mlp_layer_norm/scale/v_col
I0512 23:45:15.125962 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_9/pre_mlp_layer_norm/scale/v_row
I0512 23:45:15.125992 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_9/pre_self_attention_layer_norm/scale/m
I0512 23:45:15.126021 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_9/pre_self_attention_layer_norm/scale/v
I0512 23:45:15.126051 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_9/pre_self_attention_layer_norm/scale/v_col
I0512 23:45:15.126081 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_9/pre_self_attention_layer_norm/scale/v_row
I0512 23:45:15.126111 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_9/self_attention/key/kernel/m
I0512 23:45:15.126140 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_9/self_attention/key/kernel/v
I0512 23:45:15.126170 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_9/self_attention/key/kernel/v_col
I0512 23:45:15.126204 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_9/self_attention/key/kernel/v_row
I0512 23:45:15.126236 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_9/self_attention/out/kernel/m
I0512 23:45:15.126266 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_9/self_attention/out/kernel/v
I0512 23:45:15.126296 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_9/self_attention/out/kernel/v_col
I0512 23:45:15.126326 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_9/self_attention/out/kernel/v_row
I0512 23:45:15.126355 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_9/self_attention/query/kernel/m
I0512 23:45:15.126384 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_9/self_attention/query/kernel/v
I0512 23:45:15.126414 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_9/self_attention/query/kernel/v_col
I0512 23:45:15.126444 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_9/self_attention/query/kernel/v_row
I0512 23:45:15.126475 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_9/self_attention/value/kernel/m
I0512 23:45:15.126505 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_9/self_attention/value/kernel/v
I0512 23:45:15.126535 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_9/self_attention/value/kernel/v_col
I0512 23:45:15.126565 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_9/self_attention/value/kernel/v_row
I0512 23:45:15.126594 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/logits_dense/kernel/m
I0512 23:45:15.126624 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/logits_dense/kernel/v
I0512 23:45:15.142624 140260698863616 checkpoints.py:1054] Restoring from checkpoint: gs://rosinality-tpu-bucket/openmoe_8b_ul2/training/checkpoint_10000/checkpoint
I0512 23:45:15.126653 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/logits_dense/kernel/v_col
I0512 23:45:15.126683 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/logits_dense/kernel/v_row
I0512 23:45:15.126716 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/token_embedder/embedding/m
I0512 23:45:15.126747 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/token_embedder/embedding/v
I0512 23:45:15.126777 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/token_embedder/embedding/v_col
I0512 23:45:15.126806 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/param_states/token_embedder/embedding/v_row
I0512 23:45:15.126836 140180528838656 checkpoints.py:1100] Restoring key from ckpt: state/step
I0512 23:45:15.126866 140180528838656 checkpoints.py:1100] Restoring key from ckpt: target/decoder/decoder_norm/scale
I0512 23:45:15.126896 140180528838656 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_0/mlp/wi_0/kernel
I0512 23:45:15.126926 140180528838656 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_0/mlp/wi_1/kernel
I0512 23:45:15.126955 140180528838656 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_0/mlp/wo/kernel
I0512 23:45:15.126985 140180528838656 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_0/pre_mlp_layer_norm/scale
I0512 23:45:15.127014 140180528838656 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_0/pre_self_attention_layer_norm/scale
I0512 23:45:15.127044 140180528838656 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_0/self_attention/key/kernel
I0512 23:45:15.127073 140180528838656 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_0/self_attention/out/kernel
I0512 23:45:15.127103 140180528838656 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_0/self_attention/query/kernel
I0512 23:45:15.127132 140180528838656 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_0/self_attention/value/kernel
I0512 23:45:15.127162 140180528838656 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_1/mlp/wi_0/kernel
I0512 23:45:15.127191 140180528838656 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_1/mlp/wi_1/kernel
I0512 23:45:15.127227 140180528838656 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_1/mlp/wo/kernel
I0512 23:45:15.127257 140180528838656 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_1/pre_mlp_layer_norm/scale
I0512 23:45:15.127286 140180528838656 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_1/pre_self_attention_layer_norm/scale
I0512 23:45:15.127316 140180528838656 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_1/self_attention/key/kernel
I0512 23:45:15.127346 140180528838656 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_1/self_attention/out/kernel
I0512 23:45:15.127375 140180528838656 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_1/self_attention/query/kernel
I0512 23:45:15.127404 140180528838656 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_1/self_attention/value/kernel
I0512 23:45:15.127434 140180528838656 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_10/mlp/wi_0/kernel
I0512 23:45:15.127465 140180528838656 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_10/mlp/wi_1/kernel
I0512 23:45:15.127496 140180528838656 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_10/mlp/wo/kernel
I0512 23:45:15.127525 140180528838656 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_10/pre_mlp_layer_norm/scale
I0512 23:45:15.127555 140180528838656 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_10/pre_self_attention_layer_norm/scale
I0512 23:45:15.127584 140180528838656 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_10/self_attention/key/kernel
I0512 23:45:15.127614 140180528838656 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_10/self_attention/out/kernel
I0512 23:45:15.127643 140180528838656 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_10/self_attention/query/kernel
I0512 23:45:15.127673 140180528838656 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_10/self_attention/value/kernel
I0512 23:45:15.127702 140180528838656 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_11/extra_mlp/wi_0/kernel
I0512 23:45:15.127732 140180528838656 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_11/extra_mlp/wi_1/kernel
I0512 23:45:15.127761 140180528838656 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_11/extra_mlp/wo/kernel
I0512 23:45:15.127791 140180528838656 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_11/mlp/expert/wi_0/kernel
I0512 23:45:15.127820 140180528838656 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_11/mlp/expert/wi_1/kernel
I0512 23:45:15.127849 140180528838656 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_11/mlp/expert/wo/kernel
I0512 23:45:15.127879 140180528838656 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_11/mlp/router/router_weights/w/kernel
I0512 23:45:15.127908 140180528838656 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_11/pre_extra_mlp_layer_norm/scale
I0512 23:45:15.127938 140180528838656 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_11/pre_mlp_layer_norm/scale
I0512 23:45:15.127967 140180528838656 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_11/pre_self_attention_layer_norm/scale
I0512 23:45:15.127996 140180528838656 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_11/self_attention/key/kernel
I0512 23:45:15.128026 140180528838656 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_11/self_attention/out/kernel
I0512 23:45:15.128055 140180528838656 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_11/self_attention/query/kernel
I0512 23:45:15.128085 140180528838656 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_11/self_attention/value/kernel
I0512 23:45:15.128114 140180528838656 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_12/mlp/wi_0/kernel
I0512 23:45:15.128144 140180528838656 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_12/mlp/wi_1/kernel
I0512 23:45:15.128173 140180528838656 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_12/mlp/wo/kernel
I0512 23:45:15.128208 140180528838656 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_12/pre_mlp_layer_norm/scale
I0512 23:45:15.128239 140180528838656 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_12/pre_self_attention_layer_norm/scale
I0512 23:45:15.128269 140180528838656 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_12/self_attention/key/kernel
I0512 23:45:15.128298 140180528838656 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_12/self_attention/out/kernel
I0512 23:45:15.128328 140180528838656 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_12/self_attention/query/kernel
I0512 23:45:15.128357 140180528838656 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_12/self_attention/value/kernel
I0512 23:45:15.128386 140180528838656 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_13/mlp/wi_0/kernel
I0512 23:45:15.128416 140180528838656 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_13/mlp/wi_1/kernel
I0512 23:45:15.128445 140180528838656 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_13/mlp/wo/kernel
I0512 23:45:15.128476 140180528838656 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_13/pre_mlp_layer_norm/scale
I0512 23:45:15.128507 140180528838656 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_13/pre_self_attention_layer_norm/scale
I0512 23:45:15.128536 140180528838656 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_13/self_attention/key/kernel
I0512 23:45:15.128566 140180528838656 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_13/self_attention/out/kernel
I0512 23:45:15.128596 140180528838656 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_13/self_attention/query/kernel
I0512 23:45:15.128625 140180528838656 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_13/self_attention/value/kernel
I0512 23:45:15.128654 140180528838656 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_14/mlp/wi_0/kernel
I0512 23:45:15.128684 140180528838656 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_14/mlp/wi_1/kernel
I0512 23:45:15.128713 140180528838656 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_14/mlp/wo/kernel
I0512 23:45:15.128743 140180528838656 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_14/pre_mlp_layer_norm/scale
I0512 23:45:15.128772 140180528838656 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_14/pre_self_attention_layer_norm/scale
I0512 23:45:15.128802 140180528838656 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_14/self_attention/key/kernel
I0512 23:45:15.128832 140180528838656 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_14/self_attention/out/kernel
I0512 23:45:15.128861 140180528838656 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_14/self_attention/query/kernel
I0512 23:45:15.128891 140180528838656 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_14/self_attention/value/kernel
I0512 23:45:15.128920 140180528838656 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_15/mlp/wi_0/kernel
I0512 23:45:15.128949 140180528838656 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_15/mlp/wi_1/kernel
I0512 23:45:15.128979 140180528838656 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_15/mlp/wo/kernel
I0512 23:45:15.129009 140180528838656 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_15/pre_mlp_layer_norm/scale
I0512 23:45:15.129057 140180528838656 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_15/pre_self_attention_layer_norm/scale
I0512 23:45:15.129091 140180528838656 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_15/self_attention/key/kernel
I0512 23:45:15.129121 140180528838656 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_15/self_attention/out/kernel
I0512 23:45:15.129151 140180528838656 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_15/self_attention/query/kernel
I0512 23:45:15.129181 140180528838656 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_15/self_attention/value/kernel
I0512 23:45:15.129215 140180528838656 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_16/mlp/wi_0/kernel
I0512 23:45:15.129246 140180528838656 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_16/mlp/wi_1/kernel
I0512 23:45:15.129276 140180528838656 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_16/mlp/wo/kernel
I0512 23:45:15.129306 140180528838656 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_16/pre_mlp_layer_norm/scale
I0512 23:45:15.129335 140180528838656 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_16/pre_self_attention_layer_norm/scale
I0512 23:45:15.129365 140180528838656 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_16/self_attention/key/kernel
I0512 23:45:15.129395 140180528838656 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_16/self_attention/out/kernel
I0512 23:45:15.129424 140180528838656 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_16/self_attention/query/kernel
I0512 23:45:15.129455 140180528838656 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_16/self_attention/value/kernel
I0512 23:45:15.129486 140180528838656 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_17/extra_mlp/wi_0/kernel
I0512 23:45:15.129516 140180528838656 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_17/extra_mlp/wi_1/kernel
I0512 23:45:15.129545 140180528838656 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_17/extra_mlp/wo/kernel
I0512 23:45:15.129575 140180528838656 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_17/mlp/expert/wi_0/kernel
I0512 23:45:15.129605 140180528838656 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_17/mlp/expert/wi_1/kernel
I0512 23:45:15.129634 140180528838656 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_17/mlp/expert/wo/kernel
I0512 23:45:15.129663 140180528838656 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_17/mlp/router/router_weights/w/kernel
I0512 23:45:15.129694 140180528838656 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_17/pre_extra_mlp_layer_norm/scale
I0512 23:45:15.129728 140180528838656 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_17/pre_mlp_layer_norm/scale
I0512 23:45:15.129757 140180528838656 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_17/pre_self_attention_layer_norm/scale
I0512 23:45:15.129787 140180528838656 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_17/self_attention/key/kernel
I0512 23:45:15.129817 140180528838656 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_17/self_attention/out/kernel
I0512 23:45:15.129847 140180528838656 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_17/self_attention/query/kernel
I0512 23:45:15.129876 140180528838656 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_17/self_attention/value/kernel
I0512 23:45:15.129905 140180528838656 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_18/mlp/wi_0/kernel
I0512 23:45:15.129935 140180528838656 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_18/mlp/wi_1/kernel
I0512 23:45:15.129964 140180528838656 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_18/mlp/wo/kernel
I0512 23:45:15.129993 140180528838656 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_18/pre_mlp_layer_norm/scale
I0512 23:45:15.130023 140180528838656 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_18/pre_self_attention_layer_norm/scale
I0512 23:45:15.130052 140180528838656 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_18/self_attention/key/kernel
I0512 23:45:15.130082 140180528838656 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_18/self_attention/out/kernel
I0512 23:45:15.130111 140180528838656 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_18/self_attention/query/kernel
I0512 23:45:15.130141 140180528838656 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_18/self_attention/value/kernel
I0512 23:45:15.130170 140180528838656 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_19/mlp/wi_0/kernel
I0512 23:45:15.130205 140180528838656 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_19/mlp/wi_1/kernel
I0512 23:45:15.130236 140180528838656 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_19/mlp/wo/kernel
I0512 23:45:15.130266 140180528838656 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_19/pre_mlp_layer_norm/scale
I0512 23:45:15.130295 140180528838656 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_19/pre_self_attention_layer_norm/scale
I0512 23:45:15.130325 140180528838656 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_19/self_attention/key/kernel
I0512 23:45:15.130354 140180528838656 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_19/self_attention/out/kernel
I0512 23:45:15.130384 140180528838656 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_19/self_attention/query/kernel
I0512 23:45:15.130413 140180528838656 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_19/self_attention/value/kernel
I0512 23:45:15.130443 140180528838656 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_2/mlp/wi_0/kernel
I0512 23:45:15.130474 140180528838656 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_2/mlp/wi_1/kernel
I0512 23:45:15.130504 140180528838656 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_2/mlp/wo/kernel
I0512 23:45:15.130534 140180528838656 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_2/pre_mlp_layer_norm/scale
I0512 23:45:15.130563 140180528838656 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_2/pre_self_attention_layer_norm/scale
I0512 23:45:15.130593 140180528838656 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_2/self_attention/key/kernel
I0512 23:45:15.130622 140180528838656 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_2/self_attention/out/kernel
I0512 23:45:15.130652 140180528838656 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_2/self_attention/query/kernel
I0512 23:45:15.130681 140180528838656 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_2/self_attention/value/kernel
I0512 23:45:15.130711 140180528838656 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_20/mlp/wi_0/kernel
I0512 23:45:15.130740 140180528838656 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_20/mlp/wi_1/kernel
I0512 23:45:15.130770 140180528838656 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_20/mlp/wo/kernel
I0512 23:45:15.130799 140180528838656 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_20/pre_mlp_layer_norm/scale
I0512 23:45:15.130829 140180528838656 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_20/pre_self_attention_layer_norm/scale
I0512 23:45:15.130858 140180528838656 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_20/self_attention/key/kernel
I0512 23:45:15.130888 140180528838656 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_20/self_attention/out/kernel
I0512 23:45:15.130917 140180528838656 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_20/self_attention/query/kernel
I0512 23:45:15.130947 140180528838656 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_20/self_attention/value/kernel
I0512 23:45:15.130976 140180528838656 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_21/mlp/wi_0/kernel
I0512 23:45:15.131005 140180528838656 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_21/mlp/wi_1/kernel
I0512 23:45:15.131035 140180528838656 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_21/mlp/wo/kernel
I0512 23:45:15.131064 140180528838656 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_21/pre_mlp_layer_norm/scale
I0512 23:45:15.131094 140180528838656 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_21/pre_self_attention_layer_norm/scale
I0512 23:45:15.131123 140180528838656 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_21/self_attention/key/kernel
I0512 23:45:15.131153 140180528838656 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_21/self_attention/out/kernel
I0512 23:45:15.131182 140180528838656 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_21/self_attention/query/kernel
I0512 23:45:15.131217 140180528838656 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_21/self_attention/value/kernel
I0512 23:45:15.131248 140180528838656 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_22/mlp/wi_0/kernel
I0512 23:45:15.131277 140180528838656 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_22/mlp/wi_1/kernel
I0512 23:45:15.131307 140180528838656 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_22/mlp/wo/kernel
I0512 23:45:15.131336 140180528838656 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_22/pre_mlp_layer_norm/scale
I0512 23:45:15.131366 140180528838656 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_22/pre_self_attention_layer_norm/scale
I0512 23:45:15.131395 140180528838656 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_22/self_attention/key/kernel
I0512 23:45:15.131425 140180528838656 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_22/self_attention/out/kernel
I0512 23:45:15.131455 140180528838656 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_22/self_attention/query/kernel
I0512 23:45:15.131486 140180528838656 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_22/self_attention/value/kernel
I0512 23:45:15.131516 140180528838656 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_23/extra_mlp/wi_0/kernel
I0512 23:45:15.131546 140180528838656 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_23/extra_mlp/wi_1/kernel
I0512 23:45:15.131575 140180528838656 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_23/extra_mlp/wo/kernel
I0512 23:45:15.131604 140180528838656 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_23/mlp/expert/wi_0/kernel
I0512 23:45:15.131633 140180528838656 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_23/mlp/expert/wi_1/kernel
I0512 23:45:15.131662 140180528838656 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_23/mlp/expert/wo/kernel
I0512 23:45:15.131692 140180528838656 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_23/mlp/router/router_weights/w/kernel
I0512 23:45:15.131721 140180528838656 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_23/pre_extra_mlp_layer_norm/scale
I0512 23:45:15.131751 140180528838656 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_23/pre_mlp_layer_norm/scale
I0512 23:45:15.131780 140180528838656 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_23/pre_self_attention_layer_norm/scale
I0512 23:45:15.131810 140180528838656 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_23/self_attention/key/kernel
I0512 23:45:15.131839 140180528838656 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_23/self_attention/out/kernel
I0512 23:45:15.131869 140180528838656 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_23/self_attention/query/kernel
I0512 23:45:15.131898 140180528838656 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_23/self_attention/value/kernel
I0512 23:45:15.131927 140180528838656 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_3/mlp/wi_0/kernel
I0512 23:45:15.131957 140180528838656 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_3/mlp/wi_1/kernel
I0512 23:45:15.131986 140180528838656 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_3/mlp/wo/kernel
I0512 23:45:15.132015 140180528838656 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_3/pre_mlp_layer_norm/scale
I0512 23:45:15.132045 140180528838656 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_3/pre_self_attention_layer_norm/scale
I0512 23:45:15.132074 140180528838656 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_3/self_attention/key/kernel
I0512 23:45:15.132103 140180528838656 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_3/self_attention/out/kernel
I0512 23:45:15.132133 140180528838656 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_3/self_attention/query/kernel
I0512 23:45:15.132162 140180528838656 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_3/self_attention/value/kernel
I0512 23:45:15.132191 140180528838656 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_4/mlp/wi_0/kernel
I0512 23:45:15.132227 140180528838656 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_4/mlp/wi_1/kernel
I0512 23:45:15.132258 140180528838656 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_4/mlp/wo/kernel
I0512 23:45:15.132288 140180528838656 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_4/pre_mlp_layer_norm/scale
I0512 23:45:15.132317 140180528838656 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_4/pre_self_attention_layer_norm/scale
I0512 23:45:15.132346 140180528838656 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_4/self_attention/key/kernel
I0512 23:45:15.132375 140180528838656 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_4/self_attention/out/kernel
I0512 23:45:15.132405 140180528838656 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_4/self_attention/query/kernel
I0512 23:45:15.132434 140180528838656 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_4/self_attention/value/kernel
I0512 23:45:15.132465 140180528838656 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_5/extra_mlp/wi_0/kernel
I0512 23:45:15.132496 140180528838656 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_5/extra_mlp/wi_1/kernel
I0512 23:45:15.132526 140180528838656 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_5/extra_mlp/wo/kernel
I0512 23:45:15.132555 140180528838656 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_5/mlp/expert/wi_0/kernel
I0512 23:45:15.132585 140180528838656 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_5/mlp/expert/wi_1/kernel
I0512 23:45:15.132614 140180528838656 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_5/mlp/expert/wo/kernel
I0512 23:45:15.132644 140180528838656 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_5/mlp/router/router_weights/w/kernel
I0512 23:45:15.132673 140180528838656 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_5/pre_extra_mlp_layer_norm/scale
I0512 23:45:15.132705 140180528838656 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_5/pre_mlp_layer_norm/scale
I0512 23:45:15.132737 140180528838656 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_5/pre_self_attention_layer_norm/scale
I0512 23:45:15.132767 140180528838656 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_5/self_attention/key/kernel
I0512 23:45:15.132796 140180528838656 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_5/self_attention/out/kernel
I0512 23:45:15.132825 140180528838656 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_5/self_attention/query/kernel
I0512 23:45:15.132854 140180528838656 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_5/self_attention/value/kernel
I0512 23:45:15.132884 140180528838656 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_6/mlp/wi_0/kernel
I0512 23:45:15.132913 140180528838656 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_6/mlp/wi_1/kernel
I0512 23:45:15.132943 140180528838656 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_6/mlp/wo/kernel
I0512 23:45:15.132972 140180528838656 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_6/pre_mlp_layer_norm/scale
I0512 23:45:15.133002 140180528838656 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_6/pre_self_attention_layer_norm/scale
I0512 23:45:15.133060 140180528838656 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_6/self_attention/key/kernel
I0512 23:45:15.133096 140180528838656 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_6/self_attention/out/kernel
I0512 23:45:15.133127 140180528838656 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_6/self_attention/query/kernel
I0512 23:45:15.133157 140180528838656 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_6/self_attention/value/kernel
I0512 23:45:15.133186 140180528838656 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_7/mlp/wi_0/kernel
I0512 23:45:15.133222 140180528838656 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_7/mlp/wi_1/kernel
I0512 23:45:15.133252 140180528838656 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_7/mlp/wo/kernel
I0512 23:45:15.133282 140180528838656 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_7/pre_mlp_layer_norm/scale
I0512 23:45:15.133311 140180528838656 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_7/pre_self_attention_layer_norm/scale
I0512 23:45:15.133341 140180528838656 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_7/self_attention/key/kernel
I0512 23:45:15.133370 140180528838656 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_7/self_attention/out/kernel
I0512 23:45:15.133400 140180528838656 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_7/self_attention/query/kernel
I0512 23:45:15.133430 140180528838656 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_7/self_attention/value/kernel
I0512 23:45:15.133460 140180528838656 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_8/mlp/wi_0/kernel
I0512 23:45:15.133491 140180528838656 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_8/mlp/wi_1/kernel
I0512 23:45:15.133522 140180528838656 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_8/mlp/wo/kernel
I0512 23:45:15.133551 140180528838656 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_8/pre_mlp_layer_norm/scale
I0512 23:45:15.133581 140180528838656 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_8/pre_self_attention_layer_norm/scale
I0512 23:45:15.133610 140180528838656 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_8/self_attention/key/kernel
I0512 23:45:15.133639 140180528838656 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_8/self_attention/out/kernel
I0512 23:45:15.133669 140180528838656 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_8/self_attention/query/kernel
I0512 23:45:15.133698 140180528838656 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_8/self_attention/value/kernel
I0512 23:45:15.133728 140180528838656 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_9/mlp/wi_0/kernel
I0512 23:45:15.133757 140180528838656 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_9/mlp/wi_1/kernel
I0512 23:45:15.133787 140180528838656 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_9/mlp/wo/kernel
I0512 23:45:15.133816 140180528838656 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_9/pre_mlp_layer_norm/scale
I0512 23:45:15.133845 140180528838656 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_9/pre_self_attention_layer_norm/scale
I0512 23:45:15.133874 140180528838656 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_9/self_attention/key/kernel
I0512 23:45:15.133903 140180528838656 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_9/self_attention/out/kernel
I0512 23:45:15.133933 140180528838656 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_9/self_attention/query/kernel
I0512 23:45:15.133963 140180528838656 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_9/self_attention/value/kernel
I0512 23:45:15.133992 140180528838656 checkpoints.py:1100] Restoring key from ckpt: target/decoder/logits_dense/kernel
I0512 23:45:15.134022 140180528838656 checkpoints.py:1100] Restoring key from ckpt: target/token_embedder/embedding
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
I0000 00:00:1715557515.166490  369400 gcs_resource.cc:109] Using default AdmissionQueue with limit 32
I0000 00:00:1715557515.173917  371559 google_auth_provider.cc:180] Running on GCE, using service account 1081612546678-compute@developer.gserviceaccount.com
I0512 23:45:15.304644 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/decoder_norm/scale/m
I0512 23:45:15.304883 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/decoder_norm/scale/v
I0512 23:45:15.304923 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/decoder_norm/scale/v_col
I0512 23:45:15.304956 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/decoder_norm/scale/v_row
I0512 23:45:15.304987 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_0/mlp/wi_0/kernel/m
I0512 23:45:15.305019 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_0/mlp/wi_0/kernel/v
I0512 23:45:15.305051 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_0/mlp/wi_0/kernel/v_col
I0512 23:45:15.305083 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_0/mlp/wi_0/kernel/v_row
I0512 23:45:15.305115 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_0/mlp/wi_1/kernel/m
I0512 23:45:15.305147 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_0/mlp/wi_1/kernel/v
I0512 23:45:15.305179 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_0/mlp/wi_1/kernel/v_col
I0512 23:45:15.305210 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_0/mlp/wi_1/kernel/v_row
I0512 23:45:15.305248 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_0/mlp/wo/kernel/m
I0512 23:45:15.305282 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_0/mlp/wo/kernel/v
I0512 23:45:15.305313 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_0/mlp/wo/kernel/v_col
I0512 23:45:15.305345 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_0/mlp/wo/kernel/v_row
I0512 23:45:15.305377 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_0/pre_mlp_layer_norm/scale/m
I0512 23:45:15.305409 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_0/pre_mlp_layer_norm/scale/v
I0512 23:45:15.305441 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_0/pre_mlp_layer_norm/scale/v_col
I0512 23:45:15.305473 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_0/pre_mlp_layer_norm/scale/v_row
I0512 23:45:15.305507 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_0/pre_self_attention_layer_norm/scale/m
I0512 23:45:15.305540 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_0/pre_self_attention_layer_norm/scale/v
I0512 23:45:15.305572 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_0/pre_self_attention_layer_norm/scale/v_col
I0512 23:45:15.305604 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_0/pre_self_attention_layer_norm/scale/v_row
I0512 23:45:15.305636 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_0/self_attention/key/kernel/m
I0512 23:45:15.305667 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_0/self_attention/key/kernel/v
I0512 23:45:15.305699 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_0/self_attention/key/kernel/v_col
I0512 23:45:15.305731 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_0/self_attention/key/kernel/v_row
I0512 23:45:15.305762 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_0/self_attention/out/kernel/m
I0512 23:45:15.305794 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_0/self_attention/out/kernel/v
I0512 23:45:15.305826 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_0/self_attention/out/kernel/v_col
I0512 23:45:15.305858 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_0/self_attention/out/kernel/v_row
I0512 23:45:15.305889 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_0/self_attention/query/kernel/m
I0512 23:45:15.305920 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_0/self_attention/query/kernel/v
I0512 23:45:15.305952 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_0/self_attention/query/kernel/v_col
I0512 23:45:15.305983 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_0/self_attention/query/kernel/v_row
I0512 23:45:15.306014 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_0/self_attention/value/kernel/m
I0512 23:45:15.306046 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_0/self_attention/value/kernel/v
I0512 23:45:15.306077 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_0/self_attention/value/kernel/v_col
I0512 23:45:15.306108 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_0/self_attention/value/kernel/v_row
I0512 23:45:15.306139 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_1/mlp/wi_0/kernel/m
I0512 23:45:15.306171 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_1/mlp/wi_0/kernel/v
I0512 23:45:15.306203 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_1/mlp/wi_0/kernel/v_col
I0512 23:45:15.306239 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_1/mlp/wi_0/kernel/v_row
I0512 23:45:15.306273 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_1/mlp/wi_1/kernel/m
I0512 23:45:15.306304 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_1/mlp/wi_1/kernel/v
I0512 23:45:15.306337 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_1/mlp/wi_1/kernel/v_col
I0512 23:45:15.306368 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_1/mlp/wi_1/kernel/v_row
I0512 23:45:15.306400 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_1/mlp/wo/kernel/m
I0512 23:45:15.306431 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_1/mlp/wo/kernel/v
I0512 23:45:15.306463 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_1/mlp/wo/kernel/v_col
I0512 23:45:15.306496 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_1/mlp/wo/kernel/v_row
I0512 23:45:15.306530 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_1/pre_mlp_layer_norm/scale/m
I0512 23:45:15.306561 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_1/pre_mlp_layer_norm/scale/v
I0512 23:45:15.306593 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_1/pre_mlp_layer_norm/scale/v_col
I0512 23:45:15.306625 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_1/pre_mlp_layer_norm/scale/v_row
I0512 23:45:15.306657 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_1/pre_self_attention_layer_norm/scale/m
I0512 23:45:15.306688 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_1/pre_self_attention_layer_norm/scale/v
I0512 23:45:15.306720 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_1/pre_self_attention_layer_norm/scale/v_col
I0512 23:45:15.306752 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_1/pre_self_attention_layer_norm/scale/v_row
I0512 23:45:15.306783 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_1/self_attention/key/kernel/m
I0512 23:45:15.306814 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_1/self_attention/key/kernel/v
I0512 23:45:15.306846 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_1/self_attention/key/kernel/v_col
I0512 23:45:15.306877 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_1/self_attention/key/kernel/v_row
I0512 23:45:15.306908 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_1/self_attention/out/kernel/m
I0512 23:45:15.306939 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_1/self_attention/out/kernel/v
I0512 23:45:15.306970 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_1/self_attention/out/kernel/v_col
I0512 23:45:15.307001 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_1/self_attention/out/kernel/v_row
I0512 23:45:15.307032 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_1/self_attention/query/kernel/m
I0512 23:45:15.307063 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_1/self_attention/query/kernel/v
I0512 23:45:15.307094 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_1/self_attention/query/kernel/v_col
I0512 23:45:15.307125 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_1/self_attention/query/kernel/v_row
I0512 23:45:15.307157 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_1/self_attention/value/kernel/m
I0512 23:45:15.307188 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_1/self_attention/value/kernel/v
I0512 23:45:15.307219 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_1/self_attention/value/kernel/v_col
I0512 23:45:15.307257 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_1/self_attention/value/kernel/v_row
I0512 23:45:15.307289 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_10/mlp/wi_0/kernel/m
I0512 23:45:15.307320 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_10/mlp/wi_0/kernel/v
I0512 23:45:15.307351 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_10/mlp/wi_0/kernel/v_col
I0512 23:45:15.307387 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_10/mlp/wi_0/kernel/v_row
I0512 23:45:15.307419 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_10/mlp/wi_1/kernel/m
I0512 23:45:15.307450 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_10/mlp/wi_1/kernel/v
I0512 23:45:15.307481 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_10/mlp/wi_1/kernel/v_col
I0512 23:45:15.307515 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_10/mlp/wi_1/kernel/v_row
I0512 23:45:15.307547 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_10/mlp/wo/kernel/m
I0512 23:45:15.307578 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_10/mlp/wo/kernel/v
I0512 23:45:15.307609 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_10/mlp/wo/kernel/v_col
I0512 23:45:15.307641 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_10/mlp/wo/kernel/v_row
I0512 23:45:15.307672 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_10/pre_mlp_layer_norm/scale/m
I0512 23:45:15.307703 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_10/pre_mlp_layer_norm/scale/v
I0512 23:45:15.307734 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_10/pre_mlp_layer_norm/scale/v_col
I0512 23:45:15.307765 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_10/pre_mlp_layer_norm/scale/v_row
I0512 23:45:15.307795 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_10/pre_self_attention_layer_norm/scale/m
I0512 23:45:15.307826 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_10/pre_self_attention_layer_norm/scale/v
I0512 23:45:15.307856 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_10/pre_self_attention_layer_norm/scale/v_col
I0512 23:45:15.307887 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_10/pre_self_attention_layer_norm/scale/v_row
I0512 23:45:15.307918 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_10/self_attention/key/kernel/m
I0512 23:45:15.307949 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_10/self_attention/key/kernel/v
I0512 23:45:15.307979 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_10/self_attention/key/kernel/v_col
I0512 23:45:15.308010 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_10/self_attention/key/kernel/v_row
I0512 23:45:15.308040 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_10/self_attention/out/kernel/m
I0512 23:45:15.308071 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_10/self_attention/out/kernel/v
I0512 23:45:15.308101 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_10/self_attention/out/kernel/v_col
I0512 23:45:15.308132 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_10/self_attention/out/kernel/v_row
I0512 23:45:15.308162 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_10/self_attention/query/kernel/m
I0512 23:45:15.308192 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_10/self_attention/query/kernel/v
I0512 23:45:15.308223 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_10/self_attention/query/kernel/v_col
I0512 23:45:15.308260 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_10/self_attention/query/kernel/v_row
I0512 23:45:15.308312 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_10/self_attention/value/kernel/m
I0512 23:45:15.308349 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_10/self_attention/value/kernel/v
I0512 23:45:15.308381 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_10/self_attention/value/kernel/v_col
I0512 23:45:15.308412 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_10/self_attention/value/kernel/v_row
I0512 23:45:15.308442 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/extra_mlp/wi_0/kernel/m
I0512 23:45:15.308473 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/extra_mlp/wi_0/kernel/v
I0512 23:45:15.308506 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/extra_mlp/wi_0/kernel/v_col
I0512 23:45:15.308538 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/extra_mlp/wi_0/kernel/v_row
I0512 23:45:15.308568 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/extra_mlp/wi_1/kernel/m
I0512 23:45:15.308599 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/extra_mlp/wi_1/kernel/v
I0512 23:45:15.308629 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/extra_mlp/wi_1/kernel/v_col
I0512 23:45:15.308660 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/extra_mlp/wi_1/kernel/v_row
I0512 23:45:15.308690 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/extra_mlp/wo/kernel/m
I0512 23:45:15.308721 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/extra_mlp/wo/kernel/v
I0512 23:45:15.308752 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/extra_mlp/wo/kernel/v_col
I0512 23:45:15.308782 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/extra_mlp/wo/kernel/v_row
I0512 23:45:15.308813 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/mlp/expert/wi_0/kernel/m
I0512 23:45:15.308844 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/mlp/expert/wi_0/kernel/v
I0512 23:45:15.308874 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/mlp/expert/wi_0/kernel/v_col
I0512 23:45:15.308904 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/mlp/expert/wi_0/kernel/v_row
I0512 23:45:15.308935 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/mlp/expert/wi_1/kernel/m
I0512 23:45:15.308965 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/mlp/expert/wi_1/kernel/v
I0512 23:45:15.308996 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/mlp/expert/wi_1/kernel/v_col
I0512 23:45:15.309026 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/mlp/expert/wi_1/kernel/v_row
I0512 23:45:15.309056 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/mlp/expert/wo/kernel/m
I0512 23:45:15.309087 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/mlp/expert/wo/kernel/v
I0512 23:45:15.309118 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/mlp/expert/wo/kernel/v_col
I0512 23:45:15.309148 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/mlp/expert/wo/kernel/v_row
I0512 23:45:15.309179 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/mlp/router/router_weights/w/kernel/m
I0512 23:45:15.309209 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/mlp/router/router_weights/w/kernel/v
I0512 23:45:15.309245 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/mlp/router/router_weights/w/kernel/v_col
I0512 23:45:15.309278 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/mlp/router/router_weights/w/kernel/v_row
I0512 23:45:15.309309 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/pre_extra_mlp_layer_norm/scale/m
I0512 23:45:15.309340 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/pre_extra_mlp_layer_norm/scale/v
I0512 23:45:15.309370 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/pre_extra_mlp_layer_norm/scale/v_col
I0512 23:45:15.309401 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/pre_extra_mlp_layer_norm/scale/v_row
I0512 23:45:15.309432 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/pre_mlp_layer_norm/scale/m
I0512 23:45:15.309463 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/pre_mlp_layer_norm/scale/v
I0512 23:45:15.309495 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/pre_mlp_layer_norm/scale/v_col
I0512 23:45:15.309528 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/pre_mlp_layer_norm/scale/v_row
I0512 23:45:15.309559 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/pre_self_attention_layer_norm/scale/m
I0512 23:45:15.309590 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/pre_self_attention_layer_norm/scale/v
I0512 23:45:15.309620 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/pre_self_attention_layer_norm/scale/v_col
I0512 23:45:15.309651 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/pre_self_attention_layer_norm/scale/v_row
I0512 23:45:15.309681 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/self_attention/key/kernel/m
I0512 23:45:15.309712 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/self_attention/key/kernel/v
I0512 23:45:15.309743 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/self_attention/key/kernel/v_col
I0512 23:45:15.309773 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/self_attention/key/kernel/v_row
I0512 23:45:15.309804 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/self_attention/out/kernel/m
I0512 23:45:15.309835 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/self_attention/out/kernel/v
I0512 23:45:15.309865 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/self_attention/out/kernel/v_col
I0512 23:45:15.309896 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/self_attention/out/kernel/v_row
I0512 23:45:15.309926 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/self_attention/query/kernel/m
I0512 23:45:15.309957 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/self_attention/query/kernel/v
I0512 23:45:15.309988 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/self_attention/query/kernel/v_col
I0512 23:45:15.310018 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/self_attention/query/kernel/v_row
I0512 23:45:15.310048 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/self_attention/value/kernel/m
I0512 23:45:15.310079 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/self_attention/value/kernel/v
I0512 23:45:15.310109 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/self_attention/value/kernel/v_col
I0512 23:45:15.310140 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/self_attention/value/kernel/v_row
I0512 23:45:15.310170 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_12/mlp/wi_0/kernel/m
I0512 23:45:15.310201 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_12/mlp/wi_0/kernel/v
I0512 23:45:15.310232 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_12/mlp/wi_0/kernel/v_col
I0512 23:45:15.310274 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_12/mlp/wi_0/kernel/v_row
I0512 23:45:15.310305 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_12/mlp/wi_1/kernel/m
I0512 23:45:15.310337 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_12/mlp/wi_1/kernel/v
I0512 23:45:15.310369 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_12/mlp/wi_1/kernel/v_col
I0512 23:45:15.310400 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_12/mlp/wi_1/kernel/v_row
I0512 23:45:15.310431 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_12/mlp/wo/kernel/m
I0512 23:45:15.310462 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_12/mlp/wo/kernel/v
I0512 23:45:15.310494 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_12/mlp/wo/kernel/v_col
I0512 23:45:15.310527 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_12/mlp/wo/kernel/v_row
I0512 23:45:15.310559 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_12/pre_mlp_layer_norm/scale/m
I0512 23:45:15.310590 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_12/pre_mlp_layer_norm/scale/v
I0512 23:45:15.310621 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_12/pre_mlp_layer_norm/scale/v_col
I0512 23:45:15.310652 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_12/pre_mlp_layer_norm/scale/v_row
I0512 23:45:15.310682 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_12/pre_self_attention_layer_norm/scale/m
I0512 23:45:15.310713 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_12/pre_self_attention_layer_norm/scale/v
I0512 23:45:15.310744 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_12/pre_self_attention_layer_norm/scale/v_col
I0512 23:45:15.310775 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_12/pre_self_attention_layer_norm/scale/v_row
I0512 23:45:15.310806 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_12/self_attention/key/kernel/m
I0512 23:45:15.310837 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_12/self_attention/key/kernel/v
I0512 23:45:15.310868 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_12/self_attention/key/kernel/v_col
I0512 23:45:15.310899 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_12/self_attention/key/kernel/v_row
I0512 23:45:15.310930 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_12/self_attention/out/kernel/m
I0512 23:45:15.310961 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_12/self_attention/out/kernel/v
I0512 23:45:15.310992 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_12/self_attention/out/kernel/v_col
I0512 23:45:15.311022 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_12/self_attention/out/kernel/v_row
I0512 23:45:15.311054 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_12/self_attention/query/kernel/m
I0512 23:45:15.311084 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_12/self_attention/query/kernel/v
I0512 23:45:15.311115 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_12/self_attention/query/kernel/v_col
I0512 23:45:15.311146 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_12/self_attention/query/kernel/v_row
I0512 23:45:15.311178 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_12/self_attention/value/kernel/m
I0512 23:45:15.311210 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_12/self_attention/value/kernel/v
I0512 23:45:15.311247 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_12/self_attention/value/kernel/v_col
I0512 23:45:15.311279 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_12/self_attention/value/kernel/v_row
I0512 23:45:15.311310 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_13/mlp/wi_0/kernel/m
I0512 23:45:15.311341 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_13/mlp/wi_0/kernel/v
I0512 23:45:15.311372 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_13/mlp/wi_0/kernel/v_col
I0512 23:45:15.311403 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_13/mlp/wi_0/kernel/v_row
I0512 23:45:15.311434 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_13/mlp/wi_1/kernel/m
I0512 23:45:15.311465 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_13/mlp/wi_1/kernel/v
I0512 23:45:15.311498 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_13/mlp/wi_1/kernel/v_col
I0512 23:45:15.311531 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_13/mlp/wi_1/kernel/v_row
I0512 23:45:15.311562 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_13/mlp/wo/kernel/m
I0512 23:45:15.311593 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_13/mlp/wo/kernel/v
I0512 23:45:15.311625 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_13/mlp/wo/kernel/v_col
I0512 23:45:15.311656 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_13/mlp/wo/kernel/v_row
I0512 23:45:15.311687 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_13/pre_mlp_layer_norm/scale/m
I0512 23:45:15.311719 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_13/pre_mlp_layer_norm/scale/v
I0512 23:45:15.311750 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_13/pre_mlp_layer_norm/scale/v_col
I0512 23:45:15.311781 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_13/pre_mlp_layer_norm/scale/v_row
I0512 23:45:15.311812 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_13/pre_self_attention_layer_norm/scale/m
I0512 23:45:15.311843 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_13/pre_self_attention_layer_norm/scale/v
I0512 23:45:15.311874 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_13/pre_self_attention_layer_norm/scale/v_col
I0512 23:45:15.311905 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_13/pre_self_attention_layer_norm/scale/v_row
I0512 23:45:15.311936 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_13/self_attention/key/kernel/m
I0512 23:45:15.311967 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_13/self_attention/key/kernel/v
I0512 23:45:15.311998 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_13/self_attention/key/kernel/v_col
I0512 23:45:15.312029 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_13/self_attention/key/kernel/v_row
I0512 23:45:15.312060 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_13/self_attention/out/kernel/m
I0512 23:45:15.312091 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_13/self_attention/out/kernel/v
I0512 23:45:15.312122 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_13/self_attention/out/kernel/v_col
I0512 23:45:15.312153 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_13/self_attention/out/kernel/v_row
I0512 23:45:15.312185 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_13/self_attention/query/kernel/m
I0512 23:45:15.312216 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_13/self_attention/query/kernel/v
I0512 23:45:15.312251 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_13/self_attention/query/kernel/v_col
I0512 23:45:15.312284 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_13/self_attention/query/kernel/v_row
I0512 23:45:15.312338 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_13/self_attention/value/kernel/m
I0512 23:45:15.312371 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_13/self_attention/value/kernel/v
I0512 23:45:15.312403 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_13/self_attention/value/kernel/v_col
I0512 23:45:15.312434 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_13/self_attention/value/kernel/v_row
I0512 23:45:15.312465 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_14/mlp/wi_0/kernel/m
I0512 23:45:15.312498 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_14/mlp/wi_0/kernel/v
I0512 23:45:15.312530 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_14/mlp/wi_0/kernel/v_col
I0512 23:45:15.312561 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_14/mlp/wi_0/kernel/v_row
I0512 23:45:15.312592 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_14/mlp/wi_1/kernel/m
I0512 23:45:15.312623 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_14/mlp/wi_1/kernel/v
I0512 23:45:15.312654 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_14/mlp/wi_1/kernel/v_col
I0512 23:45:15.312685 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_14/mlp/wi_1/kernel/v_row
I0512 23:45:15.312717 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_14/mlp/wo/kernel/m
I0512 23:45:15.312748 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_14/mlp/wo/kernel/v
I0512 23:45:15.312779 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_14/mlp/wo/kernel/v_col
I0512 23:45:15.312810 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_14/mlp/wo/kernel/v_row
I0512 23:45:15.312842 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_14/pre_mlp_layer_norm/scale/m
I0512 23:45:15.312873 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_14/pre_mlp_layer_norm/scale/v
I0512 23:45:15.312904 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_14/pre_mlp_layer_norm/scale/v_col
I0512 23:45:15.312935 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_14/pre_mlp_layer_norm/scale/v_row
I0512 23:45:15.312966 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_14/pre_self_attention_layer_norm/scale/m
I0512 23:45:15.312997 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_14/pre_self_attention_layer_norm/scale/v
I0512 23:45:15.313028 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_14/pre_self_attention_layer_norm/scale/v_col
I0512 23:45:15.313059 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_14/pre_self_attention_layer_norm/scale/v_row
I0512 23:45:15.313090 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_14/self_attention/key/kernel/m
I0512 23:45:15.313121 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_14/self_attention/key/kernel/v
I0512 23:45:15.313152 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_14/self_attention/key/kernel/v_col
I0512 23:45:15.313183 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_14/self_attention/key/kernel/v_row
I0512 23:45:15.313214 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_14/self_attention/out/kernel/m
I0512 23:45:15.313251 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_14/self_attention/out/kernel/v
I0512 23:45:15.313283 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_14/self_attention/out/kernel/v_col
I0512 23:45:15.313314 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_14/self_attention/out/kernel/v_row
I0512 23:45:15.313345 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_14/self_attention/query/kernel/m
I0512 23:45:15.313376 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_14/self_attention/query/kernel/v
I0512 23:45:15.313407 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_14/self_attention/query/kernel/v_col
I0512 23:45:15.313438 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_14/self_attention/query/kernel/v_row
I0512 23:45:15.313469 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_14/self_attention/value/kernel/m
I0512 23:45:15.313502 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_14/self_attention/value/kernel/v
I0512 23:45:15.313534 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_14/self_attention/value/kernel/v_col
I0512 23:45:15.313565 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_14/self_attention/value/kernel/v_row
I0512 23:45:15.313596 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_15/mlp/wi_0/kernel/m
I0512 23:45:15.313627 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_15/mlp/wi_0/kernel/v
I0512 23:45:15.313659 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_15/mlp/wi_0/kernel/v_col
I0512 23:45:15.313690 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_15/mlp/wi_0/kernel/v_row
I0512 23:45:15.313721 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_15/mlp/wi_1/kernel/m
I0512 23:45:15.313752 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_15/mlp/wi_1/kernel/v
I0512 23:45:15.313783 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_15/mlp/wi_1/kernel/v_col
I0512 23:45:15.313815 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_15/mlp/wi_1/kernel/v_row
I0512 23:45:15.313846 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_15/mlp/wo/kernel/m
I0512 23:45:15.313876 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_15/mlp/wo/kernel/v
I0512 23:45:15.313907 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_15/mlp/wo/kernel/v_col
I0512 23:45:15.313939 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_15/mlp/wo/kernel/v_row
I0512 23:45:15.313970 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_15/pre_mlp_layer_norm/scale/m
I0512 23:45:15.314002 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_15/pre_mlp_layer_norm/scale/v
I0512 23:45:15.314033 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_15/pre_mlp_layer_norm/scale/v_col
I0512 23:45:15.314064 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_15/pre_mlp_layer_norm/scale/v_row
I0512 23:45:15.314095 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_15/pre_self_attention_layer_norm/scale/m
I0512 23:45:15.314126 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_15/pre_self_attention_layer_norm/scale/v
I0512 23:45:15.314157 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_15/pre_self_attention_layer_norm/scale/v_col
I0512 23:45:15.314188 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_15/pre_self_attention_layer_norm/scale/v_row
I0512 23:45:15.314219 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_15/self_attention/key/kernel/m
I0512 23:45:15.314256 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_15/self_attention/key/kernel/v
I0512 23:45:15.314288 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_15/self_attention/key/kernel/v_col
I0512 23:45:15.314319 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_15/self_attention/key/kernel/v_row
I0512 23:45:15.314350 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_15/self_attention/out/kernel/m
I0512 23:45:15.314381 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_15/self_attention/out/kernel/v
I0512 23:45:15.314412 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_15/self_attention/out/kernel/v_col
I0512 23:45:15.314443 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_15/self_attention/out/kernel/v_row
I0512 23:45:15.314474 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_15/self_attention/query/kernel/m
I0512 23:45:15.314507 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_15/self_attention/query/kernel/v
I0512 23:45:15.314539 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_15/self_attention/query/kernel/v_col
I0512 23:45:15.314574 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_15/self_attention/query/kernel/v_row
I0512 23:45:15.314610 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_15/self_attention/value/kernel/m
I0512 23:45:15.314646 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_15/self_attention/value/kernel/v
I0512 23:45:15.314679 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_15/self_attention/value/kernel/v_col
I0512 23:45:15.314711 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_15/self_attention/value/kernel/v_row
I0512 23:45:15.314747 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_16/mlp/wi_0/kernel/m
I0512 23:45:15.314780 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_16/mlp/wi_0/kernel/v
I0512 23:45:15.314811 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_16/mlp/wi_0/kernel/v_col
I0512 23:45:15.314842 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_16/mlp/wi_0/kernel/v_row
I0512 23:45:15.314872 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_16/mlp/wi_1/kernel/m
I0512 23:45:15.314903 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_16/mlp/wi_1/kernel/v
I0512 23:45:15.314933 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_16/mlp/wi_1/kernel/v_col
I0512 23:45:15.314963 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_16/mlp/wi_1/kernel/v_row
I0512 23:45:15.314994 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_16/mlp/wo/kernel/m
I0512 23:45:15.315024 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_16/mlp/wo/kernel/v
I0512 23:45:15.315055 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_16/mlp/wo/kernel/v_col
I0512 23:45:15.315085 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_16/mlp/wo/kernel/v_row
I0512 23:45:15.315115 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_16/pre_mlp_layer_norm/scale/m
I0512 23:45:15.315145 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_16/pre_mlp_layer_norm/scale/v
I0512 23:45:15.315176 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_16/pre_mlp_layer_norm/scale/v_col
I0512 23:45:15.315211 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_16/pre_mlp_layer_norm/scale/v_row
I0512 23:45:15.315248 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_16/pre_self_attention_layer_norm/scale/m
I0512 23:45:15.315281 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_16/pre_self_attention_layer_norm/scale/v
I0512 23:45:15.315311 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_16/pre_self_attention_layer_norm/scale/v_col
I0512 23:45:15.315342 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_16/pre_self_attention_layer_norm/scale/v_row
I0512 23:45:15.315372 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_16/self_attention/key/kernel/m
I0512 23:45:15.315402 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_16/self_attention/key/kernel/v
I0512 23:45:15.315432 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_16/self_attention/key/kernel/v_col
I0512 23:45:15.315462 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_16/self_attention/key/kernel/v_row
I0512 23:45:15.315493 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_16/self_attention/out/kernel/m
I0512 23:45:15.315524 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_16/self_attention/out/kernel/v
I0512 23:45:15.315554 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_16/self_attention/out/kernel/v_col
I0512 23:45:15.315584 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_16/self_attention/out/kernel/v_row
I0512 23:45:15.315614 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_16/self_attention/query/kernel/m
I0512 23:45:15.315644 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_16/self_attention/query/kernel/v
I0512 23:45:15.315674 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_16/self_attention/query/kernel/v_col
I0512 23:45:15.315704 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_16/self_attention/query/kernel/v_row
I0512 23:45:15.315734 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_16/self_attention/value/kernel/m
I0512 23:45:15.315764 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_16/self_attention/value/kernel/v
I0512 23:45:15.315794 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_16/self_attention/value/kernel/v_col
I0512 23:45:15.315823 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_16/self_attention/value/kernel/v_row
I0512 23:45:15.315853 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/extra_mlp/wi_0/kernel/m
I0512 23:45:15.315883 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/extra_mlp/wi_0/kernel/v
I0512 23:45:15.315913 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/extra_mlp/wi_0/kernel/v_col
I0512 23:45:15.315943 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/extra_mlp/wi_0/kernel/v_row
I0512 23:45:15.315973 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/extra_mlp/wi_1/kernel/m
I0512 23:45:15.316002 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/extra_mlp/wi_1/kernel/v
I0512 23:45:15.316032 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/extra_mlp/wi_1/kernel/v_col
I0512 23:45:15.316062 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/extra_mlp/wi_1/kernel/v_row
I0512 23:45:15.316092 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/extra_mlp/wo/kernel/m
I0512 23:45:15.316122 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/extra_mlp/wo/kernel/v
I0512 23:45:15.316152 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/extra_mlp/wo/kernel/v_col
I0512 23:45:15.316182 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/extra_mlp/wo/kernel/v_row
I0512 23:45:15.316211 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/mlp/expert/wi_0/kernel/m
I0512 23:45:15.316246 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/mlp/expert/wi_0/kernel/v
I0512 23:45:15.316278 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/mlp/expert/wi_0/kernel/v_col
I0512 23:45:15.316331 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/mlp/expert/wi_0/kernel/v_row
I0512 23:45:15.316364 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/mlp/expert/wi_1/kernel/m
I0512 23:45:15.316394 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/mlp/expert/wi_1/kernel/v
I0512 23:45:15.316424 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/mlp/expert/wi_1/kernel/v_col
I0512 23:45:15.316454 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/mlp/expert/wi_1/kernel/v_row
I0512 23:45:15.316483 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/mlp/expert/wo/kernel/m
I0512 23:45:15.316515 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/mlp/expert/wo/kernel/v
I0512 23:45:15.316546 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/mlp/expert/wo/kernel/v_col
I0512 23:45:15.316576 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/mlp/expert/wo/kernel/v_row
I0512 23:45:15.316606 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/mlp/router/router_weights/w/kernel/m
I0512 23:45:15.316635 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/mlp/router/router_weights/w/kernel/v
I0512 23:45:15.316665 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/mlp/router/router_weights/w/kernel/v_col
I0512 23:45:15.316694 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/mlp/router/router_weights/w/kernel/v_row
I0512 23:45:15.316725 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/pre_extra_mlp_layer_norm/scale/m
I0512 23:45:15.316755 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/pre_extra_mlp_layer_norm/scale/v
I0512 23:45:15.316785 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/pre_extra_mlp_layer_norm/scale/v_col
I0512 23:45:15.316814 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/pre_extra_mlp_layer_norm/scale/v_row
I0512 23:45:15.316844 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/pre_mlp_layer_norm/scale/m
I0512 23:45:15.316874 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/pre_mlp_layer_norm/scale/v
I0512 23:45:15.316904 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/pre_mlp_layer_norm/scale/v_col
I0512 23:45:15.316933 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/pre_mlp_layer_norm/scale/v_row
I0512 23:45:15.316963 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/pre_self_attention_layer_norm/scale/m
I0512 23:45:15.316992 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/pre_self_attention_layer_norm/scale/v
I0512 23:45:15.317022 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/pre_self_attention_layer_norm/scale/v_col
I0512 23:45:15.317052 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/pre_self_attention_layer_norm/scale/v_row
I0512 23:45:15.317082 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/self_attention/key/kernel/m
I0512 23:45:15.317112 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/self_attention/key/kernel/v
I0512 23:45:15.317142 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/self_attention/key/kernel/v_col
I0512 23:45:15.317172 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/self_attention/key/kernel/v_row
I0512 23:45:15.317201 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/self_attention/out/kernel/m
I0512 23:45:15.317231 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/self_attention/out/kernel/v
I0512 23:45:15.317268 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/self_attention/out/kernel/v_col
I0512 23:45:15.317298 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/self_attention/out/kernel/v_row
I0512 23:45:15.317328 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/self_attention/query/kernel/m
I0512 23:45:15.317358 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/self_attention/query/kernel/v
I0512 23:45:15.317387 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/self_attention/query/kernel/v_col
I0512 23:45:15.317417 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/self_attention/query/kernel/v_row
I0512 23:45:15.317447 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/self_attention/value/kernel/m
I0512 23:45:15.317476 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/self_attention/value/kernel/v
I0512 23:45:15.317508 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/self_attention/value/kernel/v_col
I0512 23:45:15.317539 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/self_attention/value/kernel/v_row
I0512 23:45:15.317569 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_18/mlp/wi_0/kernel/m
I0512 23:45:15.317599 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_18/mlp/wi_0/kernel/v
I0512 23:45:15.317629 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_18/mlp/wi_0/kernel/v_col
I0512 23:45:15.317659 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_18/mlp/wi_0/kernel/v_row
I0512 23:45:15.317689 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_18/mlp/wi_1/kernel/m
I0512 23:45:15.317719 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_18/mlp/wi_1/kernel/v
I0512 23:45:15.317749 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_18/mlp/wi_1/kernel/v_col
I0512 23:45:15.317780 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_18/mlp/wi_1/kernel/v_row
I0512 23:45:15.317809 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_18/mlp/wo/kernel/m
I0512 23:45:15.317839 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_18/mlp/wo/kernel/v
I0512 23:45:15.317869 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_18/mlp/wo/kernel/v_col
I0512 23:45:15.317899 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_18/mlp/wo/kernel/v_row
I0512 23:45:15.317929 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_18/pre_mlp_layer_norm/scale/m
I0512 23:45:15.317958 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_18/pre_mlp_layer_norm/scale/v
I0512 23:45:15.317988 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_18/pre_mlp_layer_norm/scale/v_col
I0512 23:45:15.318018 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_18/pre_mlp_layer_norm/scale/v_row
I0512 23:45:15.318048 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_18/pre_self_attention_layer_norm/scale/m
I0512 23:45:15.318078 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_18/pre_self_attention_layer_norm/scale/v
I0512 23:45:15.318107 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_18/pre_self_attention_layer_norm/scale/v_col
I0512 23:45:15.318138 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_18/pre_self_attention_layer_norm/scale/v_row
I0512 23:45:15.318167 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_18/self_attention/key/kernel/m
I0512 23:45:15.318197 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_18/self_attention/key/kernel/v
I0512 23:45:15.318227 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_18/self_attention/key/kernel/v_col
I0512 23:45:15.318263 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_18/self_attention/key/kernel/v_row
I0512 23:45:15.318293 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_18/self_attention/out/kernel/m
I0512 23:45:15.318323 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_18/self_attention/out/kernel/v
I0512 23:45:15.318353 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_18/self_attention/out/kernel/v_col
I0512 23:45:15.318382 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_18/self_attention/out/kernel/v_row
I0512 23:45:15.318412 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_18/self_attention/query/kernel/m
I0512 23:45:15.318441 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_18/self_attention/query/kernel/v
I0512 23:45:15.318471 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_18/self_attention/query/kernel/v_col
I0512 23:45:15.318502 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_18/self_attention/query/kernel/v_row
I0512 23:45:15.318533 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_18/self_attention/value/kernel/m
I0512 23:45:15.318563 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_18/self_attention/value/kernel/v
I0512 23:45:15.318593 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_18/self_attention/value/kernel/v_col
I0512 23:45:15.318623 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_18/self_attention/value/kernel/v_row
I0512 23:45:15.318652 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_19/mlp/wi_0/kernel/m
I0512 23:45:15.318682 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_19/mlp/wi_0/kernel/v
I0512 23:45:15.318711 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_19/mlp/wi_0/kernel/v_col
I0512 23:45:15.318741 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_19/mlp/wi_0/kernel/v_row
I0512 23:45:15.318771 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_19/mlp/wi_1/kernel/m
I0512 23:45:15.318801 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_19/mlp/wi_1/kernel/v
I0512 23:45:15.318830 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_19/mlp/wi_1/kernel/v_col
I0512 23:45:15.318860 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_19/mlp/wi_1/kernel/v_row
I0512 23:45:15.318890 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_19/mlp/wo/kernel/m
I0512 23:45:15.318919 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_19/mlp/wo/kernel/v
I0512 23:45:15.318949 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_19/mlp/wo/kernel/v_col
I0512 23:45:15.318978 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_19/mlp/wo/kernel/v_row
I0512 23:45:15.319008 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_19/pre_mlp_layer_norm/scale/m
I0512 23:45:15.319037 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_19/pre_mlp_layer_norm/scale/v
I0512 23:45:15.319067 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_19/pre_mlp_layer_norm/scale/v_col
I0512 23:45:15.319096 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_19/pre_mlp_layer_norm/scale/v_row
I0512 23:45:15.319126 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_19/pre_self_attention_layer_norm/scale/m
I0512 23:45:15.319156 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_19/pre_self_attention_layer_norm/scale/v
I0512 23:45:15.319186 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_19/pre_self_attention_layer_norm/scale/v_col
I0512 23:45:15.319216 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_19/pre_self_attention_layer_norm/scale/v_row
I0512 23:45:15.319251 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_19/self_attention/key/kernel/m
I0512 23:45:15.319281 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_19/self_attention/key/kernel/v
I0512 23:45:15.319311 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_19/self_attention/key/kernel/v_col
I0512 23:45:15.319341 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_19/self_attention/key/kernel/v_row
I0512 23:45:15.319370 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_19/self_attention/out/kernel/m
I0512 23:45:15.319400 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_19/self_attention/out/kernel/v
I0512 23:45:15.319430 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_19/self_attention/out/kernel/v_col
I0512 23:45:15.319460 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_19/self_attention/out/kernel/v_row
I0512 23:45:15.319490 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_19/self_attention/query/kernel/m
I0512 23:45:15.319521 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_19/self_attention/query/kernel/v
I0512 23:45:15.319551 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_19/self_attention/query/kernel/v_col
I0512 23:45:15.319581 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_19/self_attention/query/kernel/v_row
I0512 23:45:15.319611 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_19/self_attention/value/kernel/m
I0512 23:45:15.319641 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_19/self_attention/value/kernel/v
I0512 23:45:15.319671 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_19/self_attention/value/kernel/v_col
I0512 23:45:15.319700 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_19/self_attention/value/kernel/v_row
I0512 23:45:15.319730 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_2/mlp/wi_0/kernel/m
I0512 23:45:15.319760 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_2/mlp/wi_0/kernel/v
I0512 23:45:15.319790 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_2/mlp/wi_0/kernel/v_col
I0512 23:45:15.319819 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_2/mlp/wi_0/kernel/v_row
I0512 23:45:15.319849 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_2/mlp/wi_1/kernel/m
I0512 23:45:15.319879 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_2/mlp/wi_1/kernel/v
I0512 23:45:15.319909 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_2/mlp/wi_1/kernel/v_col
I0512 23:45:15.319939 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_2/mlp/wi_1/kernel/v_row
I0512 23:45:15.319968 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_2/mlp/wo/kernel/m
I0512 23:45:15.319998 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_2/mlp/wo/kernel/v
I0512 23:45:15.320028 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_2/mlp/wo/kernel/v_col
I0512 23:45:15.320057 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_2/mlp/wo/kernel/v_row
I0512 23:45:15.320087 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_2/pre_mlp_layer_norm/scale/m
I0512 23:45:15.320116 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_2/pre_mlp_layer_norm/scale/v
I0512 23:45:15.320146 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_2/pre_mlp_layer_norm/scale/v_col
I0512 23:45:15.320176 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_2/pre_mlp_layer_norm/scale/v_row
I0512 23:45:15.320205 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_2/pre_self_attention_layer_norm/scale/m
I0512 23:45:15.320240 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_2/pre_self_attention_layer_norm/scale/v
I0512 23:45:15.320271 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_2/pre_self_attention_layer_norm/scale/v_col
I0512 23:45:15.320320 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_2/pre_self_attention_layer_norm/scale/v_row
I0512 23:45:15.320354 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_2/self_attention/key/kernel/m
I0512 23:45:15.320384 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_2/self_attention/key/kernel/v
I0512 23:45:15.320414 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_2/self_attention/key/kernel/v_col
I0512 23:45:15.320444 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_2/self_attention/key/kernel/v_row
I0512 23:45:15.320473 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_2/self_attention/out/kernel/m
I0512 23:45:15.320505 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_2/self_attention/out/kernel/v
I0512 23:45:15.320535 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_2/self_attention/out/kernel/v_col
I0512 23:45:15.320565 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_2/self_attention/out/kernel/v_row
I0512 23:45:15.320595 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_2/self_attention/query/kernel/m
I0512 23:45:15.320624 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_2/self_attention/query/kernel/v
I0512 23:45:15.320653 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_2/self_attention/query/kernel/v_col
I0512 23:45:15.320683 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_2/self_attention/query/kernel/v_row
I0512 23:45:15.320713 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_2/self_attention/value/kernel/m
I0512 23:45:15.320742 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_2/self_attention/value/kernel/v
I0512 23:45:15.320772 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_2/self_attention/value/kernel/v_col
I0512 23:45:15.320801 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_2/self_attention/value/kernel/v_row
I0512 23:45:15.320831 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_20/mlp/wi_0/kernel/m
I0512 23:45:15.320860 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_20/mlp/wi_0/kernel/v
I0512 23:45:15.320890 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_20/mlp/wi_0/kernel/v_col
I0512 23:45:15.320920 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_20/mlp/wi_0/kernel/v_row
I0512 23:45:15.320950 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_20/mlp/wi_1/kernel/m
I0512 23:45:15.320980 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_20/mlp/wi_1/kernel/v
I0512 23:45:15.321009 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_20/mlp/wi_1/kernel/v_col
I0512 23:45:15.321039 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_20/mlp/wi_1/kernel/v_row
I0512 23:45:15.321069 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_20/mlp/wo/kernel/m
I0512 23:45:15.321098 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_20/mlp/wo/kernel/v
I0512 23:45:15.321128 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_20/mlp/wo/kernel/v_col
I0512 23:45:15.321158 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_20/mlp/wo/kernel/v_row
I0512 23:45:15.321187 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_20/pre_mlp_layer_norm/scale/m
I0512 23:45:15.321217 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_20/pre_mlp_layer_norm/scale/v
I0512 23:45:15.321253 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_20/pre_mlp_layer_norm/scale/v_col
I0512 23:45:15.321283 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_20/pre_mlp_layer_norm/scale/v_row
I0512 23:45:15.321313 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_20/pre_self_attention_layer_norm/scale/m
I0512 23:45:15.321343 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_20/pre_self_attention_layer_norm/scale/v
I0512 23:45:15.321372 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_20/pre_self_attention_layer_norm/scale/v_col
I0512 23:45:15.321402 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_20/pre_self_attention_layer_norm/scale/v_row
I0512 23:45:15.321432 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_20/self_attention/key/kernel/m
I0512 23:45:15.321461 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_20/self_attention/key/kernel/v
I0512 23:45:15.321492 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_20/self_attention/key/kernel/v_col
I0512 23:45:15.321523 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_20/self_attention/key/kernel/v_row
I0512 23:45:15.321553 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_20/self_attention/out/kernel/m
I0512 23:45:15.321583 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_20/self_attention/out/kernel/v
I0512 23:45:15.321613 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_20/self_attention/out/kernel/v_col
I0512 23:45:15.321642 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_20/self_attention/out/kernel/v_row
I0512 23:45:15.321672 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_20/self_attention/query/kernel/m
I0512 23:45:15.321702 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_20/self_attention/query/kernel/v
I0512 23:45:15.321731 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_20/self_attention/query/kernel/v_col
I0512 23:45:15.321761 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_20/self_attention/query/kernel/v_row
I0512 23:45:15.321790 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_20/self_attention/value/kernel/m
I0512 23:45:15.321820 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_20/self_attention/value/kernel/v
I0512 23:45:15.321850 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_20/self_attention/value/kernel/v_col
I0512 23:45:15.321879 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_20/self_attention/value/kernel/v_row
I0512 23:45:15.321909 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_21/mlp/wi_0/kernel/m
I0512 23:45:15.321939 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_21/mlp/wi_0/kernel/v
I0512 23:45:15.321969 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_21/mlp/wi_0/kernel/v_col
I0512 23:45:15.321999 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_21/mlp/wi_0/kernel/v_row
I0512 23:45:15.322028 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_21/mlp/wi_1/kernel/m
I0512 23:45:15.322058 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_21/mlp/wi_1/kernel/v
I0512 23:45:15.322088 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_21/mlp/wi_1/kernel/v_col
I0512 23:45:15.322117 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_21/mlp/wi_1/kernel/v_row
I0512 23:45:15.322147 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_21/mlp/wo/kernel/m
I0512 23:45:15.322177 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_21/mlp/wo/kernel/v
I0512 23:45:15.322206 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_21/mlp/wo/kernel/v_col
I0512 23:45:15.322241 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_21/mlp/wo/kernel/v_row
I0512 23:45:15.322272 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_21/pre_mlp_layer_norm/scale/m
I0512 23:45:15.322302 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_21/pre_mlp_layer_norm/scale/v
I0512 23:45:15.322332 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_21/pre_mlp_layer_norm/scale/v_col
I0512 23:45:15.322361 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_21/pre_mlp_layer_norm/scale/v_row
I0512 23:45:15.322391 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_21/pre_self_attention_layer_norm/scale/m
I0512 23:45:15.322421 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_21/pre_self_attention_layer_norm/scale/v
I0512 23:45:15.322450 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_21/pre_self_attention_layer_norm/scale/v_col
I0512 23:45:15.322480 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_21/pre_self_attention_layer_norm/scale/v_row
I0512 23:45:15.322513 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_21/self_attention/key/kernel/m
I0512 23:45:15.322543 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_21/self_attention/key/kernel/v
I0512 23:45:15.322573 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_21/self_attention/key/kernel/v_col
I0512 23:45:15.322603 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_21/self_attention/key/kernel/v_row
I0512 23:45:15.322633 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_21/self_attention/out/kernel/m
I0512 23:45:15.322663 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_21/self_attention/out/kernel/v
I0512 23:45:15.322693 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_21/self_attention/out/kernel/v_col
I0512 23:45:15.322722 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_21/self_attention/out/kernel/v_row
I0512 23:45:15.322752 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_21/self_attention/query/kernel/m
I0512 23:45:15.322806 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_21/self_attention/query/kernel/v
I0512 23:45:15.322839 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_21/self_attention/query/kernel/v_col
I0512 23:45:15.322869 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_21/self_attention/query/kernel/v_row
I0512 23:45:15.322899 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_21/self_attention/value/kernel/m
I0512 23:45:15.322929 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_21/self_attention/value/kernel/v
I0512 23:45:15.322958 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_21/self_attention/value/kernel/v_col
I0512 23:45:15.322988 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_21/self_attention/value/kernel/v_row
I0512 23:45:15.323018 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_22/mlp/wi_0/kernel/m
I0512 23:45:15.323048 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_22/mlp/wi_0/kernel/v
I0512 23:45:15.323077 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_22/mlp/wi_0/kernel/v_col
I0512 23:45:15.323107 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_22/mlp/wi_0/kernel/v_row
I0512 23:45:15.323137 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_22/mlp/wi_1/kernel/m
I0512 23:45:15.323167 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_22/mlp/wi_1/kernel/v
I0512 23:45:15.323197 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_22/mlp/wi_1/kernel/v_col
I0512 23:45:15.323226 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_22/mlp/wi_1/kernel/v_row
I0512 23:45:15.323262 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_22/mlp/wo/kernel/m
I0512 23:45:15.323292 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_22/mlp/wo/kernel/v
I0512 23:45:15.323322 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_22/mlp/wo/kernel/v_col
I0512 23:45:15.323352 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_22/mlp/wo/kernel/v_row
I0512 23:45:15.323381 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_22/pre_mlp_layer_norm/scale/m
I0512 23:45:15.323411 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_22/pre_mlp_layer_norm/scale/v
I0512 23:45:15.323441 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_22/pre_mlp_layer_norm/scale/v_col
I0512 23:45:15.323471 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_22/pre_mlp_layer_norm/scale/v_row
I0512 23:45:15.323502 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_22/pre_self_attention_layer_norm/scale/m
I0512 23:45:15.323533 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_22/pre_self_attention_layer_norm/scale/v
I0512 23:45:15.323563 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_22/pre_self_attention_layer_norm/scale/v_col
I0512 23:45:15.323593 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_22/pre_self_attention_layer_norm/scale/v_row
I0512 23:45:15.323623 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_22/self_attention/key/kernel/m
I0512 23:45:15.323652 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_22/self_attention/key/kernel/v
I0512 23:45:15.323682 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_22/self_attention/key/kernel/v_col
I0512 23:45:15.323712 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_22/self_attention/key/kernel/v_row
I0512 23:45:15.323741 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_22/self_attention/out/kernel/m
I0512 23:45:15.323771 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_22/self_attention/out/kernel/v
I0512 23:45:15.323801 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_22/self_attention/out/kernel/v_col
I0512 23:45:15.323830 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_22/self_attention/out/kernel/v_row
I0512 23:45:15.323859 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_22/self_attention/query/kernel/m
I0512 23:45:15.323889 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_22/self_attention/query/kernel/v
I0512 23:45:15.323919 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_22/self_attention/query/kernel/v_col
I0512 23:45:15.323948 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_22/self_attention/query/kernel/v_row
I0512 23:45:15.323978 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_22/self_attention/value/kernel/m
I0512 23:45:15.324007 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_22/self_attention/value/kernel/v
I0512 23:45:15.324037 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_22/self_attention/value/kernel/v_col
I0512 23:45:15.324067 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_22/self_attention/value/kernel/v_row
I0512 23:45:15.324096 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/extra_mlp/wi_0/kernel/m
I0512 23:45:15.324126 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/extra_mlp/wi_0/kernel/v
I0512 23:45:15.324156 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/extra_mlp/wi_0/kernel/v_col
I0512 23:45:15.324185 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/extra_mlp/wi_0/kernel/v_row
I0512 23:45:15.324215 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/extra_mlp/wi_1/kernel/m
I0512 23:45:15.324250 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/extra_mlp/wi_1/kernel/v
I0512 23:45:15.324281 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/extra_mlp/wi_1/kernel/v_col
I0512 23:45:15.324341 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/extra_mlp/wi_1/kernel/v_row
I0512 23:45:15.324375 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/extra_mlp/wo/kernel/m
I0512 23:45:15.324407 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/extra_mlp/wo/kernel/v
I0512 23:45:15.324437 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/extra_mlp/wo/kernel/v_col
I0512 23:45:15.324467 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/extra_mlp/wo/kernel/v_row
I0512 23:45:15.324498 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/mlp/expert/wi_0/kernel/m
I0512 23:45:15.324529 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/mlp/expert/wi_0/kernel/v
I0512 23:45:15.324559 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/mlp/expert/wi_0/kernel/v_col
I0512 23:45:15.324589 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/mlp/expert/wi_0/kernel/v_row
I0512 23:45:15.324619 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/mlp/expert/wi_1/kernel/m
I0512 23:45:15.324648 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/mlp/expert/wi_1/kernel/v
I0512 23:45:15.324678 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/mlp/expert/wi_1/kernel/v_col
I0512 23:45:15.324707 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/mlp/expert/wi_1/kernel/v_row
I0512 23:45:15.324737 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/mlp/expert/wo/kernel/m
I0512 23:45:15.324767 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/mlp/expert/wo/kernel/v
I0512 23:45:15.324797 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/mlp/expert/wo/kernel/v_col
I0512 23:45:15.324826 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/mlp/expert/wo/kernel/v_row
I0512 23:45:15.324856 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/mlp/router/router_weights/w/kernel/m
I0512 23:45:15.324886 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/mlp/router/router_weights/w/kernel/v
I0512 23:45:15.324915 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/mlp/router/router_weights/w/kernel/v_col
I0512 23:45:15.324945 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/mlp/router/router_weights/w/kernel/v_row
I0512 23:45:15.324974 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/pre_extra_mlp_layer_norm/scale/m
I0512 23:45:15.325004 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/pre_extra_mlp_layer_norm/scale/v
I0512 23:45:15.325033 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/pre_extra_mlp_layer_norm/scale/v_col
I0512 23:45:15.325063 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/pre_extra_mlp_layer_norm/scale/v_row
I0512 23:45:15.325093 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/pre_mlp_layer_norm/scale/m
I0512 23:45:15.325123 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/pre_mlp_layer_norm/scale/v
I0512 23:45:15.325153 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/pre_mlp_layer_norm/scale/v_col
I0512 23:45:15.325182 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/pre_mlp_layer_norm/scale/v_row
I0512 23:45:15.325212 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/pre_self_attention_layer_norm/scale/m
I0512 23:45:15.325247 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/pre_self_attention_layer_norm/scale/v
I0512 23:45:15.325279 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/pre_self_attention_layer_norm/scale/v_col
I0512 23:45:15.325309 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/pre_self_attention_layer_norm/scale/v_row
I0512 23:45:15.325339 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/self_attention/key/kernel/m
I0512 23:45:15.325369 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/self_attention/key/kernel/v
I0512 23:45:15.325398 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/self_attention/key/kernel/v_col
I0512 23:45:15.325428 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/self_attention/key/kernel/v_row
I0512 23:45:15.325457 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/self_attention/out/kernel/m
I0512 23:45:15.325487 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/self_attention/out/kernel/v
I0512 23:45:15.325519 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/self_attention/out/kernel/v_col
I0512 23:45:15.325549 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/self_attention/out/kernel/v_row
I0512 23:45:15.325579 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/self_attention/query/kernel/m
I0512 23:45:15.325608 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/self_attention/query/kernel/v
I0512 23:45:15.325638 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/self_attention/query/kernel/v_col
I0512 23:45:15.325668 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/self_attention/query/kernel/v_row
I0512 23:45:15.325698 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/self_attention/value/kernel/m
I0512 23:45:15.325727 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/self_attention/value/kernel/v
I0512 23:45:15.325757 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/self_attention/value/kernel/v_col
I0512 23:45:15.325787 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/self_attention/value/kernel/v_row
I0512 23:45:15.325817 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_3/mlp/wi_0/kernel/m
I0512 23:45:15.325847 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_3/mlp/wi_0/kernel/v
I0512 23:45:15.325876 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_3/mlp/wi_0/kernel/v_col
I0512 23:45:15.325906 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_3/mlp/wi_0/kernel/v_row
I0512 23:45:15.325936 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_3/mlp/wi_1/kernel/m
I0512 23:45:15.325966 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_3/mlp/wi_1/kernel/v
I0512 23:45:15.325995 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_3/mlp/wi_1/kernel/v_col
I0512 23:45:15.326025 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_3/mlp/wi_1/kernel/v_row
I0512 23:45:15.326056 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_3/mlp/wo/kernel/m
I0512 23:45:15.326085 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_3/mlp/wo/kernel/v
I0512 23:45:15.326115 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_3/mlp/wo/kernel/v_col
I0512 23:45:15.326144 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_3/mlp/wo/kernel/v_row
I0512 23:45:15.326174 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_3/pre_mlp_layer_norm/scale/m
I0512 23:45:15.326204 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_3/pre_mlp_layer_norm/scale/v
I0512 23:45:15.326233 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_3/pre_mlp_layer_norm/scale/v_col
I0512 23:45:15.326270 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_3/pre_mlp_layer_norm/scale/v_row
I0512 23:45:15.326300 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_3/pre_self_attention_layer_norm/scale/m
I0512 23:45:15.326330 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_3/pre_self_attention_layer_norm/scale/v
I0512 23:45:15.326359 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_3/pre_self_attention_layer_norm/scale/v_col
I0512 23:45:15.326394 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_3/pre_self_attention_layer_norm/scale/v_row
I0512 23:45:15.326424 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_3/self_attention/key/kernel/m
I0512 23:45:15.326454 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_3/self_attention/key/kernel/v
I0512 23:45:15.326483 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_3/self_attention/key/kernel/v_col
I0512 23:45:15.326515 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_3/self_attention/key/kernel/v_row
I0512 23:45:15.326546 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_3/self_attention/out/kernel/m
I0512 23:45:15.326576 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_3/self_attention/out/kernel/v
I0512 23:45:15.326606 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_3/self_attention/out/kernel/v_col
I0512 23:45:15.326635 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_3/self_attention/out/kernel/v_row
I0512 23:45:15.326665 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_3/self_attention/query/kernel/m
I0512 23:45:15.326694 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_3/self_attention/query/kernel/v
I0512 23:45:15.326724 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_3/self_attention/query/kernel/v_col
I0512 23:45:15.326754 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_3/self_attention/query/kernel/v_row
I0512 23:45:15.326784 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_3/self_attention/value/kernel/m
I0512 23:45:15.326814 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_3/self_attention/value/kernel/v
I0512 23:45:15.326844 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_3/self_attention/value/kernel/v_col
I0512 23:45:15.326874 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_3/self_attention/value/kernel/v_row
I0512 23:45:15.326904 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_4/mlp/wi_0/kernel/m
I0512 23:45:15.326934 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_4/mlp/wi_0/kernel/v
I0512 23:45:15.326963 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_4/mlp/wi_0/kernel/v_col
I0512 23:45:15.326993 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_4/mlp/wi_0/kernel/v_row
I0512 23:45:15.327023 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_4/mlp/wi_1/kernel/m
I0512 23:45:15.327053 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_4/mlp/wi_1/kernel/v
I0512 23:45:15.327083 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_4/mlp/wi_1/kernel/v_col
I0512 23:45:15.327113 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_4/mlp/wi_1/kernel/v_row
I0512 23:45:15.327143 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_4/mlp/wo/kernel/m
I0512 23:45:15.327172 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_4/mlp/wo/kernel/v
I0512 23:45:15.327203 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_4/mlp/wo/kernel/v_col
I0512 23:45:15.327233 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_4/mlp/wo/kernel/v_row
I0512 23:45:15.327270 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_4/pre_mlp_layer_norm/scale/m
I0512 23:45:15.327301 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_4/pre_mlp_layer_norm/scale/v
I0512 23:45:15.327331 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_4/pre_mlp_layer_norm/scale/v_col
I0512 23:45:15.327362 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_4/pre_mlp_layer_norm/scale/v_row
I0512 23:45:15.327392 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_4/pre_self_attention_layer_norm/scale/m
I0512 23:45:15.327422 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_4/pre_self_attention_layer_norm/scale/v
I0512 23:45:15.327452 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_4/pre_self_attention_layer_norm/scale/v_col
I0512 23:45:15.327482 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_4/pre_self_attention_layer_norm/scale/v_row
I0512 23:45:15.327514 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_4/self_attention/key/kernel/m
I0512 23:45:15.327545 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_4/self_attention/key/kernel/v
I0512 23:45:15.327576 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_4/self_attention/key/kernel/v_col
I0512 23:45:15.327606 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_4/self_attention/key/kernel/v_row
I0512 23:45:15.327636 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_4/self_attention/out/kernel/m
I0512 23:45:15.327666 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_4/self_attention/out/kernel/v
I0512 23:45:15.327697 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_4/self_attention/out/kernel/v_col
I0512 23:45:15.327727 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_4/self_attention/out/kernel/v_row
I0512 23:45:15.327757 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_4/self_attention/query/kernel/m
I0512 23:45:15.327788 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_4/self_attention/query/kernel/v
I0512 23:45:15.327818 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_4/self_attention/query/kernel/v_col
I0512 23:45:15.327848 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_4/self_attention/query/kernel/v_row
I0512 23:45:15.327878 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_4/self_attention/value/kernel/m
I0512 23:45:15.327909 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_4/self_attention/value/kernel/v
I0512 23:45:15.327939 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_4/self_attention/value/kernel/v_col
I0512 23:45:15.327970 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_4/self_attention/value/kernel/v_row
I0512 23:45:15.328000 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/extra_mlp/wi_0/kernel/m
I0512 23:45:15.328030 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/extra_mlp/wi_0/kernel/v
I0512 23:45:15.328060 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/extra_mlp/wi_0/kernel/v_col
I0512 23:45:15.328090 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/extra_mlp/wi_0/kernel/v_row
I0512 23:45:15.328121 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/extra_mlp/wi_1/kernel/m
I0512 23:45:15.328151 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/extra_mlp/wi_1/kernel/v
I0512 23:45:15.328181 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/extra_mlp/wi_1/kernel/v_col
I0512 23:45:15.328212 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/extra_mlp/wi_1/kernel/v_row
I0512 23:45:15.328248 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/extra_mlp/wo/kernel/m
I0512 23:45:15.328279 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/extra_mlp/wo/kernel/v
I0512 23:45:15.328330 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/extra_mlp/wo/kernel/v_col
I0512 23:45:15.328363 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/extra_mlp/wo/kernel/v_row
I0512 23:45:15.328394 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/mlp/expert/wi_0/kernel/m
I0512 23:45:15.328424 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/mlp/expert/wi_0/kernel/v
I0512 23:45:15.328455 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/mlp/expert/wi_0/kernel/v_col
I0512 23:45:15.328485 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/mlp/expert/wi_0/kernel/v_row
I0512 23:45:15.328517 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/mlp/expert/wi_1/kernel/m
I0512 23:45:15.328548 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/mlp/expert/wi_1/kernel/v
I0512 23:45:15.328578 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/mlp/expert/wi_1/kernel/v_col
I0512 23:45:15.328608 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/mlp/expert/wi_1/kernel/v_row
I0512 23:45:15.328639 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/mlp/expert/wo/kernel/m
I0512 23:45:15.328669 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/mlp/expert/wo/kernel/v
I0512 23:45:15.328699 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/mlp/expert/wo/kernel/v_col
I0512 23:45:15.328729 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/mlp/expert/wo/kernel/v_row
I0512 23:45:15.328760 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/mlp/router/router_weights/w/kernel/m
I0512 23:45:15.328790 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/mlp/router/router_weights/w/kernel/v
I0512 23:45:15.328820 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/mlp/router/router_weights/w/kernel/v_col
I0512 23:45:15.328850 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/mlp/router/router_weights/w/kernel/v_row
I0512 23:45:15.328881 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/pre_extra_mlp_layer_norm/scale/m
I0512 23:45:15.328911 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/pre_extra_mlp_layer_norm/scale/v
I0512 23:45:15.328941 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/pre_extra_mlp_layer_norm/scale/v_col
I0512 23:45:15.328972 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/pre_extra_mlp_layer_norm/scale/v_row
I0512 23:45:15.329001 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/pre_mlp_layer_norm/scale/m
I0512 23:45:15.329032 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/pre_mlp_layer_norm/scale/v
I0512 23:45:15.329062 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/pre_mlp_layer_norm/scale/v_col
I0512 23:45:15.329093 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/pre_mlp_layer_norm/scale/v_row
I0512 23:45:15.329123 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/pre_self_attention_layer_norm/scale/m
I0512 23:45:15.329153 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/pre_self_attention_layer_norm/scale/v
I0512 23:45:15.329184 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/pre_self_attention_layer_norm/scale/v_col
I0512 23:45:15.329214 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/pre_self_attention_layer_norm/scale/v_row
I0512 23:45:15.329250 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/self_attention/key/kernel/m
I0512 23:45:15.329282 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/self_attention/key/kernel/v
I0512 23:45:15.329312 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/self_attention/key/kernel/v_col
I0512 23:45:15.329342 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/self_attention/key/kernel/v_row
I0512 23:45:15.329375 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/self_attention/out/kernel/m
I0512 23:45:15.329407 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/self_attention/out/kernel/v
I0512 23:45:15.329437 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/self_attention/out/kernel/v_col
I0512 23:45:15.329467 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/self_attention/out/kernel/v_row
I0512 23:45:15.329499 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/self_attention/query/kernel/m
I0512 23:45:15.329530 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/self_attention/query/kernel/v
I0512 23:45:15.329560 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/self_attention/query/kernel/v_col
I0512 23:45:15.329591 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/self_attention/query/kernel/v_row
I0512 23:45:15.329621 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/self_attention/value/kernel/m
I0512 23:45:15.329651 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/self_attention/value/kernel/v
I0512 23:45:15.329681 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/self_attention/value/kernel/v_col
I0512 23:45:15.329711 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/self_attention/value/kernel/v_row
I0512 23:45:15.329741 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_6/mlp/wi_0/kernel/m
I0512 23:45:15.329772 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_6/mlp/wi_0/kernel/v
I0512 23:45:15.329802 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_6/mlp/wi_0/kernel/v_col
I0512 23:45:15.329832 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_6/mlp/wi_0/kernel/v_row
I0512 23:45:15.329862 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_6/mlp/wi_1/kernel/m
I0512 23:45:15.329893 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_6/mlp/wi_1/kernel/v
I0512 23:45:15.329923 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_6/mlp/wi_1/kernel/v_col
I0512 23:45:15.329954 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_6/mlp/wi_1/kernel/v_row
I0512 23:45:15.329984 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_6/mlp/wo/kernel/m
I0512 23:45:15.330014 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_6/mlp/wo/kernel/v
I0512 23:45:15.330045 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_6/mlp/wo/kernel/v_col
I0512 23:45:15.330075 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_6/mlp/wo/kernel/v_row
I0512 23:45:15.330106 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_6/pre_mlp_layer_norm/scale/m
I0512 23:45:15.330137 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_6/pre_mlp_layer_norm/scale/v
I0512 23:45:15.330168 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_6/pre_mlp_layer_norm/scale/v_col
I0512 23:45:15.330199 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_6/pre_mlp_layer_norm/scale/v_row
I0512 23:45:15.330229 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_6/pre_self_attention_layer_norm/scale/m
I0512 23:45:15.330265 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_6/pre_self_attention_layer_norm/scale/v
I0512 23:45:15.330296 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_6/pre_self_attention_layer_norm/scale/v_col
I0512 23:45:15.330326 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_6/pre_self_attention_layer_norm/scale/v_row
I0512 23:45:15.330357 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_6/self_attention/key/kernel/m
I0512 23:45:15.330387 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_6/self_attention/key/kernel/v
I0512 23:45:15.330417 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_6/self_attention/key/kernel/v_col
I0512 23:45:15.330447 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_6/self_attention/key/kernel/v_row
I0512 23:45:15.330478 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_6/self_attention/out/kernel/m
I0512 23:45:15.330510 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_6/self_attention/out/kernel/v
I0512 23:45:15.330541 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_6/self_attention/out/kernel/v_col
I0512 23:45:15.330571 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_6/self_attention/out/kernel/v_row
I0512 23:45:15.330601 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_6/self_attention/query/kernel/m
I0512 23:45:15.330632 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_6/self_attention/query/kernel/v
I0512 23:45:15.330662 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_6/self_attention/query/kernel/v_col
I0512 23:45:15.330692 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_6/self_attention/query/kernel/v_row
I0512 23:45:15.330722 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_6/self_attention/value/kernel/m
I0512 23:45:15.330752 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_6/self_attention/value/kernel/v
I0512 23:45:15.330783 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_6/self_attention/value/kernel/v_col
I0512 23:45:15.330813 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_6/self_attention/value/kernel/v_row
I0512 23:45:15.330843 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_7/mlp/wi_0/kernel/m
I0512 23:45:15.330873 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_7/mlp/wi_0/kernel/v
I0512 23:45:15.330903 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_7/mlp/wi_0/kernel/v_col
I0512 23:45:15.330933 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_7/mlp/wi_0/kernel/v_row
I0512 23:45:15.330962 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_7/mlp/wi_1/kernel/m
I0512 23:45:15.330992 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_7/mlp/wi_1/kernel/v
I0512 23:45:15.331022 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_7/mlp/wi_1/kernel/v_col
I0512 23:45:15.331052 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_7/mlp/wi_1/kernel/v_row
I0512 23:45:15.331081 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_7/mlp/wo/kernel/m
I0512 23:45:15.331111 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_7/mlp/wo/kernel/v
I0512 23:45:15.331140 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_7/mlp/wo/kernel/v_col
I0512 23:45:15.331170 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_7/mlp/wo/kernel/v_row
I0512 23:45:15.331199 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_7/pre_mlp_layer_norm/scale/m
I0512 23:45:15.331229 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_7/pre_mlp_layer_norm/scale/v
I0512 23:45:15.331266 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_7/pre_mlp_layer_norm/scale/v_col
I0512 23:45:15.331296 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_7/pre_mlp_layer_norm/scale/v_row
I0512 23:45:15.331326 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_7/pre_self_attention_layer_norm/scale/m
I0512 23:45:15.331356 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_7/pre_self_attention_layer_norm/scale/v
I0512 23:45:15.331386 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_7/pre_self_attention_layer_norm/scale/v_col
I0512 23:45:15.331415 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_7/pre_self_attention_layer_norm/scale/v_row
I0512 23:45:15.331445 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_7/self_attention/key/kernel/m
I0512 23:45:15.331475 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_7/self_attention/key/kernel/v
I0512 23:45:15.331506 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_7/self_attention/key/kernel/v_col
I0512 23:45:15.331537 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_7/self_attention/key/kernel/v_row
I0512 23:45:15.331567 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_7/self_attention/out/kernel/m
I0512 23:45:15.331596 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_7/self_attention/out/kernel/v
I0512 23:45:15.331626 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_7/self_attention/out/kernel/v_col
I0512 23:45:15.331656 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_7/self_attention/out/kernel/v_row
I0512 23:45:15.331686 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_7/self_attention/query/kernel/m
I0512 23:45:15.331715 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_7/self_attention/query/kernel/v
I0512 23:45:15.331745 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_7/self_attention/query/kernel/v_col
I0512 23:45:15.331775 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_7/self_attention/query/kernel/v_row
I0512 23:45:15.331804 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_7/self_attention/value/kernel/m
I0512 23:45:15.331834 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_7/self_attention/value/kernel/v
I0512 23:45:15.331863 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_7/self_attention/value/kernel/v_col
I0512 23:45:15.331893 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_7/self_attention/value/kernel/v_row
I0512 23:45:15.331922 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_8/mlp/wi_0/kernel/m
I0512 23:45:15.331952 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_8/mlp/wi_0/kernel/v
I0512 23:45:15.331982 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_8/mlp/wi_0/kernel/v_col
I0512 23:45:15.332012 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_8/mlp/wi_0/kernel/v_row
I0512 23:45:15.332042 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_8/mlp/wi_1/kernel/m
I0512 23:45:15.332072 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_8/mlp/wi_1/kernel/v
I0512 23:45:15.332102 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_8/mlp/wi_1/kernel/v_col
I0512 23:45:15.332132 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_8/mlp/wi_1/kernel/v_row
I0512 23:45:15.332161 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_8/mlp/wo/kernel/m
I0512 23:45:15.332191 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_8/mlp/wo/kernel/v
I0512 23:45:15.332221 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_8/mlp/wo/kernel/v_col
I0512 23:45:15.332257 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_8/mlp/wo/kernel/v_row
I0512 23:45:15.332287 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_8/pre_mlp_layer_norm/scale/m
I0512 23:45:15.332337 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_8/pre_mlp_layer_norm/scale/v
I0512 23:45:15.332369 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_8/pre_mlp_layer_norm/scale/v_col
I0512 23:45:15.332399 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_8/pre_mlp_layer_norm/scale/v_row
I0512 23:45:15.332429 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_8/pre_self_attention_layer_norm/scale/m
I0512 23:45:15.332459 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_8/pre_self_attention_layer_norm/scale/v
I0512 23:45:15.332489 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_8/pre_self_attention_layer_norm/scale/v_col
I0512 23:45:15.332521 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_8/pre_self_attention_layer_norm/scale/v_row
I0512 23:45:15.332551 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_8/self_attention/key/kernel/m
I0512 23:45:15.332580 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_8/self_attention/key/kernel/v
I0512 23:45:15.332610 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_8/self_attention/key/kernel/v_col
I0512 23:45:15.332640 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_8/self_attention/key/kernel/v_row
I0512 23:45:15.332669 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_8/self_attention/out/kernel/m
I0512 23:45:15.332700 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_8/self_attention/out/kernel/v
I0512 23:45:15.332730 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_8/self_attention/out/kernel/v_col
I0512 23:45:15.332760 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_8/self_attention/out/kernel/v_row
I0512 23:45:15.332789 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_8/self_attention/query/kernel/m
I0512 23:45:15.332819 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_8/self_attention/query/kernel/v
I0512 23:45:15.332849 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_8/self_attention/query/kernel/v_col
I0512 23:45:15.332879 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_8/self_attention/query/kernel/v_row
I0512 23:45:15.332908 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_8/self_attention/value/kernel/m
I0512 23:45:15.332937 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_8/self_attention/value/kernel/v
I0512 23:45:15.332967 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_8/self_attention/value/kernel/v_col
I0512 23:45:15.332997 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_8/self_attention/value/kernel/v_row
I0512 23:45:15.333026 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_9/mlp/wi_0/kernel/m
I0512 23:45:15.333056 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_9/mlp/wi_0/kernel/v
I0512 23:45:15.333086 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_9/mlp/wi_0/kernel/v_col
I0512 23:45:15.333115 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_9/mlp/wi_0/kernel/v_row
I0512 23:45:15.333145 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_9/mlp/wi_1/kernel/m
I0512 23:45:15.333174 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_9/mlp/wi_1/kernel/v
I0512 23:45:15.333204 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_9/mlp/wi_1/kernel/v_col
I0512 23:45:15.333233 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_9/mlp/wi_1/kernel/v_row
I0512 23:45:15.333269 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_9/mlp/wo/kernel/m
I0512 23:45:15.333299 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_9/mlp/wo/kernel/v
I0512 23:45:15.333329 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_9/mlp/wo/kernel/v_col
I0512 23:45:15.333358 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_9/mlp/wo/kernel/v_row
I0512 23:45:15.333388 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_9/pre_mlp_layer_norm/scale/m
I0512 23:45:15.333418 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_9/pre_mlp_layer_norm/scale/v
I0512 23:45:15.333447 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_9/pre_mlp_layer_norm/scale/v_col
I0512 23:45:15.333477 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_9/pre_mlp_layer_norm/scale/v_row
I0512 23:45:15.333509 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_9/pre_self_attention_layer_norm/scale/m
I0512 23:45:15.333539 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_9/pre_self_attention_layer_norm/scale/v
I0512 23:45:15.333569 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_9/pre_self_attention_layer_norm/scale/v_col
I0512 23:45:15.333599 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_9/pre_self_attention_layer_norm/scale/v_row
I0512 23:45:15.333628 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_9/self_attention/key/kernel/m
I0512 23:45:15.333658 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_9/self_attention/key/kernel/v
I0512 23:45:15.333688 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_9/self_attention/key/kernel/v_col
I0512 23:45:15.333717 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_9/self_attention/key/kernel/v_row
I0512 23:45:15.333747 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_9/self_attention/out/kernel/m
I0512 23:45:15.333776 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_9/self_attention/out/kernel/v
I0512 23:45:15.333806 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_9/self_attention/out/kernel/v_col
I0512 23:45:15.333835 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_9/self_attention/out/kernel/v_row
I0512 23:45:15.333865 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_9/self_attention/query/kernel/m
I0512 23:45:15.333894 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_9/self_attention/query/kernel/v
I0512 23:45:15.333924 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_9/self_attention/query/kernel/v_col
I0512 23:45:15.333953 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_9/self_attention/query/kernel/v_row
I0512 23:45:15.333982 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_9/self_attention/value/kernel/m
I0512 23:45:15.334012 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_9/self_attention/value/kernel/v
I0512 23:45:15.334042 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_9/self_attention/value/kernel/v_col
I0512 23:45:15.334072 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_9/self_attention/value/kernel/v_row
I0512 23:45:15.334101 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/logits_dense/kernel/m
I0512 23:45:15.334131 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/logits_dense/kernel/v
I0512 23:45:15.334160 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/logits_dense/kernel/v_col
I0512 23:45:15.334190 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/logits_dense/kernel/v_row
I0512 23:45:15.334219 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/token_embedder/embedding/m
I0512 23:45:15.334255 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/token_embedder/embedding/v
I0512 23:45:15.334285 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/token_embedder/embedding/v_col
I0512 23:45:15.334315 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/param_states/token_embedder/embedding/v_row
I0512 23:45:15.334344 140193652815872 checkpoints.py:1100] Restoring key from ckpt: state/step
I0512 23:45:15.334375 140193652815872 checkpoints.py:1100] Restoring key from ckpt: target/decoder/decoder_norm/scale
I0512 23:45:15.334406 140193652815872 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_0/mlp/wi_0/kernel
I0512 23:45:15.334435 140193652815872 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_0/mlp/wi_1/kernel
I0512 23:45:15.334465 140193652815872 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_0/mlp/wo/kernel
I0512 23:45:15.334496 140193652815872 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_0/pre_mlp_layer_norm/scale
I0512 23:45:15.334527 140193652815872 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_0/pre_self_attention_layer_norm/scale
I0512 23:45:15.334558 140193652815872 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_0/self_attention/key/kernel
I0512 23:45:15.334588 140193652815872 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_0/self_attention/out/kernel
I0512 23:45:15.334617 140193652815872 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_0/self_attention/query/kernel
I0512 23:45:15.334647 140193652815872 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_0/self_attention/value/kernel
I0512 23:45:15.334676 140193652815872 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_1/mlp/wi_0/kernel
I0512 23:45:15.334706 140193652815872 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_1/mlp/wi_1/kernel
I0512 23:45:15.334735 140193652815872 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_1/mlp/wo/kernel
I0512 23:45:15.334765 140193652815872 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_1/pre_mlp_layer_norm/scale
I0512 23:45:15.334795 140193652815872 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_1/pre_self_attention_layer_norm/scale
I0512 23:45:15.334825 140193652815872 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_1/self_attention/key/kernel
I0512 23:45:15.334855 140193652815872 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_1/self_attention/out/kernel
I0512 23:45:15.334885 140193652815872 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_1/self_attention/query/kernel
I0512 23:45:15.334914 140193652815872 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_1/self_attention/value/kernel
I0512 23:45:15.334944 140193652815872 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_10/mlp/wi_0/kernel
I0512 23:45:15.334974 140193652815872 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_10/mlp/wi_1/kernel
I0512 23:45:15.335003 140193652815872 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_10/mlp/wo/kernel
I0512 23:45:15.335033 140193652815872 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_10/pre_mlp_layer_norm/scale
I0512 23:45:15.335062 140193652815872 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_10/pre_self_attention_layer_norm/scale
I0512 23:45:15.335092 140193652815872 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_10/self_attention/key/kernel
I0512 23:45:15.335121 140193652815872 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_10/self_attention/out/kernel
I0512 23:45:15.335151 140193652815872 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_10/self_attention/query/kernel
I0512 23:45:15.335180 140193652815872 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_10/self_attention/value/kernel
I0512 23:45:15.335211 140193652815872 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_11/extra_mlp/wi_0/kernel
I0512 23:45:15.335246 140193652815872 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_11/extra_mlp/wi_1/kernel
I0512 23:45:15.335277 140193652815872 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_11/extra_mlp/wo/kernel
I0512 23:45:15.335307 140193652815872 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_11/mlp/expert/wi_0/kernel
I0512 23:45:15.335337 140193652815872 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_11/mlp/expert/wi_1/kernel
I0512 23:45:15.335367 140193652815872 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_11/mlp/expert/wo/kernel
I0512 23:45:15.335396 140193652815872 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_11/mlp/router/router_weights/w/kernel
I0512 23:45:15.335426 140193652815872 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_11/pre_extra_mlp_layer_norm/scale
I0512 23:45:15.335455 140193652815872 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_11/pre_mlp_layer_norm/scale
I0512 23:45:15.335485 140193652815872 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_11/pre_self_attention_layer_norm/scale
I0512 23:45:15.335516 140193652815872 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_11/self_attention/key/kernel
I0512 23:45:15.335546 140193652815872 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_11/self_attention/out/kernel
I0512 23:45:15.335576 140193652815872 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_11/self_attention/query/kernel
I0512 23:45:15.335606 140193652815872 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_11/self_attention/value/kernel
I0512 23:45:15.335635 140193652815872 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_12/mlp/wi_0/kernel
I0512 23:45:15.335665 140193652815872 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_12/mlp/wi_1/kernel
I0512 23:45:15.335695 140193652815872 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_12/mlp/wo/kernel
I0512 23:45:15.335724 140193652815872 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_12/pre_mlp_layer_norm/scale
I0512 23:45:15.335754 140193652815872 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_12/pre_self_attention_layer_norm/scale
I0512 23:45:15.335783 140193652815872 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_12/self_attention/key/kernel
I0512 23:45:15.335813 140193652815872 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_12/self_attention/out/kernel
I0512 23:45:15.335843 140193652815872 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_12/self_attention/query/kernel
I0512 23:45:15.335873 140193652815872 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_12/self_attention/value/kernel
I0512 23:45:15.335902 140193652815872 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_13/mlp/wi_0/kernel
I0512 23:45:15.335932 140193652815872 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_13/mlp/wi_1/kernel
I0512 23:45:15.335962 140193652815872 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_13/mlp/wo/kernel
I0512 23:45:15.335992 140193652815872 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_13/pre_mlp_layer_norm/scale
I0512 23:45:15.336021 140193652815872 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_13/pre_self_attention_layer_norm/scale
I0512 23:45:15.336052 140193652815872 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_13/self_attention/key/kernel
I0512 23:45:15.336081 140193652815872 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_13/self_attention/out/kernel
I0512 23:45:15.336111 140193652815872 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_13/self_attention/query/kernel
I0512 23:45:15.336141 140193652815872 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_13/self_attention/value/kernel
I0512 23:45:15.336171 140193652815872 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_14/mlp/wi_0/kernel
I0512 23:45:15.336201 140193652815872 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_14/mlp/wi_1/kernel
I0512 23:45:15.336231 140193652815872 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_14/mlp/wo/kernel
I0512 23:45:15.336266 140193652815872 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_14/pre_mlp_layer_norm/scale
I0512 23:45:15.336315 140193652815872 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_14/pre_self_attention_layer_norm/scale
I0512 23:45:15.336350 140193652815872 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_14/self_attention/key/kernel
I0512 23:45:15.336380 140193652815872 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_14/self_attention/out/kernel
I0512 23:45:15.336410 140193652815872 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_14/self_attention/query/kernel
I0512 23:45:15.336440 140193652815872 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_14/self_attention/value/kernel
I0512 23:45:15.336470 140193652815872 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_15/mlp/wi_0/kernel
I0512 23:45:15.336501 140193652815872 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_15/mlp/wi_1/kernel
I0512 23:45:15.336532 140193652815872 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_15/mlp/wo/kernel
I0512 23:45:15.336562 140193652815872 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_15/pre_mlp_layer_norm/scale
I0512 23:45:15.336592 140193652815872 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_15/pre_self_attention_layer_norm/scale
I0512 23:45:15.336623 140193652815872 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_15/self_attention/key/kernel
I0512 23:45:15.336652 140193652815872 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_15/self_attention/out/kernel
I0512 23:45:15.336682 140193652815872 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_15/self_attention/query/kernel
I0512 23:45:15.336712 140193652815872 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_15/self_attention/value/kernel
I0512 23:45:15.336742 140193652815872 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_16/mlp/wi_0/kernel
I0512 23:45:15.336772 140193652815872 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_16/mlp/wi_1/kernel
I0512 23:45:15.336802 140193652815872 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_16/mlp/wo/kernel
I0512 23:45:15.336831 140193652815872 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_16/pre_mlp_layer_norm/scale
I0512 23:45:15.336861 140193652815872 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_16/pre_self_attention_layer_norm/scale
I0512 23:45:15.336890 140193652815872 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_16/self_attention/key/kernel
I0512 23:45:15.336920 140193652815872 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_16/self_attention/out/kernel
I0512 23:45:15.336950 140193652815872 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_16/self_attention/query/kernel
I0512 23:45:15.336980 140193652815872 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_16/self_attention/value/kernel
I0512 23:45:15.337009 140193652815872 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_17/extra_mlp/wi_0/kernel
I0512 23:45:15.337039 140193652815872 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_17/extra_mlp/wi_1/kernel
I0512 23:45:15.337068 140193652815872 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_17/extra_mlp/wo/kernel
I0512 23:45:15.337098 140193652815872 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_17/mlp/expert/wi_0/kernel
I0512 23:45:15.337128 140193652815872 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_17/mlp/expert/wi_1/kernel
I0512 23:45:15.337158 140193652815872 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_17/mlp/expert/wo/kernel
I0512 23:45:15.337187 140193652815872 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_17/mlp/router/router_weights/w/kernel
I0512 23:45:15.337217 140193652815872 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_17/pre_extra_mlp_layer_norm/scale
I0512 23:45:15.337252 140193652815872 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_17/pre_mlp_layer_norm/scale
I0512 23:45:15.337283 140193652815872 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_17/pre_self_attention_layer_norm/scale
I0512 23:45:15.337313 140193652815872 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_17/self_attention/key/kernel
I0512 23:45:15.337343 140193652815872 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_17/self_attention/out/kernel
I0512 23:45:15.337373 140193652815872 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_17/self_attention/query/kernel
I0512 23:45:15.337402 140193652815872 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_17/self_attention/value/kernel
I0512 23:45:15.337432 140193652815872 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_18/mlp/wi_0/kernel
I0512 23:45:15.337461 140193652815872 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_18/mlp/wi_1/kernel
I0512 23:45:15.337492 140193652815872 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_18/mlp/wo/kernel
I0512 23:45:15.337523 140193652815872 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_18/pre_mlp_layer_norm/scale
I0512 23:45:15.337553 140193652815872 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_18/pre_self_attention_layer_norm/scale
I0512 23:45:15.337583 140193652815872 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_18/self_attention/key/kernel
I0512 23:45:15.337613 140193652815872 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_18/self_attention/out/kernel
I0512 23:45:15.337643 140193652815872 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_18/self_attention/query/kernel
I0512 23:45:15.337673 140193652815872 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_18/self_attention/value/kernel
I0512 23:45:15.337702 140193652815872 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_19/mlp/wi_0/kernel
I0512 23:45:15.337732 140193652815872 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_19/mlp/wi_1/kernel
I0512 23:45:15.337762 140193652815872 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_19/mlp/wo/kernel
I0512 23:45:15.337792 140193652815872 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_19/pre_mlp_layer_norm/scale
I0512 23:45:15.337822 140193652815872 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_19/pre_self_attention_layer_norm/scale
I0512 23:45:15.337852 140193652815872 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_19/self_attention/key/kernel
I0512 23:45:15.337882 140193652815872 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_19/self_attention/out/kernel
I0512 23:45:15.337911 140193652815872 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_19/self_attention/query/kernel
I0512 23:45:15.337941 140193652815872 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_19/self_attention/value/kernel
I0512 23:45:15.337971 140193652815872 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_2/mlp/wi_0/kernel
I0512 23:45:15.338001 140193652815872 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_2/mlp/wi_1/kernel
I0512 23:45:15.338030 140193652815872 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_2/mlp/wo/kernel
I0512 23:45:15.338060 140193652815872 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_2/pre_mlp_layer_norm/scale
I0512 23:45:15.338090 140193652815872 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_2/pre_self_attention_layer_norm/scale
I0512 23:45:15.338120 140193652815872 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_2/self_attention/key/kernel
I0512 23:45:15.338150 140193652815872 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_2/self_attention/out/kernel
I0512 23:45:15.338180 140193652815872 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_2/self_attention/query/kernel
I0512 23:45:15.338210 140193652815872 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_2/self_attention/value/kernel
I0512 23:45:15.338244 140193652815872 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_20/mlp/wi_0/kernel
I0512 23:45:15.338275 140193652815872 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_20/mlp/wi_1/kernel
I0512 23:45:15.338305 140193652815872 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_20/mlp/wo/kernel
I0512 23:45:15.338335 140193652815872 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_20/pre_mlp_layer_norm/scale
I0512 23:45:15.338364 140193652815872 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_20/pre_self_attention_layer_norm/scale
I0512 23:45:15.338394 140193652815872 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_20/self_attention/key/kernel
I0512 23:45:15.338423 140193652815872 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_20/self_attention/out/kernel
I0512 23:45:15.338453 140193652815872 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_20/self_attention/query/kernel
I0512 23:45:15.338483 140193652815872 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_20/self_attention/value/kernel
I0512 23:45:15.338515 140193652815872 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_21/mlp/wi_0/kernel
I0512 23:45:15.338545 140193652815872 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_21/mlp/wi_1/kernel
I0512 23:45:15.338575 140193652815872 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_21/mlp/wo/kernel
I0512 23:45:15.338604 140193652815872 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_21/pre_mlp_layer_norm/scale
I0512 23:45:15.338634 140193652815872 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_21/pre_self_attention_layer_norm/scale
I0512 23:45:15.338664 140193652815872 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_21/self_attention/key/kernel
I0512 23:45:15.338694 140193652815872 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_21/self_attention/out/kernel
I0512 23:45:15.338723 140193652815872 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_21/self_attention/query/kernel
I0512 23:45:15.338753 140193652815872 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_21/self_attention/value/kernel
I0512 23:45:15.338783 140193652815872 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_22/mlp/wi_0/kernel
I0512 23:45:15.338812 140193652815872 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_22/mlp/wi_1/kernel
I0512 23:45:15.338842 140193652815872 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_22/mlp/wo/kernel
I0512 23:45:15.338872 140193652815872 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_22/pre_mlp_layer_norm/scale
I0512 23:45:15.338901 140193652815872 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_22/pre_self_attention_layer_norm/scale
I0512 23:45:15.338931 140193652815872 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_22/self_attention/key/kernel
I0512 23:45:15.338961 140193652815872 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_22/self_attention/out/kernel
I0512 23:45:15.338991 140193652815872 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_22/self_attention/query/kernel
I0512 23:45:15.339021 140193652815872 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_22/self_attention/value/kernel
I0512 23:45:15.339051 140193652815872 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_23/extra_mlp/wi_0/kernel
I0512 23:45:15.339081 140193652815872 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_23/extra_mlp/wi_1/kernel
I0512 23:45:15.339110 140193652815872 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_23/extra_mlp/wo/kernel
I0512 23:45:15.339140 140193652815872 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_23/mlp/expert/wi_0/kernel
I0512 23:45:15.339170 140193652815872 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_23/mlp/expert/wi_1/kernel
I0512 23:45:15.339200 140193652815872 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_23/mlp/expert/wo/kernel
I0512 23:45:15.339229 140193652815872 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_23/mlp/router/router_weights/w/kernel
I0512 23:45:15.339265 140193652815872 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_23/pre_extra_mlp_layer_norm/scale
I0512 23:45:15.339296 140193652815872 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_23/pre_mlp_layer_norm/scale
I0512 23:45:15.339325 140193652815872 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_23/pre_self_attention_layer_norm/scale
I0512 23:45:15.339355 140193652815872 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_23/self_attention/key/kernel
I0512 23:45:15.339385 140193652815872 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_23/self_attention/out/kernel
I0512 23:45:15.339415 140193652815872 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_23/self_attention/query/kernel
I0512 23:45:15.339444 140193652815872 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_23/self_attention/value/kernel
I0512 23:45:15.339474 140193652815872 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_3/mlp/wi_0/kernel
I0512 23:45:15.339505 140193652815872 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_3/mlp/wi_1/kernel
I0512 23:45:15.339536 140193652815872 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_3/mlp/wo/kernel
I0512 23:45:15.339566 140193652815872 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_3/pre_mlp_layer_norm/scale
I0512 23:45:15.339596 140193652815872 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_3/pre_self_attention_layer_norm/scale
I0512 23:45:15.339626 140193652815872 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_3/self_attention/key/kernel
I0512 23:45:15.339656 140193652815872 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_3/self_attention/out/kernel
I0512 23:45:15.339685 140193652815872 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_3/self_attention/query/kernel
I0512 23:45:15.339715 140193652815872 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_3/self_attention/value/kernel
I0512 23:45:15.339745 140193652815872 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_4/mlp/wi_0/kernel
I0512 23:45:15.339775 140193652815872 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_4/mlp/wi_1/kernel
I0512 23:45:15.339804 140193652815872 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_4/mlp/wo/kernel
I0512 23:45:15.339834 140193652815872 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_4/pre_mlp_layer_norm/scale
I0512 23:45:15.339864 140193652815872 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_4/pre_self_attention_layer_norm/scale
I0512 23:45:15.339894 140193652815872 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_4/self_attention/key/kernel
I0512 23:45:15.339924 140193652815872 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_4/self_attention/out/kernel
I0512 23:45:15.339954 140193652815872 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_4/self_attention/query/kernel
I0512 23:45:15.339984 140193652815872 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_4/self_attention/value/kernel
I0512 23:45:15.340013 140193652815872 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_5/extra_mlp/wi_0/kernel
I0512 23:45:15.340044 140193652815872 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_5/extra_mlp/wi_1/kernel
I0512 23:45:15.340073 140193652815872 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_5/extra_mlp/wo/kernel
I0512 23:45:15.340103 140193652815872 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_5/mlp/expert/wi_0/kernel
I0512 23:45:15.340133 140193652815872 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_5/mlp/expert/wi_1/kernel
I0512 23:45:15.340162 140193652815872 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_5/mlp/expert/wo/kernel
I0512 23:45:15.340192 140193652815872 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_5/mlp/router/router_weights/w/kernel
I0512 23:45:15.340222 140193652815872 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_5/pre_extra_mlp_layer_norm/scale
I0512 23:45:15.340257 140193652815872 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_5/pre_mlp_layer_norm/scale
I0512 23:45:15.340287 140193652815872 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_5/pre_self_attention_layer_norm/scale
I0512 23:45:15.340346 140193652815872 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_5/self_attention/key/kernel
I0512 23:45:15.340377 140193652815872 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_5/self_attention/out/kernel
I0512 23:45:15.340407 140193652815872 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_5/self_attention/query/kernel
I0512 23:45:15.340436 140193652815872 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_5/self_attention/value/kernel
I0512 23:45:15.340466 140193652815872 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_6/mlp/wi_0/kernel
I0512 23:45:15.340498 140193652815872 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_6/mlp/wi_1/kernel
I0512 23:45:15.340529 140193652815872 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_6/mlp/wo/kernel
I0512 23:45:15.340559 140193652815872 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_6/pre_mlp_layer_norm/scale
I0512 23:45:15.340589 140193652815872 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_6/pre_self_attention_layer_norm/scale
I0512 23:45:15.340619 140193652815872 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_6/self_attention/key/kernel
I0512 23:45:15.340649 140193652815872 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_6/self_attention/out/kernel
I0512 23:45:15.340679 140193652815872 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_6/self_attention/query/kernel
I0512 23:45:15.340709 140193652815872 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_6/self_attention/value/kernel
I0512 23:45:15.340738 140193652815872 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_7/mlp/wi_0/kernel
I0512 23:45:15.340768 140193652815872 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_7/mlp/wi_1/kernel
I0512 23:45:15.340798 140193652815872 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_7/mlp/wo/kernel
I0512 23:45:15.340828 140193652815872 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_7/pre_mlp_layer_norm/scale
I0512 23:45:15.340858 140193652815872 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_7/pre_self_attention_layer_norm/scale
I0512 23:45:15.340888 140193652815872 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_7/self_attention/key/kernel
I0512 23:45:15.340918 140193652815872 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_7/self_attention/out/kernel
I0512 23:45:15.340948 140193652815872 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_7/self_attention/query/kernel
I0512 23:45:15.340978 140193652815872 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_7/self_attention/value/kernel
I0512 23:45:15.341008 140193652815872 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_8/mlp/wi_0/kernel
I0512 23:45:15.341038 140193652815872 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_8/mlp/wi_1/kernel
I0512 23:45:15.341068 140193652815872 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_8/mlp/wo/kernel
I0512 23:45:15.341098 140193652815872 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_8/pre_mlp_layer_norm/scale
I0512 23:45:15.341127 140193652815872 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_8/pre_self_attention_layer_norm/scale
I0512 23:45:15.341157 140193652815872 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_8/self_attention/key/kernel
I0512 23:45:15.341187 140193652815872 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_8/self_attention/out/kernel
I0512 23:45:15.341217 140193652815872 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_8/self_attention/query/kernel
I0512 23:45:15.341255 140193652815872 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_8/self_attention/value/kernel
I0512 23:45:15.341286 140193652815872 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_9/mlp/wi_0/kernel
I0512 23:45:15.341316 140193652815872 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_9/mlp/wi_1/kernel
I0512 23:45:15.341346 140193652815872 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_9/mlp/wo/kernel
I0512 23:45:15.341376 140193652815872 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_9/pre_mlp_layer_norm/scale
I0512 23:45:15.341406 140193652815872 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_9/pre_self_attention_layer_norm/scale
I0512 23:45:15.341436 140193652815872 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_9/self_attention/key/kernel
I0512 23:45:15.341465 140193652815872 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_9/self_attention/out/kernel
I0512 23:45:15.341497 140193652815872 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_9/self_attention/query/kernel
I0512 23:45:15.341528 140193652815872 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_9/self_attention/value/kernel
I0512 23:45:15.341558 140193652815872 checkpoints.py:1100] Restoring key from ckpt: target/decoder/logits_dense/kernel
I0512 23:45:15.341588 140193652815872 checkpoints.py:1100] Restoring key from ckpt: target/token_embedder/embedding
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
I0000 00:00:1715557515.374905  374664 gcs_resource.cc:109] Using default AdmissionQueue with limit 32
I0000 00:00:1715557515.379665  376857 google_auth_provider.cc:180] Running on GCE, using service account 1081612546678-compute@developer.gserviceaccount.com
I0512 23:45:15.586790 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/decoder_norm/scale/m
I0512 23:45:15.586969 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/decoder_norm/scale/v
I0512 23:45:15.587009 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/decoder_norm/scale/v_col
I0512 23:45:15.587042 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/decoder_norm/scale/v_row
I0512 23:45:15.587081 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_0/mlp/wi_0/kernel/m
I0512 23:45:15.587113 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_0/mlp/wi_0/kernel/v
I0512 23:45:15.587148 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_0/mlp/wi_0/kernel/v_col
I0512 23:45:15.587180 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_0/mlp/wi_0/kernel/v_row
I0512 23:45:15.587217 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_0/mlp/wi_1/kernel/m
I0512 23:45:15.587252 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_0/mlp/wi_1/kernel/v
I0512 23:45:15.587284 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_0/mlp/wi_1/kernel/v_col
I0512 23:45:15.587318 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_0/mlp/wi_1/kernel/v_row
I0512 23:45:15.587352 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_0/mlp/wo/kernel/m
I0512 23:45:15.587382 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_0/mlp/wo/kernel/v
I0512 23:45:15.587416 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_0/mlp/wo/kernel/v_col
I0512 23:45:15.587450 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_0/mlp/wo/kernel/v_row
I0512 23:45:15.587483 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_0/pre_mlp_layer_norm/scale/m
I0512 23:45:15.587518 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_0/pre_mlp_layer_norm/scale/v
I0512 23:45:15.587550 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_0/pre_mlp_layer_norm/scale/v_col
I0512 23:45:15.587584 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_0/pre_mlp_layer_norm/scale/v_row
I0512 23:45:15.587618 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_0/pre_self_attention_layer_norm/scale/m
I0512 23:45:15.587649 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_0/pre_self_attention_layer_norm/scale/v
I0512 23:45:15.587682 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_0/pre_self_attention_layer_norm/scale/v_col
I0512 23:45:15.587713 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_0/pre_self_attention_layer_norm/scale/v_row
I0512 23:45:15.587747 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_0/self_attention/key/kernel/m
I0512 23:45:15.587781 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_0/self_attention/key/kernel/v
I0512 23:45:15.587812 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_0/self_attention/key/kernel/v_col
I0512 23:45:15.587845 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_0/self_attention/key/kernel/v_row
I0512 23:45:15.587879 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_0/self_attention/out/kernel/m
I0512 23:45:15.587910 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_0/self_attention/out/kernel/v
I0512 23:45:15.587941 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_0/self_attention/out/kernel/v_col
I0512 23:45:15.587972 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_0/self_attention/out/kernel/v_row
I0512 23:45:15.588002 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_0/self_attention/query/kernel/m
I0512 23:45:15.588033 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_0/self_attention/query/kernel/v
I0512 23:45:15.588063 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_0/self_attention/query/kernel/v_col
I0512 23:45:15.588094 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_0/self_attention/query/kernel/v_row
I0512 23:45:15.588124 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_0/self_attention/value/kernel/m
I0512 23:45:15.588155 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_0/self_attention/value/kernel/v
I0512 23:45:15.588185 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_0/self_attention/value/kernel/v_col
I0512 23:45:15.588222 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_0/self_attention/value/kernel/v_row
I0512 23:45:15.588253 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_1/mlp/wi_0/kernel/m
I0512 23:45:15.588284 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_1/mlp/wi_0/kernel/v
I0512 23:45:15.588315 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_1/mlp/wi_0/kernel/v_col
I0512 23:45:15.588345 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_1/mlp/wi_0/kernel/v_row
I0512 23:45:15.588376 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_1/mlp/wi_1/kernel/m
I0512 23:45:15.588406 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_1/mlp/wi_1/kernel/v
I0512 23:45:15.588437 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_1/mlp/wi_1/kernel/v_col
I0512 23:45:15.588469 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_1/mlp/wi_1/kernel/v_row
I0512 23:45:15.588501 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_1/mlp/wo/kernel/m
I0512 23:45:15.588532 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_1/mlp/wo/kernel/v
I0512 23:45:15.588562 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_1/mlp/wo/kernel/v_col
I0512 23:45:15.588593 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_1/mlp/wo/kernel/v_row
I0512 23:45:15.588624 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_1/pre_mlp_layer_norm/scale/m
I0512 23:45:15.588655 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_1/pre_mlp_layer_norm/scale/v
I0512 23:45:15.588685 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_1/pre_mlp_layer_norm/scale/v_col
I0512 23:45:15.588716 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_1/pre_mlp_layer_norm/scale/v_row
I0512 23:45:15.588746 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_1/pre_self_attention_layer_norm/scale/m
I0512 23:45:15.588777 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_1/pre_self_attention_layer_norm/scale/v
I0512 23:45:15.588807 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_1/pre_self_attention_layer_norm/scale/v_col
I0512 23:45:15.588838 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_1/pre_self_attention_layer_norm/scale/v_row
I0512 23:45:15.588868 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_1/self_attention/key/kernel/m
I0512 23:45:15.588899 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_1/self_attention/key/kernel/v
I0512 23:45:15.588930 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_1/self_attention/key/kernel/v_col
I0512 23:45:15.588960 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_1/self_attention/key/kernel/v_row
I0512 23:45:15.588990 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_1/self_attention/out/kernel/m
I0512 23:45:15.589020 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_1/self_attention/out/kernel/v
I0512 23:45:15.589051 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_1/self_attention/out/kernel/v_col
I0512 23:45:15.589081 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_1/self_attention/out/kernel/v_row
I0512 23:45:15.589111 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_1/self_attention/query/kernel/m
I0512 23:45:15.589142 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_1/self_attention/query/kernel/v
I0512 23:45:15.589172 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_1/self_attention/query/kernel/v_col
I0512 23:45:15.589207 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_1/self_attention/query/kernel/v_row
I0512 23:45:15.589240 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_1/self_attention/value/kernel/m
I0512 23:45:15.589271 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_1/self_attention/value/kernel/v
I0512 23:45:15.589301 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_1/self_attention/value/kernel/v_col
I0512 23:45:15.589331 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_1/self_attention/value/kernel/v_row
I0512 23:45:15.589362 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_10/mlp/wi_0/kernel/m
I0512 23:45:15.589393 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_10/mlp/wi_0/kernel/v
I0512 23:45:15.589423 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_10/mlp/wi_0/kernel/v_col
I0512 23:45:15.589454 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_10/mlp/wi_0/kernel/v_row
I0512 23:45:15.589487 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_10/mlp/wi_1/kernel/m
I0512 23:45:15.589518 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_10/mlp/wi_1/kernel/v
I0512 23:45:15.589548 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_10/mlp/wi_1/kernel/v_col
I0512 23:45:15.589579 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_10/mlp/wi_1/kernel/v_row
I0512 23:45:15.589609 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_10/mlp/wo/kernel/m
I0512 23:45:15.589640 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_10/mlp/wo/kernel/v
I0512 23:45:15.589670 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_10/mlp/wo/kernel/v_col
I0512 23:45:15.589701 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_10/mlp/wo/kernel/v_row
I0512 23:45:15.589731 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_10/pre_mlp_layer_norm/scale/m
I0512 23:45:15.589761 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_10/pre_mlp_layer_norm/scale/v
I0512 23:45:15.589792 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_10/pre_mlp_layer_norm/scale/v_col
I0512 23:45:15.589823 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_10/pre_mlp_layer_norm/scale/v_row
I0512 23:45:15.589853 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_10/pre_self_attention_layer_norm/scale/m
I0512 23:45:15.589883 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_10/pre_self_attention_layer_norm/scale/v
I0512 23:45:15.589914 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_10/pre_self_attention_layer_norm/scale/v_col
I0512 23:45:15.589944 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_10/pre_self_attention_layer_norm/scale/v_row
I0512 23:45:15.589975 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_10/self_attention/key/kernel/m
I0512 23:45:15.590005 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_10/self_attention/key/kernel/v
I0512 23:45:15.590035 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_10/self_attention/key/kernel/v_col
I0512 23:45:15.590066 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_10/self_attention/key/kernel/v_row
I0512 23:45:15.590096 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_10/self_attention/out/kernel/m
I0512 23:45:15.590126 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_10/self_attention/out/kernel/v
I0512 23:45:15.590157 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_10/self_attention/out/kernel/v_col
I0512 23:45:15.590188 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_10/self_attention/out/kernel/v_row
I0512 23:45:15.590229 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_10/self_attention/query/kernel/m
I0512 23:45:15.590261 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_10/self_attention/query/kernel/v
I0512 23:45:15.590291 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_10/self_attention/query/kernel/v_col
I0512 23:45:15.590322 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_10/self_attention/query/kernel/v_row
I0512 23:45:15.590352 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_10/self_attention/value/kernel/m
I0512 23:45:15.590382 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_10/self_attention/value/kernel/v
I0512 23:45:15.590413 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_10/self_attention/value/kernel/v_col
I0512 23:45:15.590443 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_10/self_attention/value/kernel/v_row
I0512 23:45:15.590476 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/extra_mlp/wi_0/kernel/m
I0512 23:45:15.590507 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/extra_mlp/wi_0/kernel/v
I0512 23:45:15.590538 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/extra_mlp/wi_0/kernel/v_col
I0512 23:45:15.590568 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/extra_mlp/wi_0/kernel/v_row
I0512 23:45:15.590599 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/extra_mlp/wi_1/kernel/m
I0512 23:45:15.590629 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/extra_mlp/wi_1/kernel/v
I0512 23:45:15.590659 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/extra_mlp/wi_1/kernel/v_col
I0512 23:45:15.590708 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/extra_mlp/wi_1/kernel/v_row
I0512 23:45:15.590746 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/extra_mlp/wo/kernel/m
I0512 23:45:15.590778 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/extra_mlp/wo/kernel/v
I0512 23:45:15.590809 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/extra_mlp/wo/kernel/v_col
I0512 23:45:15.590839 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/extra_mlp/wo/kernel/v_row
I0512 23:45:15.590869 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/mlp/expert/wi_0/kernel/m
I0512 23:45:15.590900 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/mlp/expert/wi_0/kernel/v
I0512 23:45:15.590931 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/mlp/expert/wi_0/kernel/v_col
I0512 23:45:15.590961 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/mlp/expert/wi_0/kernel/v_row
I0512 23:45:15.590992 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/mlp/expert/wi_1/kernel/m
I0512 23:45:15.591022 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/mlp/expert/wi_1/kernel/v
I0512 23:45:15.591056 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/mlp/expert/wi_1/kernel/v_col
I0512 23:45:15.591088 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/mlp/expert/wi_1/kernel/v_row
I0512 23:45:15.591118 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/mlp/expert/wo/kernel/m
I0512 23:45:15.591149 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/mlp/expert/wo/kernel/v
I0512 23:45:15.591179 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/mlp/expert/wo/kernel/v_col
I0512 23:45:15.591215 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/mlp/expert/wo/kernel/v_row
I0512 23:45:15.591247 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/mlp/router/router_weights/w/kernel/m
I0512 23:45:15.591278 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/mlp/router/router_weights/w/kernel/v
I0512 23:45:15.591308 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/mlp/router/router_weights/w/kernel/v_col
I0512 23:45:15.591339 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/mlp/router/router_weights/w/kernel/v_row
I0512 23:45:15.591369 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/pre_extra_mlp_layer_norm/scale/m
I0512 23:45:15.591400 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/pre_extra_mlp_layer_norm/scale/v
I0512 23:45:15.591430 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/pre_extra_mlp_layer_norm/scale/v_col
I0512 23:45:15.591462 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/pre_extra_mlp_layer_norm/scale/v_row
I0512 23:45:15.591494 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/pre_mlp_layer_norm/scale/m
I0512 23:45:15.591525 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/pre_mlp_layer_norm/scale/v
I0512 23:45:15.591556 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/pre_mlp_layer_norm/scale/v_col
I0512 23:45:15.591587 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/pre_mlp_layer_norm/scale/v_row
I0512 23:45:15.591617 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/pre_self_attention_layer_norm/scale/m
I0512 23:45:15.591648 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/pre_self_attention_layer_norm/scale/v
I0512 23:45:15.591678 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/pre_self_attention_layer_norm/scale/v_col
I0512 23:45:15.591709 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/pre_self_attention_layer_norm/scale/v_row
I0512 23:45:15.591739 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/self_attention/key/kernel/m
I0512 23:45:15.591770 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/self_attention/key/kernel/v
I0512 23:45:15.591801 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/self_attention/key/kernel/v_col
I0512 23:45:15.591831 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/self_attention/key/kernel/v_row
I0512 23:45:15.591861 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/self_attention/out/kernel/m
I0512 23:45:15.591892 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/self_attention/out/kernel/v
I0512 23:45:15.591922 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/self_attention/out/kernel/v_col
I0512 23:45:15.591953 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/self_attention/out/kernel/v_row
I0512 23:45:15.591983 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/self_attention/query/kernel/m
I0512 23:45:15.592014 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/self_attention/query/kernel/v
I0512 23:45:15.592044 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/self_attention/query/kernel/v_col
I0512 23:45:15.592075 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/self_attention/query/kernel/v_row
I0512 23:45:15.592105 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/self_attention/value/kernel/m
I0512 23:45:15.592135 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/self_attention/value/kernel/v
I0512 23:45:15.592166 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/self_attention/value/kernel/v_col
I0512 23:45:15.592196 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/self_attention/value/kernel/v_row
I0512 23:45:15.592233 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_12/mlp/wi_0/kernel/m
I0512 23:45:15.592264 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_12/mlp/wi_0/kernel/v
I0512 23:45:15.592295 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_12/mlp/wi_0/kernel/v_col
I0512 23:45:15.592326 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_12/mlp/wi_0/kernel/v_row
I0512 23:45:15.592356 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_12/mlp/wi_1/kernel/m
I0512 23:45:15.592386 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_12/mlp/wi_1/kernel/v
I0512 23:45:15.592418 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_12/mlp/wi_1/kernel/v_col
I0512 23:45:15.592449 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_12/mlp/wi_1/kernel/v_row
I0512 23:45:15.592482 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_12/mlp/wo/kernel/m
I0512 23:45:15.592514 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_12/mlp/wo/kernel/v
I0512 23:45:15.592545 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_12/mlp/wo/kernel/v_col
I0512 23:45:15.592575 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_12/mlp/wo/kernel/v_row
I0512 23:45:15.592606 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_12/pre_mlp_layer_norm/scale/m
I0512 23:45:15.592636 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_12/pre_mlp_layer_norm/scale/v
I0512 23:45:15.592667 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_12/pre_mlp_layer_norm/scale/v_col
I0512 23:45:15.592697 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_12/pre_mlp_layer_norm/scale/v_row
I0512 23:45:15.592728 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_12/pre_self_attention_layer_norm/scale/m
I0512 23:45:15.592758 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_12/pre_self_attention_layer_norm/scale/v
I0512 23:45:15.592788 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_12/pre_self_attention_layer_norm/scale/v_col
I0512 23:45:15.592819 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_12/pre_self_attention_layer_norm/scale/v_row
I0512 23:45:15.592850 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_12/self_attention/key/kernel/m
I0512 23:45:15.592880 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_12/self_attention/key/kernel/v
I0512 23:45:15.592911 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_12/self_attention/key/kernel/v_col
I0512 23:45:15.592941 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_12/self_attention/key/kernel/v_row
I0512 23:45:15.592972 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_12/self_attention/out/kernel/m
I0512 23:45:15.593003 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_12/self_attention/out/kernel/v
I0512 23:45:15.593033 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_12/self_attention/out/kernel/v_col
I0512 23:45:15.593063 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_12/self_attention/out/kernel/v_row
I0512 23:45:15.593094 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_12/self_attention/query/kernel/m
I0512 23:45:15.593124 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_12/self_attention/query/kernel/v
I0512 23:45:15.593155 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_12/self_attention/query/kernel/v_col
I0512 23:45:15.593186 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_12/self_attention/query/kernel/v_row
I0512 23:45:15.593224 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_12/self_attention/value/kernel/m
I0512 23:45:15.593257 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_12/self_attention/value/kernel/v
I0512 23:45:15.593288 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_12/self_attention/value/kernel/v_col
I0512 23:45:15.593319 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_12/self_attention/value/kernel/v_row
I0512 23:45:15.593350 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_13/mlp/wi_0/kernel/m
I0512 23:45:15.593381 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_13/mlp/wi_0/kernel/v
I0512 23:45:15.593412 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_13/mlp/wi_0/kernel/v_col
I0512 23:45:15.593443 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_13/mlp/wi_0/kernel/v_row
I0512 23:45:15.593477 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_13/mlp/wi_1/kernel/m
I0512 23:45:15.593509 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_13/mlp/wi_1/kernel/v
I0512 23:45:15.593540 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_13/mlp/wi_1/kernel/v_col
I0512 23:45:15.593571 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_13/mlp/wi_1/kernel/v_row
I0512 23:45:15.593603 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_13/mlp/wo/kernel/m
I0512 23:45:15.593634 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_13/mlp/wo/kernel/v
I0512 23:45:15.593665 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_13/mlp/wo/kernel/v_col
I0512 23:45:15.593696 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_13/mlp/wo/kernel/v_row
I0512 23:45:15.593727 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_13/pre_mlp_layer_norm/scale/m
I0512 23:45:15.593758 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_13/pre_mlp_layer_norm/scale/v
I0512 23:45:15.593789 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_13/pre_mlp_layer_norm/scale/v_col
I0512 23:45:15.593820 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_13/pre_mlp_layer_norm/scale/v_row
I0512 23:45:15.593851 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_13/pre_self_attention_layer_norm/scale/m
I0512 23:45:15.593882 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_13/pre_self_attention_layer_norm/scale/v
I0512 23:45:15.593912 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_13/pre_self_attention_layer_norm/scale/v_col
I0512 23:45:15.593943 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_13/pre_self_attention_layer_norm/scale/v_row
I0512 23:45:15.593974 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_13/self_attention/key/kernel/m
I0512 23:45:15.594005 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_13/self_attention/key/kernel/v
I0512 23:45:15.594037 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_13/self_attention/key/kernel/v_col
I0512 23:45:15.594072 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_13/self_attention/key/kernel/v_row
I0512 23:45:15.594103 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_13/self_attention/out/kernel/m
I0512 23:45:15.594134 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_13/self_attention/out/kernel/v
I0512 23:45:15.594165 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_13/self_attention/out/kernel/v_col
I0512 23:45:15.594195 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_13/self_attention/out/kernel/v_row
I0512 23:45:15.594233 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_13/self_attention/query/kernel/m
I0512 23:45:15.594265 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_13/self_attention/query/kernel/v
I0512 23:45:15.594329 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_13/self_attention/query/kernel/v_col
I0512 23:45:15.594363 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_13/self_attention/query/kernel/v_row
I0512 23:45:15.594395 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_13/self_attention/value/kernel/m
I0512 23:45:15.594426 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_13/self_attention/value/kernel/v
I0512 23:45:15.594458 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_13/self_attention/value/kernel/v_col
I0512 23:45:15.594490 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_13/self_attention/value/kernel/v_row
I0512 23:45:15.594521 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_14/mlp/wi_0/kernel/m
I0512 23:45:15.594552 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_14/mlp/wi_0/kernel/v
I0512 23:45:15.594583 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_14/mlp/wi_0/kernel/v_col
I0512 23:45:15.594614 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_14/mlp/wi_0/kernel/v_row
I0512 23:45:15.594645 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_14/mlp/wi_1/kernel/m
I0512 23:45:15.594677 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_14/mlp/wi_1/kernel/v
I0512 23:45:15.594735 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_14/mlp/wi_1/kernel/v_col
I0512 23:45:15.594769 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_14/mlp/wi_1/kernel/v_row
I0512 23:45:15.594801 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_14/mlp/wo/kernel/m
I0512 23:45:15.594832 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_14/mlp/wo/kernel/v
I0512 23:45:15.594863 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_14/mlp/wo/kernel/v_col
I0512 23:45:15.594894 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_14/mlp/wo/kernel/v_row
I0512 23:45:15.594925 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_14/pre_mlp_layer_norm/scale/m
I0512 23:45:15.594957 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_14/pre_mlp_layer_norm/scale/v
I0512 23:45:15.594988 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_14/pre_mlp_layer_norm/scale/v_col
I0512 23:45:15.595019 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_14/pre_mlp_layer_norm/scale/v_row
I0512 23:45:15.595050 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_14/pre_self_attention_layer_norm/scale/m
I0512 23:45:15.595081 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_14/pre_self_attention_layer_norm/scale/v
I0512 23:45:15.595112 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_14/pre_self_attention_layer_norm/scale/v_col
I0512 23:45:15.595143 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_14/pre_self_attention_layer_norm/scale/v_row
I0512 23:45:15.595174 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_14/self_attention/key/kernel/m
I0512 23:45:15.595211 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_14/self_attention/key/kernel/v
I0512 23:45:15.595244 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_14/self_attention/key/kernel/v_col
I0512 23:45:15.595276 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_14/self_attention/key/kernel/v_row
I0512 23:45:15.595307 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_14/self_attention/out/kernel/m
I0512 23:45:15.595337 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_14/self_attention/out/kernel/v
I0512 23:45:15.595368 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_14/self_attention/out/kernel/v_col
I0512 23:45:15.595399 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_14/self_attention/out/kernel/v_row
I0512 23:45:15.595430 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_14/self_attention/query/kernel/m
I0512 23:45:15.595462 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_14/self_attention/query/kernel/v
I0512 23:45:15.595495 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_14/self_attention/query/kernel/v_col
I0512 23:45:15.595526 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_14/self_attention/query/kernel/v_row
I0512 23:45:15.595557 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_14/self_attention/value/kernel/m
I0512 23:45:15.595588 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_14/self_attention/value/kernel/v
I0512 23:45:15.595620 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_14/self_attention/value/kernel/v_col
I0512 23:45:15.595651 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_14/self_attention/value/kernel/v_row
I0512 23:45:15.595682 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_15/mlp/wi_0/kernel/m
I0512 23:45:15.595712 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_15/mlp/wi_0/kernel/v
I0512 23:45:15.595743 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_15/mlp/wi_0/kernel/v_col
I0512 23:45:15.595774 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_15/mlp/wi_0/kernel/v_row
I0512 23:45:15.595805 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_15/mlp/wi_1/kernel/m
I0512 23:45:15.595836 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_15/mlp/wi_1/kernel/v
I0512 23:45:15.595866 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_15/mlp/wi_1/kernel/v_col
I0512 23:45:15.595897 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_15/mlp/wi_1/kernel/v_row
I0512 23:45:15.595928 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_15/mlp/wo/kernel/m
I0512 23:45:15.595959 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_15/mlp/wo/kernel/v
I0512 23:45:15.595990 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_15/mlp/wo/kernel/v_col
I0512 23:45:15.596021 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_15/mlp/wo/kernel/v_row
I0512 23:45:15.596052 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_15/pre_mlp_layer_norm/scale/m
I0512 23:45:15.596083 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_15/pre_mlp_layer_norm/scale/v
I0512 23:45:15.596114 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_15/pre_mlp_layer_norm/scale/v_col
I0512 23:45:15.596145 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_15/pre_mlp_layer_norm/scale/v_row
I0512 23:45:15.596176 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_15/pre_self_attention_layer_norm/scale/m
I0512 23:45:15.596212 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_15/pre_self_attention_layer_norm/scale/v
I0512 23:45:15.596245 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_15/pre_self_attention_layer_norm/scale/v_col
I0512 23:45:15.596276 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_15/pre_self_attention_layer_norm/scale/v_row
I0512 23:45:15.596307 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_15/self_attention/key/kernel/m
I0512 23:45:15.596338 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_15/self_attention/key/kernel/v
I0512 23:45:15.596369 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_15/self_attention/key/kernel/v_col
I0512 23:45:15.596400 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_15/self_attention/key/kernel/v_row
I0512 23:45:15.596431 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_15/self_attention/out/kernel/m
I0512 23:45:15.596463 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_15/self_attention/out/kernel/v
I0512 23:45:15.596495 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_15/self_attention/out/kernel/v_col
I0512 23:45:15.596526 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_15/self_attention/out/kernel/v_row
I0512 23:45:15.596557 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_15/self_attention/query/kernel/m
I0512 23:45:15.596588 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_15/self_attention/query/kernel/v
I0512 23:45:15.596619 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_15/self_attention/query/kernel/v_col
I0512 23:45:15.596654 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_15/self_attention/query/kernel/v_row
I0512 23:45:15.596690 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_15/self_attention/value/kernel/m
I0512 23:45:15.596726 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_15/self_attention/value/kernel/v
I0512 23:45:15.596759 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_15/self_attention/value/kernel/v_col
I0512 23:45:15.596791 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_15/self_attention/value/kernel/v_row
I0512 23:45:15.596829 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_16/mlp/wi_0/kernel/m
I0512 23:45:15.596862 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_16/mlp/wi_0/kernel/v
I0512 23:45:15.596894 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_16/mlp/wi_0/kernel/v_col
I0512 23:45:15.596924 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_16/mlp/wi_0/kernel/v_row
I0512 23:45:15.596954 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_16/mlp/wi_1/kernel/m
I0512 23:45:15.596985 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_16/mlp/wi_1/kernel/v
I0512 23:45:15.597015 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_16/mlp/wi_1/kernel/v_col
I0512 23:45:15.597045 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_16/mlp/wi_1/kernel/v_row
I0512 23:45:15.597075 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_16/mlp/wo/kernel/m
I0512 23:45:15.597105 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_16/mlp/wo/kernel/v
I0512 23:45:15.597135 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_16/mlp/wo/kernel/v_col
I0512 23:45:15.597165 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_16/mlp/wo/kernel/v_row
I0512 23:45:15.597196 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_16/pre_mlp_layer_norm/scale/m
I0512 23:45:15.597232 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_16/pre_mlp_layer_norm/scale/v
I0512 23:45:15.597263 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_16/pre_mlp_layer_norm/scale/v_col
I0512 23:45:15.597295 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_16/pre_mlp_layer_norm/scale/v_row
I0512 23:45:15.597326 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_16/pre_self_attention_layer_norm/scale/m
I0512 23:45:15.597356 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_16/pre_self_attention_layer_norm/scale/v
I0512 23:45:15.597387 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_16/pre_self_attention_layer_norm/scale/v_col
I0512 23:45:15.597417 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_16/pre_self_attention_layer_norm/scale/v_row
I0512 23:45:15.597447 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_16/self_attention/key/kernel/m
I0512 23:45:15.597482 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_16/self_attention/key/kernel/v
I0512 23:45:15.597513 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_16/self_attention/key/kernel/v_col
I0512 23:45:15.597544 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_16/self_attention/key/kernel/v_row
I0512 23:45:15.597574 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_16/self_attention/out/kernel/m
I0512 23:45:15.597604 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_16/self_attention/out/kernel/v
I0512 23:45:15.597634 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_16/self_attention/out/kernel/v_col
I0512 23:45:15.597664 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_16/self_attention/out/kernel/v_row
I0512 23:45:15.597694 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_16/self_attention/query/kernel/m
I0512 23:45:15.597724 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_16/self_attention/query/kernel/v
I0512 23:45:15.597754 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_16/self_attention/query/kernel/v_col
I0512 23:45:15.597784 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_16/self_attention/query/kernel/v_row
I0512 23:45:15.597814 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_16/self_attention/value/kernel/m
I0512 23:45:15.597845 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_16/self_attention/value/kernel/v
I0512 23:45:15.597875 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_16/self_attention/value/kernel/v_col
I0512 23:45:15.597905 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_16/self_attention/value/kernel/v_row
I0512 23:45:15.597935 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/extra_mlp/wi_0/kernel/m
I0512 23:45:15.597965 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/extra_mlp/wi_0/kernel/v
I0512 23:45:15.597995 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/extra_mlp/wi_0/kernel/v_col
I0512 23:45:15.598024 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/extra_mlp/wi_0/kernel/v_row
I0512 23:45:15.598058 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/extra_mlp/wi_1/kernel/m
I0512 23:45:15.598089 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/extra_mlp/wi_1/kernel/v
I0512 23:45:15.598119 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/extra_mlp/wi_1/kernel/v_col
I0512 23:45:15.598149 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/extra_mlp/wi_1/kernel/v_row
I0512 23:45:15.598179 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/extra_mlp/wo/kernel/m
I0512 23:45:15.598214 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/extra_mlp/wo/kernel/v
I0512 23:45:15.598246 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/extra_mlp/wo/kernel/v_col
I0512 23:45:15.598276 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/extra_mlp/wo/kernel/v_row
I0512 23:45:15.598306 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/mlp/expert/wi_0/kernel/m
I0512 23:45:15.598336 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/mlp/expert/wi_0/kernel/v
I0512 23:45:15.598366 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/mlp/expert/wi_0/kernel/v_col
I0512 23:45:15.598396 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/mlp/expert/wi_0/kernel/v_row
I0512 23:45:15.598426 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/mlp/expert/wi_1/kernel/m
I0512 23:45:15.598457 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/mlp/expert/wi_1/kernel/v
I0512 23:45:15.598489 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/mlp/expert/wi_1/kernel/v_col
I0512 23:45:15.598519 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/mlp/expert/wi_1/kernel/v_row
I0512 23:45:15.598550 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/mlp/expert/wo/kernel/m
I0512 23:45:15.598580 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/mlp/expert/wo/kernel/v
I0512 23:45:15.598610 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/mlp/expert/wo/kernel/v_col
I0512 23:45:15.598640 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/mlp/expert/wo/kernel/v_row
I0512 23:45:15.598670 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/mlp/router/router_weights/w/kernel/m
I0512 23:45:15.598721 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/mlp/router/router_weights/w/kernel/v
I0512 23:45:15.598755 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/mlp/router/router_weights/w/kernel/v_col
I0512 23:45:15.598786 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/mlp/router/router_weights/w/kernel/v_row
I0512 23:45:15.598816 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/pre_extra_mlp_layer_norm/scale/m
I0512 23:45:15.598846 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/pre_extra_mlp_layer_norm/scale/v
I0512 23:45:15.598876 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/pre_extra_mlp_layer_norm/scale/v_col
I0512 23:45:15.598906 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/pre_extra_mlp_layer_norm/scale/v_row
I0512 23:45:15.598936 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/pre_mlp_layer_norm/scale/m
I0512 23:45:15.598966 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/pre_mlp_layer_norm/scale/v
I0512 23:45:15.598996 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/pre_mlp_layer_norm/scale/v_col
I0512 23:45:15.599026 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/pre_mlp_layer_norm/scale/v_row
I0512 23:45:15.599056 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/pre_self_attention_layer_norm/scale/m
I0512 23:45:15.599086 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/pre_self_attention_layer_norm/scale/v
I0512 23:45:15.599116 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/pre_self_attention_layer_norm/scale/v_col
I0512 23:45:15.599146 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/pre_self_attention_layer_norm/scale/v_row
I0512 23:45:15.599176 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/self_attention/key/kernel/m
I0512 23:45:15.599211 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/self_attention/key/kernel/v
I0512 23:45:15.599243 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/self_attention/key/kernel/v_col
I0512 23:45:15.599273 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/self_attention/key/kernel/v_row
I0512 23:45:15.599303 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/self_attention/out/kernel/m
I0512 23:45:15.599334 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/self_attention/out/kernel/v
I0512 23:45:15.599363 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/self_attention/out/kernel/v_col
I0512 23:45:15.599393 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/self_attention/out/kernel/v_row
I0512 23:45:15.599423 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/self_attention/query/kernel/m
I0512 23:45:15.599453 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/self_attention/query/kernel/v
I0512 23:45:15.599486 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/self_attention/query/kernel/v_col
I0512 23:45:15.599516 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/self_attention/query/kernel/v_row
I0512 23:45:15.599546 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/self_attention/value/kernel/m
I0512 23:45:15.599576 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/self_attention/value/kernel/v
I0512 23:45:15.599606 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/self_attention/value/kernel/v_col
I0512 23:45:15.599636 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_17/self_attention/value/kernel/v_row
I0512 23:45:15.599666 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_18/mlp/wi_0/kernel/m
I0512 23:45:15.599696 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_18/mlp/wi_0/kernel/v
I0512 23:45:15.599725 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_18/mlp/wi_0/kernel/v_col
I0512 23:45:15.599756 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_18/mlp/wi_0/kernel/v_row
I0512 23:45:15.599786 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_18/mlp/wi_1/kernel/m
I0512 23:45:15.599816 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_18/mlp/wi_1/kernel/v
I0512 23:45:15.599846 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_18/mlp/wi_1/kernel/v_col
I0512 23:45:15.599876 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_18/mlp/wi_1/kernel/v_row
I0512 23:45:15.599906 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_18/mlp/wo/kernel/m
I0512 23:45:15.599936 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_18/mlp/wo/kernel/v
I0512 23:45:15.599966 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_18/mlp/wo/kernel/v_col
I0512 23:45:15.599996 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_18/mlp/wo/kernel/v_row
I0512 23:45:15.600026 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_18/pre_mlp_layer_norm/scale/m
I0512 23:45:15.600056 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_18/pre_mlp_layer_norm/scale/v
I0512 23:45:15.600086 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_18/pre_mlp_layer_norm/scale/v_col
I0512 23:45:15.600116 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_18/pre_mlp_layer_norm/scale/v_row
I0512 23:45:15.600146 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_18/pre_self_attention_layer_norm/scale/m
I0512 23:45:15.600176 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_18/pre_self_attention_layer_norm/scale/v
I0512 23:45:15.600212 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_18/pre_self_attention_layer_norm/scale/v_col
I0512 23:45:15.600244 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_18/pre_self_attention_layer_norm/scale/v_row
I0512 23:45:15.600274 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_18/self_attention/key/kernel/m
I0512 23:45:15.600305 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_18/self_attention/key/kernel/v
I0512 23:45:15.600335 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_18/self_attention/key/kernel/v_col
I0512 23:45:15.600365 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_18/self_attention/key/kernel/v_row
I0512 23:45:15.600395 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_18/self_attention/out/kernel/m
I0512 23:45:15.600425 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_18/self_attention/out/kernel/v
I0512 23:45:15.600455 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_18/self_attention/out/kernel/v_col
I0512 23:45:15.600488 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_18/self_attention/out/kernel/v_row
I0512 23:45:15.600518 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_18/self_attention/query/kernel/m
I0512 23:45:15.600548 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_18/self_attention/query/kernel/v
I0512 23:45:15.600578 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_18/self_attention/query/kernel/v_col
I0512 23:45:15.600608 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_18/self_attention/query/kernel/v_row
I0512 23:45:15.600638 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_18/self_attention/value/kernel/m
I0512 23:45:15.600668 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_18/self_attention/value/kernel/v
I0512 23:45:15.600698 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_18/self_attention/value/kernel/v_col
I0512 23:45:15.600728 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_18/self_attention/value/kernel/v_row
I0512 23:45:15.600758 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_19/mlp/wi_0/kernel/m
I0512 23:45:15.600788 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_19/mlp/wi_0/kernel/v
I0512 23:45:15.600818 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_19/mlp/wi_0/kernel/v_col
I0512 23:45:15.600848 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_19/mlp/wi_0/kernel/v_row
I0512 23:45:15.600878 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_19/mlp/wi_1/kernel/m
I0512 23:45:15.600908 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_19/mlp/wi_1/kernel/v
I0512 23:45:15.600938 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_19/mlp/wi_1/kernel/v_col
I0512 23:45:15.600968 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_19/mlp/wi_1/kernel/v_row
I0512 23:45:15.600998 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_19/mlp/wo/kernel/m
I0512 23:45:15.601028 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_19/mlp/wo/kernel/v
I0512 23:45:15.601062 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_19/mlp/wo/kernel/v_col
I0512 23:45:15.601092 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_19/mlp/wo/kernel/v_row
I0512 23:45:15.601123 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_19/pre_mlp_layer_norm/scale/m
I0512 23:45:15.601153 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_19/pre_mlp_layer_norm/scale/v
I0512 23:45:15.601182 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_19/pre_mlp_layer_norm/scale/v_col
I0512 23:45:15.601218 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_19/pre_mlp_layer_norm/scale/v_row
I0512 23:45:15.601249 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_19/pre_self_attention_layer_norm/scale/m
I0512 23:45:15.601280 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_19/pre_self_attention_layer_norm/scale/v
I0512 23:45:15.601310 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_19/pre_self_attention_layer_norm/scale/v_col
I0512 23:45:15.601340 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_19/pre_self_attention_layer_norm/scale/v_row
I0512 23:45:15.601370 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_19/self_attention/key/kernel/m
I0512 23:45:15.601400 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_19/self_attention/key/kernel/v
I0512 23:45:15.601430 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_19/self_attention/key/kernel/v_col
I0512 23:45:15.601461 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_19/self_attention/key/kernel/v_row
I0512 23:45:15.601492 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_19/self_attention/out/kernel/m
I0512 23:45:15.601523 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_19/self_attention/out/kernel/v
I0512 23:45:15.601553 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_19/self_attention/out/kernel/v_col
I0512 23:45:15.601583 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_19/self_attention/out/kernel/v_row
I0512 23:45:15.601612 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_19/self_attention/query/kernel/m
I0512 23:45:15.601642 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_19/self_attention/query/kernel/v
I0512 23:45:15.601672 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_19/self_attention/query/kernel/v_col
I0512 23:45:15.601702 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_19/self_attention/query/kernel/v_row
I0512 23:45:15.601732 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_19/self_attention/value/kernel/m
I0512 23:45:15.601761 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_19/self_attention/value/kernel/v
I0512 23:45:15.601791 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_19/self_attention/value/kernel/v_col
I0512 23:45:15.601821 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_19/self_attention/value/kernel/v_row
I0512 23:45:15.601851 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_2/mlp/wi_0/kernel/m
I0512 23:45:15.601881 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_2/mlp/wi_0/kernel/v
I0512 23:45:15.601911 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_2/mlp/wi_0/kernel/v_col
I0512 23:45:15.601941 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_2/mlp/wi_0/kernel/v_row
I0512 23:45:15.601971 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_2/mlp/wi_1/kernel/m
I0512 23:45:15.602001 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_2/mlp/wi_1/kernel/v
I0512 23:45:15.602031 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_2/mlp/wi_1/kernel/v_col
I0512 23:45:15.602061 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_2/mlp/wi_1/kernel/v_row
I0512 23:45:15.602091 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_2/mlp/wo/kernel/m
I0512 23:45:15.602120 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_2/mlp/wo/kernel/v
I0512 23:45:15.602150 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_2/mlp/wo/kernel/v_col
I0512 23:45:15.602180 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_2/mlp/wo/kernel/v_row
I0512 23:45:15.602216 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_2/pre_mlp_layer_norm/scale/m
I0512 23:45:15.602247 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_2/pre_mlp_layer_norm/scale/v
I0512 23:45:15.602277 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_2/pre_mlp_layer_norm/scale/v_col
I0512 23:45:15.602307 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_2/pre_mlp_layer_norm/scale/v_row
I0512 23:45:15.602337 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_2/pre_self_attention_layer_norm/scale/m
I0512 23:45:15.602366 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_2/pre_self_attention_layer_norm/scale/v
I0512 23:45:15.602396 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_2/pre_self_attention_layer_norm/scale/v_col
I0512 23:45:15.602426 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_2/pre_self_attention_layer_norm/scale/v_row
I0512 23:45:15.602457 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_2/self_attention/key/kernel/m
I0512 23:45:15.602489 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_2/self_attention/key/kernel/v
I0512 23:45:15.602519 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_2/self_attention/key/kernel/v_col
I0512 23:45:15.602549 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_2/self_attention/key/kernel/v_row
I0512 23:45:15.602578 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_2/self_attention/out/kernel/m
I0512 23:45:15.602608 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_2/self_attention/out/kernel/v
I0512 23:45:15.602638 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_2/self_attention/out/kernel/v_col
I0512 23:45:15.602668 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_2/self_attention/out/kernel/v_row
I0512 23:45:15.602722 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_2/self_attention/query/kernel/m
I0512 23:45:15.602758 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_2/self_attention/query/kernel/v
I0512 23:45:15.602789 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_2/self_attention/query/kernel/v_col
I0512 23:45:15.602819 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_2/self_attention/query/kernel/v_row
I0512 23:45:15.602849 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_2/self_attention/value/kernel/m
I0512 23:45:15.602879 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_2/self_attention/value/kernel/v
I0512 23:45:15.602908 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_2/self_attention/value/kernel/v_col
I0512 23:45:15.602938 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_2/self_attention/value/kernel/v_row
I0512 23:45:15.602968 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_20/mlp/wi_0/kernel/m
I0512 23:45:15.602998 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_20/mlp/wi_0/kernel/v
I0512 23:45:15.603028 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_20/mlp/wi_0/kernel/v_col
I0512 23:45:15.603058 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_20/mlp/wi_0/kernel/v_row
I0512 23:45:15.603088 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_20/mlp/wi_1/kernel/m
I0512 23:45:15.603118 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_20/mlp/wi_1/kernel/v
I0512 23:45:15.603147 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_20/mlp/wi_1/kernel/v_col
I0512 23:45:15.603178 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_20/mlp/wi_1/kernel/v_row
I0512 23:45:15.603213 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_20/mlp/wo/kernel/m
I0512 23:45:15.603244 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_20/mlp/wo/kernel/v
I0512 23:45:15.603274 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_20/mlp/wo/kernel/v_col
I0512 23:45:15.603304 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_20/mlp/wo/kernel/v_row
I0512 23:45:15.603334 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_20/pre_mlp_layer_norm/scale/m
I0512 23:45:15.603363 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_20/pre_mlp_layer_norm/scale/v
I0512 23:45:15.603394 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_20/pre_mlp_layer_norm/scale/v_col
I0512 23:45:15.603424 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_20/pre_mlp_layer_norm/scale/v_row
I0512 23:45:15.603455 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_20/pre_self_attention_layer_norm/scale/m
I0512 23:45:15.603487 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_20/pre_self_attention_layer_norm/scale/v
I0512 23:45:15.603517 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_20/pre_self_attention_layer_norm/scale/v_col
I0512 23:45:15.603547 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_20/pre_self_attention_layer_norm/scale/v_row
I0512 23:45:15.603577 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_20/self_attention/key/kernel/m
I0512 23:45:15.603607 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_20/self_attention/key/kernel/v
I0512 23:45:15.603637 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_20/self_attention/key/kernel/v_col
I0512 23:45:15.603667 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_20/self_attention/key/kernel/v_row
I0512 23:45:15.603697 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_20/self_attention/out/kernel/m
I0512 23:45:15.603727 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_20/self_attention/out/kernel/v
I0512 23:45:15.603757 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_20/self_attention/out/kernel/v_col
I0512 23:45:15.603786 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_20/self_attention/out/kernel/v_row
I0512 23:45:15.603816 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_20/self_attention/query/kernel/m
I0512 23:45:15.603847 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_20/self_attention/query/kernel/v
I0512 23:45:15.603876 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_20/self_attention/query/kernel/v_col
I0512 23:45:15.603906 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_20/self_attention/query/kernel/v_row
I0512 23:45:15.603936 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_20/self_attention/value/kernel/m
I0512 23:45:15.603966 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_20/self_attention/value/kernel/v
I0512 23:45:15.603996 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_20/self_attention/value/kernel/v_col
I0512 23:45:15.604026 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_20/self_attention/value/kernel/v_row
I0512 23:45:15.604056 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_21/mlp/wi_0/kernel/m
I0512 23:45:15.604086 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_21/mlp/wi_0/kernel/v
I0512 23:45:15.604116 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_21/mlp/wi_0/kernel/v_col
I0512 23:45:15.604146 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_21/mlp/wi_0/kernel/v_row
I0512 23:45:15.604176 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_21/mlp/wi_1/kernel/m
I0512 23:45:15.604211 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_21/mlp/wi_1/kernel/v
I0512 23:45:15.604243 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_21/mlp/wi_1/kernel/v_col
I0512 23:45:15.604274 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_21/mlp/wi_1/kernel/v_row
I0512 23:45:15.604304 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_21/mlp/wo/kernel/m
I0512 23:45:15.604334 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_21/mlp/wo/kernel/v
I0512 23:45:15.604364 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_21/mlp/wo/kernel/v_col
I0512 23:45:15.604394 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_21/mlp/wo/kernel/v_row
I0512 23:45:15.604424 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_21/pre_mlp_layer_norm/scale/m
I0512 23:45:15.604455 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_21/pre_mlp_layer_norm/scale/v
I0512 23:45:15.604487 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_21/pre_mlp_layer_norm/scale/v_col
I0512 23:45:15.604517 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_21/pre_mlp_layer_norm/scale/v_row
I0512 23:45:15.604547 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_21/pre_self_attention_layer_norm/scale/m
I0512 23:45:15.604578 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_21/pre_self_attention_layer_norm/scale/v
I0512 23:45:15.604608 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_21/pre_self_attention_layer_norm/scale/v_col
I0512 23:45:15.604638 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_21/pre_self_attention_layer_norm/scale/v_row
I0512 23:45:15.604668 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_21/self_attention/key/kernel/m
I0512 23:45:15.604698 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_21/self_attention/key/kernel/v
I0512 23:45:15.604728 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_21/self_attention/key/kernel/v_col
I0512 23:45:15.604758 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_21/self_attention/key/kernel/v_row
I0512 23:45:15.604788 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_21/self_attention/out/kernel/m
I0512 23:45:15.604818 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_21/self_attention/out/kernel/v
I0512 23:45:15.604848 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_21/self_attention/out/kernel/v_col
I0512 23:45:15.604878 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_21/self_attention/out/kernel/v_row
I0512 23:45:15.604908 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_21/self_attention/query/kernel/m
I0512 23:45:15.604938 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_21/self_attention/query/kernel/v
I0512 23:45:15.604967 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_21/self_attention/query/kernel/v_col
I0512 23:45:15.604997 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_21/self_attention/query/kernel/v_row
I0512 23:45:15.605026 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_21/self_attention/value/kernel/m
I0512 23:45:15.605056 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_21/self_attention/value/kernel/v
I0512 23:45:15.605086 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_21/self_attention/value/kernel/v_col
I0512 23:45:15.605115 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_21/self_attention/value/kernel/v_row
I0512 23:45:15.605145 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_22/mlp/wi_0/kernel/m
I0512 23:45:15.605175 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_22/mlp/wi_0/kernel/v
I0512 23:45:15.605209 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_22/mlp/wi_0/kernel/v_col
I0512 23:45:15.605241 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_22/mlp/wi_0/kernel/v_row
I0512 23:45:15.605272 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_22/mlp/wi_1/kernel/m
I0512 23:45:15.605302 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_22/mlp/wi_1/kernel/v
I0512 23:45:15.605332 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_22/mlp/wi_1/kernel/v_col
I0512 23:45:15.605362 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_22/mlp/wi_1/kernel/v_row
I0512 23:45:15.605391 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_22/mlp/wo/kernel/m
I0512 23:45:15.605421 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_22/mlp/wo/kernel/v
I0512 23:45:15.605451 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_22/mlp/wo/kernel/v_col
I0512 23:45:15.605484 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_22/mlp/wo/kernel/v_row
I0512 23:45:15.605515 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_22/pre_mlp_layer_norm/scale/m
I0512 23:45:15.605545 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_22/pre_mlp_layer_norm/scale/v
I0512 23:45:15.605574 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_22/pre_mlp_layer_norm/scale/v_col
I0512 23:45:15.605604 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_22/pre_mlp_layer_norm/scale/v_row
I0512 23:45:15.605634 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_22/pre_self_attention_layer_norm/scale/m
I0512 23:45:15.605664 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_22/pre_self_attention_layer_norm/scale/v
I0512 23:45:15.605694 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_22/pre_self_attention_layer_norm/scale/v_col
I0512 23:45:15.605723 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_22/pre_self_attention_layer_norm/scale/v_row
I0512 23:45:15.605753 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_22/self_attention/key/kernel/m
I0512 23:45:15.605783 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_22/self_attention/key/kernel/v
I0512 23:45:15.605813 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_22/self_attention/key/kernel/v_col
I0512 23:45:15.605843 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_22/self_attention/key/kernel/v_row
I0512 23:45:15.605872 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_22/self_attention/out/kernel/m
I0512 23:45:15.605902 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_22/self_attention/out/kernel/v
I0512 23:45:15.605931 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_22/self_attention/out/kernel/v_col
I0512 23:45:15.605962 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_22/self_attention/out/kernel/v_row
I0512 23:45:15.605991 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_22/self_attention/query/kernel/m
I0512 23:45:15.606022 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_22/self_attention/query/kernel/v
I0512 23:45:15.606052 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_22/self_attention/query/kernel/v_col
I0512 23:45:15.606081 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_22/self_attention/query/kernel/v_row
I0512 23:45:15.606111 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_22/self_attention/value/kernel/m
I0512 23:45:15.606141 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_22/self_attention/value/kernel/v
I0512 23:45:15.606171 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_22/self_attention/value/kernel/v_col
I0512 23:45:15.606205 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_22/self_attention/value/kernel/v_row
I0512 23:45:15.606237 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/extra_mlp/wi_0/kernel/m
I0512 23:45:15.606267 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/extra_mlp/wi_0/kernel/v
I0512 23:45:15.606297 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/extra_mlp/wi_0/kernel/v_col
I0512 23:45:15.606327 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/extra_mlp/wi_0/kernel/v_row
I0512 23:45:15.606357 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/extra_mlp/wi_1/kernel/m
I0512 23:45:15.606386 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/extra_mlp/wi_1/kernel/v
I0512 23:45:15.606416 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/extra_mlp/wi_1/kernel/v_col
I0512 23:45:15.606446 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/extra_mlp/wi_1/kernel/v_row
I0512 23:45:15.606478 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/extra_mlp/wo/kernel/m
I0512 23:45:15.606509 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/extra_mlp/wo/kernel/v
I0512 23:45:15.606539 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/extra_mlp/wo/kernel/v_col
I0512 23:45:15.606569 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/extra_mlp/wo/kernel/v_row
I0512 23:45:15.606598 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/mlp/expert/wi_0/kernel/m
I0512 23:45:15.606628 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/mlp/expert/wi_0/kernel/v
I0512 23:45:15.606657 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/mlp/expert/wi_0/kernel/v_col
I0512 23:45:15.606706 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/mlp/expert/wi_0/kernel/v_row
I0512 23:45:15.606743 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/mlp/expert/wi_1/kernel/m
I0512 23:45:15.606774 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/mlp/expert/wi_1/kernel/v
I0512 23:45:15.606804 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/mlp/expert/wi_1/kernel/v_col
I0512 23:45:15.606834 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/mlp/expert/wi_1/kernel/v_row
I0512 23:45:15.606863 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/mlp/expert/wo/kernel/m
I0512 23:45:15.606893 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/mlp/expert/wo/kernel/v
I0512 23:45:15.606923 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/mlp/expert/wo/kernel/v_col
I0512 23:45:15.606952 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/mlp/expert/wo/kernel/v_row
I0512 23:45:15.606982 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/mlp/router/router_weights/w/kernel/m
I0512 23:45:15.607011 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/mlp/router/router_weights/w/kernel/v
I0512 23:45:15.607042 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/mlp/router/router_weights/w/kernel/v_col
I0512 23:45:15.607072 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/mlp/router/router_weights/w/kernel/v_row
I0512 23:45:15.607102 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/pre_extra_mlp_layer_norm/scale/m
I0512 23:45:15.607132 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/pre_extra_mlp_layer_norm/scale/v
I0512 23:45:15.607162 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/pre_extra_mlp_layer_norm/scale/v_col
I0512 23:45:15.607192 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/pre_extra_mlp_layer_norm/scale/v_row
I0512 23:45:15.607228 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/pre_mlp_layer_norm/scale/m
I0512 23:45:15.607258 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/pre_mlp_layer_norm/scale/v
I0512 23:45:15.607289 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/pre_mlp_layer_norm/scale/v_col
I0512 23:45:15.607319 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/pre_mlp_layer_norm/scale/v_row
I0512 23:45:15.607348 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/pre_self_attention_layer_norm/scale/m
I0512 23:45:15.607378 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/pre_self_attention_layer_norm/scale/v
I0512 23:45:15.607408 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/pre_self_attention_layer_norm/scale/v_col
I0512 23:45:15.607438 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/pre_self_attention_layer_norm/scale/v_row
I0512 23:45:15.607470 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/self_attention/key/kernel/m
I0512 23:45:15.607502 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/self_attention/key/kernel/v
I0512 23:45:15.607532 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/self_attention/key/kernel/v_col
I0512 23:45:15.607562 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/self_attention/key/kernel/v_row
I0512 23:45:15.607591 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/self_attention/out/kernel/m
I0512 23:45:15.607621 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/self_attention/out/kernel/v
I0512 23:45:15.607651 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/self_attention/out/kernel/v_col
I0512 23:45:15.607681 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/self_attention/out/kernel/v_row
I0512 23:45:15.607711 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/self_attention/query/kernel/m
I0512 23:45:15.607740 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/self_attention/query/kernel/v
I0512 23:45:15.607770 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/self_attention/query/kernel/v_col
I0512 23:45:15.607800 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/self_attention/query/kernel/v_row
I0512 23:45:15.607830 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/self_attention/value/kernel/m
I0512 23:45:15.607860 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/self_attention/value/kernel/v
I0512 23:45:15.607889 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/self_attention/value/kernel/v_col
I0512 23:45:15.607919 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_23/self_attention/value/kernel/v_row
I0512 23:45:15.607949 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_3/mlp/wi_0/kernel/m
I0512 23:45:15.607979 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_3/mlp/wi_0/kernel/v
I0512 23:45:15.608008 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_3/mlp/wi_0/kernel/v_col
I0512 23:45:15.608038 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_3/mlp/wi_0/kernel/v_row
I0512 23:45:15.608068 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_3/mlp/wi_1/kernel/m
I0512 23:45:15.608098 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_3/mlp/wi_1/kernel/v
I0512 23:45:15.608127 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_3/mlp/wi_1/kernel/v_col
I0512 23:45:15.608157 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_3/mlp/wi_1/kernel/v_row
I0512 23:45:15.608187 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_3/mlp/wo/kernel/m
I0512 23:45:15.608222 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_3/mlp/wo/kernel/v
I0512 23:45:15.608253 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_3/mlp/wo/kernel/v_col
I0512 23:45:15.608283 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_3/mlp/wo/kernel/v_row
I0512 23:45:15.608313 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_3/pre_mlp_layer_norm/scale/m
I0512 23:45:15.608343 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_3/pre_mlp_layer_norm/scale/v
I0512 23:45:15.608372 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_3/pre_mlp_layer_norm/scale/v_col
I0512 23:45:15.608402 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_3/pre_mlp_layer_norm/scale/v_row
I0512 23:45:15.608433 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_3/pre_self_attention_layer_norm/scale/m
I0512 23:45:15.608463 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_3/pre_self_attention_layer_norm/scale/v
I0512 23:45:15.608495 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_3/pre_self_attention_layer_norm/scale/v_col
I0512 23:45:15.608525 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_3/pre_self_attention_layer_norm/scale/v_row
I0512 23:45:15.608555 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_3/self_attention/key/kernel/m
I0512 23:45:15.608585 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_3/self_attention/key/kernel/v
I0512 23:45:15.608615 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_3/self_attention/key/kernel/v_col
I0512 23:45:15.608644 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_3/self_attention/key/kernel/v_row
I0512 23:45:15.608674 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_3/self_attention/out/kernel/m
I0512 23:45:15.608704 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_3/self_attention/out/kernel/v
I0512 23:45:15.608734 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_3/self_attention/out/kernel/v_col
I0512 23:45:15.608764 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_3/self_attention/out/kernel/v_row
I0512 23:45:15.608794 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_3/self_attention/query/kernel/m
I0512 23:45:15.608824 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_3/self_attention/query/kernel/v
I0512 23:45:15.608854 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_3/self_attention/query/kernel/v_col
I0512 23:45:15.608883 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_3/self_attention/query/kernel/v_row
I0512 23:45:15.608913 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_3/self_attention/value/kernel/m
I0512 23:45:15.608943 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_3/self_attention/value/kernel/v
I0512 23:45:15.608973 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_3/self_attention/value/kernel/v_col
I0512 23:45:15.609003 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_3/self_attention/value/kernel/v_row
I0512 23:45:15.609032 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_4/mlp/wi_0/kernel/m
I0512 23:45:15.609062 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_4/mlp/wi_0/kernel/v
I0512 23:45:15.609092 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_4/mlp/wi_0/kernel/v_col
I0512 23:45:15.609122 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_4/mlp/wi_0/kernel/v_row
I0512 23:45:15.609152 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_4/mlp/wi_1/kernel/m
I0512 23:45:15.609181 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_4/mlp/wi_1/kernel/v
I0512 23:45:15.609216 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_4/mlp/wi_1/kernel/v_col
I0512 23:45:15.609247 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_4/mlp/wi_1/kernel/v_row
I0512 23:45:15.609278 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_4/mlp/wo/kernel/m
I0512 23:45:15.609308 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_4/mlp/wo/kernel/v
I0512 23:45:15.609339 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_4/mlp/wo/kernel/v_col
I0512 23:45:15.609369 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_4/mlp/wo/kernel/v_row
I0512 23:45:15.609399 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_4/pre_mlp_layer_norm/scale/m
I0512 23:45:15.609429 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_4/pre_mlp_layer_norm/scale/v
I0512 23:45:15.609460 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_4/pre_mlp_layer_norm/scale/v_col
I0512 23:45:15.609492 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_4/pre_mlp_layer_norm/scale/v_row
I0512 23:45:15.609522 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_4/pre_self_attention_layer_norm/scale/m
I0512 23:45:15.609552 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_4/pre_self_attention_layer_norm/scale/v
I0512 23:45:15.609582 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_4/pre_self_attention_layer_norm/scale/v_col
I0512 23:45:15.609612 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_4/pre_self_attention_layer_norm/scale/v_row
I0512 23:45:15.609642 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_4/self_attention/key/kernel/m
I0512 23:45:15.609672 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_4/self_attention/key/kernel/v
I0512 23:45:15.609702 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_4/self_attention/key/kernel/v_col
I0512 23:45:15.609732 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_4/self_attention/key/kernel/v_row
I0512 23:45:15.609761 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_4/self_attention/out/kernel/m
I0512 23:45:15.609791 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_4/self_attention/out/kernel/v
I0512 23:45:15.609821 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_4/self_attention/out/kernel/v_col
I0512 23:45:15.609851 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_4/self_attention/out/kernel/v_row
I0512 23:45:15.609881 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_4/self_attention/query/kernel/m
I0512 23:45:15.609911 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_4/self_attention/query/kernel/v
I0512 23:45:15.609941 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_4/self_attention/query/kernel/v_col
I0512 23:45:15.609971 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_4/self_attention/query/kernel/v_row
I0512 23:45:15.610001 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_4/self_attention/value/kernel/m
I0512 23:45:15.610031 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_4/self_attention/value/kernel/v
I0512 23:45:15.610061 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_4/self_attention/value/kernel/v_col
I0512 23:45:15.610090 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_4/self_attention/value/kernel/v_row
I0512 23:45:15.610121 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/extra_mlp/wi_0/kernel/m
I0512 23:45:15.610150 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/extra_mlp/wi_0/kernel/v
I0512 23:45:15.610180 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/extra_mlp/wi_0/kernel/v_col
I0512 23:45:15.610215 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/extra_mlp/wi_0/kernel/v_row
I0512 23:45:15.610247 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/extra_mlp/wi_1/kernel/m
I0512 23:45:15.610278 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/extra_mlp/wi_1/kernel/v
I0512 23:45:15.610308 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/extra_mlp/wi_1/kernel/v_col
I0512 23:45:15.610337 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/extra_mlp/wi_1/kernel/v_row
I0512 23:45:15.610367 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/extra_mlp/wo/kernel/m
I0512 23:45:15.610397 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/extra_mlp/wo/kernel/v
I0512 23:45:15.610427 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/extra_mlp/wo/kernel/v_col
I0512 23:45:15.610457 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/extra_mlp/wo/kernel/v_row
I0512 23:45:15.610489 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/mlp/expert/wi_0/kernel/m
I0512 23:45:15.610520 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/mlp/expert/wi_0/kernel/v
I0512 23:45:15.610549 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/mlp/expert/wi_0/kernel/v_col
I0512 23:45:15.610579 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/mlp/expert/wi_0/kernel/v_row
I0512 23:45:15.610609 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/mlp/expert/wi_1/kernel/m
I0512 23:45:15.610639 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/mlp/expert/wi_1/kernel/v
I0512 23:45:15.610669 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/mlp/expert/wi_1/kernel/v_col
I0512 23:45:15.610716 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/mlp/expert/wi_1/kernel/v_row
I0512 23:45:15.610752 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/mlp/expert/wo/kernel/m
I0512 23:45:15.610782 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/mlp/expert/wo/kernel/v
I0512 23:45:15.610812 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/mlp/expert/wo/kernel/v_col
I0512 23:45:15.610842 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/mlp/expert/wo/kernel/v_row
I0512 23:45:15.610872 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/mlp/router/router_weights/w/kernel/m
I0512 23:45:15.610902 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/mlp/router/router_weights/w/kernel/v
I0512 23:45:15.610931 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/mlp/router/router_weights/w/kernel/v_col
I0512 23:45:15.610962 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/mlp/router/router_weights/w/kernel/v_row
I0512 23:45:15.610992 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/pre_extra_mlp_layer_norm/scale/m
I0512 23:45:15.611022 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/pre_extra_mlp_layer_norm/scale/v
I0512 23:45:15.611052 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/pre_extra_mlp_layer_norm/scale/v_col
I0512 23:45:15.611081 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/pre_extra_mlp_layer_norm/scale/v_row
I0512 23:45:15.611111 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/pre_mlp_layer_norm/scale/m
I0512 23:45:15.611141 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/pre_mlp_layer_norm/scale/v
I0512 23:45:15.611170 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/pre_mlp_layer_norm/scale/v_col
I0512 23:45:15.611204 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/pre_mlp_layer_norm/scale/v_row
I0512 23:45:15.611236 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/pre_self_attention_layer_norm/scale/m
I0512 23:45:15.611267 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/pre_self_attention_layer_norm/scale/v
I0512 23:45:15.611297 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/pre_self_attention_layer_norm/scale/v_col
I0512 23:45:15.611327 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/pre_self_attention_layer_norm/scale/v_row
I0512 23:45:15.611357 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/self_attention/key/kernel/m
I0512 23:45:15.611387 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/self_attention/key/kernel/v
I0512 23:45:15.611417 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/self_attention/key/kernel/v_col
I0512 23:45:15.611447 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/self_attention/key/kernel/v_row
I0512 23:45:15.611479 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/self_attention/out/kernel/m
I0512 23:45:15.611509 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/self_attention/out/kernel/v
I0512 23:45:15.611539 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/self_attention/out/kernel/v_col
I0512 23:45:15.611569 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/self_attention/out/kernel/v_row
I0512 23:45:15.611599 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/self_attention/query/kernel/m
I0512 23:45:15.611629 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/self_attention/query/kernel/v
I0512 23:45:15.611659 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/self_attention/query/kernel/v_col
I0512 23:45:15.611689 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/self_attention/query/kernel/v_row
I0512 23:45:15.611719 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/self_attention/value/kernel/m
I0512 23:45:15.611749 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/self_attention/value/kernel/v
I0512 23:45:15.611778 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/self_attention/value/kernel/v_col
I0512 23:45:15.611808 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/self_attention/value/kernel/v_row
I0512 23:45:15.611837 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_6/mlp/wi_0/kernel/m
I0512 23:45:15.611867 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_6/mlp/wi_0/kernel/v
I0512 23:45:15.611897 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_6/mlp/wi_0/kernel/v_col
I0512 23:45:15.611927 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_6/mlp/wi_0/kernel/v_row
I0512 23:45:15.611957 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_6/mlp/wi_1/kernel/m
I0512 23:45:15.611986 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_6/mlp/wi_1/kernel/v
I0512 23:45:15.612016 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_6/mlp/wi_1/kernel/v_col
I0512 23:45:15.612047 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_6/mlp/wi_1/kernel/v_row
I0512 23:45:15.612076 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_6/mlp/wo/kernel/m
I0512 23:45:15.612106 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_6/mlp/wo/kernel/v
I0512 23:45:15.612135 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_6/mlp/wo/kernel/v_col
I0512 23:45:15.612165 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_6/mlp/wo/kernel/v_row
I0512 23:45:15.612195 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_6/pre_mlp_layer_norm/scale/m
I0512 23:45:15.612231 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_6/pre_mlp_layer_norm/scale/v
I0512 23:45:15.612261 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_6/pre_mlp_layer_norm/scale/v_col
I0512 23:45:15.612291 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_6/pre_mlp_layer_norm/scale/v_row
I0512 23:45:15.612321 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_6/pre_self_attention_layer_norm/scale/m
I0512 23:45:15.612350 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_6/pre_self_attention_layer_norm/scale/v
I0512 23:45:15.612381 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_6/pre_self_attention_layer_norm/scale/v_col
I0512 23:45:15.612411 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_6/pre_self_attention_layer_norm/scale/v_row
I0512 23:45:15.612441 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_6/self_attention/key/kernel/m
I0512 23:45:15.612473 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_6/self_attention/key/kernel/v
I0512 23:45:15.612504 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_6/self_attention/key/kernel/v_col
I0512 23:45:15.612534 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_6/self_attention/key/kernel/v_row
I0512 23:45:15.612564 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_6/self_attention/out/kernel/m
I0512 23:45:15.612594 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_6/self_attention/out/kernel/v
I0512 23:45:15.612623 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_6/self_attention/out/kernel/v_col
I0512 23:45:15.612653 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_6/self_attention/out/kernel/v_row
I0512 23:45:15.612683 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_6/self_attention/query/kernel/m
I0512 23:45:15.612713 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_6/self_attention/query/kernel/v
I0512 23:45:15.612743 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_6/self_attention/query/kernel/v_col
I0512 23:45:15.612773 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_6/self_attention/query/kernel/v_row
I0512 23:45:15.612802 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_6/self_attention/value/kernel/m
I0512 23:45:15.612832 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_6/self_attention/value/kernel/v
I0512 23:45:15.612862 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_6/self_attention/value/kernel/v_col
I0512 23:45:15.612891 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_6/self_attention/value/kernel/v_row
I0512 23:45:15.612921 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_7/mlp/wi_0/kernel/m
I0512 23:45:15.612951 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_7/mlp/wi_0/kernel/v
I0512 23:45:15.612981 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_7/mlp/wi_0/kernel/v_col
I0512 23:45:15.613011 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_7/mlp/wi_0/kernel/v_row
I0512 23:45:15.613041 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_7/mlp/wi_1/kernel/m
I0512 23:45:15.613071 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_7/mlp/wi_1/kernel/v
I0512 23:45:15.613101 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_7/mlp/wi_1/kernel/v_col
I0512 23:45:15.613131 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_7/mlp/wi_1/kernel/v_row
I0512 23:45:15.613161 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_7/mlp/wo/kernel/m
I0512 23:45:15.613191 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_7/mlp/wo/kernel/v
I0512 23:45:15.613226 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_7/mlp/wo/kernel/v_col
I0512 23:45:15.613257 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_7/mlp/wo/kernel/v_row
I0512 23:45:15.613287 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_7/pre_mlp_layer_norm/scale/m
I0512 23:45:15.613317 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_7/pre_mlp_layer_norm/scale/v
I0512 23:45:15.613347 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_7/pre_mlp_layer_norm/scale/v_col
I0512 23:45:15.613377 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_7/pre_mlp_layer_norm/scale/v_row
I0512 23:45:15.613406 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_7/pre_self_attention_layer_norm/scale/m
I0512 23:45:15.613436 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_7/pre_self_attention_layer_norm/scale/v
I0512 23:45:15.613468 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_7/pre_self_attention_layer_norm/scale/v_col
I0512 23:45:15.613499 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_7/pre_self_attention_layer_norm/scale/v_row
I0512 23:45:15.613530 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_7/self_attention/key/kernel/m
I0512 23:45:15.613559 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_7/self_attention/key/kernel/v
I0512 23:45:15.613590 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_7/self_attention/key/kernel/v_col
I0512 23:45:15.613619 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_7/self_attention/key/kernel/v_row
I0512 23:45:15.613649 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_7/self_attention/out/kernel/m
I0512 23:45:15.613679 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_7/self_attention/out/kernel/v
I0512 23:45:15.613708 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_7/self_attention/out/kernel/v_col
I0512 23:45:15.613738 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_7/self_attention/out/kernel/v_row
I0512 23:45:15.613768 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_7/self_attention/query/kernel/m
I0512 23:45:15.613798 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_7/self_attention/query/kernel/v
I0512 23:45:15.613827 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_7/self_attention/query/kernel/v_col
I0512 23:45:15.613857 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_7/self_attention/query/kernel/v_row
I0512 23:45:15.613887 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_7/self_attention/value/kernel/m
I0512 23:45:15.613917 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_7/self_attention/value/kernel/v
I0512 23:45:15.613947 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_7/self_attention/value/kernel/v_col
I0512 23:45:15.613977 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_7/self_attention/value/kernel/v_row
I0512 23:45:15.614007 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_8/mlp/wi_0/kernel/m
I0512 23:45:15.614037 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_8/mlp/wi_0/kernel/v
I0512 23:45:15.614067 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_8/mlp/wi_0/kernel/v_col
I0512 23:45:15.614097 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_8/mlp/wi_0/kernel/v_row
I0512 23:45:15.614127 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_8/mlp/wi_1/kernel/m
I0512 23:45:15.614157 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_8/mlp/wi_1/kernel/v
I0512 23:45:15.614187 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_8/mlp/wi_1/kernel/v_col
I0512 23:45:15.614222 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_8/mlp/wi_1/kernel/v_row
I0512 23:45:15.614253 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_8/mlp/wo/kernel/m
I0512 23:45:15.614284 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_8/mlp/wo/kernel/v
I0512 23:45:15.614339 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_8/mlp/wo/kernel/v_col
I0512 23:45:15.614370 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_8/mlp/wo/kernel/v_row
I0512 23:45:15.614400 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_8/pre_mlp_layer_norm/scale/m
I0512 23:45:15.614430 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_8/pre_mlp_layer_norm/scale/v
I0512 23:45:15.614461 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_8/pre_mlp_layer_norm/scale/v_col
I0512 23:45:15.614492 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_8/pre_mlp_layer_norm/scale/v_row
I0512 23:45:15.614522 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_8/pre_self_attention_layer_norm/scale/m
I0512 23:45:15.614552 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_8/pre_self_attention_layer_norm/scale/v
I0512 23:45:15.614582 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_8/pre_self_attention_layer_norm/scale/v_col
I0512 23:45:15.614613 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_8/pre_self_attention_layer_norm/scale/v_row
I0512 23:45:15.614643 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_8/self_attention/key/kernel/m
I0512 23:45:15.614672 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_8/self_attention/key/kernel/v
I0512 23:45:15.614723 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_8/self_attention/key/kernel/v_col
I0512 23:45:15.614757 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_8/self_attention/key/kernel/v_row
I0512 23:45:15.614787 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_8/self_attention/out/kernel/m
I0512 23:45:15.614817 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_8/self_attention/out/kernel/v
I0512 23:45:15.614847 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_8/self_attention/out/kernel/v_col
I0512 23:45:15.614877 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_8/self_attention/out/kernel/v_row
I0512 23:45:15.614907 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_8/self_attention/query/kernel/m
I0512 23:45:15.614936 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_8/self_attention/query/kernel/v
I0512 23:45:15.614966 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_8/self_attention/query/kernel/v_col
I0512 23:45:15.614996 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_8/self_attention/query/kernel/v_row
I0512 23:45:15.615026 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_8/self_attention/value/kernel/m
I0512 23:45:15.615056 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_8/self_attention/value/kernel/v
I0512 23:45:15.615086 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_8/self_attention/value/kernel/v_col
I0512 23:45:15.615116 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_8/self_attention/value/kernel/v_row
I0512 23:45:15.615145 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_9/mlp/wi_0/kernel/m
I0512 23:45:15.615175 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_9/mlp/wi_0/kernel/v
I0512 23:45:15.615211 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_9/mlp/wi_0/kernel/v_col
I0512 23:45:15.615243 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_9/mlp/wi_0/kernel/v_row
I0512 23:45:15.615273 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_9/mlp/wi_1/kernel/m
I0512 23:45:15.615303 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_9/mlp/wi_1/kernel/v
I0512 23:45:15.615333 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_9/mlp/wi_1/kernel/v_col
I0512 23:45:15.615363 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_9/mlp/wi_1/kernel/v_row
I0512 23:45:15.615392 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_9/mlp/wo/kernel/m
I0512 23:45:15.615422 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_9/mlp/wo/kernel/v
I0512 23:45:15.615452 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_9/mlp/wo/kernel/v_col
I0512 23:45:15.615485 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_9/mlp/wo/kernel/v_row
I0512 23:45:15.615515 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_9/pre_mlp_layer_norm/scale/m
I0512 23:45:15.615545 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_9/pre_mlp_layer_norm/scale/v
I0512 23:45:15.615575 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_9/pre_mlp_layer_norm/scale/v_col
I0512 23:45:15.615605 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_9/pre_mlp_layer_norm/scale/v_row
I0512 23:45:15.615634 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_9/pre_self_attention_layer_norm/scale/m
I0512 23:45:15.615664 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_9/pre_self_attention_layer_norm/scale/v
I0512 23:45:15.615694 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_9/pre_self_attention_layer_norm/scale/v_col
I0512 23:45:15.615724 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_9/pre_self_attention_layer_norm/scale/v_row
I0512 23:45:15.615754 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_9/self_attention/key/kernel/m
I0512 23:45:15.615784 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_9/self_attention/key/kernel/v
I0512 23:45:15.615814 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_9/self_attention/key/kernel/v_col
I0512 23:45:15.615843 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_9/self_attention/key/kernel/v_row
I0512 23:45:15.615873 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_9/self_attention/out/kernel/m
I0512 23:45:15.615903 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_9/self_attention/out/kernel/v
I0512 23:45:15.615932 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_9/self_attention/out/kernel/v_col
I0512 23:45:15.615962 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_9/self_attention/out/kernel/v_row
I0512 23:45:15.615992 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_9/self_attention/query/kernel/m
I0512 23:45:15.616022 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_9/self_attention/query/kernel/v
I0512 23:45:15.616055 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_9/self_attention/query/kernel/v_col
I0512 23:45:15.616086 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_9/self_attention/query/kernel/v_row
I0512 23:45:15.616116 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_9/self_attention/value/kernel/m
I0512 23:45:15.616147 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_9/self_attention/value/kernel/v
I0512 23:45:15.616176 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_9/self_attention/value/kernel/v_col
I0512 23:45:15.616211 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_9/self_attention/value/kernel/v_row
I0512 23:45:15.616243 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/logits_dense/kernel/m
I0512 23:45:15.616273 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/logits_dense/kernel/v
I0512 23:45:15.616303 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/logits_dense/kernel/v_col
I0512 23:45:15.616333 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/logits_dense/kernel/v_row
I0512 23:45:15.616363 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/token_embedder/embedding/m
I0512 23:45:15.616393 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/token_embedder/embedding/v
I0512 23:45:15.616422 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/token_embedder/embedding/v_col
I0512 23:45:15.616452 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/param_states/token_embedder/embedding/v_row
I0512 23:45:15.616485 140260698863616 checkpoints.py:1100] Restoring key from ckpt: state/step
I0512 23:45:15.616516 140260698863616 checkpoints.py:1100] Restoring key from ckpt: target/decoder/decoder_norm/scale
I0512 23:45:15.616547 140260698863616 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_0/mlp/wi_0/kernel
I0512 23:45:15.616577 140260698863616 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_0/mlp/wi_1/kernel
I0512 23:45:15.616607 140260698863616 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_0/mlp/wo/kernel
I0512 23:45:15.616637 140260698863616 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_0/pre_mlp_layer_norm/scale
I0512 23:45:15.616667 140260698863616 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_0/pre_self_attention_layer_norm/scale
I0512 23:45:15.616697 140260698863616 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_0/self_attention/key/kernel
I0512 23:45:15.616727 140260698863616 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_0/self_attention/out/kernel
I0512 23:45:15.616757 140260698863616 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_0/self_attention/query/kernel
I0512 23:45:15.616787 140260698863616 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_0/self_attention/value/kernel
I0512 23:45:15.616816 140260698863616 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_1/mlp/wi_0/kernel
I0512 23:45:15.616846 140260698863616 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_1/mlp/wi_1/kernel
I0512 23:45:15.616877 140260698863616 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_1/mlp/wo/kernel
I0512 23:45:15.616907 140260698863616 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_1/pre_mlp_layer_norm/scale
I0512 23:45:15.616937 140260698863616 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_1/pre_self_attention_layer_norm/scale
I0512 23:45:15.616966 140260698863616 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_1/self_attention/key/kernel
I0512 23:45:15.616996 140260698863616 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_1/self_attention/out/kernel
I0512 23:45:15.617026 140260698863616 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_1/self_attention/query/kernel
I0512 23:45:15.617056 140260698863616 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_1/self_attention/value/kernel
I0512 23:45:15.617086 140260698863616 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_10/mlp/wi_0/kernel
I0512 23:45:15.617116 140260698863616 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_10/mlp/wi_1/kernel
I0512 23:45:15.617146 140260698863616 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_10/mlp/wo/kernel
I0512 23:45:15.617176 140260698863616 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_10/pre_mlp_layer_norm/scale
I0512 23:45:15.617210 140260698863616 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_10/pre_self_attention_layer_norm/scale
I0512 23:45:15.617242 140260698863616 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_10/self_attention/key/kernel
I0512 23:45:15.617272 140260698863616 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_10/self_attention/out/kernel
I0512 23:45:15.617301 140260698863616 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_10/self_attention/query/kernel
I0512 23:45:15.617331 140260698863616 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_10/self_attention/value/kernel
I0512 23:45:15.617361 140260698863616 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_11/extra_mlp/wi_0/kernel
I0512 23:45:15.617391 140260698863616 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_11/extra_mlp/wi_1/kernel
I0512 23:45:15.617420 140260698863616 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_11/extra_mlp/wo/kernel
I0512 23:45:15.617450 140260698863616 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_11/mlp/expert/wi_0/kernel
I0512 23:45:15.617482 140260698863616 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_11/mlp/expert/wi_1/kernel
I0512 23:45:15.617513 140260698863616 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_11/mlp/expert/wo/kernel
I0512 23:45:15.617543 140260698863616 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_11/mlp/router/router_weights/w/kernel
I0512 23:45:15.617573 140260698863616 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_11/pre_extra_mlp_layer_norm/scale
I0512 23:45:15.617603 140260698863616 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_11/pre_mlp_layer_norm/scale
I0512 23:45:15.617633 140260698863616 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_11/pre_self_attention_layer_norm/scale
I0512 23:45:15.617662 140260698863616 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_11/self_attention/key/kernel
I0512 23:45:15.617693 140260698863616 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_11/self_attention/out/kernel
I0512 23:45:15.617722 140260698863616 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_11/self_attention/query/kernel
I0512 23:45:15.617752 140260698863616 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_11/self_attention/value/kernel
I0512 23:45:15.617782 140260698863616 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_12/mlp/wi_0/kernel
I0512 23:45:15.617812 140260698863616 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_12/mlp/wi_1/kernel
I0512 23:45:15.617842 140260698863616 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_12/mlp/wo/kernel
I0512 23:45:15.617872 140260698863616 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_12/pre_mlp_layer_norm/scale
I0512 23:45:15.617902 140260698863616 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_12/pre_self_attention_layer_norm/scale
I0512 23:45:15.617932 140260698863616 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_12/self_attention/key/kernel
I0512 23:45:15.617962 140260698863616 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_12/self_attention/out/kernel
I0512 23:45:15.617992 140260698863616 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_12/self_attention/query/kernel
I0512 23:45:15.618022 140260698863616 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_12/self_attention/value/kernel
I0512 23:45:15.618051 140260698863616 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_13/mlp/wi_0/kernel
I0512 23:45:15.618082 140260698863616 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_13/mlp/wi_1/kernel
I0512 23:45:15.618112 140260698863616 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_13/mlp/wo/kernel
I0512 23:45:15.618141 140260698863616 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_13/pre_mlp_layer_norm/scale
I0512 23:45:15.618172 140260698863616 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_13/pre_self_attention_layer_norm/scale
I0512 23:45:15.618206 140260698863616 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_13/self_attention/key/kernel
I0512 23:45:15.618238 140260698863616 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_13/self_attention/out/kernel
I0512 23:45:15.618269 140260698863616 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_13/self_attention/query/kernel
I0512 23:45:15.618299 140260698863616 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_13/self_attention/value/kernel
I0512 23:45:15.618329 140260698863616 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_14/mlp/wi_0/kernel
I0512 23:45:15.618359 140260698863616 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_14/mlp/wi_1/kernel
I0512 23:45:15.618389 140260698863616 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_14/mlp/wo/kernel
I0512 23:45:15.618419 140260698863616 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_14/pre_mlp_layer_norm/scale
I0512 23:45:15.618449 140260698863616 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_14/pre_self_attention_layer_norm/scale
I0512 23:45:15.618482 140260698863616 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_14/self_attention/key/kernel
I0512 23:45:15.618512 140260698863616 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_14/self_attention/out/kernel
I0512 23:45:15.618542 140260698863616 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_14/self_attention/query/kernel
I0512 23:45:15.618571 140260698863616 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_14/self_attention/value/kernel
I0512 23:45:15.618602 140260698863616 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_15/mlp/wi_0/kernel
I0512 23:45:15.618631 140260698863616 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_15/mlp/wi_1/kernel
I0512 23:45:15.618662 140260698863616 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_15/mlp/wo/kernel
I0512 23:45:15.618716 140260698863616 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_15/pre_mlp_layer_norm/scale
I0512 23:45:15.618752 140260698863616 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_15/pre_self_attention_layer_norm/scale
I0512 23:45:15.618784 140260698863616 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_15/self_attention/key/kernel
I0512 23:45:15.618814 140260698863616 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_15/self_attention/out/kernel
I0512 23:45:15.618843 140260698863616 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_15/self_attention/query/kernel
I0512 23:45:15.618873 140260698863616 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_15/self_attention/value/kernel
I0512 23:45:15.618903 140260698863616 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_16/mlp/wi_0/kernel
I0512 23:45:15.618933 140260698863616 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_16/mlp/wi_1/kernel
I0512 23:45:15.618963 140260698863616 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_16/mlp/wo/kernel
I0512 23:45:15.618993 140260698863616 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_16/pre_mlp_layer_norm/scale
I0512 23:45:15.619023 140260698863616 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_16/pre_self_attention_layer_norm/scale
I0512 23:45:15.619053 140260698863616 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_16/self_attention/key/kernel
I0512 23:45:15.619083 140260698863616 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_16/self_attention/out/kernel
I0512 23:45:15.619113 140260698863616 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_16/self_attention/query/kernel
I0512 23:45:15.619143 140260698863616 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_16/self_attention/value/kernel
I0512 23:45:15.619173 140260698863616 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_17/extra_mlp/wi_0/kernel
I0512 23:45:15.619208 140260698863616 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_17/extra_mlp/wi_1/kernel
I0512 23:45:15.619240 140260698863616 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_17/extra_mlp/wo/kernel
I0512 23:45:15.619271 140260698863616 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_17/mlp/expert/wi_0/kernel
I0512 23:45:15.619301 140260698863616 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_17/mlp/expert/wi_1/kernel
I0512 23:45:15.619331 140260698863616 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_17/mlp/expert/wo/kernel
I0512 23:45:15.619360 140260698863616 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_17/mlp/router/router_weights/w/kernel
I0512 23:45:15.619390 140260698863616 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_17/pre_extra_mlp_layer_norm/scale
I0512 23:45:15.619420 140260698863616 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_17/pre_mlp_layer_norm/scale
I0512 23:45:15.619450 140260698863616 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_17/pre_self_attention_layer_norm/scale
I0512 23:45:15.619483 140260698863616 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_17/self_attention/key/kernel
I0512 23:45:15.619513 140260698863616 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_17/self_attention/out/kernel
I0512 23:45:15.619543 140260698863616 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_17/self_attention/query/kernel
I0512 23:45:15.619573 140260698863616 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_17/self_attention/value/kernel
I0512 23:45:15.619603 140260698863616 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_18/mlp/wi_0/kernel
I0512 23:45:15.619633 140260698863616 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_18/mlp/wi_1/kernel
I0512 23:45:15.619663 140260698863616 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_18/mlp/wo/kernel
I0512 23:45:15.619693 140260698863616 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_18/pre_mlp_layer_norm/scale
I0512 23:45:15.619723 140260698863616 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_18/pre_self_attention_layer_norm/scale
I0512 23:45:15.619753 140260698863616 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_18/self_attention/key/kernel
I0512 23:45:15.619782 140260698863616 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_18/self_attention/out/kernel
I0512 23:45:15.619812 140260698863616 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_18/self_attention/query/kernel
I0512 23:45:15.619842 140260698863616 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_18/self_attention/value/kernel
I0512 23:45:15.619873 140260698863616 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_19/mlp/wi_0/kernel
I0512 23:45:15.619903 140260698863616 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_19/mlp/wi_1/kernel
I0512 23:45:15.619932 140260698863616 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_19/mlp/wo/kernel
I0512 23:45:15.619962 140260698863616 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_19/pre_mlp_layer_norm/scale
I0512 23:45:15.619992 140260698863616 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_19/pre_self_attention_layer_norm/scale
I0512 23:45:15.620022 140260698863616 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_19/self_attention/key/kernel
I0512 23:45:15.620052 140260698863616 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_19/self_attention/out/kernel
I0512 23:45:15.620082 140260698863616 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_19/self_attention/query/kernel
I0512 23:45:15.620112 140260698863616 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_19/self_attention/value/kernel
I0512 23:45:15.620142 140260698863616 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_2/mlp/wi_0/kernel
I0512 23:45:15.620172 140260698863616 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_2/mlp/wi_1/kernel
I0512 23:45:15.620207 140260698863616 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_2/mlp/wo/kernel
I0512 23:45:15.620239 140260698863616 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_2/pre_mlp_layer_norm/scale
I0512 23:45:15.620270 140260698863616 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_2/pre_self_attention_layer_norm/scale
I0512 23:45:15.620300 140260698863616 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_2/self_attention/key/kernel
I0512 23:45:15.620330 140260698863616 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_2/self_attention/out/kernel
I0512 23:45:15.620360 140260698863616 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_2/self_attention/query/kernel
I0512 23:45:15.620390 140260698863616 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_2/self_attention/value/kernel
I0512 23:45:15.620420 140260698863616 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_20/mlp/wi_0/kernel
I0512 23:45:15.620450 140260698863616 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_20/mlp/wi_1/kernel
I0512 23:45:15.620482 140260698863616 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_20/mlp/wo/kernel
I0512 23:45:15.620513 140260698863616 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_20/pre_mlp_layer_norm/scale
I0512 23:45:15.620543 140260698863616 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_20/pre_self_attention_layer_norm/scale
I0512 23:45:15.620573 140260698863616 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_20/self_attention/key/kernel
I0512 23:45:15.620603 140260698863616 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_20/self_attention/out/kernel
I0512 23:45:15.620632 140260698863616 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_20/self_attention/query/kernel
I0512 23:45:15.620662 140260698863616 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_20/self_attention/value/kernel
I0512 23:45:15.620692 140260698863616 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_21/mlp/wi_0/kernel
I0512 23:45:15.620723 140260698863616 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_21/mlp/wi_1/kernel
I0512 23:45:15.620753 140260698863616 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_21/mlp/wo/kernel
I0512 23:45:15.620783 140260698863616 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_21/pre_mlp_layer_norm/scale
I0512 23:45:15.620813 140260698863616 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_21/pre_self_attention_layer_norm/scale
I0512 23:45:15.620843 140260698863616 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_21/self_attention/key/kernel
I0512 23:45:15.620872 140260698863616 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_21/self_attention/out/kernel
I0512 23:45:15.620903 140260698863616 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_21/self_attention/query/kernel
I0512 23:45:15.620932 140260698863616 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_21/self_attention/value/kernel
I0512 23:45:15.620962 140260698863616 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_22/mlp/wi_0/kernel
I0512 23:45:15.620992 140260698863616 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_22/mlp/wi_1/kernel
I0512 23:45:15.621022 140260698863616 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_22/mlp/wo/kernel
I0512 23:45:15.621052 140260698863616 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_22/pre_mlp_layer_norm/scale
I0512 23:45:15.621082 140260698863616 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_22/pre_self_attention_layer_norm/scale
I0512 23:45:15.621112 140260698863616 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_22/self_attention/key/kernel
I0512 23:45:15.621141 140260698863616 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_22/self_attention/out/kernel
I0512 23:45:15.621171 140260698863616 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_22/self_attention/query/kernel
I0512 23:45:15.621208 140260698863616 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_22/self_attention/value/kernel
I0512 23:45:15.621241 140260698863616 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_23/extra_mlp/wi_0/kernel
I0512 23:45:15.621271 140260698863616 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_23/extra_mlp/wi_1/kernel
I0512 23:45:15.621301 140260698863616 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_23/extra_mlp/wo/kernel
I0512 23:45:15.621330 140260698863616 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_23/mlp/expert/wi_0/kernel
I0512 23:45:15.621360 140260698863616 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_23/mlp/expert/wi_1/kernel
I0512 23:45:15.621390 140260698863616 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_23/mlp/expert/wo/kernel
I0512 23:45:15.621420 140260698863616 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_23/mlp/router/router_weights/w/kernel
I0512 23:45:15.621450 140260698863616 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_23/pre_extra_mlp_layer_norm/scale
I0512 23:45:15.621482 140260698863616 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_23/pre_mlp_layer_norm/scale
I0512 23:45:15.621512 140260698863616 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_23/pre_self_attention_layer_norm/scale
I0512 23:45:15.621542 140260698863616 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_23/self_attention/key/kernel
I0512 23:45:15.621572 140260698863616 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_23/self_attention/out/kernel
I0512 23:45:15.621602 140260698863616 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_23/self_attention/query/kernel
I0512 23:45:15.621632 140260698863616 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_23/self_attention/value/kernel
I0512 23:45:15.621662 140260698863616 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_3/mlp/wi_0/kernel
I0512 23:45:15.621691 140260698863616 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_3/mlp/wi_1/kernel
I0512 23:45:15.621721 140260698863616 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_3/mlp/wo/kernel
I0512 23:45:15.621751 140260698863616 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_3/pre_mlp_layer_norm/scale
I0512 23:45:15.621782 140260698863616 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_3/pre_self_attention_layer_norm/scale
I0512 23:45:15.621811 140260698863616 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_3/self_attention/key/kernel
I0512 23:45:15.621842 140260698863616 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_3/self_attention/out/kernel
I0512 23:45:15.621872 140260698863616 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_3/self_attention/query/kernel
I0512 23:45:15.621901 140260698863616 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_3/self_attention/value/kernel
I0512 23:45:15.621932 140260698863616 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_4/mlp/wi_0/kernel
I0512 23:45:15.621961 140260698863616 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_4/mlp/wi_1/kernel
I0512 23:45:15.621991 140260698863616 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_4/mlp/wo/kernel
I0512 23:45:15.622021 140260698863616 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_4/pre_mlp_layer_norm/scale
I0512 23:45:15.622051 140260698863616 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_4/pre_self_attention_layer_norm/scale
I0512 23:45:15.622081 140260698863616 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_4/self_attention/key/kernel
I0512 23:45:15.622110 140260698863616 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_4/self_attention/out/kernel
I0512 23:45:15.622140 140260698863616 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_4/self_attention/query/kernel
I0512 23:45:15.622170 140260698863616 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_4/self_attention/value/kernel
I0512 23:45:15.622199 140260698863616 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_5/extra_mlp/wi_0/kernel
I0512 23:45:15.622236 140260698863616 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_5/extra_mlp/wi_1/kernel
I0512 23:45:15.622266 140260698863616 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_5/extra_mlp/wo/kernel
I0512 23:45:15.622296 140260698863616 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_5/mlp/expert/wi_0/kernel
I0512 23:45:15.622325 140260698863616 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_5/mlp/expert/wi_1/kernel
I0512 23:45:15.622355 140260698863616 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_5/mlp/expert/wo/kernel
I0512 23:45:15.622385 140260698863616 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_5/mlp/router/router_weights/w/kernel
I0512 23:45:15.622415 140260698863616 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_5/pre_extra_mlp_layer_norm/scale
I0512 23:45:15.622445 140260698863616 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_5/pre_mlp_layer_norm/scale
I0512 23:45:15.622477 140260698863616 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_5/pre_self_attention_layer_norm/scale
I0512 23:45:15.622508 140260698863616 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_5/self_attention/key/kernel
I0512 23:45:15.622538 140260698863616 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_5/self_attention/out/kernel
I0512 23:45:15.622567 140260698863616 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_5/self_attention/query/kernel
I0512 23:45:15.622598 140260698863616 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_5/self_attention/value/kernel
I0512 23:45:15.622627 140260698863616 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_6/mlp/wi_0/kernel
I0512 23:45:15.622657 140260698863616 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_6/mlp/wi_1/kernel
I0512 23:45:15.622705 140260698863616 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_6/mlp/wo/kernel
I0512 23:45:15.622742 140260698863616 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_6/pre_mlp_layer_norm/scale
I0512 23:45:15.622774 140260698863616 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_6/pre_self_attention_layer_norm/scale
I0512 23:45:15.622804 140260698863616 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_6/self_attention/key/kernel
I0512 23:45:15.622834 140260698863616 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_6/self_attention/out/kernel
I0512 23:45:15.622863 140260698863616 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_6/self_attention/query/kernel
I0512 23:45:15.622894 140260698863616 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_6/self_attention/value/kernel
I0512 23:45:15.622924 140260698863616 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_7/mlp/wi_0/kernel
I0512 23:45:15.622954 140260698863616 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_7/mlp/wi_1/kernel
I0512 23:45:15.622984 140260698863616 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_7/mlp/wo/kernel
I0512 23:45:15.623014 140260698863616 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_7/pre_mlp_layer_norm/scale
I0512 23:45:15.623044 140260698863616 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_7/pre_self_attention_layer_norm/scale
I0512 23:45:15.623074 140260698863616 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_7/self_attention/key/kernel
I0512 23:45:15.623104 140260698863616 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_7/self_attention/out/kernel
I0512 23:45:15.623133 140260698863616 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_7/self_attention/query/kernel
I0512 23:45:15.623163 140260698863616 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_7/self_attention/value/kernel
I0512 23:45:15.623193 140260698863616 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_8/mlp/wi_0/kernel
I0512 23:45:15.623229 140260698863616 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_8/mlp/wi_1/kernel
I0512 23:45:15.623260 140260698863616 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_8/mlp/wo/kernel
I0512 23:45:15.623289 140260698863616 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_8/pre_mlp_layer_norm/scale
I0512 23:45:15.623319 140260698863616 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_8/pre_self_attention_layer_norm/scale
I0512 23:45:15.623349 140260698863616 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_8/self_attention/key/kernel
I0512 23:45:15.623379 140260698863616 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_8/self_attention/out/kernel
I0512 23:45:15.623409 140260698863616 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_8/self_attention/query/kernel
I0512 23:45:15.623439 140260698863616 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_8/self_attention/value/kernel
I0512 23:45:15.623471 140260698863616 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_9/mlp/wi_0/kernel
I0512 23:45:15.623502 140260698863616 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_9/mlp/wi_1/kernel
I0512 23:45:15.623532 140260698863616 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_9/mlp/wo/kernel
I0512 23:45:15.623562 140260698863616 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_9/pre_mlp_layer_norm/scale
I0512 23:45:15.623592 140260698863616 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_9/pre_self_attention_layer_norm/scale
I0512 23:45:15.623622 140260698863616 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_9/self_attention/key/kernel
I0512 23:45:15.623652 140260698863616 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_9/self_attention/out/kernel
I0512 23:45:15.623682 140260698863616 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_9/self_attention/query/kernel
I0512 23:45:15.623712 140260698863616 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_9/self_attention/value/kernel
I0512 23:45:15.623741 140260698863616 checkpoints.py:1100] Restoring key from ckpt: target/decoder/logits_dense/kernel
I0512 23:45:15.623771 140260698863616 checkpoints.py:1100] Restoring key from ckpt: target/token_embedder/embedding
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
I0000 00:00:1715557515.656016  390926 gcs_resource.cc:109] Using default AdmissionQueue with limit 32
I0000 00:00:1715557515.660509  393123 google_auth_provider.cc:180] Running on GCE, using service account 1081612546678-compute@developer.gserviceaccount.com
I0512 23:45:21.209086 139666521167872 train.py:421] Initialize/restore complete (10.92 seconds).
I0512 23:45:21.210534 139666521167872 train.py:573] Saving checkpoint before the training loop starts.
I0512 23:45:21.403812 140180528838656 train.py:421] Initialize/restore complete (10.68 seconds).
I0512 23:45:21.406319 140180528838656 train.py:573] Saving checkpoint before the training loop starts.
I0512 23:45:21.499715 139876724979712 train.py:421] Initialize/restore complete (11.22 seconds).
I0512 23:45:21.501241 139876724979712 train.py:573] Saving checkpoint before the training loop starts.
I0512 23:45:21.568778 140643590531072 train.py:421] Initialize/restore complete (11.37 seconds).
I0512 23:45:21.570369 140643590531072 train.py:573] Saving checkpoint before the training loop starts.
I0512 23:45:21.608773 140143678494720 train.py:421] Initialize/restore complete (11.57 seconds).
I0512 23:45:21.782229 140143678494720 utils.py:1372] Variable decoder/decoder_norm/scale                                                       size 2048         shape (embed=2048)                             partition spec ('model',)
I0512 23:45:21.782527 140143678494720 utils.py:1372] Variable decoder/layers_0/mlp/wi_0/kernel                                                 size 33554432     shape (mlp_embed=2048, mlp=16384)              partition spec (None, 'model')
I0512 23:45:21.782604 140143678494720 utils.py:1372] Variable decoder/layers_0/mlp/wi_1/kernel                                                 size 16777216     shape (mlp_embed=2048, mlp=8192)               partition spec (None, 'model')
I0512 23:45:21.782657 140143678494720 utils.py:1372] Variable decoder/layers_0/mlp/wo/kernel                                                   size 16777216     shape (mlp=8192, mlp_embed=2048)               partition spec ('model', None)
I0512 23:45:21.782706 140143678494720 utils.py:1372] Variable decoder/layers_0/pre_mlp_layer_norm/scale                                        size 2048         shape (embed=2048)                             partition spec ('model',)
I0512 23:45:21.782753 140143678494720 utils.py:1372] Variable decoder/layers_0/pre_self_attention_layer_norm/scale                             size 2048         shape (embed=2048)                             partition spec ('model',)
I0512 23:45:21.782800 140143678494720 utils.py:1372] Variable decoder/layers_0/self_attention/key/kernel                                       size 6291456      shape (embed=2048, joined_kv=3072)             partition spec (None, 'model')
I0512 23:45:21.782848 140143678494720 utils.py:1372] Variable decoder/layers_0/self_attention/out/kernel                                       size 6291456      shape (joined_kv=3072, embed=2048)             partition spec ('model', None)
I0512 23:45:21.782894 140143678494720 utils.py:1372] Variable decoder/layers_0/self_attention/query/kernel                                     size 6291456      shape (embed=2048, joined_kv=3072)             partition spec (None, 'model')
I0512 23:45:21.782941 140143678494720 utils.py:1372] Variable decoder/layers_0/self_attention/value/kernel                                     size 6291456      shape (embed=2048, joined_kv=3072)             partition spec (None, 'model')
I0512 23:45:21.782987 140143678494720 utils.py:1372] Variable decoder/layers_1/mlp/wi_0/kernel                                                 size 33554432     shape (mlp_embed=2048, mlp=16384)              partition spec (None, 'model')
I0512 23:45:21.783034 140143678494720 utils.py:1372] Variable decoder/layers_1/mlp/wi_1/kernel                                                 size 16777216     shape (mlp_embed=2048, mlp=8192)               partition spec (None, 'model')
I0512 23:45:21.783079 140143678494720 utils.py:1372] Variable decoder/layers_1/mlp/wo/kernel                                                   size 16777216     shape (mlp=8192, mlp_embed=2048)               partition spec ('model', None)
I0512 23:45:21.783125 140143678494720 utils.py:1372] Variable decoder/layers_1/pre_mlp_layer_norm/scale                                        size 2048         shape (embed=2048)                             partition spec ('model',)
I0512 23:45:21.783171 140143678494720 utils.py:1372] Variable decoder/layers_1/pre_self_attention_layer_norm/scale                             size 2048         shape (embed=2048)                             partition spec ('model',)
I0512 23:45:21.783217 140143678494720 utils.py:1372] Variable decoder/layers_1/self_attention/key/kernel                                       size 6291456      shape (embed=2048, joined_kv=3072)             partition spec (None, 'model')
I0512 23:45:21.783263 140143678494720 utils.py:1372] Variable decoder/layers_1/self_attention/out/kernel                                       size 6291456      shape (joined_kv=3072, embed=2048)             partition spec ('model', None)
I0512 23:45:21.783311 140143678494720 utils.py:1372] Variable decoder/layers_1/self_attention/query/kernel                                     size 6291456      shape (embed=2048, joined_kv=3072)             partition spec (None, 'model')
I0512 23:45:21.783359 140143678494720 utils.py:1372] Variable decoder/layers_1/self_attention/value/kernel                                     size 6291456      shape (embed=2048, joined_kv=3072)             partition spec (None, 'model')
I0512 23:45:21.783405 140143678494720 utils.py:1372] Variable decoder/layers_10/mlp/wi_0/kernel                                                size 33554432     shape (mlp_embed=2048, mlp=16384)              partition spec (None, 'model')
I0512 23:45:21.783452 140143678494720 utils.py:1372] Variable decoder/layers_10/mlp/wi_1/kernel                                                size 16777216     shape (mlp_embed=2048, mlp=8192)               partition spec (None, 'model')
I0512 23:45:21.783499 140143678494720 utils.py:1372] Variable decoder/layers_10/mlp/wo/kernel                                                  size 16777216     shape (mlp=8192, mlp_embed=2048)               partition spec ('model', None)
I0512 23:45:21.783551 140143678494720 utils.py:1372] Variable decoder/layers_10/pre_mlp_layer_norm/scale                                       size 2048         shape (embed=2048)                             partition spec ('model',)
I0512 23:45:21.783598 140143678494720 utils.py:1372] Variable decoder/layers_10/pre_self_attention_layer_norm/scale                            size 2048         shape (embed=2048)                             partition spec ('model',)
I0512 23:45:21.783645 140143678494720 utils.py:1372] Variable decoder/layers_10/self_attention/key/kernel                                      size 6291456      shape (embed=2048, joined_kv=3072)             partition spec (None, 'model')
I0512 23:45:21.783690 140143678494720 utils.py:1372] Variable decoder/layers_10/self_attention/out/kernel                                      size 6291456      shape (joined_kv=3072, embed=2048)             partition spec ('model', None)
I0512 23:45:21.783736 140143678494720 utils.py:1372] Variable decoder/layers_10/self_attention/query/kernel                                    size 6291456      shape (embed=2048, joined_kv=3072)             partition spec (None, 'model')
I0512 23:45:21.783781 140143678494720 utils.py:1372] Variable decoder/layers_10/self_attention/value/kernel                                    size 6291456      shape (embed=2048, joined_kv=3072)             partition spec (None, 'model')
I0512 23:45:21.783827 140143678494720 utils.py:1372] Variable decoder/layers_11/extra_mlp/wi_0/kernel                                          size 33554432     shape (mlp_embed=2048, mlp=16384)              partition spec (None, 'model')
I0512 23:45:21.783873 140143678494720 utils.py:1372] Variable decoder/layers_11/extra_mlp/wi_1/kernel                                          size 16777216     shape (mlp_embed=2048, mlp=8192)               partition spec (None, 'model')
I0512 23:45:21.783918 140143678494720 utils.py:1372] Variable decoder/layers_11/extra_mlp/wo/kernel                                            size 16777216     shape (mlp=8192, mlp_embed=2048)               partition spec ('model', None)
I0512 23:45:21.783964 140143678494720 utils.py:1372] Variable decoder/layers_11/mlp/expert/wi_0/kernel                                         size 268435456    shape (expert=8, mlp_embed=2048, expert_mlp=16384) partition spec ('expert', None, 'model')
I0512 23:45:21.784011 140143678494720 utils.py:1372] Variable decoder/layers_11/mlp/expert/wi_1/kernel                                         size 134217728    shape (expert=8, mlp_embed=2048, expert_mlp=8192) partition spec ('expert', None, 'model')
I0512 23:45:21.784057 140143678494720 utils.py:1372] Variable decoder/layers_11/mlp/expert/wo/kernel                                           size 134217728    shape (expert=8, expert_mlp=8192, mlp_embed=2048) partition spec ('expert', 'model', None)
I0512 23:45:21.784103 140143678494720 utils.py:1372] Variable decoder/layers_11/mlp/router/router_weights/w/kernel                             size 16384        shape (embed=2048, unmodeled=8)                partition spec ('model', None)
I0512 23:45:21.784149 140143678494720 utils.py:1372] Variable decoder/layers_11/pre_extra_mlp_layer_norm/scale                                 size 2048         shape (embed=2048)                             partition spec ('model',)
I0512 23:45:21.784195 140143678494720 utils.py:1372] Variable decoder/layers_11/pre_mlp_layer_norm/scale                                       size 2048         shape (embed=2048)                             partition spec ('model',)
I0512 23:45:21.784240 140143678494720 utils.py:1372] Variable decoder/layers_11/pre_self_attention_layer_norm/scale                            size 2048         shape (embed=2048)                             partition spec ('model',)
I0512 23:45:21.784286 140143678494720 utils.py:1372] Variable decoder/layers_11/self_attention/key/kernel                                      size 6291456      shape (embed=2048, joined_kv=3072)             partition spec (None, 'model')
I0512 23:45:21.784335 140143678494720 utils.py:1372] Variable decoder/layers_11/self_attention/out/kernel                                      size 6291456      shape (joined_kv=3072, embed=2048)             partition spec ('model', None)
I0512 23:45:21.784381 140143678494720 utils.py:1372] Variable decoder/layers_11/self_attention/query/kernel                                    size 6291456      shape (embed=2048, joined_kv=3072)             partition spec (None, 'model')
I0512 23:45:21.784426 140143678494720 utils.py:1372] Variable decoder/layers_11/self_attention/value/kernel                                    size 6291456      shape (embed=2048, joined_kv=3072)             partition spec (None, 'model')
I0512 23:45:21.784472 140143678494720 utils.py:1372] Variable decoder/layers_12/mlp/wi_0/kernel                                                size 33554432     shape (mlp_embed=2048, mlp=16384)              partition spec (None, 'model')
I0512 23:45:21.784519 140143678494720 utils.py:1372] Variable decoder/layers_12/mlp/wi_1/kernel                                                size 16777216     shape (mlp_embed=2048, mlp=8192)               partition spec (None, 'model')
I0512 23:45:21.784595 140143678494720 utils.py:1372] Variable decoder/layers_12/mlp/wo/kernel                                                  size 16777216     shape (mlp=8192, mlp_embed=2048)               partition spec ('model', None)
I0512 23:45:21.784690 140143678494720 utils.py:1372] Variable decoder/layers_12/pre_mlp_layer_norm/scale                                       size 2048         shape (embed=2048)                             partition spec ('model',)
I0512 23:45:21.784740 140143678494720 utils.py:1372] Variable decoder/layers_12/pre_self_attention_layer_norm/scale                            size 2048         shape (embed=2048)                             partition spec ('model',)
I0512 23:45:21.784786 140143678494720 utils.py:1372] Variable decoder/layers_12/self_attention/key/kernel                                      size 6291456      shape (embed=2048, joined_kv=3072)             partition spec (None, 'model')
I0512 23:45:21.784831 140143678494720 utils.py:1372] Variable decoder/layers_12/self_attention/out/kernel                                      size 6291456      shape (joined_kv=3072, embed=2048)             partition spec ('model', None)
I0512 23:45:21.784878 140143678494720 utils.py:1372] Variable decoder/layers_12/self_attention/query/kernel                                    size 6291456      shape (embed=2048, joined_kv=3072)             partition spec (None, 'model')
I0512 23:45:21.784924 140143678494720 utils.py:1372] Variable decoder/layers_12/self_attention/value/kernel                                    size 6291456      shape (embed=2048, joined_kv=3072)             partition spec (None, 'model')
I0512 23:45:21.784969 140143678494720 utils.py:1372] Variable decoder/layers_13/mlp/wi_0/kernel                                                size 33554432     shape (mlp_embed=2048, mlp=16384)              partition spec (None, 'model')
I0512 23:45:21.785016 140143678494720 utils.py:1372] Variable decoder/layers_13/mlp/wi_1/kernel                                                size 16777216     shape (mlp_embed=2048, mlp=8192)               partition spec (None, 'model')
I0512 23:45:21.785061 140143678494720 utils.py:1372] Variable decoder/layers_13/mlp/wo/kernel                                                  size 16777216     shape (mlp=8192, mlp_embed=2048)               partition spec ('model', None)
I0512 23:45:21.785107 140143678494720 utils.py:1372] Variable decoder/layers_13/pre_mlp_layer_norm/scale                                       size 2048         shape (embed=2048)                             partition spec ('model',)
I0512 23:45:21.785154 140143678494720 utils.py:1372] Variable decoder/layers_13/pre_self_attention_layer_norm/scale                            size 2048         shape (embed=2048)                             partition spec ('model',)
I0512 23:45:21.785201 140143678494720 utils.py:1372] Variable decoder/layers_13/self_attention/key/kernel                                      size 6291456      shape (embed=2048, joined_kv=3072)             partition spec (None, 'model')
I0512 23:45:21.785247 140143678494720 utils.py:1372] Variable decoder/layers_13/self_attention/out/kernel                                      size 6291456      shape (joined_kv=3072, embed=2048)             partition spec ('model', None)
I0512 23:45:21.785294 140143678494720 utils.py:1372] Variable decoder/layers_13/self_attention/query/kernel                                    size 6291456      shape (embed=2048, joined_kv=3072)             partition spec (None, 'model')
I0512 23:45:21.785343 140143678494720 utils.py:1372] Variable decoder/layers_13/self_attention/value/kernel                                    size 6291456      shape (embed=2048, joined_kv=3072)             partition spec (None, 'model')
I0512 23:45:21.785390 140143678494720 utils.py:1372] Variable decoder/layers_14/mlp/wi_0/kernel                                                size 33554432     shape (mlp_embed=2048, mlp=16384)              partition spec (None, 'model')
I0512 23:45:21.785436 140143678494720 utils.py:1372] Variable decoder/layers_14/mlp/wi_1/kernel                                                size 16777216     shape (mlp_embed=2048, mlp=8192)               partition spec (None, 'model')
I0512 23:45:21.785482 140143678494720 utils.py:1372] Variable decoder/layers_14/mlp/wo/kernel                                                  size 16777216     shape (mlp=8192, mlp_embed=2048)               partition spec ('model', None)
I0512 23:45:21.785527 140143678494720 utils.py:1372] Variable decoder/layers_14/pre_mlp_layer_norm/scale                                       size 2048         shape (embed=2048)                             partition spec ('model',)
I0512 23:45:21.785581 140143678494720 utils.py:1372] Variable decoder/layers_14/pre_self_attention_layer_norm/scale                            size 2048         shape (embed=2048)                             partition spec ('model',)
I0512 23:45:21.785627 140143678494720 utils.py:1372] Variable decoder/layers_14/self_attention/key/kernel                                      size 6291456      shape (embed=2048, joined_kv=3072)             partition spec (None, 'model')
I0512 23:45:21.785672 140143678494720 utils.py:1372] Variable decoder/layers_14/self_attention/out/kernel                                      size 6291456      shape (joined_kv=3072, embed=2048)             partition spec ('model', None)
I0512 23:45:21.785718 140143678494720 utils.py:1372] Variable decoder/layers_14/self_attention/query/kernel                                    size 6291456      shape (embed=2048, joined_kv=3072)             partition spec (None, 'model')
I0512 23:45:21.785763 140143678494720 utils.py:1372] Variable decoder/layers_14/self_attention/value/kernel                                    size 6291456      shape (embed=2048, joined_kv=3072)             partition spec (None, 'model')
I0512 23:45:21.785809 140143678494720 utils.py:1372] Variable decoder/layers_15/mlp/wi_0/kernel                                                size 33554432     shape (mlp_embed=2048, mlp=16384)              partition spec (None, 'model')
I0512 23:45:21.785855 140143678494720 utils.py:1372] Variable decoder/layers_15/mlp/wi_1/kernel                                                size 16777216     shape (mlp_embed=2048, mlp=8192)               partition spec (None, 'model')
I0512 23:45:21.785901 140143678494720 utils.py:1372] Variable decoder/layers_15/mlp/wo/kernel                                                  size 16777216     shape (mlp=8192, mlp_embed=2048)               partition spec ('model', None)
I0512 23:45:21.785946 140143678494720 utils.py:1372] Variable decoder/layers_15/pre_mlp_layer_norm/scale                                       size 2048         shape (embed=2048)                             partition spec ('model',)
I0512 23:45:21.785992 140143678494720 utils.py:1372] Variable decoder/layers_15/pre_self_attention_layer_norm/scale                            size 2048         shape (embed=2048)                             partition spec ('model',)
I0512 23:45:21.786038 140143678494720 utils.py:1372] Variable decoder/layers_15/self_attention/key/kernel                                      size 6291456      shape (embed=2048, joined_kv=3072)             partition spec (None, 'model')
I0512 23:45:21.786083 140143678494720 utils.py:1372] Variable decoder/layers_15/self_attention/out/kernel                                      size 6291456      shape (joined_kv=3072, embed=2048)             partition spec ('model', None)
I0512 23:45:21.786129 140143678494720 utils.py:1372] Variable decoder/layers_15/self_attention/query/kernel                                    size 6291456      shape (embed=2048, joined_kv=3072)             partition spec (None, 'model')
I0512 23:45:21.786175 140143678494720 utils.py:1372] Variable decoder/layers_15/self_attention/value/kernel                                    size 6291456      shape (embed=2048, joined_kv=3072)             partition spec (None, 'model')
I0512 23:45:21.786220 140143678494720 utils.py:1372] Variable decoder/layers_16/mlp/wi_0/kernel                                                size 33554432     shape (mlp_embed=2048, mlp=16384)              partition spec (None, 'model')
I0512 23:45:21.786266 140143678494720 utils.py:1372] Variable decoder/layers_16/mlp/wi_1/kernel                                                size 16777216     shape (mlp_embed=2048, mlp=8192)               partition spec (None, 'model')
I0512 23:45:21.786313 140143678494720 utils.py:1372] Variable decoder/layers_16/mlp/wo/kernel                                                  size 16777216     shape (mlp=8192, mlp_embed=2048)               partition spec ('model', None)
I0512 23:45:21.786360 140143678494720 utils.py:1372] Variable decoder/layers_16/pre_mlp_layer_norm/scale                                       size 2048         shape (embed=2048)                             partition spec ('model',)
I0512 23:45:21.786405 140143678494720 utils.py:1372] Variable decoder/layers_16/pre_self_attention_layer_norm/scale                            size 2048         shape (embed=2048)                             partition spec ('model',)
I0512 23:45:21.786451 140143678494720 utils.py:1372] Variable decoder/layers_16/self_attention/key/kernel                                      size 6291456      shape (embed=2048, joined_kv=3072)             partition spec (None, 'model')
I0512 23:45:21.786497 140143678494720 utils.py:1372] Variable decoder/layers_16/self_attention/out/kernel                                      size 6291456      shape (joined_kv=3072, embed=2048)             partition spec ('model', None)
I0512 23:45:21.786549 140143678494720 utils.py:1372] Variable decoder/layers_16/self_attention/query/kernel                                    size 6291456      shape (embed=2048, joined_kv=3072)             partition spec (None, 'model')
I0512 23:45:21.786597 140143678494720 utils.py:1372] Variable decoder/layers_16/self_attention/value/kernel                                    size 6291456      shape (embed=2048, joined_kv=3072)             partition spec (None, 'model')
I0512 23:45:21.786643 140143678494720 utils.py:1372] Variable decoder/layers_17/extra_mlp/wi_0/kernel                                          size 33554432     shape (mlp_embed=2048, mlp=16384)              partition spec (None, 'model')
I0512 23:45:21.786689 140143678494720 utils.py:1372] Variable decoder/layers_17/extra_mlp/wi_1/kernel                                          size 16777216     shape (mlp_embed=2048, mlp=8192)               partition spec (None, 'model')
I0512 23:45:21.786736 140143678494720 utils.py:1372] Variable decoder/layers_17/extra_mlp/wo/kernel                                            size 16777216     shape (mlp=8192, mlp_embed=2048)               partition spec ('model', None)
I0512 23:45:21.786797 140143678494720 utils.py:1372] Variable decoder/layers_17/mlp/expert/wi_0/kernel                                         size 268435456    shape (expert=8, mlp_embed=2048, expert_mlp=16384) partition spec ('expert', None, 'model')
I0512 23:45:21.786846 140143678494720 utils.py:1372] Variable decoder/layers_17/mlp/expert/wi_1/kernel                                         size 134217728    shape (expert=8, mlp_embed=2048, expert_mlp=8192) partition spec ('expert', None, 'model')
I0512 23:45:21.786893 140143678494720 utils.py:1372] Variable decoder/layers_17/mlp/expert/wo/kernel                                           size 134217728    shape (expert=8, expert_mlp=8192, mlp_embed=2048) partition spec ('expert', 'model', None)
I0512 23:45:21.786940 140143678494720 utils.py:1372] Variable decoder/layers_17/mlp/router/router_weights/w/kernel                             size 16384        shape (embed=2048, unmodeled=8)                partition spec ('model', None)
I0512 23:45:21.786985 140143678494720 utils.py:1372] Variable decoder/layers_17/pre_extra_mlp_layer_norm/scale                                 size 2048         shape (embed=2048)                             partition spec ('model',)
I0512 23:45:21.787032 140143678494720 utils.py:1372] Variable decoder/layers_17/pre_mlp_layer_norm/scale                                       size 2048         shape (embed=2048)                             partition spec ('model',)
I0512 23:45:21.787078 140143678494720 utils.py:1372] Variable decoder/layers_17/pre_self_attention_layer_norm/scale                            size 2048         shape (embed=2048)                             partition spec ('model',)
I0512 23:45:21.787123 140143678494720 utils.py:1372] Variable decoder/layers_17/self_attention/key/kernel                                      size 6291456      shape (embed=2048, joined_kv=3072)             partition spec (None, 'model')
I0512 23:45:21.787169 140143678494720 utils.py:1372] Variable decoder/layers_17/self_attention/out/kernel                                      size 6291456      shape (joined_kv=3072, embed=2048)             partition spec ('model', None)
I0512 23:45:21.787215 140143678494720 utils.py:1372] Variable decoder/layers_17/self_attention/query/kernel                                    size 6291456      shape (embed=2048, joined_kv=3072)             partition spec (None, 'model')
I0512 23:45:21.787261 140143678494720 utils.py:1372] Variable decoder/layers_17/self_attention/value/kernel                                    size 6291456      shape (embed=2048, joined_kv=3072)             partition spec (None, 'model')
I0512 23:45:21.787309 140143678494720 utils.py:1372] Variable decoder/layers_18/mlp/wi_0/kernel                                                size 33554432     shape (mlp_embed=2048, mlp=16384)              partition spec (None, 'model')
I0512 23:45:21.787357 140143678494720 utils.py:1372] Variable decoder/layers_18/mlp/wi_1/kernel                                                size 16777216     shape (mlp_embed=2048, mlp=8192)               partition spec (None, 'model')
I0512 23:45:21.787403 140143678494720 utils.py:1372] Variable decoder/layers_18/mlp/wo/kernel                                                  size 16777216     shape (mlp=8192, mlp_embed=2048)               partition spec ('model', None)
I0512 23:45:21.787449 140143678494720 utils.py:1372] Variable decoder/layers_18/pre_mlp_layer_norm/scale                                       size 2048         shape (embed=2048)                             partition spec ('model',)
I0512 23:45:21.787494 140143678494720 utils.py:1372] Variable decoder/layers_18/pre_self_attention_layer_norm/scale                            size 2048         shape (embed=2048)                             partition spec ('model',)
I0512 23:45:21.787545 140143678494720 utils.py:1372] Variable decoder/layers_18/self_attention/key/kernel                                      size 6291456      shape (embed=2048, joined_kv=3072)             partition spec (None, 'model')
I0512 23:45:21.787593 140143678494720 utils.py:1372] Variable decoder/layers_18/self_attention/out/kernel                                      size 6291456      shape (joined_kv=3072, embed=2048)             partition spec ('model', None)
I0512 23:45:21.787639 140143678494720 utils.py:1372] Variable decoder/layers_18/self_attention/query/kernel                                    size 6291456      shape (embed=2048, joined_kv=3072)             partition spec (None, 'model')
I0512 23:45:21.787685 140143678494720 utils.py:1372] Variable decoder/layers_18/self_attention/value/kernel                                    size 6291456      shape (embed=2048, joined_kv=3072)             partition spec (None, 'model')
I0512 23:45:21.787731 140143678494720 utils.py:1372] Variable decoder/layers_19/mlp/wi_0/kernel                                                size 33554432     shape (mlp_embed=2048, mlp=16384)              partition spec (None, 'model')
I0512 23:45:21.787778 140143678494720 utils.py:1372] Variable decoder/layers_19/mlp/wi_1/kernel                                                size 16777216     shape (mlp_embed=2048, mlp=8192)               partition spec (None, 'model')
I0512 23:45:21.787823 140143678494720 utils.py:1372] Variable decoder/layers_19/mlp/wo/kernel                                                  size 16777216     shape (mlp=8192, mlp_embed=2048)               partition spec ('model', None)
I0512 23:45:21.787868 140143678494720 utils.py:1372] Variable decoder/layers_19/pre_mlp_layer_norm/scale                                       size 2048         shape (embed=2048)                             partition spec ('model',)
I0512 23:45:21.787914 140143678494720 utils.py:1372] Variable decoder/layers_19/pre_self_attention_layer_norm/scale                            size 2048         shape (embed=2048)                             partition spec ('model',)
I0512 23:45:21.787960 140143678494720 utils.py:1372] Variable decoder/layers_19/self_attention/key/kernel                                      size 6291456      shape (embed=2048, joined_kv=3072)             partition spec (None, 'model')
I0512 23:45:21.788006 140143678494720 utils.py:1372] Variable decoder/layers_19/self_attention/out/kernel                                      size 6291456      shape (joined_kv=3072, embed=2048)             partition spec ('model', None)
I0512 23:45:21.788051 140143678494720 utils.py:1372] Variable decoder/layers_19/self_attention/query/kernel                                    size 6291456      shape (embed=2048, joined_kv=3072)             partition spec (None, 'model')
I0512 23:45:21.788096 140143678494720 utils.py:1372] Variable decoder/layers_19/self_attention/value/kernel                                    size 6291456      shape (embed=2048, joined_kv=3072)             partition spec (None, 'model')
I0512 23:45:21.788141 140143678494720 utils.py:1372] Variable decoder/layers_2/mlp/wi_0/kernel                                                 size 33554432     shape (mlp_embed=2048, mlp=16384)              partition spec (None, 'model')
I0512 23:45:21.788187 140143678494720 utils.py:1372] Variable decoder/layers_2/mlp/wi_1/kernel                                                 size 16777216     shape (mlp_embed=2048, mlp=8192)               partition spec (None, 'model')
I0512 23:45:21.788233 140143678494720 utils.py:1372] Variable decoder/layers_2/mlp/wo/kernel                                                   size 16777216     shape (mlp=8192, mlp_embed=2048)               partition spec ('model', None)
I0512 23:45:21.788278 140143678494720 utils.py:1372] Variable decoder/layers_2/pre_mlp_layer_norm/scale                                        size 2048         shape (embed=2048)                             partition spec ('model',)
I0512 23:45:21.788326 140143678494720 utils.py:1372] Variable decoder/layers_2/pre_self_attention_layer_norm/scale                             size 2048         shape (embed=2048)                             partition spec ('model',)
I0512 23:45:21.788372 140143678494720 utils.py:1372] Variable decoder/layers_2/self_attention/key/kernel                                       size 6291456      shape (embed=2048, joined_kv=3072)             partition spec (None, 'model')
I0512 23:45:21.788417 140143678494720 utils.py:1372] Variable decoder/layers_2/self_attention/out/kernel                                       size 6291456      shape (joined_kv=3072, embed=2048)             partition spec ('model', None)
I0512 23:45:21.788462 140143678494720 utils.py:1372] Variable decoder/layers_2/self_attention/query/kernel                                     size 6291456      shape (embed=2048, joined_kv=3072)             partition spec (None, 'model')
I0512 23:45:21.788507 140143678494720 utils.py:1372] Variable decoder/layers_2/self_attention/value/kernel                                     size 6291456      shape (embed=2048, joined_kv=3072)             partition spec (None, 'model')
I0512 23:45:21.788586 140143678494720 utils.py:1372] Variable decoder/layers_20/mlp/wi_0/kernel                                                size 33554432     shape (mlp_embed=2048, mlp=16384)              partition spec (None, 'model')
I0512 23:45:21.788638 140143678494720 utils.py:1372] Variable decoder/layers_20/mlp/wi_1/kernel                                                size 16777216     shape (mlp_embed=2048, mlp=8192)               partition spec (None, 'model')
I0512 23:45:21.788685 140143678494720 utils.py:1372] Variable decoder/layers_20/mlp/wo/kernel                                                  size 16777216     shape (mlp=8192, mlp_embed=2048)               partition spec ('model', None)
I0512 23:45:21.788731 140143678494720 utils.py:1372] Variable decoder/layers_20/pre_mlp_layer_norm/scale                                       size 2048         shape (embed=2048)                             partition spec ('model',)
I0512 23:45:21.788777 140143678494720 utils.py:1372] Variable decoder/layers_20/pre_self_attention_layer_norm/scale                            size 2048         shape (embed=2048)                             partition spec ('model',)
I0512 23:45:21.788823 140143678494720 utils.py:1372] Variable decoder/layers_20/self_attention/key/kernel                                      size 6291456      shape (embed=2048, joined_kv=3072)             partition spec (None, 'model')
I0512 23:45:21.788869 140143678494720 utils.py:1372] Variable decoder/layers_20/self_attention/out/kernel                                      size 6291456      shape (joined_kv=3072, embed=2048)             partition spec ('model', None)
I0512 23:45:21.788930 140143678494720 utils.py:1372] Variable decoder/layers_20/self_attention/query/kernel                                    size 6291456      shape (embed=2048, joined_kv=3072)             partition spec (None, 'model')
I0512 23:45:21.788979 140143678494720 utils.py:1372] Variable decoder/layers_20/self_attention/value/kernel                                    size 6291456      shape (embed=2048, joined_kv=3072)             partition spec (None, 'model')
I0512 23:45:21.789025 140143678494720 utils.py:1372] Variable decoder/layers_21/mlp/wi_0/kernel                                                size 33554432     shape (mlp_embed=2048, mlp=16384)              partition spec (None, 'model')
I0512 23:45:21.789071 140143678494720 utils.py:1372] Variable decoder/layers_21/mlp/wi_1/kernel                                                size 16777216     shape (mlp_embed=2048, mlp=8192)               partition spec (None, 'model')
I0512 23:45:21.789117 140143678494720 utils.py:1372] Variable decoder/layers_21/mlp/wo/kernel                                                  size 16777216     shape (mlp=8192, mlp_embed=2048)               partition spec ('model', None)
I0512 23:45:21.789162 140143678494720 utils.py:1372] Variable decoder/layers_21/pre_mlp_layer_norm/scale                                       size 2048         shape (embed=2048)                             partition spec ('model',)
I0512 23:45:21.789207 140143678494720 utils.py:1372] Variable decoder/layers_21/pre_self_attention_layer_norm/scale                            size 2048         shape (embed=2048)                             partition spec ('model',)
I0512 23:45:21.789252 140143678494720 utils.py:1372] Variable decoder/layers_21/self_attention/key/kernel                                      size 6291456      shape (embed=2048, joined_kv=3072)             partition spec (None, 'model')
I0512 23:45:21.789299 140143678494720 utils.py:1372] Variable decoder/layers_21/self_attention/out/kernel                                      size 6291456      shape (joined_kv=3072, embed=2048)             partition spec ('model', None)
I0512 23:45:21.789347 140143678494720 utils.py:1372] Variable decoder/layers_21/self_attention/query/kernel                                    size 6291456      shape (embed=2048, joined_kv=3072)             partition spec (None, 'model')
I0512 23:45:21.789393 140143678494720 utils.py:1372] Variable decoder/layers_21/self_attention/value/kernel                                    size 6291456      shape (embed=2048, joined_kv=3072)             partition spec (None, 'model')
I0512 23:45:21.789439 140143678494720 utils.py:1372] Variable decoder/layers_22/mlp/wi_0/kernel                                                size 33554432     shape (mlp_embed=2048, mlp=16384)              partition spec (None, 'model')
I0512 23:45:21.789485 140143678494720 utils.py:1372] Variable decoder/layers_22/mlp/wi_1/kernel                                                size 16777216     shape (mlp_embed=2048, mlp=8192)               partition spec (None, 'model')
I0512 23:45:21.789531 140143678494720 utils.py:1372] Variable decoder/layers_22/mlp/wo/kernel                                                  size 16777216     shape (mlp=8192, mlp_embed=2048)               partition spec ('model', None)
I0512 23:45:21.789584 140143678494720 utils.py:1372] Variable decoder/layers_22/pre_mlp_layer_norm/scale                                       size 2048         shape (embed=2048)                             partition spec ('model',)
I0512 23:45:21.789630 140143678494720 utils.py:1372] Variable decoder/layers_22/pre_self_attention_layer_norm/scale                            size 2048         shape (embed=2048)                             partition spec ('model',)
I0512 23:45:21.789677 140143678494720 utils.py:1372] Variable decoder/layers_22/self_attention/key/kernel                                      size 6291456      shape (embed=2048, joined_kv=3072)             partition spec (None, 'model')
I0512 23:45:21.789723 140143678494720 utils.py:1372] Variable decoder/layers_22/self_attention/out/kernel                                      size 6291456      shape (joined_kv=3072, embed=2048)             partition spec ('model', None)
I0512 23:45:21.789769 140143678494720 utils.py:1372] Variable decoder/layers_22/self_attention/query/kernel                                    size 6291456      shape (embed=2048, joined_kv=3072)             partition spec (None, 'model')
I0512 23:45:21.789814 140143678494720 utils.py:1372] Variable decoder/layers_22/self_attention/value/kernel                                    size 6291456      shape (embed=2048, joined_kv=3072)             partition spec (None, 'model')
I0512 23:45:21.789860 140143678494720 utils.py:1372] Variable decoder/layers_23/extra_mlp/wi_0/kernel                                          size 33554432     shape (mlp_embed=2048, mlp=16384)              partition spec (None, 'model')
I0512 23:45:21.789906 140143678494720 utils.py:1372] Variable decoder/layers_23/extra_mlp/wi_1/kernel                                          size 16777216     shape (mlp_embed=2048, mlp=8192)               partition spec (None, 'model')
I0512 23:45:21.789952 140143678494720 utils.py:1372] Variable decoder/layers_23/extra_mlp/wo/kernel                                            size 16777216     shape (mlp=8192, mlp_embed=2048)               partition spec ('model', None)
I0512 23:45:21.789998 140143678494720 utils.py:1372] Variable decoder/layers_23/mlp/expert/wi_0/kernel                                         size 268435456    shape (expert=8, mlp_embed=2048, expert_mlp=16384) partition spec ('expert', None, 'model')
I0512 23:45:21.790045 140143678494720 utils.py:1372] Variable decoder/layers_23/mlp/expert/wi_1/kernel                                         size 134217728    shape (expert=8, mlp_embed=2048, expert_mlp=8192) partition spec ('expert', None, 'model')
I0512 23:45:21.790092 140143678494720 utils.py:1372] Variable decoder/layers_23/mlp/expert/wo/kernel                                           size 134217728    shape (expert=8, expert_mlp=8192, mlp_embed=2048) partition spec ('expert', 'model', None)
I0512 23:45:21.790138 140143678494720 utils.py:1372] Variable decoder/layers_23/mlp/router/router_weights/w/kernel                             size 16384        shape (embed=2048, unmodeled=8)                partition spec ('model', None)
I0512 23:45:21.790184 140143678494720 utils.py:1372] Variable decoder/layers_23/pre_extra_mlp_layer_norm/scale                                 size 2048         shape (embed=2048)                             partition spec ('model',)
I0512 23:45:21.790230 140143678494720 utils.py:1372] Variable decoder/layers_23/pre_mlp_layer_norm/scale                                       size 2048         shape (embed=2048)                             partition spec ('model',)
I0512 23:45:21.790276 140143678494720 utils.py:1372] Variable decoder/layers_23/pre_self_attention_layer_norm/scale                            size 2048         shape (embed=2048)                             partition spec ('model',)
I0512 23:45:21.790324 140143678494720 utils.py:1372] Variable decoder/layers_23/self_attention/key/kernel                                      size 6291456      shape (embed=2048, joined_kv=3072)             partition spec (None, 'model')
I0512 23:45:21.790371 140143678494720 utils.py:1372] Variable decoder/layers_23/self_attention/out/kernel                                      size 6291456      shape (joined_kv=3072, embed=2048)             partition spec ('model', None)
I0512 23:45:21.790416 140143678494720 utils.py:1372] Variable decoder/layers_23/self_attention/query/kernel                                    size 6291456      shape (embed=2048, joined_kv=3072)             partition spec (None, 'model')
I0512 23:45:21.790462 140143678494720 utils.py:1372] Variable decoder/layers_23/self_attention/value/kernel                                    size 6291456      shape (embed=2048, joined_kv=3072)             partition spec (None, 'model')
I0512 23:45:21.790507 140143678494720 utils.py:1372] Variable decoder/layers_3/mlp/wi_0/kernel                                                 size 33554432     shape (mlp_embed=2048, mlp=16384)              partition spec (None, 'model')
I0512 23:45:21.790560 140143678494720 utils.py:1372] Variable decoder/layers_3/mlp/wi_1/kernel                                                 size 16777216     shape (mlp_embed=2048, mlp=8192)               partition spec (None, 'model')
I0512 23:45:21.790608 140143678494720 utils.py:1372] Variable decoder/layers_3/mlp/wo/kernel                                                   size 16777216     shape (mlp=8192, mlp_embed=2048)               partition spec ('model', None)
I0512 23:45:21.790653 140143678494720 utils.py:1372] Variable decoder/layers_3/pre_mlp_layer_norm/scale                                        size 2048         shape (embed=2048)                             partition spec ('model',)
I0512 23:45:21.790699 140143678494720 utils.py:1372] Variable decoder/layers_3/pre_self_attention_layer_norm/scale                             size 2048         shape (embed=2048)                             partition spec ('model',)
I0512 23:45:21.790744 140143678494720 utils.py:1372] Variable decoder/layers_3/self_attention/key/kernel                                       size 6291456      shape (embed=2048, joined_kv=3072)             partition spec (None, 'model')
I0512 23:45:21.790790 140143678494720 utils.py:1372] Variable decoder/layers_3/self_attention/out/kernel                                       size 6291456      shape (joined_kv=3072, embed=2048)             partition spec ('model', None)
I0512 23:45:21.790836 140143678494720 utils.py:1372] Variable decoder/layers_3/self_attention/query/kernel                                     size 6291456      shape (embed=2048, joined_kv=3072)             partition spec (None, 'model')
I0512 23:45:21.790882 140143678494720 utils.py:1372] Variable decoder/layers_3/self_attention/value/kernel                                     size 6291456      shape (embed=2048, joined_kv=3072)             partition spec (None, 'model')
I0512 23:45:21.790928 140143678494720 utils.py:1372] Variable decoder/layers_4/mlp/wi_0/kernel                                                 size 33554432     shape (mlp_embed=2048, mlp=16384)              partition spec (None, 'model')
I0512 23:45:21.790975 140143678494720 utils.py:1372] Variable decoder/layers_4/mlp/wi_1/kernel                                                 size 16777216     shape (mlp_embed=2048, mlp=8192)               partition spec (None, 'model')
I0512 23:45:21.791035 140143678494720 utils.py:1372] Variable decoder/layers_4/mlp/wo/kernel                                                   size 16777216     shape (mlp=8192, mlp_embed=2048)               partition spec ('model', None)
I0512 23:45:21.791084 140143678494720 utils.py:1372] Variable decoder/layers_4/pre_mlp_layer_norm/scale                                        size 2048         shape (embed=2048)                             partition spec ('model',)
I0512 23:45:21.791130 140143678494720 utils.py:1372] Variable decoder/layers_4/pre_self_attention_layer_norm/scale                             size 2048         shape (embed=2048)                             partition spec ('model',)
I0512 23:45:21.791176 140143678494720 utils.py:1372] Variable decoder/layers_4/self_attention/key/kernel                                       size 6291456      shape (embed=2048, joined_kv=3072)             partition spec (None, 'model')
I0512 23:45:21.791222 140143678494720 utils.py:1372] Variable decoder/layers_4/self_attention/out/kernel                                       size 6291456      shape (joined_kv=3072, embed=2048)             partition spec ('model', None)
I0512 23:45:21.791267 140143678494720 utils.py:1372] Variable decoder/layers_4/self_attention/query/kernel                                     size 6291456      shape (embed=2048, joined_kv=3072)             partition spec (None, 'model')
I0512 23:45:21.791316 140143678494720 utils.py:1372] Variable decoder/layers_4/self_attention/value/kernel                                     size 6291456      shape (embed=2048, joined_kv=3072)             partition spec (None, 'model')
I0512 23:45:21.791362 140143678494720 utils.py:1372] Variable decoder/layers_5/extra_mlp/wi_0/kernel                                           size 33554432     shape (mlp_embed=2048, mlp=16384)              partition spec (None, 'model')
I0512 23:45:21.791409 140143678494720 utils.py:1372] Variable decoder/layers_5/extra_mlp/wi_1/kernel                                           size 16777216     shape (mlp_embed=2048, mlp=8192)               partition spec (None, 'model')
I0512 23:45:21.791455 140143678494720 utils.py:1372] Variable decoder/layers_5/extra_mlp/wo/kernel                                             size 16777216     shape (mlp=8192, mlp_embed=2048)               partition spec ('model', None)
I0512 23:45:21.791501 140143678494720 utils.py:1372] Variable decoder/layers_5/mlp/expert/wi_0/kernel                                          size 268435456    shape (expert=8, mlp_embed=2048, expert_mlp=16384) partition spec ('expert', None, 'model')
I0512 23:45:21.791556 140143678494720 utils.py:1372] Variable decoder/layers_5/mlp/expert/wi_1/kernel                                          size 134217728    shape (expert=8, mlp_embed=2048, expert_mlp=8192) partition spec ('expert', None, 'model')
I0512 23:45:21.791604 140143678494720 utils.py:1372] Variable decoder/layers_5/mlp/expert/wo/kernel                                            size 134217728    shape (expert=8, expert_mlp=8192, mlp_embed=2048) partition spec ('expert', 'model', None)
I0512 23:45:21.791650 140143678494720 utils.py:1372] Variable decoder/layers_5/mlp/router/router_weights/w/kernel                              size 16384        shape (embed=2048, unmodeled=8)                partition spec ('model', None)
I0512 23:45:21.791696 140143678494720 utils.py:1372] Variable decoder/layers_5/pre_extra_mlp_layer_norm/scale                                  size 2048         shape (embed=2048)                             partition spec ('model',)
I0512 23:45:21.791743 140143678494720 utils.py:1372] Variable decoder/layers_5/pre_mlp_layer_norm/scale                                        size 2048         shape (embed=2048)                             partition spec ('model',)
I0512 23:45:21.791789 140143678494720 utils.py:1372] Variable decoder/layers_5/pre_self_attention_layer_norm/scale                             size 2048         shape (embed=2048)                             partition spec ('model',)
I0512 23:45:21.791835 140143678494720 utils.py:1372] Variable decoder/layers_5/self_attention/key/kernel                                       size 6291456      shape (embed=2048, joined_kv=3072)             partition spec (None, 'model')
I0512 23:45:21.791880 140143678494720 utils.py:1372] Variable decoder/layers_5/self_attention/out/kernel                                       size 6291456      shape (joined_kv=3072, embed=2048)             partition spec ('model', None)
I0512 23:45:21.791926 140143678494720 utils.py:1372] Variable decoder/layers_5/self_attention/query/kernel                                     size 6291456      shape (embed=2048, joined_kv=3072)             partition spec (None, 'model')
I0512 23:45:21.791971 140143678494720 utils.py:1372] Variable decoder/layers_5/self_attention/value/kernel                                     size 6291456      shape (embed=2048, joined_kv=3072)             partition spec (None, 'model')
I0512 23:45:21.792016 140143678494720 utils.py:1372] Variable decoder/layers_6/mlp/wi_0/kernel                                                 size 33554432     shape (mlp_embed=2048, mlp=16384)              partition spec (None, 'model')
I0512 23:45:21.792063 140143678494720 utils.py:1372] Variable decoder/layers_6/mlp/wi_1/kernel                                                 size 16777216     shape (mlp_embed=2048, mlp=8192)               partition spec (None, 'model')
I0512 23:45:21.792109 140143678494720 utils.py:1372] Variable decoder/layers_6/mlp/wo/kernel                                                   size 16777216     shape (mlp=8192, mlp_embed=2048)               partition spec ('model', None)
I0512 23:45:21.792155 140143678494720 utils.py:1372] Variable decoder/layers_6/pre_mlp_layer_norm/scale                                        size 2048         shape (embed=2048)                             partition spec ('model',)
I0512 23:45:21.792201 140143678494720 utils.py:1372] Variable decoder/layers_6/pre_self_attention_layer_norm/scale                             size 2048         shape (embed=2048)                             partition spec ('model',)
I0512 23:45:21.792248 140143678494720 utils.py:1372] Variable decoder/layers_6/self_attention/key/kernel                                       size 6291456      shape (embed=2048, joined_kv=3072)             partition spec (None, 'model')
I0512 23:45:21.792294 140143678494720 utils.py:1372] Variable decoder/layers_6/self_attention/out/kernel                                       size 6291456      shape (joined_kv=3072, embed=2048)             partition spec ('model', None)
I0512 23:45:21.792342 140143678494720 utils.py:1372] Variable decoder/layers_6/self_attention/query/kernel                                     size 6291456      shape (embed=2048, joined_kv=3072)             partition spec (None, 'model')
I0512 23:45:21.792388 140143678494720 utils.py:1372] Variable decoder/layers_6/self_attention/value/kernel                                     size 6291456      shape (embed=2048, joined_kv=3072)             partition spec (None, 'model')
I0512 23:45:21.792433 140143678494720 utils.py:1372] Variable decoder/layers_7/mlp/wi_0/kernel                                                 size 33554432     shape (mlp_embed=2048, mlp=16384)              partition spec (None, 'model')
I0512 23:45:21.792480 140143678494720 utils.py:1372] Variable decoder/layers_7/mlp/wi_1/kernel                                                 size 16777216     shape (mlp_embed=2048, mlp=8192)               partition spec (None, 'model')
I0512 23:45:21.792526 140143678494720 utils.py:1372] Variable decoder/layers_7/mlp/wo/kernel                                                   size 16777216     shape (mlp=8192, mlp_embed=2048)               partition spec ('model', None)
I0512 23:45:21.792602 140143678494720 utils.py:1372] Variable decoder/layers_7/pre_mlp_layer_norm/scale                                        size 2048         shape (embed=2048)                             partition spec ('model',)
I0512 23:45:21.792649 140143678494720 utils.py:1372] Variable decoder/layers_7/pre_self_attention_layer_norm/scale                             size 2048         shape (embed=2048)                             partition spec ('model',)
I0512 23:45:21.792695 140143678494720 utils.py:1372] Variable decoder/layers_7/self_attention/key/kernel                                       size 6291456      shape (embed=2048, joined_kv=3072)             partition spec (None, 'model')
I0512 23:45:21.792740 140143678494720 utils.py:1372] Variable decoder/layers_7/self_attention/out/kernel                                       size 6291456      shape (joined_kv=3072, embed=2048)             partition spec ('model', None)
I0512 23:45:21.792786 140143678494720 utils.py:1372] Variable decoder/layers_7/self_attention/query/kernel                                     size 6291456      shape (embed=2048, joined_kv=3072)             partition spec (None, 'model')
I0512 23:45:21.792831 140143678494720 utils.py:1372] Variable decoder/layers_7/self_attention/value/kernel                                     size 6291456      shape (embed=2048, joined_kv=3072)             partition spec (None, 'model')
I0512 23:45:21.792877 140143678494720 utils.py:1372] Variable decoder/layers_8/mlp/wi_0/kernel                                                 size 33554432     shape (mlp_embed=2048, mlp=16384)              partition spec (None, 'model')
I0512 23:45:21.792924 140143678494720 utils.py:1372] Variable decoder/layers_8/mlp/wi_1/kernel                                                 size 16777216     shape (mlp_embed=2048, mlp=8192)               partition spec (None, 'model')
I0512 23:45:21.792970 140143678494720 utils.py:1372] Variable decoder/layers_8/mlp/wo/kernel                                                   size 16777216     shape (mlp=8192, mlp_embed=2048)               partition spec ('model', None)
I0512 23:45:21.793015 140143678494720 utils.py:1372] Variable decoder/layers_8/pre_mlp_layer_norm/scale                                        size 2048         shape (embed=2048)                             partition spec ('model',)
I0512 23:45:21.793061 140143678494720 utils.py:1372] Variable decoder/layers_8/pre_self_attention_layer_norm/scale                             size 2048         shape (embed=2048)                             partition spec ('model',)
I0512 23:45:21.793106 140143678494720 utils.py:1372] Variable decoder/layers_8/self_attention/key/kernel                                       size 6291456      shape (embed=2048, joined_kv=3072)             partition spec (None, 'model')
I0512 23:45:21.793170 140143678494720 utils.py:1372] Variable decoder/layers_8/self_attention/out/kernel                                       size 6291456      shape (joined_kv=3072, embed=2048)             partition spec ('model', None)
I0512 23:45:21.793219 140143678494720 utils.py:1372] Variable decoder/layers_8/self_attention/query/kernel                                     size 6291456      shape (embed=2048, joined_kv=3072)             partition spec (None, 'model')
I0512 23:45:21.793265 140143678494720 utils.py:1372] Variable decoder/layers_8/self_attention/value/kernel                                     size 6291456      shape (embed=2048, joined_kv=3072)             partition spec (None, 'model')
I0512 23:45:21.793313 140143678494720 utils.py:1372] Variable decoder/layers_9/mlp/wi_0/kernel                                                 size 33554432     shape (mlp_embed=2048, mlp=16384)              partition spec (None, 'model')
I0512 23:45:21.793361 140143678494720 utils.py:1372] Variable decoder/layers_9/mlp/wi_1/kernel                                                 size 16777216     shape (mlp_embed=2048, mlp=8192)               partition spec (None, 'model')
I0512 23:45:21.793407 140143678494720 utils.py:1372] Variable decoder/layers_9/mlp/wo/kernel                                                   size 16777216     shape (mlp=8192, mlp_embed=2048)               partition spec ('model', None)
I0512 23:45:21.793452 140143678494720 utils.py:1372] Variable decoder/layers_9/pre_mlp_layer_norm/scale                                        size 2048         shape (embed=2048)                             partition spec ('model',)
I0512 23:45:21.793497 140143678494720 utils.py:1372] Variable decoder/layers_9/pre_self_attention_layer_norm/scale                             size 2048         shape (embed=2048)                             partition spec ('model',)
I0512 23:45:21.793549 140143678494720 utils.py:1372] Variable decoder/layers_9/self_attention/key/kernel                                       size 6291456      shape (embed=2048, joined_kv=3072)             partition spec (None, 'model')
I0512 23:45:21.793597 140143678494720 utils.py:1372] Variable decoder/layers_9/self_attention/out/kernel                                       size 6291456      shape (joined_kv=3072, embed=2048)             partition spec ('model', None)
I0512 23:45:21.793643 140143678494720 utils.py:1372] Variable decoder/layers_9/self_attention/query/kernel                                     size 6291456      shape (embed=2048, joined_kv=3072)             partition spec (None, 'model')
I0512 23:45:21.793689 140143678494720 utils.py:1372] Variable decoder/layers_9/self_attention/value/kernel                                     size 6291456      shape (embed=2048, joined_kv=3072)             partition spec (None, 'model')
I0512 23:45:21.793735 140143678494720 utils.py:1372] Variable decoder/logits_dense/kernel                                                      size 525074432    shape (embed=2048, vocab=256384)               partition spec (None, 'model')
I0512 23:45:21.793781 140143678494720 utils.py:1372] Variable token_embedder/embedding                                                         size 525074432    shape (vocab=256384, embed=2048)               partition spec ('model', None)
I0512 23:45:21.793886 140143678494720 utils.py:1372] Total number of parameters: 5412399104
I0512 23:45:21.793945 140143678494720 utils.py:1372] 
I0512 23:45:21.797547 140143678494720 utils.py:1372] Variable param_states/decoder/decoder_norm/scale/m                                        size 1            shape (1,)                                     partition spec None
I0512 23:45:21.797620 140143678494720 utils.py:1372] Variable param_states/decoder/decoder_norm/scale/v                                        size 2048         shape (embed=2048)                             partition spec ('model',)
I0512 23:45:21.797671 140143678494720 utils.py:1372] Variable param_states/decoder/decoder_norm/scale/v_col                                    size 1            shape (1,)                                     partition spec None
I0512 23:45:21.797715 140143678494720 utils.py:1372] Variable param_states/decoder/decoder_norm/scale/v_row                                    size 1            shape (1,)                                     partition spec None
I0512 23:45:21.797758 140143678494720 utils.py:1372] Variable param_states/decoder/layers_0/mlp/wi_0/kernel/m                                  size 1            shape (1,)                                     partition spec None
I0512 23:45:21.797801 140143678494720 utils.py:1372] Variable param_states/decoder/layers_0/mlp/wi_0/kernel/v                                  size 1            shape (1,)                                     partition spec None
I0512 23:45:21.797843 140143678494720 utils.py:1372] Variable param_states/decoder/layers_0/mlp/wi_0/kernel/v_col                              size 16384        shape (16384,)                                 partition spec None
I0512 23:45:21.797885 140143678494720 utils.py:1372] Variable param_states/decoder/layers_0/mlp/wi_0/kernel/v_row                              size 2048         shape (2048,)                                  partition spec None
I0512 23:45:21.797926 140143678494720 utils.py:1372] Variable param_states/decoder/layers_0/mlp/wi_1/kernel/m                                  size 1            shape (1,)                                     partition spec None
I0512 23:45:21.797968 140143678494720 utils.py:1372] Variable param_states/decoder/layers_0/mlp/wi_1/kernel/v                                  size 1            shape (1,)                                     partition spec None
I0512 23:45:21.798010 140143678494720 utils.py:1372] Variable param_states/decoder/layers_0/mlp/wi_1/kernel/v_col                              size 8192         shape (8192,)                                  partition spec None
I0512 23:45:21.798055 140143678494720 utils.py:1372] Variable param_states/decoder/layers_0/mlp/wi_1/kernel/v_row                              size 2048         shape (2048,)                                  partition spec None
I0512 23:45:21.798097 140143678494720 utils.py:1372] Variable param_states/decoder/layers_0/mlp/wo/kernel/m                                    size 1            shape (1,)                                     partition spec None
I0512 23:45:21.798139 140143678494720 utils.py:1372] Variable param_states/decoder/layers_0/mlp/wo/kernel/v                                    size 1            shape (1,)                                     partition spec None
I0512 23:45:21.798182 140143678494720 utils.py:1372] Variable param_states/decoder/layers_0/mlp/wo/kernel/v_col                                size 8192         shape (8192,)                                  partition spec None
I0512 23:45:21.798224 140143678494720 utils.py:1372] Variable param_states/decoder/layers_0/mlp/wo/kernel/v_row                                size 2048         shape (2048,)                                  partition spec None
I0512 23:45:21.798266 140143678494720 utils.py:1372] Variable param_states/decoder/layers_0/pre_mlp_layer_norm/scale/m                         size 1            shape (1,)                                     partition spec None
I0512 23:45:21.798313 140143678494720 utils.py:1372] Variable param_states/decoder/layers_0/pre_mlp_layer_norm/scale/v                         size 2048         shape (embed=2048)                             partition spec ('model',)
I0512 23:45:21.798359 140143678494720 utils.py:1372] Variable param_states/decoder/layers_0/pre_mlp_layer_norm/scale/v_col                     size 1            shape (1,)                                     partition spec None
I0512 23:45:21.798401 140143678494720 utils.py:1372] Variable param_states/decoder/layers_0/pre_mlp_layer_norm/scale/v_row                     size 1            shape (1,)                                     partition spec None
I0512 23:45:21.798442 140143678494720 utils.py:1372] Variable param_states/decoder/layers_0/pre_self_attention_layer_norm/scale/m              size 1            shape (1,)                                     partition spec None
I0512 23:45:21.798486 140143678494720 utils.py:1372] Variable param_states/decoder/layers_0/pre_self_attention_layer_norm/scale/v              size 2048         shape (embed=2048)                             partition spec ('model',)
I0512 23:45:21.798530 140143678494720 utils.py:1372] Variable param_states/decoder/layers_0/pre_self_attention_layer_norm/scale/v_col          size 1            shape (1,)                                     partition spec None
I0512 23:45:21.798580 140143678494720 utils.py:1372] Variable param_states/decoder/layers_0/pre_self_attention_layer_norm/scale/v_row          size 1            shape (1,)                                     partition spec None
I0512 23:45:21.798623 140143678494720 utils.py:1372] Variable param_states/decoder/layers_0/self_attention/key/kernel/m                        size 1            shape (1,)                                     partition spec None
I0512 23:45:21.798665 140143678494720 utils.py:1372] Variable param_states/decoder/layers_0/self_attention/key/kernel/v                        size 1            shape (1,)                                     partition spec None
I0512 23:45:21.798710 140143678494720 utils.py:1372] Variable param_states/decoder/layers_0/self_attention/key/kernel/v_col                    size 3072         shape (3072,)                                  partition spec None
I0512 23:45:21.798753 140143678494720 utils.py:1372] Variable param_states/decoder/layers_0/self_attention/key/kernel/v_row                    size 2048         shape (2048,)                                  partition spec None
I0512 23:45:21.798795 140143678494720 utils.py:1372] Variable param_states/decoder/layers_0/self_attention/out/kernel/m                        size 1            shape (1,)                                     partition spec None
I0512 23:45:21.798837 140143678494720 utils.py:1372] Variable param_states/decoder/layers_0/self_attention/out/kernel/v                        size 1            shape (1,)                                     partition spec None
I0512 23:45:21.798881 140143678494720 utils.py:1372] Variable param_states/decoder/layers_0/self_attention/out/kernel/v_col                    size 3072         shape (3072,)                                  partition spec None
I0512 23:45:21.798925 140143678494720 utils.py:1372] Variable param_states/decoder/layers_0/self_attention/out/kernel/v_row                    size 2048         shape (2048,)                                  partition spec None
I0512 23:45:21.798984 140143678494720 utils.py:1372] Variable param_states/decoder/layers_0/self_attention/query/kernel/m                      size 1            shape (1,)                                     partition spec None
I0512 23:45:21.799029 140143678494720 utils.py:1372] Variable param_states/decoder/layers_0/self_attention/query/kernel/v                      size 1            shape (1,)                                     partition spec None
I0512 23:45:21.799071 140143678494720 utils.py:1372] Variable param_states/decoder/layers_0/self_attention/query/kernel/v_col                  size 3072         shape (3072,)                                  partition spec None
I0512 23:45:21.799113 140143678494720 utils.py:1372] Variable param_states/decoder/layers_0/self_attention/query/kernel/v_row                  size 2048         shape (2048,)                                  partition spec None
I0512 23:45:21.799155 140143678494720 utils.py:1372] Variable param_states/decoder/layers_0/self_attention/value/kernel/m                      size 1            shape (1,)                                     partition spec None
I0512 23:45:21.799196 140143678494720 utils.py:1372] Variable param_states/decoder/layers_0/self_attention/value/kernel/v                      size 1            shape (1,)                                     partition spec None
I0512 23:45:21.799238 140143678494720 utils.py:1372] Variable param_states/decoder/layers_0/self_attention/value/kernel/v_col                  size 3072         shape (3072,)                                  partition spec None
I0512 23:45:21.799279 140143678494720 utils.py:1372] Variable param_states/decoder/layers_0/self_attention/value/kernel/v_row                  size 2048         shape (2048,)                                  partition spec None
I0512 23:45:21.799323 140143678494720 utils.py:1372] Variable param_states/decoder/layers_1/mlp/wi_0/kernel/m                                  size 1            shape (1,)                                     partition spec None
I0512 23:45:21.799366 140143678494720 utils.py:1372] Variable param_states/decoder/layers_1/mlp/wi_0/kernel/v                                  size 1            shape (1,)                                     partition spec None
I0512 23:45:21.799408 140143678494720 utils.py:1372] Variable param_states/decoder/layers_1/mlp/wi_0/kernel/v_col                              size 16384        shape (16384,)                                 partition spec None
I0512 23:45:21.799450 140143678494720 utils.py:1372] Variable param_states/decoder/layers_1/mlp/wi_0/kernel/v_row                              size 2048         shape (2048,)                                  partition spec None
I0512 23:45:21.799491 140143678494720 utils.py:1372] Variable param_states/decoder/layers_1/mlp/wi_1/kernel/m                                  size 1            shape (1,)                                     partition spec None
I0512 23:45:21.799533 140143678494720 utils.py:1372] Variable param_states/decoder/layers_1/mlp/wi_1/kernel/v                                  size 1            shape (1,)                                     partition spec None
I0512 23:45:21.799585 140143678494720 utils.py:1372] Variable param_states/decoder/layers_1/mlp/wi_1/kernel/v_col                              size 8192         shape (8192,)                                  partition spec None
I0512 23:45:21.799627 140143678494720 utils.py:1372] Variable param_states/decoder/layers_1/mlp/wi_1/kernel/v_row                              size 2048         shape (2048,)                                  partition spec None
I0512 23:45:21.799669 140143678494720 utils.py:1372] Variable param_states/decoder/layers_1/mlp/wo/kernel/m                                    size 1            shape (1,)                                     partition spec None
I0512 23:45:21.799710 140143678494720 utils.py:1372] Variable param_states/decoder/layers_1/mlp/wo/kernel/v                                    size 1            shape (1,)                                     partition spec None
I0512 23:45:21.799751 140143678494720 utils.py:1372] Variable param_states/decoder/layers_1/mlp/wo/kernel/v_col                                size 8192         shape (8192,)                                  partition spec None
I0512 23:45:21.799793 140143678494720 utils.py:1372] Variable param_states/decoder/layers_1/mlp/wo/kernel/v_row                                size 2048         shape (2048,)                                  partition spec None
I0512 23:45:21.799836 140143678494720 utils.py:1372] Variable param_states/decoder/layers_1/pre_mlp_layer_norm/scale/m                         size 1            shape (1,)                                     partition spec None
I0512 23:45:21.799881 140143678494720 utils.py:1372] Variable param_states/decoder/layers_1/pre_mlp_layer_norm/scale/v                         size 2048         shape (embed=2048)                             partition spec ('model',)
I0512 23:45:21.799925 140143678494720 utils.py:1372] Variable param_states/decoder/layers_1/pre_mlp_layer_norm/scale/v_col                     size 1            shape (1,)                                     partition spec None
I0512 23:45:21.799966 140143678494720 utils.py:1372] Variable param_states/decoder/layers_1/pre_mlp_layer_norm/scale/v_row                     size 1            shape (1,)                                     partition spec None
I0512 23:45:21.800009 140143678494720 utils.py:1372] Variable param_states/decoder/layers_1/pre_self_attention_layer_norm/scale/m              size 1            shape (1,)                                     partition spec None
I0512 23:45:21.800054 140143678494720 utils.py:1372] Variable param_states/decoder/layers_1/pre_self_attention_layer_norm/scale/v              size 2048         shape (embed=2048)                             partition spec ('model',)
I0512 23:45:21.800097 140143678494720 utils.py:1372] Variable param_states/decoder/layers_1/pre_self_attention_layer_norm/scale/v_col          size 1            shape (1,)                                     partition spec None
I0512 23:45:21.800139 140143678494720 utils.py:1372] Variable param_states/decoder/layers_1/pre_self_attention_layer_norm/scale/v_row          size 1            shape (1,)                                     partition spec None
I0512 23:45:21.800180 140143678494720 utils.py:1372] Variable param_states/decoder/layers_1/self_attention/key/kernel/m                        size 1            shape (1,)                                     partition spec None
I0512 23:45:21.800222 140143678494720 utils.py:1372] Variable param_states/decoder/layers_1/self_attention/key/kernel/v                        size 1            shape (1,)                                     partition spec None
I0512 23:45:21.800266 140143678494720 utils.py:1372] Variable param_states/decoder/layers_1/self_attention/key/kernel/v_col                    size 3072         shape (3072,)                                  partition spec None
I0512 23:45:21.800310 140143678494720 utils.py:1372] Variable param_states/decoder/layers_1/self_attention/key/kernel/v_row                    size 2048         shape (2048,)                                  partition spec None
I0512 23:45:21.800352 140143678494720 utils.py:1372] Variable param_states/decoder/layers_1/self_attention/out/kernel/m                        size 1            shape (1,)                                     partition spec None
I0512 23:45:21.800394 140143678494720 utils.py:1372] Variable param_states/decoder/layers_1/self_attention/out/kernel/v                        size 1            shape (1,)                                     partition spec None
I0512 23:45:21.800436 140143678494720 utils.py:1372] Variable param_states/decoder/layers_1/self_attention/out/kernel/v_col                    size 3072         shape (3072,)                                  partition spec None
I0512 23:45:21.800477 140143678494720 utils.py:1372] Variable param_states/decoder/layers_1/self_attention/out/kernel/v_row                    size 2048         shape (2048,)                                  partition spec None
I0512 23:45:21.800518 140143678494720 utils.py:1372] Variable param_states/decoder/layers_1/self_attention/query/kernel/m                      size 1            shape (1,)                                     partition spec None
I0512 23:45:21.800589 140143678494720 utils.py:1372] Variable param_states/decoder/layers_1/self_attention/query/kernel/v                      size 1            shape (1,)                                     partition spec None
I0512 23:45:21.800636 140143678494720 utils.py:1372] Variable param_states/decoder/layers_1/self_attention/query/kernel/v_col                  size 3072         shape (3072,)                                  partition spec None
I0512 23:45:21.800678 140143678494720 utils.py:1372] Variable param_states/decoder/layers_1/self_attention/query/kernel/v_row                  size 2048         shape (2048,)                                  partition spec None
I0512 23:45:21.800719 140143678494720 utils.py:1372] Variable param_states/decoder/layers_1/self_attention/value/kernel/m                      size 1            shape (1,)                                     partition spec None
I0512 23:45:21.800762 140143678494720 utils.py:1372] Variable param_states/decoder/layers_1/self_attention/value/kernel/v                      size 1            shape (1,)                                     partition spec None
I0512 23:45:21.800804 140143678494720 utils.py:1372] Variable param_states/decoder/layers_1/self_attention/value/kernel/v_col                  size 3072         shape (3072,)                                  partition spec None
I0512 23:45:21.800845 140143678494720 utils.py:1372] Variable param_states/decoder/layers_1/self_attention/value/kernel/v_row                  size 2048         shape (2048,)                                  partition spec None
I0512 23:45:21.800886 140143678494720 utils.py:1372] Variable param_states/decoder/layers_10/mlp/wi_0/kernel/m                                 size 1            shape (1,)                                     partition spec None
I0512 23:45:21.800927 140143678494720 utils.py:1372] Variable param_states/decoder/layers_10/mlp/wi_0/kernel/v                                 size 1            shape (1,)                                     partition spec None
I0512 23:45:21.800969 140143678494720 utils.py:1372] Variable param_states/decoder/layers_10/mlp/wi_0/kernel/v_col                             size 16384        shape (16384,)                                 partition spec None
I0512 23:45:21.801026 140143678494720 utils.py:1372] Variable param_states/decoder/layers_10/mlp/wi_0/kernel/v_row                             size 2048         shape (2048,)                                  partition spec None
I0512 23:45:21.801069 140143678494720 utils.py:1372] Variable param_states/decoder/layers_10/mlp/wi_1/kernel/m                                 size 1            shape (1,)                                     partition spec None
I0512 23:45:21.801112 140143678494720 utils.py:1372] Variable param_states/decoder/layers_10/mlp/wi_1/kernel/v                                 size 1            shape (1,)                                     partition spec None
I0512 23:45:21.801156 140143678494720 utils.py:1372] Variable param_states/decoder/layers_10/mlp/wi_1/kernel/v_col                             size 8192         shape (8192,)                                  partition spec None
I0512 23:45:21.801199 140143678494720 utils.py:1372] Variable param_states/decoder/layers_10/mlp/wi_1/kernel/v_row                             size 2048         shape (2048,)                                  partition spec None
I0512 23:45:21.801242 140143678494720 utils.py:1372] Variable param_states/decoder/layers_10/mlp/wo/kernel/m                                   size 1            shape (1,)                                     partition spec None
I0512 23:45:21.801284 140143678494720 utils.py:1372] Variable param_states/decoder/layers_10/mlp/wo/kernel/v                                   size 1            shape (1,)                                     partition spec None
I0512 23:45:21.801330 140143678494720 utils.py:1372] Variable param_states/decoder/layers_10/mlp/wo/kernel/v_col                               size 8192         shape (8192,)                                  partition spec None
I0512 23:45:21.801372 140143678494720 utils.py:1372] Variable param_states/decoder/layers_10/mlp/wo/kernel/v_row                               size 2048         shape (2048,)                                  partition spec None
I0512 23:45:21.801413 140143678494720 utils.py:1372] Variable param_states/decoder/layers_10/pre_mlp_layer_norm/scale/m                        size 1            shape (1,)                                     partition spec None
I0512 23:45:21.801457 140143678494720 utils.py:1372] Variable param_states/decoder/layers_10/pre_mlp_layer_norm/scale/v                        size 2048         shape (embed=2048)                             partition spec ('model',)
I0512 23:45:21.801501 140143678494720 utils.py:1372] Variable param_states/decoder/layers_10/pre_mlp_layer_norm/scale/v_col                    size 1            shape (1,)                                     partition spec None
I0512 23:45:21.801548 140143678494720 utils.py:1372] Variable param_states/decoder/layers_10/pre_mlp_layer_norm/scale/v_row                    size 1            shape (1,)                                     partition spec None
I0512 23:45:21.801592 140143678494720 utils.py:1372] Variable param_states/decoder/layers_10/pre_self_attention_layer_norm/scale/m             size 1            shape (1,)                                     partition spec None
I0512 23:45:21.801637 140143678494720 utils.py:1372] Variable param_states/decoder/layers_10/pre_self_attention_layer_norm/scale/v             size 2048         shape (embed=2048)                             partition spec ('model',)
I0512 23:45:21.801680 140143678494720 utils.py:1372] Variable param_states/decoder/layers_10/pre_self_attention_layer_norm/scale/v_col         size 1            shape (1,)                                     partition spec None
I0512 23:45:21.801723 140143678494720 utils.py:1372] Variable param_states/decoder/layers_10/pre_self_attention_layer_norm/scale/v_row         size 1            shape (1,)                                     partition spec None
I0512 23:45:21.801765 140143678494720 utils.py:1372] Variable param_states/decoder/layers_10/self_attention/key/kernel/m                       size 1            shape (1,)                                     partition spec None
I0512 23:45:21.801808 140143678494720 utils.py:1372] Variable param_states/decoder/layers_10/self_attention/key/kernel/v                       size 1            shape (1,)                                     partition spec None
I0512 23:45:21.801852 140143678494720 utils.py:1372] Variable param_states/decoder/layers_10/self_attention/key/kernel/v_col                   size 3072         shape (3072,)                                  partition spec None
I0512 23:45:21.801894 140143678494720 utils.py:1372] Variable param_states/decoder/layers_10/self_attention/key/kernel/v_row                   size 2048         shape (2048,)                                  partition spec None
I0512 23:45:21.801935 140143678494720 utils.py:1372] Variable param_states/decoder/layers_10/self_attention/out/kernel/m                       size 1            shape (1,)                                     partition spec None
I0512 23:45:21.801977 140143678494720 utils.py:1372] Variable param_states/decoder/layers_10/self_attention/out/kernel/v                       size 1            shape (1,)                                     partition spec None
I0512 23:45:21.802019 140143678494720 utils.py:1372] Variable param_states/decoder/layers_10/self_attention/out/kernel/v_col                   size 3072         shape (3072,)                                  partition spec None
I0512 23:45:21.802060 140143678494720 utils.py:1372] Variable param_states/decoder/layers_10/self_attention/out/kernel/v_row                   size 2048         shape (2048,)                                  partition spec None
I0512 23:45:21.802101 140143678494720 utils.py:1372] Variable param_states/decoder/layers_10/self_attention/query/kernel/m                     size 1            shape (1,)                                     partition spec None
I0512 23:45:21.802143 140143678494720 utils.py:1372] Variable param_states/decoder/layers_10/self_attention/query/kernel/v                     size 1            shape (1,)                                     partition spec None
I0512 23:45:21.802187 140143678494720 utils.py:1372] Variable param_states/decoder/layers_10/self_attention/query/kernel/v_col                 size 3072         shape (3072,)                                  partition spec None
I0512 23:45:21.802229 140143678494720 utils.py:1372] Variable param_states/decoder/layers_10/self_attention/query/kernel/v_row                 size 2048         shape (2048,)                                  partition spec None
I0512 23:45:21.802270 140143678494720 utils.py:1372] Variable param_states/decoder/layers_10/self_attention/value/kernel/m                     size 1            shape (1,)                                     partition spec None
I0512 23:45:21.802313 140143678494720 utils.py:1372] Variable param_states/decoder/layers_10/self_attention/value/kernel/v                     size 1            shape (1,)                                     partition spec None
I0512 23:45:21.802356 140143678494720 utils.py:1372] Variable param_states/decoder/layers_10/self_attention/value/kernel/v_col                 size 3072         shape (3072,)                                  partition spec None
I0512 23:45:21.802398 140143678494720 utils.py:1372] Variable param_states/decoder/layers_10/self_attention/value/kernel/v_row                 size 2048         shape (2048,)                                  partition spec None
I0512 23:45:21.802440 140143678494720 utils.py:1372] Variable param_states/decoder/layers_11/extra_mlp/wi_0/kernel/m                           size 1            shape (1,)                                     partition spec None
I0512 23:45:21.802482 140143678494720 utils.py:1372] Variable param_states/decoder/layers_11/extra_mlp/wi_0/kernel/v                           size 1            shape (1,)                                     partition spec None
I0512 23:45:21.802524 140143678494720 utils.py:1372] Variable param_states/decoder/layers_11/extra_mlp/wi_0/kernel/v_col                       size 16384        shape (16384,)                                 partition spec None
I0512 23:45:21.802572 140143678494720 utils.py:1372] Variable param_states/decoder/layers_11/extra_mlp/wi_0/kernel/v_row                       size 2048         shape (2048,)                                  partition spec None
I0512 23:45:21.802614 140143678494720 utils.py:1372] Variable param_states/decoder/layers_11/extra_mlp/wi_1/kernel/m                           size 1            shape (1,)                                     partition spec None
I0512 23:45:21.802656 140143678494720 utils.py:1372] Variable param_states/decoder/layers_11/extra_mlp/wi_1/kernel/v                           size 1            shape (1,)                                     partition spec None
I0512 23:45:21.802698 140143678494720 utils.py:1372] Variable param_states/decoder/layers_11/extra_mlp/wi_1/kernel/v_col                       size 8192         shape (8192,)                                  partition spec None
I0512 23:45:21.802739 140143678494720 utils.py:1372] Variable param_states/decoder/layers_11/extra_mlp/wi_1/kernel/v_row                       size 2048         shape (2048,)                                  partition spec None
I0512 23:45:21.802781 140143678494720 utils.py:1372] Variable param_states/decoder/layers_11/extra_mlp/wo/kernel/m                             size 1            shape (1,)                                     partition spec None
I0512 23:45:21.802822 140143678494720 utils.py:1372] Variable param_states/decoder/layers_11/extra_mlp/wo/kernel/v                             size 1            shape (1,)                                     partition spec None
I0512 23:45:21.802863 140143678494720 utils.py:1372] Variable param_states/decoder/layers_11/extra_mlp/wo/kernel/v_col                         size 8192         shape (8192,)                                  partition spec None
I0512 23:45:21.802904 140143678494720 utils.py:1372] Variable param_states/decoder/layers_11/extra_mlp/wo/kernel/v_row                         size 2048         shape (2048,)                                  partition spec None
I0512 23:45:21.802946 140143678494720 utils.py:1372] Variable param_states/decoder/layers_11/mlp/expert/wi_0/kernel/m                          size 1            shape (1,)                                     partition spec None
I0512 23:45:21.802987 140143678494720 utils.py:1372] Variable param_states/decoder/layers_11/mlp/expert/wi_0/kernel/v                          size 1            shape (1,)                                     partition spec None
I0512 23:45:21.803044 140143678494720 utils.py:1372] Variable param_states/decoder/layers_11/mlp/expert/wi_0/kernel/v_col                      size 131072       shape (8, 16384)                               partition spec ('expert',)
I0512 23:45:21.803090 140143678494720 utils.py:1372] Variable param_states/decoder/layers_11/mlp/expert/wi_0/kernel/v_row                      size 16384        shape (8, 2048)                                partition spec ('expert',)
I0512 23:45:21.803133 140143678494720 utils.py:1372] Variable param_states/decoder/layers_11/mlp/expert/wi_1/kernel/m                          size 1            shape (1,)                                     partition spec None
I0512 23:45:21.803175 140143678494720 utils.py:1372] Variable param_states/decoder/layers_11/mlp/expert/wi_1/kernel/v                          size 1            shape (1,)                                     partition spec None
I0512 23:45:21.803218 140143678494720 utils.py:1372] Variable param_states/decoder/layers_11/mlp/expert/wi_1/kernel/v_col                      size 65536        shape (8, 8192)                                partition spec ('expert',)
I0512 23:45:21.803261 140143678494720 utils.py:1372] Variable param_states/decoder/layers_11/mlp/expert/wi_1/kernel/v_row                      size 16384        shape (8, 2048)                                partition spec ('expert',)
I0512 23:45:21.803306 140143678494720 utils.py:1372] Variable param_states/decoder/layers_11/mlp/expert/wo/kernel/m                            size 1            shape (1,)                                     partition spec None
I0512 23:45:21.803349 140143678494720 utils.py:1372] Variable param_states/decoder/layers_11/mlp/expert/wo/kernel/v                            size 1            shape (1,)                                     partition spec None
I0512 23:45:21.803392 140143678494720 utils.py:1372] Variable param_states/decoder/layers_11/mlp/expert/wo/kernel/v_col                        size 65536        shape (8, 8192)                                partition spec ('expert',)
I0512 23:45:21.803435 140143678494720 utils.py:1372] Variable param_states/decoder/layers_11/mlp/expert/wo/kernel/v_row                        size 16384        shape (8, 2048)                                partition spec ('expert',)
I0512 23:45:21.803477 140143678494720 utils.py:1372] Variable param_states/decoder/layers_11/mlp/router/router_weights/w/kernel/m              size 1            shape (1,)                                     partition spec None
I0512 23:45:21.803523 140143678494720 utils.py:1372] Variable param_states/decoder/layers_11/mlp/router/router_weights/w/kernel/v              size 16384        shape (embed=2048, unmodeled=8)                partition spec ('model', None)
I0512 23:45:21.803575 140143678494720 utils.py:1372] Variable param_states/decoder/layers_11/mlp/router/router_weights/w/kernel/v_col          size 1            shape (1,)                                     partition spec None
I0512 23:45:21.803618 140143678494720 utils.py:1372] Variable param_states/decoder/layers_11/mlp/router/router_weights/w/kernel/v_row          size 1            shape (1,)                                     partition spec None
I0512 23:45:21.803659 140143678494720 utils.py:1372] Variable param_states/decoder/layers_11/pre_extra_mlp_layer_norm/scale/m                  size 1            shape (1,)                                     partition spec None
I0512 23:45:21.803704 140143678494720 utils.py:1372] Variable param_states/decoder/layers_11/pre_extra_mlp_layer_norm/scale/v                  size 2048         shape (embed=2048)                             partition spec ('model',)
I0512 23:45:21.803748 140143678494720 utils.py:1372] Variable param_states/decoder/layers_11/pre_extra_mlp_layer_norm/scale/v_col              size 1            shape (1,)                                     partition spec None
I0512 23:45:21.803791 140143678494720 utils.py:1372] Variable param_states/decoder/layers_11/pre_extra_mlp_layer_norm/scale/v_row              size 1            shape (1,)                                     partition spec None
I0512 23:45:21.803832 140143678494720 utils.py:1372] Variable param_states/decoder/layers_11/pre_mlp_layer_norm/scale/m                        size 1            shape (1,)                                     partition spec None
I0512 23:45:21.803876 140143678494720 utils.py:1372] Variable param_states/decoder/layers_11/pre_mlp_layer_norm/scale/v                        size 2048         shape (embed=2048)                             partition spec ('model',)
I0512 23:45:21.803920 140143678494720 utils.py:1372] Variable param_states/decoder/layers_11/pre_mlp_layer_norm/scale/v_col                    size 1            shape (1,)                                     partition spec None
I0512 23:45:21.803962 140143678494720 utils.py:1372] Variable param_states/decoder/layers_11/pre_mlp_layer_norm/scale/v_row                    size 1            shape (1,)                                     partition spec None
I0512 23:45:21.804003 140143678494720 utils.py:1372] Variable param_states/decoder/layers_11/pre_self_attention_layer_norm/scale/m             size 1            shape (1,)                                     partition spec None
I0512 23:45:21.804047 140143678494720 utils.py:1372] Variable param_states/decoder/layers_11/pre_self_attention_layer_norm/scale/v             size 2048         shape (embed=2048)                             partition spec ('model',)
I0512 23:45:21.804090 140143678494720 utils.py:1372] Variable param_states/decoder/layers_11/pre_self_attention_layer_norm/scale/v_col         size 1            shape (1,)                                     partition spec None
I0512 23:45:21.804132 140143678494720 utils.py:1372] Variable param_states/decoder/layers_11/pre_self_attention_layer_norm/scale/v_row         size 1            shape (1,)                                     partition spec None
I0512 23:45:21.804174 140143678494720 utils.py:1372] Variable param_states/decoder/layers_11/self_attention/key/kernel/m                       size 1            shape (1,)                                     partition spec None
I0512 23:45:21.804216 140143678494720 utils.py:1372] Variable param_states/decoder/layers_11/self_attention/key/kernel/v                       size 1            shape (1,)                                     partition spec None
I0512 23:45:21.804258 140143678494720 utils.py:1372] Variable param_states/decoder/layers_11/self_attention/key/kernel/v_col                   size 3072         shape (3072,)                                  partition spec None
I0512 23:45:21.804304 140143678494720 utils.py:1372] Variable param_states/decoder/layers_11/self_attention/key/kernel/v_row                   size 2048         shape (2048,)                                  partition spec None
I0512 23:45:21.804347 140143678494720 utils.py:1372] Variable param_states/decoder/layers_11/self_attention/out/kernel/m                       size 1            shape (1,)                                     partition spec None
I0512 23:45:21.804390 140143678494720 utils.py:1372] Variable param_states/decoder/layers_11/self_attention/out/kernel/v                       size 1            shape (1,)                                     partition spec None
I0512 23:45:21.804432 140143678494720 utils.py:1372] Variable param_states/decoder/layers_11/self_attention/out/kernel/v_col                   size 3072         shape (3072,)                                  partition spec None
I0512 23:45:21.804474 140143678494720 utils.py:1372] Variable param_states/decoder/layers_11/self_attention/out/kernel/v_row                   size 2048         shape (2048,)                                  partition spec None
I0512 23:45:21.804515 140143678494720 utils.py:1372] Variable param_states/decoder/layers_11/self_attention/query/kernel/m                     size 1            shape (1,)                                     partition spec None
I0512 23:45:21.804585 140143678494720 utils.py:1372] Variable param_states/decoder/layers_11/self_attention/query/kernel/v                     size 1            shape (1,)                                     partition spec None
I0512 23:45:21.804630 140143678494720 utils.py:1372] Variable param_states/decoder/layers_11/self_attention/query/kernel/v_col                 size 3072         shape (3072,)                                  partition spec None
I0512 23:45:21.804673 140143678494720 utils.py:1372] Variable param_states/decoder/layers_11/self_attention/query/kernel/v_row                 size 2048         shape (2048,)                                  partition spec None
I0512 23:45:21.804715 140143678494720 utils.py:1372] Variable param_states/decoder/layers_11/self_attention/value/kernel/m                     size 1            shape (1,)                                     partition spec None
I0512 23:45:21.804757 140143678494720 utils.py:1372] Variable param_states/decoder/layers_11/self_attention/value/kernel/v                     size 1            shape (1,)                                     partition spec None
I0512 23:45:21.804799 140143678494720 utils.py:1372] Variable param_states/decoder/layers_11/self_attention/value/kernel/v_col                 size 3072         shape (3072,)                                  partition spec None
I0512 23:45:21.804841 140143678494720 utils.py:1372] Variable param_states/decoder/layers_11/self_attention/value/kernel/v_row                 size 2048         shape (2048,)                                  partition spec None
I0512 23:45:21.804883 140143678494720 utils.py:1372] Variable param_states/decoder/layers_12/mlp/wi_0/kernel/m                                 size 1            shape (1,)                                     partition spec None
I0512 23:45:21.804926 140143678494720 utils.py:1372] Variable param_states/decoder/layers_12/mlp/wi_0/kernel/v                                 size 1            shape (1,)                                     partition spec None
I0512 23:45:21.804967 140143678494720 utils.py:1372] Variable param_states/decoder/layers_12/mlp/wi_0/kernel/v_col                             size 16384        shape (16384,)                                 partition spec None
I0512 23:45:21.805009 140143678494720 utils.py:1372] Variable param_states/decoder/layers_12/mlp/wi_0/kernel/v_row                             size 2048         shape (2048,)                                  partition spec None
I0512 23:45:21.805052 140143678494720 utils.py:1372] Variable param_states/decoder/layers_12/mlp/wi_1/kernel/m                                 size 1            shape (1,)                                     partition spec None
I0512 23:45:21.805119 140143678494720 utils.py:1372] Variable param_states/decoder/layers_12/mlp/wi_1/kernel/v                                 size 1            shape (1,)                                     partition spec None
I0512 23:45:21.805170 140143678494720 utils.py:1372] Variable param_states/decoder/layers_12/mlp/wi_1/kernel/v_col                             size 8192         shape (8192,)                                  partition spec None
I0512 23:45:21.805216 140143678494720 utils.py:1372] Variable param_states/decoder/layers_12/mlp/wi_1/kernel/v_row                             size 2048         shape (2048,)                                  partition spec None
I0512 23:45:21.805263 140143678494720 utils.py:1372] Variable param_states/decoder/layers_12/mlp/wo/kernel/m                                   size 1            shape (1,)                                     partition spec None
I0512 23:45:21.805310 140143678494720 utils.py:1372] Variable param_states/decoder/layers_12/mlp/wo/kernel/v                                   size 1            shape (1,)                                     partition spec None
I0512 23:45:21.805358 140143678494720 utils.py:1372] Variable param_states/decoder/layers_12/mlp/wo/kernel/v_col                               size 8192         shape (8192,)                                  partition spec None
I0512 23:45:21.805402 140143678494720 utils.py:1372] Variable param_states/decoder/layers_12/mlp/wo/kernel/v_row                               size 2048         shape (2048,)                                  partition spec None
I0512 23:45:21.805449 140143678494720 utils.py:1372] Variable param_states/decoder/layers_12/pre_mlp_layer_norm/scale/m                        size 1            shape (1,)                                     partition spec None
I0512 23:45:21.805498 140143678494720 utils.py:1372] Variable param_states/decoder/layers_12/pre_mlp_layer_norm/scale/v                        size 2048         shape (embed=2048)                             partition spec ('model',)
I0512 23:45:21.805555 140143678494720 utils.py:1372] Variable param_states/decoder/layers_12/pre_mlp_layer_norm/scale/v_col                    size 1            shape (1,)                                     partition spec None
I0512 23:45:21.805604 140143678494720 utils.py:1372] Variable param_states/decoder/layers_12/pre_mlp_layer_norm/scale/v_row                    size 1            shape (1,)                                     partition spec None
I0512 23:45:21.805648 140143678494720 utils.py:1372] Variable param_states/decoder/layers_12/pre_self_attention_layer_norm/scale/m             size 1            shape (1,)                                     partition spec None
I0512 23:45:21.805698 140143678494720 utils.py:1372] Variable param_states/decoder/layers_12/pre_self_attention_layer_norm/scale/v             size 2048         shape (embed=2048)                             partition spec ('model',)
I0512 23:45:21.805744 140143678494720 utils.py:1372] Variable param_states/decoder/layers_12/pre_self_attention_layer_norm/scale/v_col         size 1            shape (1,)                                     partition spec None
I0512 23:45:21.805791 140143678494720 utils.py:1372] Variable param_states/decoder/layers_12/pre_self_attention_layer_norm/scale/v_row         size 1            shape (1,)                                     partition spec None
I0512 23:45:21.805837 140143678494720 utils.py:1372] Variable param_states/decoder/layers_12/self_attention/key/kernel/m                       size 1            shape (1,)                                     partition spec None
I0512 23:45:21.805883 140143678494720 utils.py:1372] Variable param_states/decoder/layers_12/self_attention/key/kernel/v                       size 1            shape (1,)                                     partition spec None
I0512 23:45:21.805930 140143678494720 utils.py:1372] Variable param_states/decoder/layers_12/self_attention/key/kernel/v_col                   size 3072         shape (3072,)                                  partition spec None
I0512 23:45:21.805974 140143678494720 utils.py:1372] Variable param_states/decoder/layers_12/self_attention/key/kernel/v_row                   size 2048         shape (2048,)                                  partition spec None
I0512 23:45:21.806020 140143678494720 utils.py:1372] Variable param_states/decoder/layers_12/self_attention/out/kernel/m                       size 1            shape (1,)                                     partition spec None
I0512 23:45:21.806065 140143678494720 utils.py:1372] Variable param_states/decoder/layers_12/self_attention/out/kernel/v                       size 1            shape (1,)                                     partition spec None
I0512 23:45:21.806114 140143678494720 utils.py:1372] Variable param_states/decoder/layers_12/self_attention/out/kernel/v_col                   size 3072         shape (3072,)                                  partition spec None
I0512 23:45:21.806157 140143678494720 utils.py:1372] Variable param_states/decoder/layers_12/self_attention/out/kernel/v_row                   size 2048         shape (2048,)                                  partition spec None
I0512 23:45:21.806204 140143678494720 utils.py:1372] Variable param_states/decoder/layers_12/self_attention/query/kernel/m                     size 1            shape (1,)                                     partition spec None
I0512 23:45:21.806249 140143678494720 utils.py:1372] Variable param_states/decoder/layers_12/self_attention/query/kernel/v                     size 1            shape (1,)                                     partition spec None
I0512 23:45:21.806296 140143678494720 utils.py:1372] Variable param_states/decoder/layers_12/self_attention/query/kernel/v_col                 size 3072         shape (3072,)                                  partition spec None
I0512 23:45:21.806343 140143678494720 utils.py:1372] Variable param_states/decoder/layers_12/self_attention/query/kernel/v_row                 size 2048         shape (2048,)                                  partition spec None
I0512 23:45:21.806387 140143678494720 utils.py:1372] Variable param_states/decoder/layers_12/self_attention/value/kernel/m                     size 1            shape (1,)                                     partition spec None
I0512 23:45:21.806433 140143678494720 utils.py:1372] Variable param_states/decoder/layers_12/self_attention/value/kernel/v                     size 1            shape (1,)                                     partition spec None
I0512 23:45:21.806477 140143678494720 utils.py:1372] Variable param_states/decoder/layers_12/self_attention/value/kernel/v_col                 size 3072         shape (3072,)                                  partition spec None
I0512 23:45:21.806524 140143678494720 utils.py:1372] Variable param_states/decoder/layers_12/self_attention/value/kernel/v_row                 size 2048         shape (2048,)                                  partition spec None
I0512 23:45:21.806574 140143678494720 utils.py:1372] Variable param_states/decoder/layers_13/mlp/wi_0/kernel/m                                 size 1            shape (1,)                                     partition spec None
I0512 23:45:21.806623 140143678494720 utils.py:1372] Variable param_states/decoder/layers_13/mlp/wi_0/kernel/v                                 size 1            shape (1,)                                     partition spec None
I0512 23:45:21.806668 140143678494720 utils.py:1372] Variable param_states/decoder/layers_13/mlp/wi_0/kernel/v_col                             size 16384        shape (16384,)                                 partition spec None
I0512 23:45:21.806715 140143678494720 utils.py:1372] Variable param_states/decoder/layers_13/mlp/wi_0/kernel/v_row                             size 2048         shape (2048,)                                  partition spec None
I0512 23:45:21.806761 140143678494720 utils.py:1372] Variable param_states/decoder/layers_13/mlp/wi_1/kernel/m                                 size 1            shape (1,)                                     partition spec None
I0512 23:45:21.806807 140143678494720 utils.py:1372] Variable param_states/decoder/layers_13/mlp/wi_1/kernel/v                                 size 1            shape (1,)                                     partition spec None
I0512 23:45:21.806854 140143678494720 utils.py:1372] Variable param_states/decoder/layers_13/mlp/wi_1/kernel/v_col                             size 8192         shape (8192,)                                  partition spec None
I0512 23:45:21.806898 140143678494720 utils.py:1372] Variable param_states/decoder/layers_13/mlp/wi_1/kernel/v_row                             size 2048         shape (2048,)                                  partition spec None
I0512 23:45:21.806944 140143678494720 utils.py:1372] Variable param_states/decoder/layers_13/mlp/wo/kernel/m                                   size 1            shape (1,)                                     partition spec None
I0512 23:45:21.806987 140143678494720 utils.py:1372] Variable param_states/decoder/layers_13/mlp/wo/kernel/v                                   size 1            shape (1,)                                     partition spec None
I0512 23:45:21.807034 140143678494720 utils.py:1372] Variable param_states/decoder/layers_13/mlp/wo/kernel/v_col                               size 8192         shape (8192,)                                  partition spec None
I0512 23:45:21.807077 140143678494720 utils.py:1372] Variable param_states/decoder/layers_13/mlp/wo/kernel/v_row                               size 2048         shape (2048,)                                  partition spec None
I0512 23:45:21.807123 140143678494720 utils.py:1372] Variable param_states/decoder/layers_13/pre_mlp_layer_norm/scale/m                        size 1            shape (1,)                                     partition spec None
I0512 23:45:21.807172 140143678494720 utils.py:1372] Variable param_states/decoder/layers_13/pre_mlp_layer_norm/scale/v                        size 2048         shape (embed=2048)                             partition spec ('model',)
I0512 23:45:21.807219 140143678494720 utils.py:1372] Variable param_states/decoder/layers_13/pre_mlp_layer_norm/scale/v_col                    size 1            shape (1,)                                     partition spec None
I0512 23:45:21.807265 140143678494720 utils.py:1372] Variable param_states/decoder/layers_13/pre_mlp_layer_norm/scale/v_row                    size 1            shape (1,)                                     partition spec None
I0512 23:45:21.807328 140143678494720 utils.py:1372] Variable param_states/decoder/layers_13/pre_self_attention_layer_norm/scale/m             size 1            shape (1,)                                     partition spec None
I0512 23:45:21.807382 140143678494720 utils.py:1372] Variable param_states/decoder/layers_13/pre_self_attention_layer_norm/scale/v             size 2048         shape (embed=2048)                             partition spec ('model',)
I0512 23:45:21.807431 140143678494720 utils.py:1372] Variable param_states/decoder/layers_13/pre_self_attention_layer_norm/scale/v_col         size 1            shape (1,)                                     partition spec None
I0512 23:45:21.807476 140143678494720 utils.py:1372] Variable param_states/decoder/layers_13/pre_self_attention_layer_norm/scale/v_row         size 1            shape (1,)                                     partition spec None
I0512 23:45:21.807523 140143678494720 utils.py:1372] Variable param_states/decoder/layers_13/self_attention/key/kernel/m                       size 1            shape (1,)                                     partition spec None
I0512 23:45:21.807576 140143678494720 utils.py:1372] Variable param_states/decoder/layers_13/self_attention/key/kernel/v                       size 1            shape (1,)                                     partition spec None
I0512 23:45:21.807625 140143678494720 utils.py:1372] Variable param_states/decoder/layers_13/self_attention/key/kernel/v_col                   size 3072         shape (3072,)                                  partition spec None
I0512 23:45:21.807669 140143678494720 utils.py:1372] Variable param_states/decoder/layers_13/self_attention/key/kernel/v_row                   size 2048         shape (2048,)                                  partition spec None
I0512 23:45:21.807716 140143678494720 utils.py:1372] Variable param_states/decoder/layers_13/self_attention/out/kernel/m                       size 1            shape (1,)                                     partition spec None
I0512 23:45:21.807761 140143678494720 utils.py:1372] Variable param_states/decoder/layers_13/self_attention/out/kernel/v                       size 1            shape (1,)                                     partition spec None
I0512 23:45:21.807806 140143678494720 utils.py:1372] Variable param_states/decoder/layers_13/self_attention/out/kernel/v_col                   size 3072         shape (3072,)                                  partition spec None
I0512 23:45:21.807853 140143678494720 utils.py:1372] Variable param_states/decoder/layers_13/self_attention/out/kernel/v_row                   size 2048         shape (2048,)                                  partition spec None
I0512 23:45:21.807897 140143678494720 utils.py:1372] Variable param_states/decoder/layers_13/self_attention/query/kernel/m                     size 1            shape (1,)                                     partition spec None
I0512 23:45:21.807943 140143678494720 utils.py:1372] Variable param_states/decoder/layers_13/self_attention/query/kernel/v                     size 1            shape (1,)                                     partition spec None
I0512 23:45:21.807986 140143678494720 utils.py:1372] Variable param_states/decoder/layers_13/self_attention/query/kernel/v_col                 size 3072         shape (3072,)                                  partition spec None
I0512 23:45:21.808033 140143678494720 utils.py:1372] Variable param_states/decoder/layers_13/self_attention/query/kernel/v_row                 size 2048         shape (2048,)                                  partition spec None
I0512 23:45:21.808075 140143678494720 utils.py:1372] Variable param_states/decoder/layers_13/self_attention/value/kernel/m                     size 1            shape (1,)                                     partition spec None
I0512 23:45:21.808123 140143678494720 utils.py:1372] Variable param_states/decoder/layers_13/self_attention/value/kernel/v                     size 1            shape (1,)                                     partition spec None
I0512 23:45:21.808166 140143678494720 utils.py:1372] Variable param_states/decoder/layers_13/self_attention/value/kernel/v_col                 size 3072         shape (3072,)                                  partition spec None
I0512 23:45:21.808212 140143678494720 utils.py:1372] Variable param_states/decoder/layers_13/self_attention/value/kernel/v_row                 size 2048         shape (2048,)                                  partition spec None
I0512 23:45:21.808255 140143678494720 utils.py:1372] Variable param_states/decoder/layers_14/mlp/wi_0/kernel/m                                 size 1            shape (1,)                                     partition spec None
I0512 23:45:21.808299 140143678494720 utils.py:1372] Variable param_states/decoder/layers_14/mlp/wi_0/kernel/v                                 size 1            shape (1,)                                     partition spec None
I0512 23:45:21.808345 140143678494720 utils.py:1372] Variable param_states/decoder/layers_14/mlp/wi_0/kernel/v_col                             size 16384        shape (16384,)                                 partition spec None
I0512 23:45:21.808390 140143678494720 utils.py:1372] Variable param_states/decoder/layers_14/mlp/wi_0/kernel/v_row                             size 2048         shape (2048,)                                  partition spec None
I0512 23:45:21.808432 140143678494720 utils.py:1372] Variable param_states/decoder/layers_14/mlp/wi_1/kernel/m                                 size 1            shape (1,)                                     partition spec None
I0512 23:45:21.808474 140143678494720 utils.py:1372] Variable param_states/decoder/layers_14/mlp/wi_1/kernel/v                                 size 1            shape (1,)                                     partition spec None
I0512 23:45:21.808516 140143678494720 utils.py:1372] Variable param_states/decoder/layers_14/mlp/wi_1/kernel/v_col                             size 8192         shape (8192,)                                  partition spec None
I0512 23:45:21.808586 140143678494720 utils.py:1372] Variable param_states/decoder/layers_14/mlp/wi_1/kernel/v_row                             size 2048         shape (2048,)                                  partition spec None
I0512 23:45:21.808630 140143678494720 utils.py:1372] Variable param_states/decoder/layers_14/mlp/wo/kernel/m                                   size 1            shape (1,)                                     partition spec None
I0512 23:45:21.808673 140143678494720 utils.py:1372] Variable param_states/decoder/layers_14/mlp/wo/kernel/v                                   size 1            shape (1,)                                     partition spec None
I0512 23:45:21.808715 140143678494720 utils.py:1372] Variable param_states/decoder/layers_14/mlp/wo/kernel/v_col                               size 8192         shape (8192,)                                  partition spec None
I0512 23:45:21.808756 140143678494720 utils.py:1372] Variable param_states/decoder/layers_14/mlp/wo/kernel/v_row                               size 2048         shape (2048,)                                  partition spec None
I0512 23:45:21.808797 140143678494720 utils.py:1372] Variable param_states/decoder/layers_14/pre_mlp_layer_norm/scale/m                        size 1            shape (1,)                                     partition spec None
I0512 23:45:21.808842 140143678494720 utils.py:1372] Variable param_states/decoder/layers_14/pre_mlp_layer_norm/scale/v                        size 2048         shape (embed=2048)                             partition spec ('model',)
I0512 23:45:21.808886 140143678494720 utils.py:1372] Variable param_states/decoder/layers_14/pre_mlp_layer_norm/scale/v_col                    size 1            shape (1,)                                     partition spec None
I0512 23:45:21.808929 140143678494720 utils.py:1372] Variable param_states/decoder/layers_14/pre_mlp_layer_norm/scale/v_row                    size 1            shape (1,)                                     partition spec None
I0512 23:45:21.808976 140143678494720 utils.py:1372] Variable param_states/decoder/layers_14/pre_self_attention_layer_norm/scale/m             size 1            shape (1,)                                     partition spec None
I0512 23:45:21.809022 140143678494720 utils.py:1372] Variable param_states/decoder/layers_14/pre_self_attention_layer_norm/scale/v             size 2048         shape (embed=2048)                             partition spec ('model',)
I0512 23:45:21.809066 140143678494720 utils.py:1372] Variable param_states/decoder/layers_14/pre_self_attention_layer_norm/scale/v_col         size 1            shape (1,)                                     partition spec None
I0512 23:45:21.809108 140143678494720 utils.py:1372] Variable param_states/decoder/layers_14/pre_self_attention_layer_norm/scale/v_row         size 1            shape (1,)                                     partition spec None
I0512 23:45:21.809149 140143678494720 utils.py:1372] Variable param_states/decoder/layers_14/self_attention/key/kernel/m                       size 1            shape (1,)                                     partition spec None
I0512 23:45:21.809191 140143678494720 utils.py:1372] Variable param_states/decoder/layers_14/self_attention/key/kernel/v                       size 1            shape (1,)                                     partition spec None
I0512 23:45:21.809233 140143678494720 utils.py:1372] Variable param_states/decoder/layers_14/self_attention/key/kernel/v_col                   size 3072         shape (3072,)                                  partition spec None
I0512 23:45:21.809273 140143678494720 utils.py:1372] Variable param_states/decoder/layers_14/self_attention/key/kernel/v_row                   size 2048         shape (2048,)                                  partition spec None
I0512 23:45:21.809314 140143678494720 utils.py:1372] Variable param_states/decoder/layers_14/self_attention/out/kernel/m                       size 1            shape (1,)                                     partition spec None
I0512 23:45:21.809356 140143678494720 utils.py:1372] Variable param_states/decoder/layers_14/self_attention/out/kernel/v                       size 1            shape (1,)                                     partition spec None
I0512 23:45:21.809395 140143678494720 utils.py:1372] Variable param_states/decoder/layers_14/self_attention/out/kernel/v_col                   size 3072         shape (3072,)                                  partition spec None
I0512 23:45:21.809451 140143678494720 utils.py:1372] Variable param_states/decoder/layers_14/self_attention/out/kernel/v_row                   size 2048         shape (2048,)                                  partition spec None
I0512 23:45:21.809494 140143678494720 utils.py:1372] Variable param_states/decoder/layers_14/self_attention/query/kernel/m                     size 1            shape (1,)                                     partition spec None
I0512 23:45:21.809535 140143678494720 utils.py:1372] Variable param_states/decoder/layers_14/self_attention/query/kernel/v                     size 1            shape (1,)                                     partition spec None
I0512 23:45:21.809584 140143678494720 utils.py:1372] Variable param_states/decoder/layers_14/self_attention/query/kernel/v_col                 size 3072         shape (3072,)                                  partition spec None
I0512 23:45:21.809624 140143678494720 utils.py:1372] Variable param_states/decoder/layers_14/self_attention/query/kernel/v_row                 size 2048         shape (2048,)                                  partition spec None
I0512 23:45:21.809670 140143678494720 utils.py:1372] Variable param_states/decoder/layers_14/self_attention/value/kernel/m                     size 1            shape (1,)                                     partition spec None
I0512 23:45:21.809713 140143678494720 utils.py:1372] Variable param_states/decoder/layers_14/self_attention/value/kernel/v                     size 1            shape (1,)                                     partition spec None
I0512 23:45:21.809752 140143678494720 utils.py:1372] Variable param_states/decoder/layers_14/self_attention/value/kernel/v_col                 size 3072         shape (3072,)                                  partition spec None
I0512 23:45:21.809792 140143678494720 utils.py:1372] Variable param_states/decoder/layers_14/self_attention/value/kernel/v_row                 size 2048         shape (2048,)                                  partition spec None
I0512 23:45:21.809831 140143678494720 utils.py:1372] Variable param_states/decoder/layers_15/mlp/wi_0/kernel/m                                 size 1            shape (1,)                                     partition spec None
I0512 23:45:21.809871 140143678494720 utils.py:1372] Variable param_states/decoder/layers_15/mlp/wi_0/kernel/v                                 size 1            shape (1,)                                     partition spec None
I0512 23:45:21.809911 140143678494720 utils.py:1372] Variable param_states/decoder/layers_15/mlp/wi_0/kernel/v_col                             size 16384        shape (16384,)                                 partition spec None
I0512 23:45:21.809950 140143678494720 utils.py:1372] Variable param_states/decoder/layers_15/mlp/wi_0/kernel/v_row                             size 2048         shape (2048,)                                  partition spec None
I0512 23:45:21.809988 140143678494720 utils.py:1372] Variable param_states/decoder/layers_15/mlp/wi_1/kernel/m                                 size 1            shape (1,)                                     partition spec None
I0512 23:45:21.810027 140143678494720 utils.py:1372] Variable param_states/decoder/layers_15/mlp/wi_1/kernel/v                                 size 1            shape (1,)                                     partition spec None
I0512 23:45:21.810066 140143678494720 utils.py:1372] Variable param_states/decoder/layers_15/mlp/wi_1/kernel/v_col                             size 8192         shape (8192,)                                  partition spec None
I0512 23:45:21.810105 140143678494720 utils.py:1372] Variable param_states/decoder/layers_15/mlp/wi_1/kernel/v_row                             size 2048         shape (2048,)                                  partition spec None
I0512 23:45:21.810143 140143678494720 utils.py:1372] Variable param_states/decoder/layers_15/mlp/wo/kernel/m                                   size 1            shape (1,)                                     partition spec None
I0512 23:45:21.810182 140143678494720 utils.py:1372] Variable param_states/decoder/layers_15/mlp/wo/kernel/v                                   size 1            shape (1,)                                     partition spec None
I0512 23:45:21.810220 140143678494720 utils.py:1372] Variable param_states/decoder/layers_15/mlp/wo/kernel/v_col                               size 8192         shape (8192,)                                  partition spec None
I0512 23:45:21.810260 140143678494720 utils.py:1372] Variable param_states/decoder/layers_15/mlp/wo/kernel/v_row                               size 2048         shape (2048,)                                  partition spec None
I0512 23:45:21.810299 140143678494720 utils.py:1372] Variable param_states/decoder/layers_15/pre_mlp_layer_norm/scale/m                        size 1            shape (1,)                                     partition spec None
I0512 23:45:21.810343 140143678494720 utils.py:1372] Variable param_states/decoder/layers_15/pre_mlp_layer_norm/scale/v                        size 2048         shape (embed=2048)                             partition spec ('model',)
I0512 23:45:21.810385 140143678494720 utils.py:1372] Variable param_states/decoder/layers_15/pre_mlp_layer_norm/scale/v_col                    size 1            shape (1,)                                     partition spec None
I0512 23:45:21.810425 140143678494720 utils.py:1372] Variable param_states/decoder/layers_15/pre_mlp_layer_norm/scale/v_row                    size 1            shape (1,)                                     partition spec None
I0512 23:45:21.810464 140143678494720 utils.py:1372] Variable param_states/decoder/layers_15/pre_self_attention_layer_norm/scale/m             size 1            shape (1,)                                     partition spec None
I0512 23:45:21.810505 140143678494720 utils.py:1372] Variable param_states/decoder/layers_15/pre_self_attention_layer_norm/scale/v             size 2048         shape (embed=2048)                             partition spec ('model',)
I0512 23:45:21.810555 140143678494720 utils.py:1372] Variable param_states/decoder/layers_15/pre_self_attention_layer_norm/scale/v_col         size 1            shape (1,)                                     partition spec None
I0512 23:45:21.810596 140143678494720 utils.py:1372] Variable param_states/decoder/layers_15/pre_self_attention_layer_norm/scale/v_row         size 1            shape (1,)                                     partition spec None
I0512 23:45:21.810635 140143678494720 utils.py:1372] Variable param_states/decoder/layers_15/self_attention/key/kernel/m                       size 1            shape (1,)                                     partition spec None
I0512 23:45:21.810674 140143678494720 utils.py:1372] Variable param_states/decoder/layers_15/self_attention/key/kernel/v                       size 1            shape (1,)                                     partition spec None
I0512 23:45:21.810713 140143678494720 utils.py:1372] Variable param_states/decoder/layers_15/self_attention/key/kernel/v_col                   size 3072         shape (3072,)                                  partition spec None
I0512 23:45:21.810752 140143678494720 utils.py:1372] Variable param_states/decoder/layers_15/self_attention/key/kernel/v_row                   size 2048         shape (2048,)                                  partition spec None
I0512 23:45:21.810790 140143678494720 utils.py:1372] Variable param_states/decoder/layers_15/self_attention/out/kernel/m                       size 1            shape (1,)                                     partition spec None
I0512 23:45:21.810829 140143678494720 utils.py:1372] Variable param_states/decoder/layers_15/self_attention/out/kernel/v                       size 1            shape (1,)                                     partition spec None
I0512 23:45:21.810868 140143678494720 utils.py:1372] Variable param_states/decoder/layers_15/self_attention/out/kernel/v_col                   size 3072         shape (3072,)                                  partition spec None
I0512 23:45:21.810907 140143678494720 utils.py:1372] Variable param_states/decoder/layers_15/self_attention/out/kernel/v_row                   size 2048         shape (2048,)                                  partition spec None
I0512 23:45:21.810945 140143678494720 utils.py:1372] Variable param_states/decoder/layers_15/self_attention/query/kernel/m                     size 1            shape (1,)                                     partition spec None
I0512 23:45:21.810985 140143678494720 utils.py:1372] Variable param_states/decoder/layers_15/self_attention/query/kernel/v                     size 1            shape (1,)                                     partition spec None
I0512 23:45:21.811032 140143678494720 utils.py:1372] Variable param_states/decoder/layers_15/self_attention/query/kernel/v_col                 size 3072         shape (3072,)                                  partition spec None
I0512 23:45:21.811072 140143678494720 utils.py:1372] Variable param_states/decoder/layers_15/self_attention/query/kernel/v_row                 size 2048         shape (2048,)                                  partition spec None
I0512 23:45:21.811111 140143678494720 utils.py:1372] Variable param_states/decoder/layers_15/self_attention/value/kernel/m                     size 1            shape (1,)                                     partition spec None
I0512 23:45:21.811150 140143678494720 utils.py:1372] Variable param_states/decoder/layers_15/self_attention/value/kernel/v                     size 1            shape (1,)                                     partition spec None
I0512 23:45:21.811189 140143678494720 utils.py:1372] Variable param_states/decoder/layers_15/self_attention/value/kernel/v_col                 size 3072         shape (3072,)                                  partition spec None
I0512 23:45:21.811228 140143678494720 utils.py:1372] Variable param_states/decoder/layers_15/self_attention/value/kernel/v_row                 size 2048         shape (2048,)                                  partition spec None
I0512 23:45:21.811267 140143678494720 utils.py:1372] Variable param_states/decoder/layers_16/mlp/wi_0/kernel/m                                 size 1            shape (1,)                                     partition spec None
I0512 23:45:21.811308 140143678494720 utils.py:1372] Variable param_states/decoder/layers_16/mlp/wi_0/kernel/v                                 size 1            shape (1,)                                     partition spec None
I0512 23:45:21.811378 140143678494720 utils.py:1372] Variable param_states/decoder/layers_16/mlp/wi_0/kernel/v_col                             size 16384        shape (16384,)                                 partition spec None
I0512 23:45:21.811422 140143678494720 utils.py:1372] Variable param_states/decoder/layers_16/mlp/wi_0/kernel/v_row                             size 2048         shape (2048,)                                  partition spec None
I0512 23:45:21.811463 140143678494720 utils.py:1372] Variable param_states/decoder/layers_16/mlp/wi_1/kernel/m                                 size 1            shape (1,)                                     partition spec None
I0512 23:45:21.811503 140143678494720 utils.py:1372] Variable param_states/decoder/layers_16/mlp/wi_1/kernel/v                                 size 1            shape (1,)                                     partition spec None
I0512 23:45:21.811548 140143678494720 utils.py:1372] Variable param_states/decoder/layers_16/mlp/wi_1/kernel/v_col                             size 8192         shape (8192,)                                  partition spec None
I0512 23:45:21.811590 140143678494720 utils.py:1372] Variable param_states/decoder/layers_16/mlp/wi_1/kernel/v_row                             size 2048         shape (2048,)                                  partition spec None
I0512 23:45:21.811629 140143678494720 utils.py:1372] Variable param_states/decoder/layers_16/mlp/wo/kernel/m                                   size 1            shape (1,)                                     partition spec None
I0512 23:45:21.811668 140143678494720 utils.py:1372] Variable param_states/decoder/layers_16/mlp/wo/kernel/v                                   size 1            shape (1,)                                     partition spec None
I0512 23:45:21.811708 140143678494720 utils.py:1372] Variable param_states/decoder/layers_16/mlp/wo/kernel/v_col                               size 8192         shape (8192,)                                  partition spec None
I0512 23:45:21.811747 140143678494720 utils.py:1372] Variable param_states/decoder/layers_16/mlp/wo/kernel/v_row                               size 2048         shape (2048,)                                  partition spec None
I0512 23:45:21.811788 140143678494720 utils.py:1372] Variable param_states/decoder/layers_16/pre_mlp_layer_norm/scale/m                        size 1            shape (1,)                                     partition spec None
I0512 23:45:21.811831 140143678494720 utils.py:1372] Variable param_states/decoder/layers_16/pre_mlp_layer_norm/scale/v                        size 2048         shape (embed=2048)                             partition spec ('model',)
I0512 23:45:21.811873 140143678494720 utils.py:1372] Variable param_states/decoder/layers_16/pre_mlp_layer_norm/scale/v_col                    size 1            shape (1,)                                     partition spec None
I0512 23:45:21.811913 140143678494720 utils.py:1372] Variable param_states/decoder/layers_16/pre_mlp_layer_norm/scale/v_row                    size 1            shape (1,)                                     partition spec None
I0512 23:45:21.811952 140143678494720 utils.py:1372] Variable param_states/decoder/layers_16/pre_self_attention_layer_norm/scale/m             size 1            shape (1,)                                     partition spec None
I0512 23:45:21.811993 140143678494720 utils.py:1372] Variable param_states/decoder/layers_16/pre_self_attention_layer_norm/scale/v             size 2048         shape (embed=2048)                             partition spec ('model',)
I0512 23:45:21.812034 140143678494720 utils.py:1372] Variable param_states/decoder/layers_16/pre_self_attention_layer_norm/scale/v_col         size 1            shape (1,)                                     partition spec None
I0512 23:45:21.812073 140143678494720 utils.py:1372] Variable param_states/decoder/layers_16/pre_self_attention_layer_norm/scale/v_row         size 1            shape (1,)                                     partition spec None
I0512 23:45:21.812114 140143678494720 utils.py:1372] Variable param_states/decoder/layers_16/self_attention/key/kernel/m                       size 1            shape (1,)                                     partition spec None
I0512 23:45:21.812153 140143678494720 utils.py:1372] Variable param_states/decoder/layers_16/self_attention/key/kernel/v                       size 1            shape (1,)                                     partition spec None
I0512 23:45:21.812192 140143678494720 utils.py:1372] Variable param_states/decoder/layers_16/self_attention/key/kernel/v_col                   size 3072         shape (3072,)                                  partition spec None
I0512 23:45:21.812231 140143678494720 utils.py:1372] Variable param_states/decoder/layers_16/self_attention/key/kernel/v_row                   size 2048         shape (2048,)                                  partition spec None
I0512 23:45:21.812269 140143678494720 utils.py:1372] Variable param_states/decoder/layers_16/self_attention/out/kernel/m                       size 1            shape (1,)                                     partition spec None
I0512 23:45:21.812309 140143678494720 utils.py:1372] Variable param_states/decoder/layers_16/self_attention/out/kernel/v                       size 1            shape (1,)                                     partition spec None
I0512 23:45:21.812350 140143678494720 utils.py:1372] Variable param_states/decoder/layers_16/self_attention/out/kernel/v_col                   size 3072         shape (3072,)                                  partition spec None
I0512 23:45:21.812389 140143678494720 utils.py:1372] Variable param_states/decoder/layers_16/self_attention/out/kernel/v_row                   size 2048         shape (2048,)                                  partition spec None
I0512 23:45:21.812428 140143678494720 utils.py:1372] Variable param_states/decoder/layers_16/self_attention/query/kernel/m                     size 1            shape (1,)                                     partition spec None
I0512 23:45:21.812467 140143678494720 utils.py:1372] Variable param_states/decoder/layers_16/self_attention/query/kernel/v                     size 1            shape (1,)                                     partition spec None
I0512 23:45:21.812506 140143678494720 utils.py:1372] Variable param_states/decoder/layers_16/self_attention/query/kernel/v_col                 size 3072         shape (3072,)                                  partition spec None
I0512 23:45:21.812572 140143678494720 utils.py:1372] Variable param_states/decoder/layers_16/self_attention/query/kernel/v_row                 size 2048         shape (2048,)                                  partition spec None
I0512 23:45:21.812617 140143678494720 utils.py:1372] Variable param_states/decoder/layers_16/self_attention/value/kernel/m                     size 1            shape (1,)                                     partition spec None
I0512 23:45:21.812657 140143678494720 utils.py:1372] Variable param_states/decoder/layers_16/self_attention/value/kernel/v                     size 1            shape (1,)                                     partition spec None
I0512 23:45:21.812699 140143678494720 utils.py:1372] Variable param_states/decoder/layers_16/self_attention/value/kernel/v_col                 size 3072         shape (3072,)                                  partition spec None
I0512 23:45:21.812739 140143678494720 utils.py:1372] Variable param_states/decoder/layers_16/self_attention/value/kernel/v_row                 size 2048         shape (2048,)                                  partition spec None
I0512 23:45:21.812778 140143678494720 utils.py:1372] Variable param_states/decoder/layers_17/extra_mlp/wi_0/kernel/m                           size 1            shape (1,)                                     partition spec None
I0512 23:45:21.812819 140143678494720 utils.py:1372] Variable param_states/decoder/layers_17/extra_mlp/wi_0/kernel/v                           size 1            shape (1,)                                     partition spec None
I0512 23:45:21.812859 140143678494720 utils.py:1372] Variable param_states/decoder/layers_17/extra_mlp/wi_0/kernel/v_col                       size 16384        shape (16384,)                                 partition spec None
I0512 23:45:21.812898 140143678494720 utils.py:1372] Variable param_states/decoder/layers_17/extra_mlp/wi_0/kernel/v_row                       size 2048         shape (2048,)                                  partition spec None
I0512 23:45:21.812937 140143678494720 utils.py:1372] Variable param_states/decoder/layers_17/extra_mlp/wi_1/kernel/m                           size 1            shape (1,)                                     partition spec None
I0512 23:45:21.812976 140143678494720 utils.py:1372] Variable param_states/decoder/layers_17/extra_mlp/wi_1/kernel/v                           size 1            shape (1,)                                     partition spec None
I0512 23:45:21.813015 140143678494720 utils.py:1372] Variable param_states/decoder/layers_17/extra_mlp/wi_1/kernel/v_col                       size 8192         shape (8192,)                                  partition spec None
I0512 23:45:21.813054 140143678494720 utils.py:1372] Variable param_states/decoder/layers_17/extra_mlp/wi_1/kernel/v_row                       size 2048         shape (2048,)                                  partition spec None
I0512 23:45:21.813092 140143678494720 utils.py:1372] Variable param_states/decoder/layers_17/extra_mlp/wo/kernel/m                             size 1            shape (1,)                                     partition spec None
I0512 23:45:21.813132 140143678494720 utils.py:1372] Variable param_states/decoder/layers_17/extra_mlp/wo/kernel/v                             size 1            shape (1,)                                     partition spec None
I0512 23:45:21.813170 140143678494720 utils.py:1372] Variable param_states/decoder/layers_17/extra_mlp/wo/kernel/v_col                         size 8192         shape (8192,)                                  partition spec None
I0512 23:45:21.813209 140143678494720 utils.py:1372] Variable param_states/decoder/layers_17/extra_mlp/wo/kernel/v_row                         size 2048         shape (2048,)                                  partition spec None
I0512 23:45:21.813247 140143678494720 utils.py:1372] Variable param_states/decoder/layers_17/mlp/expert/wi_0/kernel/m                          size 1            shape (1,)                                     partition spec None
I0512 23:45:21.813305 140143678494720 utils.py:1372] Variable param_states/decoder/layers_17/mlp/expert/wi_0/kernel/v                          size 1            shape (1,)                                     partition spec None
I0512 23:45:21.813350 140143678494720 utils.py:1372] Variable param_states/decoder/layers_17/mlp/expert/wi_0/kernel/v_col                      size 131072       shape (8, 16384)                               partition spec ('expert',)
I0512 23:45:21.813391 140143678494720 utils.py:1372] Variable param_states/decoder/layers_17/mlp/expert/wi_0/kernel/v_row                      size 16384        shape (8, 2048)                                partition spec ('expert',)
I0512 23:45:21.813431 140143678494720 utils.py:1372] Variable param_states/decoder/layers_17/mlp/expert/wi_1/kernel/m                          size 1            shape (1,)                                     partition spec None
I0512 23:45:21.813472 140143678494720 utils.py:1372] Variable param_states/decoder/layers_17/mlp/expert/wi_1/kernel/v                          size 1            shape (1,)                                     partition spec None
I0512 23:45:21.813511 140143678494720 utils.py:1372] Variable param_states/decoder/layers_17/mlp/expert/wi_1/kernel/v_col                      size 65536        shape (8, 8192)                                partition spec ('expert',)
I0512 23:45:21.813559 140143678494720 utils.py:1372] Variable param_states/decoder/layers_17/mlp/expert/wi_1/kernel/v_row                      size 16384        shape (8, 2048)                                partition spec ('expert',)
I0512 23:45:21.813601 140143678494720 utils.py:1372] Variable param_states/decoder/layers_17/mlp/expert/wo/kernel/m                            size 1            shape (1,)                                     partition spec None
I0512 23:45:21.813647 140143678494720 utils.py:1372] Variable param_states/decoder/layers_17/mlp/expert/wo/kernel/v                            size 1            shape (1,)                                     partition spec None
I0512 23:45:21.813689 140143678494720 utils.py:1372] Variable param_states/decoder/layers_17/mlp/expert/wo/kernel/v_col                        size 65536        shape (8, 8192)                                partition spec ('expert',)
I0512 23:45:21.813730 140143678494720 utils.py:1372] Variable param_states/decoder/layers_17/mlp/expert/wo/kernel/v_row                        size 16384        shape (8, 2048)                                partition spec ('expert',)
I0512 23:45:21.813770 140143678494720 utils.py:1372] Variable param_states/decoder/layers_17/mlp/router/router_weights/w/kernel/m              size 1            shape (1,)                                     partition spec None
I0512 23:45:21.813812 140143678494720 utils.py:1372] Variable param_states/decoder/layers_17/mlp/router/router_weights/w/kernel/v              size 16384        shape (embed=2048, unmodeled=8)                partition spec ('model', None)
I0512 23:45:21.813855 140143678494720 utils.py:1372] Variable param_states/decoder/layers_17/mlp/router/router_weights/w/kernel/v_col          size 1            shape (1,)                                     partition spec None
I0512 23:45:21.813894 140143678494720 utils.py:1372] Variable param_states/decoder/layers_17/mlp/router/router_weights/w/kernel/v_row          size 1            shape (1,)                                     partition spec None
I0512 23:45:21.813935 140143678494720 utils.py:1372] Variable param_states/decoder/layers_17/pre_extra_mlp_layer_norm/scale/m                  size 1            shape (1,)                                     partition spec None
I0512 23:45:21.813978 140143678494720 utils.py:1372] Variable param_states/decoder/layers_17/pre_extra_mlp_layer_norm/scale/v                  size 2048         shape (embed=2048)                             partition spec ('model',)
I0512 23:45:21.814021 140143678494720 utils.py:1372] Variable param_states/decoder/layers_17/pre_extra_mlp_layer_norm/scale/v_col              size 1            shape (1,)                                     partition spec None
I0512 23:45:21.814061 140143678494720 utils.py:1372] Variable param_states/decoder/layers_17/pre_extra_mlp_layer_norm/scale/v_row              size 1            shape (1,)                                     partition spec None
I0512 23:45:21.814101 140143678494720 utils.py:1372] Variable param_states/decoder/layers_17/pre_mlp_layer_norm/scale/m                        size 1            shape (1,)                                     partition spec None
I0512 23:45:21.814143 140143678494720 utils.py:1372] Variable param_states/decoder/layers_17/pre_mlp_layer_norm/scale/v                        size 2048         shape (embed=2048)                             partition spec ('model',)
I0512 23:45:21.814184 140143678494720 utils.py:1372] Variable param_states/decoder/layers_17/pre_mlp_layer_norm/scale/v_col                    size 1            shape (1,)                                     partition spec None
I0512 23:45:21.814251 140143678494720 utils.py:1372] Variable param_states/decoder/layers_17/pre_mlp_layer_norm/scale/v_row                    size 1            shape (1,)                                     partition spec None
I0512 23:45:21.814291 140143678494720 utils.py:1372] Variable param_states/decoder/layers_17/pre_self_attention_layer_norm/scale/m             size 1            shape (1,)                                     partition spec None
I0512 23:45:21.814336 140143678494720 utils.py:1372] Variable param_states/decoder/layers_17/pre_self_attention_layer_norm/scale/v             size 2048         shape (embed=2048)                             partition spec ('model',)
I0512 23:45:21.814377 140143678494720 utils.py:1372] Variable param_states/decoder/layers_17/pre_self_attention_layer_norm/scale/v_col         size 1            shape (1,)                                     partition spec None
I0512 23:45:21.814417 140143678494720 utils.py:1372] Variable param_states/decoder/layers_17/pre_self_attention_layer_norm/scale/v_row         size 1            shape (1,)                                     partition spec None
I0512 23:45:21.814456 140143678494720 utils.py:1372] Variable param_states/decoder/layers_17/self_attention/key/kernel/m                       size 1            shape (1,)                                     partition spec None
I0512 23:45:21.814495 140143678494720 utils.py:1372] Variable param_states/decoder/layers_17/self_attention/key/kernel/v                       size 1            shape (1,)                                     partition spec None
I0512 23:45:21.814537 140143678494720 utils.py:1372] Variable param_states/decoder/layers_17/self_attention/key/kernel/v_col                   size 3072         shape (3072,)                                  partition spec None
I0512 23:45:21.814585 140143678494720 utils.py:1372] Variable param_states/decoder/layers_17/self_attention/key/kernel/v_row                   size 2048         shape (2048,)                                  partition spec None
I0512 23:45:21.814624 140143678494720 utils.py:1372] Variable param_states/decoder/layers_17/self_attention/out/kernel/m                       size 1            shape (1,)                                     partition spec None
I0512 23:45:21.814663 140143678494720 utils.py:1372] Variable param_states/decoder/layers_17/self_attention/out/kernel/v                       size 1            shape (1,)                                     partition spec None
I0512 23:45:21.814702 140143678494720 utils.py:1372] Variable param_states/decoder/layers_17/self_attention/out/kernel/v_col                   size 3072         shape (3072,)                                  partition spec None
I0512 23:45:21.814741 140143678494720 utils.py:1372] Variable param_states/decoder/layers_17/self_attention/out/kernel/v_row                   size 2048         shape (2048,)                                  partition spec None
I0512 23:45:21.814780 140143678494720 utils.py:1372] Variable param_states/decoder/layers_17/self_attention/query/kernel/m                     size 1            shape (1,)                                     partition spec None
I0512 23:45:21.814819 140143678494720 utils.py:1372] Variable param_states/decoder/layers_17/self_attention/query/kernel/v                     size 1            shape (1,)                                     partition spec None
I0512 23:45:21.814857 140143678494720 utils.py:1372] Variable param_states/decoder/layers_17/self_attention/query/kernel/v_col                 size 3072         shape (3072,)                                  partition spec None
I0512 23:45:21.814896 140143678494720 utils.py:1372] Variable param_states/decoder/layers_17/self_attention/query/kernel/v_row                 size 2048         shape (2048,)                                  partition spec None
I0512 23:45:21.814934 140143678494720 utils.py:1372] Variable param_states/decoder/layers_17/self_attention/value/kernel/m                     size 1            shape (1,)                                     partition spec None
I0512 23:45:21.814973 140143678494720 utils.py:1372] Variable param_states/decoder/layers_17/self_attention/value/kernel/v                     size 1            shape (1,)                                     partition spec None
I0512 23:45:21.815011 140143678494720 utils.py:1372] Variable param_states/decoder/layers_17/self_attention/value/kernel/v_col                 size 3072         shape (3072,)                                  partition spec None
I0512 23:45:21.815051 140143678494720 utils.py:1372] Variable param_states/decoder/layers_17/self_attention/value/kernel/v_row                 size 2048         shape (2048,)                                  partition spec None
I0512 23:45:21.815089 140143678494720 utils.py:1372] Variable param_states/decoder/layers_18/mlp/wi_0/kernel/m                                 size 1            shape (1,)                                     partition spec None
I0512 23:45:21.815129 140143678494720 utils.py:1372] Variable param_states/decoder/layers_18/mlp/wi_0/kernel/v                                 size 1            shape (1,)                                     partition spec None
I0512 23:45:21.815169 140143678494720 utils.py:1372] Variable param_states/decoder/layers_18/mlp/wi_0/kernel/v_col                             size 16384        shape (16384,)                                 partition spec None
I0512 23:45:21.815208 140143678494720 utils.py:1372] Variable param_states/decoder/layers_18/mlp/wi_0/kernel/v_row                             size 2048         shape (2048,)                                  partition spec None
I0512 23:45:21.815263 140143678494720 utils.py:1372] Variable param_states/decoder/layers_18/mlp/wi_1/kernel/m                                 size 1            shape (1,)                                     partition spec None
I0512 23:45:21.815308 140143678494720 utils.py:1372] Variable param_states/decoder/layers_18/mlp/wi_1/kernel/v                                 size 1            shape (1,)                                     partition spec None
I0512 23:45:21.815350 140143678494720 utils.py:1372] Variable param_states/decoder/layers_18/mlp/wi_1/kernel/v_col                             size 8192         shape (8192,)                                  partition spec None
I0512 23:45:21.815389 140143678494720 utils.py:1372] Variable param_states/decoder/layers_18/mlp/wi_1/kernel/v_row                             size 2048         shape (2048,)                                  partition spec None
I0512 23:45:21.815428 140143678494720 utils.py:1372] Variable param_states/decoder/layers_18/mlp/wo/kernel/m                                   size 1            shape (1,)                                     partition spec None
I0512 23:45:21.815468 140143678494720 utils.py:1372] Variable param_states/decoder/layers_18/mlp/wo/kernel/v                                   size 1            shape (1,)                                     partition spec None
I0512 23:45:21.815506 140143678494720 utils.py:1372] Variable param_states/decoder/layers_18/mlp/wo/kernel/v_col                               size 8192         shape (8192,)                                  partition spec None
I0512 23:45:21.815551 140143678494720 utils.py:1372] Variable param_states/decoder/layers_18/mlp/wo/kernel/v_row                               size 2048         shape (2048,)                                  partition spec None
I0512 23:45:21.815592 140143678494720 utils.py:1372] Variable param_states/decoder/layers_18/pre_mlp_layer_norm/scale/m                        size 1            shape (1,)                                     partition spec None
I0512 23:45:21.815634 140143678494720 utils.py:1372] Variable param_states/decoder/layers_18/pre_mlp_layer_norm/scale/v                        size 2048         shape (embed=2048)                             partition spec ('model',)
I0512 23:45:21.815675 140143678494720 utils.py:1372] Variable param_states/decoder/layers_18/pre_mlp_layer_norm/scale/v_col                    size 1            shape (1,)                                     partition spec None
I0512 23:45:21.815714 140143678494720 utils.py:1372] Variable param_states/decoder/layers_18/pre_mlp_layer_norm/scale/v_row                    size 1            shape (1,)                                     partition spec None
I0512 23:45:21.815753 140143678494720 utils.py:1372] Variable param_states/decoder/layers_18/pre_self_attention_layer_norm/scale/m             size 1            shape (1,)                                     partition spec None
I0512 23:45:21.815795 140143678494720 utils.py:1372] Variable param_states/decoder/layers_18/pre_self_attention_layer_norm/scale/v             size 2048         shape (embed=2048)                             partition spec ('model',)
I0512 23:45:21.815835 140143678494720 utils.py:1372] Variable param_states/decoder/layers_18/pre_self_attention_layer_norm/scale/v_col         size 1            shape (1,)                                     partition spec None
I0512 23:45:21.815874 140143678494720 utils.py:1372] Variable param_states/decoder/layers_18/pre_self_attention_layer_norm/scale/v_row         size 1            shape (1,)                                     partition spec None
I0512 23:45:21.815913 140143678494720 utils.py:1372] Variable param_states/decoder/layers_18/self_attention/key/kernel/m                       size 1            shape (1,)                                     partition spec None
I0512 23:45:21.815952 140143678494720 utils.py:1372] Variable param_states/decoder/layers_18/self_attention/key/kernel/v                       size 1            shape (1,)                                     partition spec None
I0512 23:45:21.815991 140143678494720 utils.py:1372] Variable param_states/decoder/layers_18/self_attention/key/kernel/v_col                   size 3072         shape (3072,)                                  partition spec None
I0512 23:45:21.816032 140143678494720 utils.py:1372] Variable param_states/decoder/layers_18/self_attention/key/kernel/v_row                   size 2048         shape (2048,)                                  partition spec None
I0512 23:45:21.816071 140143678494720 utils.py:1372] Variable param_states/decoder/layers_18/self_attention/out/kernel/m                       size 1            shape (1,)                                     partition spec None
I0512 23:45:21.816111 140143678494720 utils.py:1372] Variable param_states/decoder/layers_18/self_attention/out/kernel/v                       size 1            shape (1,)                                     partition spec None
I0512 23:45:21.816150 140143678494720 utils.py:1372] Variable param_states/decoder/layers_18/self_attention/out/kernel/v_col                   size 3072         shape (3072,)                                  partition spec None
I0512 23:45:21.816189 140143678494720 utils.py:1372] Variable param_states/decoder/layers_18/self_attention/out/kernel/v_row                   size 2048         shape (2048,)                                  partition spec None
I0512 23:45:21.816227 140143678494720 utils.py:1372] Variable param_states/decoder/layers_18/self_attention/query/kernel/m                     size 1            shape (1,)                                     partition spec None
I0512 23:45:21.816266 140143678494720 utils.py:1372] Variable param_states/decoder/layers_18/self_attention/query/kernel/v                     size 1            shape (1,)                                     partition spec None
I0512 23:45:21.816307 140143678494720 utils.py:1372] Variable param_states/decoder/layers_18/self_attention/query/kernel/v_col                 size 3072         shape (3072,)                                  partition spec None
I0512 23:45:21.816347 140143678494720 utils.py:1372] Variable param_states/decoder/layers_18/self_attention/query/kernel/v_row                 size 2048         shape (2048,)                                  partition spec None
I0512 23:45:21.816387 140143678494720 utils.py:1372] Variable param_states/decoder/layers_18/self_attention/value/kernel/m                     size 1            shape (1,)                                     partition spec None
I0512 23:45:21.816426 140143678494720 utils.py:1372] Variable param_states/decoder/layers_18/self_attention/value/kernel/v                     size 1            shape (1,)                                     partition spec None
I0512 23:45:21.816465 140143678494720 utils.py:1372] Variable param_states/decoder/layers_18/self_attention/value/kernel/v_col                 size 3072         shape (3072,)                                  partition spec None
I0512 23:45:21.816504 140143678494720 utils.py:1372] Variable param_states/decoder/layers_18/self_attention/value/kernel/v_row                 size 2048         shape (2048,)                                  partition spec None
I0512 23:45:21.816570 140143678494720 utils.py:1372] Variable param_states/decoder/layers_19/mlp/wi_0/kernel/m                                 size 1            shape (1,)                                     partition spec None
I0512 23:45:21.816617 140143678494720 utils.py:1372] Variable param_states/decoder/layers_19/mlp/wi_0/kernel/v                                 size 1            shape (1,)                                     partition spec None
I0512 23:45:21.816657 140143678494720 utils.py:1372] Variable param_states/decoder/layers_19/mlp/wi_0/kernel/v_col                             size 16384        shape (16384,)                                 partition spec None
I0512 23:45:21.816697 140143678494720 utils.py:1372] Variable param_states/decoder/layers_19/mlp/wi_0/kernel/v_row                             size 2048         shape (2048,)                                  partition spec None
I0512 23:45:21.816735 140143678494720 utils.py:1372] Variable param_states/decoder/layers_19/mlp/wi_1/kernel/m                                 size 1            shape (1,)                                     partition spec None
I0512 23:45:21.816775 140143678494720 utils.py:1372] Variable param_states/decoder/layers_19/mlp/wi_1/kernel/v                                 size 1            shape (1,)                                     partition spec None
I0512 23:45:21.816816 140143678494720 utils.py:1372] Variable param_states/decoder/layers_19/mlp/wi_1/kernel/v_col                             size 8192         shape (8192,)                                  partition spec None
I0512 23:45:21.816856 140143678494720 utils.py:1372] Variable param_states/decoder/layers_19/mlp/wi_1/kernel/v_row                             size 2048         shape (2048,)                                  partition spec None
I0512 23:45:21.816895 140143678494720 utils.py:1372] Variable param_states/decoder/layers_19/mlp/wo/kernel/m                                   size 1            shape (1,)                                     partition spec None
I0512 23:45:21.816935 140143678494720 utils.py:1372] Variable param_states/decoder/layers_19/mlp/wo/kernel/v                                   size 1            shape (1,)                                     partition spec None
I0512 23:45:21.816975 140143678494720 utils.py:1372] Variable param_states/decoder/layers_19/mlp/wo/kernel/v_col                               size 8192         shape (8192,)                                  partition spec None
I0512 23:45:21.817015 140143678494720 utils.py:1372] Variable param_states/decoder/layers_19/mlp/wo/kernel/v_row                               size 2048         shape (2048,)                                  partition spec None
I0512 23:45:21.817054 140143678494720 utils.py:1372] Variable param_states/decoder/layers_19/pre_mlp_layer_norm/scale/m                        size 1            shape (1,)                                     partition spec None
I0512 23:45:21.817097 140143678494720 utils.py:1372] Variable param_states/decoder/layers_19/pre_mlp_layer_norm/scale/v                        size 2048         shape (embed=2048)                             partition spec ('model',)
I0512 23:45:21.817138 140143678494720 utils.py:1372] Variable param_states/decoder/layers_19/pre_mlp_layer_norm/scale/v_col                    size 1            shape (1,)                                     partition spec None
I0512 23:45:21.817193 140143678494720 utils.py:1372] Variable param_states/decoder/layers_19/pre_mlp_layer_norm/scale/v_row                    size 1            shape (1,)                                     partition spec None
I0512 23:45:21.817236 140143678494720 utils.py:1372] Variable param_states/decoder/layers_19/pre_self_attention_layer_norm/scale/m             size 1            shape (1,)                                     partition spec None
I0512 23:45:21.817278 140143678494720 utils.py:1372] Variable param_states/decoder/layers_19/pre_self_attention_layer_norm/scale/v             size 2048         shape (embed=2048)                             partition spec ('model',)
I0512 23:45:21.817322 140143678494720 utils.py:1372] Variable param_states/decoder/layers_19/pre_self_attention_layer_norm/scale/v_col         size 1            shape (1,)                                     partition spec None
I0512 23:45:21.817363 140143678494720 utils.py:1372] Variable param_states/decoder/layers_19/pre_self_attention_layer_norm/scale/v_row         size 1            shape (1,)                                     partition spec None
I0512 23:45:21.817402 140143678494720 utils.py:1372] Variable param_states/decoder/layers_19/self_attention/key/kernel/m                       size 1            shape (1,)                                     partition spec None
I0512 23:45:21.817442 140143678494720 utils.py:1372] Variable param_states/decoder/layers_19/self_attention/key/kernel/v                       size 1            shape (1,)                                     partition spec None
I0512 23:45:21.817481 140143678494720 utils.py:1372] Variable param_states/decoder/layers_19/self_attention/key/kernel/v_col                   size 3072         shape (3072,)                                  partition spec None
I0512 23:45:21.817519 140143678494720 utils.py:1372] Variable param_states/decoder/layers_19/self_attention/key/kernel/v_row                   size 2048         shape (2048,)                                  partition spec None
I0512 23:45:21.817565 140143678494720 utils.py:1372] Variable param_states/decoder/layers_19/self_attention/out/kernel/m                       size 1            shape (1,)                                     partition spec None
I0512 23:45:21.817605 140143678494720 utils.py:1372] Variable param_states/decoder/layers_19/self_attention/out/kernel/v                       size 1            shape (1,)                                     partition spec None
I0512 23:45:21.817644 140143678494720 utils.py:1372] Variable param_states/decoder/layers_19/self_attention/out/kernel/v_col                   size 3072         shape (3072,)                                  partition spec None
I0512 23:45:21.817683 140143678494720 utils.py:1372] Variable param_states/decoder/layers_19/self_attention/out/kernel/v_row                   size 2048         shape (2048,)                                  partition spec None
I0512 23:45:21.817722 140143678494720 utils.py:1372] Variable param_states/decoder/layers_19/self_attention/query/kernel/m                     size 1            shape (1,)                                     partition spec None
I0512 23:45:21.817761 140143678494720 utils.py:1372] Variable param_states/decoder/layers_19/self_attention/query/kernel/v                     size 1            shape (1,)                                     partition spec None
I0512 23:45:21.817800 140143678494720 utils.py:1372] Variable param_states/decoder/layers_19/self_attention/query/kernel/v_col                 size 3072         shape (3072,)                                  partition spec None
I0512 23:45:21.817839 140143678494720 utils.py:1372] Variable param_states/decoder/layers_19/self_attention/query/kernel/v_row                 size 2048         shape (2048,)                                  partition spec None
I0512 23:45:21.817878 140143678494720 utils.py:1372] Variable param_states/decoder/layers_19/self_attention/value/kernel/m                     size 1            shape (1,)                                     partition spec None
I0512 23:45:21.817918 140143678494720 utils.py:1372] Variable param_states/decoder/layers_19/self_attention/value/kernel/v                     size 1            shape (1,)                                     partition spec None
I0512 23:45:21.817956 140143678494720 utils.py:1372] Variable param_states/decoder/layers_19/self_attention/value/kernel/v_col                 size 3072         shape (3072,)                                  partition spec None
I0512 23:45:21.817995 140143678494720 utils.py:1372] Variable param_states/decoder/layers_19/self_attention/value/kernel/v_row                 size 2048         shape (2048,)                                  partition spec None
I0512 23:45:21.818033 140143678494720 utils.py:1372] Variable param_states/decoder/layers_2/mlp/wi_0/kernel/m                                  size 1            shape (1,)                                     partition spec None
I0512 23:45:21.818073 140143678494720 utils.py:1372] Variable param_states/decoder/layers_2/mlp/wi_0/kernel/v                                  size 1            shape (1,)                                     partition spec None
I0512 23:45:21.818112 140143678494720 utils.py:1372] Variable param_states/decoder/layers_2/mlp/wi_0/kernel/v_col                              size 16384        shape (16384,)                                 partition spec None
I0512 23:45:21.818151 140143678494720 utils.py:1372] Variable param_states/decoder/layers_2/mlp/wi_0/kernel/v_row                              size 2048         shape (2048,)                                  partition spec None
I0512 23:45:21.818189 140143678494720 utils.py:1372] Variable param_states/decoder/layers_2/mlp/wi_1/kernel/m                                  size 1            shape (1,)                                     partition spec None
I0512 23:45:21.818228 140143678494720 utils.py:1372] Variable param_states/decoder/layers_2/mlp/wi_1/kernel/v                                  size 1            shape (1,)                                     partition spec None
I0512 23:45:21.818269 140143678494720 utils.py:1372] Variable param_states/decoder/layers_2/mlp/wi_1/kernel/v_col                              size 8192         shape (8192,)                                  partition spec None
I0512 23:45:21.818310 140143678494720 utils.py:1372] Variable param_states/decoder/layers_2/mlp/wi_1/kernel/v_row                              size 2048         shape (2048,)                                  partition spec None
I0512 23:45:21.818350 140143678494720 utils.py:1372] Variable param_states/decoder/layers_2/mlp/wo/kernel/m                                    size 1            shape (1,)                                     partition spec None
I0512 23:45:21.818390 140143678494720 utils.py:1372] Variable param_states/decoder/layers_2/mlp/wo/kernel/v                                    size 1            shape (1,)                                     partition spec None
I0512 23:45:21.818429 140143678494720 utils.py:1372] Variable param_states/decoder/layers_2/mlp/wo/kernel/v_col                                size 8192         shape (8192,)                                  partition spec None
I0512 23:45:21.818468 140143678494720 utils.py:1372] Variable param_states/decoder/layers_2/mlp/wo/kernel/v_row                                size 2048         shape (2048,)                                  partition spec None
I0512 23:45:21.818506 140143678494720 utils.py:1372] Variable param_states/decoder/layers_2/pre_mlp_layer_norm/scale/m                         size 1            shape (1,)                                     partition spec None
I0512 23:45:21.818553 140143678494720 utils.py:1372] Variable param_states/decoder/layers_2/pre_mlp_layer_norm/scale/v                         size 2048         shape (embed=2048)                             partition spec ('model',)
I0512 23:45:21.818596 140143678494720 utils.py:1372] Variable param_states/decoder/layers_2/pre_mlp_layer_norm/scale/v_col                     size 1            shape (1,)                                     partition spec None
I0512 23:45:21.818636 140143678494720 utils.py:1372] Variable param_states/decoder/layers_2/pre_mlp_layer_norm/scale/v_row                     size 1            shape (1,)                                     partition spec None
I0512 23:45:21.818675 140143678494720 utils.py:1372] Variable param_states/decoder/layers_2/pre_self_attention_layer_norm/scale/m              size 1            shape (1,)                                     partition spec None
I0512 23:45:21.818716 140143678494720 utils.py:1372] Variable param_states/decoder/layers_2/pre_self_attention_layer_norm/scale/v              size 2048         shape (embed=2048)                             partition spec ('model',)
I0512 23:45:21.818756 140143678494720 utils.py:1372] Variable param_states/decoder/layers_2/pre_self_attention_layer_norm/scale/v_col          size 1            shape (1,)                                     partition spec None
I0512 23:45:21.818796 140143678494720 utils.py:1372] Variable param_states/decoder/layers_2/pre_self_attention_layer_norm/scale/v_row          size 1            shape (1,)                                     partition spec None
I0512 23:45:21.818843 140143678494720 utils.py:1372] Variable param_states/decoder/layers_2/self_attention/key/kernel/m                        size 1            shape (1,)                                     partition spec None
I0512 23:45:21.818884 140143678494720 utils.py:1372] Variable param_states/decoder/layers_2/self_attention/key/kernel/v                        size 1            shape (1,)                                     partition spec None
I0512 23:45:21.818924 140143678494720 utils.py:1372] Variable param_states/decoder/layers_2/self_attention/key/kernel/v_col                    size 3072         shape (3072,)                                  partition spec None
I0512 23:45:21.818963 140143678494720 utils.py:1372] Variable param_states/decoder/layers_2/self_attention/key/kernel/v_row                    size 2048         shape (2048,)                                  partition spec None
I0512 23:45:21.819002 140143678494720 utils.py:1372] Variable param_states/decoder/layers_2/self_attention/out/kernel/m                        size 1            shape (1,)                                     partition spec None
I0512 23:45:21.819041 140143678494720 utils.py:1372] Variable param_states/decoder/layers_2/self_attention/out/kernel/v                        size 1            shape (1,)                                     partition spec None
I0512 23:45:21.819097 140143678494720 utils.py:1372] Variable param_states/decoder/layers_2/self_attention/out/kernel/v_col                    size 3072         shape (3072,)                                  partition spec None
I0512 23:45:21.819139 140143678494720 utils.py:1372] Variable param_states/decoder/layers_2/self_attention/out/kernel/v_row                    size 2048         shape (2048,)                                  partition spec None
I0512 23:45:21.819179 140143678494720 utils.py:1372] Variable param_states/decoder/layers_2/self_attention/query/kernel/m                      size 1            shape (1,)                                     partition spec None
I0512 23:45:21.819217 140143678494720 utils.py:1372] Variable param_states/decoder/layers_2/self_attention/query/kernel/v                      size 1            shape (1,)                                     partition spec None
I0512 23:45:21.819256 140143678494720 utils.py:1372] Variable param_states/decoder/layers_2/self_attention/query/kernel/v_col                  size 3072         shape (3072,)                                  partition spec None
I0512 23:45:21.819296 140143678494720 utils.py:1372] Variable param_states/decoder/layers_2/self_attention/query/kernel/v_row                  size 2048         shape (2048,)                                  partition spec None
I0512 23:45:21.819337 140143678494720 utils.py:1372] Variable param_states/decoder/layers_2/self_attention/value/kernel/m                      size 1            shape (1,)                                     partition spec None
I0512 23:45:21.819376 140143678494720 utils.py:1372] Variable param_states/decoder/layers_2/self_attention/value/kernel/v                      size 1            shape (1,)                                     partition spec None
I0512 23:45:21.819415 140143678494720 utils.py:1372] Variable param_states/decoder/layers_2/self_attention/value/kernel/v_col                  size 3072         shape (3072,)                                  partition spec None
I0512 23:45:21.819454 140143678494720 utils.py:1372] Variable param_states/decoder/layers_2/self_attention/value/kernel/v_row                  size 2048         shape (2048,)                                  partition spec None
I0512 23:45:21.819493 140143678494720 utils.py:1372] Variable param_states/decoder/layers_20/mlp/wi_0/kernel/m                                 size 1            shape (1,)                                     partition spec None
I0512 23:45:21.819533 140143678494720 utils.py:1372] Variable param_states/decoder/layers_20/mlp/wi_0/kernel/v                                 size 1            shape (1,)                                     partition spec None
I0512 23:45:21.819581 140143678494720 utils.py:1372] Variable param_states/decoder/layers_20/mlp/wi_0/kernel/v_col                             size 16384        shape (16384,)                                 partition spec None
I0512 23:45:21.819620 140143678494720 utils.py:1372] Variable param_states/decoder/layers_20/mlp/wi_0/kernel/v_row                             size 2048         shape (2048,)                                  partition spec None
I0512 23:45:21.819659 140143678494720 utils.py:1372] Variable param_states/decoder/layers_20/mlp/wi_1/kernel/m                                 size 1            shape (1,)                                     partition spec None
I0512 23:45:21.819699 140143678494720 utils.py:1372] Variable param_states/decoder/layers_20/mlp/wi_1/kernel/v                                 size 1            shape (1,)                                     partition spec None
I0512 23:45:21.819737 140143678494720 utils.py:1372] Variable param_states/decoder/layers_20/mlp/wi_1/kernel/v_col                             size 8192         shape (8192,)                                  partition spec None
I0512 23:45:21.819777 140143678494720 utils.py:1372] Variable param_states/decoder/layers_20/mlp/wi_1/kernel/v_row                             size 2048         shape (2048,)                                  partition spec None
I0512 23:45:21.819815 140143678494720 utils.py:1372] Variable param_states/decoder/layers_20/mlp/wo/kernel/m                                   size 1            shape (1,)                                     partition spec None
I0512 23:45:21.819854 140143678494720 utils.py:1372] Variable param_states/decoder/layers_20/mlp/wo/kernel/v                                   size 1            shape (1,)                                     partition spec None
I0512 23:45:21.819895 140143678494720 utils.py:1372] Variable param_states/decoder/layers_20/mlp/wo/kernel/v_col                               size 8192         shape (8192,)                                  partition spec None
I0512 23:45:21.819935 140143678494720 utils.py:1372] Variable param_states/decoder/layers_20/mlp/wo/kernel/v_row                               size 2048         shape (2048,)                                  partition spec None
I0512 23:45:21.819973 140143678494720 utils.py:1372] Variable param_states/decoder/layers_20/pre_mlp_layer_norm/scale/m                        size 1            shape (1,)                                     partition spec None
I0512 23:45:21.820015 140143678494720 utils.py:1372] Variable param_states/decoder/layers_20/pre_mlp_layer_norm/scale/v                        size 2048         shape (embed=2048)                             partition spec ('model',)
I0512 23:45:21.820057 140143678494720 utils.py:1372] Variable param_states/decoder/layers_20/pre_mlp_layer_norm/scale/v_col                    size 1            shape (1,)                                     partition spec None
I0512 23:45:21.820097 140143678494720 utils.py:1372] Variable param_states/decoder/layers_20/pre_mlp_layer_norm/scale/v_row                    size 1            shape (1,)                                     partition spec None
I0512 23:45:21.820136 140143678494720 utils.py:1372] Variable param_states/decoder/layers_20/pre_self_attention_layer_norm/scale/m             size 1            shape (1,)                                     partition spec None
I0512 23:45:21.820176 140143678494720 utils.py:1372] Variable param_states/decoder/layers_20/pre_self_attention_layer_norm/scale/v             size 2048         shape (embed=2048)                             partition spec ('model',)
I0512 23:45:21.820217 140143678494720 utils.py:1372] Variable param_states/decoder/layers_20/pre_self_attention_layer_norm/scale/v_col         size 1            shape (1,)                                     partition spec None
I0512 23:45:21.820256 140143678494720 utils.py:1372] Variable param_states/decoder/layers_20/pre_self_attention_layer_norm/scale/v_row         size 1            shape (1,)                                     partition spec None
I0512 23:45:21.820296 140143678494720 utils.py:1372] Variable param_states/decoder/layers_20/self_attention/key/kernel/m                       size 1            shape (1,)                                     partition spec None
I0512 23:45:21.820337 140143678494720 utils.py:1372] Variable param_states/decoder/layers_20/self_attention/key/kernel/v                       size 1            shape (1,)                                     partition spec None
I0512 23:45:21.820376 140143678494720 utils.py:1372] Variable param_states/decoder/layers_20/self_attention/key/kernel/v_col                   size 3072         shape (3072,)                                  partition spec None
I0512 23:45:21.820415 140143678494720 utils.py:1372] Variable param_states/decoder/layers_20/self_attention/key/kernel/v_row                   size 2048         shape (2048,)                                  partition spec None
I0512 23:45:21.820454 140143678494720 utils.py:1372] Variable param_states/decoder/layers_20/self_attention/out/kernel/m                       size 1            shape (1,)                                     partition spec None
I0512 23:45:21.820493 140143678494720 utils.py:1372] Variable param_states/decoder/layers_20/self_attention/out/kernel/v                       size 1            shape (1,)                                     partition spec None
I0512 23:45:21.820553 140143678494720 utils.py:1372] Variable param_states/decoder/layers_20/self_attention/out/kernel/v_col                   size 3072         shape (3072,)                                  partition spec None
I0512 23:45:21.820601 140143678494720 utils.py:1372] Variable param_states/decoder/layers_20/self_attention/out/kernel/v_row                   size 2048         shape (2048,)                                  partition spec None
I0512 23:45:21.820642 140143678494720 utils.py:1372] Variable param_states/decoder/layers_20/self_attention/query/kernel/m                     size 1            shape (1,)                                     partition spec None
I0512 23:45:21.820682 140143678494720 utils.py:1372] Variable param_states/decoder/layers_20/self_attention/query/kernel/v                     size 1            shape (1,)                                     partition spec None
I0512 23:45:21.820721 140143678494720 utils.py:1372] Variable param_states/decoder/layers_20/self_attention/query/kernel/v_col                 size 3072         shape (3072,)                                  partition spec None
I0512 23:45:21.820760 140143678494720 utils.py:1372] Variable param_states/decoder/layers_20/self_attention/query/kernel/v_row                 size 2048         shape (2048,)                                  partition spec None
I0512 23:45:21.820798 140143678494720 utils.py:1372] Variable param_states/decoder/layers_20/self_attention/value/kernel/m                     size 1            shape (1,)                                     partition spec None
I0512 23:45:21.820837 140143678494720 utils.py:1372] Variable param_states/decoder/layers_20/self_attention/value/kernel/v                     size 1            shape (1,)                                     partition spec None
I0512 23:45:21.820877 140143678494720 utils.py:1372] Variable param_states/decoder/layers_20/self_attention/value/kernel/v_col                 size 3072         shape (3072,)                                  partition spec None
I0512 23:45:21.820916 140143678494720 utils.py:1372] Variable param_states/decoder/layers_20/self_attention/value/kernel/v_row                 size 2048         shape (2048,)                                  partition spec None
I0512 23:45:21.820955 140143678494720 utils.py:1372] Variable param_states/decoder/layers_21/mlp/wi_0/kernel/m                                 size 1            shape (1,)                                     partition spec None
I0512 23:45:21.821010 140143678494720 utils.py:1372] Variable param_states/decoder/layers_21/mlp/wi_0/kernel/v                                 size 1            shape (1,)                                     partition spec None
I0512 23:45:21.821053 140143678494720 utils.py:1372] Variable param_states/decoder/layers_21/mlp/wi_0/kernel/v_col                             size 16384        shape (16384,)                                 partition spec None
I0512 23:45:21.821092 140143678494720 utils.py:1372] Variable param_states/decoder/layers_21/mlp/wi_0/kernel/v_row                             size 2048         shape (2048,)                                  partition spec None
I0512 23:45:21.821130 140143678494720 utils.py:1372] Variable param_states/decoder/layers_21/mlp/wi_1/kernel/m                                 size 1            shape (1,)                                     partition spec None
I0512 23:45:21.821169 140143678494720 utils.py:1372] Variable param_states/decoder/layers_21/mlp/wi_1/kernel/v                                 size 1            shape (1,)                                     partition spec None
I0512 23:45:21.821208 140143678494720 utils.py:1372] Variable param_states/decoder/layers_21/mlp/wi_1/kernel/v_col                             size 8192         shape (8192,)                                  partition spec None
I0512 23:45:21.821246 140143678494720 utils.py:1372] Variable param_states/decoder/layers_21/mlp/wi_1/kernel/v_row                             size 2048         shape (2048,)                                  partition spec None
I0512 23:45:21.821285 140143678494720 utils.py:1372] Variable param_states/decoder/layers_21/mlp/wo/kernel/m                                   size 1            shape (1,)                                     partition spec None
I0512 23:45:21.821327 140143678494720 utils.py:1372] Variable param_states/decoder/layers_21/mlp/wo/kernel/v                                   size 1            shape (1,)                                     partition spec None
I0512 23:45:21.821367 140143678494720 utils.py:1372] Variable param_states/decoder/layers_21/mlp/wo/kernel/v_col                               size 8192         shape (8192,)                                  partition spec None
I0512 23:45:21.821405 140143678494720 utils.py:1372] Variable param_states/decoder/layers_21/mlp/wo/kernel/v_row                               size 2048         shape (2048,)                                  partition spec None
I0512 23:45:21.821443 140143678494720 utils.py:1372] Variable param_states/decoder/layers_21/pre_mlp_layer_norm/scale/m                        size 1            shape (1,)                                     partition spec None
I0512 23:45:21.821485 140143678494720 utils.py:1372] Variable param_states/decoder/layers_21/pre_mlp_layer_norm/scale/v                        size 2048         shape (embed=2048)                             partition spec ('model',)
I0512 23:45:21.821526 140143678494720 utils.py:1372] Variable param_states/decoder/layers_21/pre_mlp_layer_norm/scale/v_col                    size 1            shape (1,)                                     partition spec None
I0512 23:45:21.821573 140143678494720 utils.py:1372] Variable param_states/decoder/layers_21/pre_mlp_layer_norm/scale/v_row                    size 1            shape (1,)                                     partition spec None
I0512 23:45:21.821613 140143678494720 utils.py:1372] Variable param_states/decoder/layers_21/pre_self_attention_layer_norm/scale/m             size 1            shape (1,)                                     partition spec None
I0512 23:45:21.821654 140143678494720 utils.py:1372] Variable param_states/decoder/layers_21/pre_self_attention_layer_norm/scale/v             size 2048         shape (embed=2048)                             partition spec ('model',)
I0512 23:45:21.821695 140143678494720 utils.py:1372] Variable param_states/decoder/layers_21/pre_self_attention_layer_norm/scale/v_col         size 1            shape (1,)                                     partition spec None
I0512 23:45:21.821734 140143678494720 utils.py:1372] Variable param_states/decoder/layers_21/pre_self_attention_layer_norm/scale/v_row         size 1            shape (1,)                                     partition spec None
I0512 23:45:21.821772 140143678494720 utils.py:1372] Variable param_states/decoder/layers_21/self_attention/key/kernel/m                       size 1            shape (1,)                                     partition spec None
I0512 23:45:21.821811 140143678494720 utils.py:1372] Variable param_states/decoder/layers_21/self_attention/key/kernel/v                       size 1            shape (1,)                                     partition spec None
I0512 23:45:21.821850 140143678494720 utils.py:1372] Variable param_states/decoder/layers_21/self_attention/key/kernel/v_col                   size 3072         shape (3072,)                                  partition spec None
I0512 23:45:21.821889 140143678494720 utils.py:1372] Variable param_states/decoder/layers_21/self_attention/key/kernel/v_row                   size 2048         shape (2048,)                                  partition spec None
I0512 23:45:21.821928 140143678494720 utils.py:1372] Variable param_states/decoder/layers_21/self_attention/out/kernel/m                       size 1            shape (1,)                                     partition spec None
I0512 23:45:21.821966 140143678494720 utils.py:1372] Variable param_states/decoder/layers_21/self_attention/out/kernel/v                       size 1            shape (1,)                                     partition spec None
I0512 23:45:21.822004 140143678494720 utils.py:1372] Variable param_states/decoder/layers_21/self_attention/out/kernel/v_col                   size 3072         shape (3072,)                                  partition spec None
I0512 23:45:21.822043 140143678494720 utils.py:1372] Variable param_states/decoder/layers_21/self_attention/out/kernel/v_row                   size 2048         shape (2048,)                                  partition spec None
I0512 23:45:21.822081 140143678494720 utils.py:1372] Variable param_states/decoder/layers_21/self_attention/query/kernel/m                     size 1            shape (1,)                                     partition spec None
I0512 23:45:21.822120 140143678494720 utils.py:1372] Variable param_states/decoder/layers_21/self_attention/query/kernel/v                     size 1            shape (1,)                                     partition spec None
I0512 23:45:21.822158 140143678494720 utils.py:1372] Variable param_states/decoder/layers_21/self_attention/query/kernel/v_col                 size 3072         shape (3072,)                                  partition spec None
I0512 23:45:21.822197 140143678494720 utils.py:1372] Variable param_states/decoder/layers_21/self_attention/query/kernel/v_row                 size 2048         shape (2048,)                                  partition spec None
I0512 23:45:21.822235 140143678494720 utils.py:1372] Variable param_states/decoder/layers_21/self_attention/value/kernel/m                     size 1            shape (1,)                                     partition spec None
I0512 23:45:21.822273 140143678494720 utils.py:1372] Variable param_states/decoder/layers_21/self_attention/value/kernel/v                     size 1            shape (1,)                                     partition spec None
I0512 23:45:21.822314 140143678494720 utils.py:1372] Variable param_states/decoder/layers_21/self_attention/value/kernel/v_col                 size 3072         shape (3072,)                                  partition spec None
I0512 23:45:21.822354 140143678494720 utils.py:1372] Variable param_states/decoder/layers_21/self_attention/value/kernel/v_row                 size 2048         shape (2048,)                                  partition spec None
I0512 23:45:21.822392 140143678494720 utils.py:1372] Variable param_states/decoder/layers_22/mlp/wi_0/kernel/m                                 size 1            shape (1,)                                     partition spec None
I0512 23:45:21.822432 140143678494720 utils.py:1372] Variable param_states/decoder/layers_22/mlp/wi_0/kernel/v                                 size 1            shape (1,)                                     partition spec None
I0512 23:45:21.822470 140143678494720 utils.py:1372] Variable param_states/decoder/layers_22/mlp/wi_0/kernel/v_col                             size 16384        shape (16384,)                                 partition spec None
I0512 23:45:21.822509 140143678494720 utils.py:1372] Variable param_states/decoder/layers_22/mlp/wi_0/kernel/v_row                             size 2048         shape (2048,)                                  partition spec None
I0512 23:45:21.822554 140143678494720 utils.py:1372] Variable param_states/decoder/layers_22/mlp/wi_1/kernel/m                                 size 1            shape (1,)                                     partition spec None
I0512 23:45:21.822595 140143678494720 utils.py:1372] Variable param_states/decoder/layers_22/mlp/wi_1/kernel/v                                 size 1            shape (1,)                                     partition spec None
I0512 23:45:21.822633 140143678494720 utils.py:1372] Variable param_states/decoder/layers_22/mlp/wi_1/kernel/v_col                             size 8192         shape (8192,)                                  partition spec None
I0512 23:45:21.822672 140143678494720 utils.py:1372] Variable param_states/decoder/layers_22/mlp/wi_1/kernel/v_row                             size 2048         shape (2048,)                                  partition spec None
I0512 23:45:21.822710 140143678494720 utils.py:1372] Variable param_states/decoder/layers_22/mlp/wo/kernel/m                                   size 1            shape (1,)                                     partition spec None
I0512 23:45:21.822749 140143678494720 utils.py:1372] Variable param_states/decoder/layers_22/mlp/wo/kernel/v                                   size 1            shape (1,)                                     partition spec None
I0512 23:45:21.822788 140143678494720 utils.py:1372] Variable param_states/decoder/layers_22/mlp/wo/kernel/v_col                               size 8192         shape (8192,)                                  partition spec None
I0512 23:45:21.822826 140143678494720 utils.py:1372] Variable param_states/decoder/layers_22/mlp/wo/kernel/v_row                               size 2048         shape (2048,)                                  partition spec None
I0512 23:45:21.822879 140143678494720 utils.py:1372] Variable param_states/decoder/layers_22/pre_mlp_layer_norm/scale/m                        size 1            shape (1,)                                     partition spec None
I0512 23:45:21.822924 140143678494720 utils.py:1372] Variable param_states/decoder/layers_22/pre_mlp_layer_norm/scale/v                        size 2048         shape (embed=2048)                             partition spec ('model',)
I0512 23:45:21.822965 140143678494720 utils.py:1372] Variable param_states/decoder/layers_22/pre_mlp_layer_norm/scale/v_col                    size 1            shape (1,)                                     partition spec None
I0512 23:45:21.823005 140143678494720 utils.py:1372] Variable param_states/decoder/layers_22/pre_mlp_layer_norm/scale/v_row                    size 1            shape (1,)                                     partition spec None
I0512 23:45:21.823043 140143678494720 utils.py:1372] Variable param_states/decoder/layers_22/pre_self_attention_layer_norm/scale/m             size 1            shape (1,)                                     partition spec None
I0512 23:45:21.823084 140143678494720 utils.py:1372] Variable param_states/decoder/layers_22/pre_self_attention_layer_norm/scale/v             size 2048         shape (embed=2048)                             partition spec ('model',)
I0512 23:45:21.823124 140143678494720 utils.py:1372] Variable param_states/decoder/layers_22/pre_self_attention_layer_norm/scale/v_col         size 1            shape (1,)                                     partition spec None
I0512 23:45:21.823163 140143678494720 utils.py:1372] Variable param_states/decoder/layers_22/pre_self_attention_layer_norm/scale/v_row         size 1            shape (1,)                                     partition spec None
I0512 23:45:21.823202 140143678494720 utils.py:1372] Variable param_states/decoder/layers_22/self_attention/key/kernel/m                       size 1            shape (1,)                                     partition spec None
I0512 23:45:21.823240 140143678494720 utils.py:1372] Variable param_states/decoder/layers_22/self_attention/key/kernel/v                       size 1            shape (1,)                                     partition spec None
I0512 23:45:21.823279 140143678494720 utils.py:1372] Variable param_states/decoder/layers_22/self_attention/key/kernel/v_col                   size 3072         shape (3072,)                                  partition spec None
I0512 23:45:21.823320 140143678494720 utils.py:1372] Variable param_states/decoder/layers_22/self_attention/key/kernel/v_row                   size 2048         shape (2048,)                                  partition spec None
I0512 23:45:21.823359 140143678494720 utils.py:1372] Variable param_states/decoder/layers_22/self_attention/out/kernel/m                       size 1            shape (1,)                                     partition spec None
I0512 23:45:21.823398 140143678494720 utils.py:1372] Variable param_states/decoder/layers_22/self_attention/out/kernel/v                       size 1            shape (1,)                                     partition spec None
I0512 23:45:21.823436 140143678494720 utils.py:1372] Variable param_states/decoder/layers_22/self_attention/out/kernel/v_col                   size 3072         shape (3072,)                                  partition spec None
I0512 23:45:21.823476 140143678494720 utils.py:1372] Variable param_states/decoder/layers_22/self_attention/out/kernel/v_row                   size 2048         shape (2048,)                                  partition spec None
I0512 23:45:21.823514 140143678494720 utils.py:1372] Variable param_states/decoder/layers_22/self_attention/query/kernel/m                     size 1            shape (1,)                                     partition spec None
I0512 23:45:21.823559 140143678494720 utils.py:1372] Variable param_states/decoder/layers_22/self_attention/query/kernel/v                     size 1            shape (1,)                                     partition spec None
I0512 23:45:21.823601 140143678494720 utils.py:1372] Variable param_states/decoder/layers_22/self_attention/query/kernel/v_col                 size 3072         shape (3072,)                                  partition spec None
I0512 23:45:21.823640 140143678494720 utils.py:1372] Variable param_states/decoder/layers_22/self_attention/query/kernel/v_row                 size 2048         shape (2048,)                                  partition spec None
I0512 23:45:21.823678 140143678494720 utils.py:1372] Variable param_states/decoder/layers_22/self_attention/value/kernel/m                     size 1            shape (1,)                                     partition spec None
I0512 23:45:21.823717 140143678494720 utils.py:1372] Variable param_states/decoder/layers_22/self_attention/value/kernel/v                     size 1            shape (1,)                                     partition spec None
I0512 23:45:21.823756 140143678494720 utils.py:1372] Variable param_states/decoder/layers_22/self_attention/value/kernel/v_col                 size 3072         shape (3072,)                                  partition spec None
I0512 23:45:21.823796 140143678494720 utils.py:1372] Variable param_states/decoder/layers_22/self_attention/value/kernel/v_row                 size 2048         shape (2048,)                                  partition spec None
I0512 23:45:21.823835 140143678494720 utils.py:1372] Variable param_states/decoder/layers_23/extra_mlp/wi_0/kernel/m                           size 1            shape (1,)                                     partition spec None
I0512 23:45:21.823875 140143678494720 utils.py:1372] Variable param_states/decoder/layers_23/extra_mlp/wi_0/kernel/v                           size 1            shape (1,)                                     partition spec None
I0512 23:45:21.823915 140143678494720 utils.py:1372] Variable param_states/decoder/layers_23/extra_mlp/wi_0/kernel/v_col                       size 16384        shape (16384,)                                 partition spec None
I0512 23:45:21.823954 140143678494720 utils.py:1372] Variable param_states/decoder/layers_23/extra_mlp/wi_0/kernel/v_row                       size 2048         shape (2048,)                                  partition spec None
I0512 23:45:21.823993 140143678494720 utils.py:1372] Variable param_states/decoder/layers_23/extra_mlp/wi_1/kernel/m                           size 1            shape (1,)                                     partition spec None
I0512 23:45:21.824032 140143678494720 utils.py:1372] Variable param_states/decoder/layers_23/extra_mlp/wi_1/kernel/v                           size 1            shape (1,)                                     partition spec None
I0512 23:45:21.824071 140143678494720 utils.py:1372] Variable param_states/decoder/layers_23/extra_mlp/wi_1/kernel/v_col                       size 8192         shape (8192,)                                  partition spec None
I0512 23:45:21.824110 140143678494720 utils.py:1372] Variable param_states/decoder/layers_23/extra_mlp/wi_1/kernel/v_row                       size 2048         shape (2048,)                                  partition spec None
I0512 23:45:21.824149 140143678494720 utils.py:1372] Variable param_states/decoder/layers_23/extra_mlp/wo/kernel/m                             size 1            shape (1,)                                     partition spec None
I0512 23:45:21.824188 140143678494720 utils.py:1372] Variable param_states/decoder/layers_23/extra_mlp/wo/kernel/v                             size 1            shape (1,)                                     partition spec None
I0512 23:45:21.824227 140143678494720 utils.py:1372] Variable param_states/decoder/layers_23/extra_mlp/wo/kernel/v_col                         size 8192         shape (8192,)                                  partition spec None
I0512 23:45:21.824266 140143678494720 utils.py:1372] Variable param_states/decoder/layers_23/extra_mlp/wo/kernel/v_row                         size 2048         shape (2048,)                                  partition spec None
I0512 23:45:21.824307 140143678494720 utils.py:1372] Variable param_states/decoder/layers_23/mlp/expert/wi_0/kernel/m                          size 1            shape (1,)                                     partition spec None
I0512 23:45:21.824348 140143678494720 utils.py:1372] Variable param_states/decoder/layers_23/mlp/expert/wi_0/kernel/v                          size 1            shape (1,)                                     partition spec None
I0512 23:45:21.824388 140143678494720 utils.py:1372] Variable param_states/decoder/layers_23/mlp/expert/wi_0/kernel/v_col                      size 131072       shape (8, 16384)                               partition spec ('expert',)
I0512 23:45:21.824428 140143678494720 utils.py:1372] Variable param_states/decoder/layers_23/mlp/expert/wi_0/kernel/v_row                      size 16384        shape (8, 2048)                                partition spec ('expert',)
I0512 23:45:21.824467 140143678494720 utils.py:1372] Variable param_states/decoder/layers_23/mlp/expert/wi_1/kernel/m                          size 1            shape (1,)                                     partition spec None
I0512 23:45:21.824507 140143678494720 utils.py:1372] Variable param_states/decoder/layers_23/mlp/expert/wi_1/kernel/v                          size 1            shape (1,)                                     partition spec None
I0512 23:45:21.824569 140143678494720 utils.py:1372] Variable param_states/decoder/layers_23/mlp/expert/wi_1/kernel/v_col                      size 65536        shape (8, 8192)                                partition spec ('expert',)
I0512 23:45:21.824615 140143678494720 utils.py:1372] Variable param_states/decoder/layers_23/mlp/expert/wi_1/kernel/v_row                      size 16384        shape (8, 2048)                                partition spec ('expert',)
I0512 23:45:21.824655 140143678494720 utils.py:1372] Variable param_states/decoder/layers_23/mlp/expert/wo/kernel/m                            size 1            shape (1,)                                     partition spec None
I0512 23:45:21.824695 140143678494720 utils.py:1372] Variable param_states/decoder/layers_23/mlp/expert/wo/kernel/v                            size 1            shape (1,)                                     partition spec None
I0512 23:45:21.824734 140143678494720 utils.py:1372] Variable param_states/decoder/layers_23/mlp/expert/wo/kernel/v_col                        size 65536        shape (8, 8192)                                partition spec ('expert',)
I0512 23:45:21.824789 140143678494720 utils.py:1372] Variable param_states/decoder/layers_23/mlp/expert/wo/kernel/v_row                        size 16384        shape (8, 2048)                                partition spec ('expert',)
I0512 23:45:21.824832 140143678494720 utils.py:1372] Variable param_states/decoder/layers_23/mlp/router/router_weights/w/kernel/m              size 1            shape (1,)                                     partition spec None
I0512 23:45:21.824876 140143678494720 utils.py:1372] Variable param_states/decoder/layers_23/mlp/router/router_weights/w/kernel/v              size 16384        shape (embed=2048, unmodeled=8)                partition spec ('model', None)
I0512 23:45:21.824918 140143678494720 utils.py:1372] Variable param_states/decoder/layers_23/mlp/router/router_weights/w/kernel/v_col          size 1            shape (1,)                                     partition spec None
I0512 23:45:21.824957 140143678494720 utils.py:1372] Variable param_states/decoder/layers_23/mlp/router/router_weights/w/kernel/v_row          size 1            shape (1,)                                     partition spec None
I0512 23:45:21.824995 140143678494720 utils.py:1372] Variable param_states/decoder/layers_23/pre_extra_mlp_layer_norm/scale/m                  size 1            shape (1,)                                     partition spec None
I0512 23:45:21.825038 140143678494720 utils.py:1372] Variable param_states/decoder/layers_23/pre_extra_mlp_layer_norm/scale/v                  size 2048         shape (embed=2048)                             partition spec ('model',)
I0512 23:45:21.825079 140143678494720 utils.py:1372] Variable param_states/decoder/layers_23/pre_extra_mlp_layer_norm/scale/v_col              size 1            shape (1,)                                     partition spec None
I0512 23:45:21.825118 140143678494720 utils.py:1372] Variable param_states/decoder/layers_23/pre_extra_mlp_layer_norm/scale/v_row              size 1            shape (1,)                                     partition spec None
I0512 23:45:21.825157 140143678494720 utils.py:1372] Variable param_states/decoder/layers_23/pre_mlp_layer_norm/scale/m                        size 1            shape (1,)                                     partition spec None
I0512 23:45:21.825198 140143678494720 utils.py:1372] Variable param_states/decoder/layers_23/pre_mlp_layer_norm/scale/v                        size 2048         shape (embed=2048)                             partition spec ('model',)
I0512 23:45:21.825238 140143678494720 utils.py:1372] Variable param_states/decoder/layers_23/pre_mlp_layer_norm/scale/v_col                    size 1            shape (1,)                                     partition spec None
I0512 23:45:21.825277 140143678494720 utils.py:1372] Variable param_states/decoder/layers_23/pre_mlp_layer_norm/scale/v_row                    size 1            shape (1,)                                     partition spec None
I0512 23:45:21.825318 140143678494720 utils.py:1372] Variable param_states/decoder/layers_23/pre_self_attention_layer_norm/scale/m             size 1            shape (1,)                                     partition spec None
I0512 23:45:21.825359 140143678494720 utils.py:1372] Variable param_states/decoder/layers_23/pre_self_attention_layer_norm/scale/v             size 2048         shape (embed=2048)                             partition spec ('model',)
I0512 23:45:21.825399 140143678494720 utils.py:1372] Variable param_states/decoder/layers_23/pre_self_attention_layer_norm/scale/v_col         size 1            shape (1,)                                     partition spec None
I0512 23:45:21.825438 140143678494720 utils.py:1372] Variable param_states/decoder/layers_23/pre_self_attention_layer_norm/scale/v_row         size 1            shape (1,)                                     partition spec None
I0512 23:45:21.825476 140143678494720 utils.py:1372] Variable param_states/decoder/layers_23/self_attention/key/kernel/m                       size 1            shape (1,)                                     partition spec None
I0512 23:45:21.825515 140143678494720 utils.py:1372] Variable param_states/decoder/layers_23/self_attention/key/kernel/v                       size 1            shape (1,)                                     partition spec None
I0512 23:45:21.825559 140143678494720 utils.py:1372] Variable param_states/decoder/layers_23/self_attention/key/kernel/v_col                   size 3072         shape (3072,)                                  partition spec None
I0512 23:45:21.825600 140143678494720 utils.py:1372] Variable param_states/decoder/layers_23/self_attention/key/kernel/v_row                   size 2048         shape (2048,)                                  partition spec None
I0512 23:45:21.825639 140143678494720 utils.py:1372] Variable param_states/decoder/layers_23/self_attention/out/kernel/m                       size 1            shape (1,)                                     partition spec None
I0512 23:45:21.825678 140143678494720 utils.py:1372] Variable param_states/decoder/layers_23/self_attention/out/kernel/v                       size 1            shape (1,)                                     partition spec None
I0512 23:45:21.825716 140143678494720 utils.py:1372] Variable param_states/decoder/layers_23/self_attention/out/kernel/v_col                   size 3072         shape (3072,)                                  partition spec None
I0512 23:45:21.825755 140143678494720 utils.py:1372] Variable param_states/decoder/layers_23/self_attention/out/kernel/v_row                   size 2048         shape (2048,)                                  partition spec None
I0512 23:45:21.825793 140143678494720 utils.py:1372] Variable param_states/decoder/layers_23/self_attention/query/kernel/m                     size 1            shape (1,)                                     partition spec None
I0512 23:45:21.825832 140143678494720 utils.py:1372] Variable param_states/decoder/layers_23/self_attention/query/kernel/v                     size 1            shape (1,)                                     partition spec None
I0512 23:45:21.825869 140143678494720 utils.py:1372] Variable param_states/decoder/layers_23/self_attention/query/kernel/v_col                 size 3072         shape (3072,)                                  partition spec None
I0512 23:45:21.825908 140143678494720 utils.py:1372] Variable param_states/decoder/layers_23/self_attention/query/kernel/v_row                 size 2048         shape (2048,)                                  partition spec None
I0512 23:45:21.825946 140143678494720 utils.py:1372] Variable param_states/decoder/layers_23/self_attention/value/kernel/m                     size 1            shape (1,)                                     partition spec None
I0512 23:45:21.825985 140143678494720 utils.py:1372] Variable param_states/decoder/layers_23/self_attention/value/kernel/v                     size 1            shape (1,)                                     partition spec None
I0512 23:45:21.826024 140143678494720 utils.py:1372] Variable param_states/decoder/layers_23/self_attention/value/kernel/v_col                 size 3072         shape (3072,)                                  partition spec None
I0512 23:45:21.826063 140143678494720 utils.py:1372] Variable param_states/decoder/layers_23/self_attention/value/kernel/v_row                 size 2048         shape (2048,)                                  partition spec None
I0512 23:45:21.826101 140143678494720 utils.py:1372] Variable param_states/decoder/layers_3/mlp/wi_0/kernel/m                                  size 1            shape (1,)                                     partition spec None
I0512 23:45:21.826141 140143678494720 utils.py:1372] Variable param_states/decoder/layers_3/mlp/wi_0/kernel/v                                  size 1            shape (1,)                                     partition spec None
I0512 23:45:21.826180 140143678494720 utils.py:1372] Variable param_states/decoder/layers_3/mlp/wi_0/kernel/v_col                              size 16384        shape (16384,)                                 partition spec None
I0512 23:45:21.826219 140143678494720 utils.py:1372] Variable param_states/decoder/layers_3/mlp/wi_0/kernel/v_row                              size 2048         shape (2048,)                                  partition spec None
I0512 23:45:21.826257 140143678494720 utils.py:1372] Variable param_states/decoder/layers_3/mlp/wi_1/kernel/m                                  size 1            shape (1,)                                     partition spec None
I0512 23:45:21.826297 140143678494720 utils.py:1372] Variable param_states/decoder/layers_3/mlp/wi_1/kernel/v                                  size 1            shape (1,)                                     partition spec None
I0512 23:45:21.826338 140143678494720 utils.py:1372] Variable param_states/decoder/layers_3/mlp/wi_1/kernel/v_col                              size 8192         shape (8192,)                                  partition spec None
I0512 23:45:21.826378 140143678494720 utils.py:1372] Variable param_states/decoder/layers_3/mlp/wi_1/kernel/v_row                              size 2048         shape (2048,)                                  partition spec None
I0512 23:45:21.826416 140143678494720 utils.py:1372] Variable param_states/decoder/layers_3/mlp/wo/kernel/m                                    size 1            shape (1,)                                     partition spec None
I0512 23:45:21.826455 140143678494720 utils.py:1372] Variable param_states/decoder/layers_3/mlp/wo/kernel/v                                    size 1            shape (1,)                                     partition spec None
I0512 23:45:21.826493 140143678494720 utils.py:1372] Variable param_states/decoder/layers_3/mlp/wo/kernel/v_col                                size 8192         shape (8192,)                                  partition spec None
I0512 23:45:21.826532 140143678494720 utils.py:1372] Variable param_states/decoder/layers_3/mlp/wo/kernel/v_row                                size 2048         shape (2048,)                                  partition spec None
I0512 23:45:21.826578 140143678494720 utils.py:1372] Variable param_states/decoder/layers_3/pre_mlp_layer_norm/scale/m                         size 1            shape (1,)                                     partition spec None
I0512 23:45:21.826621 140143678494720 utils.py:1372] Variable param_states/decoder/layers_3/pre_mlp_layer_norm/scale/v                         size 2048         shape (embed=2048)                             partition spec ('model',)
I0512 23:45:21.826677 140143678494720 utils.py:1372] Variable param_states/decoder/layers_3/pre_mlp_layer_norm/scale/v_col                     size 1            shape (1,)                                     partition spec None
I0512 23:45:21.826720 140143678494720 utils.py:1372] Variable param_states/decoder/layers_3/pre_mlp_layer_norm/scale/v_row                     size 1            shape (1,)                                     partition spec None
I0512 23:45:21.826759 140143678494720 utils.py:1372] Variable param_states/decoder/layers_3/pre_self_attention_layer_norm/scale/m              size 1            shape (1,)                                     partition spec None
I0512 23:45:21.826800 140143678494720 utils.py:1372] Variable param_states/decoder/layers_3/pre_self_attention_layer_norm/scale/v              size 2048         shape (embed=2048)                             partition spec ('model',)
I0512 23:45:21.826840 140143678494720 utils.py:1372] Variable param_states/decoder/layers_3/pre_self_attention_layer_norm/scale/v_col          size 1            shape (1,)                                     partition spec None
I0512 23:45:21.826879 140143678494720 utils.py:1372] Variable param_states/decoder/layers_3/pre_self_attention_layer_norm/scale/v_row          size 1            shape (1,)                                     partition spec None
I0512 23:45:21.826918 140143678494720 utils.py:1372] Variable param_states/decoder/layers_3/self_attention/key/kernel/m                        size 1            shape (1,)                                     partition spec None
I0512 23:45:21.826956 140143678494720 utils.py:1372] Variable param_states/decoder/layers_3/self_attention/key/kernel/v                        size 1            shape (1,)                                     partition spec None
I0512 23:45:21.826994 140143678494720 utils.py:1372] Variable param_states/decoder/layers_3/self_attention/key/kernel/v_col                    size 3072         shape (3072,)                                  partition spec None
I0512 23:45:21.827033 140143678494720 utils.py:1372] Variable param_states/decoder/layers_3/self_attention/key/kernel/v_row                    size 2048         shape (2048,)                                  partition spec None
I0512 23:45:21.827071 140143678494720 utils.py:1372] Variable param_states/decoder/layers_3/self_attention/out/kernel/m                        size 1            shape (1,)                                     partition spec None
I0512 23:45:21.827110 140143678494720 utils.py:1372] Variable param_states/decoder/layers_3/self_attention/out/kernel/v                        size 1            shape (1,)                                     partition spec None
I0512 23:45:21.827148 140143678494720 utils.py:1372] Variable param_states/decoder/layers_3/self_attention/out/kernel/v_col                    size 3072         shape (3072,)                                  partition spec None
I0512 23:45:21.827186 140143678494720 utils.py:1372] Variable param_states/decoder/layers_3/self_attention/out/kernel/v_row                    size 2048         shape (2048,)                                  partition spec None
I0512 23:45:21.827225 140143678494720 utils.py:1372] Variable param_states/decoder/layers_3/self_attention/query/kernel/m                      size 1            shape (1,)                                     partition spec None
I0512 23:45:21.827264 140143678494720 utils.py:1372] Variable param_states/decoder/layers_3/self_attention/query/kernel/v                      size 1            shape (1,)                                     partition spec None
I0512 23:45:21.827303 140143678494720 utils.py:1372] Variable param_states/decoder/layers_3/self_attention/query/kernel/v_col                  size 3072         shape (3072,)                                  partition spec None
I0512 23:45:21.827343 140143678494720 utils.py:1372] Variable param_states/decoder/layers_3/self_attention/query/kernel/v_row                  size 2048         shape (2048,)                                  partition spec None
I0512 23:45:21.827382 140143678494720 utils.py:1372] Variable param_states/decoder/layers_3/self_attention/value/kernel/m                      size 1            shape (1,)                                     partition spec None
I0512 23:45:21.827421 140143678494720 utils.py:1372] Variable param_states/decoder/layers_3/self_attention/value/kernel/v                      size 1            shape (1,)                                     partition spec None
I0512 23:45:21.827459 140143678494720 utils.py:1372] Variable param_states/decoder/layers_3/self_attention/value/kernel/v_col                  size 3072         shape (3072,)                                  partition spec None
I0512 23:45:21.827497 140143678494720 utils.py:1372] Variable param_states/decoder/layers_3/self_attention/value/kernel/v_row                  size 2048         shape (2048,)                                  partition spec None
I0512 23:45:21.827536 140143678494720 utils.py:1372] Variable param_states/decoder/layers_4/mlp/wi_0/kernel/m                                  size 1            shape (1,)                                     partition spec None
I0512 23:45:21.827584 140143678494720 utils.py:1372] Variable param_states/decoder/layers_4/mlp/wi_0/kernel/v                                  size 1            shape (1,)                                     partition spec None
I0512 23:45:21.827623 140143678494720 utils.py:1372] Variable param_states/decoder/layers_4/mlp/wi_0/kernel/v_col                              size 16384        shape (16384,)                                 partition spec None
I0512 23:45:21.827662 140143678494720 utils.py:1372] Variable param_states/decoder/layers_4/mlp/wi_0/kernel/v_row                              size 2048         shape (2048,)                                  partition spec None
I0512 23:45:21.827701 140143678494720 utils.py:1372] Variable param_states/decoder/layers_4/mlp/wi_1/kernel/m                                  size 1            shape (1,)                                     partition spec None
I0512 23:45:21.827739 140143678494720 utils.py:1372] Variable param_states/decoder/layers_4/mlp/wi_1/kernel/v                                  size 1            shape (1,)                                     partition spec None
I0512 23:45:21.827778 140143678494720 utils.py:1372] Variable param_states/decoder/layers_4/mlp/wi_1/kernel/v_col                              size 8192         shape (8192,)                                  partition spec None
I0512 23:45:21.827816 140143678494720 utils.py:1372] Variable param_states/decoder/layers_4/mlp/wi_1/kernel/v_row                              size 2048         shape (2048,)                                  partition spec None
I0512 23:45:21.827855 140143678494720 utils.py:1372] Variable param_states/decoder/layers_4/mlp/wo/kernel/m                                    size 1            shape (1,)                                     partition spec None
I0512 23:45:21.827893 140143678494720 utils.py:1372] Variable param_states/decoder/layers_4/mlp/wo/kernel/v                                    size 1            shape (1,)                                     partition spec None
I0512 23:45:21.827932 140143678494720 utils.py:1372] Variable param_states/decoder/layers_4/mlp/wo/kernel/v_col                                size 8192         shape (8192,)                                  partition spec None
I0512 23:45:21.827970 140143678494720 utils.py:1372] Variable param_states/decoder/layers_4/mlp/wo/kernel/v_row                                size 2048         shape (2048,)                                  partition spec None
I0512 23:45:21.828008 140143678494720 utils.py:1372] Variable param_states/decoder/layers_4/pre_mlp_layer_norm/scale/m                         size 1            shape (1,)                                     partition spec None
I0512 23:45:21.828049 140143678494720 utils.py:1372] Variable param_states/decoder/layers_4/pre_mlp_layer_norm/scale/v                         size 2048         shape (embed=2048)                             partition spec ('model',)
I0512 23:45:21.828090 140143678494720 utils.py:1372] Variable param_states/decoder/layers_4/pre_mlp_layer_norm/scale/v_col                     size 1            shape (1,)                                     partition spec None
I0512 23:45:21.828129 140143678494720 utils.py:1372] Variable param_states/decoder/layers_4/pre_mlp_layer_norm/scale/v_row                     size 1            shape (1,)                                     partition spec None
I0512 23:45:21.828167 140143678494720 utils.py:1372] Variable param_states/decoder/layers_4/pre_self_attention_layer_norm/scale/m              size 1            shape (1,)                                     partition spec None
I0512 23:45:21.828208 140143678494720 utils.py:1372] Variable param_states/decoder/layers_4/pre_self_attention_layer_norm/scale/v              size 2048         shape (embed=2048)                             partition spec ('model',)
I0512 23:45:21.828248 140143678494720 utils.py:1372] Variable param_states/decoder/layers_4/pre_self_attention_layer_norm/scale/v_col          size 1            shape (1,)                                     partition spec None
I0512 23:45:21.828287 140143678494720 utils.py:1372] Variable param_states/decoder/layers_4/pre_self_attention_layer_norm/scale/v_row          size 1            shape (1,)                                     partition spec None
I0512 23:45:21.828329 140143678494720 utils.py:1372] Variable param_states/decoder/layers_4/self_attention/key/kernel/m                        size 1            shape (1,)                                     partition spec None
I0512 23:45:21.828368 140143678494720 utils.py:1372] Variable param_states/decoder/layers_4/self_attention/key/kernel/v                        size 1            shape (1,)                                     partition spec None
I0512 23:45:21.828407 140143678494720 utils.py:1372] Variable param_states/decoder/layers_4/self_attention/key/kernel/v_col                    size 3072         shape (3072,)                                  partition spec None
I0512 23:45:21.828445 140143678494720 utils.py:1372] Variable param_states/decoder/layers_4/self_attention/key/kernel/v_row                    size 2048         shape (2048,)                                  partition spec None
I0512 23:45:21.828484 140143678494720 utils.py:1372] Variable param_states/decoder/layers_4/self_attention/out/kernel/m                        size 1            shape (1,)                                     partition spec None
I0512 23:45:21.828558 140143678494720 utils.py:1372] Variable param_states/decoder/layers_4/self_attention/out/kernel/v                        size 1            shape (1,)                                     partition spec None
I0512 23:45:21.828608 140143678494720 utils.py:1372] Variable param_states/decoder/layers_4/self_attention/out/kernel/v_col                    size 3072         shape (3072,)                                  partition spec None
I0512 23:45:21.828649 140143678494720 utils.py:1372] Variable param_states/decoder/layers_4/self_attention/out/kernel/v_row                    size 2048         shape (2048,)                                  partition spec None
I0512 23:45:21.828688 140143678494720 utils.py:1372] Variable param_states/decoder/layers_4/self_attention/query/kernel/m                      size 1            shape (1,)                                     partition spec None
I0512 23:45:21.828727 140143678494720 utils.py:1372] Variable param_states/decoder/layers_4/self_attention/query/kernel/v                      size 1            shape (1,)                                     partition spec None
I0512 23:45:21.828766 140143678494720 utils.py:1372] Variable param_states/decoder/layers_4/self_attention/query/kernel/v_col                  size 3072         shape (3072,)                                  partition spec None
I0512 23:45:21.828805 140143678494720 utils.py:1372] Variable param_states/decoder/layers_4/self_attention/query/kernel/v_row                  size 2048         shape (2048,)                                  partition spec None
I0512 23:45:21.828844 140143678494720 utils.py:1372] Variable param_states/decoder/layers_4/self_attention/value/kernel/m                      size 1            shape (1,)                                     partition spec None
I0512 23:45:21.828887 140143678494720 utils.py:1372] Variable param_states/decoder/layers_4/self_attention/value/kernel/v                      size 1            shape (1,)                                     partition spec None
I0512 23:45:21.828932 140143678494720 utils.py:1372] Variable param_states/decoder/layers_4/self_attention/value/kernel/v_col                  size 3072         shape (3072,)                                  partition spec None
I0512 23:45:21.828972 140143678494720 utils.py:1372] Variable param_states/decoder/layers_4/self_attention/value/kernel/v_row                  size 2048         shape (2048,)                                  partition spec None
I0512 23:45:21.829011 140143678494720 utils.py:1372] Variable param_states/decoder/layers_5/extra_mlp/wi_0/kernel/m                            size 1            shape (1,)                                     partition spec None
I0512 23:45:21.829051 140143678494720 utils.py:1372] Variable param_states/decoder/layers_5/extra_mlp/wi_0/kernel/v                            size 1            shape (1,)                                     partition spec None
I0512 23:45:21.829091 140143678494720 utils.py:1372] Variable param_states/decoder/layers_5/extra_mlp/wi_0/kernel/v_col                        size 16384        shape (16384,)                                 partition spec None
I0512 23:45:21.829130 140143678494720 utils.py:1372] Variable param_states/decoder/layers_5/extra_mlp/wi_0/kernel/v_row                        size 2048         shape (2048,)                                  partition spec None
I0512 23:45:21.829168 140143678494720 utils.py:1372] Variable param_states/decoder/layers_5/extra_mlp/wi_1/kernel/m                            size 1            shape (1,)                                     partition spec None
I0512 23:45:21.829207 140143678494720 utils.py:1372] Variable param_states/decoder/layers_5/extra_mlp/wi_1/kernel/v                            size 1            shape (1,)                                     partition spec None
I0512 23:45:21.829246 140143678494720 utils.py:1372] Variable param_states/decoder/layers_5/extra_mlp/wi_1/kernel/v_col                        size 8192         shape (8192,)                                  partition spec None
I0512 23:45:21.829284 140143678494720 utils.py:1372] Variable param_states/decoder/layers_5/extra_mlp/wi_1/kernel/v_row                        size 2048         shape (2048,)                                  partition spec None
I0512 23:45:21.829326 140143678494720 utils.py:1372] Variable param_states/decoder/layers_5/extra_mlp/wo/kernel/m                              size 1            shape (1,)                                     partition spec None
I0512 23:45:21.829365 140143678494720 utils.py:1372] Variable param_states/decoder/layers_5/extra_mlp/wo/kernel/v                              size 1            shape (1,)                                     partition spec None
I0512 23:45:21.829404 140143678494720 utils.py:1372] Variable param_states/decoder/layers_5/extra_mlp/wo/kernel/v_col                          size 8192         shape (8192,)                                  partition spec None
I0512 23:45:21.829442 140143678494720 utils.py:1372] Variable param_states/decoder/layers_5/extra_mlp/wo/kernel/v_row                          size 2048         shape (2048,)                                  partition spec None
I0512 23:45:21.829481 140143678494720 utils.py:1372] Variable param_states/decoder/layers_5/mlp/expert/wi_0/kernel/m                           size 1            shape (1,)                                     partition spec None
I0512 23:45:21.829520 140143678494720 utils.py:1372] Variable param_states/decoder/layers_5/mlp/expert/wi_0/kernel/v                           size 1            shape (1,)                                     partition spec None
I0512 23:45:21.829566 140143678494720 utils.py:1372] Variable param_states/decoder/layers_5/mlp/expert/wi_0/kernel/v_col                       size 131072       shape (8, 16384)                               partition spec ('expert',)
I0512 23:45:21.829607 140143678494720 utils.py:1372] Variable param_states/decoder/layers_5/mlp/expert/wi_0/kernel/v_row                       size 16384        shape (8, 2048)                                partition spec ('expert',)
I0512 23:45:21.829647 140143678494720 utils.py:1372] Variable param_states/decoder/layers_5/mlp/expert/wi_1/kernel/m                           size 1            shape (1,)                                     partition spec None
I0512 23:45:21.829686 140143678494720 utils.py:1372] Variable param_states/decoder/layers_5/mlp/expert/wi_1/kernel/v                           size 1            shape (1,)                                     partition spec None
I0512 23:45:21.829725 140143678494720 utils.py:1372] Variable param_states/decoder/layers_5/mlp/expert/wi_1/kernel/v_col                       size 65536        shape (8, 8192)                                partition spec ('expert',)
I0512 23:45:21.829765 140143678494720 utils.py:1372] Variable param_states/decoder/layers_5/mlp/expert/wi_1/kernel/v_row                       size 16384        shape (8, 2048)                                partition spec ('expert',)
I0512 23:45:21.829805 140143678494720 utils.py:1372] Variable param_states/decoder/layers_5/mlp/expert/wo/kernel/m                             size 1            shape (1,)                                     partition spec None
I0512 23:45:21.829844 140143678494720 utils.py:1372] Variable param_states/decoder/layers_5/mlp/expert/wo/kernel/v                             size 1            shape (1,)                                     partition spec None
I0512 23:45:21.829883 140143678494720 utils.py:1372] Variable param_states/decoder/layers_5/mlp/expert/wo/kernel/v_col                         size 65536        shape (8, 8192)                                partition spec ('expert',)
I0512 23:45:21.829923 140143678494720 utils.py:1372] Variable param_states/decoder/layers_5/mlp/expert/wo/kernel/v_row                         size 16384        shape (8, 2048)                                partition spec ('expert',)
I0512 23:45:21.829962 140143678494720 utils.py:1372] Variable param_states/decoder/layers_5/mlp/router/router_weights/w/kernel/m               size 1            shape (1,)                                     partition spec None
I0512 23:45:21.830004 140143678494720 utils.py:1372] Variable param_states/decoder/layers_5/mlp/router/router_weights/w/kernel/v               size 16384        shape (embed=2048, unmodeled=8)                partition spec ('model', None)
I0512 23:45:21.830046 140143678494720 utils.py:1372] Variable param_states/decoder/layers_5/mlp/router/router_weights/w/kernel/v_col           size 1            shape (1,)                                     partition spec None
I0512 23:45:21.830086 140143678494720 utils.py:1372] Variable param_states/decoder/layers_5/mlp/router/router_weights/w/kernel/v_row           size 1            shape (1,)                                     partition spec None
I0512 23:45:21.830124 140143678494720 utils.py:1372] Variable param_states/decoder/layers_5/pre_extra_mlp_layer_norm/scale/m                   size 1            shape (1,)                                     partition spec None
I0512 23:45:21.830165 140143678494720 utils.py:1372] Variable param_states/decoder/layers_5/pre_extra_mlp_layer_norm/scale/v                   size 2048         shape (embed=2048)                             partition spec ('model',)
I0512 23:45:21.830206 140143678494720 utils.py:1372] Variable param_states/decoder/layers_5/pre_extra_mlp_layer_norm/scale/v_col               size 1            shape (1,)                                     partition spec None
I0512 23:45:21.830245 140143678494720 utils.py:1372] Variable param_states/decoder/layers_5/pre_extra_mlp_layer_norm/scale/v_row               size 1            shape (1,)                                     partition spec None
I0512 23:45:21.830284 140143678494720 utils.py:1372] Variable param_states/decoder/layers_5/pre_mlp_layer_norm/scale/m                         size 1            shape (1,)                                     partition spec None
I0512 23:45:21.830328 140143678494720 utils.py:1372] Variable param_states/decoder/layers_5/pre_mlp_layer_norm/scale/v                         size 2048         shape (embed=2048)                             partition spec ('model',)
I0512 23:45:21.830369 140143678494720 utils.py:1372] Variable param_states/decoder/layers_5/pre_mlp_layer_norm/scale/v_col                     size 1            shape (1,)                                     partition spec None
I0512 23:45:21.830410 140143678494720 utils.py:1372] Variable param_states/decoder/layers_5/pre_mlp_layer_norm/scale/v_row                     size 1            shape (1,)                                     partition spec None
I0512 23:45:21.830465 140143678494720 utils.py:1372] Variable param_states/decoder/layers_5/pre_self_attention_layer_norm/scale/m              size 1            shape (1,)                                     partition spec None
I0512 23:45:21.830510 140143678494720 utils.py:1372] Variable param_states/decoder/layers_5/pre_self_attention_layer_norm/scale/v              size 2048         shape (embed=2048)                             partition spec ('model',)
I0512 23:45:21.830558 140143678494720 utils.py:1372] Variable param_states/decoder/layers_5/pre_self_attention_layer_norm/scale/v_col          size 1            shape (1,)                                     partition spec None
I0512 23:45:21.830599 140143678494720 utils.py:1372] Variable param_states/decoder/layers_5/pre_self_attention_layer_norm/scale/v_row          size 1            shape (1,)                                     partition spec None
I0512 23:45:21.830638 140143678494720 utils.py:1372] Variable param_states/decoder/layers_5/self_attention/key/kernel/m                        size 1            shape (1,)                                     partition spec None
I0512 23:45:21.830676 140143678494720 utils.py:1372] Variable param_states/decoder/layers_5/self_attention/key/kernel/v                        size 1            shape (1,)                                     partition spec None
I0512 23:45:21.830715 140143678494720 utils.py:1372] Variable param_states/decoder/layers_5/self_attention/key/kernel/v_col                    size 3072         shape (3072,)                                  partition spec None
I0512 23:45:21.830753 140143678494720 utils.py:1372] Variable param_states/decoder/layers_5/self_attention/key/kernel/v_row                    size 2048         shape (2048,)                                  partition spec None
I0512 23:45:21.830791 140143678494720 utils.py:1372] Variable param_states/decoder/layers_5/self_attention/out/kernel/m                        size 1            shape (1,)                                     partition spec None
I0512 23:45:21.830830 140143678494720 utils.py:1372] Variable param_states/decoder/layers_5/self_attention/out/kernel/v                        size 1            shape (1,)                                     partition spec None
I0512 23:45:21.830868 140143678494720 utils.py:1372] Variable param_states/decoder/layers_5/self_attention/out/kernel/v_col                    size 3072         shape (3072,)                                  partition spec None
I0512 23:45:21.830906 140143678494720 utils.py:1372] Variable param_states/decoder/layers_5/self_attention/out/kernel/v_row                    size 2048         shape (2048,)                                  partition spec None
I0512 23:45:21.830945 140143678494720 utils.py:1372] Variable param_states/decoder/layers_5/self_attention/query/kernel/m                      size 1            shape (1,)                                     partition spec None
I0512 23:45:21.830982 140143678494720 utils.py:1372] Variable param_states/decoder/layers_5/self_attention/query/kernel/v                      size 1            shape (1,)                                     partition spec None
I0512 23:45:21.831021 140143678494720 utils.py:1372] Variable param_states/decoder/layers_5/self_attention/query/kernel/v_col                  size 3072         shape (3072,)                                  partition spec None
I0512 23:45:21.831059 140143678494720 utils.py:1372] Variable param_states/decoder/layers_5/self_attention/query/kernel/v_row                  size 2048         shape (2048,)                                  partition spec None
I0512 23:45:21.831097 140143678494720 utils.py:1372] Variable param_states/decoder/layers_5/self_attention/value/kernel/m                      size 1            shape (1,)                                     partition spec None
I0512 23:45:21.831135 140143678494720 utils.py:1372] Variable param_states/decoder/layers_5/self_attention/value/kernel/v                      size 1            shape (1,)                                     partition spec None
I0512 23:45:21.831174 140143678494720 utils.py:1372] Variable param_states/decoder/layers_5/self_attention/value/kernel/v_col                  size 3072         shape (3072,)                                  partition spec None
I0512 23:45:21.831212 140143678494720 utils.py:1372] Variable param_states/decoder/layers_5/self_attention/value/kernel/v_row                  size 2048         shape (2048,)                                  partition spec None
I0512 23:45:21.831250 140143678494720 utils.py:1372] Variable param_states/decoder/layers_6/mlp/wi_0/kernel/m                                  size 1            shape (1,)                                     partition spec None
I0512 23:45:21.831289 140143678494720 utils.py:1372] Variable param_states/decoder/layers_6/mlp/wi_0/kernel/v                                  size 1            shape (1,)                                     partition spec None
I0512 23:45:21.831331 140143678494720 utils.py:1372] Variable param_states/decoder/layers_6/mlp/wi_0/kernel/v_col                              size 16384        shape (16384,)                                 partition spec None
I0512 23:45:21.831370 140143678494720 utils.py:1372] Variable param_states/decoder/layers_6/mlp/wi_0/kernel/v_row                              size 2048         shape (2048,)                                  partition spec None
I0512 23:45:21.831408 140143678494720 utils.py:1372] Variable param_states/decoder/layers_6/mlp/wi_1/kernel/m                                  size 1            shape (1,)                                     partition spec None
I0512 23:45:21.831447 140143678494720 utils.py:1372] Variable param_states/decoder/layers_6/mlp/wi_1/kernel/v                                  size 1            shape (1,)                                     partition spec None
I0512 23:45:21.831485 140143678494720 utils.py:1372] Variable param_states/decoder/layers_6/mlp/wi_1/kernel/v_col                              size 8192         shape (8192,)                                  partition spec None
I0512 23:45:21.831524 140143678494720 utils.py:1372] Variable param_states/decoder/layers_6/mlp/wi_1/kernel/v_row                              size 2048         shape (2048,)                                  partition spec None
I0512 23:45:21.831568 140143678494720 utils.py:1372] Variable param_states/decoder/layers_6/mlp/wo/kernel/m                                    size 1            shape (1,)                                     partition spec None
I0512 23:45:21.831608 140143678494720 utils.py:1372] Variable param_states/decoder/layers_6/mlp/wo/kernel/v                                    size 1            shape (1,)                                     partition spec None
I0512 23:45:21.831646 140143678494720 utils.py:1372] Variable param_states/decoder/layers_6/mlp/wo/kernel/v_col                                size 8192         shape (8192,)                                  partition spec None
I0512 23:45:21.831685 140143678494720 utils.py:1372] Variable param_states/decoder/layers_6/mlp/wo/kernel/v_row                                size 2048         shape (2048,)                                  partition spec None
I0512 23:45:21.831723 140143678494720 utils.py:1372] Variable param_states/decoder/layers_6/pre_mlp_layer_norm/scale/m                         size 1            shape (1,)                                     partition spec None
I0512 23:45:21.831764 140143678494720 utils.py:1372] Variable param_states/decoder/layers_6/pre_mlp_layer_norm/scale/v                         size 2048         shape (embed=2048)                             partition spec ('model',)
I0512 23:45:21.831805 140143678494720 utils.py:1372] Variable param_states/decoder/layers_6/pre_mlp_layer_norm/scale/v_col                     size 1            shape (1,)                                     partition spec None
I0512 23:45:21.831843 140143678494720 utils.py:1372] Variable param_states/decoder/layers_6/pre_mlp_layer_norm/scale/v_row                     size 1            shape (1,)                                     partition spec None
I0512 23:45:21.831882 140143678494720 utils.py:1372] Variable param_states/decoder/layers_6/pre_self_attention_layer_norm/scale/m              size 1            shape (1,)                                     partition spec None
I0512 23:45:21.831922 140143678494720 utils.py:1372] Variable param_states/decoder/layers_6/pre_self_attention_layer_norm/scale/v              size 2048         shape (embed=2048)                             partition spec ('model',)
I0512 23:45:21.831962 140143678494720 utils.py:1372] Variable param_states/decoder/layers_6/pre_self_attention_layer_norm/scale/v_col          size 1            shape (1,)                                     partition spec None
I0512 23:45:21.832000 140143678494720 utils.py:1372] Variable param_states/decoder/layers_6/pre_self_attention_layer_norm/scale/v_row          size 1            shape (1,)                                     partition spec None
I0512 23:45:21.832038 140143678494720 utils.py:1372] Variable param_states/decoder/layers_6/self_attention/key/kernel/m                        size 1            shape (1,)                                     partition spec None
I0512 23:45:21.832076 140143678494720 utils.py:1372] Variable param_states/decoder/layers_6/self_attention/key/kernel/v                        size 1            shape (1,)                                     partition spec None
I0512 23:45:21.832114 140143678494720 utils.py:1372] Variable param_states/decoder/layers_6/self_attention/key/kernel/v_col                    size 3072         shape (3072,)                                  partition spec None
I0512 23:45:21.832152 140143678494720 utils.py:1372] Variable param_states/decoder/layers_6/self_attention/key/kernel/v_row                    size 2048         shape (2048,)                                  partition spec None
I0512 23:45:21.832190 140143678494720 utils.py:1372] Variable param_states/decoder/layers_6/self_attention/out/kernel/m                        size 1            shape (1,)                                     partition spec None
I0512 23:45:21.832228 140143678494720 utils.py:1372] Variable param_states/decoder/layers_6/self_attention/out/kernel/v                        size 1            shape (1,)                                     partition spec None
I0512 23:45:21.832266 140143678494720 utils.py:1372] Variable param_states/decoder/layers_6/self_attention/out/kernel/v_col                    size 3072         shape (3072,)                                  partition spec None
I0512 23:45:21.832320 140143678494720 utils.py:1372] Variable param_states/decoder/layers_6/self_attention/out/kernel/v_row                    size 2048         shape (2048,)                                  partition spec None
I0512 23:45:21.832363 140143678494720 utils.py:1372] Variable param_states/decoder/layers_6/self_attention/query/kernel/m                      size 1            shape (1,)                                     partition spec None
I0512 23:45:21.832402 140143678494720 utils.py:1372] Variable param_states/decoder/layers_6/self_attention/query/kernel/v                      size 1            shape (1,)                                     partition spec None
I0512 23:45:21.832441 140143678494720 utils.py:1372] Variable param_states/decoder/layers_6/self_attention/query/kernel/v_col                  size 3072         shape (3072,)                                  partition spec None
I0512 23:45:21.832479 140143678494720 utils.py:1372] Variable param_states/decoder/layers_6/self_attention/query/kernel/v_row                  size 2048         shape (2048,)                                  partition spec None
I0512 23:45:21.832517 140143678494720 utils.py:1372] Variable param_states/decoder/layers_6/self_attention/value/kernel/m                      size 1            shape (1,)                                     partition spec None
I0512 23:45:21.832582 140143678494720 utils.py:1372] Variable param_states/decoder/layers_6/self_attention/value/kernel/v                      size 1            shape (1,)                                     partition spec None
I0512 23:45:21.832624 140143678494720 utils.py:1372] Variable param_states/decoder/layers_6/self_attention/value/kernel/v_col                  size 3072         shape (3072,)                                  partition spec None
I0512 23:45:21.832663 140143678494720 utils.py:1372] Variable param_states/decoder/layers_6/self_attention/value/kernel/v_row                  size 2048         shape (2048,)                                  partition spec None
I0512 23:45:21.832701 140143678494720 utils.py:1372] Variable param_states/decoder/layers_7/mlp/wi_0/kernel/m                                  size 1            shape (1,)                                     partition spec None
I0512 23:45:21.832740 140143678494720 utils.py:1372] Variable param_states/decoder/layers_7/mlp/wi_0/kernel/v                                  size 1            shape (1,)                                     partition spec None
I0512 23:45:21.832779 140143678494720 utils.py:1372] Variable param_states/decoder/layers_7/mlp/wi_0/kernel/v_col                              size 16384        shape (16384,)                                 partition spec None
I0512 23:45:21.832818 140143678494720 utils.py:1372] Variable param_states/decoder/layers_7/mlp/wi_0/kernel/v_row                              size 2048         shape (2048,)                                  partition spec None
I0512 23:45:21.832856 140143678494720 utils.py:1372] Variable param_states/decoder/layers_7/mlp/wi_1/kernel/m                                  size 1            shape (1,)                                     partition spec None
I0512 23:45:21.832894 140143678494720 utils.py:1372] Variable param_states/decoder/layers_7/mlp/wi_1/kernel/v                                  size 1            shape (1,)                                     partition spec None
I0512 23:45:21.832933 140143678494720 utils.py:1372] Variable param_states/decoder/layers_7/mlp/wi_1/kernel/v_col                              size 8192         shape (8192,)                                  partition spec None
I0512 23:45:21.832971 140143678494720 utils.py:1372] Variable param_states/decoder/layers_7/mlp/wi_1/kernel/v_row                              size 2048         shape (2048,)                                  partition spec None
I0512 23:45:21.833009 140143678494720 utils.py:1372] Variable param_states/decoder/layers_7/mlp/wo/kernel/m                                    size 1            shape (1,)                                     partition spec None
I0512 23:45:21.833048 140143678494720 utils.py:1372] Variable param_states/decoder/layers_7/mlp/wo/kernel/v                                    size 1            shape (1,)                                     partition spec None
I0512 23:45:21.833086 140143678494720 utils.py:1372] Variable param_states/decoder/layers_7/mlp/wo/kernel/v_col                                size 8192         shape (8192,)                                  partition spec None
I0512 23:45:21.833125 140143678494720 utils.py:1372] Variable param_states/decoder/layers_7/mlp/wo/kernel/v_row                                size 2048         shape (2048,)                                  partition spec None
I0512 23:45:21.833162 140143678494720 utils.py:1372] Variable param_states/decoder/layers_7/pre_mlp_layer_norm/scale/m                         size 1            shape (1,)                                     partition spec None
I0512 23:45:21.833204 140143678494720 utils.py:1372] Variable param_states/decoder/layers_7/pre_mlp_layer_norm/scale/v                         size 2048         shape (embed=2048)                             partition spec ('model',)
I0512 23:45:21.833244 140143678494720 utils.py:1372] Variable param_states/decoder/layers_7/pre_mlp_layer_norm/scale/v_col                     size 1            shape (1,)                                     partition spec None
I0512 23:45:21.833283 140143678494720 utils.py:1372] Variable param_states/decoder/layers_7/pre_mlp_layer_norm/scale/v_row                     size 1            shape (1,)                                     partition spec None
I0512 23:45:21.833323 140143678494720 utils.py:1372] Variable param_states/decoder/layers_7/pre_self_attention_layer_norm/scale/m              size 1            shape (1,)                                     partition spec None
I0512 23:45:21.833365 140143678494720 utils.py:1372] Variable param_states/decoder/layers_7/pre_self_attention_layer_norm/scale/v              size 2048         shape (embed=2048)                             partition spec ('model',)
I0512 23:45:21.833405 140143678494720 utils.py:1372] Variable param_states/decoder/layers_7/pre_self_attention_layer_norm/scale/v_col          size 1            shape (1,)                                     partition spec None
I0512 23:45:21.833444 140143678494720 utils.py:1372] Variable param_states/decoder/layers_7/pre_self_attention_layer_norm/scale/v_row          size 1            shape (1,)                                     partition spec None
I0512 23:45:21.833482 140143678494720 utils.py:1372] Variable param_states/decoder/layers_7/self_attention/key/kernel/m                        size 1            shape (1,)                                     partition spec None
I0512 23:45:21.833521 140143678494720 utils.py:1372] Variable param_states/decoder/layers_7/self_attention/key/kernel/v                        size 1            shape (1,)                                     partition spec None
I0512 23:45:21.833567 140143678494720 utils.py:1372] Variable param_states/decoder/layers_7/self_attention/key/kernel/v_col                    size 3072         shape (3072,)                                  partition spec None
I0512 23:45:21.833607 140143678494720 utils.py:1372] Variable param_states/decoder/layers_7/self_attention/key/kernel/v_row                    size 2048         shape (2048,)                                  partition spec None
I0512 23:45:21.833645 140143678494720 utils.py:1372] Variable param_states/decoder/layers_7/self_attention/out/kernel/m                        size 1            shape (1,)                                     partition spec None
I0512 23:45:21.833684 140143678494720 utils.py:1372] Variable param_states/decoder/layers_7/self_attention/out/kernel/v                        size 1            shape (1,)                                     partition spec None
I0512 23:45:21.833722 140143678494720 utils.py:1372] Variable param_states/decoder/layers_7/self_attention/out/kernel/v_col                    size 3072         shape (3072,)                                  partition spec None
I0512 23:45:21.833761 140143678494720 utils.py:1372] Variable param_states/decoder/layers_7/self_attention/out/kernel/v_row                    size 2048         shape (2048,)                                  partition spec None
I0512 23:45:21.833799 140143678494720 utils.py:1372] Variable param_states/decoder/layers_7/self_attention/query/kernel/m                      size 1            shape (1,)                                     partition spec None
I0512 23:45:21.833837 140143678494720 utils.py:1372] Variable param_states/decoder/layers_7/self_attention/query/kernel/v                      size 1            shape (1,)                                     partition spec None
I0512 23:45:21.833875 140143678494720 utils.py:1372] Variable param_states/decoder/layers_7/self_attention/query/kernel/v_col                  size 3072         shape (3072,)                                  partition spec None
I0512 23:45:21.833914 140143678494720 utils.py:1372] Variable param_states/decoder/layers_7/self_attention/query/kernel/v_row                  size 2048         shape (2048,)                                  partition spec None
I0512 23:45:21.833952 140143678494720 utils.py:1372] Variable param_states/decoder/layers_7/self_attention/value/kernel/m                      size 1            shape (1,)                                     partition spec None
I0512 23:45:21.833990 140143678494720 utils.py:1372] Variable param_states/decoder/layers_7/self_attention/value/kernel/v                      size 1            shape (1,)                                     partition spec None
I0512 23:45:21.834029 140143678494720 utils.py:1372] Variable param_states/decoder/layers_7/self_attention/value/kernel/v_col                  size 3072         shape (3072,)                                  partition spec None
I0512 23:45:21.834068 140143678494720 utils.py:1372] Variable param_states/decoder/layers_7/self_attention/value/kernel/v_row                  size 2048         shape (2048,)                                  partition spec None
I0512 23:45:21.834106 140143678494720 utils.py:1372] Variable param_states/decoder/layers_8/mlp/wi_0/kernel/m                                  size 1            shape (1,)                                     partition spec None
I0512 23:45:21.834145 140143678494720 utils.py:1372] Variable param_states/decoder/layers_8/mlp/wi_0/kernel/v                                  size 1            shape (1,)                                     partition spec None
I0512 23:45:21.834218 140143678494720 utils.py:1372] Variable param_states/decoder/layers_8/mlp/wi_0/kernel/v_col                              size 16384        shape (16384,)                                 partition spec None
I0512 23:45:21.834264 140143678494720 utils.py:1372] Variable param_states/decoder/layers_8/mlp/wi_0/kernel/v_row                              size 2048         shape (2048,)                                  partition spec None
I0512 23:45:21.834305 140143678494720 utils.py:1372] Variable param_states/decoder/layers_8/mlp/wi_1/kernel/m                                  size 1            shape (1,)                                     partition spec None
I0512 23:45:21.834345 140143678494720 utils.py:1372] Variable param_states/decoder/layers_8/mlp/wi_1/kernel/v                                  size 1            shape (1,)                                     partition spec None
I0512 23:45:21.834384 140143678494720 utils.py:1372] Variable param_states/decoder/layers_8/mlp/wi_1/kernel/v_col                              size 8192         shape (8192,)                                  partition spec None
I0512 23:45:21.834422 140143678494720 utils.py:1372] Variable param_states/decoder/layers_8/mlp/wi_1/kernel/v_row                              size 2048         shape (2048,)                                  partition spec None
I0512 23:45:21.834460 140143678494720 utils.py:1372] Variable param_states/decoder/layers_8/mlp/wo/kernel/m                                    size 1            shape (1,)                                     partition spec None
I0512 23:45:21.834499 140143678494720 utils.py:1372] Variable param_states/decoder/layers_8/mlp/wo/kernel/v                                    size 1            shape (1,)                                     partition spec None
I0512 23:45:21.834537 140143678494720 utils.py:1372] Variable param_states/decoder/layers_8/mlp/wo/kernel/v_col                                size 8192         shape (8192,)                                  partition spec None
I0512 23:45:21.834583 140143678494720 utils.py:1372] Variable param_states/decoder/layers_8/mlp/wo/kernel/v_row                                size 2048         shape (2048,)                                  partition spec None
I0512 23:45:21.834621 140143678494720 utils.py:1372] Variable param_states/decoder/layers_8/pre_mlp_layer_norm/scale/m                         size 1            shape (1,)                                     partition spec None
I0512 23:45:21.834663 140143678494720 utils.py:1372] Variable param_states/decoder/layers_8/pre_mlp_layer_norm/scale/v                         size 2048         shape (embed=2048)                             partition spec ('model',)
I0512 23:45:21.834703 140143678494720 utils.py:1372] Variable param_states/decoder/layers_8/pre_mlp_layer_norm/scale/v_col                     size 1            shape (1,)                                     partition spec None
I0512 23:45:21.834742 140143678494720 utils.py:1372] Variable param_states/decoder/layers_8/pre_mlp_layer_norm/scale/v_row                     size 1            shape (1,)                                     partition spec None
I0512 23:45:21.834780 140143678494720 utils.py:1372] Variable param_states/decoder/layers_8/pre_self_attention_layer_norm/scale/m              size 1            shape (1,)                                     partition spec None
I0512 23:45:21.834820 140143678494720 utils.py:1372] Variable param_states/decoder/layers_8/pre_self_attention_layer_norm/scale/v              size 2048         shape (embed=2048)                             partition spec ('model',)
I0512 23:45:21.834860 140143678494720 utils.py:1372] Variable param_states/decoder/layers_8/pre_self_attention_layer_norm/scale/v_col          size 1            shape (1,)                                     partition spec None
I0512 23:45:21.834899 140143678494720 utils.py:1372] Variable param_states/decoder/layers_8/pre_self_attention_layer_norm/scale/v_row          size 1            shape (1,)                                     partition spec None
I0512 23:45:21.834937 140143678494720 utils.py:1372] Variable param_states/decoder/layers_8/self_attention/key/kernel/m                        size 1            shape (1,)                                     partition spec None
I0512 23:45:21.834975 140143678494720 utils.py:1372] Variable param_states/decoder/layers_8/self_attention/key/kernel/v                        size 1            shape (1,)                                     partition spec None
I0512 23:45:21.835014 140143678494720 utils.py:1372] Variable param_states/decoder/layers_8/self_attention/key/kernel/v_col                    size 3072         shape (3072,)                                  partition spec None
I0512 23:45:21.835052 140143678494720 utils.py:1372] Variable param_states/decoder/layers_8/self_attention/key/kernel/v_row                    size 2048         shape (2048,)                                  partition spec None
I0512 23:45:21.835090 140143678494720 utils.py:1372] Variable param_states/decoder/layers_8/self_attention/out/kernel/m                        size 1            shape (1,)                                     partition spec None
I0512 23:45:21.835129 140143678494720 utils.py:1372] Variable param_states/decoder/layers_8/self_attention/out/kernel/v                        size 1            shape (1,)                                     partition spec None
I0512 23:45:21.835167 140143678494720 utils.py:1372] Variable param_states/decoder/layers_8/self_attention/out/kernel/v_col                    size 3072         shape (3072,)                                  partition spec None
I0512 23:45:21.835205 140143678494720 utils.py:1372] Variable param_states/decoder/layers_8/self_attention/out/kernel/v_row                    size 2048         shape (2048,)                                  partition spec None
I0512 23:45:21.835244 140143678494720 utils.py:1372] Variable param_states/decoder/layers_8/self_attention/query/kernel/m                      size 1            shape (1,)                                     partition spec None
I0512 23:45:21.835283 140143678494720 utils.py:1372] Variable param_states/decoder/layers_8/self_attention/query/kernel/v                      size 1            shape (1,)                                     partition spec None
I0512 23:45:21.835323 140143678494720 utils.py:1372] Variable param_states/decoder/layers_8/self_attention/query/kernel/v_col                  size 3072         shape (3072,)                                  partition spec None
I0512 23:45:21.835363 140143678494720 utils.py:1372] Variable param_states/decoder/layers_8/self_attention/query/kernel/v_row                  size 2048         shape (2048,)                                  partition spec None
I0512 23:45:21.835402 140143678494720 utils.py:1372] Variable param_states/decoder/layers_8/self_attention/value/kernel/m                      size 1            shape (1,)                                     partition spec None
I0512 23:45:21.835440 140143678494720 utils.py:1372] Variable param_states/decoder/layers_8/self_attention/value/kernel/v                      size 1            shape (1,)                                     partition spec None
I0512 23:45:21.835478 140143678494720 utils.py:1372] Variable param_states/decoder/layers_8/self_attention/value/kernel/v_col                  size 3072         shape (3072,)                                  partition spec None
I0512 23:45:21.835517 140143678494720 utils.py:1372] Variable param_states/decoder/layers_8/self_attention/value/kernel/v_row                  size 2048         shape (2048,)                                  partition spec None
I0512 23:45:21.835561 140143678494720 utils.py:1372] Variable param_states/decoder/layers_9/mlp/wi_0/kernel/m                                  size 1            shape (1,)                                     partition spec None
I0512 23:45:21.835602 140143678494720 utils.py:1372] Variable param_states/decoder/layers_9/mlp/wi_0/kernel/v                                  size 1            shape (1,)                                     partition spec None
I0512 23:45:21.835641 140143678494720 utils.py:1372] Variable param_states/decoder/layers_9/mlp/wi_0/kernel/v_col                              size 16384        shape (16384,)                                 partition spec None
I0512 23:45:21.835680 140143678494720 utils.py:1372] Variable param_states/decoder/layers_9/mlp/wi_0/kernel/v_row                              size 2048         shape (2048,)                                  partition spec None
I0512 23:45:21.835718 140143678494720 utils.py:1372] Variable param_states/decoder/layers_9/mlp/wi_1/kernel/m                                  size 1            shape (1,)                                     partition spec None
I0512 23:45:21.835757 140143678494720 utils.py:1372] Variable param_states/decoder/layers_9/mlp/wi_1/kernel/v                                  size 1            shape (1,)                                     partition spec None
I0512 23:45:21.835795 140143678494720 utils.py:1372] Variable param_states/decoder/layers_9/mlp/wi_1/kernel/v_col                              size 8192         shape (8192,)                                  partition spec None
I0512 23:45:21.835833 140143678494720 utils.py:1372] Variable param_states/decoder/layers_9/mlp/wi_1/kernel/v_row                              size 2048         shape (2048,)                                  partition spec None
I0512 23:45:21.835871 140143678494720 utils.py:1372] Variable param_states/decoder/layers_9/mlp/wo/kernel/m                                    size 1            shape (1,)                                     partition spec None
I0512 23:45:21.835910 140143678494720 utils.py:1372] Variable param_states/decoder/layers_9/mlp/wo/kernel/v                                    size 1            shape (1,)                                     partition spec None
I0512 23:45:21.835948 140143678494720 utils.py:1372] Variable param_states/decoder/layers_9/mlp/wo/kernel/v_col                                size 8192         shape (8192,)                                  partition spec None
I0512 23:45:21.835986 140143678494720 utils.py:1372] Variable param_states/decoder/layers_9/mlp/wo/kernel/v_row                                size 2048         shape (2048,)                                  partition spec None
I0512 23:45:21.836024 140143678494720 utils.py:1372] Variable param_states/decoder/layers_9/pre_mlp_layer_norm/scale/m                         size 1            shape (1,)                                     partition spec None
I0512 23:45:21.836079 140143678494720 utils.py:1372] Variable param_states/decoder/layers_9/pre_mlp_layer_norm/scale/v                         size 2048         shape (embed=2048)                             partition spec ('model',)
I0512 23:45:21.836123 140143678494720 utils.py:1372] Variable param_states/decoder/layers_9/pre_mlp_layer_norm/scale/v_col                     size 1            shape (1,)                                     partition spec None
I0512 23:45:21.836162 140143678494720 utils.py:1372] Variable param_states/decoder/layers_9/pre_mlp_layer_norm/scale/v_row                     size 1            shape (1,)                                     partition spec None
I0512 23:45:21.836200 140143678494720 utils.py:1372] Variable param_states/decoder/layers_9/pre_self_attention_layer_norm/scale/m              size 1            shape (1,)                                     partition spec None
I0512 23:45:21.836241 140143678494720 utils.py:1372] Variable param_states/decoder/layers_9/pre_self_attention_layer_norm/scale/v              size 2048         shape (embed=2048)                             partition spec ('model',)
I0512 23:45:21.836281 140143678494720 utils.py:1372] Variable param_states/decoder/layers_9/pre_self_attention_layer_norm/scale/v_col          size 1            shape (1,)                                     partition spec None
I0512 23:45:21.836323 140143678494720 utils.py:1372] Variable param_states/decoder/layers_9/pre_self_attention_layer_norm/scale/v_row          size 1            shape (1,)                                     partition spec None
I0512 23:45:21.836361 140143678494720 utils.py:1372] Variable param_states/decoder/layers_9/self_attention/key/kernel/m                        size 1            shape (1,)                                     partition spec None
I0512 23:45:21.836400 140143678494720 utils.py:1372] Variable param_states/decoder/layers_9/self_attention/key/kernel/v                        size 1            shape (1,)                                     partition spec None
I0512 23:45:21.836439 140143678494720 utils.py:1372] Variable param_states/decoder/layers_9/self_attention/key/kernel/v_col                    size 3072         shape (3072,)                                  partition spec None
I0512 23:45:21.836478 140143678494720 utils.py:1372] Variable param_states/decoder/layers_9/self_attention/key/kernel/v_row                    size 2048         shape (2048,)                                  partition spec None
I0512 23:45:21.836516 140143678494720 utils.py:1372] Variable param_states/decoder/layers_9/self_attention/out/kernel/m                        size 1            shape (1,)                                     partition spec None
I0512 23:45:21.836583 140143678494720 utils.py:1372] Variable param_states/decoder/layers_9/self_attention/out/kernel/v                        size 1            shape (1,)                                     partition spec None
I0512 23:45:21.836625 140143678494720 utils.py:1372] Variable param_states/decoder/layers_9/self_attention/out/kernel/v_col                    size 3072         shape (3072,)                                  partition spec None
I0512 23:45:21.836664 140143678494720 utils.py:1372] Variable param_states/decoder/layers_9/self_attention/out/kernel/v_row                    size 2048         shape (2048,)                                  partition spec None
I0512 23:45:21.836703 140143678494720 utils.py:1372] Variable param_states/decoder/layers_9/self_attention/query/kernel/m                      size 1            shape (1,)                                     partition spec None
I0512 23:45:21.836742 140143678494720 utils.py:1372] Variable param_states/decoder/layers_9/self_attention/query/kernel/v                      size 1            shape (1,)                                     partition spec None
I0512 23:45:21.836781 140143678494720 utils.py:1372] Variable param_states/decoder/layers_9/self_attention/query/kernel/v_col                  size 3072         shape (3072,)                                  partition spec None
I0512 23:45:21.836819 140143678494720 utils.py:1372] Variable param_states/decoder/layers_9/self_attention/query/kernel/v_row                  size 2048         shape (2048,)                                  partition spec None
I0512 23:45:21.836858 140143678494720 utils.py:1372] Variable param_states/decoder/layers_9/self_attention/value/kernel/m                      size 1            shape (1,)                                     partition spec None
I0512 23:45:21.836896 140143678494720 utils.py:1372] Variable param_states/decoder/layers_9/self_attention/value/kernel/v                      size 1            shape (1,)                                     partition spec None
I0512 23:45:21.836935 140143678494720 utils.py:1372] Variable param_states/decoder/layers_9/self_attention/value/kernel/v_col                  size 3072         shape (3072,)                                  partition spec None
I0512 23:45:21.836973 140143678494720 utils.py:1372] Variable param_states/decoder/layers_9/self_attention/value/kernel/v_row                  size 2048         shape (2048,)                                  partition spec None
I0512 23:45:21.837012 140143678494720 utils.py:1372] Variable param_states/decoder/logits_dense/kernel/m                                       size 1            shape (1,)                                     partition spec None
I0512 23:45:21.837051 140143678494720 utils.py:1372] Variable param_states/decoder/logits_dense/kernel/v                                       size 1            shape (1,)                                     partition spec None
I0512 23:45:21.837090 140143678494720 utils.py:1372] Variable param_states/decoder/logits_dense/kernel/v_col                                   size 256384       shape (256384,)                                partition spec None
I0512 23:45:21.837129 140143678494720 utils.py:1372] Variable param_states/decoder/logits_dense/kernel/v_row                                   size 2048         shape (2048,)                                  partition spec None
I0512 23:45:21.837168 140143678494720 utils.py:1372] Variable param_states/token_embedder/embedding/m                                          size 1            shape (1,)                                     partition spec None
I0512 23:45:21.837209 140143678494720 utils.py:1372] Variable param_states/token_embedder/embedding/v                                          size 1            shape (1,)                                     partition spec None
I0512 23:45:21.837248 140143678494720 utils.py:1372] Variable param_states/token_embedder/embedding/v_col                                      size 256384       shape (256384,)                                partition spec None
I0512 23:45:21.837287 140143678494720 utils.py:1372] Variable param_states/token_embedder/embedding/v_row                                      size 2048         shape (2048,)                                  partition spec None
I0512 23:45:21.837328 140143678494720 utils.py:1372] Variable step                                                                             size 1            shape ()                                       partition spec None
I0512 23:45:22.175370 139856202532864 train.py:421] Initialize/restore complete (11.58 seconds).
I0512 23:45:22.177100 139856202532864 train.py:573] Saving checkpoint before the training loop starts.
I0512 23:45:22.247447 140055078274624 logging_writer.py:64] [10000] collection=train Got texts: {'config': "    from __gin__ import dynamic_registration\n    import __main__ as train_script\n    from flax import linen\n    import flaxformer\n    from flaxformer.architectures.moe import moe_architecture\n    from flaxformer.architectures.moe import moe_enums\n    from flaxformer.architectures.moe import moe_layers\n    from flaxformer.architectures.moe import routing\n    from flaxformer.architectures.t5 import t5_architecture\n    from flaxformer.components.attention import dense_attention\n    from flaxformer.components.attention import memory_efficient_attention\n    from flaxformer.components import dense\n    from flaxformer.components import embedding\n    from flaxformer.components import layer_norm\n    from gin import config\n    import seqio\n    import t5.data.mixtures\n    from t5x import adafactor\n    from t5x.contrib.moe import adafactor_utils\n    from t5x.contrib.moe import models\n    from t5x.contrib.moe import partitioning as moe_partitioning\n    from t5x.contrib.moe import trainer as moe_trainer\n    from t5x import gin_utils\n    from t5x import partitioning\n    from t5x import trainer\n    from t5x import utils\n    \n#### Macros:\n\n    ACTIVATION_DTYPE = 'bfloat16'\n    ACTIVATION_PARTITIONING_DIMS = 1\n    ARCHITECTURE = @t5_architecture.DecoderOnly()\n    AUX_LOSS_FACTOR = 0.01\n    BATCH_SIZE = 384\n    BIAS_INIT = @bias_init/linen.initializers.normal()\n    DECODER_SPARSE_LAYOUT = %flaxformer.architectures.moe.moe_enums.LayerLayout.MIXED\n    DROPOUT_FACTORY = @dropout_factory/linen.Dropout\n    DROPOUT_RATE = 0.0\n    EMBED_DIM = 2048\n    EVAL_EXPERT_CAPACITY_FACTOR = 2.0\n    EXPERT_DROPOUT_RATE = %DROPOUT_RATE\n    EXPERT_MLP_DIM = %MLP_DIM\n    GROUP_SIZE = 4096\n    HEAD_DIM = 128\n    JITTER_NOISE = 0.0\n    LABEL_SMOOTHING = 0.0\n    LOSS_NORMALIZING_FACTOR = 'NUM_REAL_TARGET_TOKENS'\n    MIXTURE_OR_TASK_MODULE = None\n    MIXTURE_OR_TASK_NAME = 'mix_ul2_test'\n    MLP_DIM = 8192\n    MODEL = @models.MoeDecoderOnlyModel()\n    MODEL_DIR = 'gs://rosinality-tpu-bucket/openmoe_8b_ul2/training'\n    MODEL_PARALLEL_SUBMESH = None\n    MOE_TRUNCATED_DTYPE = 'bfloat16'\n    NUM_DECODER_LAYERS = 24\n    NUM_DECODER_SPARSE_LAYERS = 4\n    NUM_EMBEDDINGS = 256384\n    NUM_EXPERT_PARTITIONS = 8\n    NUM_EXPERTS = 8\n    NUM_HEADS = 24\n    NUM_MODEL_PARTITIONS = 4\n    NUM_SELECTED_EXPERTS = 2\n    OPTIMIZER = @adafactor.Adafactor()\n    RANDOM_SEED = None\n    ROUTER_Z_LOSS_FACTOR = 0.0001\n    SCALE = 0.1\n    SHUFFLE_TRAIN_EXAMPLES = True\n    TASK_FEATURE_LENGTHS = {'inputs': 1024, 'targets': 1024}\n    TRAIN_EXPERT_CAPACITY_FACTOR = 1.25\n    TRAIN_STEPS = 500000\n    USE_CACHED_TASKS = False\n    USE_HARDWARE_RNG = False\n    VOCABULARY = @seqio.SentencePieceVocabulary()\n    Z_LOSS = 0.0001\n    \n#### Parameters for adafactor.Adafactor:\n\n    adafactor.Adafactor.decay_rate = 0.8\n    adafactor.Adafactor.logical_factor_rules = @adafactor_utils.logical_factor_rules()\n    adafactor.Adafactor.step_offset = 0\n    \n#### Parameters for utils.CheckpointConfig:\n\n    utils.CheckpointConfig.restore = @utils.RestoreCheckpointConfig()\n    utils.CheckpointConfig.save = @utils.SaveCheckpointConfig()\n    \n#### Parameters for moe_partitioning.compute_num_model_partitions:\n\n    moe_partitioning.compute_num_model_partitions.model_parallel_submesh = \\\n        %MODEL_PARALLEL_SUBMESH\n    moe_partitioning.compute_num_model_partitions.num_model_partitions = \\\n        %NUM_MODEL_PARTITIONS\n    \n#### Parameters for utils.create_learning_rate_scheduler:\n\n    utils.create_learning_rate_scheduler.base_learning_rate = 1.0\n    utils.create_learning_rate_scheduler.factors = 'constant * rsqrt_decay'\n    utils.create_learning_rate_scheduler.warmup_steps = 10000\n    \n#### Parameters for train/utils.DatasetConfig:\n\n    train/utils.DatasetConfig.batch_size = %BATCH_SIZE\n    train/utils.DatasetConfig.mixture_or_task_name = %MIXTURE_OR_TASK_NAME\n    train/utils.DatasetConfig.module = %MIXTURE_OR_TASK_MODULE\n    train/utils.DatasetConfig.pack = True\n    train/utils.DatasetConfig.seed = None\n    train/utils.DatasetConfig.shuffle = %SHUFFLE_TRAIN_EXAMPLES\n    train/utils.DatasetConfig.split = 'train'\n    train/utils.DatasetConfig.task_feature_lengths = %TASK_FEATURE_LENGTHS\n    train/utils.DatasetConfig.use_cached = %USE_CACHED_TASKS\n    \n#### Parameters for train_eval/utils.DatasetConfig:\n\n    train_eval/utils.DatasetConfig.batch_size = 128\n    train_eval/utils.DatasetConfig.mixture_or_task_name = %MIXTURE_OR_TASK_NAME\n    train_eval/utils.DatasetConfig.module = %MIXTURE_OR_TASK_MODULE\n    train_eval/utils.DatasetConfig.pack = True\n    train_eval/utils.DatasetConfig.seed = 42\n    train_eval/utils.DatasetConfig.shuffle = False\n    train_eval/utils.DatasetConfig.split = 'validation'\n    train_eval/utils.DatasetConfig.task_feature_lengths = %TASK_FEATURE_LENGTHS\n    train_eval/utils.DatasetConfig.use_cached = %USE_CACHED_TASKS\n    \n#### Parameters for t5_architecture.DecoderLayer:\n\n    t5_architecture.DecoderLayer.activation_partitioning_dims = \\\n        %ACTIVATION_PARTITIONING_DIMS\n    t5_architecture.DecoderLayer.dropout_factory = %DROPOUT_FACTORY\n    t5_architecture.DecoderLayer.encoder_decoder_attention = None\n    t5_architecture.DecoderLayer.layer_norm_factory = @layer_norm.T5LayerNorm\n    t5_architecture.DecoderLayer.mlp = @dense.MlpBlock()\n    t5_architecture.DecoderLayer.scanned = False\n    t5_architecture.DecoderLayer.self_attention = \\\n        @dense_attention.MultiHeadDotProductAttention()\n    \n#### Parameters for t5_architecture.DecoderOnly:\n\n    t5_architecture.DecoderOnly.decoder_factory = @moe_architecture.SparseDecoder\n    t5_architecture.DecoderOnly.dtype = %ACTIVATION_DTYPE\n    t5_architecture.DecoderOnly.shared_token_embedder_factory = @embedding.Embed\n    \n#### Parameters for output_logits/dense.DenseGeneral:\n\n    output_logits/dense.DenseGeneral.bias_init = %BIAS_INIT\n    output_logits/dense.DenseGeneral.dtype = 'float32'\n    output_logits/dense.DenseGeneral.features = %NUM_EMBEDDINGS\n    output_logits/dense.DenseGeneral.kernel_axis_names = ['embed', 'vocab']\n    output_logits/dense.DenseGeneral.kernel_init = \\\n        @output_logits_kernel_init/linen.initializers.variance_scaling()\n    output_logits/dense.DenseGeneral.use_bias = False\n    \n#### Parameters for dropout_factory/linen.Dropout:\n\n    dropout_factory/linen.Dropout.broadcast_dims = (-2,)\n    dropout_factory/linen.Dropout.rate = %DROPOUT_RATE\n    \n#### Parameters for embedding.Embed:\n\n    embedding.Embed.attend_dtype = 'float32'\n    embedding.Embed.cast_input_dtype = 'int32'\n    embedding.Embed.dtype = %ACTIVATION_DTYPE\n    embedding.Embed.embedding_init = @token_embedder_init/linen.initializers.normal()\n    embedding.Embed.features = %EMBED_DIM\n    embedding.Embed.name = 'token_embedder'\n    embedding.Embed.num_embeddings = %NUM_EMBEDDINGS\n    embedding.Embed.one_hot = True\n    \n#### Parameters for dense.MlpBlock:\n\n    dense.MlpBlock.activations = ('swiglu', 'linear')\n    dense.MlpBlock.bias_init = %BIAS_INIT\n    dense.MlpBlock.dtype = %ACTIVATION_DTYPE\n    dense.MlpBlock.final_dropout_rate = 0\n    dense.MlpBlock.input_axis_name = 'mlp_embed'\n    dense.MlpBlock.intermediate_dim = %MLP_DIM\n    dense.MlpBlock.intermediate_dropout_rate = %DROPOUT_RATE\n    dense.MlpBlock.kernel_init = @mlp_kernel_init/linen.initializers.variance_scaling()\n    dense.MlpBlock.output_axis_name = 'mlp_embed'\n    dense.MlpBlock.use_bias = False\n    \n#### Parameters for expert/dense.MlpBlock:\n\n    expert/dense.MlpBlock.activation_partitioning_dims = 1\n    expert/dense.MlpBlock.activations = ('swiglu', 'linear')\n    expert/dense.MlpBlock.bias_init = %BIAS_INIT\n    expert/dense.MlpBlock.data_sharding_constraints = ('expert_replicas', 'mlp')\n    expert/dense.MlpBlock.dtype = %MOE_TRUNCATED_DTYPE\n    expert/dense.MlpBlock.final_dropout_rate = 0.0\n    expert/dense.MlpBlock.input_axis_name = 'mlp_embed'\n    expert/dense.MlpBlock.intermediate_axis_name = 'expert_mlp'\n    expert/dense.MlpBlock.intermediate_dim = %EXPERT_MLP_DIM\n    expert/dense.MlpBlock.intermediate_dropout_rate = %EXPERT_DROPOUT_RATE\n    expert/dense.MlpBlock.kernel_init = \\\n        @expert_kernel_init/linen.initializers.variance_scaling()\n    expert/dense.MlpBlock.output_axis_name = 'mlp_embed'\n    expert/dense.MlpBlock.use_bias = False\n    \n#### Parameters for models.MoeDecoderOnlyModel:\n\n    models.MoeDecoderOnlyModel.aux_loss_factor = %AUX_LOSS_FACTOR\n    models.MoeDecoderOnlyModel.inputs_bidirectional_attention = True\n    models.MoeDecoderOnlyModel.label_smoothing = %LABEL_SMOOTHING\n    models.MoeDecoderOnlyModel.loss_normalizing_factor = %LOSS_NORMALIZING_FACTOR\n    models.MoeDecoderOnlyModel.module = %ARCHITECTURE\n    models.MoeDecoderOnlyModel.optimizer_def = %OPTIMIZER\n    models.MoeDecoderOnlyModel.router_z_loss_factor = %ROUTER_Z_LOSS_FACTOR\n    models.MoeDecoderOnlyModel.vocabulary = %VOCABULARY\n    models.MoeDecoderOnlyModel.z_loss = %Z_LOSS\n    \n#### Parameters for moe_layers.MoeLayer:\n\n    moe_layers.MoeLayer.dtype = %MOE_TRUNCATED_DTYPE\n    moe_layers.MoeLayer.eval_capacity_factor = %EVAL_EXPERT_CAPACITY_FACTOR\n    moe_layers.MoeLayer.expert = @expert/dense.MlpBlock()\n    moe_layers.MoeLayer.max_group_size = %GROUP_SIZE\n    moe_layers.MoeLayer.num_expert_partitions = %NUM_EXPERT_PARTITIONS\n    moe_layers.MoeLayer.num_experts = %NUM_EXPERTS\n    moe_layers.MoeLayer.num_model_partitions = \\\n        @moe_partitioning.compute_num_model_partitions()\n    moe_layers.MoeLayer.train_capacity_factor = %TRAIN_EXPERT_CAPACITY_FACTOR\n    \n#### Parameters for sparse_decoder/moe_layers.MoeLayer:\n\n    sparse_decoder/moe_layers.MoeLayer.router = \\\n        @sparse_decoder/routing.TokensChooseMaskedRouter()\n    \n#### Parameters for moe_partitioning.MoePjitPartitioner:\n\n    moe_partitioning.MoePjitPartitioner.model_parallel_submesh = \\\n        %MODEL_PARALLEL_SUBMESH\n    moe_partitioning.MoePjitPartitioner.num_expert_partitions = %NUM_EXPERT_PARTITIONS\n    moe_partitioning.MoePjitPartitioner.num_partitions = %NUM_MODEL_PARTITIONS\n    \n#### Parameters for moe_trainer.MoeTrainer:\n\n    moe_trainer.MoeTrainer.learning_rate_fn = @utils.create_learning_rate_scheduler()\n    moe_trainer.MoeTrainer.num_expert_partitions = %NUM_EXPERT_PARTITIONS\n    moe_trainer.MoeTrainer.num_microbatches = 8\n    \n#### Parameters for dense_attention.MultiHeadDotProductAttention:\n\n    dense_attention.MultiHeadDotProductAttention.bias_init = %BIAS_INIT\n    dense_attention.MultiHeadDotProductAttention.broadcast_dropout = True\n    dense_attention.MultiHeadDotProductAttention.dropout_rate = %DROPOUT_RATE\n    dense_attention.MultiHeadDotProductAttention.dtype = %ACTIVATION_DTYPE\n    dense_attention.MultiHeadDotProductAttention.head_dim = %HEAD_DIM\n    dense_attention.MultiHeadDotProductAttention.kernel_init = \\\n        @attention_kernel_init/linen.initializers.variance_scaling()\n    dense_attention.MultiHeadDotProductAttention.num_heads = %NUM_HEADS\n    dense_attention.MultiHeadDotProductAttention.use_bias = False\n    dense_attention.MultiHeadDotProductAttention.use_rotary_embedding = True\n    \n#### Parameters for bias_init/linen.initializers.normal:\n\n    bias_init/linen.initializers.normal.stddev = 1e-06\n    \n#### Parameters for router_init/linen.initializers.normal:\n\n    router_init/linen.initializers.normal.stddev = 0.02\n    \n#### Parameters for token_embedder_init/linen.initializers.normal:\n\n    token_embedder_init/linen.initializers.normal.stddev = 1.0\n    \n#### Parameters for partitioning.PjitPartitioner:\n\n    partitioning.PjitPartitioner.logical_axis_rules = \\\n        @partitioning.standard_logical_axis_rules()\n    partitioning.PjitPartitioner.model_parallel_submesh = None\n    partitioning.PjitPartitioner.num_partitions = 1\n    \n#### Parameters for utils.RestoreCheckpointConfig:\n\n    utils.RestoreCheckpointConfig.path = []\n    \n#### Parameters for routing.RouterWeights:\n\n    routing.RouterWeights.bias_init = %BIAS_INIT\n    routing.RouterWeights.dtype = 'float32'\n    routing.RouterWeights.kernel_init = @router_init/linen.initializers.normal()\n    routing.RouterWeights.use_bias = False\n    \n#### Parameters for utils.SaveCheckpointConfig:\n\n    utils.SaveCheckpointConfig.dtype = 'float32'\n    utils.SaveCheckpointConfig.keep = 20\n    utils.SaveCheckpointConfig.period = 2500\n    utils.SaveCheckpointConfig.save_dataset = False\n    \n#### Parameters for seqio.SentencePieceVocabulary:\n\n    seqio.SentencePieceVocabulary.extra_ids = 300\n    seqio.SentencePieceVocabulary.sentencepiece_model_file = \\\n        'gs://rosinality-tpu-bucket/sentencepiece.model'\n    \n#### Parameters for moe_architecture.SparseDecoder:\n\n    moe_architecture.SparseDecoder.dropout_factory = %DROPOUT_FACTORY\n    moe_architecture.SparseDecoder.dtype = %ACTIVATION_DTYPE\n    moe_architecture.SparseDecoder.layer_factory = @t5_architecture.DecoderLayer\n    moe_architecture.SparseDecoder.layer_norm_factory = @layer_norm.T5LayerNorm\n    moe_architecture.SparseDecoder.num_layers = %NUM_DECODER_LAYERS\n    moe_architecture.SparseDecoder.num_sparse_layers = %NUM_DECODER_SPARSE_LAYERS\n    moe_architecture.SparseDecoder.output_logits_factory = \\\n        @output_logits/dense.DenseGeneral\n    moe_architecture.SparseDecoder.sparse_layer_factory = \\\n        @moe_architecture.SparseDecoderLayer\n    moe_architecture.SparseDecoder.sparse_layout = %DECODER_SPARSE_LAYOUT\n    \n#### Parameters for moe_architecture.SparseDecoderLayer:\n\n    moe_architecture.SparseDecoderLayer.activation_partitioning_dims = \\\n        %ACTIVATION_PARTITIONING_DIMS\n    moe_architecture.SparseDecoderLayer.dropout_factory = %DROPOUT_FACTORY\n    moe_architecture.SparseDecoderLayer.encoder_decoder_attention = None\n    moe_architecture.SparseDecoderLayer.extra_mlp = @dense.MlpBlock()\n    moe_architecture.SparseDecoderLayer.layer_norm_factory = @layer_norm.T5LayerNorm\n    moe_architecture.SparseDecoderLayer.mlp = @sparse_decoder/moe_layers.MoeLayer()\n    moe_architecture.SparseDecoderLayer.scanned = False\n    moe_architecture.SparseDecoderLayer.self_attention = \\\n        @dense_attention.MultiHeadDotProductAttention()\n    \n#### Parameters for layer_norm.T5LayerNorm:\n\n    layer_norm.T5LayerNorm.dtype = %ACTIVATION_DTYPE\n    \n#### Parameters for routing.TokensChooseMaskedRouter:\n\n    routing.TokensChooseMaskedRouter.dtype = 'float32'\n    routing.TokensChooseMaskedRouter.ignore_padding_tokens = False\n    routing.TokensChooseMaskedRouter.jitter_noise = %JITTER_NOISE\n    routing.TokensChooseMaskedRouter.num_selected_experts = %NUM_SELECTED_EXPERTS\n    routing.TokensChooseMaskedRouter.router_weights = @routing.RouterWeights()\n    \n#### Parameters for sparse_decoder/routing.TokensChooseMaskedRouter:\n\n    sparse_decoder/routing.TokensChooseMaskedRouter.batch_prioritized_routing = False\n    \n#### Parameters for sparse_encoder/routing.TokensChooseMaskedRouter:\n\n    sparse_encoder/routing.TokensChooseMaskedRouter.batch_prioritized_routing = False\n    \n#### Parameters for train_script.train:\n\n    train_script.train.checkpoint_cfg = @utils.CheckpointConfig()\n    train_script.train.eval_period = 2500\n    train_script.train.eval_steps = 20\n    train_script.train.infer_eval_dataset_cfg = None\n    train_script.train.model = %MODEL\n    train_script.train.model_dir = %MODEL_DIR\n    train_script.train.partitioner = @moe_partitioning.MoePjitPartitioner()\n    train_script.train.random_seed = %RANDOM_SEED\n    train_script.train.stats_period = 10\n    train_script.train.summarize_config_fn = @gin_utils.summarize_gin_config\n    train_script.train.total_steps = %TRAIN_STEPS\n    train_script.train.train_dataset_cfg = @train/utils.DatasetConfig()\n    train_script.train.train_eval_dataset_cfg = None\n    train_script.train.trainer_cls = @moe_trainer.MoeTrainer\n    train_script.train.use_hardware_rng = %USE_HARDWARE_RNG\n    \n#### Parameters for trainer.Trainer:\n\n    trainer.Trainer.learning_rate_fn = @utils.create_learning_rate_scheduler()\n    trainer.Trainer.num_microbatches = None\n    \n#### Parameters for attention_kernel_init/linen.initializers.variance_scaling:\n\n    attention_kernel_init/linen.initializers.variance_scaling.distribution = 'normal'\n    attention_kernel_init/linen.initializers.variance_scaling.mode = 'fan_in'\n    attention_kernel_init/linen.initializers.variance_scaling.scale = %SCALE\n    \n#### Parameters for expert_kernel_init/linen.initializers.variance_scaling:\n\n    expert_kernel_init/linen.initializers.variance_scaling.distribution = 'normal'\n    expert_kernel_init/linen.initializers.variance_scaling.mode = 'fan_in'\n    expert_kernel_init/linen.initializers.variance_scaling.scale = %SCALE\n    \n#### Parameters for mlp_kernel_init/linen.initializers.variance_scaling:\n\n    mlp_kernel_init/linen.initializers.variance_scaling.distribution = \\\n        'truncated_normal'\n    mlp_kernel_init/linen.initializers.variance_scaling.mode = 'fan_in'\n    mlp_kernel_init/linen.initializers.variance_scaling.scale = %SCALE\n    \n#### Parameters for output_logits_kernel_init/linen.initializers.variance_scaling:\n\n    output_logits_kernel_init/linen.initializers.variance_scaling.distribution = \\\n        'truncated_normal'\n    output_logits_kernel_init/linen.initializers.variance_scaling.mode = 'fan_in'\n    output_logits_kernel_init/linen.initializers.variance_scaling.scale = %SCALE"}.
I0512 23:45:22.385591 140055069881920 logging_writer.py:48] [10000] collection=train timing/init_or_restore_seconds=11.5677
I0512 23:45:22.387177 140143678494720 train.py:573] Saving checkpoint before the training loop starts.
I0512 23:45:22.433372 140193652815872 train.py:421] Initialize/restore complete (11.57 seconds).
I0512 23:45:22.435009 140193652815872 train.py:573] Saving checkpoint before the training loop starts.
I0512 23:45:22.744964 140260698863616 train.py:421] Initialize/restore complete (11.71 seconds).
I0512 23:45:22.746680 140260698863616 train.py:573] Saving checkpoint before the training loop starts.
I0512 23:45:22.842751 139666521167872 checkpoints.py:781] Skipping save checkpoint for step 10000 (directory gs://rosinality-tpu-bucket/openmoe_8b_ul2/training/checkpoint_10000 already exists)
I0512 23:45:22.842944 139666521167872 train.py:587] Starting training loop.
I0512 23:45:22.842989 139666521167872 train.py:614] Starting main loop over steps 10000-500000
I0512 23:45:22.843029 139666521167872 train.py:621] Training with artificial "epochs" of 2500 steps.
I0512 23:45:22.843075 139666521167872 train.py:625] Compiling train loop.
I0512 23:45:22.845205 139856202532864 checkpoints.py:781] Skipping save checkpoint for step 10000 (directory gs://rosinality-tpu-bucket/openmoe_8b_ul2/training/checkpoint_10000 already exists)
I0512 23:45:22.845296 139856202532864 train.py:587] Starting training loop.
I0512 23:45:22.845338 139856202532864 train.py:614] Starting main loop over steps 10000-500000
I0512 23:45:22.845378 139856202532864 train.py:621] Training with artificial "epochs" of 2500 steps.
I0512 23:45:22.845418 139856202532864 train.py:625] Compiling train loop.
I0512 23:45:22.847769 139876724979712 checkpoints.py:781] Skipping save checkpoint for step 10000 (directory gs://rosinality-tpu-bucket/openmoe_8b_ul2/training/checkpoint_10000 already exists)
I0512 23:45:22.847984 139876724979712 train.py:587] Starting training loop.
I0512 23:45:22.848029 139876724979712 train.py:614] Starting main loop over steps 10000-500000
I0512 23:45:22.848071 139876724979712 train.py:621] Training with artificial "epochs" of 2500 steps.
I0512 23:45:22.848116 139876724979712 train.py:625] Compiling train loop.
I0512 23:45:22.848490 140193652815872 checkpoints.py:781] Skipping save checkpoint for step 10000 (directory gs://rosinality-tpu-bucket/openmoe_8b_ul2/training/checkpoint_10000 already exists)
I0512 23:45:22.848586 140193652815872 train.py:587] Starting training loop.
I0512 23:45:22.848629 140193652815872 train.py:614] Starting main loop over steps 10000-500000
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
I0512 23:45:22.848668 140193652815872 train.py:621] Training with artificial "epochs" of 2500 steps.
I0512 23:45:22.848707 140193652815872 train.py:625] Compiling train loop.
I0512 23:45:22.852553 140143678494720 checkpoints.py:781] Skipping save checkpoint for step 10000 (directory gs://rosinality-tpu-bucket/openmoe_8b_ul2/training/checkpoint_10000 already exists)
I0512 23:45:22.852778 140143678494720 train.py:587] Starting training loop.
I0512 23:45:22.852832 140143678494720 train.py:614] Starting main loop over steps 10000-500000
I0512 23:45:22.852877 140143678494720 train.py:621] Training with artificial "epochs" of 2500 steps.
I0512 23:45:22.852926 140143678494720 train.py:625] Compiling train loop.
I0512 23:45:22.839962 140260698863616 checkpoints.py:781] Skipping save checkpoint for step 10000 (directory gs://rosinality-tpu-bucket/openmoe_8b_ul2/training/checkpoint_10000 already exists)
I0512 23:45:22.840164 140260698863616 train.py:587] Starting training loop.
I0512 23:45:22.840211 140260698863616 train.py:614] Starting main loop over steps 10000-500000
I0512 23:45:22.840254 140260698863616 train.py:621] Training with artificial "epochs" of 2500 steps.
I0512 23:45:22.840299 140260698863616 train.py:625] Compiling train loop.
I0512 23:45:22.846181 140180528838656 checkpoints.py:781] Skipping save checkpoint for step 10000 (directory gs://rosinality-tpu-bucket/openmoe_8b_ul2/training/checkpoint_10000 already exists)
I0512 23:45:22.846394 140180528838656 train.py:587] Starting training loop.
I0512 23:45:22.846438 140180528838656 train.py:614] Starting main loop over steps 10000-500000
I0512 23:45:22.846485 140180528838656 train.py:621] Training with artificial "epochs" of 2500 steps.
I0512 23:45:22.846536 140180528838656 train.py:625] Compiling train loop.
I0512 23:45:22.860256 140643590531072 checkpoints.py:781] Skipping save checkpoint for step 10000 (directory gs://rosinality-tpu-bucket/openmoe_8b_ul2/training/checkpoint_10000 already exists)
I0512 23:45:22.860770 140643590531072 train.py:587] Starting training loop.
I0512 23:45:22.860826 140643590531072 train.py:614] Starting main loop over steps 10000-500000
I0512 23:45:22.860871 140643590531072 train.py:621] Training with artificial "epochs" of 2500 steps.
I0512 23:45:22.860918 140643590531072 train.py:625] Compiling train loop.
I0512 23:45:22.938933 139666521167872 trainer.py:704] using microbatches: 8 microbatches, 48 size
I0512 23:45:22.942476 140193652815872 trainer.py:704] using microbatches: 8 microbatches, 48 size
I0512 23:45:22.944196 139876724979712 trainer.py:704] using microbatches: 8 microbatches, 48 size
I0512 23:45:22.950887 139856202532864 trainer.py:704] using microbatches: 8 microbatches, 48 size
I0512 23:45:22.949897 140143678494720 trainer.py:704] using microbatches: 8 microbatches, 48 size
I0512 23:45:22.937623 140260698863616 trainer.py:704] using microbatches: 8 microbatches, 48 size
I0512 23:45:22.942153 140180528838656 trainer.py:704] using microbatches: 8 microbatches, 48 size
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
I0512 23:45:22.963311 140643590531072 trainer.py:704] using microbatches: 8 microbatches, 48 size
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
I0512 23:45:23.588885 140193652815872 moe_layers.py:777] Selected group_size=4096 and num_groups=24 for input num_tokens=98304, max_group_size=4096, num_experts=8 and num_expert_replicas=1
I0512 23:45:23.596301 139876724979712 moe_layers.py:777] Selected group_size=4096 and num_groups=24 for input num_tokens=98304, max_group_size=4096, num_experts=8 and num_expert_replicas=1
I0512 23:45:23.596793 139856202532864 moe_layers.py:777] Selected group_size=4096 and num_groups=24 for input num_tokens=98304, max_group_size=4096, num_experts=8 and num_expert_replicas=1
I0512 23:45:23.600069 139666521167872 moe_layers.py:777] Selected group_size=4096 and num_groups=24 for input num_tokens=98304, max_group_size=4096, num_experts=8 and num_expert_replicas=1
I0512 23:45:23.613193 140143678494720 moe_layers.py:777] Selected group_size=4096 and num_groups=24 for input num_tokens=98304, max_group_size=4096, num_experts=8 and num_expert_replicas=1
I0512 23:45:23.609344 140180528838656 moe_layers.py:777] Selected group_size=4096 and num_groups=24 for input num_tokens=98304, max_group_size=4096, num_experts=8 and num_expert_replicas=1
I0512 23:45:23.616292 140643590531072 moe_layers.py:777] Selected group_size=4096 and num_groups=24 for input num_tokens=98304, max_group_size=4096, num_experts=8 and num_expert_replicas=1
I0512 23:45:23.628819 140260698863616 moe_layers.py:777] Selected group_size=4096 and num_groups=24 for input num_tokens=98304, max_group_size=4096, num_experts=8 and num_expert_replicas=1
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
I0512 23:45:24.457247 140193652815872 moe_layers.py:777] Selected group_size=4096 and num_groups=24 for input num_tokens=98304, max_group_size=4096, num_experts=8 and num_expert_replicas=1
I0512 23:45:24.480697 139856202532864 moe_layers.py:777] Selected group_size=4096 and num_groups=24 for input num_tokens=98304, max_group_size=4096, num_experts=8 and num_expert_replicas=1
I0512 23:45:24.503896 140643590531072 moe_layers.py:777] Selected group_size=4096 and num_groups=24 for input num_tokens=98304, max_group_size=4096, num_experts=8 and num_expert_replicas=1
I0512 23:45:24.537137 140260698863616 moe_layers.py:777] Selected group_size=4096 and num_groups=24 for input num_tokens=98304, max_group_size=4096, num_experts=8 and num_expert_replicas=1
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
I0512 23:45:24.613009 139876724979712 moe_layers.py:777] Selected group_size=4096 and num_groups=24 for input num_tokens=98304, max_group_size=4096, num_experts=8 and num_expert_replicas=1
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
I0512 23:45:24.601610 140180528838656 moe_layers.py:777] Selected group_size=4096 and num_groups=24 for input num_tokens=98304, max_group_size=4096, num_experts=8 and num_expert_replicas=1
I0512 23:45:24.635799 139666521167872 moe_layers.py:777] Selected group_size=4096 and num_groups=24 for input num_tokens=98304, max_group_size=4096, num_experts=8 and num_expert_replicas=1
I0512 23:45:24.637328 140143678494720 moe_layers.py:777] Selected group_size=4096 and num_groups=24 for input num_tokens=98304, max_group_size=4096, num_experts=8 and num_expert_replicas=1
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
I0512 23:45:24.920150 140193652815872 moe_layers.py:777] Selected group_size=4096 and num_groups=24 for input num_tokens=98304, max_group_size=4096, num_experts=8 and num_expert_replicas=1
I0512 23:45:24.947495 139856202532864 moe_layers.py:777] Selected group_size=4096 and num_groups=24 for input num_tokens=98304, max_group_size=4096, num_experts=8 and num_expert_replicas=1
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
I0512 23:45:25.014238 140260698863616 moe_layers.py:777] Selected group_size=4096 and num_groups=24 for input num_tokens=98304, max_group_size=4096, num_experts=8 and num_expert_replicas=1
I0512 23:45:25.021996 140643590531072 moe_layers.py:777] Selected group_size=4096 and num_groups=24 for input num_tokens=98304, max_group_size=4096, num_experts=8 and num_expert_replicas=1
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
I0512 23:45:25.063404 140180528838656 moe_layers.py:777] Selected group_size=4096 and num_groups=24 for input num_tokens=98304, max_group_size=4096, num_experts=8 and num_expert_replicas=1
I0512 23:45:25.102397 139666521167872 moe_layers.py:777] Selected group_size=4096 and num_groups=24 for input num_tokens=98304, max_group_size=4096, num_experts=8 and num_expert_replicas=1
I0512 23:45:25.125124 139876724979712 moe_layers.py:777] Selected group_size=4096 and num_groups=24 for input num_tokens=98304, max_group_size=4096, num_experts=8 and num_expert_replicas=1
I0512 23:45:25.124336 140143678494720 moe_layers.py:777] Selected group_size=4096 and num_groups=24 for input num_tokens=98304, max_group_size=4096, num_experts=8 and num_expert_replicas=1
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
I0512 23:45:25.384058 140193652815872 moe_layers.py:777] Selected group_size=4096 and num_groups=24 for input num_tokens=98304, max_group_size=4096, num_experts=8 and num_expert_replicas=1
I0512 23:45:25.412843 139856202532864 moe_layers.py:777] Selected group_size=4096 and num_groups=24 for input num_tokens=98304, max_group_size=4096, num_experts=8 and num_expert_replicas=1
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
I0512 23:45:25.497926 140643590531072 moe_layers.py:777] Selected group_size=4096 and num_groups=24 for input num_tokens=98304, max_group_size=4096, num_experts=8 and num_expert_replicas=1
I0512 23:45:25.511291 140260698863616 moe_layers.py:777] Selected group_size=4096 and num_groups=24 for input num_tokens=98304, max_group_size=4096, num_experts=8 and num_expert_replicas=1
I0512 23:45:25.529711 140180528838656 moe_layers.py:777] Selected group_size=4096 and num_groups=24 for input num_tokens=98304, max_group_size=4096, num_experts=8 and num_expert_replicas=1
I0512 23:45:25.583823 139876724979712 moe_layers.py:777] Selected group_size=4096 and num_groups=24 for input num_tokens=98304, max_group_size=4096, num_experts=8 and num_expert_replicas=1
I0512 23:45:25.592673 139666521167872 moe_layers.py:777] Selected group_size=4096 and num_groups=24 for input num_tokens=98304, max_group_size=4096, num_experts=8 and num_expert_replicas=1
I0512 23:45:25.619752 140143678494720 moe_layers.py:777] Selected group_size=4096 and num_groups=24 for input num_tokens=98304, max_group_size=4096, num_experts=8 and num_expert_replicas=1
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
I0512 23:45:26.807934 140643590531072 moe_layers.py:777] Selected group_size=4096 and num_groups=24 for input num_tokens=98304, max_group_size=4096, num_experts=8 and num_expert_replicas=1
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
I0512 23:45:26.834087 140260698863616 moe_layers.py:777] Selected group_size=4096 and num_groups=24 for input num_tokens=98304, max_group_size=4096, num_experts=8 and num_expert_replicas=1
I0512 23:45:26.850683 140180528838656 moe_layers.py:777] Selected group_size=4096 and num_groups=24 for input num_tokens=98304, max_group_size=4096, num_experts=8 and num_expert_replicas=1
I0512 23:45:26.884199 139666521167872 moe_layers.py:777] Selected group_size=4096 and num_groups=24 for input num_tokens=98304, max_group_size=4096, num_experts=8 and num_expert_replicas=1
I0512 23:45:26.916653 139876724979712 moe_layers.py:777] Selected group_size=4096 and num_groups=24 for input num_tokens=98304, max_group_size=4096, num_experts=8 and num_expert_replicas=1
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
I0512 23:45:27.020777 140193652815872 moe_layers.py:777] Selected group_size=4096 and num_groups=24 for input num_tokens=98304, max_group_size=4096, num_experts=8 and num_expert_replicas=1
I0512 23:45:27.170828 139856202532864 moe_layers.py:777] Selected group_size=4096 and num_groups=24 for input num_tokens=98304, max_group_size=4096, num_experts=8 and num_expert_replicas=1
I0512 23:45:27.328588 140143678494720 moe_layers.py:777] Selected group_size=4096 and num_groups=24 for input num_tokens=98304, max_group_size=4096, num_experts=8 and num_expert_replicas=1
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
I0512 23:45:27.485167 140193652815872 moe_layers.py:777] Selected group_size=4096 and num_groups=24 for input num_tokens=98304, max_group_size=4096, num_experts=8 and num_expert_replicas=1
I0512 23:45:27.632555 139856202532864 moe_layers.py:777] Selected group_size=4096 and num_groups=24 for input num_tokens=98304, max_group_size=4096, num_experts=8 and num_expert_replicas=1
I0512 23:45:27.690491 139666521167872 moe_layers.py:777] Selected group_size=4096 and num_groups=24 for input num_tokens=98304, max_group_size=4096, num_experts=8 and num_expert_replicas=1
I0512 23:45:27.744406 140643590531072 moe_layers.py:777] Selected group_size=4096 and num_groups=24 for input num_tokens=98304, max_group_size=4096, num_experts=8 and num_expert_replicas=1
I0512 23:45:27.744637 140180528838656 moe_layers.py:777] Selected group_size=4096 and num_groups=24 for input num_tokens=98304, max_group_size=4096, num_experts=8 and num_expert_replicas=1
I0512 23:45:27.785201 140143678494720 moe_layers.py:777] Selected group_size=4096 and num_groups=24 for input num_tokens=98304, max_group_size=4096, num_experts=8 and num_expert_replicas=1
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
I0512 23:45:27.832833 140260698863616 moe_layers.py:777] Selected group_size=4096 and num_groups=24 for input num_tokens=98304, max_group_size=4096, num_experts=8 and num_expert_replicas=1
I0512 23:45:27.862763 139876724979712 moe_layers.py:777] Selected group_size=4096 and num_groups=24 for input num_tokens=98304, max_group_size=4096, num_experts=8 and num_expert_replicas=1
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
I0512 23:45:27.948668 140193652815872 moe_layers.py:777] Selected group_size=4096 and num_groups=24 for input num_tokens=98304, max_group_size=4096, num_experts=8 and num_expert_replicas=1
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
I0512 23:45:28.089375 139856202532864 moe_layers.py:777] Selected group_size=4096 and num_groups=24 for input num_tokens=98304, max_group_size=4096, num_experts=8 and num_expert_replicas=1
I0512 23:45:28.169183 139666521167872 moe_layers.py:777] Selected group_size=4096 and num_groups=24 for input num_tokens=98304, max_group_size=4096, num_experts=8 and num_expert_replicas=1
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
I0512 23:45:28.204725 140643590531072 moe_layers.py:777] Selected group_size=4096 and num_groups=24 for input num_tokens=98304, max_group_size=4096, num_experts=8 and num_expert_replicas=1
I0512 23:45:28.210192 140180528838656 moe_layers.py:777] Selected group_size=4096 and num_groups=24 for input num_tokens=98304, max_group_size=4096, num_experts=8 and num_expert_replicas=1
I0512 23:45:28.276597 140143678494720 moe_layers.py:777] Selected group_size=4096 and num_groups=24 for input num_tokens=98304, max_group_size=4096, num_experts=8 and num_expert_replicas=1
I0512 23:45:28.321866 139876724979712 moe_layers.py:777] Selected group_size=4096 and num_groups=24 for input num_tokens=98304, max_group_size=4096, num_experts=8 and num_expert_replicas=1
I0512 23:45:28.305116 140260698863616 moe_layers.py:777] Selected group_size=4096 and num_groups=24 for input num_tokens=98304, max_group_size=4096, num_experts=8 and num_expert_replicas=1
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
I0512 23:45:28.415862 140193652815872 moe_layers.py:777] Selected group_size=4096 and num_groups=24 for input num_tokens=98304, max_group_size=4096, num_experts=8 and num_expert_replicas=1
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
I0512 23:45:28.563730 139856202532864 moe_layers.py:777] Selected group_size=4096 and num_groups=24 for input num_tokens=98304, max_group_size=4096, num_experts=8 and num_expert_replicas=1
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
I0512 23:45:28.634996 139666521167872 moe_layers.py:777] Selected group_size=4096 and num_groups=24 for input num_tokens=98304, max_group_size=4096, num_experts=8 and num_expert_replicas=1
I0512 23:45:28.675251 140180528838656 moe_layers.py:777] Selected group_size=4096 and num_groups=24 for input num_tokens=98304, max_group_size=4096, num_experts=8 and num_expert_replicas=1
I0512 23:45:28.684839 140643590531072 moe_layers.py:777] Selected group_size=4096 and num_groups=24 for input num_tokens=98304, max_group_size=4096, num_experts=8 and num_expert_replicas=1
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
I0512 23:45:28.739018 140143678494720 moe_layers.py:777] Selected group_size=4096 and num_groups=24 for input num_tokens=98304, max_group_size=4096, num_experts=8 and num_expert_replicas=1
I0512 23:45:28.776846 140260698863616 moe_layers.py:777] Selected group_size=4096 and num_groups=24 for input num_tokens=98304, max_group_size=4096, num_experts=8 and num_expert_replicas=1
I0512 23:45:28.815597 139876724979712 moe_layers.py:777] Selected group_size=4096 and num_groups=24 for input num_tokens=98304, max_group_size=4096, num_experts=8 and num_expert_replicas=1
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
W0512 23:45:29.709165 140193652815872 adafactor.py:358] Since rank of parameter decoder/decoder_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:29.729375 140193652815872 adafactor.py:358] Since rank of parameter decoder/layers_0/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:29.755777 140193652815872 adafactor.py:358] Since rank of parameter decoder/layers_0/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:29.780030 140193652815872 adafactor.py:358] Since rank of parameter decoder/layers_0/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:29.801985 140193652815872 adafactor.py:358] Since rank of parameter decoder/layers_0/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:29.812196 140193652815872 adafactor.py:358] Since rank of parameter decoder/layers_0/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
W0512 23:45:29.825071 140193652815872 adafactor.py:358] Since rank of parameter decoder/layers_0/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:29.849058 140193652815872 adafactor.py:358] Since rank of parameter decoder/layers_0/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:29.854999 139856202532864 adafactor.py:358] Since rank of parameter decoder/decoder_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:29.870992 140193652815872 adafactor.py:358] Since rank of parameter decoder/layers_0/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:29.875405 139856202532864 adafactor.py:358] Since rank of parameter decoder/layers_0/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:29.885572 140193652815872 adafactor.py:358] Since rank of parameter decoder/layers_0/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:29.894175 139666521167872 adafactor.py:358] Since rank of parameter decoder/decoder_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:29.899206 140193652815872 adafactor.py:358] Since rank of parameter decoder/layers_1/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:29.902461 139856202532864 adafactor.py:358] Since rank of parameter decoder/layers_0/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:29.913770 139666521167872 adafactor.py:358] Since rank of parameter decoder/layers_0/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:29.913097 140193652815872 adafactor.py:358] Since rank of parameter decoder/layers_1/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:29.927616 139856202532864 adafactor.py:358] Since rank of parameter decoder/layers_0/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:29.926940 140193652815872 adafactor.py:358] Since rank of parameter decoder/layers_1/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:29.940526 139666521167872 adafactor.py:358] Since rank of parameter decoder/layers_0/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:29.943639 140193652815872 adafactor.py:358] Since rank of parameter decoder/layers_1/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:29.950475 139856202532864 adafactor.py:358] Since rank of parameter decoder/layers_0/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:29.953970 140193652815872 adafactor.py:358] Since rank of parameter decoder/layers_1/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:29.960973 139856202532864 adafactor.py:358] Since rank of parameter decoder/layers_0/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:29.963965 139666521167872 adafactor.py:358] Since rank of parameter decoder/layers_0/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:29.964093 140193652815872 adafactor.py:358] Since rank of parameter decoder/layers_1/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:29.971674 139856202532864 adafactor.py:358] Since rank of parameter decoder/layers_0/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:29.978015 140193652815872 adafactor.py:358] Since rank of parameter decoder/layers_1/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:29.985817 139666521167872 adafactor.py:358] Since rank of parameter decoder/layers_0/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:29.974247 140180528838656 adafactor.py:358] Since rank of parameter decoder/decoder_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:29.991890 140193652815872 adafactor.py:358] Since rank of parameter decoder/layers_1/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:29.995782 139856202532864 adafactor.py:358] Since rank of parameter decoder/layers_0/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:29.996066 139666521167872 adafactor.py:358] Since rank of parameter decoder/layers_0/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:29.999125 140143678494720 adafactor.py:358] Since rank of parameter decoder/decoder_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:30.006175 139666521167872 adafactor.py:358] Since rank of parameter decoder/layers_0/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:30.005726 140193652815872 adafactor.py:358] Since rank of parameter decoder/layers_1/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:29.993930 140180528838656 adafactor.py:358] Since rank of parameter decoder/layers_0/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:30.018000 139856202532864 adafactor.py:358] Since rank of parameter decoder/layers_0/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:30.001633 140643590531072 adafactor.py:358] Since rank of parameter decoder/decoder_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:30.020224 140193652815872 adafactor.py:358] Since rank of parameter decoder/layers_10/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:30.019842 140143678494720 adafactor.py:358] Since rank of parameter decoder/layers_0/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:30.029597 139666521167872 adafactor.py:358] Since rank of parameter decoder/layers_0/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:30.032980 139856202532864 adafactor.py:358] Since rank of parameter decoder/layers_0/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:30.034311 140193652815872 adafactor.py:358] Since rank of parameter decoder/layers_10/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:30.019972 140180528838656 adafactor.py:358] Since rank of parameter decoder/layers_0/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:30.020955 140643590531072 adafactor.py:358] Since rank of parameter decoder/layers_0/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:30.047537 139856202532864 adafactor.py:358] Since rank of parameter decoder/layers_1/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:30.047765 140143678494720 adafactor.py:358] Since rank of parameter decoder/layers_0/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:30.052093 139666521167872 adafactor.py:358] Since rank of parameter decoder/layers_0/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:30.052634 140193652815872 adafactor.py:358] Since rank of parameter decoder/layers_10/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:30.044093 140180528838656 adafactor.py:358] Since rank of parameter decoder/layers_0/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:30.065875 139666521167872 adafactor.py:358] Since rank of parameter decoder/layers_0/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:30.048225 140643590531072 adafactor.py:358] Since rank of parameter decoder/layers_0/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:30.066893 140193652815872 adafactor.py:358] Since rank of parameter decoder/layers_10/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:30.073004 139856202532864 adafactor.py:358] Since rank of parameter decoder/layers_1/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:30.073948 140143678494720 adafactor.py:358] Since rank of parameter decoder/layers_0/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:30.077214 140193652815872 adafactor.py:358] Since rank of parameter decoder/layers_10/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:30.079699 139666521167872 adafactor.py:358] Since rank of parameter decoder/layers_1/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:30.066494 140180528838656 adafactor.py:358] Since rank of parameter decoder/layers_0/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:30.087328 140193652815872 adafactor.py:358] Since rank of parameter decoder/layers_10/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:30.071855 140643590531072 adafactor.py:358] Since rank of parameter decoder/layers_0/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:30.093015 139856202532864 adafactor.py:358] Since rank of parameter decoder/layers_1/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:30.093468 139666521167872 adafactor.py:358] Since rank of parameter decoder/layers_1/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:30.076739 140180528838656 adafactor.py:358] Since rank of parameter decoder/layers_0/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:30.097168 140143678494720 adafactor.py:358] Since rank of parameter decoder/layers_0/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:30.101298 140193652815872 adafactor.py:358] Since rank of parameter decoder/layers_10/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:30.087002 140180528838656 adafactor.py:358] Since rank of parameter decoder/layers_0/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:30.107254 139666521167872 adafactor.py:358] Since rank of parameter decoder/layers_1/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:30.107331 140143678494720 adafactor.py:358] Since rank of parameter decoder/layers_0/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:30.093754 140643590531072 adafactor.py:358] Since rank of parameter decoder/layers_0/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:30.111746 139856202532864 adafactor.py:358] Since rank of parameter decoder/layers_1/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:30.115128 140193652815872 adafactor.py:358] Since rank of parameter decoder/layers_10/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:30.117485 140143678494720 adafactor.py:358] Since rank of parameter decoder/layers_0/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:30.120981 139666521167872 adafactor.py:358] Since rank of parameter decoder/layers_1/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:30.103987 140643590531072 adafactor.py:358] Since rank of parameter decoder/layers_0/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:30.127003 139856202532864 adafactor.py:358] Since rank of parameter decoder/layers_1/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:30.110913 140180528838656 adafactor.py:358] Since rank of parameter decoder/layers_0/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:30.128852 140193652815872 adafactor.py:358] Since rank of parameter decoder/layers_10/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:30.131144 139666521167872 adafactor.py:358] Since rank of parameter decoder/layers_1/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:30.114051 140643590531072 adafactor.py:358] Since rank of parameter decoder/layers_0/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:30.141252 139666521167872 adafactor.py:358] Since rank of parameter decoder/layers_1/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:30.140863 140143678494720 adafactor.py:358] Since rank of parameter decoder/layers_0/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:30.143233 140193652815872 adafactor.py:358] Since rank of parameter decoder/layers_11/extra_mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:30.144432 139856202532864 adafactor.py:358] Since rank of parameter decoder/layers_1/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:30.133222 140180528838656 adafactor.py:358] Since rank of parameter decoder/layers_0/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:30.137354 140643590531072 adafactor.py:358] Since rank of parameter decoder/layers_0/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:30.155013 139666521167872 adafactor.py:358] Since rank of parameter decoder/layers_1/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:30.136893 140260698863616 adafactor.py:358] Since rank of parameter decoder/decoder_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:30.157035 140193652815872 adafactor.py:358] Since rank of parameter decoder/layers_11/extra_mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:30.163627 140143678494720 adafactor.py:358] Since rank of parameter decoder/layers_0/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:30.148305 140180528838656 adafactor.py:358] Since rank of parameter decoder/layers_0/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:30.167136 139856202532864 adafactor.py:358] Since rank of parameter decoder/layers_1/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:30.169436 139666521167872 adafactor.py:358] Since rank of parameter decoder/layers_1/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:30.170666 140193652815872 adafactor.py:358] Since rank of parameter decoder/layers_11/extra_mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:30.157390 140260698863616 adafactor.py:358] Since rank of parameter decoder/layers_0/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:30.159713 140643590531072 adafactor.py:358] Since rank of parameter decoder/layers_0/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:30.162393 140180528838656 adafactor.py:358] Since rank of parameter decoder/layers_1/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:30.177913 140143678494720 adafactor.py:358] Since rank of parameter decoder/layers_0/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:30.183210 139666521167872 adafactor.py:358] Since rank of parameter decoder/layers_1/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:30.190177 139856202532864 adafactor.py:358] Since rank of parameter decoder/layers_1/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:30.173358 140643590531072 adafactor.py:358] Since rank of parameter decoder/layers_0/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:30.176463 140180528838656 adafactor.py:358] Since rank of parameter decoder/layers_1/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:30.192133 140143678494720 adafactor.py:358] Since rank of parameter decoder/layers_1/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:30.196959 139666521167872 adafactor.py:358] Since rank of parameter decoder/layers_10/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:30.184495 140260698863616 adafactor.py:358] Since rank of parameter decoder/layers_0/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:30.187021 140643590531072 adafactor.py:358] Since rank of parameter decoder/layers_1/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:30.190632 140180528838656 adafactor.py:358] Since rank of parameter decoder/layers_1/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:30.207082 140143678494720 adafactor.py:358] Since rank of parameter decoder/layers_1/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:30.210790 139666521167872 adafactor.py:358] Since rank of parameter decoder/layers_10/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:30.212653 139856202532864 adafactor.py:358] Since rank of parameter decoder/layers_1/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:30.200878 140643590531072 adafactor.py:358] Since rank of parameter decoder/layers_1/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:30.204822 140180528838656 adafactor.py:358] Since rank of parameter decoder/layers_1/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:30.221410 140143678494720 adafactor.py:358] Since rank of parameter decoder/layers_1/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:30.224594 139666521167872 adafactor.py:358] Since rank of parameter decoder/layers_10/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:30.209535 140260698863616 adafactor.py:358] Since rank of parameter decoder/layers_0/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:30.214783 140643590531072 adafactor.py:358] Since rank of parameter decoder/layers_1/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:30.215330 140180528838656 adafactor.py:358] Since rank of parameter decoder/layers_1/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:30.236607 139856202532864 adafactor.py:358] Since rank of parameter decoder/layers_10/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:30.235331 140143678494720 adafactor.py:358] Since rank of parameter decoder/layers_1/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:30.238363 139666521167872 adafactor.py:358] Since rank of parameter decoder/layers_10/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:30.225659 140180528838656 adafactor.py:358] Since rank of parameter decoder/layers_1/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:30.245021 139876724979712 adafactor.py:358] Since rank of parameter decoder/decoder_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:30.228613 140643590531072 adafactor.py:358] Since rank of parameter decoder/layers_1/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:30.245644 140143678494720 adafactor.py:358] Since rank of parameter decoder/layers_1/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:30.248514 139666521167872 adafactor.py:358] Since rank of parameter decoder/layers_10/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:30.232306 140260698863616 adafactor.py:358] Since rank of parameter decoder/layers_0/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:30.254940 139856202532864 adafactor.py:358] Since rank of parameter decoder/layers_10/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:30.238785 140643590531072 adafactor.py:358] Since rank of parameter decoder/layers_1/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:30.239817 140180528838656 adafactor.py:358] Since rank of parameter decoder/layers_1/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:30.255879 140143678494720 adafactor.py:358] Since rank of parameter decoder/layers_1/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:30.258679 139666521167872 adafactor.py:358] Since rank of parameter decoder/layers_10/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:30.243301 140260698863616 adafactor.py:358] Since rank of parameter decoder/layers_0/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:30.262448 140193652815872 adafactor.py:358] Since rank of parameter decoder/layers_11/mlp/router/router_weights/w/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>, <FactorDim.NONE: None>) is ignored.
W0512 23:45:30.264679 139876724979712 adafactor.py:358] Since rank of parameter decoder/layers_0/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:30.249101 140643590531072 adafactor.py:358] Since rank of parameter decoder/layers_1/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:30.254150 140180528838656 adafactor.py:358] Since rank of parameter decoder/layers_1/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:30.269861 140143678494720 adafactor.py:358] Since rank of parameter decoder/layers_1/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:30.272530 139666521167872 adafactor.py:358] Since rank of parameter decoder/layers_10/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:30.253870 140260698863616 adafactor.py:358] Since rank of parameter decoder/layers_0/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:30.275720 139856202532864 adafactor.py:358] Since rank of parameter decoder/layers_10/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:30.278038 140193652815872 adafactor.py:358] Since rank of parameter decoder/layers_11/pre_extra_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:30.263159 140643590531072 adafactor.py:358] Since rank of parameter decoder/layers_1/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:30.286773 139666521167872 adafactor.py:358] Since rank of parameter decoder/layers_10/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:30.284639 140143678494720 adafactor.py:358] Since rank of parameter decoder/layers_1/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:30.268935 140180528838656 adafactor.py:358] Since rank of parameter decoder/layers_1/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:30.288244 140193652815872 adafactor.py:358] Since rank of parameter decoder/layers_11/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:30.291892 139876724979712 adafactor.py:358] Since rank of parameter decoder/layers_0/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:30.278248 140643590531072 adafactor.py:358] Since rank of parameter decoder/layers_1/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:30.278347 140260698863616 adafactor.py:358] Since rank of parameter decoder/layers_0/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:30.299170 139856202532864 adafactor.py:358] Since rank of parameter decoder/layers_10/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:30.298809 140193652815872 adafactor.py:358] Since rank of parameter decoder/layers_11/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:30.300606 139666521167872 adafactor.py:358] Since rank of parameter decoder/layers_10/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:30.283951 140180528838656 adafactor.py:358] Since rank of parameter decoder/layers_10/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:30.300493 140143678494720 adafactor.py:358] Since rank of parameter decoder/layers_1/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:30.291975 140643590531072 adafactor.py:358] Since rank of parameter decoder/layers_1/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:30.310256 140193652815872 adafactor.py:358] Since rank of parameter decoder/layers_11/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:30.314340 139666521167872 adafactor.py:358] Since rank of parameter decoder/layers_11/extra_mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:30.315668 139876724979712 adafactor.py:358] Since rank of parameter decoder/layers_0/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:30.298048 140180528838656 adafactor.py:358] Since rank of parameter decoder/layers_10/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:30.316326 139856202532864 adafactor.py:358] Since rank of parameter decoder/layers_10/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:30.314677 140143678494720 adafactor.py:358] Since rank of parameter decoder/layers_10/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:30.301026 140260698863616 adafactor.py:358] Since rank of parameter decoder/layers_0/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:30.305697 140643590531072 adafactor.py:358] Since rank of parameter decoder/layers_10/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:30.325402 140193652815872 adafactor.py:358] Since rank of parameter decoder/layers_11/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:30.328020 139666521167872 adafactor.py:358] Since rank of parameter decoder/layers_11/extra_mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:30.312040 140180528838656 adafactor.py:358] Since rank of parameter decoder/layers_10/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:30.328722 140143678494720 adafactor.py:358] Since rank of parameter decoder/layers_10/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:30.332278 139856202532864 adafactor.py:358] Since rank of parameter decoder/layers_10/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:30.319384 140643590531072 adafactor.py:358] Since rank of parameter decoder/layers_10/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:30.338077 139876724979712 adafactor.py:358] Since rank of parameter decoder/layers_0/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:30.319434 140260698863616 adafactor.py:358] Since rank of parameter decoder/layers_0/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:30.339164 140193652815872 adafactor.py:358] Since rank of parameter decoder/layers_11/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:30.341754 139666521167872 adafactor.py:358] Since rank of parameter decoder/layers_11/extra_mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:30.326121 140180528838656 adafactor.py:358] Since rank of parameter decoder/layers_10/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:30.342530 140143678494720 adafactor.py:358] Since rank of parameter decoder/layers_10/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:30.348472 139876724979712 adafactor.py:358] Since rank of parameter decoder/layers_0/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:30.333034 140643590531072 adafactor.py:358] Since rank of parameter decoder/layers_10/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:30.353229 140193652815872 adafactor.py:358] Since rank of parameter decoder/layers_11/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:30.334907 140260698863616 adafactor.py:358] Since rank of parameter decoder/layers_1/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:30.354837 139856202532864 adafactor.py:358] Since rank of parameter decoder/layers_10/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:30.336989 140180528838656 adafactor.py:358] Since rank of parameter decoder/layers_10/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:30.358767 139876724979712 adafactor.py:358] Since rank of parameter decoder/layers_0/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:30.356451 140143678494720 adafactor.py:358] Since rank of parameter decoder/layers_10/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:30.346663 140643590531072 adafactor.py:358] Since rank of parameter decoder/layers_10/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:30.347431 140180528838656 adafactor.py:358] Since rank of parameter decoder/layers_10/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:30.366659 140143678494720 adafactor.py:358] Since rank of parameter decoder/layers_10/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:30.350621 140260698863616 adafactor.py:358] Since rank of parameter decoder/layers_1/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:30.369718 140193652815872 adafactor.py:358] Since rank of parameter decoder/layers_12/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:30.356754 140643590531072 adafactor.py:358] Since rank of parameter decoder/layers_10/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:30.377585 139856202532864 adafactor.py:358] Since rank of parameter decoder/layers_10/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:30.376821 140143678494720 adafactor.py:358] Since rank of parameter decoder/layers_10/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:30.361605 140180528838656 adafactor.py:358] Since rank of parameter decoder/layers_10/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:30.382560 139876724979712 adafactor.py:358] Since rank of parameter decoder/layers_0/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:30.366737 140643590531072 adafactor.py:358] Since rank of parameter decoder/layers_10/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:30.366775 140260698863616 adafactor.py:358] Since rank of parameter decoder/layers_1/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:30.385643 140193652815872 adafactor.py:358] Since rank of parameter decoder/layers_12/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:30.390569 140143678494720 adafactor.py:358] Since rank of parameter decoder/layers_10/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:30.375497 140180528838656 adafactor.py:358] Since rank of parameter decoder/layers_10/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:30.380444 140643590531072 adafactor.py:358] Since rank of parameter decoder/layers_10/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:30.399371 139856202532864 adafactor.py:358] Since rank of parameter decoder/layers_10/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:30.399778 140193652815872 adafactor.py:358] Since rank of parameter decoder/layers_12/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:30.382738 140260698863616 adafactor.py:358] Since rank of parameter decoder/layers_1/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:30.405316 139876724979712 adafactor.py:358] Since rank of parameter decoder/layers_0/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:30.389343 140180528838656 adafactor.py:358] Since rank of parameter decoder/layers_10/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:30.405633 140143678494720 adafactor.py:358] Since rank of parameter decoder/layers_10/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:30.394662 140643590531072 adafactor.py:358] Since rank of parameter decoder/layers_10/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:30.394627 140260698863616 adafactor.py:358] Since rank of parameter decoder/layers_1/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:30.415359 140193652815872 adafactor.py:358] Since rank of parameter decoder/layers_12/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:30.417339 139856202532864 adafactor.py:358] Since rank of parameter decoder/layers_11/extra_mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:30.419209 139876724979712 adafactor.py:358] Since rank of parameter decoder/layers_0/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:30.404096 140180528838656 adafactor.py:358] Since rank of parameter decoder/layers_11/extra_mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:30.420647 140143678494720 adafactor.py:358] Since rank of parameter decoder/layers_10/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:30.406519 140260698863616 adafactor.py:358] Since rank of parameter decoder/layers_1/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:30.408262 140643590531072 adafactor.py:358] Since rank of parameter decoder/layers_10/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:30.429819 140193652815872 adafactor.py:358] Since rank of parameter decoder/layers_12/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:30.431126 139666521167872 adafactor.py:358] Since rank of parameter decoder/layers_11/mlp/router/router_weights/w/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>, <FactorDim.NONE: None>) is ignored.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
W0512 23:45:30.433088 139876724979712 adafactor.py:358] Since rank of parameter decoder/layers_1/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:30.434270 139856202532864 adafactor.py:358] Since rank of parameter decoder/layers_11/extra_mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:30.417990 140180528838656 adafactor.py:358] Since rank of parameter decoder/layers_11/extra_mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:30.434728 140143678494720 adafactor.py:358] Since rank of parameter decoder/layers_11/extra_mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:30.421879 140643590531072 adafactor.py:358] Since rank of parameter decoder/layers_11/extra_mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:30.441501 140193652815872 adafactor.py:358] Since rank of parameter decoder/layers_12/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:30.423535 140260698863616 adafactor.py:358] Since rank of parameter decoder/layers_1/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:30.447283 139876724979712 adafactor.py:358] Since rank of parameter decoder/layers_1/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:30.431936 140180528838656 adafactor.py:358] Since rank of parameter decoder/layers_11/extra_mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:30.450296 139666521167872 adafactor.py:358] Since rank of parameter decoder/layers_11/pre_extra_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:30.448623 140143678494720 adafactor.py:358] Since rank of parameter decoder/layers_11/extra_mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:30.451651 139856202532864 adafactor.py:358] Since rank of parameter decoder/layers_11/extra_mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:30.435479 140643590531072 adafactor.py:358] Since rank of parameter decoder/layers_11/extra_mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:30.439002 140260698863616 adafactor.py:358] Since rank of parameter decoder/layers_1/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:30.458228 140193652815872 adafactor.py:358] Since rank of parameter decoder/layers_12/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:30.461150 139876724979712 adafactor.py:358] Since rank of parameter decoder/layers_1/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:30.461980 139666521167872 adafactor.py:358] Since rank of parameter decoder/layers_11/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:30.462620 140143678494720 adafactor.py:358] Since rank of parameter decoder/layers_11/extra_mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:30.449312 140643590531072 adafactor.py:358] Since rank of parameter decoder/layers_11/extra_mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:30.472595 139666521167872 adafactor.py:358] Since rank of parameter decoder/layers_11/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:30.453984 140260698863616 adafactor.py:358] Since rank of parameter decoder/layers_1/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:30.473468 140193652815872 adafactor.py:358] Since rank of parameter decoder/layers_12/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:30.475008 139876724979712 adafactor.py:358] Since rank of parameter decoder/layers_1/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:30.483074 139666521167872 adafactor.py:358] Since rank of parameter decoder/layers_11/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:30.485279 139876724979712 adafactor.py:358] Since rank of parameter decoder/layers_1/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:30.487566 140193652815872 adafactor.py:358] Since rank of parameter decoder/layers_12/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:30.469907 140260698863616 adafactor.py:358] Since rank of parameter decoder/layers_10/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:30.495507 139876724979712 adafactor.py:358] Since rank of parameter decoder/layers_1/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:30.498047 139666521167872 adafactor.py:358] Since rank of parameter decoder/layers_11/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:30.501571 140193652815872 adafactor.py:358] Since rank of parameter decoder/layers_13/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:30.484713 140260698863616 adafactor.py:358] Since rank of parameter decoder/layers_10/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:30.509455 139876724979712 adafactor.py:358] Since rank of parameter decoder/layers_1/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:30.515647 139666521167872 adafactor.py:358] Since rank of parameter decoder/layers_11/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:30.516736 140193652815872 adafactor.py:358] Since rank of parameter decoder/layers_13/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:30.499976 140260698863616 adafactor.py:358] Since rank of parameter decoder/layers_10/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:30.524006 139876724979712 adafactor.py:358] Since rank of parameter decoder/layers_1/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:30.530739 140193652815872 adafactor.py:358] Since rank of parameter decoder/layers_13/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:30.531500 139666521167872 adafactor.py:358] Since rank of parameter decoder/layers_11/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:30.514780 140260698863616 adafactor.py:358] Since rank of parameter decoder/layers_10/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:30.537864 139876724979712 adafactor.py:358] Since rank of parameter decoder/layers_1/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:30.525555 140260698863616 adafactor.py:358] Since rank of parameter decoder/layers_10/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:30.526300 140180528838656 adafactor.py:358] Since rank of parameter decoder/layers_11/mlp/router/router_weights/w/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>, <FactorDim.NONE: None>) is ignored.
W0512 23:45:30.544796 140193652815872 adafactor.py:358] Since rank of parameter decoder/layers_13/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:30.547155 139666521167872 adafactor.py:358] Since rank of parameter decoder/layers_12/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:30.550050 139856202532864 adafactor.py:358] Since rank of parameter decoder/layers_11/mlp/router/router_weights/w/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>, <FactorDim.NONE: None>) is ignored.
W0512 23:45:30.551859 139876724979712 adafactor.py:358] Since rank of parameter decoder/layers_10/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:30.536174 140260698863616 adafactor.py:358] Since rank of parameter decoder/layers_10/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:30.555094 140193652815872 adafactor.py:358] Since rank of parameter decoder/layers_13/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:30.538802 140643590531072 adafactor.py:358] Since rank of parameter decoder/layers_11/mlp/router/router_weights/w/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>, <FactorDim.NONE: None>) is ignored.
W0512 23:45:30.555706 140143678494720 adafactor.py:358] Since rank of parameter decoder/layers_11/mlp/router/router_weights/w/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>, <FactorDim.NONE: None>) is ignored.
W0512 23:45:30.543123 140180528838656 adafactor.py:358] Since rank of parameter decoder/layers_11/pre_extra_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:30.562969 139666521167872 adafactor.py:358] Since rank of parameter decoder/layers_12/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:30.565980 139876724979712 adafactor.py:358] Since rank of parameter decoder/layers_10/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:30.565665 140193652815872 adafactor.py:358] Since rank of parameter decoder/layers_13/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:30.566561 139856202532864 adafactor.py:358] Since rank of parameter decoder/layers_11/pre_extra_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:30.551062 140260698863616 adafactor.py:358] Since rank of parameter decoder/layers_10/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:30.553573 140180528838656 adafactor.py:358] Since rank of parameter decoder/layers_11/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:30.554155 140643590531072 adafactor.py:358] Since rank of parameter decoder/layers_11/pre_extra_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:30.572362 140143678494720 adafactor.py:358] Since rank of parameter decoder/layers_11/pre_extra_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:30.577276 139856202532864 adafactor.py:358] Since rank of parameter decoder/layers_11/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:30.578528 139666521167872 adafactor.py:358] Since rank of parameter decoder/layers_12/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:30.580307 139876724979712 adafactor.py:358] Since rank of parameter decoder/layers_10/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:30.579765 140193652815872 adafactor.py:358] Since rank of parameter decoder/layers_13/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:30.563721 140180528838656 adafactor.py:358] Since rank of parameter decoder/layers_11/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:30.564343 140643590531072 adafactor.py:358] Since rank of parameter decoder/layers_11/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:30.565689 140260698863616 adafactor.py:358] Since rank of parameter decoder/layers_10/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:30.583211 140143678494720 adafactor.py:358] Since rank of parameter decoder/layers_11/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:30.587675 139856202532864 adafactor.py:358] Since rank of parameter decoder/layers_11/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:30.573923 140180528838656 adafactor.py:358] Since rank of parameter decoder/layers_11/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:30.574397 140643590531072 adafactor.py:358] Since rank of parameter decoder/layers_11/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:30.594046 139666521167872 adafactor.py:358] Since rank of parameter decoder/layers_12/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:30.594493 139876724979712 adafactor.py:358] Since rank of parameter decoder/layers_10/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:30.593889 140193652815872 adafactor.py:358] Since rank of parameter decoder/layers_13/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:30.593919 140143678494720 adafactor.py:358] Since rank of parameter decoder/layers_11/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:30.598023 139856202532864 adafactor.py:358] Since rank of parameter decoder/layers_11/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:30.580605 140260698863616 adafactor.py:358] Since rank of parameter decoder/layers_10/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:30.584425 140643590531072 adafactor.py:358] Since rank of parameter decoder/layers_11/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:30.605171 139876724979712 adafactor.py:358] Since rank of parameter decoder/layers_10/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:30.606064 139666521167872 adafactor.py:358] Since rank of parameter decoder/layers_12/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:30.604122 140143678494720 adafactor.py:358] Since rank of parameter decoder/layers_11/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:30.589763 140180528838656 adafactor.py:358] Since rank of parameter decoder/layers_11/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:30.608040 140193652815872 adafactor.py:358] Since rank of parameter decoder/layers_13/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:30.612299 139856202532864 adafactor.py:358] Since rank of parameter decoder/layers_11/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:30.615398 139876724979712 adafactor.py:358] Since rank of parameter decoder/layers_10/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:30.596369 140260698863616 adafactor.py:358] Since rank of parameter decoder/layers_11/extra_mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:30.598213 140643590531072 adafactor.py:358] Since rank of parameter decoder/layers_11/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:30.616138 139666521167872 adafactor.py:358] Since rank of parameter decoder/layers_12/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:30.618313 140143678494720 adafactor.py:358] Since rank of parameter decoder/layers_11/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:30.622002 140193652815872 adafactor.py:358] Since rank of parameter decoder/layers_14/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:30.605337 140180528838656 adafactor.py:358] Since rank of parameter decoder/layers_11/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:30.626436 139856202532864 adafactor.py:358] Since rank of parameter decoder/layers_11/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:30.629338 139876724979712 adafactor.py:358] Since rank of parameter decoder/layers_10/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:30.630085 139666521167872 adafactor.py:358] Since rank of parameter decoder/layers_12/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:30.611025 140260698863616 adafactor.py:358] Since rank of parameter decoder/layers_11/extra_mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:30.612898 140643590531072 adafactor.py:358] Since rank of parameter decoder/layers_11/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:30.632193 140143678494720 adafactor.py:358] Since rank of parameter decoder/layers_11/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:30.636036 140193652815872 adafactor.py:358] Since rank of parameter decoder/layers_14/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:30.620181 140180528838656 adafactor.py:358] Since rank of parameter decoder/layers_11/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:30.640455 139856202532864 adafactor.py:358] Since rank of parameter decoder/layers_11/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:30.626784 140643590531072 adafactor.py:358] Since rank of parameter decoder/layers_11/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:30.644625 139666521167872 adafactor.py:358] Since rank of parameter decoder/layers_12/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:30.643771 139876724979712 adafactor.py:358] Since rank of parameter decoder/layers_10/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:30.626613 140260698863616 adafactor.py:358] Since rank of parameter decoder/layers_11/extra_mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:30.647326 140143678494720 adafactor.py:358] Since rank of parameter decoder/layers_11/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:30.651986 140193652815872 adafactor.py:358] Since rank of parameter decoder/layers_14/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:30.634981 140180528838656 adafactor.py:358] Since rank of parameter decoder/layers_12/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:30.655261 139856202532864 adafactor.py:358] Since rank of parameter decoder/layers_12/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:30.657661 139876724979712 adafactor.py:358] Since rank of parameter decoder/layers_10/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:30.640371 140643590531072 adafactor.py:358] Since rank of parameter decoder/layers_12/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:30.659542 139666521167872 adafactor.py:358] Since rank of parameter decoder/layers_12/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:30.661142 140143678494720 adafactor.py:358] Since rank of parameter decoder/layers_12/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:30.666246 140193652815872 adafactor.py:358] Since rank of parameter decoder/layers_14/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:30.648992 140180528838656 adafactor.py:358] Since rank of parameter decoder/layers_12/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:30.669812 139856202532864 adafactor.py:358] Since rank of parameter decoder/layers_12/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:30.671898 139876724979712 adafactor.py:358] Since rank of parameter decoder/layers_11/extra_mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:30.654089 140643590531072 adafactor.py:358] Since rank of parameter decoder/layers_12/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:30.672716 139666521167872 adafactor.py:358] Since rank of parameter decoder/layers_13/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:30.676922 140193652815872 adafactor.py:358] Since rank of parameter decoder/layers_14/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:30.675178 140143678494720 adafactor.py:358] Since rank of parameter decoder/layers_12/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:30.663091 140180528838656 adafactor.py:358] Since rank of parameter decoder/layers_12/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:30.683989 139856202532864 adafactor.py:358] Since rank of parameter decoder/layers_12/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:30.685865 139876724979712 adafactor.py:358] Since rank of parameter decoder/layers_11/extra_mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:30.667995 140643590531072 adafactor.py:358] Since rank of parameter decoder/layers_12/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:30.686445 139666521167872 adafactor.py:358] Since rank of parameter decoder/layers_13/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:30.687259 140193652815872 adafactor.py:358] Since rank of parameter decoder/layers_14/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:30.689298 140143678494720 adafactor.py:358] Since rank of parameter decoder/layers_12/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:30.677125 140180528838656 adafactor.py:358] Since rank of parameter decoder/layers_12/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:30.698108 139856202532864 adafactor.py:358] Since rank of parameter decoder/layers_12/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:30.699738 139876724979712 adafactor.py:358] Since rank of parameter decoder/layers_11/extra_mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:30.681765 140643590531072 adafactor.py:358] Since rank of parameter decoder/layers_12/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:30.699781 139666521167872 adafactor.py:358] Since rank of parameter decoder/layers_13/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:30.701463 140193652815872 adafactor.py:358] Since rank of parameter decoder/layers_14/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:30.687451 140180528838656 adafactor.py:358] Since rank of parameter decoder/layers_12/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:30.704449 140143678494720 adafactor.py:358] Since rank of parameter decoder/layers_12/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:30.708434 139856202532864 adafactor.py:358] Since rank of parameter decoder/layers_12/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:30.691932 140643590531072 adafactor.py:358] Since rank of parameter decoder/layers_12/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:30.713782 139666521167872 adafactor.py:358] Since rank of parameter decoder/layers_13/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:30.697726 140180528838656 adafactor.py:358] Since rank of parameter decoder/layers_12/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:30.715353 140193652815872 adafactor.py:358] Since rank of parameter decoder/layers_14/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:30.718804 139856202532864 adafactor.py:358] Since rank of parameter decoder/layers_12/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:30.701979 140643590531072 adafactor.py:358] Since rank of parameter decoder/layers_12/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:30.715616 140143678494720 adafactor.py:358] Since rank of parameter decoder/layers_12/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:30.723592 139666521167872 adafactor.py:358] Since rank of parameter decoder/layers_13/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:30.726227 140143678494720 adafactor.py:358] Since rank of parameter decoder/layers_12/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:30.729170 140193652815872 adafactor.py:358] Since rank of parameter decoder/layers_14/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:30.711822 140180528838656 adafactor.py:358] Since rank of parameter decoder/layers_12/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:30.733032 139856202532864 adafactor.py:358] Since rank of parameter decoder/layers_12/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:30.715955 140643590531072 adafactor.py:358] Since rank of parameter decoder/layers_12/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:30.733162 139666521167872 adafactor.py:358] Since rank of parameter decoder/layers_13/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:30.723687 140260698863616 adafactor.py:358] Since rank of parameter decoder/layers_11/mlp/router/router_weights/w/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>, <FactorDim.NONE: None>) is ignored.
W0512 23:45:30.725698 140180528838656 adafactor.py:358] Since rank of parameter decoder/layers_12/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:30.741019 140143678494720 adafactor.py:358] Since rank of parameter decoder/layers_12/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:30.742896 140193652815872 adafactor.py:358] Since rank of parameter decoder/layers_15/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:30.746491 139666521167872 adafactor.py:358] Since rank of parameter decoder/layers_13/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:30.747391 139856202532864 adafactor.py:358] Since rank of parameter decoder/layers_12/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:30.729644 140643590531072 adafactor.py:358] Since rank of parameter decoder/layers_12/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:30.739489 140180528838656 adafactor.py:358] Since rank of parameter decoder/layers_12/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:30.757048 140193652815872 adafactor.py:358] Since rank of parameter decoder/layers_15/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:30.755396 140143678494720 adafactor.py:358] Since rank of parameter decoder/layers_12/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:30.740016 140260698863616 adafactor.py:358] Since rank of parameter decoder/layers_11/pre_extra_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:30.759769 139666521167872 adafactor.py:358] Since rank of parameter decoder/layers_13/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:30.761370 139856202532864 adafactor.py:358] Since rank of parameter decoder/layers_12/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:30.744108 140643590531072 adafactor.py:358] Since rank of parameter decoder/layers_12/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:30.750681 140260698863616 adafactor.py:358] Since rank of parameter decoder/layers_11/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:30.753674 140180528838656 adafactor.py:358] Since rank of parameter decoder/layers_13/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:30.769690 140143678494720 adafactor.py:358] Since rank of parameter decoder/layers_12/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:30.771857 140193652815872 adafactor.py:358] Since rank of parameter decoder/layers_15/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:30.773075 139666521167872 adafactor.py:358] Since rank of parameter decoder/layers_13/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:30.775429 139856202532864 adafactor.py:358] Since rank of parameter decoder/layers_13/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:30.757948 140643590531072 adafactor.py:358] Since rank of parameter decoder/layers_13/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:30.761192 140260698863616 adafactor.py:358] Since rank of parameter decoder/layers_11/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:30.768047 140180528838656 adafactor.py:358] Since rank of parameter decoder/layers_13/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:30.783659 140143678494720 adafactor.py:358] Since rank of parameter decoder/layers_13/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:30.787076 139666521167872 adafactor.py:358] Since rank of parameter decoder/layers_14/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:30.787992 140193652815872 adafactor.py:358] Since rank of parameter decoder/layers_15/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:30.771766 140643590531072 adafactor.py:358] Since rank of parameter decoder/layers_13/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:30.790045 139856202532864 adafactor.py:358] Since rank of parameter decoder/layers_13/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:30.790221 139876724979712 adafactor.py:358] Since rank of parameter decoder/layers_11/mlp/router/router_weights/w/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>, <FactorDim.NONE: None>) is ignored.
W0512 23:45:30.771726 140260698863616 adafactor.py:358] Since rank of parameter decoder/layers_11/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:30.798582 140193652815872 adafactor.py:358] Since rank of parameter decoder/layers_15/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:30.781843 140180528838656 adafactor.py:358] Since rank of parameter decoder/layers_13/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:30.800399 139666521167872 adafactor.py:358] Since rank of parameter decoder/layers_14/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:30.799672 140143678494720 adafactor.py:358] Since rank of parameter decoder/layers_13/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:30.785947 140643590531072 adafactor.py:358] Since rank of parameter decoder/layers_13/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:30.804051 139856202532864 adafactor.py:358] Since rank of parameter decoder/layers_13/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:30.786322 140260698863616 adafactor.py:358] Since rank of parameter decoder/layers_11/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:30.805822 139876724979712 adafactor.py:358] Since rank of parameter decoder/layers_11/pre_extra_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:30.809664 140193652815872 adafactor.py:358] Since rank of parameter decoder/layers_15/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:30.795614 140180528838656 adafactor.py:358] Since rank of parameter decoder/layers_13/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:30.813761 139666521167872 adafactor.py:358] Since rank of parameter decoder/layers_14/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:30.816142 139876724979712 adafactor.py:358] Since rank of parameter decoder/layers_11/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:30.814309 140143678494720 adafactor.py:358] Since rank of parameter decoder/layers_13/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:30.818109 139856202532864 adafactor.py:358] Since rank of parameter decoder/layers_13/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:30.800613 140643590531072 adafactor.py:358] Since rank of parameter decoder/layers_13/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:30.801002 140260698863616 adafactor.py:358] Since rank of parameter decoder/layers_11/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:30.805800 140180528838656 adafactor.py:358] Since rank of parameter decoder/layers_13/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:30.824653 140193652815872 adafactor.py:358] Since rank of parameter decoder/layers_15/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:30.826393 139876724979712 adafactor.py:358] Since rank of parameter decoder/layers_11/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:30.826913 139666521167872 adafactor.py:358] Since rank of parameter decoder/layers_14/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:30.828411 139856202532864 adafactor.py:358] Since rank of parameter decoder/layers_13/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:30.811122 140643590531072 adafactor.py:358] Since rank of parameter decoder/layers_13/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:30.828887 140143678494720 adafactor.py:358] Since rank of parameter decoder/layers_13/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:30.815879 140180528838656 adafactor.py:358] Since rank of parameter decoder/layers_13/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:30.815552 140260698863616 adafactor.py:358] Since rank of parameter decoder/layers_11/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:30.836599 139876724979712 adafactor.py:358] Since rank of parameter decoder/layers_11/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:30.836743 139666521167872 adafactor.py:358] Since rank of parameter decoder/layers_14/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:30.838675 139856202532864 adafactor.py:358] Since rank of parameter decoder/layers_13/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:30.839321 140193652815872 adafactor.py:358] Since rank of parameter decoder/layers_15/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:30.823754 140643590531072 adafactor.py:358] Since rank of parameter decoder/layers_13/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:30.839603 140143678494720 adafactor.py:358] Since rank of parameter decoder/layers_13/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:30.846883 139666521167872 adafactor.py:358] Since rank of parameter decoder/layers_14/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:30.829694 140180528838656 adafactor.py:358] Since rank of parameter decoder/layers_13/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:30.831137 140260698863616 adafactor.py:358] Since rank of parameter decoder/layers_12/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:30.850492 139876724979712 adafactor.py:358] Since rank of parameter decoder/layers_11/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:30.852728 139856202532864 adafactor.py:358] Since rank of parameter decoder/layers_13/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:30.850933 140143678494720 adafactor.py:358] Since rank of parameter decoder/layers_13/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:30.853856 140193652815872 adafactor.py:358] Since rank of parameter decoder/layers_15/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:30.841561 140643590531072 adafactor.py:358] Since rank of parameter decoder/layers_13/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:30.860471 139666521167872 adafactor.py:358] Since rank of parameter decoder/layers_14/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:30.843389 140180528838656 adafactor.py:358] Since rank of parameter decoder/layers_13/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:30.845661 140260698863616 adafactor.py:358] Since rank of parameter decoder/layers_12/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:30.865045 139876724979712 adafactor.py:358] Since rank of parameter decoder/layers_11/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:30.866745 139856202532864 adafactor.py:358] Since rank of parameter decoder/layers_13/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:30.865488 140143678494720 adafactor.py:358] Since rank of parameter decoder/layers_13/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:30.867881 140193652815872 adafactor.py:358] Since rank of parameter decoder/layers_16/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:30.873658 139666521167872 adafactor.py:358] Since rank of parameter decoder/layers_14/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:30.856176 140643590531072 adafactor.py:358] Since rank of parameter decoder/layers_13/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:30.857127 140180528838656 adafactor.py:358] Since rank of parameter decoder/layers_13/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:30.878957 139876724979712 adafactor.py:358] Since rank of parameter decoder/layers_11/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:30.860179 140260698863616 adafactor.py:358] Since rank of parameter decoder/layers_12/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:30.880631 139856202532864 adafactor.py:358] Since rank of parameter decoder/layers_13/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:30.881856 140193652815872 adafactor.py:358] Since rank of parameter decoder/layers_16/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:30.879830 140143678494720 adafactor.py:358] Since rank of parameter decoder/layers_13/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:30.887214 139666521167872 adafactor.py:358] Since rank of parameter decoder/layers_14/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:30.871005 140643590531072 adafactor.py:358] Since rank of parameter decoder/layers_13/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:30.870838 140180528838656 adafactor.py:358] Since rank of parameter decoder/layers_14/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:30.892872 139876724979712 adafactor.py:358] Since rank of parameter decoder/layers_12/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:30.874620 140260698863616 adafactor.py:358] Since rank of parameter decoder/layers_12/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:30.894586 139856202532864 adafactor.py:358] Since rank of parameter decoder/layers_14/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:30.895829 140193652815872 adafactor.py:358] Since rank of parameter decoder/layers_16/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:30.894060 140143678494720 adafactor.py:358] Since rank of parameter decoder/layers_13/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:30.900270 139666521167872 adafactor.py:358] Since rank of parameter decoder/layers_15/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:30.884705 140180528838656 adafactor.py:358] Since rank of parameter decoder/layers_14/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:30.885954 140643590531072 adafactor.py:358] Since rank of parameter decoder/layers_14/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:30.885340 140260698863616 adafactor.py:358] Since rank of parameter decoder/layers_12/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:30.906846 139876724979712 adafactor.py:358] Since rank of parameter decoder/layers_12/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:30.908585 139856202532864 adafactor.py:358] Since rank of parameter decoder/layers_14/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:30.909892 140193652815872 adafactor.py:358] Since rank of parameter decoder/layers_16/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:30.908456 140143678494720 adafactor.py:358] Since rank of parameter decoder/layers_14/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:30.914759 139666521167872 adafactor.py:358] Since rank of parameter decoder/layers_15/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:30.896185 140260698863616 adafactor.py:358] Since rank of parameter decoder/layers_12/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:30.899043 140180528838656 adafactor.py:358] Since rank of parameter decoder/layers_14/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:30.899823 140643590531072 adafactor.py:358] Since rank of parameter decoder/layers_14/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:30.920155 140193652815872 adafactor.py:358] Since rank of parameter decoder/layers_16/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:30.921595 139876724979712 adafactor.py:358] Since rank of parameter decoder/layers_12/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:30.923118 139856202532864 adafactor.py:358] Since rank of parameter decoder/layers_14/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:30.922899 140143678494720 adafactor.py:358] Since rank of parameter decoder/layers_14/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:30.927975 139666521167872 adafactor.py:358] Since rank of parameter decoder/layers_15/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:30.911112 140260698863616 adafactor.py:358] Since rank of parameter decoder/layers_12/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:30.912810 140180528838656 adafactor.py:358] Since rank of parameter decoder/layers_14/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:30.913597 140643590531072 adafactor.py:358] Since rank of parameter decoder/layers_14/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:30.931099 140193652815872 adafactor.py:358] Since rank of parameter decoder/layers_16/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:30.936549 139876724979712 adafactor.py:358] Since rank of parameter decoder/layers_12/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:30.937349 139856202532864 adafactor.py:358] Since rank of parameter decoder/layers_14/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:30.922990 140180528838656 adafactor.py:358] Since rank of parameter decoder/layers_14/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:30.941170 139666521167872 adafactor.py:358] Since rank of parameter decoder/layers_15/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:30.939046 140143678494720 adafactor.py:358] Since rank of parameter decoder/layers_14/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:30.925815 140260698863616 adafactor.py:358] Since rank of parameter decoder/layers_12/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:30.927395 140643590531072 adafactor.py:358] Since rank of parameter decoder/layers_14/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:30.945226 140193652815872 adafactor.py:358] Since rank of parameter decoder/layers_16/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:30.947576 139876724979712 adafactor.py:358] Since rank of parameter decoder/layers_12/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:30.947714 139856202532864 adafactor.py:358] Since rank of parameter decoder/layers_14/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:30.950874 139666521167872 adafactor.py:358] Since rank of parameter decoder/layers_15/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:30.933255 140180528838656 adafactor.py:358] Since rank of parameter decoder/layers_14/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:30.937736 140643590531072 adafactor.py:358] Since rank of parameter decoder/layers_14/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:30.953402 140143678494720 adafactor.py:358] Since rank of parameter decoder/layers_14/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:30.958091 139856202532864 adafactor.py:358] Since rank of parameter decoder/layers_14/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:30.958491 139876724979712 adafactor.py:358] Since rank of parameter decoder/layers_12/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:30.940254 140260698863616 adafactor.py:358] Since rank of parameter decoder/layers_12/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:30.960559 139666521167872 adafactor.py:358] Since rank of parameter decoder/layers_15/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:30.959213 140193652815872 adafactor.py:358] Since rank of parameter decoder/layers_16/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:30.947014 140180528838656 adafactor.py:358] Since rank of parameter decoder/layers_14/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:30.947867 140643590531072 adafactor.py:358] Since rank of parameter decoder/layers_14/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:30.963901 140143678494720 adafactor.py:358] Since rank of parameter decoder/layers_14/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:30.972036 139856202532864 adafactor.py:358] Since rank of parameter decoder/layers_14/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:30.972823 139876724979712 adafactor.py:358] Since rank of parameter decoder/layers_12/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:30.954842 140260698863616 adafactor.py:358] Since rank of parameter decoder/layers_13/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:30.973170 140193652815872 adafactor.py:358] Since rank of parameter decoder/layers_16/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:30.974474 139666521167872 adafactor.py:358] Since rank of parameter decoder/layers_15/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:30.974354 140143678494720 adafactor.py:358] Since rank of parameter decoder/layers_14/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:30.960759 140180528838656 adafactor.py:358] Since rank of parameter decoder/layers_14/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:30.961843 140643590531072 adafactor.py:358] Since rank of parameter decoder/layers_14/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:30.985977 139856202532864 adafactor.py:358] Since rank of parameter decoder/layers_14/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:30.986702 139876724979712 adafactor.py:358] Since rank of parameter decoder/layers_12/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:30.987179 140193652815872 adafactor.py:358] Since rank of parameter decoder/layers_17/extra_mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:30.987965 139666521167872 adafactor.py:358] Since rank of parameter decoder/layers_15/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:30.970484 140260698863616 adafactor.py:358] Since rank of parameter decoder/layers_13/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:30.974678 140180528838656 adafactor.py:358] Since rank of parameter decoder/layers_14/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:30.988638 140143678494720 adafactor.py:358] Since rank of parameter decoder/layers_14/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:30.975750 140643590531072 adafactor.py:358] Since rank of parameter decoder/layers_14/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:30.999913 139856202532864 adafactor.py:358] Since rank of parameter decoder/layers_14/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:31.001027 139666521167872 adafactor.py:358] Since rank of parameter decoder/layers_15/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:31.001423 139876724979712 adafactor.py:358] Since rank of parameter decoder/layers_12/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:31.001142 140193652815872 adafactor.py:358] Since rank of parameter decoder/layers_17/extra_mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:30.984961 140260698863616 adafactor.py:358] Since rank of parameter decoder/layers_13/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:30.988441 140180528838656 adafactor.py:358] Since rank of parameter decoder/layers_15/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:31.002898 140143678494720 adafactor.py:358] Since rank of parameter decoder/layers_14/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:30.989878 140643590531072 adafactor.py:358] Since rank of parameter decoder/layers_14/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:31.013837 139856202532864 adafactor.py:358] Since rank of parameter decoder/layers_15/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:31.014116 139666521167872 adafactor.py:358] Since rank of parameter decoder/layers_16/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:31.015416 139876724979712 adafactor.py:358] Since rank of parameter decoder/layers_13/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:31.015181 140193652815872 adafactor.py:358] Since rank of parameter decoder/layers_17/extra_mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:30.999366 140260698863616 adafactor.py:358] Since rank of parameter decoder/layers_13/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:31.017197 140143678494720 adafactor.py:358] Since rank of parameter decoder/layers_14/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:31.002243 140180528838656 adafactor.py:358] Since rank of parameter decoder/layers_15/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:31.003964 140643590531072 adafactor.py:358] Since rank of parameter decoder/layers_15/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:31.027319 139666521167872 adafactor.py:358] Since rank of parameter decoder/layers_16/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:31.027795 139856202532864 adafactor.py:358] Since rank of parameter decoder/layers_15/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:31.029330 139876724979712 adafactor.py:358] Since rank of parameter decoder/layers_13/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:31.010058 140260698863616 adafactor.py:358] Since rank of parameter decoder/layers_13/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:31.016017 140180528838656 adafactor.py:358] Since rank of parameter decoder/layers_15/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:31.031329 140143678494720 adafactor.py:358] Since rank of parameter decoder/layers_15/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:31.020121 140643590531072 adafactor.py:358] Since rank of parameter decoder/layers_15/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:31.020683 140260698863616 adafactor.py:358] Since rank of parameter decoder/layers_13/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:31.041718 139856202532864 adafactor.py:358] Since rank of parameter decoder/layers_15/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:31.041406 139666521167872 adafactor.py:358] Since rank of parameter decoder/layers_16/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:31.043168 139876724979712 adafactor.py:358] Since rank of parameter decoder/layers_13/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:31.030387 140180528838656 adafactor.py:358] Since rank of parameter decoder/layers_15/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:31.045809 140143678494720 adafactor.py:358] Since rank of parameter decoder/layers_15/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:31.034113 140643590531072 adafactor.py:358] Since rank of parameter decoder/layers_15/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:31.035439 140260698863616 adafactor.py:358] Since rank of parameter decoder/layers_13/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:31.054599 139666521167872 adafactor.py:358] Since rank of parameter decoder/layers_16/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:31.056203 139856202532864 adafactor.py:358] Since rank of parameter decoder/layers_15/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:31.057117 139876724979712 adafactor.py:358] Since rank of parameter decoder/layers_13/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:31.040494 140180528838656 adafactor.py:358] Since rank of parameter decoder/layers_15/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:31.060288 140143678494720 adafactor.py:358] Since rank of parameter decoder/layers_15/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:31.064380 139666521167872 adafactor.py:358] Since rank of parameter decoder/layers_16/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:31.048012 140643590531072 adafactor.py:358] Since rank of parameter decoder/layers_15/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:31.066505 139856202532864 adafactor.py:358] Since rank of parameter decoder/layers_15/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:31.067358 139876724979712 adafactor.py:358] Since rank of parameter decoder/layers_13/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:31.050646 140180528838656 adafactor.py:358] Since rank of parameter decoder/layers_15/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:31.049953 140260698863616 adafactor.py:358] Since rank of parameter decoder/layers_13/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:31.071526 140193652815872 adafactor.py:358] Since rank of parameter decoder/layers_17/mlp/router/router_weights/w/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>, <FactorDim.NONE: None>) is ignored.
W0512 23:45:31.073999 139666521167872 adafactor.py:358] Since rank of parameter decoder/layers_16/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:31.058195 140643590531072 adafactor.py:358] Since rank of parameter decoder/layers_15/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:31.076725 139856202532864 adafactor.py:358] Since rank of parameter decoder/layers_15/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:31.077507 139876724979712 adafactor.py:358] Since rank of parameter decoder/layers_13/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:31.076119 140143678494720 adafactor.py:358] Since rank of parameter decoder/layers_15/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:31.064364 140180528838656 adafactor.py:358] Since rank of parameter decoder/layers_15/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:31.081879 140193652815872 adafactor.py:358] Since rank of parameter decoder/layers_17/pre_extra_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:31.064399 140260698863616 adafactor.py:358] Since rank of parameter decoder/layers_13/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:31.068281 140643590531072 adafactor.py:358] Since rank of parameter decoder/layers_15/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:31.087138 139666521167872 adafactor.py:358] Since rank of parameter decoder/layers_16/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:31.087105 140143678494720 adafactor.py:358] Since rank of parameter decoder/layers_15/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:31.090645 139856202532864 adafactor.py:358] Since rank of parameter decoder/layers_15/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:31.091446 139876724979712 adafactor.py:358] Since rank of parameter decoder/layers_13/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:31.092128 140193652815872 adafactor.py:358] Since rank of parameter decoder/layers_17/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:31.078148 140180528838656 adafactor.py:358] Since rank of parameter decoder/layers_15/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:31.078617 140260698863616 adafactor.py:358] Since rank of parameter decoder/layers_14/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
W0512 23:45:31.082331 140643590531072 adafactor.py:358] Since rank of parameter decoder/layers_15/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:31.100385 139666521167872 adafactor.py:358] Since rank of parameter decoder/layers_16/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:31.097747 140143678494720 adafactor.py:358] Since rank of parameter decoder/layers_15/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:31.102322 140193652815872 adafactor.py:358] Since rank of parameter decoder/layers_17/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:31.104696 139856202532864 adafactor.py:358] Since rank of parameter decoder/layers_15/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:31.105317 139876724979712 adafactor.py:358] Since rank of parameter decoder/layers_13/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:31.091971 140180528838656 adafactor.py:358] Since rank of parameter decoder/layers_15/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:31.093033 140260698863616 adafactor.py:358] Since rank of parameter decoder/layers_14/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:31.112562 140193652815872 adafactor.py:358] Since rank of parameter decoder/layers_17/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:31.113448 139666521167872 adafactor.py:358] Since rank of parameter decoder/layers_16/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:31.096285 140643590531072 adafactor.py:358] Since rank of parameter decoder/layers_15/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:31.112235 140143678494720 adafactor.py:358] Since rank of parameter decoder/layers_15/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:31.118755 139856202532864 adafactor.py:358] Since rank of parameter decoder/layers_15/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:31.119183 139876724979712 adafactor.py:358] Since rank of parameter decoder/layers_13/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:31.105812 140180528838656 adafactor.py:358] Since rank of parameter decoder/layers_16/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:31.126474 139666521167872 adafactor.py:358] Since rank of parameter decoder/layers_17/extra_mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:31.126446 140193652815872 adafactor.py:358] Since rank of parameter decoder/layers_17/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:31.108315 140260698863616 adafactor.py:358] Since rank of parameter decoder/layers_14/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:31.109884 140643590531072 adafactor.py:358] Since rank of parameter decoder/layers_15/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:31.126692 140143678494720 adafactor.py:358] Since rank of parameter decoder/layers_15/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:31.132873 139856202532864 adafactor.py:358] Since rank of parameter decoder/layers_16/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:31.133644 139876724979712 adafactor.py:358] Since rank of parameter decoder/layers_14/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:31.119539 140180528838656 adafactor.py:358] Since rank of parameter decoder/layers_16/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:31.139688 139666521167872 adafactor.py:358] Since rank of parameter decoder/layers_17/extra_mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:31.140476 140193652815872 adafactor.py:358] Since rank of parameter decoder/layers_17/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:31.123647 140643590531072 adafactor.py:358] Since rank of parameter decoder/layers_16/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:31.122945 140260698863616 adafactor.py:358] Since rank of parameter decoder/layers_14/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:31.141681 140143678494720 adafactor.py:358] Since rank of parameter decoder/layers_15/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:31.146932 139856202532864 adafactor.py:358] Since rank of parameter decoder/layers_16/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:31.147585 139876724979712 adafactor.py:358] Since rank of parameter decoder/layers_14/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:31.133301 140180528838656 adafactor.py:358] Since rank of parameter decoder/layers_16/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:31.133623 140260698863616 adafactor.py:358] Since rank of parameter decoder/layers_14/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:31.153202 139666521167872 adafactor.py:358] Since rank of parameter decoder/layers_17/extra_mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:31.137520 140643590531072 adafactor.py:358] Since rank of parameter decoder/layers_16/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:31.156997 140193652815872 adafactor.py:358] Since rank of parameter decoder/layers_17/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:31.155978 140143678494720 adafactor.py:358] Since rank of parameter decoder/layers_16/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:31.161146 139856202532864 adafactor.py:358] Since rank of parameter decoder/layers_16/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:31.161451 139876724979712 adafactor.py:358] Since rank of parameter decoder/layers_14/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:31.144178 140260698863616 adafactor.py:358] Since rank of parameter decoder/layers_14/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:31.147073 140180528838656 adafactor.py:358] Since rank of parameter decoder/layers_16/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:31.152561 140643590531072 adafactor.py:358] Since rank of parameter decoder/layers_16/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:31.170871 140193652815872 adafactor.py:358] Since rank of parameter decoder/layers_18/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:31.170492 140143678494720 adafactor.py:358] Since rank of parameter decoder/layers_16/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:31.175231 139856202532864 adafactor.py:358] Since rank of parameter decoder/layers_16/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:31.175324 139876724979712 adafactor.py:358] Since rank of parameter decoder/layers_14/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:31.157246 140180528838656 adafactor.py:358] Since rank of parameter decoder/layers_16/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:31.158667 140260698863616 adafactor.py:358] Since rank of parameter decoder/layers_14/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:31.185595 139856202532864 adafactor.py:358] Since rank of parameter decoder/layers_16/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:31.185544 139876724979712 adafactor.py:358] Since rank of parameter decoder/layers_14/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:31.185096 140193652815872 adafactor.py:358] Since rank of parameter decoder/layers_18/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:31.167825 140180528838656 adafactor.py:358] Since rank of parameter decoder/layers_16/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:31.167685 140643590531072 adafactor.py:358] Since rank of parameter decoder/layers_16/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:31.185041 140143678494720 adafactor.py:358] Since rank of parameter decoder/layers_16/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
W0512 23:45:31.173159 140260698863616 adafactor.py:358] Since rank of parameter decoder/layers_14/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:31.195821 139876724979712 adafactor.py:358] Since rank of parameter decoder/layers_14/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:31.177948 140643590531072 adafactor.py:358] Since rank of parameter decoder/layers_16/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:31.196355 139856202532864 adafactor.py:358] Since rank of parameter decoder/layers_16/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:31.181638 140180528838656 adafactor.py:358] Since rank of parameter decoder/layers_16/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:31.199393 140193652815872 adafactor.py:358] Since rank of parameter decoder/layers_18/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
W0512 23:45:31.199366 140143678494720 adafactor.py:358] Since rank of parameter decoder/layers_16/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:31.187997 140643590531072 adafactor.py:358] Since rank of parameter decoder/layers_16/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:31.206504 139666521167872 adafactor.py:358] Since rank of parameter decoder/layers_17/mlp/router/router_weights/w/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>, <FactorDim.NONE: None>) is ignored.
W0512 23:45:31.188068 140260698863616 adafactor.py:358] Since rank of parameter decoder/layers_14/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:31.209729 139876724979712 adafactor.py:358] Since rank of parameter decoder/layers_14/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:31.210365 139856202532864 adafactor.py:358] Since rank of parameter decoder/layers_16/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:31.210505 140143678494720 adafactor.py:358] Since rank of parameter decoder/layers_16/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:31.195420 140180528838656 adafactor.py:358] Since rank of parameter decoder/layers_16/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:31.213236 140193652815872 adafactor.py:358] Since rank of parameter decoder/layers_18/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:31.216423 139666521167872 adafactor.py:358] Since rank of parameter decoder/layers_17/pre_extra_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:31.201880 140643590531072 adafactor.py:358] Since rank of parameter decoder/layers_16/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:31.202838 140260698863616 adafactor.py:358] Since rank of parameter decoder/layers_15/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:31.220680 140143678494720 adafactor.py:358] Since rank of parameter decoder/layers_16/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:31.223674 139876724979712 adafactor.py:358] Since rank of parameter decoder/layers_14/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:31.223438 140193652815872 adafactor.py:358] Since rank of parameter decoder/layers_18/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:31.224513 139856202532864 adafactor.py:358] Since rank of parameter decoder/layers_16/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:31.226151 139666521167872 adafactor.py:358] Since rank of parameter decoder/layers_17/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:31.209225 140180528838656 adafactor.py:358] Since rank of parameter decoder/layers_16/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:31.216050 140643590531072 adafactor.py:358] Since rank of parameter decoder/layers_16/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:31.233574 140193652815872 adafactor.py:358] Since rank of parameter decoder/layers_18/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:31.235766 139666521167872 adafactor.py:358] Since rank of parameter decoder/layers_17/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:31.234562 140143678494720 adafactor.py:358] Since rank of parameter decoder/layers_16/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:31.218060 140260698863616 adafactor.py:358] Since rank of parameter decoder/layers_15/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:31.237673 139876724979712 adafactor.py:358] Since rank of parameter decoder/layers_14/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:31.238523 139856202532864 adafactor.py:358] Since rank of parameter decoder/layers_16/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:31.222952 140180528838656 adafactor.py:358] Since rank of parameter decoder/layers_17/extra_mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:31.245438 139666521167872 adafactor.py:358] Since rank of parameter decoder/layers_17/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:31.247303 140193652815872 adafactor.py:358] Since rank of parameter decoder/layers_18/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:31.229894 140643590531072 adafactor.py:358] Since rank of parameter decoder/layers_16/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:31.248394 140143678494720 adafactor.py:358] Since rank of parameter decoder/layers_16/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:31.251613 139876724979712 adafactor.py:358] Since rank of parameter decoder/layers_15/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:31.252508 139856202532864 adafactor.py:358] Since rank of parameter decoder/layers_17/extra_mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:31.233435 140260698863616 adafactor.py:358] Since rank of parameter decoder/layers_15/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:31.236715 140180528838656 adafactor.py:358] Since rank of parameter decoder/layers_17/extra_mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:31.258913 139666521167872 adafactor.py:358] Since rank of parameter decoder/layers_17/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:31.261197 140193652815872 adafactor.py:358] Since rank of parameter decoder/layers_18/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:31.243892 140643590531072 adafactor.py:358] Since rank of parameter decoder/layers_17/extra_mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:31.262163 140143678494720 adafactor.py:358] Since rank of parameter decoder/layers_16/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:31.266162 139876724979712 adafactor.py:358] Since rank of parameter decoder/layers_15/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:31.266533 139856202532864 adafactor.py:358] Since rank of parameter decoder/layers_17/extra_mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:31.250539 140180528838656 adafactor.py:358] Since rank of parameter decoder/layers_17/extra_mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:31.249499 140260698863616 adafactor.py:358] Since rank of parameter decoder/layers_15/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:31.272155 139666521167872 adafactor.py:358] Since rank of parameter decoder/layers_17/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:31.274976 140193652815872 adafactor.py:358] Since rank of parameter decoder/layers_18/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:31.257847 140643590531072 adafactor.py:358] Since rank of parameter decoder/layers_17/extra_mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:31.275878 140143678494720 adafactor.py:358] Since rank of parameter decoder/layers_17/extra_mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:31.260417 140260698863616 adafactor.py:358] Since rank of parameter decoder/layers_15/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:31.280573 139856202532864 adafactor.py:358] Since rank of parameter decoder/layers_17/extra_mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:31.280125 139876724979712 adafactor.py:358] Since rank of parameter decoder/layers_15/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:31.288086 139666521167872 adafactor.py:358] Since rank of parameter decoder/layers_17/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:31.289052 140193652815872 adafactor.py:358] Since rank of parameter decoder/layers_19/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:31.272088 140643590531072 adafactor.py:358] Since rank of parameter decoder/layers_17/extra_mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:31.271073 140260698863616 adafactor.py:358] Since rank of parameter decoder/layers_15/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:31.289731 140143678494720 adafactor.py:358] Since rank of parameter decoder/layers_17/extra_mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:31.294124 139876724979712 adafactor.py:358] Since rank of parameter decoder/layers_15/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:31.302476 139666521167872 adafactor.py:358] Since rank of parameter decoder/layers_18/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:31.302822 140193652815872 adafactor.py:358] Since rank of parameter decoder/layers_19/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:31.304421 139876724979712 adafactor.py:358] Since rank of parameter decoder/layers_15/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:31.285832 140260698863616 adafactor.py:358] Since rank of parameter decoder/layers_15/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:31.303652 140143678494720 adafactor.py:358] Since rank of parameter decoder/layers_17/extra_mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:31.314626 139876724979712 adafactor.py:358] Since rank of parameter decoder/layers_15/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:31.316237 139666521167872 adafactor.py:358] Since rank of parameter decoder/layers_18/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:31.316606 140193652815872 adafactor.py:358] Since rank of parameter decoder/layers_19/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:31.300666 140260698863616 adafactor.py:358] Since rank of parameter decoder/layers_15/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:31.306820 140180528838656 adafactor.py:358] Since rank of parameter decoder/layers_17/mlp/router/router_weights/w/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>, <FactorDim.NONE: None>) is ignored.
W0512 23:45:31.328583 139876724979712 adafactor.py:358] Since rank of parameter decoder/layers_15/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:31.329520 139666521167872 adafactor.py:358] Since rank of parameter decoder/layers_18/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:31.330802 140193652815872 adafactor.py:358] Since rank of parameter decoder/layers_19/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:31.315312 140260698863616 adafactor.py:358] Since rank of parameter decoder/layers_15/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:31.317091 140180528838656 adafactor.py:358] Since rank of parameter decoder/layers_17/pre_extra_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:31.337414 139856202532864 adafactor.py:358] Since rank of parameter decoder/layers_17/mlp/router/router_weights/w/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>, <FactorDim.NONE: None>) is ignored.
W0512 23:45:31.340921 140193652815872 adafactor.py:358] Since rank of parameter decoder/layers_19/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:31.342446 139876724979712 adafactor.py:358] Since rank of parameter decoder/layers_15/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:31.342761 139666521167872 adafactor.py:358] Since rank of parameter decoder/layers_18/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:31.327219 140180528838656 adafactor.py:358] Since rank of parameter decoder/layers_17/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:31.328926 140643590531072 adafactor.py:358] Since rank of parameter decoder/layers_17/mlp/router/router_weights/w/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>, <FactorDim.NONE: None>) is ignored.
W0512 23:45:31.347932 139856202532864 adafactor.py:358] Since rank of parameter decoder/layers_17/pre_extra_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:31.351051 140193652815872 adafactor.py:358] Since rank of parameter decoder/layers_19/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:31.352573 139666521167872 adafactor.py:358] Since rank of parameter decoder/layers_18/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:31.334404 140260698863616 adafactor.py:358] Since rank of parameter decoder/layers_16/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:31.337355 140180528838656 adafactor.py:358] Since rank of parameter decoder/layers_17/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:31.356381 139876724979712 adafactor.py:358] Since rank of parameter decoder/layers_15/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:31.339548 140643590531072 adafactor.py:358] Since rank of parameter decoder/layers_17/pre_extra_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:31.358254 139856202532864 adafactor.py:358] Since rank of parameter decoder/layers_17/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:31.362219 139666521167872 adafactor.py:358] Since rank of parameter decoder/layers_18/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:31.360065 140143678494720 adafactor.py:358] Since rank of parameter decoder/layers_17/mlp/router/router_weights/w/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>, <FactorDim.NONE: None>) is ignored.
W0512 23:45:31.347434 140180528838656 adafactor.py:358] Since rank of parameter decoder/layers_17/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:31.364844 140193652815872 adafactor.py:358] Since rank of parameter decoder/layers_19/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:31.349936 140643590531072 adafactor.py:358] Since rank of parameter decoder/layers_17/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:31.368569 139856202532864 adafactor.py:358] Since rank of parameter decoder/layers_17/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:31.349220 140260698863616 adafactor.py:358] Since rank of parameter decoder/layers_16/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:31.370350 139876724979712 adafactor.py:358] Since rank of parameter decoder/layers_16/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:31.370592 140143678494720 adafactor.py:358] Since rank of parameter decoder/layers_17/pre_extra_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:31.375557 139666521167872 adafactor.py:358] Since rank of parameter decoder/layers_18/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:31.360111 140643590531072 adafactor.py:358] Since rank of parameter decoder/layers_17/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:31.378831 139856202532864 adafactor.py:358] Since rank of parameter decoder/layers_17/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:31.361115 140180528838656 adafactor.py:358] Since rank of parameter decoder/layers_17/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:31.378622 140193652815872 adafactor.py:358] Since rank of parameter decoder/layers_19/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:31.364042 140260698863616 adafactor.py:358] Since rank of parameter decoder/layers_16/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:31.380745 140143678494720 adafactor.py:358] Since rank of parameter decoder/layers_17/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:31.384283 139876724979712 adafactor.py:358] Since rank of parameter decoder/layers_16/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:31.370198 140643590531072 adafactor.py:358] Since rank of parameter decoder/layers_17/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:31.388766 139666521167872 adafactor.py:358] Since rank of parameter decoder/layers_18/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:31.374811 140180528838656 adafactor.py:358] Since rank of parameter decoder/layers_17/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:31.392485 140193652815872 adafactor.py:358] Since rank of parameter decoder/layers_19/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:31.390860 140143678494720 adafactor.py:358] Since rank of parameter decoder/layers_17/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:31.395046 139856202532864 adafactor.py:358] Since rank of parameter decoder/layers_17/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:31.378649 140260698863616 adafactor.py:358] Since rank of parameter decoder/layers_16/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:31.398630 139876724979712 adafactor.py:358] Since rank of parameter decoder/layers_16/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:31.401927 139666521167872 adafactor.py:358] Since rank of parameter decoder/layers_18/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:31.384169 140643590531072 adafactor.py:358] Since rank of parameter decoder/layers_17/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:31.401017 140143678494720 adafactor.py:358] Since rank of parameter decoder/layers_17/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:31.406247 140193652815872 adafactor.py:358] Since rank of parameter decoder/layers_2/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:31.409057 139856202532864 adafactor.py:358] Since rank of parameter decoder/layers_17/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:31.390989 140180528838656 adafactor.py:358] Since rank of parameter decoder/layers_17/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:31.390110 140260698863616 adafactor.py:358] Since rank of parameter decoder/layers_16/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:31.412519 139876724979712 adafactor.py:358] Since rank of parameter decoder/layers_16/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:31.414982 139666521167872 adafactor.py:358] Since rank of parameter decoder/layers_19/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:31.397991 140643590531072 adafactor.py:358] Since rank of parameter decoder/layers_17/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:31.414918 140143678494720 adafactor.py:358] Since rank of parameter decoder/layers_17/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:31.419943 140193652815872 adafactor.py:358] Since rank of parameter decoder/layers_2/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:31.402024 140260698863616 adafactor.py:358] Since rank of parameter decoder/layers_16/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:31.422735 139876724979712 adafactor.py:358] Since rank of parameter decoder/layers_16/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:31.404794 140180528838656 adafactor.py:358] Since rank of parameter decoder/layers_18/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:31.425448 139856202532864 adafactor.py:358] Since rank of parameter decoder/layers_17/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:31.428864 139666521167872 adafactor.py:358] Since rank of parameter decoder/layers_19/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:31.428756 140143678494720 adafactor.py:358] Since rank of parameter decoder/layers_17/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:31.432976 139876724979712 adafactor.py:358] Since rank of parameter decoder/layers_16/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:31.414884 140643590531072 adafactor.py:358] Since rank of parameter decoder/layers_17/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:31.433735 140193652815872 adafactor.py:358] Since rank of parameter decoder/layers_2/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:31.416861 140260698863616 adafactor.py:358] Since rank of parameter decoder/layers_16/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:31.418563 140180528838656 adafactor.py:358] Since rank of parameter decoder/layers_18/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:31.439494 139856202532864 adafactor.py:358] Since rank of parameter decoder/layers_18/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:31.442213 139666521167872 adafactor.py:358] Since rank of parameter decoder/layers_19/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:31.446876 139876724979712 adafactor.py:358] Since rank of parameter decoder/layers_16/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:31.429483 140643590531072 adafactor.py:358] Since rank of parameter decoder/layers_18/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:31.445107 140143678494720 adafactor.py:358] Since rank of parameter decoder/layers_17/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:31.447489 140193652815872 adafactor.py:358] Since rank of parameter decoder/layers_2/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:31.432778 140180528838656 adafactor.py:358] Since rank of parameter decoder/layers_18/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:31.431720 140260698863616 adafactor.py:358] Since rank of parameter decoder/layers_16/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:31.453495 139856202532864 adafactor.py:358] Since rank of parameter decoder/layers_18/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:31.455470 139666521167872 adafactor.py:358] Since rank of parameter decoder/layers_19/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:31.458050 140193652815872 adafactor.py:358] Since rank of parameter decoder/layers_2/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:31.460823 139876724979712 adafactor.py:358] Since rank of parameter decoder/layers_16/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:31.458811 140143678494720 adafactor.py:358] Since rank of parameter decoder/layers_18/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:31.443700 140643590531072 adafactor.py:358] Since rank of parameter decoder/layers_18/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:31.446594 140180528838656 adafactor.py:358] Since rank of parameter decoder/layers_18/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:31.465210 139666521167872 adafactor.py:358] Since rank of parameter decoder/layers_19/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:31.446178 140260698863616 adafactor.py:358] Since rank of parameter decoder/layers_16/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:31.467981 139856202532864 adafactor.py:358] Since rank of parameter decoder/layers_18/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:31.468153 140193652815872 adafactor.py:358] Since rank of parameter decoder/layers_2/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:31.474734 139876724979712 adafactor.py:358] Since rank of parameter decoder/layers_16/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:31.456745 140180528838656 adafactor.py:358] Since rank of parameter decoder/layers_18/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:31.474854 139666521167872 adafactor.py:358] Since rank of parameter decoder/layers_19/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:31.473073 140143678494720 adafactor.py:358] Since rank of parameter decoder/layers_18/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:31.458342 140643590531072 adafactor.py:358] Since rank of parameter decoder/layers_18/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:31.460539 140260698863616 adafactor.py:358] Since rank of parameter decoder/layers_17/extra_mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:31.481951 139856202532864 adafactor.py:358] Since rank of parameter decoder/layers_18/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:31.482002 140193652815872 adafactor.py:358] Since rank of parameter decoder/layers_2/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:31.466880 140180528838656 adafactor.py:358] Since rank of parameter decoder/layers_18/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:31.488219 139666521167872 adafactor.py:358] Since rank of parameter decoder/layers_19/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:31.488805 139876724979712 adafactor.py:358] Since rank of parameter decoder/layers_17/extra_mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:31.486905 140143678494720 adafactor.py:358] Since rank of parameter decoder/layers_18/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:31.473045 140643590531072 adafactor.py:358] Since rank of parameter decoder/layers_18/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:31.492223 139856202532864 adafactor.py:358] Since rank of parameter decoder/layers_18/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:31.475175 140260698863616 adafactor.py:358] Since rank of parameter decoder/layers_17/extra_mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:31.495811 140193652815872 adafactor.py:358] Since rank of parameter decoder/layers_2/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:31.480562 140180528838656 adafactor.py:358] Since rank of parameter decoder/layers_18/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:31.501547 139666521167872 adafactor.py:358] Since rank of parameter decoder/layers_19/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:31.484010 140643590531072 adafactor.py:358] Since rank of parameter decoder/layers_18/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:31.502561 139856202532864 adafactor.py:358] Since rank of parameter decoder/layers_18/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:31.502708 139876724979712 adafactor.py:358] Since rank of parameter decoder/layers_17/extra_mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:31.500702 140143678494720 adafactor.py:358] Since rank of parameter decoder/layers_18/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:31.490033 140260698863616 adafactor.py:358] Since rank of parameter decoder/layers_17/extra_mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:31.509610 140193652815872 adafactor.py:358] Since rank of parameter decoder/layers_2/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:31.494275 140643590531072 adafactor.py:358] Since rank of parameter decoder/layers_18/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:31.494354 140180528838656 adafactor.py:358] Since rank of parameter decoder/layers_18/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:31.510820 140143678494720 adafactor.py:358] Since rank of parameter decoder/layers_18/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:31.516481 139856202532864 adafactor.py:358] Since rank of parameter decoder/layers_18/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:31.516718 139876724979712 adafactor.py:358] Since rank of parameter decoder/layers_17/extra_mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:31.516841 139666521167872 adafactor.py:358] Since rank of parameter decoder/layers_19/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:31.521214 140143678494720 adafactor.py:358] Since rank of parameter decoder/layers_18/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:31.523374 140193652815872 adafactor.py:358] Since rank of parameter decoder/layers_20/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:31.508099 140643590531072 adafactor.py:358] Since rank of parameter decoder/layers_18/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:31.508089 140180528838656 adafactor.py:358] Since rank of parameter decoder/layers_18/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:31.530509 139856202532864 adafactor.py:358] Since rank of parameter decoder/layers_18/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:31.531459 139666521167872 adafactor.py:358] Since rank of parameter decoder/layers_2/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:31.534905 140143678494720 adafactor.py:358] Since rank of parameter decoder/layers_18/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:31.537218 140193652815872 adafactor.py:358] Since rank of parameter decoder/layers_20/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:31.521796 140180528838656 adafactor.py:358] Since rank of parameter decoder/layers_19/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:31.521957 140643590531072 adafactor.py:358] Since rank of parameter decoder/layers_18/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:31.544496 139856202532864 adafactor.py:358] Since rank of parameter decoder/layers_18/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:31.545591 139666521167872 adafactor.py:358] Since rank of parameter decoder/layers_2/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:31.551149 140193652815872 adafactor.py:358] Since rank of parameter decoder/layers_20/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:31.549813 140143678494720 adafactor.py:358] Since rank of parameter decoder/layers_18/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:31.535600 140643590531072 adafactor.py:358] Since rank of parameter decoder/layers_18/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:31.535503 140180528838656 adafactor.py:358] Since rank of parameter decoder/layers_19/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:31.558428 139856202532864 adafactor.py:358] Since rank of parameter decoder/layers_19/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:31.560465 139666521167872 adafactor.py:358] Since rank of parameter decoder/layers_2/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:31.563607 140143678494720 adafactor.py:358] Since rank of parameter decoder/layers_18/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
W0512 23:45:31.549239 140643590531072 adafactor.py:358] Since rank of parameter decoder/layers_19/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:31.549237 140180528838656 adafactor.py:358] Since rank of parameter decoder/layers_19/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:31.569661 140193652815872 adafactor.py:358] Since rank of parameter decoder/layers_20/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:31.552553 140260698863616 adafactor.py:358] Since rank of parameter decoder/layers_17/mlp/router/router_weights/w/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>, <FactorDim.NONE: None>) is ignored.
W0512 23:45:31.572393 139856202532864 adafactor.py:358] Since rank of parameter decoder/layers_19/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:31.573566 139876724979712 adafactor.py:358] Since rank of parameter decoder/layers_17/mlp/router/router_weights/w/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>, <FactorDim.NONE: None>) is ignored.
W0512 23:45:31.574445 139666521167872 adafactor.py:358] Since rank of parameter decoder/layers_2/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:31.577368 140143678494720 adafactor.py:358] Since rank of parameter decoder/layers_19/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:31.579901 140193652815872 adafactor.py:358] Since rank of parameter decoder/layers_20/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:31.563470 140643590531072 adafactor.py:358] Since rank of parameter decoder/layers_19/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:31.563373 140180528838656 adafactor.py:358] Since rank of parameter decoder/layers_19/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:31.564275 140260698863616 adafactor.py:358] Since rank of parameter decoder/layers_17/pre_extra_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:31.584030 139876724979712 adafactor.py:358] Since rank of parameter decoder/layers_17/pre_extra_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:31.584877 139666521167872 adafactor.py:358] Since rank of parameter decoder/layers_2/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:31.586454 139856202532864 adafactor.py:358] Since rank of parameter decoder/layers_19/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:31.590083 140193652815872 adafactor.py:358] Since rank of parameter decoder/layers_20/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:31.573502 140180528838656 adafactor.py:358] Since rank of parameter decoder/layers_19/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:31.591046 140143678494720 adafactor.py:358] Since rank of parameter decoder/layers_19/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:31.594284 139876724979712 adafactor.py:358] Since rank of parameter decoder/layers_17/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:31.575344 140260698863616 adafactor.py:358] Since rank of parameter decoder/layers_17/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:31.595001 139666521167872 adafactor.py:358] Since rank of parameter decoder/layers_2/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:31.577359 140643590531072 adafactor.py:358] Since rank of parameter decoder/layers_19/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:31.600959 139856202532864 adafactor.py:358] Since rank of parameter decoder/layers_19/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:31.583549 140180528838656 adafactor.py:358] Since rank of parameter decoder/layers_19/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:31.604576 139876724979712 adafactor.py:358] Since rank of parameter decoder/layers_17/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:31.585839 140260698863616 adafactor.py:358] Since rank of parameter decoder/layers_17/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:31.604541 140193652815872 adafactor.py:358] Since rank of parameter decoder/layers_20/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:31.604809 140143678494720 adafactor.py:358] Since rank of parameter decoder/layers_19/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:31.591168 140643590531072 adafactor.py:358] Since rank of parameter decoder/layers_19/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:31.609078 139666521167872 adafactor.py:358] Since rank of parameter decoder/layers_2/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:31.611231 139856202532864 adafactor.py:358] Since rank of parameter decoder/layers_19/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:31.614784 139876724979712 adafactor.py:358] Since rank of parameter decoder/layers_17/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:31.597213 140180528838656 adafactor.py:358] Since rank of parameter decoder/layers_19/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:31.618373 140193652815872 adafactor.py:358] Since rank of parameter decoder/layers_20/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:31.600738 140260698863616 adafactor.py:358] Since rank of parameter decoder/layers_17/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:31.621453 139856202532864 adafactor.py:358] Since rank of parameter decoder/layers_19/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:31.619075 140143678494720 adafactor.py:358] Since rank of parameter decoder/layers_19/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:31.623399 139666521167872 adafactor.py:358] Since rank of parameter decoder/layers_2/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:31.628704 139876724979712 adafactor.py:358] Since rank of parameter decoder/layers_17/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:31.610905 140180528838656 adafactor.py:358] Since rank of parameter decoder/layers_19/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:31.629223 140143678494720 adafactor.py:358] Since rank of parameter decoder/layers_19/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:31.632132 140193652815872 adafactor.py:358] Since rank of parameter decoder/layers_20/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:31.615825 140643590531072 adafactor.py:358] Since rank of parameter decoder/layers_19/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:31.615499 140260698863616 adafactor.py:358] Since rank of parameter decoder/layers_17/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:31.635469 139856202532864 adafactor.py:358] Since rank of parameter decoder/layers_19/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:31.638048 139666521167872 adafactor.py:358] Since rank of parameter decoder/layers_2/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:31.639377 140143678494720 adafactor.py:358] Since rank of parameter decoder/layers_19/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:31.642576 139876724979712 adafactor.py:358] Since rank of parameter decoder/layers_17/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:31.624697 140180528838656 adafactor.py:358] Since rank of parameter decoder/layers_19/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
W0512 23:45:31.625963 140643590531072 adafactor.py:358] Since rank of parameter decoder/layers_19/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:31.645961 140193652815872 adafactor.py:358] Since rank of parameter decoder/layers_21/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:31.629913 140260698863616 adafactor.py:358] Since rank of parameter decoder/layers_17/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:31.650417 139856202532864 adafactor.py:358] Since rank of parameter decoder/layers_19/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:31.652820 139666521167872 adafactor.py:358] Since rank of parameter decoder/layers_20/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:31.653151 140143678494720 adafactor.py:358] Since rank of parameter decoder/layers_19/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:31.638411 140180528838656 adafactor.py:358] Since rank of parameter decoder/layers_2/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:31.640150 140643590531072 adafactor.py:358] Since rank of parameter decoder/layers_19/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:31.658777 139876724979712 adafactor.py:358] Since rank of parameter decoder/layers_17/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:31.659796 140193652815872 adafactor.py:358] Since rank of parameter decoder/layers_21/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:31.647048 140260698863616 adafactor.py:358] Since rank of parameter decoder/layers_17/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:31.667044 139666521167872 adafactor.py:358] Since rank of parameter decoder/layers_20/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:31.669432 139856202532864 adafactor.py:358] Since rank of parameter decoder/layers_19/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:31.666916 140143678494720 adafactor.py:358] Since rank of parameter decoder/layers_19/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:31.652112 140180528838656 adafactor.py:358] Since rank of parameter decoder/layers_2/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:31.654174 140643590531072 adafactor.py:358] Since rank of parameter decoder/layers_19/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:31.673208 139876724979712 adafactor.py:358] Since rank of parameter decoder/layers_18/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:31.673685 140193652815872 adafactor.py:358] Since rank of parameter decoder/layers_21/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:31.661415 140260698863616 adafactor.py:358] Since rank of parameter decoder/layers_18/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:31.681556 139666521167872 adafactor.py:358] Since rank of parameter decoder/layers_20/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:31.680650 140143678494720 adafactor.py:358] Since rank of parameter decoder/layers_19/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:31.665839 140180528838656 adafactor.py:358] Since rank of parameter decoder/layers_2/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:31.685902 139856202532864 adafactor.py:358] Since rank of parameter decoder/layers_2/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:31.668025 140643590531072 adafactor.py:358] Since rank of parameter decoder/layers_19/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:31.687119 139876724979712 adafactor.py:358] Since rank of parameter decoder/layers_18/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:31.687432 140193652815872 adafactor.py:358] Since rank of parameter decoder/layers_21/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:31.675815 140260698863616 adafactor.py:358] Since rank of parameter decoder/layers_18/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:31.696171 139666521167872 adafactor.py:358] Since rank of parameter decoder/layers_20/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:31.694416 140143678494720 adafactor.py:358] Since rank of parameter decoder/layers_2/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:31.679550 140180528838656 adafactor.py:358] Since rank of parameter decoder/layers_2/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:31.697579 140193652815872 adafactor.py:358] Since rank of parameter decoder/layers_21/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:31.681771 140643590531072 adafactor.py:358] Since rank of parameter decoder/layers_2/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:31.700963 139876724979712 adafactor.py:358] Since rank of parameter decoder/layers_18/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:31.701141 139856202532864 adafactor.py:358] Since rank of parameter decoder/layers_2/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:31.706530 139666521167872 adafactor.py:358] Since rank of parameter decoder/layers_20/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:31.690190 140180528838656 adafactor.py:358] Since rank of parameter decoder/layers_2/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:31.707687 140193652815872 adafactor.py:358] Since rank of parameter decoder/layers_21/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:31.691385 140260698863616 adafactor.py:358] Since rank of parameter decoder/layers_18/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:31.708117 140143678494720 adafactor.py:358] Since rank of parameter decoder/layers_2/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
W0512 23:45:31.695593 140643590531072 adafactor.py:358] Since rank of parameter decoder/layers_2/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:31.714914 139876724979712 adafactor.py:358] Since rank of parameter decoder/layers_18/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:31.716158 139856202532864 adafactor.py:358] Since rank of parameter decoder/layers_2/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:31.717289 139666521167872 adafactor.py:358] Since rank of parameter decoder/layers_20/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:31.700340 140180528838656 adafactor.py:358] Since rank of parameter decoder/layers_2/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:31.721469 140193652815872 adafactor.py:358] Since rank of parameter decoder/layers_21/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:31.722026 140143678494720 adafactor.py:358] Since rank of parameter decoder/layers_2/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:31.706057 140260698863616 adafactor.py:358] Since rank of parameter decoder/layers_18/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:31.727266 139876724979712 adafactor.py:358] Since rank of parameter decoder/layers_18/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:31.710113 140643590531072 adafactor.py:358] Since rank of parameter decoder/layers_2/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
W0512 23:45:31.731776 139856202532864 adafactor.py:358] Since rank of parameter decoder/layers_2/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:31.731553 139666521167872 adafactor.py:358] Since rank of parameter decoder/layers_20/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:31.715077 140180528838656 adafactor.py:358] Since rank of parameter decoder/layers_2/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:31.716860 140260698863616 adafactor.py:358] Since rank of parameter decoder/layers_18/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:31.735699 140193652815872 adafactor.py:358] Since rank of parameter decoder/layers_21/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:31.739381 139876724979712 adafactor.py:358] Since rank of parameter decoder/layers_18/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:31.738314 140143678494720 adafactor.py:358] Since rank of parameter decoder/layers_2/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:31.723911 140643590531072 adafactor.py:358] Since rank of parameter decoder/layers_2/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:31.745183 139856202532864 adafactor.py:358] Since rank of parameter decoder/layers_2/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:31.745764 139666521167872 adafactor.py:358] Since rank of parameter decoder/layers_20/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:31.727308 140260698863616 adafactor.py:358] Since rank of parameter decoder/layers_18/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:31.729012 140180528838656 adafactor.py:358] Since rank of parameter decoder/layers_2/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:31.749531 140193652815872 adafactor.py:358] Since rank of parameter decoder/layers_21/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:31.734024 140643590531072 adafactor.py:358] Since rank of parameter decoder/layers_2/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:31.749598 140143678494720 adafactor.py:358] Since rank of parameter decoder/layers_2/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:31.755634 139876724979712 adafactor.py:358] Since rank of parameter decoder/layers_18/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:31.756054 139856202532864 adafactor.py:358] Since rank of parameter decoder/layers_2/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:31.760021 139666521167872 adafactor.py:358] Since rank of parameter decoder/layers_20/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:31.741780 140260698863616 adafactor.py:358] Since rank of parameter decoder/layers_18/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:31.742777 140180528838656 adafactor.py:358] Since rank of parameter decoder/layers_2/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:31.744008 140643590531072 adafactor.py:358] Since rank of parameter decoder/layers_2/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:31.759692 140143678494720 adafactor.py:358] Since rank of parameter decoder/layers_2/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:31.763356 140193652815872 adafactor.py:358] Since rank of parameter decoder/layers_22/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:31.770906 139856202532864 adafactor.py:358] Since rank of parameter decoder/layers_2/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:31.772645 139876724979712 adafactor.py:358] Since rank of parameter decoder/layers_18/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:31.774052 139666521167872 adafactor.py:358] Since rank of parameter decoder/layers_21/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:31.756511 140180528838656 adafactor.py:358] Since rank of parameter decoder/layers_20/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:31.756206 140260698863616 adafactor.py:358] Since rank of parameter decoder/layers_18/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:31.757749 140643590531072 adafactor.py:358] Since rank of parameter decoder/layers_2/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:31.773337 140143678494720 adafactor.py:358] Since rank of parameter decoder/layers_2/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:31.777093 140193652815872 adafactor.py:358] Since rank of parameter decoder/layers_22/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:31.787043 139856202532864 adafactor.py:358] Since rank of parameter decoder/layers_2/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:31.787936 139666521167872 adafactor.py:358] Since rank of parameter decoder/layers_21/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:31.770463 140180528838656 adafactor.py:358] Since rank of parameter decoder/layers_20/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:31.770307 140260698863616 adafactor.py:358] Since rank of parameter decoder/layers_18/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:31.771645 140643590531072 adafactor.py:358] Since rank of parameter decoder/layers_2/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:31.789842 139876724979712 adafactor.py:358] Since rank of parameter decoder/layers_18/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:31.787125 140143678494720 adafactor.py:358] Since rank of parameter decoder/layers_2/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:31.790845 140193652815872 adafactor.py:358] Since rank of parameter decoder/layers_22/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:31.802029 139666521167872 adafactor.py:358] Since rank of parameter decoder/layers_21/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:31.784379 140180528838656 adafactor.py:358] Since rank of parameter decoder/layers_20/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:31.800794 140143678494720 adafactor.py:358] Since rank of parameter decoder/layers_2/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:31.785634 140643590531072 adafactor.py:358] Since rank of parameter decoder/layers_2/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:31.784506 140260698863616 adafactor.py:358] Since rank of parameter decoder/layers_19/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:31.804249 139876724979712 adafactor.py:358] Since rank of parameter decoder/layers_19/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:31.805909 139856202532864 adafactor.py:358] Since rank of parameter decoder/layers_2/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:31.815885 139666521167872 adafactor.py:358] Since rank of parameter decoder/layers_21/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:31.815217 140193652815872 adafactor.py:358] Since rank of parameter decoder/layers_22/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:31.798118 140180528838656 adafactor.py:358] Since rank of parameter decoder/layers_20/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:31.814458 140143678494720 adafactor.py:358] Since rank of parameter decoder/layers_20/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:31.799506 140643590531072 adafactor.py:358] Since rank of parameter decoder/layers_20/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:31.798964 140260698863616 adafactor.py:358] Since rank of parameter decoder/layers_19/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:31.820162 139876724979712 adafactor.py:358] Since rank of parameter decoder/layers_19/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:31.821244 139856202532864 adafactor.py:358] Since rank of parameter decoder/layers_20/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:31.825442 140193652815872 adafactor.py:358] Since rank of parameter decoder/layers_22/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:31.808250 140180528838656 adafactor.py:358] Since rank of parameter decoder/layers_20/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:31.826516 139666521167872 adafactor.py:358] Since rank of parameter decoder/layers_21/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:31.828192 140143678494720 adafactor.py:358] Since rank of parameter decoder/layers_20/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:31.813783 140643590531072 adafactor.py:358] Since rank of parameter decoder/layers_20/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:31.813342 140260698863616 adafactor.py:358] Since rank of parameter decoder/layers_19/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:31.835775 139856202532864 adafactor.py:358] Since rank of parameter decoder/layers_20/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:31.835499 140193652815872 adafactor.py:358] Since rank of parameter decoder/layers_22/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:31.836937 139876724979712 adafactor.py:358] Since rank of parameter decoder/layers_19/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:31.836715 139666521167872 adafactor.py:358] Since rank of parameter decoder/layers_21/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:31.821006 140180528838656 adafactor.py:358] Since rank of parameter decoder/layers_20/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:31.842296 140143678494720 adafactor.py:358] Since rank of parameter decoder/layers_20/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:31.828092 140643590531072 adafactor.py:358] Since rank of parameter decoder/layers_20/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:31.828715 140260698863616 adafactor.py:358] Since rank of parameter decoder/layers_19/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:31.849434 140193652815872 adafactor.py:358] Since rank of parameter decoder/layers_22/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:31.850507 139666521167872 adafactor.py:358] Since rank of parameter decoder/layers_21/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:31.851750 139876724979712 adafactor.py:358] Since rank of parameter decoder/layers_19/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:31.853083 139856202532864 adafactor.py:358] Since rank of parameter decoder/layers_20/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:31.835617 140180528838656 adafactor.py:358] Since rank of parameter decoder/layers_20/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
W0512 23:45:31.856020 140143678494720 adafactor.py:358] Since rank of parameter decoder/layers_20/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:31.840098 140260698863616 adafactor.py:358] Since rank of parameter decoder/layers_19/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:31.843132 140643590531072 adafactor.py:358] Since rank of parameter decoder/layers_20/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:31.863994 140193652815872 adafactor.py:358] Since rank of parameter decoder/layers_22/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:31.864754 139666521167872 adafactor.py:358] Since rank of parameter decoder/layers_21/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:31.864850 139876724979712 adafactor.py:358] Since rank of parameter decoder/layers_19/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:31.849521 140180528838656 adafactor.py:358] Since rank of parameter decoder/layers_20/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:31.868045 139856202532864 adafactor.py:358] Since rank of parameter decoder/layers_20/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:31.866106 140143678494720 adafactor.py:358] Since rank of parameter decoder/layers_20/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:31.852246 140260698863616 adafactor.py:358] Since rank of parameter decoder/layers_19/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:31.853658 140643590531072 adafactor.py:358] Since rank of parameter decoder/layers_20/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:31.876066 139876724979712 adafactor.py:358] Since rank of parameter decoder/layers_19/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:31.878994 139856202532864 adafactor.py:358] Since rank of parameter decoder/layers_20/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:31.878817 139666521167872 adafactor.py:358] Since rank of parameter decoder/layers_21/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:31.878289 140193652815872 adafactor.py:358] Since rank of parameter decoder/layers_22/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:31.876683 140143678494720 adafactor.py:358] Since rank of parameter decoder/layers_20/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:31.863144 140180528838656 adafactor.py:358] Since rank of parameter decoder/layers_20/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:31.864024 140643590531072 adafactor.py:358] Since rank of parameter decoder/layers_20/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:31.869150 140260698863616 adafactor.py:358] Since rank of parameter decoder/layers_19/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:31.889848 139856202532864 adafactor.py:358] Since rank of parameter decoder/layers_20/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:31.891035 139876724979712 adafactor.py:358] Since rank of parameter decoder/layers_19/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:31.892010 140193652815872 adafactor.py:358] Since rank of parameter decoder/layers_23/extra_mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:31.890370 140143678494720 adafactor.py:358] Since rank of parameter decoder/layers_20/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:31.893064 139666521167872 adafactor.py:358] Since rank of parameter decoder/layers_22/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:31.876844 140180528838656 adafactor.py:358] Since rank of parameter decoder/layers_21/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:31.878108 140643590531072 adafactor.py:358] Since rank of parameter decoder/layers_20/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:31.905435 139856202532864 adafactor.py:358] Since rank of parameter decoder/layers_20/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:31.886007 140260698863616 adafactor.py:358] Since rank of parameter decoder/layers_19/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:31.905683 139876724979712 adafactor.py:358] Since rank of parameter decoder/layers_19/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:31.905892 140193652815872 adafactor.py:358] Since rank of parameter decoder/layers_23/extra_mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:31.906867 139666521167872 adafactor.py:358] Since rank of parameter decoder/layers_22/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:31.904342 140143678494720 adafactor.py:358] Since rank of parameter decoder/layers_20/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:31.890742 140180528838656 adafactor.py:358] Since rank of parameter decoder/layers_21/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:31.892054 140643590531072 adafactor.py:358] Since rank of parameter decoder/layers_20/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:31.920361 139856202532864 adafactor.py:358] Since rank of parameter decoder/layers_20/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:31.919615 140193652815872 adafactor.py:358] Since rank of parameter decoder/layers_23/extra_mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:31.918165 140143678494720 adafactor.py:358] Since rank of parameter decoder/layers_20/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:31.920785 139666521167872 adafactor.py:358] Since rank of parameter decoder/layers_22/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:31.902189 140260698863616 adafactor.py:358] Since rank of parameter decoder/layers_19/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:31.922244 139876724979712 adafactor.py:358] Since rank of parameter decoder/layers_19/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:31.904605 140180528838656 adafactor.py:358] Since rank of parameter decoder/layers_21/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:31.905895 140643590531072 adafactor.py:358] Since rank of parameter decoder/layers_20/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:31.934705 139666521167872 adafactor.py:358] Since rank of parameter decoder/layers_22/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:31.932101 140143678494720 adafactor.py:358] Since rank of parameter decoder/layers_21/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:31.934997 139856202532864 adafactor.py:358] Since rank of parameter decoder/layers_20/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:31.918420 140180528838656 adafactor.py:358] Since rank of parameter decoder/layers_21/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:31.937292 139876724979712 adafactor.py:358] Since rank of parameter decoder/layers_2/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:31.918182 140260698863616 adafactor.py:358] Since rank of parameter decoder/layers_2/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:31.919714 140643590531072 adafactor.py:358] Since rank of parameter decoder/layers_21/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:31.944856 139666521167872 adafactor.py:358] Since rank of parameter decoder/layers_22/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:31.928546 140180528838656 adafactor.py:358] Since rank of parameter decoder/layers_21/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:31.945839 140143678494720 adafactor.py:358] Since rank of parameter decoder/layers_21/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:31.950023 139856202532864 adafactor.py:358] Since rank of parameter decoder/layers_21/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:31.933604 140643590531072 adafactor.py:358] Since rank of parameter decoder/layers_21/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:31.953045 139876724979712 adafactor.py:358] Since rank of parameter decoder/layers_2/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:31.934346 140260698863616 adafactor.py:358] Since rank of parameter decoder/layers_2/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:31.955030 139666521167872 adafactor.py:358] Since rank of parameter decoder/layers_22/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:31.938680 140180528838656 adafactor.py:358] Since rank of parameter decoder/layers_21/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:31.959592 140143678494720 adafactor.py:358] Since rank of parameter decoder/layers_21/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:31.965502 139856202532864 adafactor.py:358] Since rank of parameter decoder/layers_21/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:31.947528 140643590531072 adafactor.py:358] Since rank of parameter decoder/layers_21/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:31.968461 139876724979712 adafactor.py:358] Since rank of parameter decoder/layers_2/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:31.949400 140260698863616 adafactor.py:358] Since rank of parameter decoder/layers_2/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:31.969419 139666521167872 adafactor.py:358] Since rank of parameter decoder/layers_22/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:31.955206 140180528838656 adafactor.py:358] Since rank of parameter decoder/layers_21/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:31.974796 140193652815872 adafactor.py:358] Since rank of parameter decoder/layers_23/mlp/router/router_weights/w/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>, <FactorDim.NONE: None>) is ignored.
W0512 23:45:31.973427 140143678494720 adafactor.py:358] Since rank of parameter decoder/layers_21/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:31.961465 140643590531072 adafactor.py:358] Since rank of parameter decoder/layers_21/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:31.981417 139856202532864 adafactor.py:358] Since rank of parameter decoder/layers_21/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:31.983210 139876724979712 adafactor.py:358] Since rank of parameter decoder/layers_2/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:31.983378 139666521167872 adafactor.py:358] Since rank of parameter decoder/layers_22/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:31.985086 140193652815872 adafactor.py:358] Since rank of parameter decoder/layers_23/pre_extra_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:31.966744 140260698863616 adafactor.py:358] Since rank of parameter decoder/layers_2/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:31.983590 140143678494720 adafactor.py:358] Since rank of parameter decoder/layers_21/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:31.969708 140180528838656 adafactor.py:358] Since rank of parameter decoder/layers_21/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:31.972600 140643590531072 adafactor.py:358] Since rank of parameter decoder/layers_21/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:31.993537 139876724979712 adafactor.py:358] Since rank of parameter decoder/layers_2/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:31.995149 140193652815872 adafactor.py:358] Since rank of parameter decoder/layers_23/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:31.993692 140143678494720 adafactor.py:358] Since rank of parameter decoder/layers_21/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:31.996628 139856202532864 adafactor.py:358] Since rank of parameter decoder/layers_21/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:31.997221 139666521167872 adafactor.py:358] Since rank of parameter decoder/layers_22/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:31.978561 140260698863616 adafactor.py:358] Since rank of parameter decoder/layers_2/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:31.982744 140643590531072 adafactor.py:358] Since rank of parameter decoder/layers_21/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:31.983526 140180528838656 adafactor.py:358] Since rank of parameter decoder/layers_21/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:32.004153 139876724979712 adafactor.py:358] Since rank of parameter decoder/layers_2/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:32.005748 140193652815872 adafactor.py:358] Since rank of parameter decoder/layers_23/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:32.007509 139856202532864 adafactor.py:358] Since rank of parameter decoder/layers_21/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:31.990053 140260698863616 adafactor.py:358] Since rank of parameter decoder/layers_2/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:32.007895 140143678494720 adafactor.py:358] Since rank of parameter decoder/layers_21/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:32.011018 139666521167872 adafactor.py:358] Since rank of parameter decoder/layers_23/extra_mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:31.996733 140643590531072 adafactor.py:358] Since rank of parameter decoder/layers_21/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:31.997379 140180528838656 adafactor.py:358] Since rank of parameter decoder/layers_22/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:32.015802 140193652815872 adafactor.py:358] Since rank of parameter decoder/layers_23/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:32.018163 139856202532864 adafactor.py:358] Since rank of parameter decoder/layers_21/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:32.019224 139876724979712 adafactor.py:358] Since rank of parameter decoder/layers_2/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:32.021627 140143678494720 adafactor.py:358] Since rank of parameter decoder/layers_21/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:32.024968 139666521167872 adafactor.py:358] Since rank of parameter decoder/layers_23/extra_mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:32.007017 140260698863616 adafactor.py:358] Since rank of parameter decoder/layers_2/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:32.010572 140643590531072 adafactor.py:358] Since rank of parameter decoder/layers_21/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:32.011197 140180528838656 adafactor.py:358] Since rank of parameter decoder/layers_22/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:32.029530 140193652815872 adafactor.py:358] Since rank of parameter decoder/layers_23/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:32.032588 139856202532864 adafactor.py:358] Since rank of parameter decoder/layers_21/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:32.033510 139876724979712 adafactor.py:358] Since rank of parameter decoder/layers_2/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:32.035314 140143678494720 adafactor.py:358] Since rank of parameter decoder/layers_21/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:32.039765 139666521167872 adafactor.py:358] Since rank of parameter decoder/layers_23/extra_mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:32.022448 140260698863616 adafactor.py:358] Since rank of parameter decoder/layers_2/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:32.024546 140643590531072 adafactor.py:358] Since rank of parameter decoder/layers_21/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:32.025080 140180528838656 adafactor.py:358] Since rank of parameter decoder/layers_22/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:32.043267 140193652815872 adafactor.py:358] Since rank of parameter decoder/layers_23/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:32.047668 139856202532864 adafactor.py:358] Since rank of parameter decoder/layers_21/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:32.047710 139876724979712 adafactor.py:358] Since rank of parameter decoder/layers_2/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:32.049080 140143678494720 adafactor.py:358] Since rank of parameter decoder/layers_22/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:32.038329 140643590531072 adafactor.py:358] Since rank of parameter decoder/layers_22/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:32.037495 140260698863616 adafactor.py:358] Since rank of parameter decoder/layers_2/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:32.038931 140180528838656 adafactor.py:358] Since rank of parameter decoder/layers_22/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:32.057001 140193652815872 adafactor.py:358] Since rank of parameter decoder/layers_23/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:32.061974 139856202532864 adafactor.py:358] Since rank of parameter decoder/layers_21/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:32.061943 139876724979712 adafactor.py:358] Since rank of parameter decoder/layers_20/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:32.062792 140143678494720 adafactor.py:358] Since rank of parameter decoder/layers_22/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:32.049047 140180528838656 adafactor.py:358] Since rank of parameter decoder/layers_22/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:32.052175 140643590531072 adafactor.py:358] Since rank of parameter decoder/layers_22/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:32.052337 140260698863616 adafactor.py:358] Since rank of parameter decoder/layers_20/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:32.070783 140193652815872 adafactor.py:358] Since rank of parameter decoder/layers_3/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:32.076362 139856202532864 adafactor.py:358] Since rank of parameter decoder/layers_22/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:32.076958 139876724979712 adafactor.py:358] Since rank of parameter decoder/layers_20/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:32.059134 140180528838656 adafactor.py:358] Since rank of parameter decoder/layers_22/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:32.076461 140143678494720 adafactor.py:358] Since rank of parameter decoder/layers_22/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:32.066137 140643590531072 adafactor.py:358] Since rank of parameter decoder/layers_22/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:32.084560 140193652815872 adafactor.py:358] Since rank of parameter decoder/layers_3/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:32.067768 140260698863616 adafactor.py:358] Since rank of parameter decoder/layers_20/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:32.090868 139856202532864 adafactor.py:358] Since rank of parameter decoder/layers_22/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:32.072952 140180528838656 adafactor.py:358] Since rank of parameter decoder/layers_22/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:32.091413 139876724979712 adafactor.py:358] Since rank of parameter decoder/layers_20/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:32.090174 140143678494720 adafactor.py:358] Since rank of parameter decoder/layers_22/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
W0512 23:45:32.096109 139666521167872 adafactor.py:358] Since rank of parameter decoder/layers_23/mlp/router/router_weights/w/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>, <FactorDim.NONE: None>) is ignored.
W0512 23:45:32.080360 140643590531072 adafactor.py:358] Since rank of parameter decoder/layers_22/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:32.098263 140193652815872 adafactor.py:358] Since rank of parameter decoder/layers_3/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:32.083096 140260698863616 adafactor.py:358] Since rank of parameter decoder/layers_20/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:32.100737 140143678494720 adafactor.py:358] Since rank of parameter decoder/layers_22/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:32.105172 139856202532864 adafactor.py:358] Since rank of parameter decoder/layers_22/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:32.106588 139876724979712 adafactor.py:358] Since rank of parameter decoder/layers_20/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:32.106434 139666521167872 adafactor.py:358] Since rank of parameter decoder/layers_23/pre_extra_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:32.089696 140180528838656 adafactor.py:358] Since rank of parameter decoder/layers_22/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:32.090730 140643590531072 adafactor.py:358] Since rank of parameter decoder/layers_22/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:32.112010 140193652815872 adafactor.py:358] Since rank of parameter decoder/layers_3/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:32.111538 140143678494720 adafactor.py:358] Since rank of parameter decoder/layers_22/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:32.117287 139876724979712 adafactor.py:358] Since rank of parameter decoder/layers_20/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:32.117041 139666521167872 adafactor.py:358] Since rank of parameter decoder/layers_23/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:32.098737 140260698863616 adafactor.py:358] Since rank of parameter decoder/layers_20/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:32.100883 140643590531072 adafactor.py:358] Since rank of parameter decoder/layers_22/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:32.119658 139856202532864 adafactor.py:358] Since rank of parameter decoder/layers_22/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:32.104024 140180528838656 adafactor.py:358] Since rank of parameter decoder/layers_22/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:32.122131 140193652815872 adafactor.py:358] Since rank of parameter decoder/layers_3/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:32.127594 139876724979712 adafactor.py:358] Since rank of parameter decoder/layers_20/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:32.127503 139666521167872 adafactor.py:358] Since rank of parameter decoder/layers_23/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:32.126237 140143678494720 adafactor.py:358] Since rank of parameter decoder/layers_22/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:32.109828 140260698863616 adafactor.py:358] Since rank of parameter decoder/layers_20/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:32.130106 139856202532864 adafactor.py:358] Since rank of parameter decoder/layers_22/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:32.132347 140193652815872 adafactor.py:358] Since rank of parameter decoder/layers_3/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:32.115765 140643590531072 adafactor.py:358] Since rank of parameter decoder/layers_22/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:32.117759 140180528838656 adafactor.py:358] Since rank of parameter decoder/layers_23/extra_mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:32.137703 139666521167872 adafactor.py:358] Since rank of parameter decoder/layers_23/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:32.121063 140260698863616 adafactor.py:358] Since rank of parameter decoder/layers_20/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:32.140722 139856202532864 adafactor.py:358] Since rank of parameter decoder/layers_22/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:32.141630 139876724979712 adafactor.py:358] Since rank of parameter decoder/layers_20/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:32.141486 140143678494720 adafactor.py:358] Since rank of parameter decoder/layers_22/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:32.146657 140193652815872 adafactor.py:358] Since rank of parameter decoder/layers_3/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:32.131627 140180528838656 adafactor.py:358] Since rank of parameter decoder/layers_23/extra_mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:32.133616 140643590531072 adafactor.py:358] Since rank of parameter decoder/layers_22/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:32.151867 139666521167872 adafactor.py:358] Since rank of parameter decoder/layers_23/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:32.154946 139856202532864 adafactor.py:358] Since rank of parameter decoder/layers_22/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:32.155767 139876724979712 adafactor.py:358] Since rank of parameter decoder/layers_20/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:32.137077 140260698863616 adafactor.py:358] Since rank of parameter decoder/layers_20/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:32.156151 140143678494720 adafactor.py:358] Since rank of parameter decoder/layers_22/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:32.160443 140193652815872 adafactor.py:358] Since rank of parameter decoder/layers_3/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:32.145399 140180528838656 adafactor.py:358] Since rank of parameter decoder/layers_23/extra_mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:32.147283 140643590531072 adafactor.py:358] Since rank of parameter decoder/layers_22/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:32.165790 139666521167872 adafactor.py:358] Since rank of parameter decoder/layers_23/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:32.169172 139856202532864 adafactor.py:358] Since rank of parameter decoder/layers_22/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:32.169793 139876724979712 adafactor.py:358] Since rank of parameter decoder/layers_20/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:32.152480 140260698863616 adafactor.py:358] Since rank of parameter decoder/layers_20/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
W0512 23:45:32.170523 140143678494720 adafactor.py:358] Since rank of parameter decoder/layers_23/extra_mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:32.174237 140193652815872 adafactor.py:358] Since rank of parameter decoder/layers_3/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:32.160935 140643590531072 adafactor.py:358] Since rank of parameter decoder/layers_23/extra_mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:32.179674 139666521167872 adafactor.py:358] Since rank of parameter decoder/layers_23/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:32.183920 139856202532864 adafactor.py:358] Since rank of parameter decoder/layers_22/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:32.183982 139876724979712 adafactor.py:358] Since rank of parameter decoder/layers_21/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:32.167587 140260698863616 adafactor.py:358] Since rank of parameter decoder/layers_20/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:32.184302 140143678494720 adafactor.py:358] Since rank of parameter decoder/layers_23/extra_mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:32.188003 140193652815872 adafactor.py:358] Since rank of parameter decoder/layers_4/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:32.174748 140643590531072 adafactor.py:358] Since rank of parameter decoder/layers_23/extra_mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:32.193498 139666521167872 adafactor.py:358] Since rank of parameter decoder/layers_3/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:32.198022 139856202532864 adafactor.py:358] Since rank of parameter decoder/layers_23/extra_mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:32.198059 139876724979712 adafactor.py:358] Since rank of parameter decoder/layers_21/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:32.198138 140143678494720 adafactor.py:358] Since rank of parameter decoder/layers_23/extra_mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:32.182934 140260698863616 adafactor.py:358] Since rank of parameter decoder/layers_21/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:32.201836 140193652815872 adafactor.py:358] Since rank of parameter decoder/layers_4/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:32.188597 140643590531072 adafactor.py:358] Since rank of parameter decoder/layers_23/extra_mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:32.207370 139666521167872 adafactor.py:358] Since rank of parameter decoder/layers_3/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:32.212293 139856202532864 adafactor.py:358] Since rank of parameter decoder/layers_23/extra_mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:32.212511 139876724979712 adafactor.py:358] Since rank of parameter decoder/layers_21/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
W0512 23:45:32.215976 140193652815872 adafactor.py:358] Since rank of parameter decoder/layers_4/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:32.198177 140260698863616 adafactor.py:358] Since rank of parameter decoder/layers_21/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:32.221223 139666521167872 adafactor.py:358] Since rank of parameter decoder/layers_3/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:32.226409 139856202532864 adafactor.py:358] Since rank of parameter decoder/layers_23/extra_mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:32.208121 140180528838656 adafactor.py:358] Since rank of parameter decoder/layers_23/mlp/router/router_weights/w/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>, <FactorDim.NONE: None>) is ignored.
W0512 23:45:32.226922 139876724979712 adafactor.py:358] Since rank of parameter decoder/layers_21/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:32.231683 140193652815872 adafactor.py:358] Since rank of parameter decoder/layers_4/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:32.213663 140260698863616 adafactor.py:358] Since rank of parameter decoder/layers_21/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:32.235528 139666521167872 adafactor.py:358] Since rank of parameter decoder/layers_3/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:32.237902 139876724979712 adafactor.py:358] Since rank of parameter decoder/layers_21/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:32.219966 140180528838656 adafactor.py:358] Since rank of parameter decoder/layers_23/pre_extra_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:32.244045 140193652815872 adafactor.py:358] Since rank of parameter decoder/layers_4/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:32.245702 139666521167872 adafactor.py:358] Since rank of parameter decoder/layers_3/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:32.248435 139876724979712 adafactor.py:358] Since rank of parameter decoder/layers_21/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:32.229122 140260698863616 adafactor.py:358] Since rank of parameter decoder/layers_21/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:32.231534 140180528838656 adafactor.py:358] Since rank of parameter decoder/layers_23/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:32.255928 139666521167872 adafactor.py:358] Since rank of parameter decoder/layers_3/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:32.253267 140143678494720 adafactor.py:358] Since rank of parameter decoder/layers_23/mlp/router/router_weights/w/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>, <FactorDim.NONE: None>) is ignored.
W0512 23:45:32.256331 140193652815872 adafactor.py:358] Since rank of parameter decoder/layers_4/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:32.240546 140260698863616 adafactor.py:358] Since rank of parameter decoder/layers_21/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:32.242972 140180528838656 adafactor.py:358] Since rank of parameter decoder/layers_23/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:32.262828 139876724979712 adafactor.py:358] Since rank of parameter decoder/layers_21/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:32.245054 140643590531072 adafactor.py:358] Since rank of parameter decoder/layers_23/mlp/router/router_weights/w/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>, <FactorDim.NONE: None>) is ignored.
W0512 23:45:32.263574 140143678494720 adafactor.py:358] Since rank of parameter decoder/layers_23/pre_extra_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:32.269819 139666521167872 adafactor.py:358] Since rank of parameter decoder/layers_3/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:32.251880 140260698863616 adafactor.py:358] Since rank of parameter decoder/layers_21/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:32.254113 140180528838656 adafactor.py:358] Since rank of parameter decoder/layers_23/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:32.271689 140193652815872 adafactor.py:358] Since rank of parameter decoder/layers_4/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:32.277019 139876724979712 adafactor.py:358] Since rank of parameter decoder/layers_21/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:32.274074 140143678494720 adafactor.py:358] Since rank of parameter decoder/layers_23/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:32.259267 140643590531072 adafactor.py:358] Since rank of parameter decoder/layers_23/pre_extra_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:32.283410 139856202532864 adafactor.py:358] Since rank of parameter decoder/layers_23/mlp/router/router_weights/w/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>, <FactorDim.NONE: None>) is ignored.
W0512 23:45:32.283982 139666521167872 adafactor.py:358] Since rank of parameter decoder/layers_3/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:32.267691 140260698863616 adafactor.py:358] Since rank of parameter decoder/layers_21/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:32.284170 140143678494720 adafactor.py:358] Since rank of parameter decoder/layers_23/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:32.269596 140643590531072 adafactor.py:358] Since rank of parameter decoder/layers_23/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:32.269658 140180528838656 adafactor.py:358] Since rank of parameter decoder/layers_23/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
W0512 23:45:32.291216 139876724979712 adafactor.py:358] Since rank of parameter decoder/layers_21/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:32.290740 140193652815872 adafactor.py:358] Since rank of parameter decoder/layers_4/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:32.293977 139856202532864 adafactor.py:358] Since rank of parameter decoder/layers_23/pre_extra_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:32.279680 140643590531072 adafactor.py:358] Since rank of parameter decoder/layers_23/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:32.298027 139666521167872 adafactor.py:358] Since rank of parameter decoder/layers_3/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:32.295673 140143678494720 adafactor.py:358] Since rank of parameter decoder/layers_23/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:32.284106 140260698863616 adafactor.py:358] Since rank of parameter decoder/layers_21/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:32.285273 140180528838656 adafactor.py:358] Since rank of parameter decoder/layers_23/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:32.305139 139856202532864 adafactor.py:358] Since rank of parameter decoder/layers_23/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:32.305266 139876724979712 adafactor.py:358] Since rank of parameter decoder/layers_22/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:32.306360 140193652815872 adafactor.py:358] Since rank of parameter decoder/layers_4/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:32.289738 140643590531072 adafactor.py:358] Since rank of parameter decoder/layers_23/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:32.311967 139666521167872 adafactor.py:358] Since rank of parameter decoder/layers_4/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:32.311675 140143678494720 adafactor.py:358] Since rank of parameter decoder/layers_23/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:32.317119 139856202532864 adafactor.py:358] Since rank of parameter decoder/layers_23/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:32.299767 140180528838656 adafactor.py:358] Since rank of parameter decoder/layers_23/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:32.299699 140260698863616 adafactor.py:358] Since rank of parameter decoder/layers_21/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:32.319318 139876724979712 adafactor.py:358] Since rank of parameter decoder/layers_22/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:32.303537 140643590531072 adafactor.py:358] Since rank of parameter decoder/layers_23/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:32.325915 139666521167872 adafactor.py:358] Since rank of parameter decoder/layers_4/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:32.325922 140193652815872 adafactor.py:358] Since rank of parameter decoder/layers_5/extra_mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:32.328385 139856202532864 adafactor.py:358] Since rank of parameter decoder/layers_23/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:32.326570 140143678494720 adafactor.py:358] Since rank of parameter decoder/layers_23/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:32.314534 140180528838656 adafactor.py:358] Since rank of parameter decoder/layers_3/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:32.333391 139876724979712 adafactor.py:358] Since rank of parameter decoder/layers_22/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:32.317352 140643590531072 adafactor.py:358] Since rank of parameter decoder/layers_23/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:32.315076 140260698863616 adafactor.py:358] Since rank of parameter decoder/layers_22/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:32.339910 139666521167872 adafactor.py:358] Since rank of parameter decoder/layers_4/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:32.342709 139856202532864 adafactor.py:358] Since rank of parameter decoder/layers_23/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:32.342699 140193652815872 adafactor.py:358] Since rank of parameter decoder/layers_5/extra_mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:32.340954 140143678494720 adafactor.py:358] Since rank of parameter decoder/layers_23/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:32.329295 140180528838656 adafactor.py:358] Since rank of parameter decoder/layers_3/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:32.347588 139876724979712 adafactor.py:358] Since rank of parameter decoder/layers_22/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:32.330940 140643590531072 adafactor.py:358] Since rank of parameter decoder/layers_23/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:32.331064 140260698863616 adafactor.py:358] Since rank of parameter decoder/layers_22/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:32.353888 139666521167872 adafactor.py:358] Since rank of parameter decoder/layers_4/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:32.356874 139856202532864 adafactor.py:358] Since rank of parameter decoder/layers_23/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:32.358207 139876724979712 adafactor.py:358] Since rank of parameter decoder/layers_22/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:32.356038 140143678494720 adafactor.py:358] Since rank of parameter decoder/layers_3/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:32.359522 140193652815872 adafactor.py:358] Since rank of parameter decoder/layers_5/extra_mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:32.344611 140643590531072 adafactor.py:358] Since rank of parameter decoder/layers_3/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:32.345726 140260698863616 adafactor.py:358] Since rank of parameter decoder/layers_22/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:32.364663 139666521167872 adafactor.py:358] Since rank of parameter decoder/layers_4/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:32.347164 140180528838656 adafactor.py:358] Since rank of parameter decoder/layers_3/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:32.368576 139876724979712 adafactor.py:358] Since rank of parameter decoder/layers_22/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:32.371094 139856202532864 adafactor.py:358] Since rank of parameter decoder/layers_23/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:32.371343 140143678494720 adafactor.py:358] Since rank of parameter decoder/layers_3/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:32.374881 139666521167872 adafactor.py:358] Since rank of parameter decoder/layers_4/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:32.358330 140643590531072 adafactor.py:358] Since rank of parameter decoder/layers_3/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:32.360184 140260698863616 adafactor.py:358] Since rank of parameter decoder/layers_22/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:32.361663 140180528838656 adafactor.py:358] Since rank of parameter decoder/layers_3/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:32.383604 139876724979712 adafactor.py:358] Since rank of parameter decoder/layers_22/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:32.385377 139856202532864 adafactor.py:358] Since rank of parameter decoder/layers_3/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:32.388745 139666521167872 adafactor.py:358] Since rank of parameter decoder/layers_4/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:32.386062 140143678494720 adafactor.py:358] Since rank of parameter decoder/layers_3/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:32.370679 140260698863616 adafactor.py:358] Since rank of parameter decoder/layers_22/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:32.372075 140643590531072 adafactor.py:358] Since rank of parameter decoder/layers_3/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:32.372277 140180528838656 adafactor.py:358] Since rank of parameter decoder/layers_3/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:32.397719 139876724979712 adafactor.py:358] Since rank of parameter decoder/layers_22/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:32.399635 139856202532864 adafactor.py:358] Since rank of parameter decoder/layers_3/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:32.381172 140260698863616 adafactor.py:358] Since rank of parameter decoder/layers_22/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:32.402610 139666521167872 adafactor.py:358] Since rank of parameter decoder/layers_4/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:32.385346 140180528838656 adafactor.py:358] Since rank of parameter decoder/layers_3/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:32.400911 140143678494720 adafactor.py:358] Since rank of parameter decoder/layers_3/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:32.389608 140643590531072 adafactor.py:358] Since rank of parameter decoder/layers_3/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:32.411828 139876724979712 adafactor.py:358] Since rank of parameter decoder/layers_22/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:32.411388 140143678494720 adafactor.py:358] Since rank of parameter decoder/layers_3/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:32.414306 139856202532864 adafactor.py:358] Since rank of parameter decoder/layers_3/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:32.395687 140260698863616 adafactor.py:358] Since rank of parameter decoder/layers_22/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:32.416568 139666521167872 adafactor.py:358] Since rank of parameter decoder/layers_4/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:32.399898 140643590531072 adafactor.py:358] Since rank of parameter decoder/layers_3/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:32.401225 140180528838656 adafactor.py:358] Since rank of parameter decoder/layers_3/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:32.423365 140193652815872 adafactor.py:358] Since rank of parameter decoder/layers_5/mlp/router/router_weights/w/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>, <FactorDim.NONE: None>) is ignored.
W0512 23:45:32.422898 140143678494720 adafactor.py:358] Since rank of parameter decoder/layers_3/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:32.425962 139876724979712 adafactor.py:358] Since rank of parameter decoder/layers_23/extra_mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:32.409877 140643590531072 adafactor.py:358] Since rank of parameter decoder/layers_3/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:32.428505 139856202532864 adafactor.py:358] Since rank of parameter decoder/layers_3/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:32.409946 140260698863616 adafactor.py:358] Since rank of parameter decoder/layers_22/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:32.431069 139666521167872 adafactor.py:358] Since rank of parameter decoder/layers_5/extra_mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:32.415705 140180528838656 adafactor.py:358] Since rank of parameter decoder/layers_3/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:32.435178 140193652815872 adafactor.py:358] Since rank of parameter decoder/layers_5/pre_extra_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:32.439230 139856202532864 adafactor.py:358] Since rank of parameter decoder/layers_3/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:32.440279 139876724979712 adafactor.py:358] Since rank of parameter decoder/layers_23/extra_mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:32.423659 140643590531072 adafactor.py:358] Since rank of parameter decoder/layers_3/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:32.439579 140143678494720 adafactor.py:358] Since rank of parameter decoder/layers_3/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:32.424781 140260698863616 adafactor.py:358] Since rank of parameter decoder/layers_22/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:32.445302 139666521167872 adafactor.py:358] Since rank of parameter decoder/layers_5/extra_mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:32.447124 140193652815872 adafactor.py:358] Since rank of parameter decoder/layers_5/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:32.430821 140180528838656 adafactor.py:358] Since rank of parameter decoder/layers_3/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:32.449878 139856202532864 adafactor.py:358] Since rank of parameter decoder/layers_3/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:32.454668 139876724979712 adafactor.py:358] Since rank of parameter decoder/layers_23/extra_mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:32.437443 140643590531072 adafactor.py:358] Since rank of parameter decoder/layers_3/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:32.454567 140143678494720 adafactor.py:358] Since rank of parameter decoder/layers_3/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:32.438935 140260698863616 adafactor.py:358] Since rank of parameter decoder/layers_23/extra_mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:32.457530 140193652815872 adafactor.py:358] Since rank of parameter decoder/layers_5/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:32.459237 139666521167872 adafactor.py:358] Since rank of parameter decoder/layers_5/extra_mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:32.445395 140180528838656 adafactor.py:358] Since rank of parameter decoder/layers_4/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:32.465289 139856202532864 adafactor.py:358] Since rank of parameter decoder/layers_3/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:32.450983 140643590531072 adafactor.py:358] Since rank of parameter decoder/layers_3/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:32.468437 140193652815872 adafactor.py:358] Since rank of parameter decoder/layers_5/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:32.453214 140260698863616 adafactor.py:358] Since rank of parameter decoder/layers_23/extra_mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:32.469793 140143678494720 adafactor.py:358] Since rank of parameter decoder/layers_3/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:32.460347 140180528838656 adafactor.py:358] Since rank of parameter decoder/layers_4/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:32.479529 139856202532864 adafactor.py:358] Since rank of parameter decoder/layers_3/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:32.464587 140643590531072 adafactor.py:358] Since rank of parameter decoder/layers_4/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:32.484963 140193652815872 adafactor.py:358] Since rank of parameter decoder/layers_5/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:32.467456 140260698863616 adafactor.py:358] Since rank of parameter decoder/layers_23/extra_mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:32.484627 140143678494720 adafactor.py:358] Since rank of parameter decoder/layers_4/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:32.475118 140180528838656 adafactor.py:358] Since rank of parameter decoder/layers_4/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:32.493681 139856202532864 adafactor.py:358] Since rank of parameter decoder/layers_3/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:32.478412 140643590531072 adafactor.py:358] Since rank of parameter decoder/layers_4/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:32.499154 140193652815872 adafactor.py:358] Since rank of parameter decoder/layers_5/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:32.499531 140143678494720 adafactor.py:358] Since rank of parameter decoder/layers_4/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:32.508104 139856202532864 adafactor.py:358] Since rank of parameter decoder/layers_4/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:32.489985 140180528838656 adafactor.py:358] Since rank of parameter decoder/layers_4/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:32.492087 140643590531072 adafactor.py:358] Since rank of parameter decoder/layers_4/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:32.512297 139876724979712 adafactor.py:358] Since rank of parameter decoder/layers_23/mlp/router/router_weights/w/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>, <FactorDim.NONE: None>) is ignored.
W0512 23:45:32.514495 140193652815872 adafactor.py:358] Since rank of parameter decoder/layers_5/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:32.516062 139666521167872 adafactor.py:358] Since rank of parameter decoder/layers_5/mlp/router/router_weights/w/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>, <FactorDim.NONE: None>) is ignored.
W0512 23:45:32.514395 140143678494720 adafactor.py:358] Since rank of parameter decoder/layers_4/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:32.500851 140180528838656 adafactor.py:358] Since rank of parameter decoder/layers_4/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:32.522718 139856202532864 adafactor.py:358] Since rank of parameter decoder/layers_4/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:32.523298 139876724979712 adafactor.py:358] Since rank of parameter decoder/layers_23/pre_extra_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:32.505748 140643590531072 adafactor.py:358] Since rank of parameter decoder/layers_4/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:32.526581 139666521167872 adafactor.py:358] Since rank of parameter decoder/layers_5/pre_extra_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:32.511967 140180528838656 adafactor.py:358] Since rank of parameter decoder/layers_4/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:32.530028 140193652815872 adafactor.py:358] Since rank of parameter decoder/layers_6/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:32.529195 140143678494720 adafactor.py:358] Since rank of parameter decoder/layers_4/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:32.533737 139876724979712 adafactor.py:358] Since rank of parameter decoder/layers_23/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:32.516386 140643590531072 adafactor.py:358] Since rank of parameter decoder/layers_4/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:32.537181 139856202532864 adafactor.py:358] Since rank of parameter decoder/layers_4/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:32.537019 139666521167872 adafactor.py:358] Since rank of parameter decoder/layers_5/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:32.540249 140143678494720 adafactor.py:358] Since rank of parameter decoder/layers_4/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:32.524683 140260698863616 adafactor.py:358] Since rank of parameter decoder/layers_23/mlp/router/router_weights/w/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>, <FactorDim.NONE: None>) is ignored.
W0512 23:45:32.544054 139876724979712 adafactor.py:358] Since rank of parameter decoder/layers_23/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:32.527040 140180528838656 adafactor.py:358] Since rank of parameter decoder/layers_4/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:32.545333 140193652815872 adafactor.py:358] Since rank of parameter decoder/layers_6/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:32.529325 140643590531072 adafactor.py:358] Since rank of parameter decoder/layers_4/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:32.547277 139666521167872 adafactor.py:358] Since rank of parameter decoder/layers_5/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:32.551723 139856202532864 adafactor.py:358] Since rank of parameter decoder/layers_4/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:32.551303 140143678494720 adafactor.py:358] Since rank of parameter decoder/layers_4/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:32.554359 139876724979712 adafactor.py:358] Since rank of parameter decoder/layers_23/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:32.535310 140260698863616 adafactor.py:358] Since rank of parameter decoder/layers_23/pre_extra_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:32.557482 139666521167872 adafactor.py:358] Since rank of parameter decoder/layers_5/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:32.559267 140193652815872 adafactor.py:358] Since rank of parameter decoder/layers_6/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:32.542569 140180528838656 adafactor.py:358] Since rank of parameter decoder/layers_4/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:32.543173 140643590531072 adafactor.py:358] Since rank of parameter decoder/layers_4/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:32.562382 139856202532864 adafactor.py:358] Since rank of parameter decoder/layers_4/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:32.545665 140260698863616 adafactor.py:358] Since rank of parameter decoder/layers_23/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:32.568713 139876724979712 adafactor.py:358] Since rank of parameter decoder/layers_23/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:32.567248 140143678494720 adafactor.py:358] Since rank of parameter decoder/layers_4/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:32.571490 139666521167872 adafactor.py:358] Since rank of parameter decoder/layers_5/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:32.573016 139856202532864 adafactor.py:358] Since rank of parameter decoder/layers_4/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:32.573462 140193652815872 adafactor.py:358] Since rank of parameter decoder/layers_6/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:32.556939 140643590531072 adafactor.py:358] Since rank of parameter decoder/layers_4/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:32.557599 140180528838656 adafactor.py:358] Since rank of parameter decoder/layers_4/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:32.556832 140260698863616 adafactor.py:358] Since rank of parameter decoder/layers_23/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:32.583169 139876724979712 adafactor.py:358] Since rank of parameter decoder/layers_23/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:32.582205 140143678494720 adafactor.py:358] Since rank of parameter decoder/layers_4/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:32.584157 140193652815872 adafactor.py:358] Since rank of parameter decoder/layers_6/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:32.585593 139666521167872 adafactor.py:358] Since rank of parameter decoder/layers_5/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:32.587333 139856202532864 adafactor.py:358] Since rank of parameter decoder/layers_4/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:32.568184 140260698863616 adafactor.py:358] Since rank of parameter decoder/layers_23/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:32.570531 140643590531072 adafactor.py:358] Since rank of parameter decoder/layers_4/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:32.572434 140180528838656 adafactor.py:358] Since rank of parameter decoder/layers_5/extra_mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:32.594344 140193652815872 adafactor.py:358] Since rank of parameter decoder/layers_6/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:32.597451 139876724979712 adafactor.py:358] Since rank of parameter decoder/layers_23/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:32.599595 139666521167872 adafactor.py:358] Since rank of parameter decoder/layers_5/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:32.597122 140143678494720 adafactor.py:358] Since rank of parameter decoder/layers_4/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:32.602426 139856202532864 adafactor.py:358] Since rank of parameter decoder/layers_4/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:32.584176 140643590531072 adafactor.py:358] Since rank of parameter decoder/layers_5/extra_mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:32.583470 140260698863616 adafactor.py:358] Since rank of parameter decoder/layers_23/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:32.587396 140180528838656 adafactor.py:358] Since rank of parameter decoder/layers_5/extra_mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:32.608671 140193652815872 adafactor.py:358] Since rank of parameter decoder/layers_6/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:32.611618 139876724979712 adafactor.py:358] Since rank of parameter decoder/layers_3/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:32.611174 140143678494720 adafactor.py:358] Since rank of parameter decoder/layers_5/extra_mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:32.614382 139666521167872 adafactor.py:358] Since rank of parameter decoder/layers_6/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:32.598024 140643590531072 adafactor.py:358] Since rank of parameter decoder/layers_5/extra_mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:32.597901 140260698863616 adafactor.py:358] Since rank of parameter decoder/layers_23/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:32.618345 139856202532864 adafactor.py:358] Since rank of parameter decoder/layers_4/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:32.602679 140180528838656 adafactor.py:358] Since rank of parameter decoder/layers_5/extra_mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:32.623015 140193652815872 adafactor.py:358] Since rank of parameter decoder/layers_6/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:32.625750 139876724979712 adafactor.py:358] Since rank of parameter decoder/layers_3/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:32.625211 140143678494720 adafactor.py:358] Since rank of parameter decoder/layers_5/extra_mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:32.629356 139666521167872 adafactor.py:358] Since rank of parameter decoder/layers_6/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:32.611923 140643590531072 adafactor.py:358] Since rank of parameter decoder/layers_5/extra_mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:32.612234 140260698863616 adafactor.py:358] Since rank of parameter decoder/layers_23/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:32.632608 139856202532864 adafactor.py:358] Since rank of parameter decoder/layers_5/extra_mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:32.637182 140193652815872 adafactor.py:358] Since rank of parameter decoder/layers_6/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:32.639859 139876724979712 adafactor.py:358] Since rank of parameter decoder/layers_3/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:32.639161 140143678494720 adafactor.py:358] Since rank of parameter decoder/layers_5/extra_mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:32.645131 139666521167872 adafactor.py:358] Since rank of parameter decoder/layers_6/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:32.626802 140260698863616 adafactor.py:358] Since rank of parameter decoder/layers_3/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:32.646981 139856202532864 adafactor.py:358] Since rank of parameter decoder/layers_5/extra_mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:32.651205 140193652815872 adafactor.py:358] Since rank of parameter decoder/layers_7/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:32.654490 139876724979712 adafactor.py:358] Since rank of parameter decoder/layers_3/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:32.660123 139666521167872 adafactor.py:358] Since rank of parameter decoder/layers_6/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:32.641776 140260698863616 adafactor.py:358] Since rank of parameter decoder/layers_3/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:32.661499 139856202532864 adafactor.py:358] Since rank of parameter decoder/layers_5/extra_mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:32.664882 139876724979712 adafactor.py:358] Since rank of parameter decoder/layers_3/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:32.665220 140193652815872 adafactor.py:358] Since rank of parameter decoder/layers_7/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:32.670508 139666521167872 adafactor.py:358] Since rank of parameter decoder/layers_6/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:32.656256 140260698863616 adafactor.py:358] Since rank of parameter decoder/layers_3/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:32.675161 139876724979712 adafactor.py:358] Since rank of parameter decoder/layers_3/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:32.679360 140193652815872 adafactor.py:358] Since rank of parameter decoder/layers_7/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:32.680745 139666521167872 adafactor.py:358] Since rank of parameter decoder/layers_6/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:32.663342 140180528838656 adafactor.py:358] Since rank of parameter decoder/layers_5/mlp/router/router_weights/w/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>, <FactorDim.NONE: None>) is ignored.
W0512 23:45:32.667847 140643590531072 adafactor.py:358] Since rank of parameter decoder/layers_5/mlp/router/router_weights/w/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>, <FactorDim.NONE: None>) is ignored.
W0512 23:45:32.689469 139876724979712 adafactor.py:358] Since rank of parameter decoder/layers_3/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:32.670629 140260698863616 adafactor.py:358] Since rank of parameter decoder/layers_3/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:32.693637 140193652815872 adafactor.py:358] Since rank of parameter decoder/layers_7/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:32.694700 139666521167872 adafactor.py:358] Since rank of parameter decoder/layers_6/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:32.676580 140180528838656 adafactor.py:358] Since rank of parameter decoder/layers_5/pre_extra_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:32.678227 140643590531072 adafactor.py:358] Since rank of parameter decoder/layers_5/pre_extra_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:32.695081 140143678494720 adafactor.py:358] Since rank of parameter decoder/layers_5/mlp/router/router_weights/w/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>, <FactorDim.NONE: None>) is ignored.
W0512 23:45:32.681485 140260698863616 adafactor.py:358] Since rank of parameter decoder/layers_3/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:32.703441 139876724979712 adafactor.py:358] Since rank of parameter decoder/layers_3/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:32.704213 140193652815872 adafactor.py:358] Since rank of parameter decoder/layers_7/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:32.688367 140643590531072 adafactor.py:358] Since rank of parameter decoder/layers_5/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:32.689684 140180528838656 adafactor.py:358] Since rank of parameter decoder/layers_5/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:32.708593 139666521167872 adafactor.py:358] Since rank of parameter decoder/layers_6/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:32.706231 140143678494720 adafactor.py:358] Since rank of parameter decoder/layers_5/pre_extra_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:32.692270 140260698863616 adafactor.py:358] Since rank of parameter decoder/layers_3/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:32.714322 140193652815872 adafactor.py:358] Since rank of parameter decoder/layers_7/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:32.698421 140643590531072 adafactor.py:358] Since rank of parameter decoder/layers_5/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:32.717489 139876724979712 adafactor.py:358] Since rank of parameter decoder/layers_3/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:32.700251 140180528838656 adafactor.py:358] Since rank of parameter decoder/layers_5/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:32.718806 139856202532864 adafactor.py:358] Since rank of parameter decoder/layers_5/mlp/router/router_weights/w/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>, <FactorDim.NONE: None>) is ignored.
W0512 23:45:32.716827 140143678494720 adafactor.py:358] Since rank of parameter decoder/layers_5/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:32.722444 139666521167872 adafactor.py:358] Since rank of parameter decoder/layers_6/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:32.708447 140643590531072 adafactor.py:358] Since rank of parameter decoder/layers_5/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:32.707685 140260698863616 adafactor.py:358] Since rank of parameter decoder/layers_3/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:32.710863 140180528838656 adafactor.py:358] Since rank of parameter decoder/layers_5/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:32.729770 139856202532864 adafactor.py:358] Since rank of parameter decoder/layers_5/pre_extra_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:32.729004 140193652815872 adafactor.py:358] Since rank of parameter decoder/layers_7/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:32.727220 140143678494720 adafactor.py:358] Since rank of parameter decoder/layers_5/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:32.731457 139876724979712 adafactor.py:358] Since rank of parameter decoder/layers_4/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:32.736277 139666521167872 adafactor.py:358] Since rank of parameter decoder/layers_7/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:32.722213 140643590531072 adafactor.py:358] Since rank of parameter decoder/layers_5/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:32.738095 140143678494720 adafactor.py:358] Since rank of parameter decoder/layers_5/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:32.741199 139856202532864 adafactor.py:358] Since rank of parameter decoder/layers_5/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:32.721977 140260698863616 adafactor.py:358] Since rank of parameter decoder/layers_3/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:32.742937 140193652815872 adafactor.py:358] Since rank of parameter decoder/layers_7/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:32.726097 140180528838656 adafactor.py:358] Since rank of parameter decoder/layers_5/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:32.745524 139876724979712 adafactor.py:358] Since rank of parameter decoder/layers_4/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:32.750070 139666521167872 adafactor.py:358] Since rank of parameter decoder/layers_7/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:32.751666 139856202532864 adafactor.py:358] Since rank of parameter decoder/layers_5/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:32.735996 140643590531072 adafactor.py:358] Since rank of parameter decoder/layers_5/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:32.751800 140143678494720 adafactor.py:358] Since rank of parameter decoder/layers_5/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:32.736186 140260698863616 adafactor.py:358] Since rank of parameter decoder/layers_3/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:32.756748 140193652815872 adafactor.py:358] Since rank of parameter decoder/layers_7/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
W0512 23:45:32.741536 140180528838656 adafactor.py:358] Since rank of parameter decoder/layers_5/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:32.759703 139876724979712 adafactor.py:358] Since rank of parameter decoder/layers_4/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:32.762351 139856202532864 adafactor.py:358] Since rank of parameter decoder/layers_5/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:32.763955 139666521167872 adafactor.py:358] Since rank of parameter decoder/layers_7/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:32.765724 140143678494720 adafactor.py:358] Since rank of parameter decoder/layers_5/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:32.750370 140260698863616 adafactor.py:358] Since rank of parameter decoder/layers_4/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:32.751744 140643590531072 adafactor.py:358] Since rank of parameter decoder/layers_5/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:32.770629 140193652815872 adafactor.py:358] Since rank of parameter decoder/layers_8/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:32.773760 139876724979712 adafactor.py:358] Since rank of parameter decoder/layers_4/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:32.755878 140180528838656 adafactor.py:358] Since rank of parameter decoder/layers_5/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:32.776750 139856202532864 adafactor.py:358] Since rank of parameter decoder/layers_5/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:32.778683 139666521167872 adafactor.py:358] Since rank of parameter decoder/layers_7/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:32.779434 140143678494720 adafactor.py:358] Since rank of parameter decoder/layers_5/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:32.764844 140260698863616 adafactor.py:358] Since rank of parameter decoder/layers_4/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:32.784713 139876724979712 adafactor.py:358] Since rank of parameter decoder/layers_4/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:32.784551 140193652815872 adafactor.py:358] Since rank of parameter decoder/layers_8/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:32.767775 140643590531072 adafactor.py:358] Since rank of parameter decoder/layers_6/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:32.770484 140180528838656 adafactor.py:358] Since rank of parameter decoder/layers_6/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:32.789516 139666521167872 adafactor.py:358] Since rank of parameter decoder/layers_7/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:32.791077 139856202532864 adafactor.py:358] Since rank of parameter decoder/layers_5/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:32.794934 139876724979712 adafactor.py:358] Since rank of parameter decoder/layers_4/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:32.793610 140143678494720 adafactor.py:358] Since rank of parameter decoder/layers_6/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:32.779294 140260698863616 adafactor.py:358] Since rank of parameter decoder/layers_4/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:32.798388 140193652815872 adafactor.py:358] Since rank of parameter decoder/layers_8/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:32.800046 139666521167872 adafactor.py:358] Since rank of parameter decoder/layers_7/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:32.785099 140180528838656 adafactor.py:358] Since rank of parameter decoder/layers_6/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:32.785024 140643590531072 adafactor.py:358] Since rank of parameter decoder/layers_6/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:32.805170 139856202532864 adafactor.py:358] Since rank of parameter decoder/layers_5/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:32.808920 139876724979712 adafactor.py:358] Since rank of parameter decoder/layers_4/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:32.807854 140143678494720 adafactor.py:358] Since rank of parameter decoder/layers_6/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:32.793612 140260698863616 adafactor.py:358] Since rank of parameter decoder/layers_4/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:32.812388 140193652815872 adafactor.py:358] Since rank of parameter decoder/layers_8/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:32.814101 139666521167872 adafactor.py:358] Since rank of parameter decoder/layers_7/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:32.799470 140180528838656 adafactor.py:358] Since rank of parameter decoder/layers_6/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:32.819277 139856202532864 adafactor.py:358] Since rank of parameter decoder/layers_6/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:32.802798 140643590531072 adafactor.py:358] Since rank of parameter decoder/layers_6/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:32.822874 139876724979712 adafactor.py:358] Since rank of parameter decoder/layers_4/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:32.804342 140260698863616 adafactor.py:358] Since rank of parameter decoder/layers_4/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:32.823196 140193652815872 adafactor.py:358] Since rank of parameter decoder/layers_8/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:32.822146 140143678494720 adafactor.py:358] Since rank of parameter decoder/layers_6/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:32.828127 139666521167872 adafactor.py:358] Since rank of parameter decoder/layers_7/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:32.814422 140180528838656 adafactor.py:358] Since rank of parameter decoder/layers_6/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:32.833601 139856202532864 adafactor.py:358] Since rank of parameter decoder/layers_6/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:32.814866 140260698863616 adafactor.py:358] Since rank of parameter decoder/layers_4/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:32.833476 140193652815872 adafactor.py:358] Since rank of parameter decoder/layers_8/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:32.836948 139876724979712 adafactor.py:358] Since rank of parameter decoder/layers_4/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:32.837798 140143678494720 adafactor.py:358] Since rank of parameter decoder/layers_6/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:32.823437 140643590531072 adafactor.py:358] Since rank of parameter decoder/layers_6/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:32.842092 139666521167872 adafactor.py:358] Since rank of parameter decoder/layers_7/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:32.825158 140180528838656 adafactor.py:358] Since rank of parameter decoder/layers_6/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:32.847910 139856202532864 adafactor.py:358] Since rank of parameter decoder/layers_6/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:32.829012 140260698863616 adafactor.py:358] Since rank of parameter decoder/layers_4/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:32.847589 140193652815872 adafactor.py:358] Since rank of parameter decoder/layers_8/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:32.850915 139876724979712 adafactor.py:358] Since rank of parameter decoder/layers_5/extra_mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:32.835044 140643590531072 adafactor.py:358] Since rank of parameter decoder/layers_6/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:32.850386 140143678494720 adafactor.py:358] Since rank of parameter decoder/layers_6/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:32.835349 140180528838656 adafactor.py:358] Since rank of parameter decoder/layers_6/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:32.856047 139666521167872 adafactor.py:358] Since rank of parameter decoder/layers_8/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:32.862109 139856202532864 adafactor.py:358] Since rank of parameter decoder/layers_6/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:32.843754 140260698863616 adafactor.py:358] Since rank of parameter decoder/layers_4/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:32.862596 140193652815872 adafactor.py:358] Since rank of parameter decoder/layers_8/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:32.865045 139876724979712 adafactor.py:358] Since rank of parameter decoder/layers_5/extra_mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:32.862293 140143678494720 adafactor.py:358] Since rank of parameter decoder/layers_6/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:32.849645 140180528838656 adafactor.py:358] Since rank of parameter decoder/layers_6/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:32.870080 139666521167872 adafactor.py:358] Since rank of parameter decoder/layers_8/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:32.872910 139856202532864 adafactor.py:358] Since rank of parameter decoder/layers_6/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:32.856851 140643590531072 adafactor.py:358] Since rank of parameter decoder/layers_6/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:32.857994 140260698863616 adafactor.py:358] Since rank of parameter decoder/layers_4/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:32.877486 140193652815872 adafactor.py:358] Since rank of parameter decoder/layers_8/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:32.878945 139876724979712 adafactor.py:358] Since rank of parameter decoder/layers_5/extra_mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:32.876508 140143678494720 adafactor.py:358] Since rank of parameter decoder/layers_6/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:32.863446 140180528838656 adafactor.py:358] Since rank of parameter decoder/layers_6/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:32.883376 139856202532864 adafactor.py:358] Since rank of parameter decoder/layers_6/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:32.884566 139666521167872 adafactor.py:358] Since rank of parameter decoder/layers_8/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:32.872182 140260698863616 adafactor.py:358] Since rank of parameter decoder/layers_5/extra_mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:32.891528 140193652815872 adafactor.py:358] Since rank of parameter decoder/layers_9/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:32.875055 140643590531072 adafactor.py:358] Since rank of parameter decoder/layers_6/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:32.890373 140143678494720 adafactor.py:358] Since rank of parameter decoder/layers_6/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:32.877389 140180528838656 adafactor.py:358] Since rank of parameter decoder/layers_6/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:32.897564 139856202532864 adafactor.py:358] Since rank of parameter decoder/layers_6/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:32.898556 139666521167872 adafactor.py:358] Since rank of parameter decoder/layers_8/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:32.886403 140260698863616 adafactor.py:358] Since rank of parameter decoder/layers_5/extra_mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:32.905550 140193652815872 adafactor.py:358] Since rank of parameter decoder/layers_9/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:32.904230 140143678494720 adafactor.py:358] Since rank of parameter decoder/layers_6/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:32.891191 140180528838656 adafactor.py:358] Since rank of parameter decoder/layers_7/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:32.909317 139666521167872 adafactor.py:358] Since rank of parameter decoder/layers_8/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:32.911710 139856202532864 adafactor.py:358] Since rank of parameter decoder/layers_6/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:32.896825 140643590531072 adafactor.py:358] Since rank of parameter decoder/layers_6/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:32.919591 139666521167872 adafactor.py:358] Since rank of parameter decoder/layers_8/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:32.900593 140260698863616 adafactor.py:358] Since rank of parameter decoder/layers_5/extra_mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:32.919762 140193652815872 adafactor.py:358] Since rank of parameter decoder/layers_9/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:32.918019 140143678494720 adafactor.py:358] Since rank of parameter decoder/layers_7/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:32.905137 140180528838656 adafactor.py:358] Since rank of parameter decoder/layers_7/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:32.925923 139856202532864 adafactor.py:358] Since rank of parameter decoder/layers_6/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:32.911348 140643590531072 adafactor.py:358] Since rank of parameter decoder/layers_6/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:32.933453 139666521167872 adafactor.py:358] Since rank of parameter decoder/layers_8/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:32.931948 140143678494720 adafactor.py:358] Since rank of parameter decoder/layers_7/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:32.933917 140193652815872 adafactor.py:358] Since rank of parameter decoder/layers_9/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:32.935772 139876724979712 adafactor.py:358] Since rank of parameter decoder/layers_5/mlp/router/router_weights/w/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>, <FactorDim.NONE: None>) is ignored.
W0512 23:45:32.919357 140180528838656 adafactor.py:358] Since rank of parameter decoder/layers_7/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:32.939935 139856202532864 adafactor.py:358] Since rank of parameter decoder/layers_7/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:32.944426 140193652815872 adafactor.py:358] Since rank of parameter decoder/layers_9/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:32.946243 139876724979712 adafactor.py:358] Since rank of parameter decoder/layers_5/pre_extra_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:32.928790 140643590531072 adafactor.py:358] Since rank of parameter decoder/layers_7/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:32.947375 139666521167872 adafactor.py:358] Since rank of parameter decoder/layers_8/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:32.945912 140143678494720 adafactor.py:358] Since rank of parameter decoder/layers_7/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:32.933378 140180528838656 adafactor.py:358] Since rank of parameter decoder/layers_7/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:32.954036 139856202532864 adafactor.py:358] Since rank of parameter decoder/layers_7/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:32.954632 140193652815872 adafactor.py:358] Since rank of parameter decoder/layers_9/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:32.956980 139876724979712 adafactor.py:358] Since rank of parameter decoder/layers_5/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:32.961173 139666521167872 adafactor.py:358] Since rank of parameter decoder/layers_8/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:32.943624 140180528838656 adafactor.py:358] Since rank of parameter decoder/layers_7/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:32.944006 140643590531072 adafactor.py:358] Since rank of parameter decoder/layers_7/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:32.959806 140143678494720 adafactor.py:358] Since rank of parameter decoder/layers_7/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:32.968060 139856202532864 adafactor.py:358] Since rank of parameter decoder/layers_7/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:32.968104 139876724979712 adafactor.py:358] Since rank of parameter decoder/layers_5/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:32.969985 140193652815872 adafactor.py:358] Since rank of parameter decoder/layers_9/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:32.953863 140180528838656 adafactor.py:358] Since rank of parameter decoder/layers_7/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:32.969918 140143678494720 adafactor.py:358] Since rank of parameter decoder/layers_7/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:32.975065 139666521167872 adafactor.py:358] Since rank of parameter decoder/layers_9/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:32.958353 140643590531072 adafactor.py:358] Since rank of parameter decoder/layers_7/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:32.958395 140260698863616 adafactor.py:358] Since rank of parameter decoder/layers_5/mlp/router/router_weights/w/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>, <FactorDim.NONE: None>) is ignored.
W0512 23:45:32.979320 139876724979712 adafactor.py:358] Since rank of parameter decoder/layers_5/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:32.982207 139856202532864 adafactor.py:358] Since rank of parameter decoder/layers_7/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:32.980613 140143678494720 adafactor.py:358] Since rank of parameter decoder/layers_7/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:32.968290 140180528838656 adafactor.py:358] Since rank of parameter decoder/layers_7/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:32.985651 140193652815872 adafactor.py:358] Since rank of parameter decoder/layers_9/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:32.969072 140260698863616 adafactor.py:358] Since rank of parameter decoder/layers_5/pre_extra_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:32.988923 139666521167872 adafactor.py:358] Since rank of parameter decoder/layers_9/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:32.973440 140643590531072 adafactor.py:358] Since rank of parameter decoder/layers_7/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:32.992600 139856202532864 adafactor.py:358] Since rank of parameter decoder/layers_7/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:32.994866 139876724979712 adafactor.py:358] Since rank of parameter decoder/layers_5/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:32.994418 140143678494720 adafactor.py:358] Since rank of parameter decoder/layers_7/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:32.979952 140260698863616 adafactor.py:358] Since rank of parameter decoder/layers_5/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:32.982157 140180528838656 adafactor.py:358] Since rank of parameter decoder/layers_7/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:33.000256 140193652815872 adafactor.py:358] Since rank of parameter decoder/layers_9/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:32.984571 140643590531072 adafactor.py:358] Since rank of parameter decoder/layers_7/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:33.002952 139856202532864 adafactor.py:358] Since rank of parameter decoder/layers_7/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:33.002789 139666521167872 adafactor.py:358] Since rank of parameter decoder/layers_9/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:32.990359 140260698863616 adafactor.py:358] Since rank of parameter decoder/layers_5/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:33.010088 139876724979712 adafactor.py:358] Since rank of parameter decoder/layers_5/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:33.008099 140143678494720 adafactor.py:358] Since rank of parameter decoder/layers_7/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:32.995611 140643590531072 adafactor.py:358] Since rank of parameter decoder/layers_7/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:32.996001 140180528838656 adafactor.py:358] Since rank of parameter decoder/layers_7/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:33.014137 140193652815872 adafactor.py:358] Since rank of parameter decoder/logits_dense/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:33.016650 139666521167872 adafactor.py:358] Since rank of parameter decoder/layers_9/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:33.017575 139856202532864 adafactor.py:358] Since rank of parameter decoder/layers_7/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:33.000790 140260698863616 adafactor.py:358] Since rank of parameter decoder/layers_5/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:33.022114 140143678494720 adafactor.py:358] Since rank of parameter decoder/layers_7/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:33.025172 139876724979712 adafactor.py:358] Since rank of parameter decoder/layers_5/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:33.026813 139666521167872 adafactor.py:358] Since rank of parameter decoder/layers_9/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:33.009870 140643590531072 adafactor.py:358] Since rank of parameter decoder/layers_7/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:33.009922 140180528838656 adafactor.py:358] Since rank of parameter decoder/layers_8/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:33.031841 139856202532864 adafactor.py:358] Since rank of parameter decoder/layers_7/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:33.016122 140260698863616 adafactor.py:358] Since rank of parameter decoder/layers_5/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:33.037512 139666521167872 adafactor.py:358] Since rank of parameter decoder/layers_9/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:33.036228 140143678494720 adafactor.py:358] Since rank of parameter decoder/layers_8/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:33.038523 140193652815872 adafactor.py:358] Since rank of parameter token_embedder/embedding 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:33.040103 139876724979712 adafactor.py:358] Since rank of parameter decoder/layers_6/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:33.024748 140643590531072 adafactor.py:358] Since rank of parameter decoder/layers_7/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:33.025142 140180528838656 adafactor.py:358] Since rank of parameter decoder/layers_8/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:33.046463 139856202532864 adafactor.py:358] Since rank of parameter decoder/layers_7/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:33.030465 140260698863616 adafactor.py:358] Since rank of parameter decoder/layers_5/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:33.051295 139666521167872 adafactor.py:358] Since rank of parameter decoder/layers_9/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:33.050395 140143678494720 adafactor.py:358] Since rank of parameter decoder/layers_8/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:33.055015 139876724979712 adafactor.py:358] Since rank of parameter decoder/layers_6/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:33.039593 140643590531072 adafactor.py:358] Since rank of parameter decoder/layers_7/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:33.039604 140180528838656 adafactor.py:358] Since rank of parameter decoder/layers_8/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:33.060551 139856202532864 adafactor.py:358] Since rank of parameter decoder/layers_8/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:33.044899 140260698863616 adafactor.py:358] Since rank of parameter decoder/layers_5/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:33.065139 139666521167872 adafactor.py:358] Since rank of parameter decoder/layers_9/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:33.064423 140143678494720 adafactor.py:358] Since rank of parameter decoder/layers_8/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:33.069785 139876724979712 adafactor.py:358] Since rank of parameter decoder/layers_6/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:33.054407 140180528838656 adafactor.py:358] Since rank of parameter decoder/layers_8/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:33.055157 140643590531072 adafactor.py:358] Since rank of parameter decoder/layers_8/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:33.075942 139856202532864 adafactor.py:358] Since rank of parameter decoder/layers_8/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:33.059065 140260698863616 adafactor.py:358] Since rank of parameter decoder/layers_6/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:33.079040 139666521167872 adafactor.py:358] Since rank of parameter decoder/layers_9/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:33.078350 140143678494720 adafactor.py:358] Since rank of parameter decoder/layers_8/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:33.065521 140180528838656 adafactor.py:358] Since rank of parameter decoder/layers_8/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:33.083866 139876724979712 adafactor.py:358] Since rank of parameter decoder/layers_6/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:33.069679 140643590531072 adafactor.py:358] Since rank of parameter decoder/layers_8/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:33.088455 140143678494720 adafactor.py:358] Since rank of parameter decoder/layers_8/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:33.091482 139856202532864 adafactor.py:358] Since rank of parameter decoder/layers_8/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:33.073481 140260698863616 adafactor.py:358] Since rank of parameter decoder/layers_6/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:33.092932 139666521167872 adafactor.py:358] Since rank of parameter decoder/logits_dense/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:33.094162 139876724979712 adafactor.py:358] Since rank of parameter decoder/layers_6/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:33.076140 140180528838656 adafactor.py:358] Since rank of parameter decoder/layers_8/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:33.098645 140143678494720 adafactor.py:358] Since rank of parameter decoder/layers_8/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:33.084067 140643590531072 adafactor.py:358] Since rank of parameter decoder/layers_8/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:33.104462 139876724979712 adafactor.py:358] Since rank of parameter decoder/layers_6/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:33.105799 139856202532864 adafactor.py:358] Since rank of parameter decoder/layers_8/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:33.087913 140260698863616 adafactor.py:358] Since rank of parameter decoder/layers_6/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:33.090300 140180528838656 adafactor.py:358] Since rank of parameter decoder/layers_8/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:33.112991 140143678494720 adafactor.py:358] Since rank of parameter decoder/layers_8/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:33.098376 140643590531072 adafactor.py:358] Since rank of parameter decoder/layers_8/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:33.116866 139856202532864 adafactor.py:358] Since rank of parameter decoder/layers_8/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:33.116872 139666521167872 adafactor.py:358] Since rank of parameter token_embedder/embedding 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:33.118423 139876724979712 adafactor.py:358] Since rank of parameter decoder/layers_6/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:33.102158 140260698863616 adafactor.py:358] Since rank of parameter decoder/layers_6/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:33.104940 140180528838656 adafactor.py:358] Since rank of parameter decoder/layers_8/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:33.109771 140643590531072 adafactor.py:358] Since rank of parameter decoder/layers_8/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:33.128331 139856202532864 adafactor.py:358] Since rank of parameter decoder/layers_8/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:33.127564 140143678494720 adafactor.py:358] Since rank of parameter decoder/layers_8/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:33.132400 139876724979712 adafactor.py:358] Since rank of parameter decoder/layers_6/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:33.113692 140260698863616 adafactor.py:358] Since rank of parameter decoder/layers_6/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:33.118876 140180528838656 adafactor.py:358] Since rank of parameter decoder/layers_8/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:33.120616 140643590531072 adafactor.py:358] Since rank of parameter decoder/layers_8/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:33.142826 139856202532864 adafactor.py:358] Since rank of parameter decoder/layers_8/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:33.124116 140260698863616 adafactor.py:358] Since rank of parameter decoder/layers_6/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:33.141884 140143678494720 adafactor.py:358] Since rank of parameter decoder/layers_8/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:33.146418 139876724979712 adafactor.py:358] Since rank of parameter decoder/layers_6/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:33.133132 140180528838656 adafactor.py:358] Since rank of parameter decoder/layers_9/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:33.134593 140643590531072 adafactor.py:358] Since rank of parameter decoder/layers_8/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:33.138380 140260698863616 adafactor.py:358] Since rank of parameter decoder/layers_6/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:33.158014 139856202532864 adafactor.py:358] Since rank of parameter decoder/layers_8/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:33.156027 140143678494720 adafactor.py:358] Since rank of parameter decoder/layers_9/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:33.160444 139876724979712 adafactor.py:358] Since rank of parameter decoder/layers_7/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:33.147114 140180528838656 adafactor.py:358] Since rank of parameter decoder/layers_9/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:33.149406 140643590531072 adafactor.py:358] Since rank of parameter decoder/layers_8/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:33.172200 139856202532864 adafactor.py:358] Since rank of parameter decoder/layers_8/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:33.152800 140260698863616 adafactor.py:358] Since rank of parameter decoder/layers_6/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:33.170281 140143678494720 adafactor.py:358] Since rank of parameter decoder/layers_9/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:33.174535 139876724979712 adafactor.py:358] Since rank of parameter decoder/layers_7/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:33.161022 140180528838656 adafactor.py:358] Since rank of parameter decoder/layers_9/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:33.163381 140643590531072 adafactor.py:358] Since rank of parameter decoder/layers_8/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:33.186341 139856202532864 adafactor.py:358] Since rank of parameter decoder/layers_9/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:33.184659 140143678494720 adafactor.py:358] Since rank of parameter decoder/layers_9/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:33.168571 140260698863616 adafactor.py:358] Since rank of parameter decoder/layers_6/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:33.188508 139876724979712 adafactor.py:358] Since rank of parameter decoder/layers_7/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:33.175016 140180528838656 adafactor.py:358] Since rank of parameter decoder/layers_9/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:33.177326 140643590531072 adafactor.py:358] Since rank of parameter decoder/layers_9/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:33.200681 139856202532864 adafactor.py:358] Since rank of parameter decoder/layers_9/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:33.198864 140143678494720 adafactor.py:358] Since rank of parameter decoder/layers_9/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:33.183268 140260698863616 adafactor.py:358] Since rank of parameter decoder/layers_7/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:33.202969 139876724979712 adafactor.py:358] Since rank of parameter decoder/layers_7/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:33.185265 140180528838656 adafactor.py:358] Since rank of parameter decoder/layers_9/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:33.191206 140643590531072 adafactor.py:358] Since rank of parameter decoder/layers_9/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:33.209283 140143678494720 adafactor.py:358] Since rank of parameter decoder/layers_9/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:33.213254 139876724979712 adafactor.py:358] Since rank of parameter decoder/layers_7/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:33.195432 140180528838656 adafactor.py:358] Since rank of parameter decoder/layers_9/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:33.215048 139856202532864 adafactor.py:358] Since rank of parameter decoder/layers_9/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:33.197646 140260698863616 adafactor.py:358] Since rank of parameter decoder/layers_7/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:33.219724 140143678494720 adafactor.py:358] Since rank of parameter decoder/layers_9/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:33.223481 139876724979712 adafactor.py:358] Since rank of parameter decoder/layers_7/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:33.205786 140643590531072 adafactor.py:358] Since rank of parameter decoder/layers_9/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:33.209341 140180528838656 adafactor.py:358] Since rank of parameter decoder/layers_9/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:33.229359 139856202532864 adafactor.py:358] Since rank of parameter decoder/layers_9/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:33.211988 140260698863616 adafactor.py:358] Since rank of parameter decoder/layers_7/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:33.233437 140143678494720 adafactor.py:358] Since rank of parameter decoder/layers_9/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:33.237505 139876724979712 adafactor.py:358] Since rank of parameter decoder/layers_7/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:33.220174 140643590531072 adafactor.py:358] Since rank of parameter decoder/layers_9/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:33.240784 139856202532864 adafactor.py:358] Since rank of parameter decoder/layers_9/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:33.223170 140180528838656 adafactor.py:358] Since rank of parameter decoder/layers_9/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:33.226726 140260698863616 adafactor.py:358] Since rank of parameter decoder/layers_7/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:33.230629 140643590531072 adafactor.py:358] Since rank of parameter decoder/layers_9/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:33.247681 140143678494720 adafactor.py:358] Since rank of parameter decoder/layers_9/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:33.251409 139856202532864 adafactor.py:358] Since rank of parameter decoder/layers_9/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:33.251554 139876724979712 adafactor.py:358] Since rank of parameter decoder/layers_7/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:33.237929 140180528838656 adafactor.py:358] Since rank of parameter decoder/layers_9/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:33.237251 140260698863616 adafactor.py:358] Since rank of parameter decoder/layers_7/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:33.242256 140643590531072 adafactor.py:358] Since rank of parameter decoder/layers_9/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:33.261648 140143678494720 adafactor.py:358] Since rank of parameter decoder/layers_9/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:33.265910 139856202532864 adafactor.py:358] Since rank of parameter decoder/layers_9/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:33.265807 139876724979712 adafactor.py:358] Since rank of parameter decoder/layers_7/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:33.247640 140260698863616 adafactor.py:358] Since rank of parameter decoder/layers_7/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:33.251786 140180528838656 adafactor.py:358] Since rank of parameter decoder/logits_dense/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:33.257162 140643590531072 adafactor.py:358] Since rank of parameter decoder/layers_9/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:33.275819 140143678494720 adafactor.py:358] Since rank of parameter decoder/logits_dense/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:33.280759 139856202532864 adafactor.py:358] Since rank of parameter decoder/layers_9/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:33.262422 140260698863616 adafactor.py:358] Since rank of parameter decoder/layers_7/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:33.281599 139876724979712 adafactor.py:358] Since rank of parameter decoder/layers_8/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:33.271168 140643590531072 adafactor.py:358] Since rank of parameter decoder/layers_9/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:33.275741 140180528838656 adafactor.py:358] Since rank of parameter token_embedder/embedding 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:33.296209 139856202532864 adafactor.py:358] Since rank of parameter decoder/layers_9/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:33.297194 139876724979712 adafactor.py:358] Since rank of parameter decoder/layers_8/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:33.278540 140260698863616 adafactor.py:358] Since rank of parameter decoder/layers_7/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:33.300163 140143678494720 adafactor.py:358] Since rank of parameter token_embedder/embedding 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:33.285055 140643590531072 adafactor.py:358] Since rank of parameter decoder/layers_9/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:33.310469 139856202532864 adafactor.py:358] Since rank of parameter decoder/logits_dense/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:33.313119 139876724979712 adafactor.py:358] Since rank of parameter decoder/layers_8/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:33.294448 140260698863616 adafactor.py:358] Since rank of parameter decoder/layers_7/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:33.300100 140643590531072 adafactor.py:358] Since rank of parameter decoder/logits_dense/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:33.328244 139876724979712 adafactor.py:358] Since rank of parameter decoder/layers_8/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:33.308956 140260698863616 adafactor.py:358] Since rank of parameter decoder/layers_8/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:33.335153 139856202532864 adafactor.py:358] Since rank of parameter token_embedder/embedding 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:33.340584 139876724979712 adafactor.py:358] Since rank of parameter decoder/layers_8/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:33.324102 140260698863616 adafactor.py:358] Since rank of parameter decoder/layers_8/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:33.326345 140643590531072 adafactor.py:358] Since rank of parameter token_embedder/embedding 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:33.351622 139876724979712 adafactor.py:358] Since rank of parameter decoder/layers_8/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:33.340419 140260698863616 adafactor.py:358] Since rank of parameter decoder/layers_8/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:33.366729 139876724979712 adafactor.py:358] Since rank of parameter decoder/layers_8/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:33.356703 140260698863616 adafactor.py:358] Since rank of parameter decoder/layers_8/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:33.380939 139876724979712 adafactor.py:358] Since rank of parameter decoder/layers_8/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:33.369121 140260698863616 adafactor.py:358] Since rank of parameter decoder/layers_8/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:33.394980 139876724979712 adafactor.py:358] Since rank of parameter decoder/layers_8/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:33.379955 140260698863616 adafactor.py:358] Since rank of parameter decoder/layers_8/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:33.409187 139876724979712 adafactor.py:358] Since rank of parameter decoder/layers_9/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:33.394565 140260698863616 adafactor.py:358] Since rank of parameter decoder/layers_8/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:33.423398 139876724979712 adafactor.py:358] Since rank of parameter decoder/layers_9/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:33.409777 140260698863616 adafactor.py:358] Since rank of parameter decoder/layers_8/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:33.438992 139876724979712 adafactor.py:358] Since rank of parameter decoder/layers_9/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:33.424097 140260698863616 adafactor.py:358] Since rank of parameter decoder/layers_8/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:33.454486 139876724979712 adafactor.py:358] Since rank of parameter decoder/layers_9/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:33.440301 140260698863616 adafactor.py:358] Since rank of parameter decoder/layers_9/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:33.465734 139876724979712 adafactor.py:358] Since rank of parameter decoder/layers_9/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:33.456042 140260698863616 adafactor.py:358] Since rank of parameter decoder/layers_9/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:33.477758 139876724979712 adafactor.py:358] Since rank of parameter decoder/layers_9/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:33.472162 140260698863616 adafactor.py:358] Since rank of parameter decoder/layers_9/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:33.492972 139876724979712 adafactor.py:358] Since rank of parameter decoder/layers_9/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:33.486763 140260698863616 adafactor.py:358] Since rank of parameter decoder/layers_9/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:33.508147 139876724979712 adafactor.py:358] Since rank of parameter decoder/layers_9/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:33.497513 140260698863616 adafactor.py:358] Since rank of parameter decoder/layers_9/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 23:45:33.523590 139876724979712 adafactor.py:358] Since rank of parameter decoder/layers_9/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:33.508106 140260698863616 adafactor.py:358] Since rank of parameter decoder/layers_9/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:33.537662 139876724979712 adafactor.py:358] Since rank of parameter decoder/logits_dense/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:33.522499 140260698863616 adafactor.py:358] Since rank of parameter decoder/layers_9/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:33.536867 140260698863616 adafactor.py:358] Since rank of parameter decoder/layers_9/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:33.562821 139876724979712 adafactor.py:358] Since rank of parameter token_embedder/embedding 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 23:45:33.551739 140260698863616 adafactor.py:358] Since rank of parameter decoder/layers_9/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:33.565903 140260698863616 adafactor.py:358] Since rank of parameter decoder/logits_dense/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 23:45:33.591775 140260698863616 adafactor.py:358] Since rank of parameter token_embedder/embedding 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
I0512 23:51:13.370078 140143678494720 train.py:675] Epoch 4 of 200
I0512 23:51:13.370583 140055069881920 logging_writer.py:48] [10000] collection=train timing/compilation_seconds=350.508
I0512 23:51:13.372557 140143678494720 train.py:681] BEGIN Train loop.
I0512 23:51:13.373065 140055069881920 logging_writer.py:48] [10000] collection=train timing/train_iter_warmup=1.00136e-05
I0512 23:51:13.373179 140143678494720 train.py:686] Training for 2500 steps.
I0512 23:51:13.379459 140143678494720 trainer.py:518] Training: step 10000
I0512 23:51:52.247819 140193652815872 train.py:675] Epoch 4 of 200
I0512 23:51:52.248263 140193652815872 train.py:681] BEGIN Train loop.
I0512 23:51:52.248343 140193652815872 train.py:686] Training for 2500 steps.
I0512 23:51:52.254535 140193652815872 trainer.py:518] Training: step 10000
I0512 23:52:01.367516 140260698863616 train.py:675] Epoch 4 of 200
I0512 23:52:01.368015 140260698863616 train.py:681] BEGIN Train loop.
I0512 23:52:01.368060 140260698863616 train.py:686] Training for 2500 steps.
I0512 23:52:01.374345 140260698863616 trainer.py:518] Training: step 10000
I0512 23:52:11.577456 140260698863616 trainer.py:518] Training: step 10005
I0512 23:52:20.701365 139666521167872 train.py:675] Epoch 4 of 200
I0512 23:52:20.701779 139666521167872 train.py:681] BEGIN Train loop.
I0512 23:52:20.701833 139666521167872 train.py:686] Training for 2500 steps.
I0512 23:52:20.710775 139666521167872 trainer.py:518] Training: step 10000
I0512 23:52:24.003714 140643590531072 train.py:675] Epoch 4 of 200
I0512 23:52:24.004125 140643590531072 train.py:681] BEGIN Train loop.
I0512 23:52:24.004179 140643590531072 train.py:686] Training for 2500 steps.
I0512 23:52:24.015427 140643590531072 trainer.py:518] Training: step 10000
I0512 23:52:26.541131 139856202532864 train.py:675] Epoch 4 of 200
I0512 23:52:26.541546 139856202532864 train.py:681] BEGIN Train loop.
I0512 23:52:26.541597 139856202532864 train.py:686] Training for 2500 steps.
I0512 23:52:26.551918 139856202532864 trainer.py:518] Training: step 10000
I0512 23:52:35.419743 139666521167872 trainer.py:518] Training: step 10001
I0512 23:52:36.065825 140643590531072 trainer.py:518] Training: step 10002
I0512 23:52:41.095963 139856202532864 trainer.py:518] Training: step 10001
I0512 23:53:03.048859 140180528838656 train.py:675] Epoch 4 of 200
I0512 23:53:03.049773 140180528838656 train.py:681] BEGIN Train loop.
I0512 23:53:03.049860 140180528838656 train.py:686] Training for 2500 steps.
I0512 23:53:03.074670 140180528838656 trainer.py:518] Training: step 10000
I0512 23:53:05.573265 139876724979712 train.py:675] Epoch 4 of 200
I0512 23:53:05.573805 139876724979712 train.py:681] BEGIN Train loop.
I0512 23:53:05.573860 139876724979712 train.py:686] Training for 2500 steps.
I0512 23:53:05.580205 139876724979712 trainer.py:518] Training: step 10000
I0512 23:53:22.231585 139876724979712 trainer.py:518] Training: step 10001
I0512 23:53:31.050051 139666521167872 trainer.py:518] Training: step 10013
I0512 23:53:31.051480 139856202532864 trainer.py:518] Training: step 10013
I0512 23:53:31.048947 140143678494720 trainer.py:518] Training: step 10013
I0512 23:53:31.055034 140193652815872 trainer.py:518] Training: step 10013
I0512 23:53:31.050361 140643590531072 trainer.py:518] Training: step 10013
I0512 23:53:31.052029 140180528838656 trainer.py:518] Training: step 10013
I0512 23:53:31.057433 140260698863616 trainer.py:518] Training: step 10013
I0512 23:53:40.206447 139876724979712 trainer.py:518] Training: step 10014
I0512 23:53:48.992416 140180528838656 trainer.py:518] Training: step 10015
I0512 23:53:49.035995 140143678494720 trainer.py:518] Training: step 10015
I0512 23:53:49.039788 140643590531072 trainer.py:518] Training: step 10015
I0512 23:53:49.110878 140193652815872 trainer.py:518] Training: step 10015
I0512 23:53:49.118088 139856202532864 trainer.py:518] Training: step 10015
I0512 23:53:49.129063 139666521167872 trainer.py:518] Training: step 10015
I0512 23:53:49.121876 140260698863616 trainer.py:518] Training: step 10015
I0512 23:53:58.192375 139876724979712 trainer.py:518] Training: step 10016
I0512 23:54:06.850541 140143678494720 trainer.py:518] Training: step 10017
I0512 23:54:06.892505 139666521167872 trainer.py:518] Training: step 10017
I0512 23:54:06.951612 140193652815872 trainer.py:518] Training: step 10017
I0512 23:54:07.074107 140643590531072 trainer.py:518] Training: step 10017
I0512 23:54:07.080373 140260698863616 trainer.py:518] Training: step 10017
I0512 23:54:07.156356 139856202532864 trainer.py:518] Training: step 10017
I0512 23:54:07.198412 140180528838656 trainer.py:518] Training: step 10017
I0512 23:54:16.122435 139876724979712 trainer.py:518] Training: step 10018
I0512 23:54:24.726771 140193652815872 trainer.py:518] Training: step 10019
I0512 23:54:24.796495 140643590531072 trainer.py:518] Training: step 10019
I0512 23:54:24.904778 140180528838656 trainer.py:518] Training: step 10019
I0512 23:54:24.921663 140143678494720 trainer.py:518] Training: step 10019
I0512 23:54:24.958673 139856202532864 trainer.py:518] Training: step 10019
I0512 23:54:24.949824 140260698863616 trainer.py:518] Training: step 10019
I0512 23:54:24.994714 139666521167872 trainer.py:518] Training: step 10019
I0512 23:54:51.502358 139856202532864 trainer.py:518] Training: step 10020
I0512 23:54:51.502352 139876724979712 trainer.py:518] Training: step 10020
I0512 23:54:51.503325 140193652815872 trainer.py:518] Training: step 10020
I0512 23:54:51.505191 140143678494720 trainer.py:518] Training: step 10020
I0512 23:54:51.514238 139666521167872 trainer.py:518] Training: step 10020
I0512 23:54:51.503873 140260698863616 trainer.py:518] Training: step 10020
I0512 23:54:51.505626 140643590531072 trainer.py:518] Training: step 10020
I0512 23:54:51.510097 140180528838656 trainer.py:518] Training: step 10020
I0512 23:55:09.380162 139876724979712 trainer.py:518] Training: step 10023
I0512 23:55:09.380004 140193652815872 trainer.py:518] Training: step 10023
I0512 23:55:09.382928 140143678494720 trainer.py:518] Training: step 10023
I0512 23:55:09.389237 139856202532864 trainer.py:518] Training: step 10023
I0512 23:55:09.389999 139666521167872 trainer.py:518] Training: step 10023
I0512 23:55:09.381228 140260698863616 trainer.py:518] Training: step 10023
I0512 23:55:09.383234 140643590531072 trainer.py:518] Training: step 10023
I0512 23:55:09.387003 140180528838656 trainer.py:518] Training: step 10023
I0512 23:55:27.259067 140193652815872 trainer.py:518] Training: step 10024
I0512 23:55:27.260913 139666521167872 trainer.py:518] Training: step 10024
I0512 23:55:27.261927 140143678494720 trainer.py:518] Training: step 10024
I0512 23:55:27.272702 139856202532864 trainer.py:518] Training: step 10024
I0512 23:55:27.257329 140643590531072 trainer.py:518] Training: step 10024
I0512 23:55:27.257914 140260698863616 trainer.py:518] Training: step 10024
I0512 23:55:27.277661 139876724979712 trainer.py:518] Training: step 10024
I0512 23:55:27.263137 140180528838656 trainer.py:518] Training: step 10024
I0512 23:55:45.135483 139876724979712 trainer.py:518] Training: step 10025
I0512 23:55:45.137686 139856202532864 trainer.py:518] Training: step 10025
I0512 23:55:45.140206 140193652815872 trainer.py:518] Training: step 10025
I0512 23:55:45.139623 140143678494720 trainer.py:518] Training: step 10025
I0512 23:55:45.145046 139666521167872 trainer.py:518] Training: step 10025
I0512 23:55:45.136333 140260698863616 trainer.py:518] Training: step 10025
I0512 23:55:45.138566 140643590531072 trainer.py:518] Training: step 10025
I0512 23:55:45.139743 140180528838656 trainer.py:518] Training: step 10025
I0512 23:56:03.014216 140193652815872 trainer.py:518] Training: step 10026
I0512 23:56:03.014858 140143678494720 trainer.py:518] Training: step 10026
I0512 23:56:03.021347 139876724979712 trainer.py:518] Training: step 10026
I0512 23:56:03.028153 139856202532864 trainer.py:518] Training: step 10026
I0512 23:56:03.016940 140643590531072 trainer.py:518] Training: step 10026
I0512 23:56:03.038336 139666521167872 trainer.py:518] Training: step 10026
I0512 23:56:03.027950 140260698863616 trainer.py:518] Training: step 10026
I0512 23:56:03.052052 140180528838656 trainer.py:518] Training: step 10026
I0512 23:56:20.893588 140193652815872 trainer.py:518] Training: step 10027
I0512 23:56:20.895871 139856202532864 trainer.py:518] Training: step 10027
I0512 23:56:20.896687 140143678494720 trainer.py:518] Training: step 10027
I0512 23:56:20.902705 139666521167872 trainer.py:518] Training: step 10027
I0512 23:56:20.895727 140260698863616 trainer.py:518] Training: step 10027
I0512 23:56:20.903846 140643590531072 trainer.py:518] Training: step 10027
I0512 23:56:20.927003 139876724979712 trainer.py:518] Training: step 10027
I0512 23:56:20.910234 140180528838656 trainer.py:518] Training: step 10027
I0512 23:56:38.770979 139666521167872 trainer.py:518] Training: step 10029
I0512 23:56:38.772626 139856202532864 trainer.py:518] Training: step 10029
I0512 23:56:38.772602 140193652815872 trainer.py:518] Training: step 10029
I0512 23:56:38.781068 139876724979712 trainer.py:518] Training: step 10029
I0512 23:56:38.772806 140180528838656 trainer.py:518] Training: step 10029
I0512 23:56:38.774307 140260698863616 trainer.py:518] Training: step 10029
I0512 23:56:38.802424 140143678494720 trainer.py:518] Training: step 10029
I0512 23:57:05.599603 140643590531072 trainer.py:518] Training: step 10030
I0512 23:57:14.548559 140260698863616 trainer.py:518] Training: step 10030
I0512 23:57:23.465877 140193652815872 trainer.py:518] Training: step 10030
I0512 23:57:23.467609 139876724979712 trainer.py:518] Training: step 10030
I0512 23:57:23.471196 139666521167872 trainer.py:518] Training: step 10030
I0512 23:57:23.466455 140643590531072 trainer.py:518] Training: step 10032
I0512 23:57:23.467817 140180528838656 trainer.py:518] Training: step 10030
I0512 23:57:32.403198 140143678494720 trainer.py:518] Training: step 10030
I0512 23:57:32.411230 139856202532864 trainer.py:518] Training: step 10030
I0512 23:57:41.349838 140193652815872 trainer.py:518] Training: step 10032
I0512 23:57:41.346352 140643590531072 trainer.py:518] Training: step 10033
I0512 23:57:41.347777 140260698863616 trainer.py:518] Training: step 10033
I0512 23:57:41.355824 140180528838656 trainer.py:518] Training: step 10032
I0512 23:57:50.281771 140055069881920 logging_writer.py:48] [10010] collection=train accuracy=0.500623, cross_ent_loss=20.422895431518555, cross_ent_loss_per_all_target_tokens=2.59691e-05, experts/auxiliary_loss=0.1408402919769287, experts/expert_usage=7.846593379974365, experts/fraction_tokens_left_behind=2.3216240406036377, experts/router_confidence=2.5812249183654785, experts/router_z_loss=0.001767679350450635, learning_rate=0.00999775, learning_rate/current=0.0099955, loss=20.654157638549805, loss_per_all_target_tokens=2.62631e-05, loss_per_nonpadding_target_token=6.3399e-05, non_padding_fraction/loss_weights=0.414251, timing/seconds=218.0802001953125, timing/seqs=3840, timing/seqs_per_second=17.608200073242188, timing/seqs_per_second_per_core=0.5502562522888184, timing/steps_per_second=0.04585468769073486, timing/target_tokens_per_second=36061.59375, timing/target_tokens_per_second_per_core=1126.9248046875, timing/uptime=356.36, z_loss=0.0886518731713295, z_loss_per_all_target_tokens=1.12727e-07
I0512 23:57:50.285709 139876724979712 trainer.py:518] Training: step 10032
I0512 23:57:50.287124 139666521167872 trainer.py:518] Training: step 10032
I0512 23:57:50.311886 140143678494720 trainer.py:518] Training: step 10032
I0512 23:57:59.222721 140193652815872 trainer.py:518] Training: step 10033
I0512 23:57:59.225852 139856202532864 trainer.py:518] Training: step 10032
I0512 23:57:59.222882 140180528838656 trainer.py:518] Training: step 10033
I0512 23:58:08.164007 140055069881920 logging_writer.py:48] [10020] collection=train accuracy=0.50336, cross_ent_loss=20.233842849731445, cross_ent_loss_per_all_target_tokens=2.57287e-05, experts/auxiliary_loss=0.1413300484418869, experts/expert_usage=7.852717876434326, experts/fraction_tokens_left_behind=2.253648519515991, experts/router_confidence=2.5926620960235596, experts/router_z_loss=0.00174894486553967, learning_rate=0.00999276, learning_rate/current=0.00999051, loss=20.46630859375, loss_per_all_target_tokens=2.60243e-05, loss_per_nonpadding_target_token=6.39549e-05, non_padding_fraction/loss_weights=0.406916, timing/seconds=89.37986755371094, timing/seqs=3840, timing/seqs_per_second=42.96269607543945, timing/seqs_per_second_per_core=1.342584252357483, timing/steps_per_second=0.11188202351331711, timing/target_tokens_per_second=87987.6015625, timing/target_tokens_per_second_per_core=2749.612548828125, timing/uptime=551.715, z_loss=0.0893862172961235, z_loss_per_all_target_tokens=1.1366e-07
I0512 23:58:08.161019 140643590531072 trainer.py:518] Training: step 10034
I0512 23:58:08.160645 140260698863616 trainer.py:518] Training: step 10034
I0512 23:58:09.521234 139856202532864 trainer.py:518] Training: step 10037
I0512 23:58:17.104079 139666521167872 trainer.py:518] Training: step 10034
I0512 23:58:17.103560 140143678494720 trainer.py:518] Training: step 10034
I0512 23:58:17.120445 139876724979712 trainer.py:518] Training: step 10034
I0512 23:58:26.048611 140055069881920 logging_writer.py:48] [10030] collection=train accuracy=0.510921, cross_ent_loss=19.991252899169922, cross_ent_loss_per_all_target_tokens=2.54202e-05, experts/auxiliary_loss=0.14303623139858246, experts/expert_usage=7.830586910247803, experts/fraction_tokens_left_behind=2.165008306503296, experts/router_confidence=2.6297242641448975, experts/router_z_loss=0.001705056056380272, learning_rate=0.00998777, learning_rate/current=0.00998553, loss=20.224864959716797, loss_per_all_target_tokens=2.57172e-05, loss_per_nonpadding_target_token=6.16637e-05, non_padding_fraction/loss_weights=0.417057, timing/seconds=89.37529754638672, timing/seqs=3840, timing/seqs_per_second=42.96489334106445, timing/seqs_per_second_per_core=1.3426529169082642, timing/steps_per_second=0.11188774555921555, timing/target_tokens_per_second=87992.1015625, timing/target_tokens_per_second_per_core=2749.753173828125, timing/uptime=694.725, z_loss=0.0888710767030716, z_loss_per_all_target_tokens=1.13005e-07
I0512 23:58:26.044223 140180528838656 trainer.py:518] Training: step 10035
I0512 23:58:26.046809 140643590531072 trainer.py:518] Training: step 10035
I0512 23:58:26.047687 140260698863616 trainer.py:518] Training: step 10035
I0512 23:58:26.073784 140193652815872 trainer.py:518] Training: step 10035
I0512 23:58:26.253399 139856202532864 trainer.py:518] Training: step 10046
I0512 23:58:27.228648 139666521167872 trainer.py:518] Training: step 10042
I0512 23:58:35.085007 140143678494720 trainer.py:518] Training: step 10047
I0512 23:58:35.274242 139876724979712 trainer.py:518] Training: step 10047
I0512 23:58:36.086736 140180528838656 trainer.py:518] Training: step 10044
I0512 23:58:36.131501 140193652815872 trainer.py:518] Training: step 10044
I0512 23:58:43.920425 140643590531072 trainer.py:518] Training: step 10042
I0512 23:58:43.979030 140260698863616 trainer.py:518] Training: step 10041
I0512 23:58:44.132580 139666521167872 trainer.py:518] Training: step 10048
I0512 23:58:44.434277 139856202532864 trainer.py:518] Training: step 10048
I0512 23:58:52.901623 140193652815872 trainer.py:518] Training: step 10049
I0512 23:58:52.938925 140143678494720 trainer.py:518] Training: step 10049
I0512 23:58:53.055804 140180528838656 trainer.py:518] Training: step 10049
I0512 23:58:53.086853 139876724979712 trainer.py:518] Training: step 10049
I0512 23:59:19.678175 140193652815872 trainer.py:518] Training: step 10050
I0512 23:59:19.678160 139856202532864 trainer.py:518] Training: step 10050
I0512 23:59:19.682098 139666521167872 trainer.py:518] Training: step 10050
I0512 23:59:19.689917 139876724979712 trainer.py:518] Training: step 10050
I0512 23:59:19.681029 140643590531072 trainer.py:518] Training: step 10050
I0512 23:59:19.683082 140143678494720 trainer.py:518] Training: step 10050
I0512 23:59:19.708940 140260698863616 trainer.py:518] Training: step 10050
I0512 23:59:19.883541 140180528838656 trainer.py:518] Training: step 10050
I0512 23:59:37.647119 139876724979712 trainer.py:518] Training: step 10051
I0512 23:59:37.670656 140643590531072 trainer.py:518] Training: step 10051
I0512 23:59:46.494786 139856202532864 trainer.py:518] Training: step 10052
I0512 23:59:46.496378 140193652815872 trainer.py:518] Training: step 10052
I0512 23:59:46.496872 140180528838656 trainer.py:518] Training: step 10052
I0512 23:59:46.501041 139666521167872 trainer.py:518] Training: step 10052
I0512 23:59:46.501579 140143678494720 trainer.py:518] Training: step 10052
I0512 23:59:46.517112 140260698863616 trainer.py:518] Training: step 10052
I0512 23:59:55.437243 139876724979712 trainer.py:518] Training: step 10052
I0512 23:59:55.435956 140643590531072 trainer.py:518] Training: step 10052
I0513 00:00:04.375268 140193652815872 trainer.py:518] Training: step 10053
I0513 00:00:04.378105 139666521167872 trainer.py:518] Training: step 10053
I0513 00:00:04.373287 140260698863616 trainer.py:518] Training: step 10053
I0513 00:00:04.373536 140143678494720 trainer.py:518] Training: step 10053
I0513 00:00:04.397783 140180528838656 trainer.py:518] Training: step 10053
I0513 00:00:13.314146 139856202532864 trainer.py:518] Training: step 10053
I0513 00:00:13.314234 139876724979712 trainer.py:518] Training: step 10053
I0513 00:00:13.318931 140643590531072 trainer.py:518] Training: step 10053
I0513 00:00:22.250986 140143678494720 trainer.py:518] Training: step 10054
I0513 00:00:22.254046 140193652815872 trainer.py:518] Training: step 10054
I0513 00:00:22.266855 140180528838656 trainer.py:518] Training: step 10054
I0513 00:00:22.251893 140260698863616 trainer.py:518] Training: step 10054
I0513 00:00:22.263986 139666521167872 trainer.py:518] Training: step 10054
I0513 00:00:31.198215 139856202532864 trainer.py:518] Training: step 10054
I0513 00:00:31.193041 140643590531072 trainer.py:518] Training: step 10054
I0513 00:00:31.195175 139876724979712 trainer.py:518] Training: step 10054
I0513 00:00:40.130706 140193652815872 trainer.py:518] Training: step 10055
I0513 00:00:40.129337 140143678494720 trainer.py:518] Training: step 10055
I0513 00:00:40.144666 140180528838656 trainer.py:518] Training: step 10055
I0513 00:00:40.133390 140260698863616 trainer.py:518] Training: step 10055
I0513 00:00:40.130753 139666521167872 trainer.py:518] Training: step 10055
I0513 00:00:49.072099 139856202532864 trainer.py:518] Training: step 10055
I0513 00:00:49.072195 140643590531072 trainer.py:518] Training: step 10055
I0513 00:00:49.072657 139876724979712 trainer.py:518] Training: step 10055
I0513 00:00:58.008772 140143678494720 trainer.py:518] Training: step 10056
I0513 00:00:58.014802 140193652815872 trainer.py:518] Training: step 10056
I0513 00:00:58.016241 140180528838656 trainer.py:518] Training: step 10056
I0513 00:00:58.009948 140260698863616 trainer.py:518] Training: step 10056
I0513 00:00:58.019409 139666521167872 trainer.py:518] Training: step 10056
I0513 00:01:06.949090 139856202532864 trainer.py:518] Training: step 10056
I0513 00:01:06.948463 140643590531072 trainer.py:518] Training: step 10056
I0513 00:01:06.948837 139876724979712 trainer.py:518] Training: step 10056
I0513 00:01:15.888270 140193652815872 trainer.py:518] Training: step 10057
I0513 00:01:15.888625 140143678494720 trainer.py:518] Training: step 10057
I0513 00:01:15.890274 140180528838656 trainer.py:518] Training: step 10057
I0513 00:01:15.888102 140260698863616 trainer.py:518] Training: step 10057
I0513 00:01:15.890857 139666521167872 trainer.py:518] Training: step 10057
I0513 00:01:24.826321 139876724979712 trainer.py:518] Training: step 10057
I0513 00:01:24.828775 140643590531072 trainer.py:518] Training: step 10057
I0513 00:01:33.765685 140193652815872 trainer.py:518] Training: step 10058
I0513 00:01:33.766180 140180528838656 trainer.py:518] Training: step 10058
I0513 00:01:33.766373 139856202532864 trainer.py:518] Training: step 10058
I0513 00:01:33.766880 140143678494720 trainer.py:518] Training: step 10058
I0513 00:01:33.765849 139666521167872 trainer.py:518] Training: step 10058
I0513 00:01:33.772913 140260698863616 trainer.py:518] Training: step 10058
I0513 00:01:42.705593 140055069881920 logging_writer.py:48] [10040] collection=train accuracy=0.506187, cross_ent_loss=20.102970123291016, cross_ent_loss_per_all_target_tokens=2.55622e-05, experts/auxiliary_loss=0.141753688454628, experts/expert_usage=7.838589668273926, experts/fraction_tokens_left_behind=2.240729808807373, experts/router_confidence=2.5997631549835205, experts/router_z_loss=0.0017427444690838456, learning_rate=0.00998279, learning_rate/current=0.00998056, loss=20.336698532104492, loss_per_all_target_tokens=2.58594e-05, loss_per_nonpadding_target_token=6.29478e-05, non_padding_fraction/loss_weights=0.410808, timing/seconds=89.38716888427734, timing/seqs=3840, timing/seqs_per_second=42.95918655395508, timing/seqs_per_second_per_core=1.3424745798110962, timing/steps_per_second=0.11187288165092468, timing/target_tokens_per_second=87980.4140625, timing/target_tokens_per_second_per_core=2749.387939453125, timing/uptime=775.675, z_loss=0.09023160487413406, z_loss_per_all_target_tokens=1.14735e-07
I0513 00:01:42.704766 139876724979712 trainer.py:518] Training: step 10058
I0513 00:01:42.705719 140643590531072 trainer.py:518] Training: step 10058
I0513 00:01:51.643696 140193652815872 trainer.py:518] Training: step 10059
I0513 00:01:51.644436 139856202532864 trainer.py:518] Training: step 10059
I0513 00:01:51.644393 140180528838656 trainer.py:518] Training: step 10059
I0513 00:01:51.644582 140143678494720 trainer.py:518] Training: step 10059
I0513 00:01:51.647770 139666521167872 trainer.py:518] Training: step 10059
I0513 00:01:51.651583 140260698863616 trainer.py:518] Training: step 10059
I0513 00:02:00.582943 140643590531072 trainer.py:518] Training: step 10063
I0513 00:02:00.765488 139876724979712 trainer.py:518] Training: step 10062
I0513 00:02:09.522398 140055069881920 logging_writer.py:48] [10050] collection=train accuracy=0.50737, cross_ent_loss=20.018444061279297, cross_ent_loss_per_all_target_tokens=2.54548e-05, experts/auxiliary_loss=0.14076876640319824, experts/expert_usage=7.89279317855835, experts/fraction_tokens_left_behind=2.269935369491577, experts/router_confidence=2.5841376781463623, experts/router_z_loss=0.0017599245766177773, learning_rate=0.00997782, learning_rate/current=0.00997559, loss=20.250873565673828, loss_per_all_target_tokens=2.57503e-05, loss_per_nonpadding_target_token=6.37447e-05, non_padding_fraction/loss_weights=0.40396, timing/seconds=89.38187408447266, timing/seqs=3840, timing/seqs_per_second=42.96173095703125, timing/seqs_per_second_per_core=1.3425540924072266, timing/steps_per_second=0.11187951266765594, timing/target_tokens_per_second=87985.625, timing/target_tokens_per_second_per_core=2749.55078125, timing/uptime=819.977, z_loss=0.0899006649851799, z_loss_per_all_target_tokens=1.14315e-07
I0513 00:02:09.525100 140193652815872 trainer.py:518] Training: step 10063
I0513 00:02:09.527936 140180528838656 trainer.py:518] Training: step 10063
I0513 00:02:09.555253 140143678494720 trainer.py:518] Training: step 10064
I0513 00:02:09.576568 140260698863616 trainer.py:518] Training: step 10065
I0513 00:02:09.660810 139856202532864 trainer.py:518] Training: step 10069
I0513 00:02:09.658270 139666521167872 trainer.py:518] Training: step 10064
I0513 00:02:18.461180 140643590531072 trainer.py:518] Training: step 10070
I0513 00:02:18.471397 139876724979712 trainer.py:518] Training: step 10070
I0513 00:02:45.276729 139856202532864 trainer.py:518] Training: step 10072
I0513 00:02:45.276254 140143678494720 trainer.py:518] Training: step 10072
I0513 00:02:45.280240 140193652815872 trainer.py:518] Training: step 10072
I0513 00:02:45.277299 140643590531072 trainer.py:518] Training: step 10072
I0513 00:02:45.278171 139876724979712 trainer.py:518] Training: step 10072
I0513 00:02:45.301831 140180528838656 trainer.py:518] Training: step 10072
I0513 00:02:45.282492 139666521167872 trainer.py:518] Training: step 10072
I0513 00:02:45.285484 140260698863616 trainer.py:518] Training: step 10072
I0513 00:03:03.153801 140143678494720 trainer.py:518] Training: step 10073
I0513 00:03:03.156078 140193652815872 trainer.py:518] Training: step 10073
I0513 00:03:03.159590 140180528838656 trainer.py:518] Training: step 10073
I0513 00:03:03.166507 139856202532864 trainer.py:518] Training: step 10073
I0513 00:03:03.155145 140260698863616 trainer.py:518] Training: step 10073
I0513 00:03:03.154346 139666521167872 trainer.py:518] Training: step 10073
I0513 00:03:03.158549 139876724979712 trainer.py:518] Training: step 10073
I0513 00:03:03.163550 140643590531072 trainer.py:518] Training: step 10073
I0513 00:03:21.033099 140180528838656 trainer.py:518] Training: step 10074
I0513 00:03:21.034188 139856202532864 trainer.py:518] Training: step 10074
I0513 00:03:21.036080 140193652815872 trainer.py:518] Training: step 10074
I0513 00:03:21.037155 140143678494720 trainer.py:518] Training: step 10074
I0513 00:03:21.031956 140643590531072 trainer.py:518] Training: step 10074
I0513 00:03:21.033442 139876724979712 trainer.py:518] Training: step 10074
I0513 00:03:21.034410 140260698863616 trainer.py:518] Training: step 10074
I0513 00:03:21.032174 139666521167872 trainer.py:518] Training: step 10074
I0513 00:03:38.910433 140193652815872 trainer.py:518] Training: step 10075
I0513 00:03:38.911528 140180528838656 trainer.py:518] Training: step 10075
I0513 00:03:38.910410 140143678494720 trainer.py:518] Training: step 10075
I0513 00:03:38.913267 139856202532864 trainer.py:518] Training: step 10075
I0513 00:03:38.909301 139876724979712 trainer.py:518] Training: step 10075
I0513 00:03:38.910555 140260698863616 trainer.py:518] Training: step 10075
I0513 00:03:38.909334 139666521167872 trainer.py:518] Training: step 10075
I0513 00:03:38.920524 140643590531072 trainer.py:518] Training: step 10075
I0513 00:03:56.787616 140193652815872 trainer.py:518] Training: step 10076
I0513 00:03:56.787981 140180528838656 trainer.py:518] Training: step 10076
I0513 00:03:56.786950 140143678494720 trainer.py:518] Training: step 10076
I0513 00:03:56.789317 139856202532864 trainer.py:518] Training: step 10076
I0513 00:03:56.787186 139876724979712 trainer.py:518] Training: step 10076
I0513 00:03:56.787492 140643590531072 trainer.py:518] Training: step 10076
I0513 00:03:56.791587 140260698863616 trainer.py:518] Training: step 10076
I0513 00:03:56.800579 139666521167872 trainer.py:518] Training: step 10076
I0513 00:04:14.666734 140180528838656 trainer.py:518] Training: step 10077
I0513 00:04:14.667089 140193652815872 trainer.py:518] Training: step 10077
I0513 00:04:14.665524 140143678494720 trainer.py:518] Training: step 10077
I0513 00:04:14.668904 139856202532864 trainer.py:518] Training: step 10077
I0513 00:04:14.667132 139876724979712 trainer.py:518] Training: step 10077
I0513 00:04:14.664754 139666521167872 trainer.py:518] Training: step 10077
I0513 00:04:14.667868 140260698863616 trainer.py:518] Training: step 10077
I0513 00:04:14.673902 140643590531072 trainer.py:518] Training: step 10077
I0513 00:04:32.543377 140193652815872 trainer.py:518] Training: step 10078
I0513 00:04:32.543960 139856202532864 trainer.py:518] Training: step 10078
I0513 00:04:32.544024 140180528838656 trainer.py:518] Training: step 10078
I0513 00:04:32.543301 140143678494720 trainer.py:518] Training: step 10078
I0513 00:04:32.542714 139876724979712 trainer.py:518] Training: step 10078
I0513 00:04:32.542870 140260698863616 trainer.py:518] Training: step 10078
I0513 00:04:32.543485 140643590531072 trainer.py:518] Training: step 10078
I0513 00:04:32.548563 139666521167872 trainer.py:518] Training: step 10078
I0513 00:04:41.483684 140055069881920 logging_writer.py:48] [10060] collection=train accuracy=0.507236, cross_ent_loss=20.094526290893555, cross_ent_loss_per_all_target_tokens=2.55515e-05, experts/auxiliary_loss=0.1412120908498764, experts/expert_usage=7.882047176361084, experts/fraction_tokens_left_behind=2.248525619506836, experts/router_confidence=2.5912747383117676, experts/router_z_loss=0.0017535350052639842, learning_rate=0.00997286, learning_rate/current=0.00997063, loss=20.326478958129883, loss_per_all_target_tokens=2.58465e-05, loss_per_nonpadding_target_token=6.31515e-05, non_padding_fraction/loss_weights=0.409277, timing/seconds=89.38963317871094, timing/seqs=3840, timing/seqs_per_second=42.95800018310547, timing/seqs_per_second_per_core=1.342437505722046, timing/steps_per_second=0.11186979711055756, timing/target_tokens_per_second=87977.984375, timing/target_tokens_per_second_per_core=2749.31201171875, timing/uptime=989.756, z_loss=0.08898600190877914, z_loss_per_all_target_tokens=1.13152e-07
I0513 00:04:50.421903 140193652815872 trainer.py:518] Training: step 10079
I0513 00:04:50.422074 139856202532864 trainer.py:518] Training: step 10079
I0513 00:04:50.422689 140143678494720 trainer.py:518] Training: step 10079
I0513 00:04:50.424153 140180528838656 trainer.py:518] Training: step 10079
I0513 00:04:50.421988 139876724979712 trainer.py:518] Training: step 10079
I0513 00:04:50.422073 140260698863616 trainer.py:518] Training: step 10079
I0513 00:04:50.423503 140643590531072 trainer.py:518] Training: step 10079
I0513 00:04:50.424157 139666521167872 trainer.py:518] Training: step 10079
I0513 00:05:08.300791 139856202532864 trainer.py:518] Training: step 10086
I0513 00:05:08.301335 140193652815872 trainer.py:518] Training: step 10085
I0513 00:05:08.300332 140143678494720 trainer.py:518] Training: step 10083
I0513 00:05:08.304076 140055069881920 logging_writer.py:48] [10070] collection=train accuracy=0.504162, cross_ent_loss=20.223312377929688, cross_ent_loss_per_all_target_tokens=2.57153e-05, experts/auxiliary_loss=0.1410091370344162, experts/expert_usage=7.867543697357178, experts/fraction_tokens_left_behind=2.269754648208618, experts/router_confidence=2.590550422668457, experts/router_z_loss=0.001760011538863182, learning_rate=0.00996791, learning_rate/current=0.00996568, loss=20.454315185546875, loss_per_all_target_tokens=2.6009e-05, loss_per_nonpadding_target_token=6.4453e-05, non_padding_fraction/loss_weights=0.403534, timing/seconds=89.37965393066406, timing/seqs=3840, timing/seqs_per_second=42.962799072265625, timing/seqs_per_second_per_core=1.3425874710083008, timing/steps_per_second=0.111882284283638, timing/target_tokens_per_second=87987.8125, timing/target_tokens_per_second_per_core=2749.619140625, timing/uptime=1007.92, z_loss=0.08823227137327194, z_loss_per_all_target_tokens=1.12193e-07
I0513 00:05:08.300154 140643590531072 trainer.py:518] Training: step 10086
I0513 00:05:08.300933 140260698863616 trainer.py:518] Training: step 10084
I0513 00:05:08.302485 139666521167872 trainer.py:518] Training: step 10083
I0513 00:05:08.340314 139876724979712 trainer.py:518] Training: step 10086
I0513 00:05:08.462618 140180528838656 trainer.py:518] Training: step 10088
I0513 00:05:35.134569 140643590531072 trainer.py:518] Training: step 10091
I0513 00:05:44.058734 140193652815872 trainer.py:518] Training: step 10092
I0513 00:05:44.058599 140143678494720 trainer.py:518] Training: step 10092
I0513 00:05:44.061589 140180528838656 trainer.py:518] Training: step 10092
I0513 00:05:44.065181 139856202532864 trainer.py:518] Training: step 10092
I0513 00:05:44.058470 139876724979712 trainer.py:518] Training: step 10092
I0513 00:05:44.059003 140260698863616 trainer.py:518] Training: step 10092
I0513 00:05:44.058977 139666521167872 trainer.py:518] Training: step 10092
I0513 00:05:52.998698 140643590531072 trainer.py:518] Training: step 10092
I0513 00:06:01.938277 140193652815872 trainer.py:518] Training: step 10093
I0513 00:06:01.938219 140180528838656 trainer.py:518] Training: step 10093
I0513 00:06:01.937201 140143678494720 trainer.py:518] Training: step 10093
I0513 00:06:01.940484 139856202532864 trainer.py:518] Training: step 10093
I0513 00:06:01.937427 139876724979712 trainer.py:518] Training: step 10093
I0513 00:06:01.937401 139666521167872 trainer.py:518] Training: step 10093
I0513 00:06:01.958634 140260698863616 trainer.py:518] Training: step 10093
I0513 00:06:10.876758 140643590531072 trainer.py:518] Training: step 10093
I0513 00:06:19.815704 140193652815872 trainer.py:518] Training: step 10094
I0513 00:06:19.815144 140143678494720 trainer.py:518] Training: step 10094
I0513 00:06:19.817365 139856202532864 trainer.py:518] Training: step 10094
I0513 00:06:19.815721 140260698863616 trainer.py:518] Training: step 10094
I0513 00:06:19.815984 139876724979712 trainer.py:518] Training: step 10094
I0513 00:06:19.838405 140180528838656 trainer.py:518] Training: step 10094
I0513 00:06:28.757090 140643590531072 trainer.py:518] Training: step 10094
I0513 00:06:28.772080 139666521167872 trainer.py:518] Training: step 10094
I0513 00:06:37.694187 139856202532864 trainer.py:518] Training: step 10095
I0513 00:06:37.693811 140143678494720 trainer.py:518] Training: step 10095
I0513 00:06:37.697257 140180528838656 trainer.py:518] Training: step 10095
I0513 00:06:37.698494 140193652815872 trainer.py:518] Training: step 10095
I0513 00:06:37.693872 139876724979712 trainer.py:518] Training: step 10095
I0513 00:06:37.698349 140260698863616 trainer.py:518] Training: step 10095
I0513 00:06:46.637529 140643590531072 trainer.py:518] Training: step 10095
I0513 00:06:46.644775 139666521167872 trainer.py:518] Training: step 10095
I0513 00:06:55.571453 140143678494720 trainer.py:518] Training: step 10096
I0513 00:06:55.573803 140180528838656 trainer.py:518] Training: step 10096
I0513 00:06:55.574822 140193652815872 trainer.py:518] Training: step 10096
I0513 00:06:55.576132 139856202532864 trainer.py:518] Training: step 10096
I0513 00:06:55.572275 140260698863616 trainer.py:518] Training: step 10096
I0513 00:06:55.572850 139876724979712 trainer.py:518] Training: step 10096
I0513 00:07:04.513094 140643590531072 trainer.py:518] Training: step 10096
I0513 00:07:04.522943 139666521167872 trainer.py:518] Training: step 10096
I0513 00:07:13.450616 139856202532864 trainer.py:518] Training: step 10097
I0513 00:07:13.451735 140180528838656 trainer.py:518] Training: step 10097
I0513 00:07:13.450549 140143678494720 trainer.py:518] Training: step 10097
I0513 00:07:13.452485 140193652815872 trainer.py:518] Training: step 10097
I0513 00:07:13.450101 140260698863616 trainer.py:518] Training: step 10097
I0513 00:07:13.450429 139876724979712 trainer.py:518] Training: step 10097
I0513 00:07:22.389749 140643590531072 trainer.py:518] Training: step 10097
I0513 00:07:22.390114 139666521167872 trainer.py:518] Training: step 10097
I0513 00:07:31.328897 139856202532864 trainer.py:518] Training: step 10098
I0513 00:07:31.328412 140143678494720 trainer.py:518] Training: step 10098
I0513 00:07:31.330734 140193652815872 trainer.py:518] Training: step 10098
I0513 00:07:31.328332 139876724979712 trainer.py:518] Training: step 10098
I0513 00:07:31.328971 140260698863616 trainer.py:518] Training: step 10098
I0513 00:07:31.351663 140180528838656 trainer.py:518] Training: step 10098
I0513 00:07:40.269103 140055069881920 logging_writer.py:48] [10080] collection=train accuracy=0.508226, cross_ent_loss=20.068357467651367, cross_ent_loss_per_all_target_tokens=2.55182e-05, experts/auxiliary_loss=0.14297285676002502, experts/expert_usage=7.851016998291016, experts/fraction_tokens_left_behind=2.1822469234466553, experts/router_confidence=2.6222147941589355, experts/router_z_loss=0.0017163141164928675, learning_rate=0.00996296, learning_rate/current=0.00996073, loss=20.301368713378906, loss_per_all_target_tokens=2.58145e-05, loss_per_nonpadding_target_token=6.15613e-05, non_padding_fraction/loss_weights=0.41933, timing/seconds=89.39041900634766, timing/seqs=3840, timing/seqs_per_second=42.95762634277344, timing/seqs_per_second_per_core=1.34242582321167, timing/steps_per_second=0.11186882108449936, timing/target_tokens_per_second=87977.21875, timing/target_tokens_per_second_per_core=2749.2880859375, timing/uptime=1168.55, z_loss=0.08832480758428574, z_loss_per_all_target_tokens=1.12311e-07
I0513 00:07:40.270666 140643590531072 trainer.py:518] Training: step 10098
I0513 00:07:40.270331 139666521167872 trainer.py:518] Training: step 10098
I0513 00:07:49.208232 140193652815872 trainer.py:518] Training: step 10099
I0513 00:07:49.209387 139856202532864 trainer.py:518] Training: step 10099
I0513 00:07:49.208744 140143678494720 trainer.py:518] Training: step 10099
I0513 00:07:49.210896 140180528838656 trainer.py:518] Training: step 10099
I0513 00:07:49.207707 139876724979712 trainer.py:518] Training: step 10099
I0513 00:07:49.208093 140260698863616 trainer.py:518] Training: step 10099
I0513 00:07:58.146652 139666521167872 trainer.py:518] Training: step 10102
I0513 00:07:58.228640 140643590531072 trainer.py:518] Training: step 10101
I0513 00:08:07.087252 140193652815872 trainer.py:518] Training: step 10105
I0513 00:08:07.086814 140143678494720 trainer.py:518] Training: step 10103
I0513 00:08:07.089043 140055069881920 logging_writer.py:48] [10090] collection=train accuracy=0.506834, cross_ent_loss=20.11130142211914, cross_ent_loss_per_all_target_tokens=2.55728e-05, experts/auxiliary_loss=0.14030371606349945, experts/expert_usage=7.884714603424072, experts/fraction_tokens_left_behind=2.307516574859619, experts/router_confidence=2.5822622776031494, experts/router_z_loss=0.0017789145931601524, learning_rate=0.00995802, learning_rate/current=0.00995579, loss=20.342153549194336, loss_per_all_target_tokens=2.58664e-05, loss_per_nonpadding_target_token=6.39934e-05, non_padding_fraction/loss_weights=0.404204, timing/seconds=89.38359069824219, timing/seqs=3840, timing/seqs_per_second=42.960906982421875, timing/seqs_per_second_per_core=1.3425283432006836, timing/steps_per_second=0.11187735944986343, timing/target_tokens_per_second=87983.9375, timing/target_tokens_per_second_per_core=2749.498046875, timing/uptime=1186.86, z_loss=0.08876698464155197, z_loss_per_all_target_tokens=1.12873e-07
I0513 00:08:07.088884 139876724979712 trainer.py:518] Training: step 10105
I0513 00:08:07.151057 140260698863616 trainer.py:518] Training: step 10104
I0513 00:08:07.193996 140180528838656 trainer.py:518] Training: step 10104
I0513 00:08:07.194855 139856202532864 trainer.py:518] Training: step 10104
I0513 00:08:16.025956 139666521167872 trainer.py:518] Training: step 10110
I0513 00:08:16.087800 140643590531072 trainer.py:518] Training: step 10110
I0513 00:08:33.927975 139856202532864 trainer.py:518] Training: step 10111
I0513 00:08:42.844356 140193652815872 trainer.py:518] Training: step 10112
I0513 00:08:42.843700 140143678494720 trainer.py:518] Training: step 10112
I0513 00:08:42.847677 140180528838656 trainer.py:518] Training: step 10112
I0513 00:08:42.844439 140260698863616 trainer.py:518] Training: step 10112
I0513 00:08:42.846061 139876724979712 trainer.py:518] Training: step 10112
I0513 00:08:42.846288 140643590531072 trainer.py:518] Training: step 10112
I0513 00:08:42.844367 139666521167872 trainer.py:518] Training: step 10112
I0513 00:08:51.783405 139856202532864 trainer.py:518] Training: step 10112
I0513 00:09:00.721646 140143678494720 trainer.py:518] Training: step 10113
I0513 00:09:00.727145 140193652815872 trainer.py:518] Training: step 10113
I0513 00:09:00.721764 139876724979712 trainer.py:518] Training: step 10113
I0513 00:09:00.721388 139666521167872 trainer.py:518] Training: step 10113
I0513 00:09:00.724456 140260698863616 trainer.py:518] Training: step 10113
I0513 00:09:00.747369 140180528838656 trainer.py:518] Training: step 10113
I0513 00:09:00.733237 140643590531072 trainer.py:518] Training: step 10113
I0513 00:09:09.660836 139856202532864 trainer.py:518] Training: step 10113
I0513 00:09:18.600578 140180528838656 trainer.py:518] Training: step 10114
I0513 00:09:18.601048 140193652815872 trainer.py:518] Training: step 10114
I0513 00:09:18.599899 140143678494720 trainer.py:518] Training: step 10114
I0513 00:09:18.598773 139876724979712 trainer.py:518] Training: step 10114
I0513 00:09:18.598824 140260698863616 trainer.py:518] Training: step 10114
I0513 00:09:18.601114 140643590531072 trainer.py:518] Training: step 10114
I0513 00:09:18.607224 139666521167872 trainer.py:518] Training: step 10114
I0513 00:09:27.538354 139856202532864 trainer.py:518] Training: step 10114
I0513 00:09:36.477776 140180528838656 trainer.py:518] Training: step 10115
I0513 00:09:36.478260 140193652815872 trainer.py:518] Training: step 10115
I0513 00:09:36.476946 140143678494720 trainer.py:518] Training: step 10115
I0513 00:09:36.477322 140643590531072 trainer.py:518] Training: step 10115
I0513 00:09:36.479142 139876724979712 trainer.py:518] Training: step 10115
I0513 00:09:36.482021 140260698863616 trainer.py:518] Training: step 10115
I0513 00:09:36.488722 139666521167872 trainer.py:518] Training: step 10115
I0513 00:09:45.419793 139856202532864 trainer.py:518] Training: step 10115
I0513 00:09:54.357080 140193652815872 trainer.py:518] Training: step 10116
I0513 00:09:54.359514 140143678494720 trainer.py:518] Training: step 10116
I0513 00:09:54.355931 140260698863616 trainer.py:518] Training: step 10116
I0513 00:09:54.356151 140643590531072 trainer.py:518] Training: step 10116
I0513 00:09:54.357131 139876724979712 trainer.py:518] Training: step 10116
I0513 00:09:54.355365 139666521167872 trainer.py:518] Training: step 10116
I0513 00:10:03.294763 139856202532864 trainer.py:518] Training: step 10116
I0513 00:10:03.301938 140180528838656 trainer.py:518] Training: step 10117
I0513 00:10:12.235296 140193652815872 trainer.py:518] Training: step 10117
I0513 00:10:12.235139 140143678494720 trainer.py:518] Training: step 10117
I0513 00:10:12.235470 139876724979712 trainer.py:518] Training: step 10117
I0513 00:10:12.235888 140260698863616 trainer.py:518] Training: step 10117
I0513 00:10:12.236707 140643590531072 trainer.py:518] Training: step 10117
I0513 00:10:12.235574 139666521167872 trainer.py:518] Training: step 10117
I0513 00:10:21.176904 139856202532864 trainer.py:518] Training: step 10117
I0513 00:10:21.179319 140180528838656 trainer.py:518] Training: step 10118
I0513 00:10:30.112968 140193652815872 trainer.py:518] Training: step 10118
I0513 00:10:30.112830 140143678494720 trainer.py:518] Training: step 10118
I0513 00:10:30.113412 139876724979712 trainer.py:518] Training: step 10118
I0513 00:10:30.113618 140260698863616 trainer.py:518] Training: step 10118
I0513 00:10:30.113416 140643590531072 trainer.py:518] Training: step 10118
I0513 00:10:30.115886 139666521167872 trainer.py:518] Training: step 10118
I0513 00:10:39.054023 140180528838656 trainer.py:518] Training: step 10119
I0513 00:10:39.053604 140055069881920 logging_writer.py:48] [10100] collection=train accuracy=0.503977, cross_ent_loss=20.367534637451172, cross_ent_loss_per_all_target_tokens=2.58987e-05, experts/auxiliary_loss=0.14187316596508026, experts/expert_usage=7.862060070037842, experts/fraction_tokens_left_behind=2.244885206222534, experts/router_confidence=2.6130435466766357, experts/router_z_loss=0.001757240854203701, learning_rate=0.00995308, learning_rate/current=0.00995086, loss=20.609642028808594, loss_per_all_target_tokens=2.62065e-05, loss_per_nonpadding_target_token=6.30864e-05, non_padding_fraction/loss_weights=0.415407, timing/seconds=89.38642120361328, timing/seqs=3840, timing/seqs_per_second=42.95954513549805, timing/seqs_per_second_per_core=1.342485785484314, timing/steps_per_second=0.1118738129734993, timing/target_tokens_per_second=87981.1484375, timing/target_tokens_per_second_per_core=2749.410888671875, timing/uptime=1347.32, z_loss=0.09847848117351532, z_loss_per_all_target_tokens=1.25222e-07
I0513 00:10:39.062276 139856202532864 trainer.py:518] Training: step 10118
I0513 00:10:47.992429 140193652815872 trainer.py:518] Training: step 10119
I0513 00:10:47.991723 140143678494720 trainer.py:518] Training: step 10119
I0513 00:10:47.990884 140643590531072 trainer.py:518] Training: step 10119
I0513 00:10:47.992575 140260698863616 trainer.py:518] Training: step 10119
I0513 00:10:47.990876 139666521167872 trainer.py:518] Training: step 10119
I0513 00:10:47.998402 139876724979712 trainer.py:518] Training: step 10119
I0513 00:10:56.930858 139856202532864 trainer.py:518] Training: step 10123
I0513 00:10:56.953353 140180528838656 trainer.py:518] Training: step 10120
I0513 00:10:57.993264 139666521167872 trainer.py:518] Training: step 10125
I0513 00:10:58.102104 140193652815872 trainer.py:518] Training: step 10124
I0513 00:11:05.869232 140143678494720 trainer.py:518] Training: step 10124
I0513 00:11:05.870471 140055069881920 logging_writer.py:48] [10110] collection=train accuracy=0.511817, cross_ent_loss=19.87344741821289, cross_ent_loss_per_all_target_tokens=2.52704e-05, experts/auxiliary_loss=0.1405356377363205, experts/expert_usage=7.878777503967285, experts/fraction_tokens_left_behind=2.2734012603759766, experts/router_confidence=2.5936803817749023, experts/router_z_loss=0.0017862586537376046, learning_rate=0.00994816, learning_rate/current=0.00994594, loss=20.117223739624023, loss_per_all_target_tokens=2.55804e-05, loss_per_nonpadding_target_token=6.3273e-05, non_padding_fraction/loss_weights=0.404286, timing/seconds=89.38350677490234, timing/seqs=3840, timing/seqs_per_second=42.96094512939453, timing/seqs_per_second_per_core=1.342529535293579, timing/steps_per_second=0.11187746375799179, timing/target_tokens_per_second=87984.015625, timing/target_tokens_per_second_per_core=2749.50048828125, timing/uptime=1365.59, z_loss=0.10145280510187149, z_loss_per_all_target_tokens=1.29004e-07
I0513 00:11:05.870860 139876724979712 trainer.py:518] Training: step 10123
I0513 00:11:05.873726 140643590531072 trainer.py:518] Training: step 10125
I0513 00:11:05.914351 140260698863616 trainer.py:518] Training: step 10124
I0513 00:11:14.808201 140193652815872 trainer.py:518] Training: step 10130
I0513 00:11:14.807871 139666521167872 trainer.py:518] Training: step 10130
I0513 00:11:14.836085 139856202532864 trainer.py:518] Training: step 10130
I0513 00:11:15.012031 140180528838656 trainer.py:518] Training: step 10130
I0513 00:11:41.625742 140193652815872 trainer.py:518] Training: step 10132
I0513 00:11:41.627642 139856202532864 trainer.py:518] Training: step 10132
I0513 00:11:41.629243 140143678494720 trainer.py:518] Training: step 10132
I0513 00:11:41.633773 140180528838656 trainer.py:518] Training: step 10132
I0513 00:11:41.625138 140260698863616 trainer.py:518] Training: step 10132
I0513 00:11:41.628818 139876724979712 trainer.py:518] Training: step 10132
I0513 00:11:41.626163 139666521167872 trainer.py:518] Training: step 10132
I0513 00:11:41.635953 140643590531072 trainer.py:518] Training: step 10132
I0513 00:11:59.503177 139856202532864 trainer.py:518] Training: step 10133
I0513 00:11:59.503933 140180528838656 trainer.py:518] Training: step 10133
I0513 00:11:59.505065 140193652815872 trainer.py:518] Training: step 10133
I0513 00:11:59.504071 140143678494720 trainer.py:518] Training: step 10133
I0513 00:11:59.504268 140260698863616 trainer.py:518] Training: step 10133
I0513 00:11:59.505687 139876724979712 trainer.py:518] Training: step 10133
I0513 00:11:59.506431 140643590531072 trainer.py:518] Training: step 10133
I0513 00:11:59.504983 139666521167872 trainer.py:518] Training: step 10133
I0513 00:12:17.381146 139856202532864 trainer.py:518] Training: step 10134
I0513 00:12:17.381778 140193652815872 trainer.py:518] Training: step 10134
I0513 00:12:17.380406 140143678494720 trainer.py:518] Training: step 10134
I0513 00:12:17.380757 140260698863616 trainer.py:518] Training: step 10134
I0513 00:12:17.398735 140180528838656 trainer.py:518] Training: step 10134
I0513 00:12:17.381754 139666521167872 trainer.py:518] Training: step 10134
I0513 00:12:17.387504 140643590531072 trainer.py:518] Training: step 10134
I0513 00:12:17.403680 139876724979712 trainer.py:518] Training: step 10134
I0513 00:12:35.259097 140193652815872 trainer.py:518] Training: step 10135
I0513 00:12:35.259430 139856202532864 trainer.py:518] Training: step 10135
I0513 00:12:35.265394 140143678494720 trainer.py:518] Training: step 10135
I0513 00:12:35.267643 140180528838656 trainer.py:518] Training: step 10135
I0513 00:12:35.259284 140643590531072 trainer.py:518] Training: step 10135
I0513 00:12:35.260821 140260698863616 trainer.py:518] Training: step 10135
I0513 00:12:35.261950 139876724979712 trainer.py:518] Training: step 10135
I0513 00:12:35.263139 139666521167872 trainer.py:518] Training: step 10135
I0513 00:12:53.137453 140193652815872 trainer.py:518] Training: step 10136
I0513 00:12:53.137601 139856202532864 trainer.py:518] Training: step 10136
I0513 00:12:53.137988 140143678494720 trainer.py:518] Training: step 10136
I0513 00:12:53.143985 140180528838656 trainer.py:518] Training: step 10136
I0513 00:12:53.139508 140260698863616 trainer.py:518] Training: step 10136
I0513 00:12:53.137531 139666521167872 trainer.py:518] Training: step 10136
I0513 00:12:53.143625 139876724979712 trainer.py:518] Training: step 10136
I0513 00:12:53.145033 140643590531072 trainer.py:518] Training: step 10136
I0513 00:13:11.015046 140193652815872 trainer.py:518] Training: step 10137
I0513 00:13:11.015616 139856202532864 trainer.py:518] Training: step 10137
I0513 00:13:11.015761 140143678494720 trainer.py:518] Training: step 10137
I0513 00:13:11.017896 140180528838656 trainer.py:518] Training: step 10137
I0513 00:13:11.015440 140260698863616 trainer.py:518] Training: step 10137
I0513 00:13:11.014915 139666521167872 trainer.py:518] Training: step 10137
I0513 00:13:11.018437 139876724979712 trainer.py:518] Training: step 10137
I0513 00:13:11.024070 140643590531072 trainer.py:518] Training: step 10137
I0513 00:13:28.896595 140193652815872 trainer.py:518] Training: step 10138
I0513 00:13:28.896038 140143678494720 trainer.py:518] Training: step 10138
I0513 00:13:28.897861 139856202532864 trainer.py:518] Training: step 10138
I0513 00:13:28.908710 140180528838656 trainer.py:518] Training: step 10138
I0513 00:13:28.894485 140643590531072 trainer.py:518] Training: step 10138
I0513 00:13:28.895900 140260698863616 trainer.py:518] Training: step 10138
I0513 00:13:28.896555 139876724979712 trainer.py:518] Training: step 10138
I0513 00:13:28.900032 139666521167872 trainer.py:518] Training: step 10138
I0513 00:13:37.836691 140055069881920 logging_writer.py:48] [10120] collection=train accuracy=0.515386, cross_ent_loss=19.7539119720459, cross_ent_loss_per_all_target_tokens=2.51184e-05, experts/auxiliary_loss=0.1401132494211197, experts/expert_usage=7.864865303039551, experts/fraction_tokens_left_behind=2.3171846866607666, experts/router_confidence=2.5790200233459473, experts/router_z_loss=0.0017945902654901147, learning_rate=0.00994324, learning_rate/current=0.00994103, loss=19.994630813598633, loss_per_all_target_tokens=2.54245e-05, loss_per_nonpadding_target_token=6.21518e-05, non_padding_fraction/loss_weights=0.409071, timing/seconds=89.38760375976562, timing/seqs=3840, timing/seqs_per_second=42.95897674560547, timing/seqs_per_second_per_core=1.342468023300171, timing/steps_per_second=0.11187233030796051, timing/target_tokens_per_second=87979.984375, timing/target_tokens_per_second_per_core=2749.37451171875, timing/uptime=1526.11, z_loss=0.09880970418453217, z_loss_per_all_target_tokens=1.25643e-07
I0513 00:13:46.771823 139856202532864 trainer.py:518] Training: step 10139
I0513 00:13:46.772320 140193652815872 trainer.py:518] Training: step 10139
I0513 00:13:46.785885 140180528838656 trainer.py:518] Training: step 10139
I0513 00:13:46.772463 140260698863616 trainer.py:518] Training: step 10139
I0513 00:13:46.772076 139666521167872 trainer.py:518] Training: step 10139
I0513 00:13:46.792703 140143678494720 trainer.py:518] Training: step 10139
I0513 00:13:46.778431 140643590531072 trainer.py:518] Training: step 10139
I0513 00:13:46.797325 139876724979712 trainer.py:518] Training: step 10139
I0513 00:14:04.649145 139856202532864 trainer.py:518] Training: step 10146
I0513 00:14:04.649952 140180528838656 trainer.py:518] Training: step 10143
I0513 00:14:04.650460 140143678494720 trainer.py:518] Training: step 10143
I0513 00:14:04.655395 140055069881920 logging_writer.py:48] [10130] collection=train accuracy=0.50638, cross_ent_loss=20.173559188842773, cross_ent_loss_per_all_target_tokens=2.5652e-05, experts/auxiliary_loss=0.1425924301147461, experts/expert_usage=7.831273078918457, experts/fraction_tokens_left_behind=2.2040507793426514, experts/router_confidence=2.631138324737549, experts/router_z_loss=0.001736988895572722, learning_rate=0.00993833, learning_rate/current=0.00993612, loss=20.41449546813965, loss_per_all_target_tokens=2.59584e-05, loss_per_nonpadding_target_token=6.17159e-05, non_padding_fraction/loss_weights=0.420611, timing/seconds=89.38375854492188, timing/seqs=3840, timing/seqs_per_second=42.96082305908203, timing/seqs_per_second_per_core=1.3425257205963135, timing/steps_per_second=0.11187714338302612, timing/target_tokens_per_second=87983.765625, timing/target_tokens_per_second_per_core=2749.49267578125, timing/uptime=1544.3, z_loss=0.09660940617322922, z_loss_per_all_target_tokens=1.22845e-07
I0513 00:14:04.659792 139876724979712 trainer.py:518] Training: step 10144
I0513 00:14:04.682807 140260698863616 trainer.py:518] Training: step 10147
I0513 00:14:04.695879 139666521167872 trainer.py:518] Training: step 10145
I0513 00:14:04.718731 140643590531072 trainer.py:518] Training: step 10146
I0513 00:14:04.788079 140193652815872 trainer.py:518] Training: step 10146
I0513 00:14:40.406007 140193652815872 trainer.py:518] Training: step 10152
I0513 00:14:40.406593 140180528838656 trainer.py:518] Training: step 10152
I0513 00:14:40.406926 139856202532864 trainer.py:518] Training: step 10152
I0513 00:14:40.407746 140143678494720 trainer.py:518] Training: step 10152
I0513 00:14:40.406732 139876724979712 trainer.py:518] Training: step 10152
I0513 00:14:40.405096 139666521167872 trainer.py:518] Training: step 10152
I0513 00:14:40.410212 140643590531072 trainer.py:518] Training: step 10152
I0513 00:14:40.415439 140260698863616 trainer.py:518] Training: step 10152
I0513 00:14:58.282751 139856202532864 trainer.py:518] Training: step 10153
I0513 00:14:58.283001 140193652815872 trainer.py:518] Training: step 10153
I0513 00:14:58.287515 140143678494720 trainer.py:518] Training: step 10153
I0513 00:14:58.290232 140180528838656 trainer.py:518] Training: step 10153
I0513 00:14:58.282922 140260698863616 trainer.py:518] Training: step 10153
I0513 00:14:58.282669 139666521167872 trainer.py:518] Training: step 10153
I0513 00:14:58.285900 140643590531072 trainer.py:518] Training: step 10153
I0513 00:14:58.297805 139876724979712 trainer.py:518] Training: step 10153
I0513 00:15:16.160485 139856202532864 trainer.py:518] Training: step 10154
I0513 00:15:16.160814 140193652815872 trainer.py:518] Training: step 10154
I0513 00:15:16.164316 140180528838656 trainer.py:518] Training: step 10154
I0513 00:15:16.160611 140260698863616 trainer.py:518] Training: step 10154
I0513 00:15:16.162188 139876724979712 trainer.py:518] Training: step 10154
I0513 00:15:16.160188 139666521167872 trainer.py:518] Training: step 10154
I0513 00:15:16.164179 140643590531072 trainer.py:518] Training: step 10154
I0513 00:15:25.099865 140143678494720 trainer.py:518] Training: step 10154
I0513 00:15:34.038988 140193652815872 trainer.py:518] Training: step 10155
I0513 00:15:34.039120 139856202532864 trainer.py:518] Training: step 10155
I0513 00:15:34.041304 140180528838656 trainer.py:518] Training: step 10155
I0513 00:15:34.039578 140260698863616 trainer.py:518] Training: step 10155
I0513 00:15:34.040266 139876724979712 trainer.py:518] Training: step 10155
I0513 00:15:34.041505 140643590531072 trainer.py:518] Training: step 10155
I0513 00:15:34.054516 139666521167872 trainer.py:518] Training: step 10155
I0513 00:15:42.981705 140143678494720 trainer.py:518] Training: step 10155
I0513 00:15:51.915657 140193652815872 trainer.py:518] Training: step 10156
I0513 00:15:51.915843 139856202532864 trainer.py:518] Training: step 10156
I0513 00:15:51.917912 140180528838656 trainer.py:518] Training: step 10156
I0513 00:15:51.916063 140260698863616 trainer.py:518] Training: step 10156
I0513 00:15:51.917474 139876724979712 trainer.py:518] Training: step 10156
I0513 00:15:51.916537 139666521167872 trainer.py:518] Training: step 10156
I0513 00:15:51.920206 140643590531072 trainer.py:518] Training: step 10156
I0513 00:16:00.857635 140143678494720 trainer.py:518] Training: step 10156
I0513 00:16:09.795931 139856202532864 trainer.py:518] Training: step 10157
I0513 00:16:09.796461 140193652815872 trainer.py:518] Training: step 10157
I0513 00:16:09.797556 140180528838656 trainer.py:518] Training: step 10157
I0513 00:16:09.794173 140260698863616 trainer.py:518] Training: step 10157
I0513 00:16:09.797241 140643590531072 trainer.py:518] Training: step 10157
I0513 00:16:09.798406 139876724979712 trainer.py:518] Training: step 10157
I0513 00:16:09.796376 139666521167872 trainer.py:518] Training: step 10157
I0513 00:16:27.672608 139856202532864 trainer.py:518] Training: step 10158
I0513 00:16:27.673714 140193652815872 trainer.py:518] Training: step 10158
I0513 00:16:27.672848 140143678494720 trainer.py:518] Training: step 10158
I0513 00:16:27.686835 140180528838656 trainer.py:518] Training: step 10158
I0513 00:16:27.671888 139666521167872 trainer.py:518] Training: step 10158
I0513 00:16:27.674606 140643590531072 trainer.py:518] Training: step 10158
I0513 00:16:27.675153 140260698863616 trainer.py:518] Training: step 10158
I0513 00:16:27.681799 139876724979712 trainer.py:518] Training: step 10158
I0513 00:16:36.612218 140055069881920 logging_writer.py:48] [10140] collection=train accuracy=0.520464, cross_ent_loss=19.512588500976562, cross_ent_loss_per_all_target_tokens=2.48115e-05, experts/auxiliary_loss=0.14100269973278046, experts/expert_usage=7.842936038970947, experts/fraction_tokens_left_behind=2.277928352355957, experts/router_confidence=2.6030335426330566, experts/router_z_loss=0.0017716985894367099, learning_rate=0.00993342, learning_rate/current=0.00993122, loss=19.75044822692871, loss_per_all_target_tokens=2.5114e-05, loss_per_nonpadding_target_token=6.2014e-05, non_padding_fraction/loss_weights=0.404973, timing/seconds=89.38462829589844, timing/seqs=3840, timing/seqs_per_second=42.96040344238281, timing/seqs_per_second_per_core=1.342512607574463, timing/steps_per_second=0.11187605559825897, timing/target_tokens_per_second=87982.90625, timing/target_tokens_per_second_per_core=2749.4658203125, timing/uptime=1705.04, z_loss=0.09508409351110458, z_loss_per_all_target_tokens=1.20906e-07
I0513 00:16:45.551687 140180528838656 trainer.py:518] Training: step 10159
I0513 00:16:45.552051 139856202532864 trainer.py:518] Training: step 10159
I0513 00:16:45.552032 140143678494720 trainer.py:518] Training: step 10159
I0513 00:16:45.555784 140193652815872 trainer.py:518] Training: step 10159
I0513 00:16:45.552663 140643590531072 trainer.py:518] Training: step 10159
I0513 00:16:45.552820 139876724979712 trainer.py:518] Training: step 10159
I0513 00:16:45.553262 140260698863616 trainer.py:518] Training: step 10159
I0513 00:16:45.550846 139666521167872 trainer.py:518] Training: step 10159
I0513 00:16:55.679564 140260698863616 trainer.py:518] Training: step 10166
I0513 00:16:55.733999 139876724979712 trainer.py:518] Training: step 10165
I0513 00:17:03.430058 140193652815872 trainer.py:518] Training: step 10166
I0513 00:17:03.429556 140055069881920 logging_writer.py:48] [10150] collection=train accuracy=0.51314, cross_ent_loss=19.845460891723633, cross_ent_loss_per_all_target_tokens=2.52348e-05, experts/auxiliary_loss=0.1410858929157257, experts/expert_usage=7.857025146484375, experts/fraction_tokens_left_behind=2.2881131172180176, experts/router_confidence=2.5944173336029053, experts/router_z_loss=0.0017624956090003252, learning_rate=0.00992852, learning_rate/current=0.00992632, loss=20.082494735717773, loss_per_all_target_tokens=2.55362e-05, loss_per_nonpadding_target_token=6.36332e-05, non_padding_fraction/loss_weights=0.401303, timing/seconds=89.37815856933594, timing/seqs=3840, timing/seqs_per_second=42.96351623535156, timing/seqs_per_second_per_core=1.3426098823547363, timing/steps_per_second=0.11188416182994843, timing/target_tokens_per_second=87989.28125, timing/target_tokens_per_second_per_core=2749.6650390625, timing/uptime=1723.36, z_loss=0.09418625384569168, z_loss_per_all_target_tokens=1.19764e-07
I0513 00:17:03.432543 140180528838656 trainer.py:518] Training: step 10166
I0513 00:17:03.434077 140643590531072 trainer.py:518] Training: step 10163
I0513 00:17:03.464111 139856202532864 trainer.py:518] Training: step 10168
I0513 00:17:03.545841 139666521167872 trainer.py:518] Training: step 10164
I0513 00:17:03.635772 140143678494720 trainer.py:518] Training: step 10169
I0513 00:17:12.371641 139876724979712 trainer.py:518] Training: step 10170
I0513 00:17:12.390222 140260698863616 trainer.py:518] Training: step 10170
I0513 00:17:30.262821 140180528838656 trainer.py:518] Training: step 10171
I0513 00:17:30.294081 140643590531072 trainer.py:518] Training: step 10171
I0513 00:17:39.186204 139856202532864 trainer.py:518] Training: step 10172
I0513 00:17:39.187761 140193652815872 trainer.py:518] Training: step 10172
I0513 00:17:39.193086 140143678494720 trainer.py:518] Training: step 10172
I0513 00:17:39.185898 140260698863616 trainer.py:518] Training: step 10172
I0513 00:17:39.193589 139666521167872 trainer.py:518] Training: step 10172
I0513 00:17:39.212185 139876724979712 trainer.py:518] Training: step 10172
I0513 00:17:48.133932 140180528838656 trainer.py:518] Training: step 10172
I0513 00:17:48.125702 140643590531072 trainer.py:518] Training: step 10172
I0513 00:17:57.063720 140193652815872 trainer.py:518] Training: step 10173
I0513 00:17:57.067440 140143678494720 trainer.py:518] Training: step 10173
I0513 00:17:57.071907 139856202532864 trainer.py:518] Training: step 10173
I0513 00:17:57.063948 140260698863616 trainer.py:518] Training: step 10173
I0513 00:17:57.067859 139666521167872 trainer.py:518] Training: step 10173
I0513 00:17:57.073535 139876724979712 trainer.py:518] Training: step 10173
I0513 00:18:06.002776 140180528838656 trainer.py:518] Training: step 10173
I0513 00:18:06.008003 140643590531072 trainer.py:518] Training: step 10173
I0513 00:18:14.945894 139856202532864 trainer.py:518] Training: step 10174
I0513 00:18:14.946209 140193652815872 trainer.py:518] Training: step 10174
I0513 00:18:14.948314 140143678494720 trainer.py:518] Training: step 10174
I0513 00:18:14.942983 139666521167872 trainer.py:518] Training: step 10174
I0513 00:18:14.950925 140260698863616 trainer.py:518] Training: step 10174
I0513 00:18:14.961761 139876724979712 trainer.py:518] Training: step 10174
I0513 00:18:23.882947 140180528838656 trainer.py:518] Training: step 10174
I0513 00:18:23.890808 140643590531072 trainer.py:518] Training: step 10174
I0513 00:18:32.822120 140193652815872 trainer.py:518] Training: step 10175
I0513 00:18:32.823377 139856202532864 trainer.py:518] Training: step 10175
I0513 00:18:32.825049 140143678494720 trainer.py:518] Training: step 10175
I0513 00:18:32.821286 140260698863616 trainer.py:518] Training: step 10175
I0513 00:18:32.825257 139666521167872 trainer.py:518] Training: step 10175
I0513 00:18:32.840973 139876724979712 trainer.py:518] Training: step 10175
I0513 00:18:41.763347 140180528838656 trainer.py:518] Training: step 10175
I0513 00:18:41.760847 140643590531072 trainer.py:518] Training: step 10175
I0513 00:18:50.700205 139856202532864 trainer.py:518] Training: step 10176
I0513 00:18:50.700621 140193652815872 trainer.py:518] Training: step 10176
I0513 00:18:50.700198 140143678494720 trainer.py:518] Training: step 10176
I0513 00:18:50.700066 140260698863616 trainer.py:518] Training: step 10176
I0513 00:18:50.699956 139876724979712 trainer.py:518] Training: step 10176
I0513 00:18:50.699809 139666521167872 trainer.py:518] Training: step 10176
I0513 00:18:59.641094 140643590531072 trainer.py:518] Training: step 10176
I0513 00:18:59.668548 140180528838656 trainer.py:518] Training: step 10176
I0513 00:19:08.576847 140193652815872 trainer.py:518] Training: step 10177
I0513 00:19:08.576920 139856202532864 trainer.py:518] Training: step 10177
I0513 00:19:08.578134 140143678494720 trainer.py:518] Training: step 10177
I0513 00:19:08.577691 140260698863616 trainer.py:518] Training: step 10177
I0513 00:19:08.576732 139666521167872 trainer.py:518] Training: step 10177
I0513 00:19:08.580086 139876724979712 trainer.py:518] Training: step 10177
I0513 00:19:17.517499 140180528838656 trainer.py:518] Training: step 10177
I0513 00:19:17.518977 140643590531072 trainer.py:518] Training: step 10177
I0513 00:19:26.455745 140193652815872 trainer.py:518] Training: step 10178
I0513 00:19:26.455646 139856202532864 trainer.py:518] Training: step 10178
I0513 00:19:26.456237 140143678494720 trainer.py:518] Training: step 10178
I0513 00:19:26.455024 140260698863616 trainer.py:518] Training: step 10178
I0513 00:19:26.455251 139666521167872 trainer.py:518] Training: step 10178
I0513 00:19:26.469578 139876724979712 trainer.py:518] Training: step 10178
I0513 00:19:35.395750 140180528838656 trainer.py:518] Training: step 10178
I0513 00:19:35.395192 140643590531072 trainer.py:518] Training: step 10178
I0513 00:19:44.334082 139856202532864 trainer.py:518] Training: step 10179
I0513 00:19:44.333798 140055069881920 logging_writer.py:48] [10160] collection=train accuracy=0.518861, cross_ent_loss=19.50176429748535, cross_ent_loss_per_all_target_tokens=2.47978e-05, experts/auxiliary_loss=0.1413845717906952, experts/expert_usage=7.873816967010498, experts/fraction_tokens_left_behind=2.238960027694702, experts/router_confidence=2.590939998626709, experts/router_z_loss=0.001742571941576898, learning_rate=0.00992363, learning_rate/current=0.00992144, loss=19.73889923095703, loss_per_all_target_tokens=2.50993e-05, loss_per_nonpadding_target_token=6.2586e-05, non_padding_fraction/loss_weights=0.401037, timing/seconds=89.39405059814453, timing/seqs=3840, timing/seqs_per_second=42.95587921142578, timing/seqs_per_second_per_core=1.3423712253570557, timing/steps_per_second=0.11186426877975464, timing/target_tokens_per_second=87973.640625, timing/target_tokens_per_second_per_core=2749.17626953125, timing/uptime=1883.73, z_loss=0.09400718659162521, z_loss_per_all_target_tokens=1.19536e-07
I0513 00:19:44.336779 140193652815872 trainer.py:518] Training: step 10179
I0513 00:19:44.335639 140143678494720 trainer.py:518] Training: step 10179
I0513 00:19:44.335027 140260698863616 trainer.py:518] Training: step 10179
I0513 00:19:44.333783 139666521167872 trainer.py:518] Training: step 10179
I0513 00:19:44.337575 139876724979712 trainer.py:518] Training: step 10179
I0513 00:19:53.355502 140643590531072 trainer.py:518] Training: step 10181
I0513 00:19:53.437645 140180528838656 trainer.py:518] Training: step 10183
I0513 00:20:02.210797 140193652815872 trainer.py:518] Training: step 10184
I0513 00:20:02.210800 140143678494720 trainer.py:518] Training: step 10183
I0513 00:20:02.215638 140055069881920 logging_writer.py:48] [10170] collection=train accuracy=0.507155, cross_ent_loss=20.16477394104004, cross_ent_loss_per_all_target_tokens=2.56408e-05, experts/auxiliary_loss=0.14129756391048431, experts/expert_usage=7.898897647857666, experts/fraction_tokens_left_behind=2.2340400218963623, experts/router_confidence=2.607114315032959, experts/router_z_loss=0.001752344542182982, learning_rate=0.00991875, learning_rate/current=0.00991656, loss=20.401098251342773, loss_per_all_target_tokens=2.59413e-05, loss_per_nonpadding_target_token=6.45934e-05, non_padding_fraction/loss_weights=0.40161, timing/seconds=89.37445068359375, timing/seqs=3840, timing/seqs_per_second=42.965301513671875, timing/seqs_per_second_per_core=1.342665672302246, timing/steps_per_second=0.11188880354166031, timing/target_tokens_per_second=87992.9375, timing/target_tokens_per_second_per_core=2749.779296875, timing/uptime=1901.83, z_loss=0.09327493607997894, z_loss_per_all_target_tokens=1.18605e-07
I0513 00:20:02.212032 140260698863616 trainer.py:518] Training: step 10186
I0513 00:20:02.210922 139666521167872 trainer.py:518] Training: step 10184
I0513 00:20:02.214068 139876724979712 trainer.py:518] Training: step 10183
I0513 00:20:02.244717 139856202532864 trainer.py:518] Training: step 10187
I0513 00:20:11.153107 140180528838656 trainer.py:518] Training: step 10190
I0513 00:20:11.178965 140643590531072 trainer.py:518] Training: step 10190
I0513 00:20:29.123116 140180528838656 trainer.py:518] Training: step 10191
I0513 00:20:37.966403 139856202532864 trainer.py:518] Training: step 10192
I0513 00:20:37.969837 140193652815872 trainer.py:518] Training: step 10192
I0513 00:20:37.968874 140143678494720 trainer.py:518] Training: step 10192
I0513 00:20:37.967371 140260698863616 trainer.py:518] Training: step 10192
I0513 00:20:37.969329 140643590531072 trainer.py:518] Training: step 10192
I0513 00:20:37.967469 139666521167872 trainer.py:518] Training: step 10192
I0513 00:20:37.972506 139876724979712 trainer.py:518] Training: step 10192
I0513 00:20:46.921253 140180528838656 trainer.py:518] Training: step 10192
I0513 00:20:55.844022 139856202532864 trainer.py:518] Training: step 10193
I0513 00:20:55.845783 140193652815872 trainer.py:518] Training: step 10193
I0513 00:20:55.845195 140143678494720 trainer.py:518] Training: step 10193
I0513 00:20:55.844640 140260698863616 trainer.py:518] Training: step 10193
I0513 00:20:55.845441 139876724979712 trainer.py:518] Training: step 10193
I0513 00:20:55.844675 139666521167872 trainer.py:518] Training: step 10193
I0513 00:20:55.849554 140643590531072 trainer.py:518] Training: step 10193
I0513 00:21:04.787030 140180528838656 trainer.py:518] Training: step 10193
I0513 00:21:13.722343 139856202532864 trainer.py:518] Training: step 10194
I0513 00:21:13.722672 140193652815872 trainer.py:518] Training: step 10194
I0513 00:21:13.724589 140143678494720 trainer.py:518] Training: step 10194
I0513 00:21:13.727724 139876724979712 trainer.py:518] Training: step 10194
I0513 00:21:13.732542 140643590531072 trainer.py:518] Training: step 10194
I0513 00:21:13.737140 139666521167872 trainer.py:518] Training: step 10194
I0513 00:21:22.661412 140180528838656 trainer.py:518] Training: step 10194
I0513 00:21:22.670330 140260698863616 trainer.py:518] Training: step 10194
I0513 00:21:31.600908 139856202532864 trainer.py:518] Training: step 10195
I0513 00:21:31.600929 140193652815872 trainer.py:518] Training: step 10195
I0513 00:21:31.609000 140143678494720 trainer.py:518] Training: step 10195
I0513 00:21:31.601502 139666521167872 trainer.py:518] Training: step 10195
I0513 00:21:31.609308 139876724979712 trainer.py:518] Training: step 10195
I0513 00:21:31.619287 140643590531072 trainer.py:518] Training: step 10195
I0513 00:21:40.549156 140180528838656 trainer.py:518] Training: step 10195
I0513 00:21:40.542402 140260698863616 trainer.py:518] Training: step 10195
I0513 00:21:49.479732 140193652815872 trainer.py:518] Training: step 10196
I0513 00:21:49.480126 139856202532864 trainer.py:518] Training: step 10196
I0513 00:21:49.479067 140143678494720 trainer.py:518] Training: step 10196
I0513 00:21:49.482634 140643590531072 trainer.py:518] Training: step 10196
I0513 00:21:49.486068 139876724979712 trainer.py:518] Training: step 10196
I0513 00:21:49.483839 139666521167872 trainer.py:518] Training: step 10196
I0513 00:21:58.418948 140180528838656 trainer.py:518] Training: step 10196
I0513 00:21:58.419606 140260698863616 trainer.py:518] Training: step 10196
I0513 00:22:07.357915 139856202532864 trainer.py:518] Training: step 10197
I0513 00:22:07.358025 140193652815872 trainer.py:518] Training: step 10197
I0513 00:22:07.358934 140143678494720 trainer.py:518] Training: step 10197
I0513 00:22:07.358384 139876724979712 trainer.py:518] Training: step 10197
I0513 00:22:07.358862 140643590531072 trainer.py:518] Training: step 10197
I0513 00:22:07.367685 139666521167872 trainer.py:518] Training: step 10197
I0513 00:22:16.302932 140180528838656 trainer.py:518] Training: step 10197
I0513 00:22:25.236719 139856202532864 trainer.py:518] Training: step 10198
I0513 00:22:25.236995 140193652815872 trainer.py:518] Training: step 10198
I0513 00:22:25.239820 140143678494720 trainer.py:518] Training: step 10198
I0513 00:22:25.236861 140260698863616 trainer.py:518] Training: step 10198
I0513 00:22:25.238564 139876724979712 trainer.py:518] Training: step 10198
I0513 00:22:25.239324 140643590531072 trainer.py:518] Training: step 10198
I0513 00:22:25.237989 139666521167872 trainer.py:518] Training: step 10198
I0513 00:22:34.179578 140180528838656 trainer.py:518] Training: step 10198
I0513 00:22:34.183638 140055069881920 logging_writer.py:48] [10180] collection=train accuracy=0.516751, cross_ent_loss=19.661657333374023, cross_ent_loss_per_all_target_tokens=2.50011e-05, experts/auxiliary_loss=0.14159300923347473, experts/expert_usage=7.884929656982422, experts/fraction_tokens_left_behind=2.2323317527770996, experts/router_confidence=2.6057701110839844, experts/router_z_loss=0.0017535152146592736, learning_rate=0.00991388, learning_rate/current=0.00991168, loss=19.896041870117188, loss_per_all_target_tokens=2.52991e-05, loss_per_nonpadding_target_token=6.10668e-05, non_padding_fraction/loss_weights=0.414286, timing/seconds=89.38825225830078, timing/seqs=3840, timing/seqs_per_second=42.95866394042969, timing/seqs_per_second_per_core=1.3424582481384277, timing/steps_per_second=0.11187151819467545, timing/target_tokens_per_second=87979.34375, timing/target_tokens_per_second_per_core=2749.3544921875, timing/uptime=2062.66, z_loss=0.0910394936800003, z_loss_per_all_target_tokens=1.15763e-07
I0513 00:22:43.116911 139856202532864 trainer.py:518] Training: step 10199
I0513 00:22:43.116586 140143678494720 trainer.py:518] Training: step 10199
I0513 00:22:43.116001 140260698863616 trainer.py:518] Training: step 10199
I0513 00:22:43.133376 140193652815872 trainer.py:518] Training: step 10199
I0513 00:22:43.117177 140643590531072 trainer.py:518] Training: step 10199
I0513 00:22:43.115168 139666521167872 trainer.py:518] Training: step 10199
I0513 00:22:43.121102 139876724979712 trainer.py:518] Training: step 10199
I0513 00:22:52.113593 140180528838656 trainer.py:518] Training: step 10202
I0513 00:22:53.393892 140260698863616 trainer.py:518] Training: step 10206
I0513 00:23:00.995384 140193652815872 trainer.py:518] Training: step 10205
I0513 00:23:00.995031 140143678494720 trainer.py:518] Training: step 10203
I0513 00:23:00.998617 140055069881920 logging_writer.py:48] [10190] collection=train accuracy=0.517922, cross_ent_loss=19.5726318359375, cross_ent_loss_per_all_target_tokens=2.48879e-05, experts/auxiliary_loss=0.14039376378059387, experts/expert_usage=7.883423805236816, experts/fraction_tokens_left_behind=2.298739433288574, experts/router_confidence=2.5866498947143555, experts/router_z_loss=0.0017812803853303194, learning_rate=0.00990901, learning_rate/current=0.00990682, loss=19.804141998291016, loss_per_all_target_tokens=2.51823e-05, loss_per_nonpadding_target_token=6.06606e-05, non_padding_fraction/loss_weights=0.415134, timing/seconds=89.3829345703125, timing/seqs=3840, timing/seqs_per_second=42.961219787597656, timing/seqs_per_second_per_core=1.3425381183624268, timing/steps_per_second=0.1118781790137291, timing/target_tokens_per_second=87984.578125, timing/target_tokens_per_second_per_core=2749.51806640625, timing/uptime=2080.81, z_loss=0.08933409303426743, z_loss_per_all_target_tokens=1.13594e-07
I0513 00:23:00.995440 139666521167872 trainer.py:518] Training: step 10205
I0513 00:23:01.039795 140643590531072 trainer.py:518] Training: step 10203
I0513 00:23:01.089539 139856202532864 trainer.py:518] Training: step 10207
I0513 00:23:01.206148 139876724979712 trainer.py:518] Training: step 10203
I0513 00:23:09.933519 140180528838656 trainer.py:518] Training: step 10210
I0513 00:23:09.951870 140260698863616 trainer.py:518] Training: step 10210
I0513 00:23:27.839775 139666521167872 trainer.py:518] Training: step 10211
I0513 00:23:36.751702 139856202532864 trainer.py:518] Training: step 10212
I0513 00:23:36.751882 140180528838656 trainer.py:518] Training: step 10212
I0513 00:23:36.752920 140193652815872 trainer.py:518] Training: step 10212
I0513 00:23:36.752743 140143678494720 trainer.py:518] Training: step 10212
I0513 00:23:36.753577 139876724979712 trainer.py:518] Training: step 10212
I0513 00:23:36.755840 140260698863616 trainer.py:518] Training: step 10212
I0513 00:23:36.763414 140643590531072 trainer.py:518] Training: step 10212
I0513 00:23:45.704664 139666521167872 trainer.py:518] Training: step 10212
I0513 00:23:54.631514 139856202532864 trainer.py:518] Training: step 10213
I0513 00:23:54.632570 140180528838656 trainer.py:518] Training: step 10213
I0513 00:23:54.633533 140193652815872 trainer.py:518] Training: step 10213
I0513 00:23:54.636359 140143678494720 trainer.py:518] Training: step 10213
I0513 00:23:54.633166 140260698863616 trainer.py:518] Training: step 10213
I0513 00:23:54.643164 140643590531072 trainer.py:518] Training: step 10213
I0513 00:23:54.647895 139876724979712 trainer.py:518] Training: step 10213
I0513 00:24:03.574624 139666521167872 trainer.py:518] Training: step 10213
I0513 00:24:12.508900 139856202532864 trainer.py:518] Training: step 10214
I0513 00:24:12.510012 140180528838656 trainer.py:518] Training: step 10214
I0513 00:24:12.509821 140143678494720 trainer.py:518] Training: step 10214
I0513 00:24:12.516529 140193652815872 trainer.py:518] Training: step 10214
I0513 00:24:12.509799 140643590531072 trainer.py:518] Training: step 10214
I0513 00:24:12.515161 139876724979712 trainer.py:518] Training: step 10214
I0513 00:24:12.519790 140260698863616 trainer.py:518] Training: step 10214
I0513 00:24:21.448647 139666521167872 trainer.py:518] Training: step 10214
I0513 00:24:30.387390 140180528838656 trainer.py:518] Training: step 10215
I0513 00:24:30.388657 140193652815872 trainer.py:518] Training: step 10215
I0513 00:24:30.387336 140143678494720 trainer.py:518] Training: step 10215
I0513 00:24:30.388977 139856202532864 trainer.py:518] Training: step 10215
I0513 00:24:30.387632 140260698863616 trainer.py:518] Training: step 10215
I0513 00:24:30.387671 140643590531072 trainer.py:518] Training: step 10215
I0513 00:24:30.390404 139876724979712 trainer.py:518] Training: step 10215
I0513 00:24:39.333212 139666521167872 trainer.py:518] Training: step 10215
I0513 00:24:48.266924 140193652815872 trainer.py:518] Training: step 10216
I0513 00:24:48.267299 140180528838656 trainer.py:518] Training: step 10216
I0513 00:24:48.268261 140143678494720 trainer.py:518] Training: step 10216
I0513 00:24:48.270671 139856202532864 trainer.py:518] Training: step 10216
I0513 00:24:48.267397 140260698863616 trainer.py:518] Training: step 10216
I0513 00:24:48.267471 140643590531072 trainer.py:518] Training: step 10216
I0513 00:24:48.282526 139876724979712 trainer.py:518] Training: step 10216
I0513 00:24:57.211760 139666521167872 trainer.py:518] Training: step 10216
I0513 00:25:06.143952 140180528838656 trainer.py:518] Training: step 10217
I0513 00:25:06.144154 140193652815872 trainer.py:518] Training: step 10217
I0513 00:25:06.145051 140143678494720 trainer.py:518] Training: step 10217
I0513 00:25:06.149650 139856202532864 trainer.py:518] Training: step 10217
I0513 00:25:06.144132 140643590531072 trainer.py:518] Training: step 10217
I0513 00:25:06.145046 140260698863616 trainer.py:518] Training: step 10217
I0513 00:25:06.165759 139876724979712 trainer.py:518] Training: step 10217
I0513 00:25:15.091735 139666521167872 trainer.py:518] Training: step 10217
I0513 00:25:24.022594 140193652815872 trainer.py:518] Training: step 10218
I0513 00:25:24.023121 140180528838656 trainer.py:518] Training: step 10218
I0513 00:25:24.024721 139856202532864 trainer.py:518] Training: step 10218
I0513 00:25:24.023278 140143678494720 trainer.py:518] Training: step 10218
I0513 00:25:24.025373 140643590531072 trainer.py:518] Training: step 10218
I0513 00:25:24.027358 139876724979712 trainer.py:518] Training: step 10218
I0513 00:25:24.030003 140260698863616 trainer.py:518] Training: step 10218
I0513 00:25:32.962842 140055069881920 logging_writer.py:48] [10200] collection=train accuracy=0.515318, cross_ent_loss=19.744159698486328, cross_ent_loss_per_all_target_tokens=2.5106e-05, experts/auxiliary_loss=0.14152537286281586, experts/expert_usage=7.862309455871582, experts/fraction_tokens_left_behind=2.256396770477295, experts/router_confidence=2.6029560565948486, experts/router_z_loss=0.0017542698187753558, learning_rate=0.00990415, learning_rate/current=0.00990196, loss=19.975906372070312, loss_per_all_target_tokens=2.54007e-05, loss_per_nonpadding_target_token=6.10618e-05, non_padding_fraction/loss_weights=0.415983, timing/seconds=89.38864135742188, timing/seqs=3840, timing/seqs_per_second=42.95847702026367, timing/seqs_per_second_per_core=1.3424524068832397, timing/steps_per_second=0.11187103390693665, timing/target_tokens_per_second=87978.9609375, timing/target_tokens_per_second_per_core=2749.342529296875, timing/uptime=2241.31, z_loss=0.08846800774335861, z_loss_per_all_target_tokens=1.12493e-07
I0513 00:25:32.963297 139666521167872 trainer.py:518] Training: step 10218
I0513 00:25:41.902024 140180528838656 trainer.py:518] Training: step 10219
I0513 00:25:41.902284 140193652815872 trainer.py:518] Training: step 10219
I0513 00:25:41.903020 139856202532864 trainer.py:518] Training: step 10219
I0513 00:25:41.904192 140143678494720 trainer.py:518] Training: step 10219
I0513 00:25:41.902471 139876724979712 trainer.py:518] Training: step 10219
I0513 00:25:41.902667 140643590531072 trainer.py:518] Training: step 10219
I0513 00:25:41.905938 140260698863616 trainer.py:518] Training: step 10219
I0513 00:25:50.842954 139666521167872 trainer.py:518] Training: step 10223
I0513 00:25:52.338585 140260698863616 trainer.py:518] Training: step 10226
I0513 00:25:59.779885 140143678494720 trainer.py:518] Training: step 10223
I0513 00:25:59.783764 140055069881920 logging_writer.py:48] [10210] collection=train accuracy=0.504834, cross_ent_loss=20.17970085144043, cross_ent_loss_per_all_target_tokens=2.56598e-05, experts/auxiliary_loss=0.14225225150585175, experts/expert_usage=7.861311435699463, experts/fraction_tokens_left_behind=2.203634262084961, experts/router_confidence=2.625190019607544, experts/router_z_loss=0.0017260952154174447, learning_rate=0.00989929, learning_rate/current=0.00989711, loss=20.412870407104492, loss_per_all_target_tokens=2.59563e-05, loss_per_nonpadding_target_token=6.29576e-05, non_padding_fraction/loss_weights=0.412282, timing/seconds=89.3806381225586, timing/seqs=3840, timing/seqs_per_second=42.96232223510742, timing/seqs_per_second_per_core=1.342572569847107, timing/steps_per_second=0.11188104748725891, timing/target_tokens_per_second=87986.8359375, timing/target_tokens_per_second_per_core=2749.588623046875, timing/uptime=2260.88, z_loss=0.08918864279985428, z_loss_per_all_target_tokens=1.13409e-07
I0513 00:25:59.788728 140193652815872 trainer.py:518] Training: step 10225
I0513 00:25:59.792213 139856202532864 trainer.py:518] Training: step 10224
I0513 00:25:59.779804 139876724979712 trainer.py:518] Training: step 10224
I0513 00:25:59.787720 140643590531072 trainer.py:518] Training: step 10223
I0513 00:25:59.817294 140180528838656 trainer.py:518] Training: step 10226
I0513 00:26:08.735369 140260698863616 trainer.py:518] Training: step 10230
I0513 00:26:08.752850 139666521167872 trainer.py:518] Training: step 10230
I0513 00:26:26.656113 139876724979712 trainer.py:518] Training: step 10231
I0513 00:26:35.538381 140193652815872 trainer.py:518] Training: step 10232
I0513 00:26:35.539760 140180528838656 trainer.py:518] Training: step 10232
I0513 00:26:35.538654 140143678494720 trainer.py:518] Training: step 10232
I0513 00:26:35.544509 139856202532864 trainer.py:518] Training: step 10232
I0513 00:26:35.539155 140643590531072 trainer.py:518] Training: step 10232
I0513 00:26:35.540044 140260698863616 trainer.py:518] Training: step 10232
I0513 00:26:35.539671 139666521167872 trainer.py:518] Training: step 10232
I0513 00:26:44.481951 139876724979712 trainer.py:518] Training: step 10232
I0513 00:26:53.415337 140193652815872 trainer.py:518] Training: step 10233
I0513 00:26:53.415502 140180528838656 trainer.py:518] Training: step 10233
I0513 00:26:53.414762 140143678494720 trainer.py:518] Training: step 10233
I0513 00:26:53.416719 139856202532864 trainer.py:518] Training: step 10233
I0513 00:26:53.415598 140643590531072 trainer.py:518] Training: step 10233
I0513 00:26:53.416877 140260698863616 trainer.py:518] Training: step 10233
I0513 00:26:53.414740 139666521167872 trainer.py:518] Training: step 10233
I0513 00:27:02.369007 139876724979712 trainer.py:518] Training: step 10233
I0513 00:27:11.293018 140193652815872 trainer.py:518] Training: step 10234
I0513 00:27:11.293215 140180528838656 trainer.py:518] Training: step 10234
I0513 00:27:11.292496 140143678494720 trainer.py:518] Training: step 10234
I0513 00:27:11.297605 139856202532864 trainer.py:518] Training: step 10234
I0513 00:27:11.293057 140643590531072 trainer.py:518] Training: step 10234
I0513 00:27:11.294149 140260698863616 trainer.py:518] Training: step 10234
I0513 00:27:11.292256 139666521167872 trainer.py:518] Training: step 10234
I0513 00:27:20.231882 139876724979712 trainer.py:518] Training: step 10234
I0513 00:27:29.172888 140180528838656 trainer.py:518] Training: step 10235
I0513 00:27:29.171441 140143678494720 trainer.py:518] Training: step 10235
I0513 00:27:29.172618 140193652815872 trainer.py:518] Training: step 10235
I0513 00:27:29.176091 139856202532864 trainer.py:518] Training: step 10235
I0513 00:27:29.171970 140643590531072 trainer.py:518] Training: step 10235
I0513 00:27:29.170670 139666521167872 trainer.py:518] Training: step 10235
I0513 00:27:29.188128 140260698863616 trainer.py:518] Training: step 10235
I0513 00:27:38.118468 139876724979712 trainer.py:518] Training: step 10235
I0513 00:27:47.049366 140193652815872 trainer.py:518] Training: step 10236
I0513 00:27:47.049617 140180528838656 trainer.py:518] Training: step 10236
I0513 00:27:47.049620 140143678494720 trainer.py:518] Training: step 10236
I0513 00:27:47.052731 139856202532864 trainer.py:518] Training: step 10236
I0513 00:27:47.050089 140643590531072 trainer.py:518] Training: step 10236
I0513 00:27:47.049725 139666521167872 trainer.py:518] Training: step 10236
I0513 00:27:47.052779 140260698863616 trainer.py:518] Training: step 10236
I0513 00:27:55.991002 139876724979712 trainer.py:518] Training: step 10236
I0513 00:28:04.927515 140193652815872 trainer.py:518] Training: step 10237
I0513 00:28:04.927932 140180528838656 trainer.py:518] Training: step 10237
I0513 00:28:04.927656 140143678494720 trainer.py:518] Training: step 10237
I0513 00:28:04.934131 139856202532864 trainer.py:518] Training: step 10237
I0513 00:28:04.928814 140260698863616 trainer.py:518] Training: step 10237
I0513 00:28:04.928723 140643590531072 trainer.py:518] Training: step 10237
I0513 00:28:04.926891 139666521167872 trainer.py:518] Training: step 10237
I0513 00:28:13.869204 139876724979712 trainer.py:518] Training: step 10237
I0513 00:28:22.804912 140193652815872 trainer.py:518] Training: step 10238
I0513 00:28:22.806137 140180528838656 trainer.py:518] Training: step 10238
I0513 00:28:22.806128 139856202532864 trainer.py:518] Training: step 10238
I0513 00:28:22.805162 140143678494720 trainer.py:518] Training: step 10238
I0513 00:28:22.806135 140643590531072 trainer.py:518] Training: step 10238
I0513 00:28:22.809946 140260698863616 trainer.py:518] Training: step 10238
I0513 00:28:22.820595 139666521167872 trainer.py:518] Training: step 10238
I0513 00:28:31.744989 140055069881920 logging_writer.py:48] [10220] collection=train accuracy=0.513186, cross_ent_loss=19.892080307006836, cross_ent_loss_per_all_target_tokens=2.52941e-05, experts/auxiliary_loss=0.140238955616951, experts/expert_usage=7.878848552703857, experts/fraction_tokens_left_behind=2.3079068660736084, experts/router_confidence=2.581523895263672, experts/router_z_loss=0.0017864014953374863, learning_rate=0.00989445, learning_rate/current=0.00989227, loss=20.123607635498047, loss_per_all_target_tokens=2.55885e-05, loss_per_nonpadding_target_token=6.18175e-05, non_padding_fraction/loss_weights=0.413936, timing/seconds=89.38267517089844, timing/seqs=3840, timing/seqs_per_second=42.96134567260742, timing/seqs_per_second_per_core=1.342542052268982, timing/steps_per_second=0.11187850683927536, timing/target_tokens_per_second=87984.8359375, timing/target_tokens_per_second_per_core=2749.526123046875, timing/uptime=2420.02, z_loss=0.08950164169073105, z_loss_per_all_target_tokens=1.13807e-07
I0513 00:28:31.745083 139876724979712 trainer.py:518] Training: step 10238
I0513 00:28:40.683508 140193652815872 trainer.py:518] Training: step 10239
I0513 00:28:40.684111 140180528838656 trainer.py:518] Training: step 10239
I0513 00:28:40.684028 140143678494720 trainer.py:518] Training: step 10239
I0513 00:28:40.686864 139856202532864 trainer.py:518] Training: step 10239
I0513 00:28:40.684010 140643590531072 trainer.py:518] Training: step 10239
I0513 00:28:40.682923 139666521167872 trainer.py:518] Training: step 10239
I0513 00:28:40.693911 140260698863616 trainer.py:518] Training: step 10239
I0513 00:28:49.658548 139876724979712 trainer.py:518] Training: step 10243
I0513 00:28:58.561959 140193652815872 trainer.py:518] Training: step 10246
I0513 00:28:58.564898 139856202532864 trainer.py:518] Training: step 10246
I0513 00:28:58.564068 140143678494720 trainer.py:518] Training: step 10244
I0513 00:28:58.565602 140055069881920 logging_writer.py:48] [10230] collection=train accuracy=0.519543, cross_ent_loss=19.563825607299805, cross_ent_loss_per_all_target_tokens=2.48767e-05, experts/auxiliary_loss=0.14005863666534424, experts/expert_usage=7.865760803222656, experts/fraction_tokens_left_behind=2.3285930156707764, experts/router_confidence=2.577840566635132, experts/router_z_loss=0.0017902235267683864, learning_rate=0.00988961, learning_rate/current=0.00988743, loss=19.791797637939453, loss_per_all_target_tokens=2.51666e-05, loss_per_nonpadding_target_token=6.22622e-05, non_padding_fraction/loss_weights=0.404203, timing/seconds=89.3849105834961, timing/seqs=3840, timing/seqs_per_second=42.96027374267578, timing/seqs_per_second_per_core=1.3425085544586182, timing/steps_per_second=0.11187571287155151, timing/target_tokens_per_second=87982.640625, timing/target_tokens_per_second_per_core=2749.45751953125, timing/uptime=2438.29, z_loss=0.08612474054098129, z_loss_per_all_target_tokens=1.09513e-07
I0513 00:28:58.563766 140643590531072 trainer.py:518] Training: step 10246
I0513 00:28:58.561338 139666521167872 trainer.py:518] Training: step 10245
I0513 00:28:58.659876 140260698863616 trainer.py:518] Training: step 10247
I0513 00:28:58.718085 140180528838656 trainer.py:518] Training: step 10244
I0513 00:29:07.500829 139876724979712 trainer.py:518] Training: step 10250
I0513 00:29:34.318649 140193652815872 trainer.py:518] Training: step 10252
I0513 00:29:34.318921 139856202532864 trainer.py:518] Training: step 10252
I0513 00:29:34.318655 140143678494720 trainer.py:518] Training: step 10252
I0513 00:29:34.324469 140180528838656 trainer.py:518] Training: step 10252
I0513 00:29:34.318652 140643590531072 trainer.py:518] Training: step 10252
I0513 00:29:34.318489 139666521167872 trainer.py:518] Training: step 10252
I0513 00:29:34.322181 140260698863616 trainer.py:518] Training: step 10252
I0513 00:29:34.322176 139876724979712 trainer.py:518] Training: step 10252
I0513 00:29:52.195079 140180528838656 trainer.py:518] Training: step 10253
I0513 00:29:52.195439 140193652815872 trainer.py:518] Training: step 10253
I0513 00:29:52.196009 140143678494720 trainer.py:518] Training: step 10253
I0513 00:29:52.205455 139856202532864 trainer.py:518] Training: step 10253
I0513 00:29:52.196614 140260698863616 trainer.py:518] Training: step 10253
I0513 00:29:52.198179 139876724979712 trainer.py:518] Training: step 10253
I0513 00:29:52.195885 139666521167872 trainer.py:518] Training: step 10253
I0513 00:29:52.199730 140643590531072 trainer.py:518] Training: step 10253
I0513 00:30:10.072461 140193652815872 trainer.py:518] Training: step 10254
I0513 00:30:10.072706 140180528838656 trainer.py:518] Training: step 10254
I0513 00:30:10.072467 140143678494720 trainer.py:518] Training: step 10254
I0513 00:30:10.085505 139856202532864 trainer.py:518] Training: step 10254
I0513 00:30:10.072539 139876724979712 trainer.py:518] Training: step 10254
I0513 00:30:10.073969 140260698863616 trainer.py:518] Training: step 10254
I0513 00:30:10.075874 140643590531072 trainer.py:518] Training: step 10254
I0513 00:30:10.102630 139666521167872 trainer.py:518] Training: step 10254
I0513 00:30:27.950390 140193652815872 trainer.py:518] Training: step 10255
I0513 00:30:27.951475 140180528838656 trainer.py:518] Training: step 10255
I0513 00:30:27.950709 140143678494720 trainer.py:518] Training: step 10255
I0513 00:30:27.960850 139856202532864 trainer.py:518] Training: step 10255
I0513 00:30:27.950793 139876724979712 trainer.py:518] Training: step 10255
I0513 00:30:27.951289 139666521167872 trainer.py:518] Training: step 10255
I0513 00:30:27.955649 140260698863616 trainer.py:518] Training: step 10255
I0513 00:30:36.893920 140643590531072 trainer.py:518] Training: step 10255
I0513 00:30:45.828128 140193652815872 trainer.py:518] Training: step 10256
I0513 00:30:45.829138 140180528838656 trainer.py:518] Training: step 10256
I0513 00:30:45.828292 140143678494720 trainer.py:518] Training: step 10256
I0513 00:30:45.832177 139856202532864 trainer.py:518] Training: step 10256
I0513 00:30:45.830653 140260698863616 trainer.py:518] Training: step 10256
I0513 00:30:45.847805 139876724979712 trainer.py:518] Training: step 10256
I0513 00:30:45.829592 139666521167872 trainer.py:518] Training: step 10256
I0513 00:30:54.769156 140643590531072 trainer.py:518] Training: step 10256
I0513 00:31:03.706454 140193652815872 trainer.py:518] Training: step 10257
I0513 00:31:03.706965 140180528838656 trainer.py:518] Training: step 10257
I0513 00:31:03.707747 140143678494720 trainer.py:518] Training: step 10257
I0513 00:31:03.709715 139876724979712 trainer.py:518] Training: step 10257
I0513 00:31:03.710664 139856202532864 trainer.py:518] Training: step 10257
I0513 00:31:03.707844 139666521167872 trainer.py:518] Training: step 10257
I0513 00:31:03.715756 140260698863616 trainer.py:518] Training: step 10257
I0513 00:31:21.585253 140193652815872 trainer.py:518] Training: step 10258
I0513 00:31:21.585465 140180528838656 trainer.py:518] Training: step 10258
I0513 00:31:21.585737 139876724979712 trainer.py:518] Training: step 10258
I0513 00:31:21.585776 140143678494720 trainer.py:518] Training: step 10258
I0513 00:31:21.588218 139856202532864 trainer.py:518] Training: step 10258
I0513 00:31:21.586907 140260698863616 trainer.py:518] Training: step 10258
I0513 00:31:21.587629 140643590531072 trainer.py:518] Training: step 10258
I0513 00:31:21.586386 139666521167872 trainer.py:518] Training: step 10258
I0513 00:31:30.524671 140055069881920 logging_writer.py:48] [10240] collection=train accuracy=0.522742, cross_ent_loss=19.3837890625, cross_ent_loss_per_all_target_tokens=2.46478e-05, experts/auxiliary_loss=0.139861062169075, experts/expert_usage=7.874492645263672, experts/fraction_tokens_left_behind=2.3271472454071045, experts/router_confidence=2.5757534503936768, experts/router_z_loss=0.0017923472914844751, learning_rate=0.00988477, learning_rate/current=0.0098826, loss=19.612899780273438, loss_per_all_target_tokens=2.49391e-05, loss_per_nonpadding_target_token=6.18356e-05, non_padding_fraction/loss_weights=0.403313, timing/seconds=89.3860092163086, timing/seqs=3840, timing/seqs_per_second=42.95974349975586, timing/seqs_per_second_per_core=1.3424919843673706, timing/steps_per_second=0.11187433451414108, timing/target_tokens_per_second=87981.5546875, timing/target_tokens_per_second_per_core=2749.423583984375, timing/uptime=2598.9, z_loss=0.08745759725570679, z_loss_per_all_target_tokens=1.11208e-07
I0513 00:31:39.464520 140193652815872 trainer.py:518] Training: step 10259
I0513 00:31:39.464995 139856202532864 trainer.py:518] Training: step 10259
I0513 00:31:39.464399 140143678494720 trainer.py:518] Training: step 10259
I0513 00:31:39.465841 140180528838656 trainer.py:518] Training: step 10259
I0513 00:31:39.467617 139876724979712 trainer.py:518] Training: step 10259
I0513 00:31:39.467008 140260698863616 trainer.py:518] Training: step 10259
I0513 00:31:39.464777 139666521167872 trainer.py:518] Training: step 10259
I0513 00:31:48.404017 140643590531072 trainer.py:518] Training: step 10260
I0513 00:31:57.342084 139876724979712 trainer.py:518] Training: step 10264
I0513 00:31:57.343270 139856202532864 trainer.py:518] Training: step 10263
I0513 00:31:57.343076 140055069881920 logging_writer.py:48] [10250] collection=train accuracy=0.509349, cross_ent_loss=19.971826553344727, cross_ent_loss_per_all_target_tokens=2.53955e-05, experts/auxiliary_loss=0.14095692336559296, experts/expert_usage=7.873069763183594, experts/fraction_tokens_left_behind=2.264239549636841, experts/router_confidence=2.599396228790283, experts/router_z_loss=0.0017671635141596198, learning_rate=0.00987995, learning_rate/current=0.00987778, loss=20.20140266418457, loss_per_all_target_tokens=2.56874e-05, loss_per_nonpadding_target_token=6.29695e-05, non_padding_fraction/loss_weights=0.407934, timing/seconds=89.3818588256836, timing/seqs=3840, timing/seqs_per_second=42.96173858642578, timing/seqs_per_second_per_core=1.3425543308258057, timing/steps_per_second=0.11187952756881714, timing/target_tokens_per_second=87985.640625, timing/target_tokens_per_second_per_core=2749.55126953125, timing/uptime=2617.55, z_loss=0.08685079962015152, z_loss_per_all_target_tokens=1.10437e-07
I0513 00:31:57.342700 139666521167872 trainer.py:518] Training: step 10266
I0513 00:31:57.352481 140260698863616 trainer.py:518] Training: step 10266
I0513 00:31:57.372173 140193652815872 trainer.py:518] Training: step 10265
I0513 00:31:57.373077 140180528838656 trainer.py:518] Training: step 10266
I0513 00:31:57.374334 140143678494720 trainer.py:518] Training: step 10264
I0513 00:32:06.282050 140643590531072 trainer.py:518] Training: step 10270
I0513 00:32:33.099406 140193652815872 trainer.py:518] Training: step 10272
I0513 00:32:33.099829 140143678494720 trainer.py:518] Training: step 10272
I0513 00:32:33.104202 139856202532864 trainer.py:518] Training: step 10272
I0513 00:32:33.104680 139876724979712 trainer.py:518] Training: step 10272
I0513 00:32:33.111579 140180528838656 trainer.py:518] Training: step 10272
I0513 00:32:33.102306 140260698863616 trainer.py:518] Training: step 10272
I0513 00:32:33.102046 140643590531072 trainer.py:518] Training: step 10272
I0513 00:32:33.100222 139666521167872 trainer.py:518] Training: step 10272
I0513 00:32:50.975722 139856202532864 trainer.py:518] Training: step 10273
I0513 00:32:50.976482 140193652815872 trainer.py:518] Training: step 10273
I0513 00:32:50.977829 140143678494720 trainer.py:518] Training: step 10273
I0513 00:32:50.982693 139876724979712 trainer.py:518] Training: step 10273
I0513 00:32:50.983180 140180528838656 trainer.py:518] Training: step 10273
I0513 00:32:50.976815 139666521167872 trainer.py:518] Training: step 10273
I0513 00:32:50.979710 140643590531072 trainer.py:518] Training: step 10273
I0513 00:32:50.994638 140260698863616 trainer.py:518] Training: step 10273
I0513 00:33:08.855644 140193652815872 trainer.py:518] Training: step 10274
I0513 00:33:08.855082 140143678494720 trainer.py:518] Training: step 10274
I0513 00:33:08.856860 140180528838656 trainer.py:518] Training: step 10274
I0513 00:33:08.867219 139856202532864 trainer.py:518] Training: step 10274
I0513 00:33:08.855850 140643590531072 trainer.py:518] Training: step 10274
I0513 00:33:08.873823 139876724979712 trainer.py:518] Training: step 10274
I0513 00:33:08.855791 139666521167872 trainer.py:518] Training: step 10274
I0513 00:33:08.861278 140260698863616 trainer.py:518] Training: step 10274
I0513 00:33:26.734486 140180528838656 trainer.py:518] Training: step 10275
I0513 00:33:26.734794 140143678494720 trainer.py:518] Training: step 10275
I0513 00:33:26.738312 139856202532864 trainer.py:518] Training: step 10275
I0513 00:33:26.738793 140193652815872 trainer.py:518] Training: step 10275
I0513 00:33:26.739427 139876724979712 trainer.py:518] Training: step 10275
I0513 00:33:26.735176 140643590531072 trainer.py:518] Training: step 10275
I0513 00:33:26.737752 140260698863616 trainer.py:518] Training: step 10275
I0513 00:33:26.735352 139666521167872 trainer.py:518] Training: step 10275
I0513 00:33:44.612985 140193652815872 trainer.py:518] Training: step 10276
I0513 00:33:44.613544 139876724979712 trainer.py:518] Training: step 10276
I0513 00:33:44.614852 140180528838656 trainer.py:518] Training: step 10276
I0513 00:33:44.613403 140143678494720 trainer.py:518] Training: step 10276
I0513 00:33:44.615478 139856202532864 trainer.py:518] Training: step 10276
I0513 00:33:44.613156 140643590531072 trainer.py:518] Training: step 10276
I0513 00:33:44.613715 139666521167872 trainer.py:518] Training: step 10276
I0513 00:33:44.620455 140260698863616 trainer.py:518] Training: step 10276
I0513 00:34:02.491178 140193652815872 trainer.py:518] Training: step 10277
I0513 00:34:02.490919 139876724979712 trainer.py:518] Training: step 10277
I0513 00:34:02.491404 139856202532864 trainer.py:518] Training: step 10277
I0513 00:34:02.491283 140180528838656 trainer.py:518] Training: step 10277
I0513 00:34:02.491066 140643590531072 trainer.py:518] Training: step 10277
I0513 00:34:02.491612 139666521167872 trainer.py:518] Training: step 10277
I0513 00:34:02.504304 140260698863616 trainer.py:518] Training: step 10277
I0513 00:34:11.431426 140143678494720 trainer.py:518] Training: step 10278
I0513 00:34:20.369176 139876724979712 trainer.py:518] Training: step 10278
I0513 00:34:20.369977 139856202532864 trainer.py:518] Training: step 10278
I0513 00:34:20.371525 140193652815872 trainer.py:518] Training: step 10278
I0513 00:34:20.373299 140180528838656 trainer.py:518] Training: step 10278
I0513 00:34:20.369408 140643590531072 trainer.py:518] Training: step 10278
I0513 00:34:20.370290 140260698863616 trainer.py:518] Training: step 10278
I0513 00:34:20.369653 139666521167872 trainer.py:518] Training: step 10278
I0513 00:34:29.309562 140143678494720 trainer.py:518] Training: step 10279
I0513 00:34:29.312797 140055069881920 logging_writer.py:48] [10260] collection=train accuracy=0.507006, cross_ent_loss=20.081722259521484, cross_ent_loss_per_all_target_tokens=2.55352e-05, experts/auxiliary_loss=0.1413748562335968, experts/expert_usage=7.86315393447876, experts/fraction_tokens_left_behind=2.2616240978240967, experts/router_confidence=2.598207712173462, experts/router_z_loss=0.0017578030237928033, learning_rate=0.00987513, learning_rate/current=0.00987296, loss=20.31460189819336, loss_per_all_target_tokens=2.58314e-05, loss_per_nonpadding_target_token=6.31316e-05, non_padding_fraction/loss_weights=0.409167, timing/seconds=89.39469146728516, timing/seqs=3840, timing/seqs_per_second=42.955570220947266, timing/seqs_per_second_per_core=1.342361569404602, timing/steps_per_second=0.11186346411705017, timing/target_tokens_per_second=87973.0078125, timing/target_tokens_per_second_per_core=2749.156494140625, timing/uptime=2777.6, z_loss=0.08974535763263702, z_loss_per_all_target_tokens=1.14117e-07
I0513 00:34:38.248699 139876724979712 trainer.py:518] Training: step 10279
I0513 00:34:38.249197 140193652815872 trainer.py:518] Training: step 10279
I0513 00:34:38.249492 139856202532864 trainer.py:518] Training: step 10279
I0513 00:34:38.249886 140180528838656 trainer.py:518] Training: step 10279
I0513 00:34:38.249360 140643590531072 trainer.py:518] Training: step 10279
I0513 00:34:38.249027 139666521167872 trainer.py:518] Training: step 10279
I0513 00:34:38.252601 140260698863616 trainer.py:518] Training: step 10279
I0513 00:34:56.126460 140193652815872 trainer.py:518] Training: step 10284
I0513 00:34:56.126383 140180528838656 trainer.py:518] Training: step 10284
I0513 00:34:56.128290 139856202532864 trainer.py:518] Training: step 10286
I0513 00:34:56.127876 140143678494720 trainer.py:518] Training: step 10280
I0513 00:34:56.134326 140055069881920 logging_writer.py:48] [10270] collection=train accuracy=0.521422, cross_ent_loss=19.403244018554688, cross_ent_loss_per_all_target_tokens=2.46725e-05, experts/auxiliary_loss=0.14027050137519836, experts/expert_usage=7.870652198791504, experts/fraction_tokens_left_behind=2.3367936611175537, experts/router_confidence=2.582428216934204, experts/router_z_loss=0.0017936978256329894, learning_rate=0.00987032, learning_rate/current=0.00986815, loss=19.635847091674805, loss_per_all_target_tokens=2.49683e-05, loss_per_nonpadding_target_token=5.9413e-05, non_padding_fraction/loss_weights=0.420249, timing/seconds=89.37702178955078, timing/seqs=3840, timing/seqs_per_second=42.96406555175781, timing/seqs_per_second_per_core=1.3426270484924316, timing/steps_per_second=0.11188558489084244, timing/target_tokens_per_second=87990.40625, timing/target_tokens_per_second_per_core=2749.7001953125, timing/uptime=2795.72, z_loss=0.09053866565227509, z_loss_per_all_target_tokens=1.15126e-07
I0513 00:34:56.127056 140643590531072 trainer.py:518] Training: step 10286
I0513 00:34:56.126873 139666521167872 trainer.py:518] Training: step 10286
I0513 00:34:56.155840 139876724979712 trainer.py:518] Training: step 10284
I0513 00:34:56.332139 140260698863616 trainer.py:518] Training: step 10287
I0513 00:35:22.985179 140193652815872 trainer.py:518] Training: step 10291
I0513 00:35:31.883575 140143678494720 trainer.py:518] Training: step 10292
I0513 00:35:31.888361 140180528838656 trainer.py:518] Training: step 10292
I0513 00:35:31.889198 139856202532864 trainer.py:518] Training: step 10292
I0513 00:35:31.892414 139876724979712 trainer.py:518] Training: step 10292
I0513 00:35:31.884757 140643590531072 trainer.py:518] Training: step 10292
I0513 00:35:31.886327 140260698863616 trainer.py:518] Training: step 10292
I0513 00:35:31.884611 139666521167872 trainer.py:518] Training: step 10292
I0513 00:35:40.823077 140193652815872 trainer.py:518] Training: step 10292
I0513 00:35:49.761485 140143678494720 trainer.py:518] Training: step 10293
I0513 00:35:49.762845 140180528838656 trainer.py:518] Training: step 10293
I0513 00:35:49.764140 139856202532864 trainer.py:518] Training: step 10293
I0513 00:35:49.763975 139876724979712 trainer.py:518] Training: step 10293
I0513 00:35:49.762145 140643590531072 trainer.py:518] Training: step 10293
I0513 00:35:49.762813 140260698863616 trainer.py:518] Training: step 10293
I0513 00:35:49.762822 139666521167872 trainer.py:518] Training: step 10293
I0513 00:35:58.701523 140193652815872 trainer.py:518] Training: step 10293
I0513 00:36:07.641319 139856202532864 trainer.py:518] Training: step 10294
I0513 00:36:07.641660 140180528838656 trainer.py:518] Training: step 10294
I0513 00:36:07.640390 140143678494720 trainer.py:518] Training: step 10294
I0513 00:36:07.646574 139876724979712 trainer.py:518] Training: step 10294
I0513 00:36:07.640931 140643590531072 trainer.py:518] Training: step 10294
I0513 00:36:07.641473 140260698863616 trainer.py:518] Training: step 10294
I0513 00:36:07.641961 139666521167872 trainer.py:518] Training: step 10294
I0513 00:36:16.580515 140193652815872 trainer.py:518] Training: step 10294
I0513 00:36:25.520025 140143678494720 trainer.py:518] Training: step 10295
I0513 00:36:25.521830 139856202532864 trainer.py:518] Training: step 10295
I0513 00:36:25.526016 139876724979712 trainer.py:518] Training: step 10295
I0513 00:36:25.520769 140260698863616 trainer.py:518] Training: step 10295
I0513 00:36:25.520843 140643590531072 trainer.py:518] Training: step 10295
I0513 00:36:25.539373 140180528838656 trainer.py:518] Training: step 10295
I0513 00:36:25.521144 139666521167872 trainer.py:518] Training: step 10295
I0513 00:36:34.465099 140193652815872 trainer.py:518] Training: step 10295
I0513 00:36:43.398304 139876724979712 trainer.py:518] Training: step 10296
I0513 00:36:43.397591 140143678494720 trainer.py:518] Training: step 10296
I0513 00:36:43.403837 139856202532864 trainer.py:518] Training: step 10296
I0513 00:36:43.405015 140180528838656 trainer.py:518] Training: step 10296
I0513 00:36:43.399153 140643590531072 trainer.py:518] Training: step 10296
I0513 00:36:43.401279 140260698863616 trainer.py:518] Training: step 10296
I0513 00:36:43.398817 139666521167872 trainer.py:518] Training: step 10296
I0513 00:36:52.337622 140193652815872 trainer.py:518] Training: step 10296
I0513 00:37:01.276528 139876724979712 trainer.py:518] Training: step 10297
I0513 00:37:01.277272 140180528838656 trainer.py:518] Training: step 10297
I0513 00:37:01.277197 140143678494720 trainer.py:518] Training: step 10297
I0513 00:37:01.277761 140643590531072 trainer.py:518] Training: step 10297
I0513 00:37:01.278881 140260698863616 trainer.py:518] Training: step 10297
I0513 00:37:01.277515 139666521167872 trainer.py:518] Training: step 10297
I0513 00:37:01.297472 139856202532864 trainer.py:518] Training: step 10297
I0513 00:37:10.216989 140193652815872 trainer.py:518] Training: step 10297
I0513 00:37:19.155401 140180528838656 trainer.py:518] Training: step 10298
I0513 00:37:19.155220 139876724979712 trainer.py:518] Training: step 10298
I0513 00:37:19.155801 140143678494720 trainer.py:518] Training: step 10298
I0513 00:37:19.158693 139856202532864 trainer.py:518] Training: step 10298
I0513 00:37:19.155784 140643590531072 trainer.py:518] Training: step 10298
I0513 00:37:19.156913 140260698863616 trainer.py:518] Training: step 10298
I0513 00:37:19.155869 139666521167872 trainer.py:518] Training: step 10298
I0513 00:37:28.095985 140055069881920 logging_writer.py:48] [10280] collection=train accuracy=0.514833, cross_ent_loss=19.729923248291016, cross_ent_loss_per_all_target_tokens=2.50879e-05, experts/auxiliary_loss=0.1411452740430832, experts/expert_usage=7.864421844482422, experts/fraction_tokens_left_behind=2.2590208053588867, experts/router_confidence=2.602522611618042, experts/router_z_loss=0.0017568860203027725, learning_rate=0.00986551, learning_rate/current=0.00986335, loss=19.960071563720703, loss_per_all_target_tokens=2.53805e-05, loss_per_nonpadding_target_token=6.30637e-05, non_padding_fraction/loss_weights=0.402459, timing/seconds=89.38829803466797, timing/seqs=3840, timing/seqs_per_second=42.958641052246094, timing/seqs_per_second_per_core=1.3424575328826904, timing/steps_per_second=0.11187146604061127, timing/target_tokens_per_second=87979.296875, timing/target_tokens_per_second_per_core=2749.35302734375, timing/uptime=2965.25, z_loss=0.08724456280469894, z_loss_per_all_target_tokens=1.10937e-07
I0513 00:37:28.099699 140193652815872 trainer.py:518] Training: step 10298
I0513 00:37:37.033444 139856202532864 trainer.py:518] Training: step 10299
I0513 00:37:37.034250 140180528838656 trainer.py:518] Training: step 10299
I0513 00:37:37.034956 140143678494720 trainer.py:518] Training: step 10299
I0513 00:37:37.035427 139876724979712 trainer.py:518] Training: step 10299
I0513 00:37:37.039399 140643590531072 trainer.py:518] Training: step 10299
I0513 00:37:37.034351 139666521167872 trainer.py:518] Training: step 10299
I0513 00:37:37.054718 140260698863616 trainer.py:518] Training: step 10299
I0513 00:37:45.972744 140193652815872 trainer.py:518] Training: step 10303
I0513 00:37:54.912074 139666521167872 trainer.py:518] Training: step 10305
I0513 00:37:54.911679 140180528838656 trainer.py:518] Training: step 10305
I0513 00:37:54.911736 140643590531072 trainer.py:518] Training: step 10305
I0513 00:37:54.911499 140143678494720 trainer.py:518] Training: step 10303
I0513 00:37:54.914165 140055069881920 logging_writer.py:48] [10290] collection=train accuracy=0.503914, cross_ent_loss=20.230321884155273, cross_ent_loss_per_all_target_tokens=2.57242e-05, experts/auxiliary_loss=0.14078263938426971, experts/expert_usage=7.862941265106201, experts/fraction_tokens_left_behind=2.2845864295959473, experts/router_confidence=2.600151777267456, experts/router_z_loss=0.0017694495618343353, learning_rate=0.00986072, learning_rate/current=0.00985856, loss=20.46175765991211, loss_per_all_target_tokens=2.60185e-05, loss_per_nonpadding_target_token=6.29414e-05, non_padding_fraction/loss_weights=0.413376, timing/seconds=89.3875961303711, timing/seqs=3840, timing/seqs_per_second=42.95897674560547, timing/seqs_per_second_per_core=1.342468023300171, timing/steps_per_second=0.11187233775854111, timing/target_tokens_per_second=87979.984375, timing/target_tokens_per_second_per_core=2749.37451171875, timing/uptime=2975.36, z_loss=0.08888763189315796, z_loss_per_all_target_tokens=1.13026e-07
I0513 00:37:54.955434 139856202532864 trainer.py:518] Training: step 10304
I0513 00:37:54.968204 139876724979712 trainer.py:518] Training: step 10307
I0513 00:37:55.037977 140260698863616 trainer.py:518] Training: step 10309
I0513 00:38:03.852462 140193652815872 trainer.py:518] Training: step 10310
I0513 00:38:30.668767 139876724979712 trainer.py:518] Training: step 10312
I0513 00:38:30.668844 140180528838656 trainer.py:518] Training: step 10312
I0513 00:38:30.669684 139666521167872 trainer.py:518] Training: step 10312
I0513 00:38:30.669206 140643590531072 trainer.py:518] Training: step 10312
I0513 00:38:30.670008 140193652815872 trainer.py:518] Training: step 10312
I0513 00:38:30.671224 139856202532864 trainer.py:518] Training: step 10312
I0513 00:38:30.669492 140260698863616 trainer.py:518] Training: step 10312
I0513 00:38:30.669071 140143678494720 trainer.py:518] Training: step 10312
I0513 00:38:48.546833 139856202532864 trainer.py:518] Training: step 10313
I0513 00:38:48.546159 139876724979712 trainer.py:518] Training: step 10313
I0513 00:38:48.546028 140143678494720 trainer.py:518] Training: step 10313
I0513 00:38:48.546487 140180528838656 trainer.py:518] Training: step 10313
I0513 00:38:48.546529 140193652815872 trainer.py:518] Training: step 10313
I0513 00:38:48.547026 139666521167872 trainer.py:518] Training: step 10313
I0513 00:38:48.548043 140260698863616 trainer.py:518] Training: step 10313
I0513 00:38:48.566766 140643590531072 trainer.py:518] Training: step 10313
I0513 00:39:06.424814 139666521167872 trainer.py:518] Training: step 10314
I0513 00:39:06.424238 139876724979712 trainer.py:518] Training: step 10314
I0513 00:39:06.424162 140180528838656 trainer.py:518] Training: step 10314
I0513 00:39:06.424361 140193652815872 trainer.py:518] Training: step 10314
I0513 00:39:06.425451 139856202532864 trainer.py:518] Training: step 10314
I0513 00:39:06.424912 140260698863616 trainer.py:518] Training: step 10314
I0513 00:39:06.423858 140143678494720 trainer.py:518] Training: step 10314
I0513 00:39:06.425867 140643590531072 trainer.py:518] Training: step 10314
I0513 00:39:24.302517 139666521167872 trainer.py:518] Training: step 10315
I0513 00:39:24.301781 140180528838656 trainer.py:518] Training: step 10315
I0513 00:39:24.302701 139856202532864 trainer.py:518] Training: step 10315
I0513 00:39:24.301937 140193652815872 trainer.py:518] Training: step 10315
I0513 00:39:24.306721 139876724979712 trainer.py:518] Training: step 10315
I0513 00:39:24.303337 140260698863616 trainer.py:518] Training: step 10315
I0513 00:39:24.302321 140643590531072 trainer.py:518] Training: step 10315
I0513 00:39:24.301839 140143678494720 trainer.py:518] Training: step 10315
I0513 00:39:42.179662 139876724979712 trainer.py:518] Training: step 10316
I0513 00:39:42.180615 139666521167872 trainer.py:518] Training: step 10316
I0513 00:39:42.180269 140180528838656 trainer.py:518] Training: step 10316
I0513 00:39:42.180031 140193652815872 trainer.py:518] Training: step 10316
I0513 00:39:42.181518 139856202532864 trainer.py:518] Training: step 10316
I0513 00:39:42.180706 140643590531072 trainer.py:518] Training: step 10316
I0513 00:39:42.179916 140143678494720 trainer.py:518] Training: step 10316
I0513 00:39:42.186039 140260698863616 trainer.py:518] Training: step 10316
I0513 00:40:00.057259 139876724979712 trainer.py:518] Training: step 10317
I0513 00:40:00.058086 139666521167872 trainer.py:518] Training: step 10317
I0513 00:40:00.057555 140180528838656 trainer.py:518] Training: step 10317
I0513 00:40:00.057597 140193652815872 trainer.py:518] Training: step 10317
I0513 00:40:00.067283 139856202532864 trainer.py:518] Training: step 10317
I0513 00:40:00.058511 140260698863616 trainer.py:518] Training: step 10317
I0513 00:40:00.057789 140643590531072 trainer.py:518] Training: step 10317
I0513 00:40:00.060086 140143678494720 trainer.py:518] Training: step 10317
I0513 00:40:17.936255 139666521167872 trainer.py:518] Training: step 10318
I0513 00:40:17.935547 139876724979712 trainer.py:518] Training: step 10318
I0513 00:40:17.935811 140180528838656 trainer.py:518] Training: step 10318
I0513 00:40:17.936091 140193652815872 trainer.py:518] Training: step 10318
I0513 00:40:17.937291 139856202532864 trainer.py:518] Training: step 10318
I0513 00:40:17.936198 140643590531072 trainer.py:518] Training: step 10318
I0513 00:40:17.935911 140143678494720 trainer.py:518] Training: step 10318
I0513 00:40:17.938640 140260698863616 trainer.py:518] Training: step 10318
I0513 00:40:26.876423 140055069881920 logging_writer.py:48] [10300] collection=train accuracy=0.510813, cross_ent_loss=19.95831871032715, cross_ent_loss_per_all_target_tokens=2.53783e-05, experts/auxiliary_loss=0.14002083241939545, experts/expert_usage=7.865872859954834, experts/fraction_tokens_left_behind=2.3644492626190186, experts/router_confidence=2.576672077178955, experts/router_z_loss=0.0017967924941331148, learning_rate=0.00985592, learning_rate/current=0.00985377, loss=20.188234329223633, loss_per_all_target_tokens=2.56707e-05, loss_per_nonpadding_target_token=6.23437e-05, non_padding_fraction/loss_weights=0.41176, timing/seconds=89.38786315917969, timing/seqs=3840, timing/seqs_per_second=42.95885467529297, timing/seqs_per_second_per_core=1.3424642086029053, timing/steps_per_second=0.11187201738357544, timing/target_tokens_per_second=87979.734375, timing/target_tokens_per_second_per_core=2749.36669921875, timing/uptime=3135.14, z_loss=0.0880981832742691, z_loss_per_all_target_tokens=1.12023e-07
I0513 00:40:35.814586 140180528838656 trainer.py:518] Training: step 10319
I0513 00:40:35.814978 139876724979712 trainer.py:518] Training: step 10319
I0513 00:40:35.815993 139666521167872 trainer.py:518] Training: step 10319
I0513 00:40:35.815408 140193652815872 trainer.py:518] Training: step 10319
I0513 00:40:35.818168 139856202532864 trainer.py:518] Training: step 10319
I0513 00:40:35.816027 140643590531072 trainer.py:518] Training: step 10319
I0513 00:40:35.815205 140143678494720 trainer.py:518] Training: step 10319
I0513 00:40:35.823399 140260698863616 trainer.py:518] Training: step 10319
I0513 00:40:45.989711 140193652815872 trainer.py:518] Training: step 10326
I0513 00:40:53.692790 139666521167872 trainer.py:518] Training: step 10327
I0513 00:40:53.692110 139876724979712 trainer.py:518] Training: step 10326
I0513 00:40:53.692075 140143678494720 trainer.py:518] Training: step 10323
I0513 00:40:53.695610 140055069881920 logging_writer.py:48] [10310] collection=train accuracy=0.524172, cross_ent_loss=19.311323165893555, cross_ent_loss_per_all_target_tokens=2.45556e-05, experts/auxiliary_loss=0.1405661702156067, experts/expert_usage=7.8789286613464355, experts/fraction_tokens_left_behind=2.2952122688293457, experts/router_confidence=2.5829575061798096, experts/router_z_loss=0.001775373937562108, learning_rate=0.00985114, learning_rate/current=0.00984899, loss=19.5391902923584, loss_per_all_target_tokens=2.48454e-05, loss_per_nonpadding_target_token=5.95659e-05, non_padding_fraction/loss_weights=0.417107, timing/seconds=89.3847427368164, timing/seqs=3840, timing/seqs_per_second=42.960350036621094, timing/seqs_per_second_per_core=1.3425109386444092, timing/steps_per_second=0.11187591403722763, timing/target_tokens_per_second=87982.796875, timing/target_tokens_per_second_per_core=2749.46240234375, timing/uptime=3153.24, z_loss=0.08552434295415878, z_loss_per_all_target_tokens=1.0875e-07
I0513 00:40:53.725077 140180528838656 trainer.py:518] Training: step 10327
I0513 00:40:53.727178 140643590531072 trainer.py:518] Training: step 10325
I0513 00:40:53.842627 140260698863616 trainer.py:518] Training: step 10328
I0513 00:40:54.506218 139856202532864 trainer.py:518] Training: step 10324
I0513 00:41:02.631224 140193652815872 trainer.py:518] Training: step 10330
I0513 00:41:21.095297 140260698863616 trainer.py:518] Training: step 10331
I0513 00:41:29.448653 139666521167872 trainer.py:518] Training: step 10332
I0513 00:41:29.448034 140180528838656 trainer.py:518] Training: step 10332
I0513 00:41:29.448482 139876724979712 trainer.py:518] Training: step 10332
I0513 00:41:29.448980 140193652815872 trainer.py:518] Training: step 10332
I0513 00:41:29.452248 139856202532864 trainer.py:518] Training: step 10332
I0513 00:41:29.447527 140143678494720 trainer.py:518] Training: step 10332
I0513 00:41:29.450554 140643590531072 trainer.py:518] Training: step 10332
I0513 00:41:38.393476 140260698863616 trainer.py:518] Training: step 10332
I0513 00:41:47.326680 139856202532864 trainer.py:518] Training: step 10333
I0513 00:41:47.327607 139666521167872 trainer.py:518] Training: step 10333
I0513 00:41:47.326947 140180528838656 trainer.py:518] Training: step 10333
I0513 00:41:47.327004 140193652815872 trainer.py:518] Training: step 10333
I0513 00:41:47.328184 139876724979712 trainer.py:518] Training: step 10333
I0513 00:41:47.327054 140643590531072 trainer.py:518] Training: step 10333
I0513 00:41:47.326377 140143678494720 trainer.py:518] Training: step 10333
I0513 00:41:56.265846 140260698863616 trainer.py:518] Training: step 10333
I0513 00:42:05.205042 139876724979712 trainer.py:518] Training: step 10334
I0513 00:42:05.206142 139666521167872 trainer.py:518] Training: step 10334
I0513 00:42:05.205482 140180528838656 trainer.py:518] Training: step 10334
I0513 00:42:05.205718 140193652815872 trainer.py:518] Training: step 10334
I0513 00:42:05.216155 139856202532864 trainer.py:518] Training: step 10334
I0513 00:42:05.205290 140643590531072 trainer.py:518] Training: step 10334
I0513 00:42:05.204741 140143678494720 trainer.py:518] Training: step 10334
I0513 00:42:14.145238 140260698863616 trainer.py:518] Training: step 10334
I0513 00:42:23.082372 139876724979712 trainer.py:518] Training: step 10335
I0513 00:42:23.083917 139666521167872 trainer.py:518] Training: step 10335
I0513 00:42:23.083223 140180528838656 trainer.py:518] Training: step 10335
I0513 00:42:23.084292 139856202532864 trainer.py:518] Training: step 10335
I0513 00:42:23.083695 140193652815872 trainer.py:518] Training: step 10335
I0513 00:42:23.082048 140143678494720 trainer.py:518] Training: step 10335
I0513 00:42:23.083352 140643590531072 trainer.py:518] Training: step 10335
I0513 00:42:32.026682 140260698863616 trainer.py:518] Training: step 10335
I0513 00:42:40.961596 139666521167872 trainer.py:518] Training: step 10336
I0513 00:42:40.961029 139876724979712 trainer.py:518] Training: step 10336
I0513 00:42:40.961180 140180528838656 trainer.py:518] Training: step 10336
I0513 00:42:40.961467 140193652815872 trainer.py:518] Training: step 10336
I0513 00:42:40.963136 139856202532864 trainer.py:518] Training: step 10336
I0513 00:42:40.961395 140643590531072 trainer.py:518] Training: step 10336
I0513 00:42:40.960942 140143678494720 trainer.py:518] Training: step 10336
I0513 00:42:49.905114 140260698863616 trainer.py:518] Training: step 10336
I0513 00:42:58.838225 139876724979712 trainer.py:518] Training: step 10337
I0513 00:42:58.839393 139666521167872 trainer.py:518] Training: step 10337
I0513 00:42:58.838463 140180528838656 trainer.py:518] Training: step 10337
I0513 00:42:58.839365 140193652815872 trainer.py:518] Training: step 10337
I0513 00:42:58.841850 139856202532864 trainer.py:518] Training: step 10337
I0513 00:42:58.838663 140643590531072 trainer.py:518] Training: step 10337
I0513 00:42:58.838279 140143678494720 trainer.py:518] Training: step 10337
I0513 00:43:07.777798 140260698863616 trainer.py:518] Training: step 10337
I0513 00:43:16.716801 139876724979712 trainer.py:518] Training: step 10338
I0513 00:43:16.717797 139666521167872 trainer.py:518] Training: step 10338
I0513 00:43:16.716920 140180528838656 trainer.py:518] Training: step 10338
I0513 00:43:16.717524 140193652815872 trainer.py:518] Training: step 10338
I0513 00:43:16.717113 140643590531072 trainer.py:518] Training: step 10338
I0513 00:43:16.736721 139856202532864 trainer.py:518] Training: step 10338
I0513 00:43:16.717270 140143678494720 trainer.py:518] Training: step 10338
I0513 00:43:25.656780 140260698863616 trainer.py:518] Training: step 10338
I0513 00:43:25.656200 140055069881920 logging_writer.py:48] [10320] collection=train accuracy=0.514304, cross_ent_loss=19.844249725341797, cross_ent_loss_per_all_target_tokens=2.52333e-05, experts/auxiliary_loss=0.14086173474788666, experts/expert_usage=7.870597839355469, experts/fraction_tokens_left_behind=2.296130895614624, experts/router_confidence=2.5928597450256348, experts/router_z_loss=0.0017649050569161773, learning_rate=0.00984637, learning_rate/current=0.00984422, loss=20.074796676635742, loss_per_all_target_tokens=2.55264e-05, loss_per_nonpadding_target_token=6.52713e-05, non_padding_fraction/loss_weights=0.391082, timing/seconds=89.38792419433594, timing/seqs=3840, timing/seqs_per_second=42.958824157714844, timing/seqs_per_second_per_core=1.3424632549285889, timing/steps_per_second=0.11187194287776947, timing/target_tokens_per_second=87979.671875, timing/target_tokens_per_second_per_core=2749.36474609375, timing/uptime=3313.92, z_loss=0.08791940659284592, z_loss_per_all_target_tokens=1.11795e-07
I0513 00:43:34.595561 139876724979712 trainer.py:518] Training: step 10339
I0513 00:43:34.595841 140180528838656 trainer.py:518] Training: step 10339
I0513 00:43:34.596521 139856202532864 trainer.py:518] Training: step 10339
I0513 00:43:34.597046 139666521167872 trainer.py:518] Training: step 10339
I0513 00:43:34.596422 140193652815872 trainer.py:518] Training: step 10339
I0513 00:43:34.596596 140643590531072 trainer.py:518] Training: step 10339
I0513 00:43:34.596232 140143678494720 trainer.py:518] Training: step 10339
I0513 00:43:43.534659 140260698863616 trainer.py:518] Training: step 10344
I0513 00:43:52.473685 139876724979712 trainer.py:518] Training: step 10346
I0513 00:43:52.474326 140180528838656 trainer.py:518] Training: step 10345
I0513 00:43:52.475298 139666521167872 trainer.py:518] Training: step 10345
I0513 00:43:52.475333 140193652815872 trainer.py:518] Training: step 10346
I0513 00:43:52.476363 139856202532864 trainer.py:518] Training: step 10345
I0513 00:43:52.475572 140143678494720 trainer.py:518] Training: step 10343
I0513 00:43:52.478666 140055069881920 logging_writer.py:48] [10330] collection=train accuracy=0.522148, cross_ent_loss=19.353078842163086, cross_ent_loss_per_all_target_tokens=2.46087e-05, experts/auxiliary_loss=0.1424415111541748, experts/expert_usage=7.826596260070801, experts/fraction_tokens_left_behind=2.2066423892974854, experts/router_confidence=2.6236369609832764, experts/router_z_loss=0.0017244163900613785, learning_rate=0.0098416, learning_rate/current=0.00983945, loss=19.58209800720215, loss_per_all_target_tokens=2.48999e-05, loss_per_nonpadding_target_token=6.08574e-05, non_padding_fraction/loss_weights=0.409152, timing/seconds=89.38567352294922, timing/seqs=3840, timing/seqs_per_second=42.959903717041016, timing/seqs_per_second_per_core=1.3424969911575317, timing/steps_per_second=0.11187475174665451, timing/target_tokens_per_second=87981.8828125, timing/target_tokens_per_second_per_core=2749.433837890625, timing/uptime=3332.46, z_loss=0.08484978973865509, z_loss_per_all_target_tokens=1.07892e-07
I0513 00:43:52.503184 140643590531072 trainer.py:518] Training: step 10347
I0513 00:44:01.413021 140260698863616 trainer.py:518] Training: step 10350
I0513 00:44:28.229487 140180528838656 trainer.py:518] Training: step 10352
I0513 00:44:28.229578 139876724979712 trainer.py:518] Training: step 10352
I0513 00:44:28.231036 139666521167872 trainer.py:518] Training: step 10352
I0513 00:44:28.230586 139856202532864 trainer.py:518] Training: step 10352
I0513 00:44:28.230287 140193652815872 trainer.py:518] Training: step 10352
I0513 00:44:28.229546 140260698863616 trainer.py:518] Training: step 10352
I0513 00:44:28.229816 140643590531072 trainer.py:518] Training: step 10352
I0513 00:44:28.229650 140143678494720 trainer.py:518] Training: step 10352
I0513 00:44:46.107810 139876724979712 trainer.py:518] Training: step 10353
I0513 00:44:46.107875 140180528838656 trainer.py:518] Training: step 10353
I0513 00:44:46.109368 139666521167872 trainer.py:518] Training: step 10353
I0513 00:44:46.109132 139856202532864 trainer.py:518] Training: step 10353
I0513 00:44:46.108055 140193652815872 trainer.py:518] Training: step 10353
I0513 00:44:46.108195 140260698863616 trainer.py:518] Training: step 10353
I0513 00:44:46.107885 140643590531072 trainer.py:518] Training: step 10353
I0513 00:44:46.108132 140143678494720 trainer.py:518] Training: step 10353
I0513 00:45:03.985552 139876724979712 trainer.py:518] Training: step 10354
I0513 00:45:03.985435 140180528838656 trainer.py:518] Training: step 10354
I0513 00:45:03.985809 139856202532864 trainer.py:518] Training: step 10354
I0513 00:45:03.986599 139666521167872 trainer.py:518] Training: step 10354
I0513 00:45:03.985658 140260698863616 trainer.py:518] Training: step 10354
I0513 00:45:03.985811 140643590531072 trainer.py:518] Training: step 10354
I0513 00:45:03.985318 140143678494720 trainer.py:518] Training: step 10354
I0513 00:45:12.925420 140193652815872 trainer.py:518] Training: step 10354
I0513 00:45:21.863637 139876724979712 trainer.py:518] Training: step 10355
I0513 00:45:21.863839 140180528838656 trainer.py:518] Training: step 10355
I0513 00:45:21.864429 139856202532864 trainer.py:518] Training: step 10355
I0513 00:45:21.865022 139666521167872 trainer.py:518] Training: step 10355
I0513 00:45:21.863876 140260698863616 trainer.py:518] Training: step 10355
I0513 00:45:21.864238 140643590531072 trainer.py:518] Training: step 10355
I0513 00:45:21.863746 140143678494720 trainer.py:518] Training: step 10355
I0513 00:45:30.802350 140193652815872 trainer.py:518] Training: step 10355
I0513 00:45:39.743274 139856202532864 trainer.py:518] Training: step 10356
I0513 00:45:39.742748 140180528838656 trainer.py:518] Training: step 10356
I0513 00:45:39.743043 139876724979712 trainer.py:518] Training: step 10356
I0513 00:45:39.743886 139666521167872 trainer.py:518] Training: step 10356
I0513 00:45:39.743024 140260698863616 trainer.py:518] Training: step 10356
I0513 00:45:39.743136 140643590531072 trainer.py:518] Training: step 10356
I0513 00:45:39.742245 140143678494720 trainer.py:518] Training: step 10356
I0513 00:45:48.682583 140193652815872 trainer.py:518] Training: step 10356
I0513 00:45:57.619950 139876724979712 trainer.py:518] Training: step 10357
I0513 00:45:57.620349 140180528838656 trainer.py:518] Training: step 10357
I0513 00:45:57.621572 139666521167872 trainer.py:518] Training: step 10357
I0513 00:45:57.621658 139856202532864 trainer.py:518] Training: step 10357
I0513 00:45:57.620237 140260698863616 trainer.py:518] Training: step 10357
I0513 00:45:57.620996 140643590531072 trainer.py:518] Training: step 10357
I0513 00:45:57.620726 140143678494720 trainer.py:518] Training: step 10357
I0513 00:46:15.498358 139876724979712 trainer.py:518] Training: step 10358
I0513 00:46:15.499292 139856202532864 trainer.py:518] Training: step 10358
I0513 00:46:15.499763 139666521167872 trainer.py:518] Training: step 10358
I0513 00:46:15.499176 140180528838656 trainer.py:518] Training: step 10358
I0513 00:46:15.498938 140193652815872 trainer.py:518] Training: step 10358
I0513 00:46:15.498563 140260698863616 trainer.py:518] Training: step 10358
I0513 00:46:15.499082 140643590531072 trainer.py:518] Training: step 10358
I0513 00:46:15.498696 140143678494720 trainer.py:518] Training: step 10358
I0513 00:46:24.438734 140055069881920 logging_writer.py:48] [10340] collection=train accuracy=0.518287, cross_ent_loss=19.579801559448242, cross_ent_loss_per_all_target_tokens=2.4897e-05, experts/auxiliary_loss=0.14182870090007782, experts/expert_usage=7.846426486968994, experts/fraction_tokens_left_behind=2.2284624576568604, experts/router_confidence=2.613205671310425, experts/router_z_loss=0.0017419345676898956, learning_rate=0.00983683, learning_rate/current=0.00983469, loss=19.80898666381836, loss_per_all_target_tokens=2.51884e-05, loss_per_nonpadding_target_token=6.21125e-05, non_padding_fraction/loss_weights=0.405529, timing/seconds=89.39073181152344, timing/seqs=3840, timing/seqs_per_second=42.95747375488281, timing/seqs_per_second_per_core=1.342421054840088, timing/steps_per_second=0.11186841875314713, timing/target_tokens_per_second=87976.90625, timing/target_tokens_per_second_per_core=2749.2783203125, timing/uptime=3492.7, z_loss=0.08561376482248306, z_loss_per_all_target_tokens=1.08864e-07
I0513 00:46:33.376873 139876724979712 trainer.py:518] Training: step 10359
I0513 00:46:33.377441 140180528838656 trainer.py:518] Training: step 10359
I0513 00:46:33.378851 139666521167872 trainer.py:518] Training: step 10359
I0513 00:46:33.377781 140193652815872 trainer.py:518] Training: step 10359
I0513 00:46:33.379357 139856202532864 trainer.py:518] Training: step 10359
I0513 00:46:33.377768 140260698863616 trainer.py:518] Training: step 10359
I0513 00:46:33.378004 140643590531072 trainer.py:518] Training: step 10359
I0513 00:46:33.378256 140143678494720 trainer.py:518] Training: step 10359
I0513 00:46:51.257336 139856202532864 trainer.py:518] Training: step 10367
I0513 00:46:51.256309 140143678494720 trainer.py:518] Training: step 10363
I0513 00:46:51.257465 140643590531072 trainer.py:518] Training: step 10366
I0513 00:46:51.259391 140055069881920 logging_writer.py:48] [10350] collection=train accuracy=0.510063, cross_ent_loss=19.966787338256836, cross_ent_loss_per_all_target_tokens=2.53891e-05, experts/auxiliary_loss=0.14314092695713043, experts/expert_usage=7.85678243637085, experts/fraction_tokens_left_behind=2.159802198410034, experts/router_confidence=2.6371848583221436, experts/router_z_loss=0.0017054075142368674, learning_rate=0.00983208, learning_rate/current=0.00982994, loss=20.1960391998291, loss_per_all_target_tokens=2.56806e-05, loss_per_nonpadding_target_token=6.30444e-05, non_padding_fraction/loss_weights=0.407341, timing/seconds=89.38201904296875, timing/seqs=3840, timing/seqs_per_second=42.96166229248047, timing/seqs_per_second_per_core=1.3425519466400146, timing/steps_per_second=0.11187932640314102, timing/target_tokens_per_second=87985.484375, timing/target_tokens_per_second_per_core=2749.54638671875, timing/uptime=3510.79, z_loss=0.08440780639648438, z_loss_per_all_target_tokens=1.0733e-07
I0513 00:46:51.286265 139876724979712 trainer.py:518] Training: step 10368
I0513 00:46:51.288648 139666521167872 trainer.py:518] Training: step 10367
I0513 00:46:51.288569 140180528838656 trainer.py:518] Training: step 10367
I0513 00:46:51.290441 140260698863616 trainer.py:518] Training: step 10366
I0513 00:47:00.196275 140193652815872 trainer.py:518] Training: step 10370
I0513 00:47:27.013362 139666521167872 trainer.py:518] Training: step 10372
I0513 00:47:27.012385 140180528838656 trainer.py:518] Training: step 10372
I0513 00:47:27.013150 139856202532864 trainer.py:518] Training: step 10372
I0513 00:47:27.012352 140193652815872 trainer.py:518] Training: step 10372
I0513 00:47:27.012914 139876724979712 trainer.py:518] Training: step 10372
I0513 00:47:27.012315 140260698863616 trainer.py:518] Training: step 10372
I0513 00:47:27.011931 140143678494720 trainer.py:518] Training: step 10372
I0513 00:47:27.013713 140643590531072 trainer.py:518] Training: step 10372
I0513 00:47:44.890688 139876724979712 trainer.py:518] Training: step 10373
I0513 00:47:44.892199 139666521167872 trainer.py:518] Training: step 10373
I0513 00:47:44.891448 140180528838656 trainer.py:518] Training: step 10373
I0513 00:47:44.891866 139856202532864 trainer.py:518] Training: step 10373
I0513 00:47:44.891507 140193652815872 trainer.py:518] Training: step 10373
I0513 00:47:44.891352 140260698863616 trainer.py:518] Training: step 10373
I0513 00:47:44.890930 140143678494720 trainer.py:518] Training: step 10373
I0513 00:47:44.892031 140643590531072 trainer.py:518] Training: step 10373
I0513 00:48:02.768647 139876724979712 trainer.py:518] Training: step 10374
I0513 00:48:02.769344 139856202532864 trainer.py:518] Training: step 10374
I0513 00:48:02.770291 139666521167872 trainer.py:518] Training: step 10374
I0513 00:48:02.768855 140193652815872 trainer.py:518] Training: step 10374
I0513 00:48:02.769489 140180528838656 trainer.py:518] Training: step 10374
I0513 00:48:02.769238 140260698863616 trainer.py:518] Training: step 10374
I0513 00:48:02.768356 140143678494720 trainer.py:518] Training: step 10374
I0513 00:48:02.770242 140643590531072 trainer.py:518] Training: step 10374
I0513 00:48:20.647697 139856202532864 trainer.py:518] Training: step 10375
I0513 00:48:20.647557 139876724979712 trainer.py:518] Training: step 10375
I0513 00:48:20.647813 140193652815872 trainer.py:518] Training: step 10375
I0513 00:48:20.649205 139666521167872 trainer.py:518] Training: step 10375
I0513 00:48:20.648976 140180528838656 trainer.py:518] Training: step 10375
I0513 00:48:20.647910 140260698863616 trainer.py:518] Training: step 10375
I0513 00:48:20.647072 140143678494720 trainer.py:518] Training: step 10375
I0513 00:48:20.648442 140643590531072 trainer.py:518] Training: step 10375
I0513 00:48:38.526716 139876724979712 trainer.py:518] Training: step 10376
I0513 00:48:38.528160 139666521167872 trainer.py:518] Training: step 10376
I0513 00:48:38.527400 140180528838656 trainer.py:518] Training: step 10376
I0513 00:48:38.526978 140193652815872 trainer.py:518] Training: step 10376
I0513 00:48:38.528080 139856202532864 trainer.py:518] Training: step 10376
I0513 00:48:38.526808 140260698863616 trainer.py:518] Training: step 10376
I0513 00:48:38.526612 140143678494720 trainer.py:518] Training: step 10376
I0513 00:48:38.527797 140643590531072 trainer.py:518] Training: step 10376
I0513 00:48:56.405169 139876724979712 trainer.py:518] Training: step 10377
I0513 00:48:56.405692 139856202532864 trainer.py:518] Training: step 10377
I0513 00:48:56.406135 139666521167872 trainer.py:518] Training: step 10377
I0513 00:48:56.405252 140193652815872 trainer.py:518] Training: step 10377
I0513 00:48:56.406104 140180528838656 trainer.py:518] Training: step 10377
I0513 00:48:56.405763 140260698863616 trainer.py:518] Training: step 10377
I0513 00:48:56.406128 140643590531072 trainer.py:518] Training: step 10377
I0513 00:48:56.405229 140143678494720 trainer.py:518] Training: step 10377
I0513 00:49:14.284398 139666521167872 trainer.py:518] Training: step 10378
I0513 00:49:14.283746 139876724979712 trainer.py:518] Training: step 10378
I0513 00:49:14.284426 139856202532864 trainer.py:518] Training: step 10378
I0513 00:49:14.283917 140193652815872 trainer.py:518] Training: step 10378
I0513 00:49:14.284949 140180528838656 trainer.py:518] Training: step 10378
I0513 00:49:14.283905 140260698863616 trainer.py:518] Training: step 10378
I0513 00:49:14.283964 140143678494720 trainer.py:518] Training: step 10378
I0513 00:49:14.284951 140643590531072 trainer.py:518] Training: step 10378
I0513 00:49:23.223769 140055069881920 logging_writer.py:48] [10360] collection=train accuracy=0.514324, cross_ent_loss=19.769058227539062, cross_ent_loss_per_all_target_tokens=2.51377e-05, experts/auxiliary_loss=0.1413010209798813, experts/expert_usage=7.8894572257995605, experts/fraction_tokens_left_behind=2.236968755722046, experts/router_confidence=2.605964422225952, experts/router_z_loss=0.0017569191986694932, learning_rate=0.00982733, learning_rate/current=0.00982519, loss=19.996801376342773, loss_per_all_target_tokens=2.54272e-05, loss_per_nonpadding_target_token=6.15577e-05, non_padding_fraction/loss_weights=0.413064, timing/seconds=89.39232635498047, timing/seqs=3840, timing/seqs_per_second=42.95670700073242, timing/seqs_per_second_per_core=1.3423970937728882, timing/steps_per_second=0.11186642944812775, timing/target_tokens_per_second=87975.3359375, timing/target_tokens_per_second_per_core=2749.229248046875, timing/uptime=3671.48, z_loss=0.08468493074178696, z_loss_per_all_target_tokens=1.07682e-07
I0513 00:49:32.162888 139876724979712 trainer.py:518] Training: step 10379
I0513 00:49:32.164213 139666521167872 trainer.py:518] Training: step 10379
I0513 00:49:32.163250 140193652815872 trainer.py:518] Training: step 10379
I0513 00:49:32.164016 140180528838656 trainer.py:518] Training: step 10379
I0513 00:49:32.164531 139856202532864 trainer.py:518] Training: step 10379
I0513 00:49:32.163994 140260698863616 trainer.py:518] Training: step 10379
I0513 00:49:32.163640 140643590531072 trainer.py:518] Training: step 10379
I0513 00:49:32.163636 140143678494720 trainer.py:518] Training: step 10379
I0513 00:49:50.041651 139856202532864 trainer.py:518] Training: step 10386
I0513 00:49:50.042249 139666521167872 trainer.py:518] Training: step 10386
I0513 00:49:50.040929 140193652815872 trainer.py:518] Training: step 10386
I0513 00:49:50.041837 140180528838656 trainer.py:518] Training: step 10386
I0513 00:49:50.041116 140143678494720 trainer.py:518] Training: step 10383
I0513 00:49:50.044387 140055069881920 logging_writer.py:48] [10370] collection=train accuracy=0.510895, cross_ent_loss=19.977373123168945, cross_ent_loss_per_all_target_tokens=2.54025e-05, experts/auxiliary_loss=0.14042077958583832, experts/expert_usage=7.873894691467285, experts/fraction_tokens_left_behind=2.3090832233428955, experts/router_confidence=2.585980176925659, experts/router_z_loss=0.00177692249417305, learning_rate=0.00982259, learning_rate/current=0.00982045, loss=20.204938888549805, loss_per_all_target_tokens=2.56919e-05, loss_per_nonpadding_target_token=6.28274e-05, non_padding_fraction/loss_weights=0.408929, timing/seconds=89.3834228515625, timing/seqs=3840, timing/seqs_per_second=42.96098327636719, timing/seqs_per_second_per_core=1.3425307273864746, timing/steps_per_second=0.11187756061553955, timing/target_tokens_per_second=87984.09375, timing/target_tokens_per_second_per_core=2749.5029296875, timing/uptime=3689.58, z_loss=0.08536898344755173, z_loss_per_all_target_tokens=1.08552e-07
I0513 00:49:50.070052 139876724979712 trainer.py:518] Training: step 10387
I0513 00:49:50.072530 140643590531072 trainer.py:518] Training: step 10387
I0513 00:49:50.076051 140260698863616 trainer.py:518] Training: step 10387
I0513 00:50:25.796881 139876724979712 trainer.py:518] Training: step 10392
I0513 00:50:25.798607 139666521167872 trainer.py:518] Training: step 10392
I0513 00:50:25.797609 140193652815872 trainer.py:518] Training: step 10392
I0513 00:50:25.798645 139856202532864 trainer.py:518] Training: step 10392
I0513 00:50:25.798279 140180528838656 trainer.py:518] Training: step 10392
I0513 00:50:25.797005 140260698863616 trainer.py:518] Training: step 10392
I0513 00:50:25.796816 140143678494720 trainer.py:518] Training: step 10392
I0513 00:50:25.798243 140643590531072 trainer.py:518] Training: step 10392
I0513 00:50:43.676733 139876724979712 trainer.py:518] Training: step 10393
I0513 00:50:43.677189 139856202532864 trainer.py:518] Training: step 10393
I0513 00:50:43.678040 139666521167872 trainer.py:518] Training: step 10393
I0513 00:50:43.677472 140180528838656 trainer.py:518] Training: step 10393
I0513 00:50:43.677230 140193652815872 trainer.py:518] Training: step 10393
I0513 00:50:43.676595 140260698863616 trainer.py:518] Training: step 10393
I0513 00:50:43.676288 140143678494720 trainer.py:518] Training: step 10393
I0513 00:50:43.677736 140643590531072 trainer.py:518] Training: step 10393
I0513 00:51:01.555503 139876724979712 trainer.py:518] Training: step 10394
I0513 00:51:01.556378 139856202532864 trainer.py:518] Training: step 10394
I0513 00:51:01.555778 140193652815872 trainer.py:518] Training: step 10394
I0513 00:51:01.557098 139666521167872 trainer.py:518] Training: step 10394
I0513 00:51:01.555865 140260698863616 trainer.py:518] Training: step 10394
I0513 00:51:01.555103 140143678494720 trainer.py:518] Training: step 10394
I0513 00:51:10.496548 140180528838656 trainer.py:518] Training: step 10394
I0513 00:51:10.496471 140643590531072 trainer.py:518] Training: step 10394
I0513 00:51:19.435391 139856202532864 trainer.py:518] Training: step 10395
I0513 00:51:19.436050 139666521167872 trainer.py:518] Training: step 10395
I0513 00:51:19.435257 140193652815872 trainer.py:518] Training: step 10395
I0513 00:51:19.434943 140260698863616 trainer.py:518] Training: step 10395
I0513 00:51:19.434556 140143678494720 trainer.py:518] Training: step 10395
I0513 00:51:28.372826 139876724979712 trainer.py:518] Training: step 10395
I0513 00:51:28.377169 140643590531072 trainer.py:518] Training: step 10395
I0513 00:51:37.315172 139666521167872 trainer.py:518] Training: step 10396
I0513 00:51:37.313900 140193652815872 trainer.py:518] Training: step 10396
I0513 00:51:37.314649 140180528838656 trainer.py:518] Training: step 10395
I0513 00:51:37.315106 139856202532864 trainer.py:518] Training: step 10396
I0513 00:51:37.313783 140260698863616 trainer.py:518] Training: step 10396
I0513 00:51:37.313265 140143678494720 trainer.py:518] Training: step 10396
I0513 00:51:46.253275 139876724979712 trainer.py:518] Training: step 10396
I0513 00:51:46.254351 140643590531072 trainer.py:518] Training: step 10396
I0513 00:51:55.192441 139856202532864 trainer.py:518] Training: step 10397
I0513 00:51:55.193209 139666521167872 trainer.py:518] Training: step 10397
I0513 00:51:55.191982 140193652815872 trainer.py:518] Training: step 10397
I0513 00:51:55.192424 140180528838656 trainer.py:518] Training: step 10396
I0513 00:51:55.192294 140260698863616 trainer.py:518] Training: step 10397
I0513 00:51:55.191873 140143678494720 trainer.py:518] Training: step 10397
I0513 00:52:13.070202 139856202532864 trainer.py:518] Training: step 10398
I0513 00:52:13.070048 139876724979712 trainer.py:518] Training: step 10398
I0513 00:52:13.071469 139666521167872 trainer.py:518] Training: step 10398
I0513 00:52:13.070487 140193652815872 trainer.py:518] Training: step 10398
I0513 00:52:13.071452 140180528838656 trainer.py:518] Training: step 10397
I0513 00:52:13.070256 140260698863616 trainer.py:518] Training: step 10398
I0513 00:52:13.069454 140143678494720 trainer.py:518] Training: step 10398
I0513 00:52:13.071079 140643590531072 trainer.py:518] Training: step 10398
I0513 00:52:22.008890 140055069881920 logging_writer.py:48] [10380] collection=train accuracy=0.518879, cross_ent_loss=19.527341842651367, cross_ent_loss_per_all_target_tokens=2.48303e-05, experts/auxiliary_loss=0.14090824127197266, experts/expert_usage=7.878994941711426, experts/fraction_tokens_left_behind=2.271723747253418, experts/router_confidence=2.5942587852478027, experts/router_z_loss=0.00176408258266747, learning_rate=0.00981785, learning_rate/current=0.00981572, loss=19.75324821472168, loss_per_all_target_tokens=2.51176e-05, loss_per_nonpadding_target_token=6.27913e-05, non_padding_fraction/loss_weights=0.400016, timing/seconds=89.39009857177734, timing/seqs=3840, timing/seqs_per_second=42.9577751159668, timing/seqs_per_second_per_core=1.3424304723739624, timing/steps_per_second=0.1118692085146904, timing/target_tokens_per_second=87977.5234375, timing/target_tokens_per_second_per_core=2749.297607421875, timing/uptime=3850.26, z_loss=0.08323480188846588, z_loss_per_all_target_tokens=1.05839e-07
I0513 00:52:30.947602 139876724979712 trainer.py:518] Training: step 10399
I0513 00:52:30.948457 139856202532864 trainer.py:518] Training: step 10399
I0513 00:52:30.948096 140180528838656 trainer.py:518] Training: step 10400
I0513 00:52:30.949327 139666521167872 trainer.py:518] Training: step 10399
I0513 00:52:30.948484 140193652815872 trainer.py:518] Training: step 10399
I0513 00:52:30.948385 140260698863616 trainer.py:518] Training: step 10399
I0513 00:52:30.948685 140643590531072 trainer.py:518] Training: step 10399
I0513 00:52:30.948400 140143678494720 trainer.py:518] Training: step 10399
I0513 00:52:48.826716 139666521167872 trainer.py:518] Training: step 10406
I0513 00:52:48.826391 139856202532864 trainer.py:518] Training: step 10404
I0513 00:52:48.826309 140260698863616 trainer.py:518] Training: step 10408
I0513 00:52:48.825440 140143678494720 trainer.py:518] Training: step 10403
I0513 00:52:48.828646 140055069881920 logging_writer.py:48] [10390] collection=train accuracy=0.523115, cross_ent_loss=19.353601455688477, cross_ent_loss_per_all_target_tokens=2.46094e-05, experts/auxiliary_loss=0.14076226949691772, experts/expert_usage=7.858908176422119, experts/fraction_tokens_left_behind=2.2923104763031006, experts/router_confidence=2.589142084121704, experts/router_z_loss=0.001771409879438579, learning_rate=0.00981312, learning_rate/current=0.009811, loss=19.57797622680664, loss_per_all_target_tokens=2.48947e-05, loss_per_nonpadding_target_token=5.95715e-05, non_padding_fraction/loss_weights=0.417896, timing/seconds=89.386474609375, timing/seqs=3840, timing/seqs_per_second=42.95951843261719, timing/seqs_per_second_per_core=1.342484951019287, timing/steps_per_second=0.11187374591827393, timing/target_tokens_per_second=87981.09375, timing/target_tokens_per_second_per_core=2749.4091796875, timing/uptime=3868.36, z_loss=0.08184109628200531, z_loss_per_all_target_tokens=1.04066e-07
I0513 00:52:48.857063 140193652815872 trainer.py:518] Training: step 10407
I0513 00:52:57.764941 139876724979712 trainer.py:518] Training: step 10410
I0513 00:52:57.765775 140180528838656 trainer.py:518] Training: step 10410
I0513 00:52:57.765638 140643590531072 trainer.py:518] Training: step 10410
I0513 00:53:24.583648 139856202532864 trainer.py:518] Training: step 10412
I0513 00:53:24.583246 139876724979712 trainer.py:518] Training: step 10412
I0513 00:53:24.584105 139666521167872 trainer.py:518] Training: step 10412
I0513 00:53:24.583330 140193652815872 trainer.py:518] Training: step 10412
I0513 00:53:24.584273 140180528838656 trainer.py:518] Training: step 10412
I0513 00:53:24.582998 140260698863616 trainer.py:518] Training: step 10412
I0513 00:53:24.582601 140143678494720 trainer.py:518] Training: step 10412
I0513 00:53:24.584019 140643590531072 trainer.py:518] Training: step 10412
I0513 00:53:42.460545 139876724979712 trainer.py:518] Training: step 10413
I0513 00:53:42.461221 139856202532864 trainer.py:518] Training: step 10413
I0513 00:53:42.461907 139666521167872 trainer.py:518] Training: step 10413
I0513 00:53:42.461587 140180528838656 trainer.py:518] Training: step 10413
I0513 00:53:42.460973 140193652815872 trainer.py:518] Training: step 10413
I0513 00:53:42.460729 140260698863616 trainer.py:518] Training: step 10413
I0513 00:53:42.460424 140143678494720 trainer.py:518] Training: step 10413
I0513 00:53:42.461785 140643590531072 trainer.py:518] Training: step 10413
I0513 00:54:00.339422 139876724979712 trainer.py:518] Training: step 10414
I0513 00:54:00.340273 139856202532864 trainer.py:518] Training: step 10414
I0513 00:54:00.339785 140193652815872 trainer.py:518] Training: step 10414
I0513 00:54:00.340600 140180528838656 trainer.py:518] Training: step 10414
I0513 00:54:00.344982 139666521167872 trainer.py:518] Training: step 10414
I0513 00:54:00.339737 140260698863616 trainer.py:518] Training: step 10414
I0513 00:54:00.339511 140143678494720 trainer.py:518] Training: step 10414
I0513 00:54:00.340794 140643590531072 trainer.py:518] Training: step 10414
I0513 00:54:18.218516 139856202532864 trainer.py:518] Training: step 10415
I0513 00:54:18.218429 139876724979712 trainer.py:518] Training: step 10415
I0513 00:54:18.218553 140193652815872 trainer.py:518] Training: step 10415
I0513 00:54:18.219183 140180528838656 trainer.py:518] Training: step 10415
I0513 00:54:18.225770 139666521167872 trainer.py:518] Training: step 10415
I0513 00:54:18.218879 140260698863616 trainer.py:518] Training: step 10415
I0513 00:54:18.217790 140143678494720 trainer.py:518] Training: step 10415
I0513 00:54:18.219017 140643590531072 trainer.py:518] Training: step 10415
I0513 00:54:36.095830 139876724979712 trainer.py:518] Training: step 10416
I0513 00:54:36.096934 139856202532864 trainer.py:518] Training: step 10416
I0513 00:54:36.096451 140193652815872 trainer.py:518] Training: step 10416
I0513 00:54:36.097552 140180528838656 trainer.py:518] Training: step 10416
I0513 00:54:36.106835 139666521167872 trainer.py:518] Training: step 10416
I0513 00:54:36.096719 140260698863616 trainer.py:518] Training: step 10416
I0513 00:54:36.096741 140643590531072 trainer.py:518] Training: step 10416
I0513 00:54:36.096289 140143678494720 trainer.py:518] Training: step 10416
I0513 00:54:53.974878 139876724979712 trainer.py:518] Training: step 10417
I0513 00:54:53.975460 139856202532864 trainer.py:518] Training: step 10417
I0513 00:54:53.975454 140193652815872 trainer.py:518] Training: step 10417
I0513 00:54:53.976394 140180528838656 trainer.py:518] Training: step 10417
I0513 00:54:53.980905 139666521167872 trainer.py:518] Training: step 10417
I0513 00:54:53.975247 140260698863616 trainer.py:518] Training: step 10417
I0513 00:54:53.975720 140643590531072 trainer.py:518] Training: step 10417
I0513 00:54:53.975170 140143678494720 trainer.py:518] Training: step 10417
I0513 00:55:11.853278 139876724979712 trainer.py:518] Training: step 10418
I0513 00:55:11.853791 139856202532864 trainer.py:518] Training: step 10418
I0513 00:55:11.853670 140193652815872 trainer.py:518] Training: step 10418
I0513 00:55:11.854609 140180528838656 trainer.py:518] Training: step 10418
I0513 00:55:11.857057 139666521167872 trainer.py:518] Training: step 10418
I0513 00:55:11.853439 140260698863616 trainer.py:518] Training: step 10418
I0513 00:55:11.854462 140643590531072 trainer.py:518] Training: step 10418
I0513 00:55:11.854424 140143678494720 trainer.py:518] Training: step 10418
I0513 00:55:20.793825 140055069881920 logging_writer.py:48] [10400] collection=train accuracy=0.517766, cross_ent_loss=19.529314041137695, cross_ent_loss_per_all_target_tokens=2.48328e-05, experts/auxiliary_loss=0.14102864265441895, experts/expert_usage=7.856592655181885, experts/fraction_tokens_left_behind=2.2796127796173096, experts/router_confidence=2.5928544998168945, experts/router_z_loss=0.0017638284480199218, learning_rate=0.0098084, learning_rate/current=0.00980628, loss=19.755462646484375, loss_per_all_target_tokens=2.51204e-05, loss_per_nonpadding_target_token=6.09452e-05, non_padding_fraction/loss_weights=0.41218, timing/seconds=89.39115905761719, timing/seqs=3840, timing/seqs_per_second=42.95726776123047, timing/seqs_per_second_per_core=1.3424146175384521, timing/steps_per_second=0.11186788976192474, timing/target_tokens_per_second=87976.484375, timing/target_tokens_per_second_per_core=2749.26513671875, timing/uptime=4029.05, z_loss=0.08335848897695541, z_loss_per_all_target_tokens=1.05996e-07
I0513 00:55:29.732076 139876724979712 trainer.py:518] Training: step 10419
I0513 00:55:29.732574 139856202532864 trainer.py:518] Training: step 10419
I0513 00:55:29.733247 140180528838656 trainer.py:518] Training: step 10419
I0513 00:55:29.733074 140193652815872 trainer.py:518] Training: step 10419
I0513 00:55:29.740866 139666521167872 trainer.py:518] Training: step 10419
I0513 00:55:29.733042 140260698863616 trainer.py:518] Training: step 10419
I0513 00:55:29.733625 140643590531072 trainer.py:518] Training: step 10419
I0513 00:55:29.732913 140143678494720 trainer.py:518] Training: step 10419
I0513 00:55:47.610447 139856202532864 trainer.py:518] Training: step 10426
I0513 00:55:47.609209 139876724979712 trainer.py:518] Training: step 10426
I0513 00:55:47.609617 140193652815872 trainer.py:518] Training: step 10426
I0513 00:55:47.610599 140180528838656 trainer.py:518] Training: step 10426
I0513 00:55:47.609525 140143678494720 trainer.py:518] Training: step 10423
I0513 00:55:47.612170 140055069881920 logging_writer.py:48] [10410] collection=train accuracy=0.521152, cross_ent_loss=19.52655601501465, cross_ent_loss_per_all_target_tokens=2.48293e-05, experts/auxiliary_loss=0.14125420153141022, experts/expert_usage=7.870034694671631, experts/fraction_tokens_left_behind=2.2614011764526367, experts/router_confidence=2.5893595218658447, experts/router_z_loss=0.0017563978908583522, learning_rate=0.00980369, learning_rate/current=0.00980157, loss=19.75305938720703, loss_per_all_target_tokens=2.51173e-05, loss_per_nonpadding_target_token=6.14951e-05, non_padding_fraction/loss_weights=0.408444, timing/seconds=89.38298797607422, timing/seqs=3840, timing/seqs_per_second=42.96119689941406, timing/seqs_per_second_per_core=1.3425374031066895, timing/steps_per_second=0.11187811195850372, timing/target_tokens_per_second=87984.53125, timing/target_tokens_per_second_per_core=2749.5166015625, timing/uptime=4047.14, z_loss=0.08349352329969406, z_loss_per_all_target_tokens=1.06167e-07
I0513 00:55:47.623380 139666521167872 trainer.py:518] Training: step 10424
I0513 00:55:47.643258 140643590531072 trainer.py:518] Training: step 10426
I0513 00:55:47.641642 140260698863616 trainer.py:518] Training: step 10428
I0513 00:56:23.366870 139856202532864 trainer.py:518] Training: step 10432
I0513 00:56:23.366228 139876724979712 trainer.py:518] Training: step 10432
I0513 00:56:23.366725 140193652815872 trainer.py:518] Training: step 10432
I0513 00:56:23.367326 140643590531072 trainer.py:518] Training: step 10432
I0513 00:56:23.367940 140180528838656 trainer.py:518] Training: step 10432
I0513 00:56:23.366965 140143678494720 trainer.py:518] Training: step 10432
I0513 00:56:23.385017 139666521167872 trainer.py:518] Training: step 10432
I0513 00:56:23.366398 140260698863616 trainer.py:518] Training: step 10432
I0513 00:56:41.246062 139856202532864 trainer.py:518] Training: step 10433
I0513 00:56:41.245679 139876724979712 trainer.py:518] Training: step 10433
I0513 00:56:41.246057 140193652815872 trainer.py:518] Training: step 10433
I0513 00:56:41.246862 140180528838656 trainer.py:518] Training: step 10433
I0513 00:56:41.245702 140143678494720 trainer.py:518] Training: step 10433
I0513 00:56:41.246939 140643590531072 trainer.py:518] Training: step 10433
I0513 00:56:41.250999 139666521167872 trainer.py:518] Training: step 10433
I0513 00:56:41.245867 140260698863616 trainer.py:518] Training: step 10433
I0513 00:56:59.125207 139856202532864 trainer.py:518] Training: step 10434
I0513 00:56:59.124964 139876724979712 trainer.py:518] Training: step 10434
I0513 00:56:59.125210 140193652815872 trainer.py:518] Training: step 10434
I0513 00:56:59.125963 140180528838656 trainer.py:518] Training: step 10434
I0513 00:56:59.124494 140143678494720 trainer.py:518] Training: step 10434
I0513 00:56:59.125552 140643590531072 trainer.py:518] Training: step 10434
I0513 00:56:59.132256 139666521167872 trainer.py:518] Training: step 10434
I0513 00:56:59.125584 140260698863616 trainer.py:518] Training: step 10434
I0513 00:57:17.002876 139856202532864 trainer.py:518] Training: step 10435
I0513 00:57:17.002383 139876724979712 trainer.py:518] Training: step 10435
I0513 00:57:17.002489 140193652815872 trainer.py:518] Training: step 10435
I0513 00:57:17.003449 140643590531072 trainer.py:518] Training: step 10435
I0513 00:57:17.002596 140143678494720 trainer.py:518] Training: step 10435
I0513 00:57:17.002588 140260698863616 trainer.py:518] Training: step 10435
I0513 00:57:25.946220 140180528838656 trainer.py:518] Training: step 10435
I0513 00:57:25.955061 139666521167872 trainer.py:518] Training: step 10435
I0513 00:57:34.882788 139856202532864 trainer.py:518] Training: step 10436
I0513 00:57:34.882021 139876724979712 trainer.py:518] Training: step 10436
I0513 00:57:34.882189 140193652815872 trainer.py:518] Training: step 10436
I0513 00:57:34.881943 140143678494720 trainer.py:518] Training: step 10436
I0513 00:57:34.883062 140643590531072 trainer.py:518] Training: step 10436
I0513 00:57:34.882897 140260698863616 trainer.py:518] Training: step 10436
I0513 00:57:43.822743 140180528838656 trainer.py:518] Training: step 10436
I0513 00:57:43.825113 139666521167872 trainer.py:518] Training: step 10436
I0513 00:57:52.759403 139856202532864 trainer.py:518] Training: step 10437
I0513 00:57:52.759212 139876724979712 trainer.py:518] Training: step 10437
I0513 00:57:52.759168 140193652815872 trainer.py:518] Training: step 10437
I0513 00:57:52.760125 140643590531072 trainer.py:518] Training: step 10437
I0513 00:57:52.759549 140143678494720 trainer.py:518] Training: step 10437
I0513 00:57:52.759611 140260698863616 trainer.py:518] Training: step 10437
I0513 00:58:10.637252 139856202532864 trainer.py:518] Training: step 10438
I0513 00:58:10.636956 140193652815872 trainer.py:518] Training: step 10438
I0513 00:58:10.636988 139876724979712 trainer.py:518] Training: step 10438
I0513 00:58:10.637824 140643590531072 trainer.py:518] Training: step 10438
I0513 00:58:10.638481 140180528838656 trainer.py:518] Training: step 10438
I0513 00:58:10.637389 140143678494720 trainer.py:518] Training: step 10438
I0513 00:58:10.643045 139666521167872 trainer.py:518] Training: step 10438
I0513 00:58:10.637450 140260698863616 trainer.py:518] Training: step 10438
I0513 00:58:19.577291 140055069881920 logging_writer.py:48] [10420] collection=train accuracy=0.521689, cross_ent_loss=19.47713851928711, cross_ent_loss_per_all_target_tokens=2.47665e-05, experts/auxiliary_loss=0.1397607922554016, experts/expert_usage=7.863063335418701, experts/fraction_tokens_left_behind=2.357386827468872, experts/router_confidence=2.5683553218841553, experts/router_z_loss=0.0018031143117696047, learning_rate=0.00979898, learning_rate/current=0.00979686, loss=19.70119285583496, loss_per_all_target_tokens=2.50514e-05, loss_per_nonpadding_target_token=6.09503e-05, non_padding_fraction/loss_weights=0.411013, timing/seconds=89.38919067382812, timing/seqs=3840, timing/seqs_per_second=42.958213806152344, timing/seqs_per_second_per_core=1.3424441814422607, timing/steps_per_second=0.11187034845352173, timing/target_tokens_per_second=87978.421875, timing/target_tokens_per_second_per_core=2749.32568359375, timing/uptime=4207.83, z_loss=0.08249055594205856, z_loss_per_all_target_tokens=1.04892e-07
I0513 00:58:28.516343 139856202532864 trainer.py:518] Training: step 10439
I0513 00:58:28.517194 139666521167872 trainer.py:518] Training: step 10439
I0513 00:58:28.515699 139876724979712 trainer.py:518] Training: step 10439
I0513 00:58:28.516438 140193652815872 trainer.py:518] Training: step 10439
I0513 00:58:28.516812 140180528838656 trainer.py:518] Training: step 10439
I0513 00:58:28.517012 140643590531072 trainer.py:518] Training: step 10439
I0513 00:58:28.516463 140143678494720 trainer.py:518] Training: step 10439
I0513 00:58:28.515963 140260698863616 trainer.py:518] Training: step 10439
I0513 00:58:38.533698 140180528838656 trainer.py:518] Training: step 10446
I0513 00:58:38.555642 139666521167872 trainer.py:518] Training: step 10446
I0513 00:58:46.394426 139856202532864 trainer.py:518] Training: step 10446
I0513 00:58:46.393924 139876724979712 trainer.py:518] Training: step 10446
I0513 00:58:46.394422 140193652815872 trainer.py:518] Training: step 10446
I0513 00:58:46.394088 140143678494720 trainer.py:518] Training: step 10443
I0513 00:58:46.397285 140055069881920 logging_writer.py:48] [10430] collection=train accuracy=0.519154, cross_ent_loss=19.590503692626953, cross_ent_loss_per_all_target_tokens=2.49106e-05, experts/auxiliary_loss=0.1406736522912979, experts/expert_usage=7.848278045654297, experts/fraction_tokens_left_behind=2.3099117279052734, experts/router_confidence=2.58431339263916, experts/router_z_loss=0.0017816420877352357, learning_rate=0.00979428, learning_rate/current=0.00979216, loss=19.815650939941406, loss_per_all_target_tokens=2.51969e-05, loss_per_nonpadding_target_token=5.97805e-05, non_padding_fraction/loss_weights=0.42149, timing/seconds=89.3867416381836, timing/seqs=3840, timing/seqs_per_second=42.95939254760742, timing/seqs_per_second_per_core=1.342481017112732, timing/steps_per_second=0.11187341809272766, timing/target_tokens_per_second=87980.8359375, timing/target_tokens_per_second_per_core=2749.401123046875, timing/uptime=4225.93, z_loss=0.0826914831995964, z_loss_per_all_target_tokens=1.05148e-07
I0513 00:58:46.425723 140643590531072 trainer.py:518] Training: step 10447
I0513 00:58:46.426425 140260698863616 trainer.py:518] Training: step 10449
I0513 00:58:55.333991 140180528838656 trainer.py:518] Training: step 10450
I0513 00:58:55.337263 139666521167872 trainer.py:518] Training: step 10450
I0513 00:59:22.152070 139856202532864 trainer.py:518] Training: step 10452
I0513 00:59:22.151628 140193652815872 trainer.py:518] Training: step 10452
I0513 00:59:22.151576 139876724979712 trainer.py:518] Training: step 10452
I0513 00:59:22.152285 140643590531072 trainer.py:518] Training: step 10452
I0513 00:59:22.151502 140143678494720 trainer.py:518] Training: step 10452
I0513 00:59:22.153190 140180528838656 trainer.py:518] Training: step 10452
I0513 00:59:22.166864 139666521167872 trainer.py:518] Training: step 10452
I0513 00:59:22.151456 140260698863616 trainer.py:518] Training: step 10452
I0513 00:59:40.028934 139856202532864 trainer.py:518] Training: step 10453
I0513 00:59:40.028644 139876724979712 trainer.py:518] Training: step 10453
I0513 00:59:40.028890 140193652815872 trainer.py:518] Training: step 10453
I0513 00:59:40.030192 140180528838656 trainer.py:518] Training: step 10453
I0513 00:59:40.028924 140143678494720 trainer.py:518] Training: step 10453
I0513 00:59:40.029984 140643590531072 trainer.py:518] Training: step 10453
I0513 00:59:40.036052 139666521167872 trainer.py:518] Training: step 10453
I0513 00:59:40.029032 140260698863616 trainer.py:518] Training: step 10453
I0513 00:59:57.907721 139666521167872 trainer.py:518] Training: step 10454
I0513 00:59:57.907698 139856202532864 trainer.py:518] Training: step 10454
I0513 00:59:57.906873 139876724979712 trainer.py:518] Training: step 10454
I0513 00:59:57.906999 140193652815872 trainer.py:518] Training: step 10454
I0513 00:59:57.908054 140180528838656 trainer.py:518] Training: step 10454
I0513 00:59:57.906900 140143678494720 trainer.py:518] Training: step 10454
I0513 00:59:57.907852 140643590531072 trainer.py:518] Training: step 10454
I0513 00:59:57.907769 140260698863616 trainer.py:518] Training: step 10454
I0513 01:00:15.785067 139856202532864 trainer.py:518] Training: step 10455
I0513 01:00:15.784700 139876724979712 trainer.py:518] Training: step 10455
I0513 01:00:15.784920 140193652815872 trainer.py:518] Training: step 10455
I0513 01:00:15.784451 140143678494720 trainer.py:518] Training: step 10455
I0513 01:00:15.785778 140643590531072 trainer.py:518] Training: step 10455
I0513 01:00:15.786269 140180528838656 trainer.py:518] Training: step 10455
I0513 01:00:15.795433 139666521167872 trainer.py:518] Training: step 10455
I0513 01:00:15.785012 140260698863616 trainer.py:518] Training: step 10455
I0513 01:00:33.664312 139856202532864 trainer.py:518] Training: step 10456
I0513 01:00:33.664892 139666521167872 trainer.py:518] Training: step 10456
I0513 01:00:33.663153 139876724979712 trainer.py:518] Training: step 10456
I0513 01:00:33.663508 140193652815872 trainer.py:518] Training: step 10456
I0513 01:00:33.663989 140180528838656 trainer.py:518] Training: step 10456
I0513 01:00:33.663834 140643590531072 trainer.py:518] Training: step 10456
I0513 01:00:33.662915 140143678494720 trainer.py:518] Training: step 10456
I0513 01:00:33.662943 140260698863616 trainer.py:518] Training: step 10456
I0513 01:00:51.542648 139856202532864 trainer.py:518] Training: step 10457
I0513 01:00:51.542661 140193652815872 trainer.py:518] Training: step 10457
I0513 01:00:51.543081 140180528838656 trainer.py:518] Training: step 10457
I0513 01:00:51.542971 140143678494720 trainer.py:518] Training: step 10457
I0513 01:00:51.542870 140643590531072 trainer.py:518] Training: step 10457
I0513 01:00:51.542746 139876724979712 trainer.py:518] Training: step 10457
I0513 01:00:51.542516 140260698863616 trainer.py:518] Training: step 10457
I0513 01:00:51.553979 139666521167872 trainer.py:518] Training: step 10457
I0513 01:01:09.421002 139856202532864 trainer.py:518] Training: step 10458
I0513 01:01:09.420049 140193652815872 trainer.py:518] Training: step 10458
I0513 01:01:09.420150 140143678494720 trainer.py:518] Training: step 10458
I0513 01:01:09.421033 140643590531072 trainer.py:518] Training: step 10458
I0513 01:01:09.421416 140180528838656 trainer.py:518] Training: step 10458
I0513 01:01:09.420606 139876724979712 trainer.py:518] Training: step 10458
I0513 01:01:09.420111 140260698863616 trainer.py:518] Training: step 10458
I0513 01:01:09.422663 139666521167872 trainer.py:518] Training: step 10458
I0513 01:01:18.360848 140055069881920 logging_writer.py:48] [10440] collection=train accuracy=0.512373, cross_ent_loss=19.8973445892334, cross_ent_loss_per_all_target_tokens=2.53008e-05, experts/auxiliary_loss=0.14160116016864777, experts/expert_usage=7.843262672424316, experts/fraction_tokens_left_behind=2.235058546066284, experts/router_confidence=2.599484443664551, experts/router_z_loss=0.0017441209638491273, learning_rate=0.00978958, learning_rate/current=0.00978747, loss=20.12223243713379, loss_per_all_target_tokens=2.55867e-05, loss_per_nonpadding_target_token=6.25318e-05, non_padding_fraction/loss_weights=0.40918, timing/seconds=89.38960266113281, timing/seqs=3840, timing/seqs_per_second=42.95801544189453, timing/seqs_per_second_per_core=1.342437982559204, timing/steps_per_second=0.11186983436346054, timing/target_tokens_per_second=87978.015625, timing/target_tokens_per_second_per_core=2749.31298828125, timing/uptime=4386.62, z_loss=0.08154328167438507, z_loss_per_all_target_tokens=1.03688e-07
I0513 01:01:27.299742 139856202532864 trainer.py:518] Training: step 10459
I0513 01:01:27.299528 140193652815872 trainer.py:518] Training: step 10459
I0513 01:01:27.299509 140643590531072 trainer.py:518] Training: step 10459
I0513 01:01:27.300022 140180528838656 trainer.py:518] Training: step 10459
I0513 01:01:27.299960 140143678494720 trainer.py:518] Training: step 10459
I0513 01:01:27.299406 139876724979712 trainer.py:518] Training: step 10459
I0513 01:01:27.299828 140260698863616 trainer.py:518] Training: step 10459
I0513 01:01:27.327669 139666521167872 trainer.py:518] Training: step 10459
I0513 01:01:45.176632 139856202532864 trainer.py:518] Training: step 10466
I0513 01:01:45.175974 140193652815872 trainer.py:518] Training: step 10466
I0513 01:01:45.176286 140143678494720 trainer.py:518] Training: step 10463
I0513 01:01:45.177457 140180528838656 trainer.py:518] Training: step 10466
I0513 01:01:45.179573 140055069881920 logging_writer.py:48] [10450] collection=train accuracy=0.517092, cross_ent_loss=19.655227661132812, cross_ent_loss_per_all_target_tokens=2.49929e-05, experts/auxiliary_loss=0.14118008315563202, experts/expert_usage=7.834958553314209, experts/fraction_tokens_left_behind=2.2818806171417236, experts/router_confidence=2.5883195400238037, experts/router_z_loss=0.0017632873496040702, learning_rate=0.0097849, learning_rate/current=0.00978279, loss=19.885051727294922, loss_per_all_target_tokens=2.52851e-05, loss_per_nonpadding_target_token=6.00466e-05, non_padding_fraction/loss_weights=0.421092, timing/seconds=89.38398742675781, timing/seqs=3840, timing/seqs_per_second=42.96071243286133, timing/seqs_per_second_per_core=1.3425222635269165, timing/steps_per_second=0.11187686026096344, timing/target_tokens_per_second=87983.5390625, timing/target_tokens_per_second_per_core=2749.485595703125, timing/uptime=4404.71, z_loss=0.08688022941350937, z_loss_per_all_target_tokens=1.10474e-07
I0513 01:01:45.176775 139876724979712 trainer.py:518] Training: step 10467
I0513 01:01:45.207940 140643590531072 trainer.py:518] Training: step 10467
I0513 01:01:45.211740 140260698863616 trainer.py:518] Training: step 10469
I0513 01:01:45.333645 139666521167872 trainer.py:518] Training: step 10466
I0513 01:02:20.934455 139856202532864 trainer.py:518] Training: step 10472
I0513 01:02:20.934297 140193652815872 trainer.py:518] Training: step 10472
I0513 01:02:20.933779 140143678494720 trainer.py:518] Training: step 10472
I0513 01:02:20.934520 140643590531072 trainer.py:518] Training: step 10472
I0513 01:02:20.935657 140180528838656 trainer.py:518] Training: step 10472
I0513 01:02:20.934123 140260698863616 trainer.py:518] Training: step 10472
I0513 01:02:20.935165 139876724979712 trainer.py:518] Training: step 10472
I0513 01:02:20.934980 139666521167872 trainer.py:518] Training: step 10472
I0513 01:02:38.812642 139856202532864 trainer.py:518] Training: step 10473
I0513 01:02:38.812414 140193652815872 trainer.py:518] Training: step 10473
I0513 01:02:38.812006 140143678494720 trainer.py:518] Training: step 10473
I0513 01:02:38.813244 140180528838656 trainer.py:518] Training: step 10473
I0513 01:02:38.813288 140643590531072 trainer.py:518] Training: step 10473
I0513 01:02:38.813072 139876724979712 trainer.py:518] Training: step 10473
I0513 01:02:38.812331 140260698863616 trainer.py:518] Training: step 10473
I0513 01:02:38.814738 139666521167872 trainer.py:518] Training: step 10473
I0513 01:02:56.691490 139856202532864 trainer.py:518] Training: step 10474
I0513 01:02:56.690418 140143678494720 trainer.py:518] Training: step 10474
I0513 01:02:56.691151 140193652815872 trainer.py:518] Training: step 10474
I0513 01:02:56.690947 140643590531072 trainer.py:518] Training: step 10474
I0513 01:02:56.691938 140180528838656 trainer.py:518] Training: step 10474
I0513 01:02:56.691174 139876724979712 trainer.py:518] Training: step 10474
I0513 01:02:56.690485 140260698863616 trainer.py:518] Training: step 10474
I0513 01:02:56.703400 139666521167872 trainer.py:518] Training: step 10474
I0513 01:03:14.569476 139856202532864 trainer.py:518] Training: step 10475
I0513 01:03:14.569237 140193652815872 trainer.py:518] Training: step 10475
I0513 01:03:14.568906 140143678494720 trainer.py:518] Training: step 10475
I0513 01:03:14.569508 140643590531072 trainer.py:518] Training: step 10475
I0513 01:03:14.570025 140180528838656 trainer.py:518] Training: step 10475
I0513 01:03:14.569447 139876724979712 trainer.py:518] Training: step 10475
I0513 01:03:14.571952 139666521167872 trainer.py:518] Training: step 10475
I0513 01:03:23.506946 140260698863616 trainer.py:518] Training: step 10475
I0513 01:03:32.447330 139856202532864 trainer.py:518] Training: step 10476
I0513 01:03:32.446314 140193652815872 trainer.py:518] Training: step 10476
I0513 01:03:32.445899 140143678494720 trainer.py:518] Training: step 10476
I0513 01:03:32.447017 140643590531072 trainer.py:518] Training: step 10476
I0513 01:03:32.447523 140180528838656 trainer.py:518] Training: step 10476
I0513 01:03:32.446790 139876724979712 trainer.py:518] Training: step 10476
I0513 01:03:32.451383 139666521167872 trainer.py:518] Training: step 10476
I0513 01:03:41.386458 140260698863616 trainer.py:518] Training: step 10476
I0513 01:03:50.324879 139856202532864 trainer.py:518] Training: step 10477
I0513 01:03:50.324339 140193652815872 trainer.py:518] Training: step 10477
I0513 01:03:50.324912 140643590531072 trainer.py:518] Training: step 10477
I0513 01:03:50.324803 140143678494720 trainer.py:518] Training: step 10477
I0513 01:03:50.325351 140180528838656 trainer.py:518] Training: step 10477
I0513 01:03:50.324780 139876724979712 trainer.py:518] Training: step 10477
I0513 01:03:50.328681 139666521167872 trainer.py:518] Training: step 10477
I0513 01:04:08.203531 139856202532864 trainer.py:518] Training: step 10478
I0513 01:04:08.203045 140193652815872 trainer.py:518] Training: step 10478
I0513 01:04:08.202635 140143678494720 trainer.py:518] Training: step 10478
I0513 01:04:08.203339 140643590531072 trainer.py:518] Training: step 10478
I0513 01:04:08.204191 140180528838656 trainer.py:518] Training: step 10478
I0513 01:04:08.203426 139876724979712 trainer.py:518] Training: step 10478
I0513 01:04:08.203157 140260698863616 trainer.py:518] Training: step 10478
I0513 01:04:08.224858 139666521167872 trainer.py:518] Training: step 10478
I0513 01:04:17.141955 140055069881920 logging_writer.py:48] [10460] collection=train accuracy=0.509139, cross_ent_loss=20.046804428100586, cross_ent_loss_per_all_target_tokens=2.54908e-05, experts/auxiliary_loss=0.1410965472459793, experts/expert_usage=7.8548736572265625, experts/fraction_tokens_left_behind=2.2863123416900635, experts/router_confidence=2.5947959423065186, experts/router_z_loss=0.001763387001119554, learning_rate=0.00978021, learning_rate/current=0.00977811, loss=20.273744583129883, loss_per_all_target_tokens=2.57794e-05, loss_per_nonpadding_target_token=6.35661e-05, non_padding_fraction/loss_weights=0.405553, timing/seconds=89.391845703125, timing/seqs=3840, timing/seqs_per_second=42.956939697265625, timing/seqs_per_second_per_core=1.3424043655395508, timing/steps_per_second=0.1118670329451561, timing/target_tokens_per_second=87975.8125, timing/target_tokens_per_second_per_core=2749.244140625, timing/uptime=4565.4, z_loss=0.08407916873693466, z_loss_per_all_target_tokens=1.06912e-07
I0513 01:04:26.082903 139856202532864 trainer.py:518] Training: step 10479
I0513 01:04:26.082559 140643590531072 trainer.py:518] Training: step 10479
I0513 01:04:26.083015 140193652815872 trainer.py:518] Training: step 10479
I0513 01:04:26.083107 140180528838656 trainer.py:518] Training: step 10479
I0513 01:04:26.083365 140143678494720 trainer.py:518] Training: step 10479
I0513 01:04:26.083009 139876724979712 trainer.py:518] Training: step 10479
I0513 01:04:26.082215 140260698863616 trainer.py:518] Training: step 10479
I0513 01:04:26.082629 139666521167872 trainer.py:518] Training: step 10479
I0513 01:04:43.959258 140143678494720 trainer.py:518] Training: step 10483
I0513 01:04:43.959919 140193652815872 trainer.py:518] Training: step 10485
I0513 01:04:43.959783 140643590531072 trainer.py:518] Training: step 10486
I0513 01:04:43.960680 140180528838656 trainer.py:518] Training: step 10487
I0513 01:04:43.962343 140055069881920 logging_writer.py:48] [10470] collection=train accuracy=0.518909, cross_ent_loss=19.509403228759766, cross_ent_loss_per_all_target_tokens=2.48075e-05, experts/auxiliary_loss=0.14026066660881042, experts/expert_usage=7.881813049316406, experts/fraction_tokens_left_behind=2.322293758392334, experts/router_confidence=2.5772368907928467, experts/router_z_loss=0.0017811462748795748, learning_rate=0.00977554, learning_rate/current=0.00977344, loss=19.732797622680664, loss_per_all_target_tokens=2.50915e-05, loss_per_nonpadding_target_token=6.22107e-05, non_padding_fraction/loss_weights=0.403332, timing/seconds=89.38392639160156, timing/seqs=3840, timing/seqs_per_second=42.96074676513672, timing/seqs_per_second_per_core=1.3425233364105225, timing/steps_per_second=0.11187694221735, timing/target_tokens_per_second=87983.609375, timing/target_tokens_per_second_per_core=2749.48779296875, timing/uptime=4583.49, z_loss=0.08135324716567993, z_loss_per_all_target_tokens=1.03446e-07
I0513 01:04:43.961280 139666521167872 trainer.py:518] Training: step 10482
I0513 01:04:43.990420 139856202532864 trainer.py:518] Training: step 10485
I0513 01:04:43.989970 139876724979712 trainer.py:518] Training: step 10485
I0513 01:04:52.898889 140260698863616 trainer.py:518] Training: step 10490
I0513 01:05:19.717040 139856202532864 trainer.py:518] Training: step 10492
I0513 01:05:19.716320 140193652815872 trainer.py:518] Training: step 10492
I0513 01:05:19.715992 140143678494720 trainer.py:518] Training: step 10492
I0513 01:05:19.716957 140643590531072 trainer.py:518] Training: step 10492
I0513 01:05:19.717370 140180528838656 trainer.py:518] Training: step 10492
I0513 01:05:19.716700 139876724979712 trainer.py:518] Training: step 10492
I0513 01:05:19.716156 140260698863616 trainer.py:518] Training: step 10492
I0513 01:05:19.717736 139666521167872 trainer.py:518] Training: step 10492
I0513 01:05:37.594420 139856202532864 trainer.py:518] Training: step 10493
I0513 01:05:37.593389 140143678494720 trainer.py:518] Training: step 10493
I0513 01:05:37.594160 140193652815872 trainer.py:518] Training: step 10493
I0513 01:05:37.594093 140643590531072 trainer.py:518] Training: step 10493
I0513 01:05:37.594368 139876724979712 trainer.py:518] Training: step 10493
I0513 01:05:37.593941 140260698863616 trainer.py:518] Training: step 10493
I0513 01:05:37.610545 139666521167872 trainer.py:518] Training: step 10493
I0513 01:05:46.534869 140180528838656 trainer.py:518] Training: step 10493
I0513 01:05:55.473002 139856202532864 trainer.py:518] Training: step 10494
I0513 01:05:55.472270 140193652815872 trainer.py:518] Training: step 10494
I0513 01:05:55.472366 140143678494720 trainer.py:518] Training: step 10494
I0513 01:05:55.472806 140643590531072 trainer.py:518] Training: step 10494
I0513 01:05:55.472856 139876724979712 trainer.py:518] Training: step 10494
I0513 01:05:55.472476 140260698863616 trainer.py:518] Training: step 10494
I0513 01:05:55.477064 139666521167872 trainer.py:518] Training: step 10494
I0513 01:06:04.413068 140180528838656 trainer.py:518] Training: step 10494
I0513 01:06:13.350805 139856202532864 trainer.py:518] Training: step 10495
I0513 01:06:13.349995 140143678494720 trainer.py:518] Training: step 10495
I0513 01:06:13.350541 140193652815872 trainer.py:518] Training: step 10495
I0513 01:06:13.351370 140643590531072 trainer.py:518] Training: step 10495
I0513 01:06:13.350937 139876724979712 trainer.py:518] Training: step 10495
I0513 01:06:13.350378 140260698863616 trainer.py:518] Training: step 10495
I0513 01:06:13.354841 139666521167872 trainer.py:518] Training: step 10495
I0513 01:06:22.289456 140180528838656 trainer.py:518] Training: step 10495
I0513 01:06:31.228693 140193652815872 trainer.py:518] Training: step 10496
I0513 01:06:31.229872 139856202532864 trainer.py:518] Training: step 10496
I0513 01:06:31.228845 140143678494720 trainer.py:518] Training: step 10496
I0513 01:06:31.229138 140643590531072 trainer.py:518] Training: step 10496
I0513 01:06:31.229673 139876724979712 trainer.py:518] Training: step 10496
I0513 01:06:31.228687 140260698863616 trainer.py:518] Training: step 10496
I0513 01:06:31.231382 139666521167872 trainer.py:518] Training: step 10496
I0513 01:06:40.169288 140180528838656 trainer.py:518] Training: step 10496
I0513 01:06:49.106573 139856202532864 trainer.py:518] Training: step 10497
I0513 01:06:49.106279 140193652815872 trainer.py:518] Training: step 10497
I0513 01:06:49.105999 140143678494720 trainer.py:518] Training: step 10497
I0513 01:06:49.106399 140643590531072 trainer.py:518] Training: step 10497
I0513 01:06:49.106814 139876724979712 trainer.py:518] Training: step 10497
I0513 01:06:49.106256 140260698863616 trainer.py:518] Training: step 10497
I0513 01:06:49.107518 139666521167872 trainer.py:518] Training: step 10497
I0513 01:07:06.984441 139856202532864 trainer.py:518] Training: step 10498
I0513 01:07:06.983800 140193652815872 trainer.py:518] Training: step 10498
I0513 01:07:06.983659 140143678494720 trainer.py:518] Training: step 10498
I0513 01:07:06.984473 140643590531072 trainer.py:518] Training: step 10498
I0513 01:07:06.985184 140180528838656 trainer.py:518] Training: step 10498
I0513 01:07:06.984714 139876724979712 trainer.py:518] Training: step 10498
I0513 01:07:06.984116 140260698863616 trainer.py:518] Training: step 10498
I0513 01:07:06.989992 139666521167872 trainer.py:518] Training: step 10498
I0513 01:07:15.924419 140055069881920 logging_writer.py:48] [10480] collection=train accuracy=0.514105, cross_ent_loss=19.727924346923828, cross_ent_loss_per_all_target_tokens=2.50854e-05, experts/auxiliary_loss=0.1400059461593628, experts/expert_usage=7.859509468078613, experts/fraction_tokens_left_behind=2.3229448795318604, experts/router_confidence=2.56498122215271, experts/router_z_loss=0.001777252065949142, learning_rate=0.00977087, learning_rate/current=0.00976877, loss=19.951868057250977, loss_per_all_target_tokens=2.53701e-05, loss_per_nonpadding_target_token=6.30511e-05, non_padding_fraction/loss_weights=0.402374, timing/seconds=89.3880386352539, timing/seqs=3840, timing/seqs_per_second=42.95876693725586, timing/seqs_per_second_per_core=1.3424614667892456, timing/steps_per_second=0.11187179386615753, timing/target_tokens_per_second=87979.5546875, timing/target_tokens_per_second_per_core=2749.361083984375, timing/uptime=4744.18, z_loss=0.08215954154729843, z_loss_per_all_target_tokens=1.04471e-07
I0513 01:07:24.863687 139856202532864 trainer.py:518] Training: step 10499
I0513 01:07:24.863463 140643590531072 trainer.py:518] Training: step 10499
I0513 01:07:24.863828 140193652815872 trainer.py:518] Training: step 10499
I0513 01:07:24.863927 140143678494720 trainer.py:518] Training: step 10499
I0513 01:07:24.864385 140180528838656 trainer.py:518] Training: step 10499
I0513 01:07:24.863677 139876724979712 trainer.py:518] Training: step 10499
I0513 01:07:24.863619 140260698863616 trainer.py:518] Training: step 10499
I0513 01:07:24.870815 139666521167872 trainer.py:518] Training: step 10499
I0513 01:07:42.740763 140193652815872 trainer.py:518] Training: step 10505
I0513 01:07:42.742468 139856202532864 trainer.py:518] Training: step 10505
I0513 01:07:42.741977 140143678494720 trainer.py:518] Training: step 10502
I0513 01:07:42.745343 140055069881920 logging_writer.py:48] [10490] collection=train accuracy=0.515476, cross_ent_loss=19.731592178344727, cross_ent_loss_per_all_target_tokens=2.509e-05, experts/auxiliary_loss=0.139823317527771, experts/expert_usage=7.8398919105529785, experts/fraction_tokens_left_behind=2.3533096313476562, experts/router_confidence=2.5711963176727295, experts/router_z_loss=0.0017945269355550408, learning_rate=0.00976621, learning_rate/current=0.00976412, loss=19.956266403198242, loss_per_all_target_tokens=2.53757e-05, loss_per_nonpadding_target_token=6.22521e-05, non_padding_fraction/loss_weights=0.407628, timing/seconds=89.38457489013672, timing/seqs=3840, timing/seqs_per_second=42.96043395996094, timing/seqs_per_second_per_core=1.3425135612487793, timing/steps_per_second=0.11187613010406494, timing/target_tokens_per_second=87982.96875, timing/target_tokens_per_second_per_core=2749.4677734375, timing/uptime=4762.28, z_loss=0.08305738121271133, z_loss_per_all_target_tokens=1.05613e-07
I0513 01:07:42.741520 139876724979712 trainer.py:518] Training: step 10505
I0513 01:07:42.740970 140260698863616 trainer.py:518] Training: step 10505
I0513 01:07:42.774230 140643590531072 trainer.py:518] Training: step 10507
I0513 01:07:42.826911 139666521167872 trainer.py:518] Training: step 10508
I0513 01:07:51.680633 140180528838656 trainer.py:518] Training: step 10510
I0513 01:08:18.499950 139856202532864 trainer.py:518] Training: step 10512
I0513 01:08:18.499357 140193652815872 trainer.py:518] Training: step 10512
I0513 01:08:18.499195 140180528838656 trainer.py:518] Training: step 10512
I0513 01:08:18.499383 140643590531072 trainer.py:518] Training: step 10512
I0513 01:08:18.499265 140143678494720 trainer.py:518] Training: step 10512
I0513 01:08:18.499250 139876724979712 trainer.py:518] Training: step 10512
I0513 01:08:18.498723 140260698863616 trainer.py:518] Training: step 10512
I0513 01:08:18.499654 139666521167872 trainer.py:518] Training: step 10512
I0513 01:08:36.377511 139856202532864 trainer.py:518] Training: step 10513
I0513 01:08:36.376298 140143678494720 trainer.py:518] Training: step 10513
I0513 01:08:36.376661 140193652815872 trainer.py:518] Training: step 10513
I0513 01:08:36.376804 140180528838656 trainer.py:518] Training: step 10513
I0513 01:08:36.376555 140643590531072 trainer.py:518] Training: step 10513
I0513 01:08:36.377207 139876724979712 trainer.py:518] Training: step 10513
I0513 01:08:36.376206 140260698863616 trainer.py:518] Training: step 10513
I0513 01:08:36.377338 139666521167872 trainer.py:518] Training: step 10513
I0513 01:08:54.255927 139856202532864 trainer.py:518] Training: step 10514
I0513 01:08:54.255192 140193652815872 trainer.py:518] Training: step 10514
I0513 01:08:54.255492 140180528838656 trainer.py:518] Training: step 10514
I0513 01:08:54.254893 140143678494720 trainer.py:518] Training: step 10514
I0513 01:08:54.255649 140643590531072 trainer.py:518] Training: step 10514
I0513 01:08:54.255305 139876724979712 trainer.py:518] Training: step 10514
I0513 01:08:54.254730 140260698863616 trainer.py:518] Training: step 10514
I0513 01:08:54.255788 139666521167872 trainer.py:518] Training: step 10514
I0513 01:09:12.133655 139856202532864 trainer.py:518] Training: step 10515
I0513 01:09:12.133033 140193652815872 trainer.py:518] Training: step 10515
I0513 01:09:12.132704 140143678494720 trainer.py:518] Training: step 10515
I0513 01:09:12.133863 140180528838656 trainer.py:518] Training: step 10515
I0513 01:09:12.133834 140643590531072 trainer.py:518] Training: step 10515
I0513 01:09:12.133444 139876724979712 trainer.py:518] Training: step 10515
I0513 01:09:12.132810 140260698863616 trainer.py:518] Training: step 10515
I0513 01:09:12.140833 139666521167872 trainer.py:518] Training: step 10515
I0513 01:09:30.010293 140193652815872 trainer.py:518] Training: step 10516
I0513 01:09:30.011183 139856202532864 trainer.py:518] Training: step 10516
I0513 01:09:30.010242 140143678494720 trainer.py:518] Training: step 10516
I0513 01:09:30.010616 140180528838656 trainer.py:518] Training: step 10516
I0513 01:09:30.011132 140643590531072 trainer.py:518] Training: step 10516
I0513 01:09:30.010259 139876724979712 trainer.py:518] Training: step 10516
I0513 01:09:30.010538 140260698863616 trainer.py:518] Training: step 10516
I0513 01:09:30.011385 139666521167872 trainer.py:518] Training: step 10516
I0513 01:09:47.889297 139856202532864 trainer.py:518] Training: step 10517
I0513 01:09:47.888460 140180528838656 trainer.py:518] Training: step 10517
I0513 01:09:47.888656 140193652815872 trainer.py:518] Training: step 10517
I0513 01:09:47.888780 140143678494720 trainer.py:518] Training: step 10517
I0513 01:09:47.889572 140643590531072 trainer.py:518] Training: step 10517
I0513 01:09:47.888217 139876724979712 trainer.py:518] Training: step 10517
I0513 01:09:47.888253 140260698863616 trainer.py:518] Training: step 10517
I0513 01:09:47.897809 139666521167872 trainer.py:518] Training: step 10517
I0513 01:10:05.768674 139856202532864 trainer.py:518] Training: step 10518
I0513 01:10:05.767914 140193652815872 trainer.py:518] Training: step 10518
I0513 01:10:05.767716 140143678494720 trainer.py:518] Training: step 10518
I0513 01:10:05.768502 140180528838656 trainer.py:518] Training: step 10518
I0513 01:10:05.768882 140643590531072 trainer.py:518] Training: step 10518
I0513 01:10:05.767497 139876724979712 trainer.py:518] Training: step 10518
I0513 01:10:05.767855 140260698863616 trainer.py:518] Training: step 10518
I0513 01:10:05.776025 139666521167872 trainer.py:518] Training: step 10518
I0513 01:10:14.707328 140055069881920 logging_writer.py:48] [10500] collection=train accuracy=0.530658, cross_ent_loss=18.953353881835938, cross_ent_loss_per_all_target_tokens=2.41004e-05, experts/auxiliary_loss=0.14095543324947357, experts/expert_usage=7.822117805480957, experts/fraction_tokens_left_behind=2.2886836528778076, experts/router_confidence=2.5860273838043213, experts/router_z_loss=0.0017671637469902635, learning_rate=0.00976156, learning_rate/current=0.00975947, loss=19.177345275878906, loss_per_all_target_tokens=2.43853e-05, loss_per_nonpadding_target_token=5.90087e-05, non_padding_fraction/loss_weights=0.413249, timing/seconds=89.38787078857422, timing/seqs=3840, timing/seqs_per_second=42.9588508605957, timing/seqs_per_second_per_core=1.3424640893936157, timing/steps_per_second=0.11187200248241425, timing/target_tokens_per_second=87979.7265625, timing/target_tokens_per_second_per_core=2749.366455078125, timing/uptime=4923.27, z_loss=0.08126947283744812, z_loss_per_all_target_tokens=1.03339e-07
I0513 01:10:23.647274 139856202532864 trainer.py:518] Training: step 10519
I0513 01:10:23.646707 140193652815872 trainer.py:518] Training: step 10519
I0513 01:10:23.647020 140180528838656 trainer.py:518] Training: step 10519
I0513 01:10:23.647070 140643590531072 trainer.py:518] Training: step 10519
I0513 01:10:23.647312 140143678494720 trainer.py:518] Training: step 10519
I0513 01:10:23.646321 139876724979712 trainer.py:518] Training: step 10519
I0513 01:10:23.646401 140260698863616 trainer.py:518] Training: step 10519
I0513 01:10:23.666905 139666521167872 trainer.py:518] Training: step 10519
I0513 01:10:41.525915 139856202532864 trainer.py:518] Training: step 10525
I0513 01:10:41.525190 140055069881920 logging_writer.py:48] [10510] collection=train accuracy=0.507312, cross_ent_loss=20.074012756347656, cross_ent_loss_per_all_target_tokens=2.55254e-05, experts/auxiliary_loss=0.14024917781352997, experts/expert_usage=7.851504802703857, experts/fraction_tokens_left_behind=2.3012313842773438, experts/router_confidence=2.576366901397705, experts/router_z_loss=0.0017745919758453965, learning_rate=0.00975691, learning_rate/current=0.00975482, loss=20.302045822143555, loss_per_all_target_tokens=2.58154e-05, loss_per_nonpadding_target_token=6.35756e-05, non_padding_fraction/loss_weights=0.406058, timing/seconds=89.38513946533203, timing/seqs=3840, timing/seqs_per_second=42.96015930175781, timing/seqs_per_second_per_core=1.3425049781799316, timing/steps_per_second=0.11187541484832764, timing/target_tokens_per_second=87982.40625, timing/target_tokens_per_second_per_core=2749.4501953125, timing/uptime=4941.09, z_loss=0.08600465208292007, z_loss_per_all_target_tokens=1.09361e-07
I0513 01:10:41.524695 140260698863616 trainer.py:518] Training: step 10529
I0513 01:10:41.530781 139666521167872 trainer.py:518] Training: step 10525
I0513 01:10:41.551677 140143678494720 trainer.py:518] Training: step 10524
I0513 01:10:41.553922 140193652815872 trainer.py:518] Training: step 10527
I0513 01:10:41.557164 140643590531072 trainer.py:518] Training: step 10525
I0513 01:10:41.560007 140180528838656 trainer.py:518] Training: step 10527
I0513 01:10:41.554938 139876724979712 trainer.py:518] Training: step 10527
I0513 01:11:17.283009 139856202532864 trainer.py:518] Training: step 10532
I0513 01:11:17.282142 140193652815872 trainer.py:518] Training: step 10532
I0513 01:11:17.282055 140643590531072 trainer.py:518] Training: step 10532
I0513 01:11:17.282762 140180528838656 trainer.py:518] Training: step 10532
I0513 01:11:17.282209 140143678494720 trainer.py:518] Training: step 10532
I0513 01:11:17.281781 140260698863616 trainer.py:518] Training: step 10532
I0513 01:11:17.282697 139876724979712 trainer.py:518] Training: step 10532
I0513 01:11:17.283908 139666521167872 trainer.py:518] Training: step 10532
I0513 01:11:35.160385 140193652815872 trainer.py:518] Training: step 10533
I0513 01:11:35.161512 139856202532864 trainer.py:518] Training: step 10533
I0513 01:11:35.160796 140180528838656 trainer.py:518] Training: step 10533
I0513 01:11:35.160691 140143678494720 trainer.py:518] Training: step 10533
I0513 01:11:35.160926 140643590531072 trainer.py:518] Training: step 10533
I0513 01:11:35.160654 139876724979712 trainer.py:518] Training: step 10533
I0513 01:11:35.161066 139666521167872 trainer.py:518] Training: step 10533
I0513 01:11:44.099887 140260698863616 trainer.py:518] Training: step 10533
I0513 01:11:53.039863 139856202532864 trainer.py:518] Training: step 10534
I0513 01:11:53.039206 140193652815872 trainer.py:518] Training: step 10534
I0513 01:11:53.039437 140180528838656 trainer.py:518] Training: step 10534
I0513 01:11:53.039003 140143678494720 trainer.py:518] Training: step 10534
I0513 01:11:53.039378 140643590531072 trainer.py:518] Training: step 10534
I0513 01:11:53.039508 139876724979712 trainer.py:518] Training: step 10534
I0513 01:11:53.039953 139666521167872 trainer.py:518] Training: step 10534
I0513 01:12:01.977678 140260698863616 trainer.py:518] Training: step 10534
I0513 01:12:10.917623 139856202532864 trainer.py:518] Training: step 10535
I0513 01:12:10.916690 140180528838656 trainer.py:518] Training: step 10535
I0513 01:12:10.916641 140193652815872 trainer.py:518] Training: step 10535
I0513 01:12:10.916697 140143678494720 trainer.py:518] Training: step 10535
I0513 01:12:10.916924 140643590531072 trainer.py:518] Training: step 10535
I0513 01:12:10.917019 139876724979712 trainer.py:518] Training: step 10535
I0513 01:12:10.936516 139666521167872 trainer.py:518] Training: step 10535
I0513 01:12:19.858855 140260698863616 trainer.py:518] Training: step 10535
I0513 01:12:28.795974 139856202532864 trainer.py:518] Training: step 10536
I0513 01:12:28.794862 140193652815872 trainer.py:518] Training: step 10536
I0513 01:12:28.795097 140143678494720 trainer.py:518] Training: step 10536
I0513 01:12:28.795656 140180528838656 trainer.py:518] Training: step 10536
I0513 01:12:28.795608 140643590531072 trainer.py:518] Training: step 10536
I0513 01:12:28.795542 139876724979712 trainer.py:518] Training: step 10536
I0513 01:12:28.795769 139666521167872 trainer.py:518] Training: step 10536
I0513 01:12:37.734886 140260698863616 trainer.py:518] Training: step 10536
I0513 01:12:46.673912 139856202532864 trainer.py:518] Training: step 10537
I0513 01:12:46.672679 140193652815872 trainer.py:518] Training: step 10537
I0513 01:12:46.672919 140180528838656 trainer.py:518] Training: step 10537
I0513 01:12:46.672719 140643590531072 trainer.py:518] Training: step 10537
I0513 01:12:46.672729 140143678494720 trainer.py:518] Training: step 10537
I0513 01:12:46.672627 139876724979712 trainer.py:518] Training: step 10537
I0513 01:12:46.673217 139666521167872 trainer.py:518] Training: step 10537
I0513 01:13:04.552061 139856202532864 trainer.py:518] Training: step 10538
I0513 01:13:04.551015 140193652815872 trainer.py:518] Training: step 10538
I0513 01:13:04.551070 140643590531072 trainer.py:518] Training: step 10538
I0513 01:13:04.551458 140180528838656 trainer.py:518] Training: step 10538
I0513 01:13:04.551414 140143678494720 trainer.py:518] Training: step 10538
I0513 01:13:04.551140 139876724979712 trainer.py:518] Training: step 10538
I0513 01:13:04.550844 140260698863616 trainer.py:518] Training: step 10538
I0513 01:13:04.551259 139666521167872 trainer.py:518] Training: step 10538
I0513 01:13:13.490889 140055069881920 logging_writer.py:48] [10520] collection=train accuracy=0.525128, cross_ent_loss=19.17018699645996, cross_ent_loss_per_all_target_tokens=2.43762e-05, experts/auxiliary_loss=0.13995349407196045, experts/expert_usage=7.8785858154296875, experts/fraction_tokens_left_behind=2.301326036453247, experts/router_confidence=2.5735831260681152, experts/router_z_loss=0.001779713318683207, learning_rate=0.00975227, learning_rate/current=0.00975018, loss=19.392995834350586, loss_per_all_target_tokens=2.46595e-05, loss_per_nonpadding_target_token=6.14656e-05, non_padding_fraction/loss_weights=0.401191, timing/seconds=89.38983917236328, timing/seqs=3840, timing/seqs_per_second=42.95790100097656, timing/seqs_per_second_per_core=1.3424344062805176, timing/steps_per_second=0.11186953634023666, timing/target_tokens_per_second=87977.78125, timing/target_tokens_per_second_per_core=2749.3056640625, timing/uptime=5101.75, z_loss=0.08107545226812363, z_loss_per_all_target_tokens=1.03093e-07
I0513 01:13:22.430616 139856202532864 trainer.py:518] Training: step 10539
I0513 01:13:22.430346 140193652815872 trainer.py:518] Training: step 10539
I0513 01:13:22.430462 140643590531072 trainer.py:518] Training: step 10539
I0513 01:13:22.430647 140143678494720 trainer.py:518] Training: step 10539
I0513 01:13:22.431130 140180528838656 trainer.py:518] Training: step 10539
I0513 01:13:22.430019 139876724979712 trainer.py:518] Training: step 10539
I0513 01:13:22.429760 140260698863616 trainer.py:518] Training: step 10539
I0513 01:13:22.429739 139666521167872 trainer.py:518] Training: step 10539
I0513 01:13:32.482171 140643590531072 trainer.py:518] Training: step 10544
I0513 01:13:32.487339 140180528838656 trainer.py:518] Training: step 10544
I0513 01:13:32.483769 139876724979712 trainer.py:518] Training: step 10543
I0513 01:13:32.506104 140193652815872 trainer.py:518] Training: step 10543
I0513 01:13:32.522849 139856202532864 trainer.py:518] Training: step 10544
I0513 01:13:32.519006 140260698863616 trainer.py:518] Training: step 10545
I0513 01:13:32.575781 139666521167872 trainer.py:518] Training: step 10543
I0513 01:13:40.307136 140143678494720 trainer.py:518] Training: step 10543
I0513 01:13:40.310983 140055069881920 logging_writer.py:48] [10530] collection=train accuracy=0.522438, cross_ent_loss=19.359373092651367, cross_ent_loss_per_all_target_tokens=2.46167e-05, experts/auxiliary_loss=0.1406019777059555, experts/expert_usage=7.875481605529785, experts/fraction_tokens_left_behind=2.2851786613464355, experts/router_confidence=2.585718870162964, experts/router_z_loss=0.0017722913762554526, learning_rate=0.00974763, learning_rate/current=0.00974555, loss=19.5821590423584, loss_per_all_target_tokens=2.49e-05, loss_per_nonpadding_target_token=5.97932e-05, non_padding_fraction/loss_weights=0.416435, timing/seconds=89.3853988647461, timing/seqs=3840, timing/seqs_per_second=42.96003723144531, timing/seqs_per_second_per_core=1.342501163482666, timing/steps_per_second=0.11187509447336197, timing/target_tokens_per_second=87982.15625, timing/target_tokens_per_second_per_core=2749.4423828125, timing/uptime=5119.82, z_loss=0.08040725439786911, z_loss_per_all_target_tokens=1.02243e-07
I0513 01:13:49.246551 139856202532864 trainer.py:518] Training: step 10550
I0513 01:13:49.246391 140193652815872 trainer.py:518] Training: step 10550
I0513 01:13:49.247266 140643590531072 trainer.py:518] Training: step 10550
I0513 01:13:49.247977 140180528838656 trainer.py:518] Training: step 10550
I0513 01:13:49.246418 139876724979712 trainer.py:518] Training: step 10550
I0513 01:13:49.246944 140260698863616 trainer.py:518] Training: step 10550
I0513 01:13:49.250120 139666521167872 trainer.py:518] Training: step 10550
I0513 01:14:16.064921 139856202532864 trainer.py:518] Training: step 10552
I0513 01:14:16.064067 140193652815872 trainer.py:518] Training: step 10552
I0513 01:14:16.064325 140180528838656 trainer.py:518] Training: step 10552
I0513 01:14:16.064210 140143678494720 trainer.py:518] Training: step 10552
I0513 01:14:16.064784 140643590531072 trainer.py:518] Training: step 10552
I0513 01:14:16.064229 139876724979712 trainer.py:518] Training: step 10552
I0513 01:14:16.064217 140260698863616 trainer.py:518] Training: step 10552
I0513 01:14:16.064044 139666521167872 trainer.py:518] Training: step 10552
I0513 01:14:33.942082 139856202532864 trainer.py:518] Training: step 10553
I0513 01:14:33.941029 140193652815872 trainer.py:518] Training: step 10553
I0513 01:14:33.940926 140143678494720 trainer.py:518] Training: step 10553
I0513 01:14:33.941506 140180528838656 trainer.py:518] Training: step 10553
I0513 01:14:33.941500 140643590531072 trainer.py:518] Training: step 10553
I0513 01:14:33.941478 139876724979712 trainer.py:518] Training: step 10553
I0513 01:14:33.941035 140260698863616 trainer.py:518] Training: step 10553
I0513 01:14:33.940962 139666521167872 trainer.py:518] Training: step 10553
I0513 01:14:51.820576 139856202532864 trainer.py:518] Training: step 10554
I0513 01:14:51.820041 140180528838656 trainer.py:518] Training: step 10554
I0513 01:14:51.820089 140193652815872 trainer.py:518] Training: step 10554
I0513 01:14:51.820271 140643590531072 trainer.py:518] Training: step 10554
I0513 01:14:51.820041 139876724979712 trainer.py:518] Training: step 10554
I0513 01:14:51.820016 140260698863616 trainer.py:518] Training: step 10554
I0513 01:14:51.819756 139666521167872 trainer.py:518] Training: step 10554
I0513 01:15:00.758681 140143678494720 trainer.py:518] Training: step 10554
I0513 01:15:09.698291 139856202532864 trainer.py:518] Training: step 10555
I0513 01:15:09.697981 140193652815872 trainer.py:518] Training: step 10555
I0513 01:15:09.698837 140643590531072 trainer.py:518] Training: step 10555
I0513 01:15:09.698343 139876724979712 trainer.py:518] Training: step 10555
I0513 01:15:09.698014 140260698863616 trainer.py:518] Training: step 10555
I0513 01:15:09.697805 139666521167872 trainer.py:518] Training: step 10555
I0513 01:15:18.639049 140143678494720 trainer.py:518] Training: step 10555
I0513 01:15:18.640653 140180528838656 trainer.py:518] Training: step 10555
I0513 01:15:27.577359 139856202532864 trainer.py:518] Training: step 10556
I0513 01:15:27.576961 140193652815872 trainer.py:518] Training: step 10556
I0513 01:15:27.577206 140643590531072 trainer.py:518] Training: step 10556
I0513 01:15:27.576533 139876724979712 trainer.py:518] Training: step 10556
I0513 01:15:27.576733 140260698863616 trainer.py:518] Training: step 10556
I0513 01:15:27.576489 139666521167872 trainer.py:518] Training: step 10556
I0513 01:15:36.516456 140180528838656 trainer.py:518] Training: step 10556
I0513 01:15:36.517006 140143678494720 trainer.py:518] Training: step 10556
I0513 01:15:45.455152 140193652815872 trainer.py:518] Training: step 10557
I0513 01:15:45.455265 139856202532864 trainer.py:518] Training: step 10557
I0513 01:15:45.455836 140643590531072 trainer.py:518] Training: step 10557
I0513 01:15:45.455309 139876724979712 trainer.py:518] Training: step 10557
I0513 01:15:45.454931 140260698863616 trainer.py:518] Training: step 10557
I0513 01:15:45.454992 139666521167872 trainer.py:518] Training: step 10557
I0513 01:16:03.333050 139856202532864 trainer.py:518] Training: step 10558
I0513 01:16:03.333018 140193652815872 trainer.py:518] Training: step 10558
I0513 01:16:03.333309 140180528838656 trainer.py:518] Training: step 10558
I0513 01:16:03.333223 140643590531072 trainer.py:518] Training: step 10558
I0513 01:16:03.333190 140143678494720 trainer.py:518] Training: step 10558
I0513 01:16:03.332522 139876724979712 trainer.py:518] Training: step 10558
I0513 01:16:03.333552 140260698863616 trainer.py:518] Training: step 10558
I0513 01:16:03.332779 139666521167872 trainer.py:518] Training: step 10558
I0513 01:16:12.269956 140055069881920 logging_writer.py:48] [10540] collection=train accuracy=0.511197, cross_ent_loss=19.886783599853516, cross_ent_loss_per_all_target_tokens=2.52874e-05, experts/auxiliary_loss=0.14175519347190857, experts/expert_usage=7.854119300842285, experts/fraction_tokens_left_behind=2.240802526473999, experts/router_confidence=2.6056714057922363, experts/router_z_loss=0.001741250278428197, learning_rate=0.00974301, learning_rate/current=0.00974093, loss=20.112648010253906, loss_per_all_target_tokens=2.55746e-05, loss_per_nonpadding_target_token=6.25293e-05, non_padding_fraction/loss_weights=0.409001, timing/seconds=89.38967895507812, timing/seqs=3840, timing/seqs_per_second=42.95798110961914, timing/seqs_per_second_per_core=1.3424369096755981, timing/steps_per_second=0.11186973750591278, timing/target_tokens_per_second=87977.9453125, timing/target_tokens_per_second_per_core=2749.310791015625, timing/uptime=5280.53, z_loss=0.08236892521381378, z_loss_per_all_target_tokens=1.04738e-07
I0513 01:16:21.210915 139856202532864 trainer.py:518] Training: step 10559
I0513 01:16:21.210740 140180528838656 trainer.py:518] Training: step 10559
I0513 01:16:21.210950 140193652815872 trainer.py:518] Training: step 10559
I0513 01:16:21.211205 140143678494720 trainer.py:518] Training: step 10559
I0513 01:16:21.211953 140643590531072 trainer.py:518] Training: step 10559
I0513 01:16:21.210350 139876724979712 trainer.py:518] Training: step 10559
I0513 01:16:21.210581 140260698863616 trainer.py:518] Training: step 10559
I0513 01:16:21.210609 139666521167872 trainer.py:518] Training: step 10559
I0513 01:16:39.088502 139856202532864 trainer.py:518] Training: step 10565
I0513 01:16:39.086754 140055069881920 logging_writer.py:48] [10550] collection=train accuracy=0.515985, cross_ent_loss=19.73003578186035, cross_ent_loss_per_all_target_tokens=2.5088e-05, experts/auxiliary_loss=0.14172640442848206, experts/expert_usage=7.866937160491943, experts/fraction_tokens_left_behind=2.2435905933380127, experts/router_confidence=2.608366012573242, experts/router_z_loss=0.0017465365817770362, learning_rate=0.00973839, learning_rate/current=0.00973631, loss=19.956449508666992, loss_per_all_target_tokens=2.53759e-05, loss_per_nonpadding_target_token=6.2062e-05, non_padding_fraction/loss_weights=0.40888, timing/seconds=89.38238525390625, timing/seqs=3840, timing/seqs_per_second=42.96148681640625, timing/seqs_per_second_per_core=1.3425464630126953, timing/steps_per_second=0.11187887191772461, timing/target_tokens_per_second=87985.125, timing/target_tokens_per_second_per_core=2749.53515625, timing/uptime=5298.76, z_loss=0.08294007182121277, z_loss_per_all_target_tokens=1.05464e-07
I0513 01:16:39.088311 140643590531072 trainer.py:518] Training: step 10566
I0513 01:16:39.117393 140193652815872 trainer.py:518] Training: step 10566
I0513 01:16:39.118060 139876724979712 trainer.py:518] Training: step 10567
I0513 01:16:39.119295 139666521167872 trainer.py:518] Training: step 10567
I0513 01:16:39.122080 140260698863616 trainer.py:518] Training: step 10567
I0513 01:16:48.027195 140180528838656 trainer.py:518] Training: step 10570
I0513 01:16:48.030833 140143678494720 trainer.py:518] Training: step 10570
I0513 01:17:14.846531 139856202532864 trainer.py:518] Training: step 10572
I0513 01:17:14.845850 140143678494720 trainer.py:518] Training: step 10572
I0513 01:17:14.846466 140180528838656 trainer.py:518] Training: step 10572
I0513 01:17:14.846582 140193652815872 trainer.py:518] Training: step 10572
I0513 01:17:14.846736 140643590531072 trainer.py:518] Training: step 10572
I0513 01:17:14.847018 139876724979712 trainer.py:518] Training: step 10572
I0513 01:17:14.846255 140260698863616 trainer.py:518] Training: step 10572
I0513 01:17:14.846087 139666521167872 trainer.py:518] Training: step 10572
I0513 01:17:32.724516 139856202532864 trainer.py:518] Training: step 10573
I0513 01:17:32.724274 140180528838656 trainer.py:518] Training: step 10573
I0513 01:17:32.724533 140193652815872 trainer.py:518] Training: step 10573
I0513 01:17:32.724131 140143678494720 trainer.py:518] Training: step 10573
I0513 01:17:32.724430 140643590531072 trainer.py:518] Training: step 10573
I0513 01:17:32.724295 139876724979712 trainer.py:518] Training: step 10573
I0513 01:17:32.724050 140260698863616 trainer.py:518] Training: step 10573
I0513 01:17:32.724411 139666521167872 trainer.py:518] Training: step 10573
I0513 01:17:50.602464 139856202532864 trainer.py:518] Training: step 10574
I0513 01:17:50.602276 140180528838656 trainer.py:518] Training: step 10574
I0513 01:17:50.602080 140143678494720 trainer.py:518] Training: step 10574
I0513 01:17:50.602611 140193652815872 trainer.py:518] Training: step 10574
I0513 01:17:50.602804 140643590531072 trainer.py:518] Training: step 10574
I0513 01:17:50.602878 139876724979712 trainer.py:518] Training: step 10574
I0513 01:17:50.602784 140260698863616 trainer.py:518] Training: step 10574
I0513 01:17:50.602631 139666521167872 trainer.py:518] Training: step 10574
I0513 01:18:08.480138 139856202532864 trainer.py:518] Training: step 10575
I0513 01:18:08.479731 140193652815872 trainer.py:518] Training: step 10575
I0513 01:18:08.480194 140180528838656 trainer.py:518] Training: step 10575
I0513 01:18:08.479867 140143678494720 trainer.py:518] Training: step 10575
I0513 01:18:08.480416 140643590531072 trainer.py:518] Training: step 10575
I0513 01:18:08.479809 140260698863616 trainer.py:518] Training: step 10575
I0513 01:18:08.480159 139666521167872 trainer.py:518] Training: step 10575
I0513 01:18:17.421871 139876724979712 trainer.py:518] Training: step 10575
I0513 01:18:26.358892 139856202532864 trainer.py:518] Training: step 10576
I0513 01:18:26.358071 140193652815872 trainer.py:518] Training: step 10576
I0513 01:18:26.358479 140180528838656 trainer.py:518] Training: step 10576
I0513 01:18:26.358220 140143678494720 trainer.py:518] Training: step 10576
I0513 01:18:26.359262 140643590531072 trainer.py:518] Training: step 10576
I0513 01:18:26.358299 140260698863616 trainer.py:518] Training: step 10576
I0513 01:18:26.358484 139666521167872 trainer.py:518] Training: step 10576
I0513 01:18:35.298759 139876724979712 trainer.py:518] Training: step 10576
I0513 01:18:44.238141 139856202532864 trainer.py:518] Training: step 10577
I0513 01:18:44.237544 140193652815872 trainer.py:518] Training: step 10577
I0513 01:18:44.237668 140180528838656 trainer.py:518] Training: step 10577
I0513 01:18:44.237694 140643590531072 trainer.py:518] Training: step 10577
I0513 01:18:44.237939 140143678494720 trainer.py:518] Training: step 10577
I0513 01:18:44.238018 140260698863616 trainer.py:518] Training: step 10577
I0513 01:18:44.237934 139666521167872 trainer.py:518] Training: step 10577
I0513 01:19:02.116695 139856202532864 trainer.py:518] Training: step 10578
I0513 01:19:02.116206 140180528838656 trainer.py:518] Training: step 10578
I0513 01:19:02.116333 140193652815872 trainer.py:518] Training: step 10578
I0513 01:19:02.116677 140643590531072 trainer.py:518] Training: step 10578
I0513 01:19:02.116747 140143678494720 trainer.py:518] Training: step 10578
I0513 01:19:02.116409 139876724979712 trainer.py:518] Training: step 10578
I0513 01:19:02.116288 140260698863616 trainer.py:518] Training: step 10578
I0513 01:19:02.116116 139666521167872 trainer.py:518] Training: step 10578
I0513 01:19:11.055679 140055069881920 logging_writer.py:48] [10560] collection=train accuracy=0.511054, cross_ent_loss=19.914152145385742, cross_ent_loss_per_all_target_tokens=2.53222e-05, experts/auxiliary_loss=0.14129692316055298, experts/expert_usage=7.878413677215576, experts/fraction_tokens_left_behind=2.2329983711242676, experts/router_confidence=2.60017466545105, experts/router_z_loss=0.001747298869304359, learning_rate=0.00973377, learning_rate/current=0.0097317, loss=20.139829635620117, loss_per_all_target_tokens=2.56091e-05, loss_per_nonpadding_target_token=6.30025e-05, non_padding_fraction/loss_weights=0.406478, timing/seconds=89.38652038574219, timing/seqs=3840, timing/seqs_per_second=42.959495544433594, timing/seqs_per_second_per_core=1.3424842357635498, timing/steps_per_second=0.11187368631362915, timing/target_tokens_per_second=87981.046875, timing/target_tokens_per_second_per_core=2749.40771484375, timing/uptime=5459.31, z_loss=0.08263429254293442, z_loss_per_all_target_tokens=1.05075e-07
I0513 01:19:19.995932 139856202532864 trainer.py:518] Training: step 10579
I0513 01:19:19.995480 140193652815872 trainer.py:518] Training: step 10579
I0513 01:19:19.995465 140180528838656 trainer.py:518] Training: step 10579
I0513 01:19:19.995807 140643590531072 trainer.py:518] Training: step 10579
I0513 01:19:19.996421 140143678494720 trainer.py:518] Training: step 10579
I0513 01:19:19.994917 139876724979712 trainer.py:518] Training: step 10579
I0513 01:19:19.995361 140260698863616 trainer.py:518] Training: step 10579
I0513 01:19:19.995649 139666521167872 trainer.py:518] Training: step 10579
I0513 01:19:30.107139 139666521167872 trainer.py:518] Training: step 10585
I0513 01:19:30.243371 139876724979712 trainer.py:518] Training: step 10587
I0513 01:19:30.341222 139856202532864 trainer.py:518] Training: step 10585
I0513 01:19:37.873332 140193652815872 trainer.py:518] Training: step 10585
I0513 01:19:37.873615 140180528838656 trainer.py:518] Training: step 10586
I0513 01:19:37.873677 140055069881920 logging_writer.py:48] [10570] collection=train accuracy=0.529724, cross_ent_loss=18.998693466186523, cross_ent_loss_per_all_target_tokens=2.41581e-05, experts/auxiliary_loss=0.140697181224823, experts/expert_usage=7.887331485748291, experts/fraction_tokens_left_behind=2.26938796043396, experts/router_confidence=2.590066909790039, experts/router_z_loss=0.0017683840123936534, learning_rate=0.00972916, learning_rate/current=0.00972709, loss=19.224454879760742, loss_per_all_target_tokens=2.44452e-05, loss_per_nonpadding_target_token=5.94748e-05, non_padding_fraction/loss_weights=0.411017, timing/seconds=89.38550567626953, timing/seqs=3840, timing/seqs_per_second=42.95998764038086, timing/seqs_per_second_per_core=1.3424996137619019, timing/steps_per_second=0.11187496781349182, timing/target_tokens_per_second=87982.0546875, timing/target_tokens_per_second_per_core=2749.439208984375, timing/uptime=5477.16, z_loss=0.0832941010594368, z_loss_per_all_target_tokens=1.05914e-07
I0513 01:19:37.874875 140260698863616 trainer.py:518] Training: step 10586
I0513 01:19:37.907606 140143678494720 trainer.py:518] Training: step 10584
I0513 01:19:37.911211 140643590531072 trainer.py:518] Training: step 10586
I0513 01:19:46.812744 139856202532864 trainer.py:518] Training: step 10590
I0513 01:19:46.813056 139876724979712 trainer.py:518] Training: step 10590
I0513 01:19:46.813231 139666521167872 trainer.py:518] Training: step 10590
I0513 01:20:13.631651 139856202532864 trainer.py:518] Training: step 10592
I0513 01:20:13.630508 140143678494720 trainer.py:518] Training: step 10592
I0513 01:20:13.631453 140193652815872 trainer.py:518] Training: step 10592
I0513 01:20:13.632116 140180528838656 trainer.py:518] Training: step 10592
I0513 01:20:13.632054 140643590531072 trainer.py:518] Training: step 10592
I0513 01:20:13.631395 139876724979712 trainer.py:518] Training: step 10592
I0513 01:20:13.632216 140260698863616 trainer.py:518] Training: step 10592
I0513 01:20:13.632074 139666521167872 trainer.py:518] Training: step 10592
I0513 01:20:31.509350 139856202532864 trainer.py:518] Training: step 10593
I0513 01:20:31.508814 140180528838656 trainer.py:518] Training: step 10593
I0513 01:20:31.508789 140193652815872 trainer.py:518] Training: step 10593
I0513 01:20:31.509087 140643590531072 trainer.py:518] Training: step 10593
I0513 01:20:31.509505 140260698863616 trainer.py:518] Training: step 10593
I0513 01:20:31.509654 139666521167872 trainer.py:518] Training: step 10593
I0513 01:20:40.447360 140143678494720 trainer.py:518] Training: step 10593
I0513 01:20:40.448109 139876724979712 trainer.py:518] Training: step 10593
I0513 01:20:49.387486 139856202532864 trainer.py:518] Training: step 10594
I0513 01:20:49.386821 140193652815872 trainer.py:518] Training: step 10594
I0513 01:20:49.387016 140180528838656 trainer.py:518] Training: step 10594
I0513 01:20:49.387311 140643590531072 trainer.py:518] Training: step 10594
I0513 01:20:49.387740 139666521167872 trainer.py:518] Training: step 10594
I0513 01:20:58.325550 140143678494720 trainer.py:518] Training: step 10594
I0513 01:20:58.326371 140260698863616 trainer.py:518] Training: step 10594
I0513 01:21:07.265207 139856202532864 trainer.py:518] Training: step 10595
I0513 01:21:07.264137 140193652815872 trainer.py:518] Training: step 10595
I0513 01:21:07.264325 140180528838656 trainer.py:518] Training: step 10595
I0513 01:21:07.264162 140643590531072 trainer.py:518] Training: step 10595
I0513 01:21:07.264901 139876724979712 trainer.py:518] Training: step 10594
I0513 01:21:07.264784 139666521167872 trainer.py:518] Training: step 10595
I0513 01:21:16.205305 140143678494720 trainer.py:518] Training: step 10595
I0513 01:21:16.206537 140260698863616 trainer.py:518] Training: step 10595
I0513 01:21:25.141587 140193652815872 trainer.py:518] Training: step 10596
I0513 01:21:25.142821 139856202532864 trainer.py:518] Training: step 10596
I0513 01:21:25.142448 140643590531072 trainer.py:518] Training: step 10596
I0513 01:21:25.142671 140180528838656 trainer.py:518] Training: step 10596
I0513 01:21:25.142477 139876724979712 trainer.py:518] Training: step 10595
I0513 01:21:25.142452 139666521167872 trainer.py:518] Training: step 10596
I0513 01:21:34.082459 140143678494720 trainer.py:518] Training: step 10596
I0513 01:21:34.082354 140260698863616 trainer.py:518] Training: step 10596
I0513 01:21:43.020319 139856202532864 trainer.py:518] Training: step 10597
I0513 01:21:43.019593 140193652815872 trainer.py:518] Training: step 10597
I0513 01:21:43.020267 140180528838656 trainer.py:518] Training: step 10597
I0513 01:21:43.020275 140643590531072 trainer.py:518] Training: step 10597
I0513 01:21:43.019679 139876724979712 trainer.py:518] Training: step 10596
I0513 01:21:43.020337 139666521167872 trainer.py:518] Training: step 10597
I0513 01:22:00.897827 139856202532864 trainer.py:518] Training: step 10598
I0513 01:22:00.896906 140193652815872 trainer.py:518] Training: step 10598
I0513 01:22:00.897544 140180528838656 trainer.py:518] Training: step 10598
I0513 01:22:00.897454 140643590531072 trainer.py:518] Training: step 10598
I0513 01:22:00.897576 140143678494720 trainer.py:518] Training: step 10598
I0513 01:22:00.897090 139876724979712 trainer.py:518] Training: step 10597
I0513 01:22:00.898033 140260698863616 trainer.py:518] Training: step 10598
I0513 01:22:00.897901 139666521167872 trainer.py:518] Training: step 10598
I0513 01:22:09.834214 140055069881920 logging_writer.py:48] [10580] collection=train accuracy=0.512166, cross_ent_loss=19.77191734313965, cross_ent_loss_per_all_target_tokens=2.51413e-05, experts/auxiliary_loss=0.14097407460212708, experts/expert_usage=7.854203224182129, experts/fraction_tokens_left_behind=2.290877342224121, experts/router_confidence=2.594601631164551, experts/router_z_loss=0.0017610689392313361, learning_rate=0.00972456, learning_rate/current=0.00972249, loss=19.997045516967773, loss_per_all_target_tokens=2.54276e-05, loss_per_nonpadding_target_token=6.37439e-05, non_padding_fraction/loss_weights=0.398902, timing/seconds=89.39202880859375, timing/seqs=3840, timing/seqs_per_second=42.956851959228516, timing/seqs_per_second_per_core=1.3424016237258911, timing/steps_per_second=0.11186680197715759, timing/target_tokens_per_second=87975.6328125, timing/target_tokens_per_second_per_core=2749.238525390625, timing/uptime=5638.1, z_loss=0.0823904424905777, z_loss_per_all_target_tokens=1.04765e-07
I0513 01:22:18.776732 139856202532864 trainer.py:518] Training: step 10599
I0513 01:22:18.776562 140180528838656 trainer.py:518] Training: step 10599
I0513 01:22:18.776486 140193652815872 trainer.py:518] Training: step 10599
I0513 01:22:18.776707 140143678494720 trainer.py:518] Training: step 10599
I0513 01:22:18.777192 140643590531072 trainer.py:518] Training: step 10599
I0513 01:22:18.775827 139876724979712 trainer.py:518] Training: step 10602
I0513 01:22:18.776458 140260698863616 trainer.py:518] Training: step 10599
I0513 01:22:18.776639 139666521167872 trainer.py:518] Training: step 10599
I0513 01:22:36.654002 139856202532864 trainer.py:518] Training: step 10606
I0513 01:22:36.652029 140055069881920 logging_writer.py:48] [10590] collection=train accuracy=0.508148, cross_ent_loss=20.038297653198242, cross_ent_loss_per_all_target_tokens=2.548e-05, experts/auxiliary_loss=0.14115697145462036, experts/expert_usage=7.881829261779785, experts/fraction_tokens_left_behind=2.2428481578826904, experts/router_confidence=2.5993776321411133, experts/router_z_loss=0.001750534400343895, learning_rate=0.00971997, learning_rate/current=0.0097179, loss=20.263029098510742, loss_per_all_target_tokens=2.57658e-05, loss_per_nonpadding_target_token=6.32168e-05, non_padding_fraction/loss_weights=0.407578, timing/seconds=89.3828353881836, timing/seqs=3840, timing/seqs_per_second=42.96126937866211, timing/seqs_per_second_per_core=1.342539668083191, timing/steps_per_second=0.11187830567359924, timing/target_tokens_per_second=87984.6796875, timing/target_tokens_per_second_per_core=2749.521240234375, timing/uptime=5656.31, z_loss=0.0818253681063652, z_loss_per_all_target_tokens=1.04046e-07
I0513 01:22:36.653589 140643590531072 trainer.py:518] Training: step 10605
I0513 01:22:36.654221 139666521167872 trainer.py:518] Training: step 10606
I0513 01:22:36.684880 140193652815872 trainer.py:518] Training: step 10606
I0513 01:22:36.687469 140180528838656 trainer.py:518] Training: step 10607
I0513 01:22:45.591957 140143678494720 trainer.py:518] Training: step 10610
I0513 01:22:45.610131 139876724979712 trainer.py:518] Training: step 10610
I0513 01:22:45.611138 140260698863616 trainer.py:518] Training: step 10610
I0513 01:23:12.410133 139856202532864 trainer.py:518] Training: step 10612
I0513 01:23:12.409286 140143678494720 trainer.py:518] Training: step 10612
I0513 01:23:12.410272 140193652815872 trainer.py:518] Training: step 10612
I0513 01:23:12.410441 140180528838656 trainer.py:518] Training: step 10612
I0513 01:23:12.410204 140643590531072 trainer.py:518] Training: step 10612
I0513 01:23:12.410814 139876724979712 trainer.py:518] Training: step 10612
I0513 01:23:12.410685 140260698863616 trainer.py:518] Training: step 10612
I0513 01:23:12.410714 139666521167872 trainer.py:518] Training: step 10612
I0513 01:23:30.286849 139856202532864 trainer.py:518] Training: step 10613
I0513 01:23:30.286737 140193652815872 trainer.py:518] Training: step 10613
I0513 01:23:30.286418 140143678494720 trainer.py:518] Training: step 10613
I0513 01:23:30.287169 140180528838656 trainer.py:518] Training: step 10613
I0513 01:23:30.287177 140643590531072 trainer.py:518] Training: step 10613
I0513 01:23:30.286688 139876724979712 trainer.py:518] Training: step 10613
I0513 01:23:30.287673 139666521167872 trainer.py:518] Training: step 10613
I0513 01:23:39.226497 140260698863616 trainer.py:518] Training: step 10613
I0513 01:23:48.164311 139856202532864 trainer.py:518] Training: step 10614
I0513 01:23:48.164431 140193652815872 trainer.py:518] Training: step 10614
I0513 01:23:48.165021 140180528838656 trainer.py:518] Training: step 10614
I0513 01:23:48.164688 140643590531072 trainer.py:518] Training: step 10614
I0513 01:23:48.164493 140143678494720 trainer.py:518] Training: step 10614
I0513 01:23:48.164480 139876724979712 trainer.py:518] Training: step 10614
I0513 01:23:48.165437 139666521167872 trainer.py:518] Training: step 10614
I0513 01:23:57.104549 140260698863616 trainer.py:518] Training: step 10614
I0513 01:24:06.043022 139856202532864 trainer.py:518] Training: step 10615
I0513 01:24:06.043376 140193652815872 trainer.py:518] Training: step 10615
I0513 01:24:06.043083 140643590531072 trainer.py:518] Training: step 10615
I0513 01:24:06.043499 140180528838656 trainer.py:518] Training: step 10615
I0513 01:24:06.043025 140143678494720 trainer.py:518] Training: step 10615
I0513 01:24:06.044004 139876724979712 trainer.py:518] Training: step 10615
I0513 01:24:06.043722 139666521167872 trainer.py:518] Training: step 10615
I0513 01:24:14.985576 140260698863616 trainer.py:518] Training: step 10615
I0513 01:24:23.921837 139856202532864 trainer.py:518] Training: step 10616
I0513 01:24:23.920487 140143678494720 trainer.py:518] Training: step 10616
I0513 01:24:23.921555 140180528838656 trainer.py:518] Training: step 10616
I0513 01:24:23.921869 140193652815872 trainer.py:518] Training: step 10616
I0513 01:24:23.921739 140643590531072 trainer.py:518] Training: step 10616
I0513 01:24:23.921785 139876724979712 trainer.py:518] Training: step 10616
I0513 01:24:23.921938 139666521167872 trainer.py:518] Training: step 10616
I0513 01:24:32.861724 140260698863616 trainer.py:518] Training: step 10616
I0513 01:24:41.799502 139856202532864 trainer.py:518] Training: step 10617
I0513 01:24:41.799505 140193652815872 trainer.py:518] Training: step 10617
I0513 01:24:41.799772 140180528838656 trainer.py:518] Training: step 10617
I0513 01:24:41.799479 140143678494720 trainer.py:518] Training: step 10617
I0513 01:24:41.799852 140643590531072 trainer.py:518] Training: step 10617
I0513 01:24:41.799962 139876724979712 trainer.py:518] Training: step 10617
I0513 01:24:41.799556 139666521167872 trainer.py:518] Training: step 10617
I0513 01:24:59.677641 139856202532864 trainer.py:518] Training: step 10618
I0513 01:24:59.677520 140193652815872 trainer.py:518] Training: step 10618
I0513 01:24:59.678297 140180528838656 trainer.py:518] Training: step 10618
I0513 01:24:59.678219 140643590531072 trainer.py:518] Training: step 10618
I0513 01:24:59.678051 140143678494720 trainer.py:518] Training: step 10618
I0513 01:24:59.677934 139876724979712 trainer.py:518] Training: step 10618
I0513 01:24:59.678676 140260698863616 trainer.py:518] Training: step 10618
I0513 01:24:59.677947 139666521167872 trainer.py:518] Training: step 10618
I0513 01:25:08.617412 140055069881920 logging_writer.py:48] [10600] collection=train accuracy=0.506544, cross_ent_loss=20.13313865661621, cross_ent_loss_per_all_target_tokens=2.56006e-05, experts/auxiliary_loss=0.14148865640163422, experts/expert_usage=7.872589111328125, experts/fraction_tokens_left_behind=2.2459304332733154, experts/router_confidence=2.60603666305542, experts/router_z_loss=0.0017499933019280434, learning_rate=0.00971538, learning_rate/current=0.00971332, loss=20.35699462890625, loss_per_all_target_tokens=2.58853e-05, loss_per_nonpadding_target_token=6.16863e-05, non_padding_fraction/loss_weights=0.419627, timing/seconds=89.38683319091797, timing/seqs=3840, timing/seqs_per_second=42.959346771240234, timing/seqs_per_second_per_core=1.3424795866012573, timing/steps_per_second=0.11187329888343811, timing/target_tokens_per_second=87980.7421875, timing/target_tokens_per_second_per_core=2749.398193359375, timing/uptime=5816.88, z_loss=0.08061875402927399, z_loss_per_all_target_tokens=1.02512e-07
I0513 01:25:17.557003 139856202532864 trainer.py:518] Training: step 10619
I0513 01:25:17.557694 140193652815872 trainer.py:518] Training: step 10619
I0513 01:25:17.557643 140180528838656 trainer.py:518] Training: step 10619
I0513 01:25:17.557614 140643590531072 trainer.py:518] Training: step 10619
I0513 01:25:17.557421 140143678494720 trainer.py:518] Training: step 10619
I0513 01:25:17.557111 139876724979712 trainer.py:518] Training: step 10619
I0513 01:25:17.556898 140260698863616 trainer.py:518] Training: step 10619
I0513 01:25:17.557734 139666521167872 trainer.py:518] Training: step 10619
I0513 01:25:27.744275 140260698863616 trainer.py:518] Training: step 10628
I0513 01:25:35.435647 140643590531072 trainer.py:518] Training: step 10624
I0513 01:25:35.436697 140180528838656 trainer.py:518] Training: step 10628
I0513 01:25:35.436488 140055069881920 logging_writer.py:48] [10610] collection=train accuracy=0.509517, cross_ent_loss=20.039886474609375, cross_ent_loss_per_all_target_tokens=2.5482e-05, experts/auxiliary_loss=0.1417853683233261, experts/expert_usage=7.886261940002441, experts/fraction_tokens_left_behind=2.207963228225708, experts/router_confidence=2.610215425491333, experts/router_z_loss=0.0017355576856061816, learning_rate=0.0097108, learning_rate/current=0.00970874, loss=20.264822006225586, loss_per_all_target_tokens=2.57681e-05, loss_per_nonpadding_target_token=6.36252e-05, non_padding_fraction/loss_weights=0.404997, timing/seconds=89.38300323486328, timing/seqs=3840, timing/seqs_per_second=42.96118927001953, timing/seqs_per_second_per_core=1.3425371646881104, timing/steps_per_second=0.11187809705734253, timing/target_tokens_per_second=87984.515625, timing/target_tokens_per_second_per_core=2749.51611328125, timing/uptime=5834.73, z_loss=0.08141583949327469, z_loss_per_all_target_tokens=1.03526e-07
I0513 01:25:35.435822 139876724979712 trainer.py:518] Training: step 10626
I0513 01:25:35.436778 139666521167872 trainer.py:518] Training: step 10626
I0513 01:25:35.461931 140143678494720 trainer.py:518] Training: step 10624
I0513 01:25:35.514631 139856202532864 trainer.py:518] Training: step 10627
I0513 01:25:35.538305 140193652815872 trainer.py:518] Training: step 10626
I0513 01:25:44.374989 140260698863616 trainer.py:518] Training: step 10630
I0513 01:26:11.191394 140143678494720 trainer.py:518] Training: step 10632
I0513 01:26:11.192089 139856202532864 trainer.py:518] Training: step 10632
I0513 01:26:11.191940 140643590531072 trainer.py:518] Training: step 10632
I0513 01:26:11.191439 140193652815872 trainer.py:518] Training: step 10632
I0513 01:26:11.192000 140180528838656 trainer.py:518] Training: step 10632
I0513 01:26:11.191560 139876724979712 trainer.py:518] Training: step 10632
I0513 01:26:11.192634 140260698863616 trainer.py:518] Training: step 10632
I0513 01:26:11.191466 139666521167872 trainer.py:518] Training: step 10632
I0513 01:26:29.071116 140143678494720 trainer.py:518] Training: step 10633
I0513 01:26:29.071836 140643590531072 trainer.py:518] Training: step 10633
I0513 01:26:29.071935 139856202532864 trainer.py:518] Training: step 10633
I0513 01:26:29.071070 140193652815872 trainer.py:518] Training: step 10633
I0513 01:26:29.071868 140180528838656 trainer.py:518] Training: step 10633
I0513 01:26:29.071486 139876724979712 trainer.py:518] Training: step 10633
I0513 01:26:29.071712 140260698863616 trainer.py:518] Training: step 10633
I0513 01:26:29.071696 139666521167872 trainer.py:518] Training: step 10633
I0513 01:26:46.949707 140643590531072 trainer.py:518] Training: step 10634
I0513 01:26:46.950076 139856202532864 trainer.py:518] Training: step 10634
I0513 01:26:46.950096 140180528838656 trainer.py:518] Training: step 10634
I0513 01:26:46.949233 140193652815872 trainer.py:518] Training: step 10634
I0513 01:26:46.949646 139876724979712 trainer.py:518] Training: step 10634
I0513 01:26:46.950079 140260698863616 trainer.py:518] Training: step 10634
I0513 01:26:46.949248 139666521167872 trainer.py:518] Training: step 10634
I0513 01:26:55.888463 140143678494720 trainer.py:518] Training: step 10634
I0513 01:27:04.828708 140643590531072 trainer.py:518] Training: step 10635
I0513 01:27:04.828991 139856202532864 trainer.py:518] Training: step 10635
I0513 01:27:04.827868 140193652815872 trainer.py:518] Training: step 10635
I0513 01:27:04.828751 140180528838656 trainer.py:518] Training: step 10635
I0513 01:27:04.828517 139876724979712 trainer.py:518] Training: step 10635
I0513 01:27:04.828613 140260698863616 trainer.py:518] Training: step 10635
I0513 01:27:04.827746 139666521167872 trainer.py:518] Training: step 10635
I0513 01:27:13.765977 140143678494720 trainer.py:518] Training: step 10635
I0513 01:27:22.706446 140643590531072 trainer.py:518] Training: step 10636
I0513 01:27:22.706951 139856202532864 trainer.py:518] Training: step 10636
I0513 01:27:22.706434 140193652815872 trainer.py:518] Training: step 10636
I0513 01:27:22.706940 140180528838656 trainer.py:518] Training: step 10636
I0513 01:27:22.706549 139876724979712 trainer.py:518] Training: step 10636
I0513 01:27:22.706377 140260698863616 trainer.py:518] Training: step 10636
I0513 01:27:22.706289 139666521167872 trainer.py:518] Training: step 10636
I0513 01:27:31.646940 140143678494720 trainer.py:518] Training: step 10636
I0513 01:27:40.584019 139856202532864 trainer.py:518] Training: step 10637
I0513 01:27:40.583355 140193652815872 trainer.py:518] Training: step 10637
I0513 01:27:40.584776 140643590531072 trainer.py:518] Training: step 10637
I0513 01:27:40.584606 140180528838656 trainer.py:518] Training: step 10637
I0513 01:27:40.583596 139876724979712 trainer.py:518] Training: step 10637
I0513 01:27:40.583742 140260698863616 trainer.py:518] Training: step 10637
I0513 01:27:40.583212 139666521167872 trainer.py:518] Training: step 10637
I0513 01:27:58.460863 140143678494720 trainer.py:518] Training: step 10638
I0513 01:27:58.461284 139856202532864 trainer.py:518] Training: step 10638
I0513 01:27:58.461282 140643590531072 trainer.py:518] Training: step 10638
I0513 01:27:58.460792 140193652815872 trainer.py:518] Training: step 10638
I0513 01:27:58.462018 140180528838656 trainer.py:518] Training: step 10638
I0513 01:27:58.460692 139876724979712 trainer.py:518] Training: step 10638
I0513 01:27:58.461465 140260698863616 trainer.py:518] Training: step 10638
I0513 01:27:58.460642 139666521167872 trainer.py:518] Training: step 10638
I0513 01:28:07.398159 140055069881920 logging_writer.py:48] [10620] collection=train accuracy=0.519613, cross_ent_loss=19.46695899963379, cross_ent_loss_per_all_target_tokens=2.47535e-05, experts/auxiliary_loss=0.14188116788864136, experts/expert_usage=7.847653865814209, experts/fraction_tokens_left_behind=2.225198268890381, experts/router_confidence=2.6096701622009277, experts/router_z_loss=0.0017347530229017138, learning_rate=0.00970622, learning_rate/current=0.00970417, loss=19.69068717956543, loss_per_all_target_tokens=2.5038e-05, loss_per_nonpadding_target_token=6.20799e-05, non_padding_fraction/loss_weights=0.403319, timing/seconds=89.39175415039062, timing/seqs=3840, timing/seqs_per_second=42.95698165893555, timing/seqs_per_second_per_core=1.3424056768417358, timing/steps_per_second=0.11186714470386505, timing/target_tokens_per_second=87975.8984375, timing/target_tokens_per_second_per_core=2749.246826171875, timing/uptime=5995.66, z_loss=0.0801115557551384, z_loss_per_all_target_tokens=1.01867e-07
I0513 01:28:16.339509 140143678494720 trainer.py:518] Training: step 10639
I0513 01:28:16.339772 139856202532864 trainer.py:518] Training: step 10639
I0513 01:28:16.339545 140643590531072 trainer.py:518] Training: step 10639
I0513 01:28:16.339530 140193652815872 trainer.py:518] Training: step 10639
I0513 01:28:16.340450 140180528838656 trainer.py:518] Training: step 10639
I0513 01:28:16.338842 139876724979712 trainer.py:518] Training: step 10639
I0513 01:28:16.339562 140260698863616 trainer.py:518] Training: step 10639
I0513 01:28:16.338645 139666521167872 trainer.py:518] Training: step 10639
I0513 01:28:34.214463 140055069881920 logging_writer.py:48] [10630] collection=train accuracy=0.516041, cross_ent_loss=19.63424301147461, cross_ent_loss_per_all_target_tokens=2.49662e-05, experts/auxiliary_loss=0.13996104896068573, experts/expert_usage=7.852893352508545, experts/fraction_tokens_left_behind=2.349620819091797, experts/router_confidence=2.571092128753662, experts/router_z_loss=0.001790601760149002, learning_rate=0.00970165, learning_rate/current=0.0096996, loss=19.85552406311035, loss_per_all_target_tokens=2.52476e-05, loss_per_nonpadding_target_token=5.98254e-05, non_padding_fraction/loss_weights=0.422021, timing/seconds=89.38563537597656, timing/seqs=3840, timing/seqs_per_second=42.959922790527344, timing/seqs_per_second_per_core=1.3424975872039795, timing/steps_per_second=0.11187479645013809, timing/target_tokens_per_second=87981.921875, timing/target_tokens_per_second_per_core=2749.43505859375, timing/uptime=6014.66, z_loss=0.07952968031167984, z_loss_per_all_target_tokens=1.01127e-07
I0513 01:28:34.215968 139856202532864 trainer.py:518] Training: step 10645
I0513 01:28:34.215972 140193652815872 trainer.py:518] Training: step 10646
I0513 01:28:34.247723 140643590531072 trainer.py:518] Training: step 10648
I0513 01:28:34.250328 140180528838656 trainer.py:518] Training: step 10647
I0513 01:28:34.245633 139876724979712 trainer.py:518] Training: step 10647
I0513 01:28:34.246456 139666521167872 trainer.py:518] Training: step 10647
I0513 01:28:34.249946 140260698863616 trainer.py:518] Training: step 10648
I0513 01:28:43.154727 140143678494720 trainer.py:518] Training: step 10650
I0513 01:29:09.972521 139856202532864 trainer.py:518] Training: step 10652
I0513 01:29:09.972350 140643590531072 trainer.py:518] Training: step 10652
I0513 01:29:09.972338 140143678494720 trainer.py:518] Training: step 10652
I0513 01:29:09.972284 140193652815872 trainer.py:518] Training: step 10652
I0513 01:29:09.973420 140180528838656 trainer.py:518] Training: step 10652
I0513 01:29:09.972480 139876724979712 trainer.py:518] Training: step 10652
I0513 01:29:09.972780 140260698863616 trainer.py:518] Training: step 10652
I0513 01:29:09.972042 139666521167872 trainer.py:518] Training: step 10652
I0513 01:29:27.850126 139856202532864 trainer.py:518] Training: step 10653
I0513 01:29:27.850209 140643590531072 trainer.py:518] Training: step 10653
I0513 01:29:27.849565 140193652815872 trainer.py:518] Training: step 10653
I0513 01:29:27.850892 140180528838656 trainer.py:518] Training: step 10653
I0513 01:29:27.849749 139876724979712 trainer.py:518] Training: step 10653
I0513 01:29:27.849454 139666521167872 trainer.py:518] Training: step 10653
I0513 01:29:36.788995 140143678494720 trainer.py:518] Training: step 10653
I0513 01:29:36.789965 140260698863616 trainer.py:518] Training: step 10653
I0513 01:29:45.727347 139856202532864 trainer.py:518] Training: step 10654
I0513 01:29:45.727945 140643590531072 trainer.py:518] Training: step 10654
I0513 01:29:45.727199 140193652815872 trainer.py:518] Training: step 10654
I0513 01:29:45.728660 140180528838656 trainer.py:518] Training: step 10654
I0513 01:29:45.727773 139876724979712 trainer.py:518] Training: step 10654
I0513 01:29:45.727295 139666521167872 trainer.py:518] Training: step 10654
I0513 01:29:54.666369 140143678494720 trainer.py:518] Training: step 10654
I0513 01:29:54.667229 140260698863616 trainer.py:518] Training: step 10654
I0513 01:30:03.605521 139856202532864 trainer.py:518] Training: step 10655
I0513 01:30:03.605988 140643590531072 trainer.py:518] Training: step 10655
I0513 01:30:03.605306 140193652815872 trainer.py:518] Training: step 10655
I0513 01:30:03.606507 140180528838656 trainer.py:518] Training: step 10655
I0513 01:30:03.606160 139876724979712 trainer.py:518] Training: step 10655
I0513 01:30:03.605273 139666521167872 trainer.py:518] Training: step 10655
I0513 01:30:12.547672 140143678494720 trainer.py:518] Training: step 10655
I0513 01:30:12.549037 140260698863616 trainer.py:518] Training: step 10655
I0513 01:30:21.484860 139856202532864 trainer.py:518] Training: step 10656
I0513 01:30:21.485262 140643590531072 trainer.py:518] Training: step 10656
I0513 01:30:21.484631 140193652815872 trainer.py:518] Training: step 10656
I0513 01:30:21.485373 140180528838656 trainer.py:518] Training: step 10656
I0513 01:30:21.484612 139876724979712 trainer.py:518] Training: step 10656
I0513 01:30:21.484352 139666521167872 trainer.py:518] Training: step 10656
I0513 01:30:30.425524 140143678494720 trainer.py:518] Training: step 10656
I0513 01:30:30.424529 140260698863616 trainer.py:518] Training: step 10656
I0513 01:30:39.362599 140643590531072 trainer.py:518] Training: step 10657
I0513 01:30:39.362917 139856202532864 trainer.py:518] Training: step 10657
I0513 01:30:39.362487 140193652815872 trainer.py:518] Training: step 10657
I0513 01:30:39.364020 140180528838656 trainer.py:518] Training: step 10657
I0513 01:30:39.362703 139876724979712 trainer.py:518] Training: step 10657
I0513 01:30:39.362640 139666521167872 trainer.py:518] Training: step 10657
I0513 01:30:57.240707 139856202532864 trainer.py:518] Training: step 10658
I0513 01:30:57.241015 140643590531072 trainer.py:518] Training: step 10658
I0513 01:30:57.241141 140143678494720 trainer.py:518] Training: step 10658
I0513 01:30:57.240261 140193652815872 trainer.py:518] Training: step 10658
I0513 01:30:57.241312 140180528838656 trainer.py:518] Training: step 10658
I0513 01:30:57.240359 139876724979712 trainer.py:518] Training: step 10658
I0513 01:30:57.240996 140260698863616 trainer.py:518] Training: step 10658
I0513 01:30:57.240245 139666521167872 trainer.py:518] Training: step 10658
I0513 01:31:06.178245 140055069881920 logging_writer.py:48] [10640] collection=train accuracy=0.511765, cross_ent_loss=19.768430709838867, cross_ent_loss_per_all_target_tokens=2.51369e-05, experts/auxiliary_loss=0.14015629887580872, experts/expert_usage=7.876191139221191, experts/fraction_tokens_left_behind=2.30737042427063, experts/router_confidence=2.5740020275115967, experts/router_z_loss=0.0017817342886701226, learning_rate=0.00969709, learning_rate/current=0.00969504, loss=19.99099349975586, loss_per_all_target_tokens=2.54199e-05, loss_per_nonpadding_target_token=6.03246e-05, non_padding_fraction/loss_weights=0.421385, timing/seconds=89.38568115234375, timing/seqs=3840, timing/seqs_per_second=42.95989990234375, timing/seqs_per_second_per_core=1.3424968719482422, timing/steps_per_second=0.11187474429607391, timing/target_tokens_per_second=87981.875, timing/target_tokens_per_second_per_core=2749.43359375, timing/uptime=6174.44, z_loss=0.08062532544136047, z_loss_per_all_target_tokens=1.0252e-07
I0513 01:31:15.119083 140643590531072 trainer.py:518] Training: step 10659
I0513 01:31:15.119213 139856202532864 trainer.py:518] Training: step 10659
I0513 01:31:15.119224 140193652815872 trainer.py:518] Training: step 10659
I0513 01:31:15.119511 140180528838656 trainer.py:518] Training: step 10659
I0513 01:31:15.118573 139876724979712 trainer.py:518] Training: step 10659
I0513 01:31:15.118947 140260698863616 trainer.py:518] Training: step 10659
I0513 01:31:15.118387 139666521167872 trainer.py:518] Training: step 10659
I0513 01:31:24.058485 140143678494720 trainer.py:518] Training: step 10660
I0513 01:31:32.995997 139856202532864 trainer.py:518] Training: step 10665
I0513 01:31:32.996621 140055069881920 logging_writer.py:48] [10650] collection=train accuracy=0.511863, cross_ent_loss=19.84177589416504, cross_ent_loss_per_all_target_tokens=2.52301e-05, experts/auxiliary_loss=0.14081789553165436, experts/expert_usage=7.875541687011719, experts/fraction_tokens_left_behind=2.271225690841675, experts/router_confidence=2.59208083152771, experts/router_z_loss=0.001760105020366609, learning_rate=0.00969253, learning_rate/current=0.00969049, loss=20.064098358154297, loss_per_all_target_tokens=2.55128e-05, loss_per_nonpadding_target_token=6.2595e-05, non_padding_fraction/loss_weights=0.407586, timing/seconds=89.38219451904297, timing/seqs=3840, timing/seqs_per_second=42.961578369140625, timing/seqs_per_second_per_core=1.3425493240356445, timing/steps_per_second=0.11187911033630371, timing/target_tokens_per_second=87985.3125, timing/target_tokens_per_second_per_core=2749.541015625, timing/uptime=6192.29, z_loss=0.07974019646644592, z_loss_per_all_target_tokens=1.01395e-07
I0513 01:31:32.995956 140193652815872 trainer.py:518] Training: step 10665
I0513 01:31:33.025678 140643590531072 trainer.py:518] Training: step 10667
I0513 01:31:33.025802 139876724979712 trainer.py:518] Training: step 10666
I0513 01:31:33.056406 140180528838656 trainer.py:518] Training: step 10668
I0513 01:31:33.120459 139666521167872 trainer.py:518] Training: step 10668
I0513 01:31:41.935046 140143678494720 trainer.py:518] Training: step 10670
I0513 01:31:41.935279 140260698863616 trainer.py:518] Training: step 10670
I0513 01:32:08.751796 140143678494720 trainer.py:518] Training: step 10672
I0513 01:32:08.752727 139856202532864 trainer.py:518] Training: step 10672
I0513 01:32:08.752936 140643590531072 trainer.py:518] Training: step 10672
I0513 01:32:08.752854 140193652815872 trainer.py:518] Training: step 10672
I0513 01:32:08.752878 140180528838656 trainer.py:518] Training: step 10672
I0513 01:32:08.753632 139876724979712 trainer.py:518] Training: step 10672
I0513 01:32:08.753488 140260698863616 trainer.py:518] Training: step 10672
I0513 01:32:08.752273 139666521167872 trainer.py:518] Training: step 10672
I0513 01:32:26.630199 140143678494720 trainer.py:518] Training: step 10673
I0513 01:32:26.631191 139856202532864 trainer.py:518] Training: step 10673
I0513 01:32:26.631189 140643590531072 trainer.py:518] Training: step 10673
I0513 01:32:26.630965 140193652815872 trainer.py:518] Training: step 10673
I0513 01:32:26.630982 140180528838656 trainer.py:518] Training: step 10673
I0513 01:32:26.631197 139876724979712 trainer.py:518] Training: step 10673
I0513 01:32:26.631211 140260698863616 trainer.py:518] Training: step 10673
I0513 01:32:26.630655 139666521167872 trainer.py:518] Training: step 10673
I0513 01:32:44.510498 140143678494720 trainer.py:518] Training: step 10674
I0513 01:32:44.511328 139856202532864 trainer.py:518] Training: step 10674
I0513 01:32:44.510609 140180528838656 trainer.py:518] Training: step 10674
I0513 01:32:44.512029 140643590531072 trainer.py:518] Training: step 10674
I0513 01:32:44.511228 140193652815872 trainer.py:518] Training: step 10674
I0513 01:32:44.511526 139876724979712 trainer.py:518] Training: step 10674
I0513 01:32:44.511185 140260698863616 trainer.py:518] Training: step 10674
I0513 01:32:44.510565 139666521167872 trainer.py:518] Training: step 10674
I0513 01:33:02.389089 140143678494720 trainer.py:518] Training: step 10675
I0513 01:33:02.390127 139856202532864 trainer.py:518] Training: step 10675
I0513 01:33:02.390084 140180528838656 trainer.py:518] Training: step 10675
I0513 01:33:02.390293 140193652815872 trainer.py:518] Training: step 10675
I0513 01:33:02.390231 139876724979712 trainer.py:518] Training: step 10675
I0513 01:33:02.389943 140260698863616 trainer.py:518] Training: step 10675
I0513 01:33:02.389188 139666521167872 trainer.py:518] Training: step 10675
I0513 01:33:11.331124 140643590531072 trainer.py:518] Training: step 10675
I0513 01:33:20.267358 140143678494720 trainer.py:518] Training: step 10676
I0513 01:33:20.268172 139856202532864 trainer.py:518] Training: step 10676
I0513 01:33:20.267933 140180528838656 trainer.py:518] Training: step 10676
I0513 01:33:20.267992 140193652815872 trainer.py:518] Training: step 10676
I0513 01:33:20.268438 139876724979712 trainer.py:518] Training: step 10676
I0513 01:33:20.267691 140260698863616 trainer.py:518] Training: step 10676
I0513 01:33:20.267537 139666521167872 trainer.py:518] Training: step 10676
I0513 01:33:29.208120 140643590531072 trainer.py:518] Training: step 10676
I0513 01:33:38.145875 140143678494720 trainer.py:518] Training: step 10677
I0513 01:33:38.147280 139856202532864 trainer.py:518] Training: step 10677
I0513 01:33:38.146024 140180528838656 trainer.py:518] Training: step 10677
I0513 01:33:38.146747 140193652815872 trainer.py:518] Training: step 10677
I0513 01:33:38.145979 139876724979712 trainer.py:518] Training: step 10677
I0513 01:33:38.146056 140260698863616 trainer.py:518] Training: step 10677
I0513 01:33:38.145996 139666521167872 trainer.py:518] Training: step 10677
I0513 01:33:56.024251 140143678494720 trainer.py:518] Training: step 10678
I0513 01:33:56.025014 139856202532864 trainer.py:518] Training: step 10678
I0513 01:33:56.024919 140643590531072 trainer.py:518] Training: step 10678
I0513 01:33:56.024377 140193652815872 trainer.py:518] Training: step 10678
I0513 01:33:56.024423 140180528838656 trainer.py:518] Training: step 10678
I0513 01:33:56.024851 139876724979712 trainer.py:518] Training: step 10678
I0513 01:33:56.024706 140260698863616 trainer.py:518] Training: step 10678
I0513 01:33:56.024227 139666521167872 trainer.py:518] Training: step 10678
I0513 01:34:04.964398 140055069881920 logging_writer.py:48] [10660] collection=train accuracy=0.51882, cross_ent_loss=19.566137313842773, cross_ent_loss_per_all_target_tokens=2.48796e-05, experts/auxiliary_loss=0.14143578708171844, experts/expert_usage=7.846746921539307, experts/fraction_tokens_left_behind=2.2697360515594482, experts/router_confidence=2.5906193256378174, experts/router_z_loss=0.0017504309071227908, learning_rate=0.00968799, learning_rate/current=0.00968594, loss=19.78815269470215, loss_per_all_target_tokens=2.51619e-05, loss_per_nonpadding_target_token=6.22383e-05, non_padding_fraction/loss_weights=0.404284, timing/seconds=89.38920593261719, timing/seqs=3840, timing/seqs_per_second=42.95820999145508, timing/seqs_per_second_per_core=1.3424440622329712, timing/steps_per_second=0.11187033355236053, timing/target_tokens_per_second=87978.4140625, timing/target_tokens_per_second_per_core=2749.325439453125, timing/uptime=6353.2, z_loss=0.07882960140705109, z_loss_per_all_target_tokens=1.00237e-07
I0513 01:34:13.904118 140143678494720 trainer.py:518] Training: step 10679
I0513 01:34:13.904334 140643590531072 trainer.py:518] Training: step 10679
I0513 01:34:13.904605 139856202532864 trainer.py:518] Training: step 10679
I0513 01:34:13.904102 140193652815872 trainer.py:518] Training: step 10679
I0513 01:34:13.904447 140180528838656 trainer.py:518] Training: step 10679
I0513 01:34:13.903819 139876724979712 trainer.py:518] Training: step 10679
I0513 01:34:13.903423 140260698863616 trainer.py:518] Training: step 10679
I0513 01:34:13.904000 139666521167872 trainer.py:518] Training: step 10679
I0513 01:34:31.781518 140055069881920 logging_writer.py:48] [10670] collection=train accuracy=0.519211, cross_ent_loss=19.507482528686523, cross_ent_loss_per_all_target_tokens=2.4805e-05, experts/auxiliary_loss=0.14040443301200867, experts/expert_usage=7.8567705154418945, experts/fraction_tokens_left_behind=2.3007867336273193, experts/router_confidence=2.5778090953826904, experts/router_z_loss=0.0017739530885592103, learning_rate=0.00968344, learning_rate/current=0.0096814, loss=19.727201461791992, loss_per_all_target_tokens=2.50844e-05, loss_per_nonpadding_target_token=5.97551e-05, non_padding_fraction/loss_weights=0.419787, timing/seconds=89.38433837890625, timing/seqs=3840, timing/seqs_per_second=42.96054458618164, timing/seqs_per_second_per_core=1.3425170183181763, timing/steps_per_second=0.11187642067670822, timing/target_tokens_per_second=87983.1953125, timing/target_tokens_per_second_per_core=2749.474853515625, timing/uptime=6371.21, z_loss=0.07754235714673996, z_loss_per_all_target_tokens=9.86002e-08
I0513 01:34:31.781418 140180528838656 trainer.py:518] Training: step 10686
I0513 01:34:31.781160 139666521167872 trainer.py:518] Training: step 10686
I0513 01:34:31.808610 140143678494720 trainer.py:518] Training: step 10684
I0513 01:34:31.812940 139856202532864 trainer.py:518] Training: step 10687
I0513 01:34:31.813097 140193652815872 trainer.py:518] Training: step 10686
I0513 01:34:31.811909 139876724979712 trainer.py:518] Training: step 10688
I0513 01:34:31.814604 140260698863616 trainer.py:518] Training: step 10689
I0513 01:34:40.719128 140643590531072 trainer.py:518] Training: step 10690
