# Register necessary SeqIO Tasks/Mixtures.
from __gin__ import dynamic_registration
import t5.data.mixtures
import __main__ as train_script
import seqio
import flaxformer
from t5x import utils
from t5x import trainer
from t5x.contrib.moe import trainer as moe_trainer
from t5x.contrib.moe import models


include 'flaxformer/flaxformer/t5x/configs/moe/models/st_moe_decoder_only_base.gin'
include 't5x/contrib/moe/configs/runs/pretrain.gin'
# include 't5x/configs/runs/pretrain.gin'

MIXTURE_OR_TASK_NAME = "c4_v220_prefix_lm"
USE_CACHED_TASKS = False
TASK_FEATURE_LENGTHS = {"inputs": 512, "targets": 512}
TRAIN_STEPS = 500000
DROPOUT_RATE = 0.0
BATCH_SIZE = 128
NUM_MODEL_PARTITIONS = 1


train/utils.DatasetConfig:
  split = 'train'

train_script.train:
  eval_period = 5000
  
moe_trainer.MoeTrainer:
  num_microbatches = 1
  
models.MoeDecoderOnlyModel.inputs_bidirectional_attention=True
 
