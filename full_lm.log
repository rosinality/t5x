Using ssh batch size of 8. Attempting to SSH into 1 nodes with a total of 8 workers.
SSH: Attempting to connect to worker 0...
SSH: Attempting to connect to worker 1...
SSH: Attempting to connect to worker 2...
SSH: Attempting to connect to worker 3...
SSH: Attempting to connect to worker 4...
SSH: Attempting to connect to worker 5...
SSH: Attempting to connect to worker 6...
SSH: Attempting to connect to worker 7...
2024-05-12 01:44:48.785456: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2024-05-12 01:44:48.822970: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2024-05-12 01:44:48.837616: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2024-05-12 01:44:48.868551: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2024-05-12 01:44:48.865997: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2024-05-12 01:44:48.906012: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2024-05-12 01:44:49.020625: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2024-05-12 01:44:49.058493: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
WARNING:absl:Type handler registry overriding type "<class 'float'>" collision on scalar
WARNING:absl:Type handler registry overriding type "<class 'bytes'>" collision on scalar
WARNING:absl:Type handler registry overriding type "<class 'numpy.number'>" collision on scalar
WARNING:absl:Type handler registry overriding type "<class 'float'>" collision on scalar
WARNING:absl:Type handler registry overriding type "<class 'bytes'>" collision on scalar
WARNING:absl:Type handler registry overriding type "<class 'numpy.number'>" collision on scalar
WARNING:absl:Type handler registry overriding type "<class 'float'>" collision on scalar
WARNING:absl:Type handler registry overriding type "<class 'bytes'>" collision on scalar
WARNING:absl:Type handler registry overriding type "<class 'numpy.number'>" collision on scalar
WARNING:absl:Type handler registry overriding type "<class 'float'>" collision on scalar
WARNING:absl:Type handler registry overriding type "<class 'bytes'>" collision on scalar
WARNING:absl:Type handler registry overriding type "<class 'numpy.number'>" collision on scalar
WARNING:absl:Type handler registry overriding type "<class 'float'>" collision on scalar
WARNING:absl:Type handler registry overriding type "<class 'bytes'>" collision on scalar
WARNING:absl:Type handler registry overriding type "<class 'numpy.number'>" collision on scalar
WARNING:absl:Type handler registry overriding type "<class 'float'>" collision on scalar
WARNING:absl:Type handler registry overriding type "<class 'bytes'>" collision on scalar
WARNING:absl:Type handler registry overriding type "<class 'numpy.number'>" collision on scalar
WARNING:absl:Type handler registry overriding type "<class 'float'>" collision on scalar
WARNING:absl:Type handler registry overriding type "<class 'bytes'>" collision on scalar
WARNING:absl:Type handler registry overriding type "<class 'numpy.number'>" collision on scalar
WARNING:absl:Type handler registry overriding type "<class 'float'>" collision on scalar
WARNING:absl:Type handler registry overriding type "<class 'bytes'>" collision on scalar
WARNING:absl:Type handler registry overriding type "<class 'numpy.number'>" collision on scalar
I0512 01:44:50.943313 140392160929792 resource_reader.py:50] system_path_file_exists:t5x/examples/t5/t5_1_1/examples/openmoe_large_full_lm.gin
I0512 01:44:50.944177 140392160929792 resource_reader.py:37] gin-config opened resource file:/home/rosinality/t5x/t5x/examples/t5/t5_1_1/examples/openmoe_large_full_lm.gin
I0512 01:44:50.978130 139997397456896 resource_reader.py:50] system_path_file_exists:t5x/examples/t5/t5_1_1/examples/openmoe_large_full_lm.gin
I0512 01:44:50.978958 139997397456896 resource_reader.py:37] gin-config opened resource file:/home/rosinality/t5x/t5x/examples/t5/t5_1_1/examples/openmoe_large_full_lm.gin
I0512 01:44:50.996213 140577131780096 resource_reader.py:50] system_path_file_exists:t5x/examples/t5/t5_1_1/examples/openmoe_large_full_lm.gin
I0512 01:44:50.997039 140577131780096 resource_reader.py:37] gin-config opened resource file:/home/rosinality/t5x/t5x/examples/t5/t5_1_1/examples/openmoe_large_full_lm.gin
I0512 01:44:51.029889 140188559153152 resource_reader.py:50] system_path_file_exists:t5x/examples/t5/t5_1_1/examples/openmoe_large_full_lm.gin
I0512 01:44:51.030743 140188559153152 resource_reader.py:37] gin-config opened resource file:/home/rosinality/t5x/t5x/examples/t5/t5_1_1/examples/openmoe_large_full_lm.gin
I0512 01:44:51.034100 140172769531904 resource_reader.py:50] system_path_file_exists:t5x/examples/t5/t5_1_1/examples/openmoe_large_full_lm.gin
I0512 01:44:51.034949 140172769531904 resource_reader.py:37] gin-config opened resource file:/home/rosinality/t5x/t5x/examples/t5/t5_1_1/examples/openmoe_large_full_lm.gin
I0512 01:44:51.082536 140316961150976 resource_reader.py:50] system_path_file_exists:t5x/examples/t5/t5_1_1/examples/openmoe_large_full_lm.gin
I0512 01:44:51.083376 140316961150976 resource_reader.py:37] gin-config opened resource file:/home/rosinality/t5x/t5x/examples/t5/t5_1_1/examples/openmoe_large_full_lm.gin
I0512 01:44:51.190477 140392160929792 resource_reader.py:50] system_path_file_exists:flaxformer/flaxformer/t5x/configs/moe/models/st_moe_decoder_only_large.gin
E0512 01:44:51.190902 140392160929792 resource_reader.py:55] Path not found: flaxformer/flaxformer/t5x/configs/moe/models/st_moe_decoder_only_large.gin
I0512 01:44:51.190964 140392160929792 resource_reader.py:50] system_path_file_exists:./flaxformer/flaxformer/t5x/configs/moe/models/st_moe_decoder_only_large.gin
E0512 01:44:51.191015 140392160929792 resource_reader.py:55] Path not found: ./flaxformer/flaxformer/t5x/configs/moe/models/st_moe_decoder_only_large.gin
I0512 01:44:51.185799 139812357228544 resource_reader.py:50] system_path_file_exists:t5x/examples/t5/t5_1_1/examples/openmoe_large_full_lm.gin
I0512 01:44:51.186628 139812357228544 resource_reader.py:37] gin-config opened resource file:/home/rosinality/t5x/t5x/examples/t5/t5_1_1/examples/openmoe_large_full_lm.gin
I0512 01:44:51.224714 139997397456896 resource_reader.py:50] system_path_file_exists:flaxformer/flaxformer/t5x/configs/moe/models/st_moe_decoder_only_large.gin
E0512 01:44:51.225171 139997397456896 resource_reader.py:55] Path not found: flaxformer/flaxformer/t5x/configs/moe/models/st_moe_decoder_only_large.gin
I0512 01:44:51.225235 139997397456896 resource_reader.py:50] system_path_file_exists:./flaxformer/flaxformer/t5x/configs/moe/models/st_moe_decoder_only_large.gin
E0512 01:44:51.225282 139997397456896 resource_reader.py:55] Path not found: ./flaxformer/flaxformer/t5x/configs/moe/models/st_moe_decoder_only_large.gin
I0512 01:44:51.243236 140577131780096 resource_reader.py:50] system_path_file_exists:flaxformer/flaxformer/t5x/configs/moe/models/st_moe_decoder_only_large.gin
E0512 01:44:51.243674 140577131780096 resource_reader.py:55] Path not found: flaxformer/flaxformer/t5x/configs/moe/models/st_moe_decoder_only_large.gin
I0512 01:44:51.243736 140577131780096 resource_reader.py:50] system_path_file_exists:./flaxformer/flaxformer/t5x/configs/moe/models/st_moe_decoder_only_large.gin
I0512 01:44:51.246636 140392160929792 resource_reader.py:50] system_path_file_exists:flaxformer/flaxformer/t5x/configs/moe/models/tokens_choose_decoder_only_large.gin
E0512 01:44:51.246831 140392160929792 resource_reader.py:55] Path not found: flaxformer/flaxformer/t5x/configs/moe/models/tokens_choose_decoder_only_large.gin
E0512 01:44:51.243790 140577131780096 resource_reader.py:55] Path not found: ./flaxformer/flaxformer/t5x/configs/moe/models/st_moe_decoder_only_large.gin
I0512 01:44:51.246889 140392160929792 resource_reader.py:50] system_path_file_exists:./flaxformer/flaxformer/t5x/configs/moe/models/tokens_choose_decoder_only_large.gin
I0512 01:44:51.224391 139676058949632 resource_reader.py:50] system_path_file_exists:t5x/examples/t5/t5_1_1/examples/openmoe_large_full_lm.gin
E0512 01:44:51.246933 140392160929792 resource_reader.py:55] Path not found: ./flaxformer/flaxformer/t5x/configs/moe/models/tokens_choose_decoder_only_large.gin
I0512 01:44:51.247806 140392160929792 resource_reader.py:50] system_path_file_exists:flaxformer/flaxformer/t5x/configs/moe/architectures/moe_decoder_only.gin
E0512 01:44:51.247940 140392160929792 resource_reader.py:55] Path not found: flaxformer/flaxformer/t5x/configs/moe/architectures/moe_decoder_only.gin
I0512 01:44:51.247993 140392160929792 resource_reader.py:50] system_path_file_exists:./flaxformer/flaxformer/t5x/configs/moe/architectures/moe_decoder_only.gin
E0512 01:44:51.248035 140392160929792 resource_reader.py:55] Path not found: ./flaxformer/flaxformer/t5x/configs/moe/architectures/moe_decoder_only.gin
I0512 01:44:51.225209 139676058949632 resource_reader.py:37] gin-config opened resource file:/home/rosinality/t5x/t5x/examples/t5/t5_1_1/examples/openmoe_large_full_lm.gin
I0512 01:44:51.276521 140188559153152 resource_reader.py:50] system_path_file_exists:flaxformer/flaxformer/t5x/configs/moe/models/st_moe_decoder_only_large.gin
E0512 01:44:51.276937 140188559153152 resource_reader.py:55] Path not found: flaxformer/flaxformer/t5x/configs/moe/models/st_moe_decoder_only_large.gin
I0512 01:44:51.276999 140188559153152 resource_reader.py:50] system_path_file_exists:./flaxformer/flaxformer/t5x/configs/moe/models/st_moe_decoder_only_large.gin
E0512 01:44:51.277055 140188559153152 resource_reader.py:55] Path not found: ./flaxformer/flaxformer/t5x/configs/moe/models/st_moe_decoder_only_large.gin
I0512 01:44:51.281165 139997397456896 resource_reader.py:50] system_path_file_exists:flaxformer/flaxformer/t5x/configs/moe/models/tokens_choose_decoder_only_large.gin
I0512 01:44:51.282777 140172769531904 resource_reader.py:50] system_path_file_exists:flaxformer/flaxformer/t5x/configs/moe/models/st_moe_decoder_only_large.gin
E0512 01:44:51.283205 140172769531904 resource_reader.py:55] Path not found: flaxformer/flaxformer/t5x/configs/moe/models/st_moe_decoder_only_large.gin
I0512 01:44:51.283266 140172769531904 resource_reader.py:50] system_path_file_exists:./flaxformer/flaxformer/t5x/configs/moe/models/st_moe_decoder_only_large.gin
E0512 01:44:51.281361 139997397456896 resource_reader.py:55] Path not found: flaxformer/flaxformer/t5x/configs/moe/models/tokens_choose_decoder_only_large.gin
I0512 01:44:51.281420 139997397456896 resource_reader.py:50] system_path_file_exists:./flaxformer/flaxformer/t5x/configs/moe/models/tokens_choose_decoder_only_large.gin
E0512 01:44:51.281465 139997397456896 resource_reader.py:55] Path not found: ./flaxformer/flaxformer/t5x/configs/moe/models/tokens_choose_decoder_only_large.gin
I0512 01:44:51.282325 139997397456896 resource_reader.py:50] system_path_file_exists:flaxformer/flaxformer/t5x/configs/moe/architectures/moe_decoder_only.gin
E0512 01:44:51.283314 140172769531904 resource_reader.py:55] Path not found: ./flaxformer/flaxformer/t5x/configs/moe/models/st_moe_decoder_only_large.gin
E0512 01:44:51.282457 139997397456896 resource_reader.py:55] Path not found: flaxformer/flaxformer/t5x/configs/moe/architectures/moe_decoder_only.gin
I0512 01:44:51.282510 139997397456896 resource_reader.py:50] system_path_file_exists:./flaxformer/flaxformer/t5x/configs/moe/architectures/moe_decoder_only.gin
E0512 01:44:51.282555 139997397456896 resource_reader.py:55] Path not found: ./flaxformer/flaxformer/t5x/configs/moe/architectures/moe_decoder_only.gin
I0512 01:44:51.292038 140392160929792 resource_reader.py:50] system_path_file_exists:t5x/contrib/moe/configs/runs/pretrain.gin
I0512 01:44:51.299879 140577131780096 resource_reader.py:50] system_path_file_exists:flaxformer/flaxformer/t5x/configs/moe/models/tokens_choose_decoder_only_large.gin
E0512 01:44:51.300070 140577131780096 resource_reader.py:55] Path not found: flaxformer/flaxformer/t5x/configs/moe/models/tokens_choose_decoder_only_large.gin
I0512 01:44:51.300128 140577131780096 resource_reader.py:50] system_path_file_exists:./flaxformer/flaxformer/t5x/configs/moe/models/tokens_choose_decoder_only_large.gin
E0512 01:44:51.300174 140577131780096 resource_reader.py:55] Path not found: ./flaxformer/flaxformer/t5x/configs/moe/models/tokens_choose_decoder_only_large.gin
I0512 01:44:51.301040 140577131780096 resource_reader.py:50] system_path_file_exists:flaxformer/flaxformer/t5x/configs/moe/architectures/moe_decoder_only.gin
E0512 01:44:51.301173 140577131780096 resource_reader.py:55] Path not found: flaxformer/flaxformer/t5x/configs/moe/architectures/moe_decoder_only.gin
I0512 01:44:51.301227 140577131780096 resource_reader.py:50] system_path_file_exists:./flaxformer/flaxformer/t5x/configs/moe/architectures/moe_decoder_only.gin
E0512 01:44:51.301269 140577131780096 resource_reader.py:55] Path not found: ./flaxformer/flaxformer/t5x/configs/moe/architectures/moe_decoder_only.gin
I0512 01:44:51.327164 139997397456896 resource_reader.py:50] system_path_file_exists:t5x/contrib/moe/configs/runs/pretrain.gin
I0512 01:44:51.332738 140188559153152 resource_reader.py:50] system_path_file_exists:flaxformer/flaxformer/t5x/configs/moe/models/tokens_choose_decoder_only_large.gin
E0512 01:44:51.332926 140188559153152 resource_reader.py:55] Path not found: flaxformer/flaxformer/t5x/configs/moe/models/tokens_choose_decoder_only_large.gin
I0512 01:44:51.332983 140188559153152 resource_reader.py:50] system_path_file_exists:./flaxformer/flaxformer/t5x/configs/moe/models/tokens_choose_decoder_only_large.gin
E0512 01:44:51.333035 140188559153152 resource_reader.py:55] Path not found: ./flaxformer/flaxformer/t5x/configs/moe/models/tokens_choose_decoder_only_large.gin
I0512 01:44:51.333915 140188559153152 resource_reader.py:50] system_path_file_exists:flaxformer/flaxformer/t5x/configs/moe/architectures/moe_decoder_only.gin
E0512 01:44:51.334056 140188559153152 resource_reader.py:55] Path not found: flaxformer/flaxformer/t5x/configs/moe/architectures/moe_decoder_only.gin
I0512 01:44:51.334110 140188559153152 resource_reader.py:50] system_path_file_exists:./flaxformer/flaxformer/t5x/configs/moe/architectures/moe_decoder_only.gin
E0512 01:44:51.334152 140188559153152 resource_reader.py:55] Path not found: ./flaxformer/flaxformer/t5x/configs/moe/architectures/moe_decoder_only.gin
I0512 01:44:51.331641 140316961150976 resource_reader.py:50] system_path_file_exists:flaxformer/flaxformer/t5x/configs/moe/models/st_moe_decoder_only_large.gin
E0512 01:44:51.332068 140316961150976 resource_reader.py:55] Path not found: flaxformer/flaxformer/t5x/configs/moe/models/st_moe_decoder_only_large.gin
I0512 01:44:51.332132 140316961150976 resource_reader.py:50] system_path_file_exists:./flaxformer/flaxformer/t5x/configs/moe/models/st_moe_decoder_only_large.gin
E0512 01:44:51.332180 140316961150976 resource_reader.py:55] Path not found: ./flaxformer/flaxformer/t5x/configs/moe/models/st_moe_decoder_only_large.gin
I0512 01:44:51.339124 140172769531904 resource_reader.py:50] system_path_file_exists:flaxformer/flaxformer/t5x/configs/moe/models/tokens_choose_decoder_only_large.gin
E0512 01:44:51.339308 140172769531904 resource_reader.py:55] Path not found: flaxformer/flaxformer/t5x/configs/moe/models/tokens_choose_decoder_only_large.gin
I0512 01:44:51.339366 140172769531904 resource_reader.py:50] system_path_file_exists:./flaxformer/flaxformer/t5x/configs/moe/models/tokens_choose_decoder_only_large.gin
E0512 01:44:51.339409 140172769531904 resource_reader.py:55] Path not found: ./flaxformer/flaxformer/t5x/configs/moe/models/tokens_choose_decoder_only_large.gin
I0512 01:44:51.340276 140172769531904 resource_reader.py:50] system_path_file_exists:flaxformer/flaxformer/t5x/configs/moe/architectures/moe_decoder_only.gin
E0512 01:44:51.340427 140172769531904 resource_reader.py:55] Path not found: flaxformer/flaxformer/t5x/configs/moe/architectures/moe_decoder_only.gin
I0512 01:44:51.340485 140172769531904 resource_reader.py:50] system_path_file_exists:./flaxformer/flaxformer/t5x/configs/moe/architectures/moe_decoder_only.gin
E0512 01:44:51.340526 140172769531904 resource_reader.py:55] Path not found: ./flaxformer/flaxformer/t5x/configs/moe/architectures/moe_decoder_only.gin
I0512 01:44:51.345632 140577131780096 resource_reader.py:50] system_path_file_exists:t5x/contrib/moe/configs/runs/pretrain.gin
I0512 01:44:51.292394 140392160929792 resource_reader.py:37] gin-config opened resource file:/home/rosinality/t5x/t5x/contrib/moe/configs/runs/pretrain.gin
I0512 01:44:51.292899 140392160929792 resource_reader.py:50] system_path_file_exists:t5x/configs/runs/pretrain.gin
I0512 01:44:51.293202 140392160929792 resource_reader.py:37] gin-config opened resource file:/home/rosinality/t5x/t5x/configs/runs/pretrain.gin
I0512 01:44:51.307106 140392160929792 gin_utils.py:83] Gin Configuration:
I0512 01:44:51.322987 140392160929792 gin_utils.py:85] from __gin__ import dynamic_registration
I0512 01:44:51.323057 140392160929792 gin_utils.py:85] import __main__ as train_script
I0512 01:44:51.323099 140392160929792 gin_utils.py:85] from flax import linen
I0512 01:44:51.323133 140392160929792 gin_utils.py:85] import flaxformer
I0512 01:44:51.323166 140392160929792 gin_utils.py:85] from flaxformer.architectures.moe import moe_architecture
I0512 01:44:51.323198 140392160929792 gin_utils.py:85] from flaxformer.architectures.moe import moe_enums
I0512 01:44:51.323231 140392160929792 gin_utils.py:85] from flaxformer.architectures.moe import moe_layers
I0512 01:44:51.323263 140392160929792 gin_utils.py:85] from flaxformer.architectures.moe import routing
I0512 01:44:51.323295 140392160929792 gin_utils.py:85] from flaxformer.architectures.t5 import t5_architecture
I0512 01:44:51.323327 140392160929792 gin_utils.py:85] from flaxformer.components.attention import dense_attention
I0512 01:44:51.323359 140392160929792 gin_utils.py:85] from flaxformer.components.attention import memory_efficient_attention
I0512 01:44:51.323390 140392160929792 gin_utils.py:85] from flaxformer.components import dense
I0512 01:44:51.323422 140392160929792 gin_utils.py:85] from flaxformer.components import embedding
I0512 01:44:51.323454 140392160929792 gin_utils.py:85] from flaxformer.components import layer_norm
I0512 01:44:51.323486 140392160929792 gin_utils.py:85] from gin import config
I0512 01:44:51.323518 140392160929792 gin_utils.py:85] import seqio
I0512 01:44:51.323549 140392160929792 gin_utils.py:85] import t5.data.mixtures
I0512 01:44:51.323581 140392160929792 gin_utils.py:85] from t5x import adafactor
I0512 01:44:51.323613 140392160929792 gin_utils.py:85] from t5x.contrib.moe import adafactor_utils
I0512 01:44:51.323644 140392160929792 gin_utils.py:85] from t5x.contrib.moe import models
I0512 01:44:51.323676 140392160929792 gin_utils.py:85] from t5x.contrib.moe import partitioning
I0512 01:44:51.323708 140392160929792 gin_utils.py:85] from t5x.contrib.moe import trainer as moe_trainer
I0512 01:44:51.323747 140392160929792 gin_utils.py:85] from t5x import gin_utils
I0512 01:44:51.323781 140392160929792 gin_utils.py:85] from t5x import partitioning as partitioning2
I0512 01:44:51.323837 140392160929792 gin_utils.py:85] from t5x import trainer
I0512 01:44:51.323871 140392160929792 gin_utils.py:85] from t5x import utils
I0512 01:44:51.323903 140392160929792 gin_utils.py:85] 
I0512 01:44:51.323935 140392160929792 gin_utils.py:85] # Macros:
I0512 01:44:51.323967 140392160929792 gin_utils.py:85] # ==============================================================================
I0512 01:44:51.324001 140392160929792 gin_utils.py:85] ACTIVATION_DTYPE = 'bfloat16'
I0512 01:44:51.324034 140392160929792 gin_utils.py:85] ACTIVATION_PARTITIONING_DIMS = 1
I0512 01:44:51.324066 140392160929792 gin_utils.py:85] ARCHITECTURE = @t5_architecture.DecoderOnly()
I0512 01:44:51.324098 140392160929792 gin_utils.py:85] AUX_LOSS_FACTOR = 0.01
I0512 01:44:51.324130 140392160929792 gin_utils.py:85] BATCH_SIZE = 384
I0512 01:44:51.324161 140392160929792 gin_utils.py:85] BIAS_INIT = @bias_init/linen.initializers.normal()
I0512 01:44:51.324193 140392160929792 gin_utils.py:85] DECODER_SPARSE_LAYOUT = %flaxformer.architectures.moe.moe_enums.LayerLayout.MIXED
I0512 01:44:51.324224 140392160929792 gin_utils.py:85] DROPOUT_FACTORY = @dropout_factory/linen.Dropout
I0512 01:44:51.324256 140392160929792 gin_utils.py:85] DROPOUT_RATE = 0.0
I0512 01:44:51.324288 140392160929792 gin_utils.py:85] EMBED_DIM = 2048
I0512 01:44:51.324320 140392160929792 gin_utils.py:85] EVAL_EXPERT_CAPACITY_FACTOR = 2.0
I0512 01:44:51.324351 140392160929792 gin_utils.py:85] EXPERT_DROPOUT_RATE = %DROPOUT_RATE
I0512 01:44:51.324383 140392160929792 gin_utils.py:85] EXPERT_MLP_DIM = %MLP_DIM
I0512 01:44:51.324415 140392160929792 gin_utils.py:85] GROUP_SIZE = 4096
I0512 01:44:51.324446 140392160929792 gin_utils.py:85] HEAD_DIM = 128
I0512 01:44:51.324478 140392160929792 gin_utils.py:85] JITTER_NOISE = 0.0
I0512 01:44:51.324509 140392160929792 gin_utils.py:85] LABEL_SMOOTHING = 0.0
I0512 01:44:51.324541 140392160929792 gin_utils.py:85] LOSS_NORMALIZING_FACTOR = 'NUM_REAL_TARGET_TOKENS'
I0512 01:44:51.324573 140392160929792 gin_utils.py:85] MIXTURE_OR_TASK_MODULE = None
I0512 01:44:51.324605 140392160929792 gin_utils.py:85] MIXTURE_OR_TASK_NAME = 'mix_full_lm_test'
I0512 01:44:51.324639 140392160929792 gin_utils.py:85] MLP_DIM = 8192
I0512 01:44:51.324672 140392160929792 gin_utils.py:85] MODEL = @models.MoeDecoderOnlyModel()
I0512 01:44:51.324704 140392160929792 gin_utils.py:85] MODEL_DIR = 'gs://rosinality-tpu-bucket/openmoe_8b/training'
I0512 01:44:51.324741 140392160929792 gin_utils.py:85] MODEL_PARALLEL_SUBMESH = None
I0512 01:44:51.324775 140392160929792 gin_utils.py:85] MOE_TRUNCATED_DTYPE = 'bfloat16'
I0512 01:44:51.324807 140392160929792 gin_utils.py:85] NUM_DECODER_LAYERS = 24
I0512 01:44:51.324839 140392160929792 gin_utils.py:85] NUM_DECODER_SPARSE_LAYERS = 4
I0512 01:44:51.324870 140392160929792 gin_utils.py:85] NUM_EMBEDDINGS = 256384
I0512 01:44:51.324901 140392160929792 gin_utils.py:85] NUM_EXPERT_PARTITIONS = 8
I0512 01:44:51.324933 140392160929792 gin_utils.py:85] NUM_EXPERTS = 8
I0512 01:44:51.324965 140392160929792 gin_utils.py:85] NUM_HEADS = 24
I0512 01:44:51.324998 140392160929792 gin_utils.py:85] NUM_MODEL_PARTITIONS = 4
I0512 01:44:51.325031 140392160929792 gin_utils.py:85] NUM_SELECTED_EXPERTS = 2
I0512 01:44:51.325063 140392160929792 gin_utils.py:85] OPTIMIZER = @adafactor.Adafactor()
I0512 01:44:51.325095 140392160929792 gin_utils.py:85] RANDOM_SEED = None
I0512 01:44:51.325127 140392160929792 gin_utils.py:85] ROUTER_Z_LOSS_FACTOR = 0.0001
I0512 01:44:51.325159 140392160929792 gin_utils.py:85] SCALE = 0.1
I0512 01:44:51.325191 140392160929792 gin_utils.py:85] SHUFFLE_TRAIN_EXAMPLES = True
I0512 01:44:51.325222 140392160929792 gin_utils.py:85] TASK_FEATURE_LENGTHS = {'targets': 2048}
I0512 01:44:51.325254 140392160929792 gin_utils.py:85] TRAIN_EXPERT_CAPACITY_FACTOR = 1.25
I0512 01:44:51.325286 140392160929792 gin_utils.py:85] TRAIN_STEPS = 500000
I0512 01:44:51.325318 140392160929792 gin_utils.py:85] USE_CACHED_TASKS = False
I0512 01:44:51.325349 140392160929792 gin_utils.py:85] USE_HARDWARE_RNG = False
I0512 01:44:51.325381 140392160929792 gin_utils.py:85] VOCABULARY = @seqio.SentencePieceVocabulary()
I0512 01:44:51.325413 140392160929792 gin_utils.py:85] Z_LOSS = 0.0001
I0512 01:44:51.325445 140392160929792 gin_utils.py:85] 
I0512 01:44:51.325476 140392160929792 gin_utils.py:85] # Parameters for adafactor.Adafactor:
I0512 01:44:51.325508 140392160929792 gin_utils.py:85] # ==============================================================================
I0512 01:44:51.325540 140392160929792 gin_utils.py:85] adafactor.Adafactor.decay_rate = 0.8
I0512 01:44:51.325572 140392160929792 gin_utils.py:85] adafactor.Adafactor.logical_factor_rules = @adafactor_utils.logical_factor_rules()
I0512 01:44:51.325603 140392160929792 gin_utils.py:85] adafactor.Adafactor.step_offset = 0
I0512 01:44:51.325635 140392160929792 gin_utils.py:85] 
I0512 01:44:51.378709 140188559153152 resource_reader.py:50] system_path_file_exists:t5x/contrib/moe/configs/runs/pretrain.gin
I0512 01:44:51.384504 140172769531904 resource_reader.py:50] system_path_file_exists:t5x/contrib/moe/configs/runs/pretrain.gin
I0512 01:44:51.327538 139997397456896 resource_reader.py:37] gin-config opened resource file:/home/rosinality/t5x/t5x/contrib/moe/configs/runs/pretrain.gin
I0512 01:44:51.328042 139997397456896 resource_reader.py:50] system_path_file_exists:t5x/configs/runs/pretrain.gin
I0512 01:44:51.328352 139997397456896 resource_reader.py:37] gin-config opened resource file:/home/rosinality/t5x/t5x/configs/runs/pretrain.gin
I0512 01:44:51.342298 139997397456896 gin_utils.py:83] Gin Configuration:
I0512 01:44:51.358203 139997397456896 gin_utils.py:85] from __gin__ import dynamic_registration
I0512 01:44:51.358273 139997397456896 gin_utils.py:85] import __main__ as train_script
I0512 01:44:51.358322 139997397456896 gin_utils.py:85] from flax import linen
I0512 01:44:51.358359 139997397456896 gin_utils.py:85] import flaxformer
I0512 01:44:51.358392 139997397456896 gin_utils.py:85] from flaxformer.architectures.moe import moe_architecture
I0512 01:44:51.358425 139997397456896 gin_utils.py:85] from flaxformer.architectures.moe import moe_enums
I0512 01:44:51.358458 139997397456896 gin_utils.py:85] from flaxformer.architectures.moe import moe_layers
I0512 01:44:51.358490 139997397456896 gin_utils.py:85] from flaxformer.architectures.moe import routing
I0512 01:44:51.358525 139997397456896 gin_utils.py:85] from flaxformer.architectures.t5 import t5_architecture
I0512 01:44:51.358560 139997397456896 gin_utils.py:85] from flaxformer.components.attention import dense_attention
I0512 01:44:51.358596 139997397456896 gin_utils.py:85] from flaxformer.components.attention import memory_efficient_attention
I0512 01:44:51.358629 139997397456896 gin_utils.py:85] from flaxformer.components import dense
I0512 01:44:51.358662 139997397456896 gin_utils.py:85] from flaxformer.components import embedding
I0512 01:44:51.358694 139997397456896 gin_utils.py:85] from flaxformer.components import layer_norm
I0512 01:44:51.358726 139997397456896 gin_utils.py:85] from gin import config
I0512 01:44:51.358758 139997397456896 gin_utils.py:85] import seqio
I0512 01:44:51.358790 139997397456896 gin_utils.py:85] import t5.data.mixtures
I0512 01:44:51.358822 139997397456896 gin_utils.py:85] from t5x import adafactor
I0512 01:44:51.358854 139997397456896 gin_utils.py:85] from t5x.contrib.moe import adafactor_utils
I0512 01:44:51.358886 139997397456896 gin_utils.py:85] from t5x.contrib.moe import models
I0512 01:44:51.358918 139997397456896 gin_utils.py:85] from t5x.contrib.moe import partitioning
I0512 01:44:51.358950 139997397456896 gin_utils.py:85] from t5x.contrib.moe import trainer as moe_trainer
I0512 01:44:51.358982 139997397456896 gin_utils.py:85] from t5x import gin_utils
I0512 01:44:51.359014 139997397456896 gin_utils.py:85] from t5x import partitioning as partitioning2
I0512 01:44:51.359046 139997397456896 gin_utils.py:85] from t5x import trainer
I0512 01:44:51.359078 139997397456896 gin_utils.py:85] from t5x import utils
I0512 01:44:51.359109 139997397456896 gin_utils.py:85] 
I0512 01:44:51.359142 139997397456896 gin_utils.py:85] # Macros:
I0512 01:44:51.359174 139997397456896 gin_utils.py:85] # ==============================================================================
I0512 01:44:51.359206 139997397456896 gin_utils.py:85] ACTIVATION_DTYPE = 'bfloat16'
I0512 01:44:51.359238 139997397456896 gin_utils.py:85] ACTIVATION_PARTITIONING_DIMS = 1
I0512 01:44:51.359270 139997397456896 gin_utils.py:85] ARCHITECTURE = @t5_architecture.DecoderOnly()
I0512 01:44:51.359302 139997397456896 gin_utils.py:85] AUX_LOSS_FACTOR = 0.01
I0512 01:44:51.359340 139997397456896 gin_utils.py:85] BATCH_SIZE = 384
I0512 01:44:51.359373 139997397456896 gin_utils.py:85] BIAS_INIT = @bias_init/linen.initializers.normal()
I0512 01:44:51.359405 139997397456896 gin_utils.py:85] DECODER_SPARSE_LAYOUT = %flaxformer.architectures.moe.moe_enums.LayerLayout.MIXED
I0512 01:44:51.359438 139997397456896 gin_utils.py:85] DROPOUT_FACTORY = @dropout_factory/linen.Dropout
I0512 01:44:51.359470 139997397456896 gin_utils.py:85] DROPOUT_RATE = 0.0
I0512 01:44:51.359502 139997397456896 gin_utils.py:85] EMBED_DIM = 2048
I0512 01:44:51.359538 139997397456896 gin_utils.py:85] EVAL_EXPERT_CAPACITY_FACTOR = 2.0
I0512 01:44:51.359571 139997397456896 gin_utils.py:85] EXPERT_DROPOUT_RATE = %DROPOUT_RATE
I0512 01:44:51.359605 139997397456896 gin_utils.py:85] EXPERT_MLP_DIM = %MLP_DIM
I0512 01:44:51.359637 139997397456896 gin_utils.py:85] GROUP_SIZE = 4096
I0512 01:44:51.359669 139997397456896 gin_utils.py:85] HEAD_DIM = 128
I0512 01:44:51.359700 139997397456896 gin_utils.py:85] JITTER_NOISE = 0.0
I0512 01:44:51.359732 139997397456896 gin_utils.py:85] LABEL_SMOOTHING = 0.0
I0512 01:44:51.359764 139997397456896 gin_utils.py:85] LOSS_NORMALIZING_FACTOR = 'NUM_REAL_TARGET_TOKENS'
I0512 01:44:51.359796 139997397456896 gin_utils.py:85] MIXTURE_OR_TASK_MODULE = None
I0512 01:44:51.359827 139997397456896 gin_utils.py:85] MIXTURE_OR_TASK_NAME = 'mix_full_lm_test'
I0512 01:44:51.359859 139997397456896 gin_utils.py:85] MLP_DIM = 8192
I0512 01:44:51.359891 139997397456896 gin_utils.py:85] MODEL = @models.MoeDecoderOnlyModel()
I0512 01:44:51.359922 139997397456896 gin_utils.py:85] MODEL_DIR = 'gs://rosinality-tpu-bucket/openmoe_8b/training'
I0512 01:44:51.359954 139997397456896 gin_utils.py:85] MODEL_PARALLEL_SUBMESH = None
I0512 01:44:51.359987 139997397456896 gin_utils.py:85] MOE_TRUNCATED_DTYPE = 'bfloat16'
I0512 01:44:51.360018 139997397456896 gin_utils.py:85] NUM_DECODER_LAYERS = 24
I0512 01:44:51.360050 139997397456896 gin_utils.py:85] NUM_DECODER_SPARSE_LAYERS = 4
I0512 01:44:51.360082 139997397456896 gin_utils.py:85] NUM_EMBEDDINGS = 256384
I0512 01:44:51.360114 139997397456896 gin_utils.py:85] NUM_EXPERT_PARTITIONS = 8
I0512 01:44:51.360145 139997397456896 gin_utils.py:85] NUM_EXPERTS = 8
I0512 01:44:51.388968 140316961150976 resource_reader.py:50] system_path_file_exists:flaxformer/flaxformer/t5x/configs/moe/models/tokens_choose_decoder_only_large.gin
E0512 01:44:51.389144 140316961150976 resource_reader.py:55] Path not found: flaxformer/flaxformer/t5x/configs/moe/models/tokens_choose_decoder_only_large.gin
I0512 01:44:51.389202 140316961150976 resource_reader.py:50] system_path_file_exists:./flaxformer/flaxformer/t5x/configs/moe/models/tokens_choose_decoder_only_large.gin
I0512 01:44:51.360177 139997397456896 gin_utils.py:85] NUM_HEADS = 24
I0512 01:44:51.360208 139997397456896 gin_utils.py:85] NUM_MODEL_PARTITIONS = 4
I0512 01:44:51.360240 139997397456896 gin_utils.py:85] NUM_SELECTED_EXPERTS = 2
I0512 01:44:51.360272 139997397456896 gin_utils.py:85] OPTIMIZER = @adafactor.Adafactor()
I0512 01:44:51.360303 139997397456896 gin_utils.py:85] RANDOM_SEED = None
I0512 01:44:51.360342 139997397456896 gin_utils.py:85] ROUTER_Z_LOSS_FACTOR = 0.0001
I0512 01:44:51.360374 139997397456896 gin_utils.py:85] SCALE = 0.1
I0512 01:44:51.360406 139997397456896 gin_utils.py:85] SHUFFLE_TRAIN_EXAMPLES = True
I0512 01:44:51.360438 139997397456896 gin_utils.py:85] TASK_FEATURE_LENGTHS = {'targets': 2048}
I0512 01:44:51.360470 139997397456896 gin_utils.py:85] TRAIN_EXPERT_CAPACITY_FACTOR = 1.25
I0512 01:44:51.360502 139997397456896 gin_utils.py:85] TRAIN_STEPS = 500000
I0512 01:44:51.360533 139997397456896 gin_utils.py:85] USE_CACHED_TASKS = False
I0512 01:44:51.360566 139997397456896 gin_utils.py:85] USE_HARDWARE_RNG = False
I0512 01:44:51.360600 139997397456896 gin_utils.py:85] VOCABULARY = @seqio.SentencePieceVocabulary()
I0512 01:44:51.360632 139997397456896 gin_utils.py:85] Z_LOSS = 0.0001
I0512 01:44:51.360664 139997397456896 gin_utils.py:85] 
I0512 01:44:51.360696 139997397456896 gin_utils.py:85] # Parameters for adafactor.Adafactor:
I0512 01:44:51.360728 139997397456896 gin_utils.py:85] # ==============================================================================
I0512 01:44:51.360760 139997397456896 gin_utils.py:85] adafactor.Adafactor.decay_rate = 0.8
I0512 01:44:51.360792 139997397456896 gin_utils.py:85] adafactor.Adafactor.logical_factor_rules = @adafactor_utils.logical_factor_rules()
I0512 01:44:51.360824 139997397456896 gin_utils.py:85] adafactor.Adafactor.step_offset = 0
I0512 01:44:51.360856 139997397456896 gin_utils.py:85] 
E0512 01:44:51.389245 140316961150976 resource_reader.py:55] Path not found: ./flaxformer/flaxformer/t5x/configs/moe/models/tokens_choose_decoder_only_large.gin
I0512 01:44:51.390128 140316961150976 resource_reader.py:50] system_path_file_exists:flaxformer/flaxformer/t5x/configs/moe/architectures/moe_decoder_only.gin
E0512 01:44:51.390260 140316961150976 resource_reader.py:55] Path not found: flaxformer/flaxformer/t5x/configs/moe/architectures/moe_decoder_only.gin
I0512 01:44:51.390311 140316961150976 resource_reader.py:50] system_path_file_exists:./flaxformer/flaxformer/t5x/configs/moe/architectures/moe_decoder_only.gin
E0512 01:44:51.390351 140316961150976 resource_reader.py:55] Path not found: ./flaxformer/flaxformer/t5x/configs/moe/architectures/moe_decoder_only.gin
I0512 01:44:51.325667 140392160929792 gin_utils.py:85] # Parameters for utils.CheckpointConfig:
I0512 01:44:51.325698 140392160929792 gin_utils.py:85] # ==============================================================================
I0512 01:44:51.325735 140392160929792 gin_utils.py:85] utils.CheckpointConfig.restore = @utils.RestoreCheckpointConfig()
I0512 01:44:51.325768 140392160929792 gin_utils.py:85] utils.CheckpointConfig.save = @utils.SaveCheckpointConfig()
I0512 01:44:51.325800 140392160929792 gin_utils.py:85] 
I0512 01:44:51.325832 140392160929792 gin_utils.py:85] # Parameters for partitioning.compute_num_model_partitions:
I0512 01:44:51.325864 140392160929792 gin_utils.py:85] # ==============================================================================
I0512 01:44:51.325896 140392160929792 gin_utils.py:85] partitioning.compute_num_model_partitions.model_parallel_submesh = \
I0512 01:44:51.325928 140392160929792 gin_utils.py:85]     %MODEL_PARALLEL_SUBMESH
I0512 01:44:51.325960 140392160929792 gin_utils.py:85] partitioning.compute_num_model_partitions.num_model_partitions = \
I0512 01:44:51.325994 140392160929792 gin_utils.py:85]     %NUM_MODEL_PARTITIONS
I0512 01:44:51.326027 140392160929792 gin_utils.py:85] 
I0512 01:44:51.326059 140392160929792 gin_utils.py:85] # Parameters for utils.create_learning_rate_scheduler:
I0512 01:44:51.326114 140392160929792 gin_utils.py:85] # ==============================================================================
I0512 01:44:51.326151 140392160929792 gin_utils.py:85] utils.create_learning_rate_scheduler.base_learning_rate = 1.0
I0512 01:44:51.326184 140392160929792 gin_utils.py:85] utils.create_learning_rate_scheduler.factors = 'constant * rsqrt_decay'
I0512 01:44:51.326216 140392160929792 gin_utils.py:85] utils.create_learning_rate_scheduler.warmup_steps = 10000
I0512 01:44:51.326248 140392160929792 gin_utils.py:85] 
I0512 01:44:51.326279 140392160929792 gin_utils.py:85] # Parameters for train/utils.DatasetConfig:
I0512 01:44:51.326311 140392160929792 gin_utils.py:85] # ==============================================================================
I0512 01:44:51.326343 140392160929792 gin_utils.py:85] train/utils.DatasetConfig.batch_size = %BATCH_SIZE
I0512 01:44:51.326375 140392160929792 gin_utils.py:85] train/utils.DatasetConfig.mixture_or_task_name = %MIXTURE_OR_TASK_NAME
I0512 01:44:51.326407 140392160929792 gin_utils.py:85] train/utils.DatasetConfig.module = %MIXTURE_OR_TASK_MODULE
I0512 01:44:51.326438 140392160929792 gin_utils.py:85] train/utils.DatasetConfig.pack = True
I0512 01:44:51.326470 140392160929792 gin_utils.py:85] train/utils.DatasetConfig.seed = 42
I0512 01:44:51.326502 140392160929792 gin_utils.py:85] train/utils.DatasetConfig.shuffle = %SHUFFLE_TRAIN_EXAMPLES
I0512 01:44:51.326534 140392160929792 gin_utils.py:85] train/utils.DatasetConfig.split = 'train'
I0512 01:44:51.326566 140392160929792 gin_utils.py:85] train/utils.DatasetConfig.task_feature_lengths = %TASK_FEATURE_LENGTHS
I0512 01:44:51.326597 140392160929792 gin_utils.py:85] train/utils.DatasetConfig.use_cached = %USE_CACHED_TASKS
I0512 01:44:51.326629 140392160929792 gin_utils.py:85] 
I0512 01:44:51.326661 140392160929792 gin_utils.py:85] # Parameters for train_eval/utils.DatasetConfig:
I0512 01:44:51.326692 140392160929792 gin_utils.py:85] # ==============================================================================
I0512 01:44:51.326729 140392160929792 gin_utils.py:85] train_eval/utils.DatasetConfig.batch_size = 128
I0512 01:44:51.326763 140392160929792 gin_utils.py:85] train_eval/utils.DatasetConfig.mixture_or_task_name = %MIXTURE_OR_TASK_NAME
I0512 01:44:51.326796 140392160929792 gin_utils.py:85] train_eval/utils.DatasetConfig.module = %MIXTURE_OR_TASK_MODULE
I0512 01:44:51.326828 140392160929792 gin_utils.py:85] train_eval/utils.DatasetConfig.pack = True
I0512 01:44:51.326859 140392160929792 gin_utils.py:85] train_eval/utils.DatasetConfig.seed = 42
I0512 01:44:51.326891 140392160929792 gin_utils.py:85] train_eval/utils.DatasetConfig.shuffle = False
I0512 01:44:51.326923 140392160929792 gin_utils.py:85] train_eval/utils.DatasetConfig.split = 'validation'
I0512 01:44:51.326955 140392160929792 gin_utils.py:85] train_eval/utils.DatasetConfig.task_feature_lengths = %TASK_FEATURE_LENGTHS
I0512 01:44:51.326988 140392160929792 gin_utils.py:85] train_eval/utils.DatasetConfig.use_cached = %USE_CACHED_TASKS
I0512 01:44:51.327021 140392160929792 gin_utils.py:85] 
I0512 01:44:51.327053 140392160929792 gin_utils.py:85] # Parameters for t5_architecture.DecoderLayer:
I0512 01:44:51.327085 140392160929792 gin_utils.py:85] # ==============================================================================
I0512 01:44:51.327117 140392160929792 gin_utils.py:85] t5_architecture.DecoderLayer.activation_partitioning_dims = \
I0512 01:44:51.327149 140392160929792 gin_utils.py:85]     %ACTIVATION_PARTITIONING_DIMS
I0512 01:44:51.327181 140392160929792 gin_utils.py:85] t5_architecture.DecoderLayer.dropout_factory = %DROPOUT_FACTORY
I0512 01:44:51.327213 140392160929792 gin_utils.py:85] t5_architecture.DecoderLayer.encoder_decoder_attention = None
I0512 01:44:51.327244 140392160929792 gin_utils.py:85] t5_architecture.DecoderLayer.layer_norm_factory = @layer_norm.T5LayerNorm
I0512 01:44:51.327276 140392160929792 gin_utils.py:85] t5_architecture.DecoderLayer.mlp = @dense.MlpBlock()
I0512 01:44:51.327307 140392160929792 gin_utils.py:85] t5_architecture.DecoderLayer.scanned = False
I0512 01:44:51.327339 140392160929792 gin_utils.py:85] t5_architecture.DecoderLayer.self_attention = \
I0512 01:44:51.327371 140392160929792 gin_utils.py:85]     @dense_attention.MultiHeadDotProductAttention()
I0512 01:44:51.327403 140392160929792 gin_utils.py:85] 
I0512 01:44:51.327435 140392160929792 gin_utils.py:85] # Parameters for t5_architecture.DecoderOnly:
I0512 01:44:51.327466 140392160929792 gin_utils.py:85] # ==============================================================================
I0512 01:44:51.327498 140392160929792 gin_utils.py:85] t5_architecture.DecoderOnly.decoder_factory = @moe_architecture.SparseDecoder
I0512 01:44:51.327530 140392160929792 gin_utils.py:85] t5_architecture.DecoderOnly.dtype = %ACTIVATION_DTYPE
I0512 01:44:51.327562 140392160929792 gin_utils.py:85] t5_architecture.DecoderOnly.shared_token_embedder_factory = @embedding.Embed
I0512 01:44:51.327593 140392160929792 gin_utils.py:85] 
I0512 01:44:51.327625 140392160929792 gin_utils.py:85] # Parameters for output_logits/dense.DenseGeneral:
I0512 01:44:51.327661 140392160929792 gin_utils.py:85] # ==============================================================================
I0512 01:44:51.327693 140392160929792 gin_utils.py:85] output_logits/dense.DenseGeneral.bias_init = %BIAS_INIT
I0512 01:44:51.327730 140392160929792 gin_utils.py:85] output_logits/dense.DenseGeneral.dtype = 'float32'
I0512 01:44:51.327764 140392160929792 gin_utils.py:85] output_logits/dense.DenseGeneral.features = %NUM_EMBEDDINGS
I0512 01:44:51.327795 140392160929792 gin_utils.py:85] output_logits/dense.DenseGeneral.kernel_axis_names = ['embed', 'vocab']
I0512 01:44:51.327827 140392160929792 gin_utils.py:85] output_logits/dense.DenseGeneral.kernel_init = \
I0512 01:44:51.327859 140392160929792 gin_utils.py:85]     @output_logits_kernel_init/linen.initializers.variance_scaling()
I0512 01:44:51.327891 140392160929792 gin_utils.py:85] output_logits/dense.DenseGeneral.use_bias = False
I0512 01:44:51.327923 140392160929792 gin_utils.py:85] 
I0512 01:44:51.327954 140392160929792 gin_utils.py:85] # Parameters for dropout_factory/linen.Dropout:
I0512 01:44:51.327987 140392160929792 gin_utils.py:85] # ==============================================================================
I0512 01:44:51.328021 140392160929792 gin_utils.py:85] dropout_factory/linen.Dropout.broadcast_dims = (-2,)
I0512 01:44:51.328052 140392160929792 gin_utils.py:85] dropout_factory/linen.Dropout.rate = %DROPOUT_RATE
I0512 01:44:51.328084 140392160929792 gin_utils.py:85] 
I0512 01:44:51.346008 140577131780096 resource_reader.py:37] gin-config opened resource file:/home/rosinality/t5x/t5x/contrib/moe/configs/runs/pretrain.gin
I0512 01:44:51.346513 140577131780096 resource_reader.py:50] system_path_file_exists:t5x/configs/runs/pretrain.gin
I0512 01:44:51.346826 140577131780096 resource_reader.py:37] gin-config opened resource file:/home/rosinality/t5x/t5x/configs/runs/pretrain.gin
I0512 01:44:51.360787 140577131780096 gin_utils.py:83] Gin Configuration:
I0512 01:44:51.376479 140577131780096 gin_utils.py:85] from __gin__ import dynamic_registration
I0512 01:44:51.376549 140577131780096 gin_utils.py:85] import __main__ as train_script
I0512 01:44:51.376590 140577131780096 gin_utils.py:85] from flax import linen
I0512 01:44:51.376626 140577131780096 gin_utils.py:85] import flaxformer
I0512 01:44:51.376659 140577131780096 gin_utils.py:85] from flaxformer.architectures.moe import moe_architecture
I0512 01:44:51.376693 140577131780096 gin_utils.py:85] from flaxformer.architectures.moe import moe_enums
I0512 01:44:51.376727 140577131780096 gin_utils.py:85] from flaxformer.architectures.moe import moe_layers
I0512 01:44:51.376760 140577131780096 gin_utils.py:85] from flaxformer.architectures.moe import routing
I0512 01:44:51.376801 140577131780096 gin_utils.py:85] from flaxformer.architectures.t5 import t5_architecture
I0512 01:44:51.376836 140577131780096 gin_utils.py:85] from flaxformer.components.attention import dense_attention
I0512 01:44:51.376869 140577131780096 gin_utils.py:85] from flaxformer.components.attention import memory_efficient_attention
I0512 01:44:51.376907 140577131780096 gin_utils.py:85] from flaxformer.components import dense
I0512 01:44:51.376940 140577131780096 gin_utils.py:85] from flaxformer.components import embedding
I0512 01:44:51.376972 140577131780096 gin_utils.py:85] from flaxformer.components import layer_norm
I0512 01:44:51.377005 140577131780096 gin_utils.py:85] from gin import config
I0512 01:44:51.328116 140392160929792 gin_utils.py:85] # Parameters for embedding.Embed:
I0512 01:44:51.328147 140392160929792 gin_utils.py:85] # ==============================================================================
I0512 01:44:51.328179 140392160929792 gin_utils.py:85] embedding.Embed.attend_dtype = 'float32'
I0512 01:44:51.328211 140392160929792 gin_utils.py:85] embedding.Embed.cast_input_dtype = 'int32'
I0512 01:44:51.328243 140392160929792 gin_utils.py:85] embedding.Embed.dtype = %ACTIVATION_DTYPE
I0512 01:44:51.328274 140392160929792 gin_utils.py:85] embedding.Embed.embedding_init = @token_embedder_init/linen.initializers.normal()
I0512 01:44:51.328306 140392160929792 gin_utils.py:85] embedding.Embed.features = %EMBED_DIM
I0512 01:44:51.328338 140392160929792 gin_utils.py:85] embedding.Embed.name = 'token_embedder'
I0512 01:44:51.328370 140392160929792 gin_utils.py:85] embedding.Embed.num_embeddings = %NUM_EMBEDDINGS
I0512 01:44:51.328401 140392160929792 gin_utils.py:85] embedding.Embed.one_hot = True
I0512 01:44:51.328433 140392160929792 gin_utils.py:85] 
I0512 01:44:51.328464 140392160929792 gin_utils.py:85] # Parameters for dense.MlpBlock:
I0512 01:44:51.328496 140392160929792 gin_utils.py:85] # ==============================================================================
I0512 01:44:51.328528 140392160929792 gin_utils.py:85] dense.MlpBlock.activations = ('swiglu', 'linear')
I0512 01:44:51.328560 140392160929792 gin_utils.py:85] dense.MlpBlock.bias_init = %BIAS_INIT
I0512 01:44:51.328592 140392160929792 gin_utils.py:85] dense.MlpBlock.dtype = %ACTIVATION_DTYPE
I0512 01:44:51.328623 140392160929792 gin_utils.py:85] dense.MlpBlock.final_dropout_rate = 0
I0512 01:44:51.328655 140392160929792 gin_utils.py:85] dense.MlpBlock.input_axis_name = 'mlp_embed'
I0512 01:44:51.328686 140392160929792 gin_utils.py:85] dense.MlpBlock.intermediate_dim = %MLP_DIM
I0512 01:44:51.328717 140392160929792 gin_utils.py:85] dense.MlpBlock.intermediate_dropout_rate = %DROPOUT_RATE
I0512 01:44:51.377039 140577131780096 gin_utils.py:85] import seqio
I0512 01:44:51.377073 140577131780096 gin_utils.py:85] import t5.data.mixtures
I0512 01:44:51.377105 140577131780096 gin_utils.py:85] from t5x import adafactor
I0512 01:44:51.377138 140577131780096 gin_utils.py:85] from t5x.contrib.moe import adafactor_utils
I0512 01:44:51.377170 140577131780096 gin_utils.py:85] from t5x.contrib.moe import models
I0512 01:44:51.377203 140577131780096 gin_utils.py:85] from t5x.contrib.moe import partitioning
I0512 01:44:51.377235 140577131780096 gin_utils.py:85] from t5x.contrib.moe import trainer as moe_trainer
I0512 01:44:51.377268 140577131780096 gin_utils.py:85] from t5x import gin_utils
I0512 01:44:51.377300 140577131780096 gin_utils.py:85] from t5x import partitioning as partitioning2
I0512 01:44:51.377332 140577131780096 gin_utils.py:85] from t5x import trainer
I0512 01:44:51.377364 140577131780096 gin_utils.py:85] from t5x import utils
I0512 01:44:51.377396 140577131780096 gin_utils.py:85] 
I0512 01:44:51.377429 140577131780096 gin_utils.py:85] # Macros:
I0512 01:44:51.377461 140577131780096 gin_utils.py:85] # ==============================================================================
I0512 01:44:51.377494 140577131780096 gin_utils.py:85] ACTIVATION_DTYPE = 'bfloat16'
I0512 01:44:51.377527 140577131780096 gin_utils.py:85] ACTIVATION_PARTITIONING_DIMS = 1
I0512 01:44:51.377559 140577131780096 gin_utils.py:85] ARCHITECTURE = @t5_architecture.DecoderOnly()
I0512 01:44:51.377592 140577131780096 gin_utils.py:85] AUX_LOSS_FACTOR = 0.01
I0512 01:44:51.377624 140577131780096 gin_utils.py:85] BATCH_SIZE = 384
I0512 01:44:51.377657 140577131780096 gin_utils.py:85] BIAS_INIT = @bias_init/linen.initializers.normal()
I0512 01:44:51.377689 140577131780096 gin_utils.py:85] DECODER_SPARSE_LAYOUT = %flaxformer.architectures.moe.moe_enums.LayerLayout.MIXED
I0512 01:44:51.377722 140577131780096 gin_utils.py:85] DROPOUT_FACTORY = @dropout_factory/linen.Dropout
I0512 01:44:51.377754 140577131780096 gin_utils.py:85] DROPOUT_RATE = 0.0
I0512 01:44:51.377792 140577131780096 gin_utils.py:85] EMBED_DIM = 2048
I0512 01:44:51.377826 140577131780096 gin_utils.py:85] EVAL_EXPERT_CAPACITY_FACTOR = 2.0
I0512 01:44:51.377858 140577131780096 gin_utils.py:85] EXPERT_DROPOUT_RATE = %DROPOUT_RATE
I0512 01:44:51.377891 140577131780096 gin_utils.py:85] EXPERT_MLP_DIM = %MLP_DIM
I0512 01:44:51.377923 140577131780096 gin_utils.py:85] GROUP_SIZE = 4096
I0512 01:44:51.377956 140577131780096 gin_utils.py:85] HEAD_DIM = 128
I0512 01:44:51.377988 140577131780096 gin_utils.py:85] JITTER_NOISE = 0.0
I0512 01:44:51.378021 140577131780096 gin_utils.py:85] LABEL_SMOOTHING = 0.0
I0512 01:44:51.378056 140577131780096 gin_utils.py:85] LOSS_NORMALIZING_FACTOR = 'NUM_REAL_TARGET_TOKENS'
I0512 01:44:51.378088 140577131780096 gin_utils.py:85] MIXTURE_OR_TASK_MODULE = None
I0512 01:44:51.378120 140577131780096 gin_utils.py:85] MIXTURE_OR_TASK_NAME = 'mix_full_lm_test'
I0512 01:44:51.378153 140577131780096 gin_utils.py:85] MLP_DIM = 8192
I0512 01:44:51.378185 140577131780096 gin_utils.py:85] MODEL = @models.MoeDecoderOnlyModel()
I0512 01:44:51.378217 140577131780096 gin_utils.py:85] MODEL_DIR = 'gs://rosinality-tpu-bucket/openmoe_8b/training'
I0512 01:44:51.378250 140577131780096 gin_utils.py:85] MODEL_PARALLEL_SUBMESH = None
I0512 01:44:51.378283 140577131780096 gin_utils.py:85] MOE_TRUNCATED_DTYPE = 'bfloat16'
I0512 01:44:51.378315 140577131780096 gin_utils.py:85] NUM_DECODER_LAYERS = 24
I0512 01:44:51.378347 140577131780096 gin_utils.py:85] NUM_DECODER_SPARSE_LAYERS = 4
I0512 01:44:51.378380 140577131780096 gin_utils.py:85] NUM_EMBEDDINGS = 256384
I0512 01:44:51.378412 140577131780096 gin_utils.py:85] NUM_EXPERT_PARTITIONS = 8
I0512 01:44:51.378445 140577131780096 gin_utils.py:85] NUM_EXPERTS = 8
I0512 01:44:51.378477 140577131780096 gin_utils.py:85] NUM_HEADS = 24
I0512 01:44:51.378509 140577131780096 gin_utils.py:85] NUM_MODEL_PARTITIONS = 4
I0512 01:44:51.378541 140577131780096 gin_utils.py:85] NUM_SELECTED_EXPERTS = 2
I0512 01:44:51.378574 140577131780096 gin_utils.py:85] OPTIMIZER = @adafactor.Adafactor()
I0512 01:44:51.378606 140577131780096 gin_utils.py:85] RANDOM_SEED = None
I0512 01:44:51.378638 140577131780096 gin_utils.py:85] ROUTER_Z_LOSS_FACTOR = 0.0001
I0512 01:44:51.378671 140577131780096 gin_utils.py:85] SCALE = 0.1
I0512 01:44:51.378703 140577131780096 gin_utils.py:85] SHUFFLE_TRAIN_EXAMPLES = True
I0512 01:44:51.378735 140577131780096 gin_utils.py:85] TASK_FEATURE_LENGTHS = {'targets': 2048}
I0512 01:44:51.378768 140577131780096 gin_utils.py:85] TRAIN_EXPERT_CAPACITY_FACTOR = 1.25
I0512 01:44:51.378807 140577131780096 gin_utils.py:85] TRAIN_STEPS = 500000
I0512 01:44:51.378840 140577131780096 gin_utils.py:85] USE_CACHED_TASKS = False
I0512 01:44:51.378872 140577131780096 gin_utils.py:85] USE_HARDWARE_RNG = False
I0512 01:44:51.378905 140577131780096 gin_utils.py:85] VOCABULARY = @seqio.SentencePieceVocabulary()
I0512 01:44:51.378937 140577131780096 gin_utils.py:85] Z_LOSS = 0.0001
I0512 01:44:51.378969 140577131780096 gin_utils.py:85] 
I0512 01:44:51.379002 140577131780096 gin_utils.py:85] # Parameters for adafactor.Adafactor:
I0512 01:44:51.379035 140577131780096 gin_utils.py:85] # ==============================================================================
I0512 01:44:51.379069 140577131780096 gin_utils.py:85] adafactor.Adafactor.decay_rate = 0.8
I0512 01:44:51.379102 140577131780096 gin_utils.py:85] adafactor.Adafactor.logical_factor_rules = @adafactor_utils.logical_factor_rules()
I0512 01:44:51.379134 140577131780096 gin_utils.py:85] adafactor.Adafactor.step_offset = 0
I0512 01:44:51.379167 140577131780096 gin_utils.py:85] 
I0512 01:44:51.434426 140316961150976 resource_reader.py:50] system_path_file_exists:t5x/contrib/moe/configs/runs/pretrain.gin
I0512 01:44:51.379076 140188559153152 resource_reader.py:37] gin-config opened resource file:/home/rosinality/t5x/t5x/contrib/moe/configs/runs/pretrain.gin
I0512 01:44:51.379600 140188559153152 resource_reader.py:50] system_path_file_exists:t5x/configs/runs/pretrain.gin
I0512 01:44:51.379910 140188559153152 resource_reader.py:37] gin-config opened resource file:/home/rosinality/t5x/t5x/configs/runs/pretrain.gin
I0512 01:44:51.393850 140188559153152 gin_utils.py:83] Gin Configuration:
I0512 01:44:51.409707 140188559153152 gin_utils.py:85] from __gin__ import dynamic_registration
I0512 01:44:51.409776 140188559153152 gin_utils.py:85] import __main__ as train_script
I0512 01:44:51.409818 140188559153152 gin_utils.py:85] from flax import linen
I0512 01:44:51.409853 140188559153152 gin_utils.py:85] import flaxformer
I0512 01:44:51.409887 140188559153152 gin_utils.py:85] from flaxformer.architectures.moe import moe_architecture
I0512 01:44:51.409921 140188559153152 gin_utils.py:85] from flaxformer.architectures.moe import moe_enums
I0512 01:44:51.409954 140188559153152 gin_utils.py:85] from flaxformer.architectures.moe import moe_layers
I0512 01:44:51.409988 140188559153152 gin_utils.py:85] from flaxformer.architectures.moe import routing
I0512 01:44:51.410021 140188559153152 gin_utils.py:85] from flaxformer.architectures.t5 import t5_architecture
I0512 01:44:51.410063 140188559153152 gin_utils.py:85] from flaxformer.components.attention import dense_attention
I0512 01:44:51.410097 140188559153152 gin_utils.py:85] from flaxformer.components.attention import memory_efficient_attention
I0512 01:44:51.410130 140188559153152 gin_utils.py:85] from flaxformer.components import dense
I0512 01:44:51.410164 140188559153152 gin_utils.py:85] from flaxformer.components import embedding
I0512 01:44:51.410197 140188559153152 gin_utils.py:85] from flaxformer.components import layer_norm
I0512 01:44:51.410230 140188559153152 gin_utils.py:85] from gin import config
I0512 01:44:51.410290 140188559153152 gin_utils.py:85] import seqio
I0512 01:44:51.410330 140188559153152 gin_utils.py:85] import t5.data.mixtures
I0512 01:44:51.410364 140188559153152 gin_utils.py:85] from t5x import adafactor
I0512 01:44:51.410397 140188559153152 gin_utils.py:85] from t5x.contrib.moe import adafactor_utils
I0512 01:44:51.410430 140188559153152 gin_utils.py:85] from t5x.contrib.moe import models
I0512 01:44:51.410463 140188559153152 gin_utils.py:85] from t5x.contrib.moe import partitioning as moe_partitioning
I0512 01:44:51.410496 140188559153152 gin_utils.py:85] from t5x.contrib.moe import trainer as moe_trainer
I0512 01:44:51.410529 140188559153152 gin_utils.py:85] from t5x import gin_utils
I0512 01:44:51.410562 140188559153152 gin_utils.py:85] from t5x import partitioning
I0512 01:44:51.410595 140188559153152 gin_utils.py:85] from t5x import trainer
I0512 01:44:51.410628 140188559153152 gin_utils.py:85] from t5x import utils
I0512 01:44:51.410661 140188559153152 gin_utils.py:85] 
I0512 01:44:51.410694 140188559153152 gin_utils.py:85] # Macros:
I0512 01:44:51.410727 140188559153152 gin_utils.py:85] # ==============================================================================
I0512 01:44:51.410760 140188559153152 gin_utils.py:85] ACTIVATION_DTYPE = 'bfloat16'
I0512 01:44:51.410794 140188559153152 gin_utils.py:85] ACTIVATION_PARTITIONING_DIMS = 1
I0512 01:44:51.410826 140188559153152 gin_utils.py:85] ARCHITECTURE = @t5_architecture.DecoderOnly()
I0512 01:44:51.410859 140188559153152 gin_utils.py:85] AUX_LOSS_FACTOR = 0.01
I0512 01:44:51.410892 140188559153152 gin_utils.py:85] BATCH_SIZE = 384
I0512 01:44:51.410925 140188559153152 gin_utils.py:85] BIAS_INIT = @bias_init/linen.initializers.normal()
I0512 01:44:51.410958 140188559153152 gin_utils.py:85] DECODER_SPARSE_LAYOUT = %flaxformer.architectures.moe.moe_enums.LayerLayout.MIXED
I0512 01:44:51.410991 140188559153152 gin_utils.py:85] DROPOUT_FACTORY = @dropout_factory/linen.Dropout
I0512 01:44:51.411032 140188559153152 gin_utils.py:85] DROPOUT_RATE = 0.0
I0512 01:44:51.411067 140188559153152 gin_utils.py:85] EMBED_DIM = 2048
I0512 01:44:51.411101 140188559153152 gin_utils.py:85] EVAL_EXPERT_CAPACITY_FACTOR = 2.0
I0512 01:44:51.411134 140188559153152 gin_utils.py:85] EXPERT_DROPOUT_RATE = %DROPOUT_RATE
I0512 01:44:51.411166 140188559153152 gin_utils.py:85] EXPERT_MLP_DIM = %MLP_DIM
I0512 01:44:51.411200 140188559153152 gin_utils.py:85] GROUP_SIZE = 4096
I0512 01:44:51.411232 140188559153152 gin_utils.py:85] HEAD_DIM = 128
I0512 01:44:51.411265 140188559153152 gin_utils.py:85] JITTER_NOISE = 0.0
I0512 01:44:51.411300 140188559153152 gin_utils.py:85] LABEL_SMOOTHING = 0.0
I0512 01:44:51.411334 140188559153152 gin_utils.py:85] LOSS_NORMALIZING_FACTOR = 'NUM_REAL_TARGET_TOKENS'
I0512 01:44:51.411367 140188559153152 gin_utils.py:85] MIXTURE_OR_TASK_MODULE = None
I0512 01:44:51.411399 140188559153152 gin_utils.py:85] MIXTURE_OR_TASK_NAME = 'mix_full_lm_test'
I0512 01:44:51.411432 140188559153152 gin_utils.py:85] MLP_DIM = 8192
I0512 01:44:51.411465 140188559153152 gin_utils.py:85] MODEL = @models.MoeDecoderOnlyModel()
I0512 01:44:51.411498 140188559153152 gin_utils.py:85] MODEL_DIR = 'gs://rosinality-tpu-bucket/openmoe_8b/training'
I0512 01:44:51.411531 140188559153152 gin_utils.py:85] MODEL_PARALLEL_SUBMESH = None
I0512 01:44:51.411564 140188559153152 gin_utils.py:85] MOE_TRUNCATED_DTYPE = 'bfloat16'
I0512 01:44:51.411597 140188559153152 gin_utils.py:85] NUM_DECODER_LAYERS = 24
I0512 01:44:51.411629 140188559153152 gin_utils.py:85] NUM_DECODER_SPARSE_LAYERS = 4
I0512 01:44:51.411662 140188559153152 gin_utils.py:85] NUM_EMBEDDINGS = 256384
I0512 01:44:51.411695 140188559153152 gin_utils.py:85] NUM_EXPERT_PARTITIONS = 8
I0512 01:44:51.411727 140188559153152 gin_utils.py:85] NUM_EXPERTS = 8
I0512 01:44:51.384872 140172769531904 resource_reader.py:37] gin-config opened resource file:/home/rosinality/t5x/t5x/contrib/moe/configs/runs/pretrain.gin
I0512 01:44:51.385406 140172769531904 resource_reader.py:50] system_path_file_exists:t5x/configs/runs/pretrain.gin
I0512 01:44:51.385715 140172769531904 resource_reader.py:37] gin-config opened resource file:/home/rosinality/t5x/t5x/configs/runs/pretrain.gin
I0512 01:44:51.399729 140172769531904 gin_utils.py:83] Gin Configuration:
I0512 01:44:51.415450 140172769531904 gin_utils.py:85] from __gin__ import dynamic_registration
I0512 01:44:51.415518 140172769531904 gin_utils.py:85] import __main__ as train_script
I0512 01:44:51.415559 140172769531904 gin_utils.py:85] from flax import linen
I0512 01:44:51.415591 140172769531904 gin_utils.py:85] import flaxformer
I0512 01:44:51.415623 140172769531904 gin_utils.py:85] from flaxformer.architectures.moe import moe_architecture
I0512 01:44:51.415654 140172769531904 gin_utils.py:85] from flaxformer.architectures.moe import moe_enums
I0512 01:44:51.415685 140172769531904 gin_utils.py:85] from flaxformer.architectures.moe import moe_layers
I0512 01:44:51.415716 140172769531904 gin_utils.py:85] from flaxformer.architectures.moe import routing
I0512 01:44:51.415747 140172769531904 gin_utils.py:85] from flaxformer.architectures.t5 import t5_architecture
I0512 01:44:51.415778 140172769531904 gin_utils.py:85] from flaxformer.components.attention import dense_attention
I0512 01:44:51.415808 140172769531904 gin_utils.py:85] from flaxformer.components.attention import memory_efficient_attention
I0512 01:44:51.415839 140172769531904 gin_utils.py:85] from flaxformer.components import dense
I0512 01:44:51.415870 140172769531904 gin_utils.py:85] from flaxformer.components import embedding
I0512 01:44:51.415901 140172769531904 gin_utils.py:85] from flaxformer.components import layer_norm
I0512 01:44:51.415931 140172769531904 gin_utils.py:85] from gin import config
I0512 01:44:51.415962 140172769531904 gin_utils.py:85] import seqio
I0512 01:44:51.415993 140172769531904 gin_utils.py:85] import t5.data.mixtures
I0512 01:44:51.416023 140172769531904 gin_utils.py:85] from t5x import adafactor
I0512 01:44:51.416062 140172769531904 gin_utils.py:85] from t5x.contrib.moe import adafactor_utils
I0512 01:44:51.416094 140172769531904 gin_utils.py:85] from t5x.contrib.moe import models
I0512 01:44:51.416126 140172769531904 gin_utils.py:85] from t5x.contrib.moe import partitioning as moe_partitioning
I0512 01:44:51.416157 140172769531904 gin_utils.py:85] from t5x.contrib.moe import trainer as moe_trainer
I0512 01:44:51.416188 140172769531904 gin_utils.py:85] from t5x import gin_utils
I0512 01:44:51.416219 140172769531904 gin_utils.py:85] from t5x import partitioning
I0512 01:44:51.416249 140172769531904 gin_utils.py:85] from t5x import trainer
I0512 01:44:51.416280 140172769531904 gin_utils.py:85] from t5x import utils
I0512 01:44:51.416314 140172769531904 gin_utils.py:85] 
I0512 01:44:51.416347 140172769531904 gin_utils.py:85] # Macros:
I0512 01:44:51.416378 140172769531904 gin_utils.py:85] # ==============================================================================
I0512 01:44:51.416428 140172769531904 gin_utils.py:85] ACTIVATION_DTYPE = 'bfloat16'
I0512 01:44:51.416465 140172769531904 gin_utils.py:85] ACTIVATION_PARTITIONING_DIMS = 1
I0512 01:44:51.416497 140172769531904 gin_utils.py:85] ARCHITECTURE = @t5_architecture.DecoderOnly()
I0512 01:44:51.416528 140172769531904 gin_utils.py:85] AUX_LOSS_FACTOR = 0.01
I0512 01:44:51.416559 140172769531904 gin_utils.py:85] BATCH_SIZE = 384
I0512 01:44:51.416590 140172769531904 gin_utils.py:85] BIAS_INIT = @bias_init/linen.initializers.normal()
I0512 01:44:51.416620 140172769531904 gin_utils.py:85] DECODER_SPARSE_LAYOUT = %flaxformer.architectures.moe.moe_enums.LayerLayout.MIXED
I0512 01:44:51.416651 140172769531904 gin_utils.py:85] DROPOUT_FACTORY = @dropout_factory/linen.Dropout
I0512 01:44:51.416682 140172769531904 gin_utils.py:85] DROPOUT_RATE = 0.0
I0512 01:44:51.416713 140172769531904 gin_utils.py:85] EMBED_DIM = 2048
I0512 01:44:51.416744 140172769531904 gin_utils.py:85] EVAL_EXPERT_CAPACITY_FACTOR = 2.0
I0512 01:44:51.416775 140172769531904 gin_utils.py:85] EXPERT_DROPOUT_RATE = %DROPOUT_RATE
I0512 01:44:51.416805 140172769531904 gin_utils.py:85] EXPERT_MLP_DIM = %MLP_DIM
I0512 01:44:51.416836 140172769531904 gin_utils.py:85] GROUP_SIZE = 4096
I0512 01:44:51.416867 140172769531904 gin_utils.py:85] HEAD_DIM = 128
I0512 01:44:51.416898 140172769531904 gin_utils.py:85] JITTER_NOISE = 0.0
I0512 01:44:51.416933 140172769531904 gin_utils.py:85] LABEL_SMOOTHING = 0.0
I0512 01:44:51.416963 140172769531904 gin_utils.py:85] LOSS_NORMALIZING_FACTOR = 'NUM_REAL_TARGET_TOKENS'
I0512 01:44:51.416994 140172769531904 gin_utils.py:85] MIXTURE_OR_TASK_MODULE = None
I0512 01:44:51.417025 140172769531904 gin_utils.py:85] MIXTURE_OR_TASK_NAME = 'mix_full_lm_test'
I0512 01:44:51.417061 140172769531904 gin_utils.py:85] MLP_DIM = 8192
I0512 01:44:51.417093 140172769531904 gin_utils.py:85] MODEL = @models.MoeDecoderOnlyModel()
I0512 01:44:51.417124 140172769531904 gin_utils.py:85] MODEL_DIR = 'gs://rosinality-tpu-bucket/openmoe_8b/training'
I0512 01:44:51.417154 140172769531904 gin_utils.py:85] MODEL_PARALLEL_SUBMESH = None
I0512 01:44:51.417185 140172769531904 gin_utils.py:85] MOE_TRUNCATED_DTYPE = 'bfloat16'
I0512 01:44:51.417216 140172769531904 gin_utils.py:85] NUM_DECODER_LAYERS = 24
I0512 01:44:51.417246 140172769531904 gin_utils.py:85] NUM_DECODER_SPARSE_LAYERS = 4
I0512 01:44:51.417277 140172769531904 gin_utils.py:85] NUM_EMBEDDINGS = 256384
I0512 01:44:51.417309 140172769531904 gin_utils.py:85] NUM_EXPERT_PARTITIONS = 8
I0512 01:44:51.417341 140172769531904 gin_utils.py:85] NUM_EXPERTS = 8
I0512 01:44:51.411760 140188559153152 gin_utils.py:85] NUM_HEADS = 24
I0512 01:44:51.411793 140188559153152 gin_utils.py:85] NUM_MODEL_PARTITIONS = 4
I0512 01:44:51.411825 140188559153152 gin_utils.py:85] NUM_SELECTED_EXPERTS = 2
I0512 01:44:51.411858 140188559153152 gin_utils.py:85] OPTIMIZER = @adafactor.Adafactor()
I0512 01:44:51.411891 140188559153152 gin_utils.py:85] RANDOM_SEED = None
I0512 01:44:51.411923 140188559153152 gin_utils.py:85] ROUTER_Z_LOSS_FACTOR = 0.0001
I0512 01:44:51.411957 140188559153152 gin_utils.py:85] SCALE = 0.1
I0512 01:44:51.411989 140188559153152 gin_utils.py:85] SHUFFLE_TRAIN_EXAMPLES = True
I0512 01:44:51.412022 140188559153152 gin_utils.py:85] TASK_FEATURE_LENGTHS = {'targets': 2048}
I0512 01:44:51.412062 140188559153152 gin_utils.py:85] TRAIN_EXPERT_CAPACITY_FACTOR = 1.25
I0512 01:44:51.412095 140188559153152 gin_utils.py:85] TRAIN_STEPS = 500000
I0512 01:44:51.412127 140188559153152 gin_utils.py:85] USE_CACHED_TASKS = False
I0512 01:44:51.412160 140188559153152 gin_utils.py:85] USE_HARDWARE_RNG = False
I0512 01:44:51.412193 140188559153152 gin_utils.py:85] VOCABULARY = @seqio.SentencePieceVocabulary()
I0512 01:44:51.412225 140188559153152 gin_utils.py:85] Z_LOSS = 0.0001
I0512 01:44:51.412258 140188559153152 gin_utils.py:85] 
I0512 01:44:51.412293 140188559153152 gin_utils.py:85] # Parameters for adafactor.Adafactor:
I0512 01:44:51.412327 140188559153152 gin_utils.py:85] # ==============================================================================
I0512 01:44:51.412360 140188559153152 gin_utils.py:85] adafactor.Adafactor.decay_rate = 0.8
I0512 01:44:51.412393 140188559153152 gin_utils.py:85] adafactor.Adafactor.logical_factor_rules = @adafactor_utils.logical_factor_rules()
I0512 01:44:51.412426 140188559153152 gin_utils.py:85] adafactor.Adafactor.step_offset = 0
I0512 01:44:51.412458 140188559153152 gin_utils.py:85] 
I0512 01:44:51.417372 140172769531904 gin_utils.py:85] NUM_HEADS = 24
I0512 01:44:51.417403 140172769531904 gin_utils.py:85] NUM_MODEL_PARTITIONS = 4
I0512 01:44:51.417434 140172769531904 gin_utils.py:85] NUM_SELECTED_EXPERTS = 2
I0512 01:44:51.417464 140172769531904 gin_utils.py:85] OPTIMIZER = @adafactor.Adafactor()
I0512 01:44:51.417495 140172769531904 gin_utils.py:85] RANDOM_SEED = None
I0512 01:44:51.417526 140172769531904 gin_utils.py:85] ROUTER_Z_LOSS_FACTOR = 0.0001
I0512 01:44:51.417558 140172769531904 gin_utils.py:85] SCALE = 0.1
I0512 01:44:51.417596 140172769531904 gin_utils.py:85] SHUFFLE_TRAIN_EXAMPLES = True
I0512 01:44:51.417630 140172769531904 gin_utils.py:85] TASK_FEATURE_LENGTHS = {'targets': 2048}
I0512 01:44:51.417661 140172769531904 gin_utils.py:85] TRAIN_EXPERT_CAPACITY_FACTOR = 1.25
I0512 01:44:51.417691 140172769531904 gin_utils.py:85] TRAIN_STEPS = 500000
I0512 01:44:51.417723 140172769531904 gin_utils.py:85] USE_CACHED_TASKS = False
I0512 01:44:51.417755 140172769531904 gin_utils.py:85] USE_HARDWARE_RNG = False
I0512 01:44:51.417787 140172769531904 gin_utils.py:85] VOCABULARY = @seqio.SentencePieceVocabulary()
I0512 01:44:51.417819 140172769531904 gin_utils.py:85] Z_LOSS = 0.0001
I0512 01:44:51.417850 140172769531904 gin_utils.py:85] 
I0512 01:44:51.417881 140172769531904 gin_utils.py:85] # Parameters for adafactor.Adafactor:
I0512 01:44:51.417912 140172769531904 gin_utils.py:85] # ==============================================================================
I0512 01:44:51.417942 140172769531904 gin_utils.py:85] adafactor.Adafactor.decay_rate = 0.8
I0512 01:44:51.417973 140172769531904 gin_utils.py:85] adafactor.Adafactor.logical_factor_rules = @adafactor_utils.logical_factor_rules()
I0512 01:44:51.418004 140172769531904 gin_utils.py:85] adafactor.Adafactor.step_offset = 0
I0512 01:44:51.418034 140172769531904 gin_utils.py:85] 
I0512 01:44:51.360887 139997397456896 gin_utils.py:85] # Parameters for utils.CheckpointConfig:
I0512 01:44:51.360919 139997397456896 gin_utils.py:85] # ==============================================================================
I0512 01:44:51.360951 139997397456896 gin_utils.py:85] utils.CheckpointConfig.restore = @utils.RestoreCheckpointConfig()
I0512 01:44:51.361006 139997397456896 gin_utils.py:85] utils.CheckpointConfig.save = @utils.SaveCheckpointConfig()
I0512 01:44:51.361040 139997397456896 gin_utils.py:85] 
I0512 01:44:51.361072 139997397456896 gin_utils.py:85] # Parameters for partitioning.compute_num_model_partitions:
I0512 01:44:51.361104 139997397456896 gin_utils.py:85] # ==============================================================================
I0512 01:44:51.361136 139997397456896 gin_utils.py:85] partitioning.compute_num_model_partitions.model_parallel_submesh = \
I0512 01:44:51.361168 139997397456896 gin_utils.py:85]     %MODEL_PARALLEL_SUBMESH
I0512 01:44:51.361200 139997397456896 gin_utils.py:85] partitioning.compute_num_model_partitions.num_model_partitions = \
I0512 01:44:51.361232 139997397456896 gin_utils.py:85]     %NUM_MODEL_PARTITIONS
I0512 01:44:51.361264 139997397456896 gin_utils.py:85] 
I0512 01:44:51.361296 139997397456896 gin_utils.py:85] # Parameters for utils.create_learning_rate_scheduler:
I0512 01:44:51.361334 139997397456896 gin_utils.py:85] # ==============================================================================
I0512 01:44:51.361367 139997397456896 gin_utils.py:85] utils.create_learning_rate_scheduler.base_learning_rate = 1.0
I0512 01:44:51.361399 139997397456896 gin_utils.py:85] utils.create_learning_rate_scheduler.factors = 'constant * rsqrt_decay'
I0512 01:44:51.361431 139997397456896 gin_utils.py:85] utils.create_learning_rate_scheduler.warmup_steps = 10000
I0512 01:44:51.361463 139997397456896 gin_utils.py:85] 
I0512 01:44:51.361495 139997397456896 gin_utils.py:85] # Parameters for train/utils.DatasetConfig:
I0512 01:44:51.361531 139997397456896 gin_utils.py:85] # ==============================================================================
I0512 01:44:51.361565 139997397456896 gin_utils.py:85] train/utils.DatasetConfig.batch_size = %BATCH_SIZE
I0512 01:44:51.361599 139997397456896 gin_utils.py:85] train/utils.DatasetConfig.mixture_or_task_name = %MIXTURE_OR_TASK_NAME
I0512 01:44:51.361631 139997397456896 gin_utils.py:85] train/utils.DatasetConfig.module = %MIXTURE_OR_TASK_MODULE
I0512 01:44:51.361663 139997397456896 gin_utils.py:85] train/utils.DatasetConfig.pack = True
I0512 01:44:51.361695 139997397456896 gin_utils.py:85] train/utils.DatasetConfig.seed = 42
I0512 01:44:51.361726 139997397456896 gin_utils.py:85] train/utils.DatasetConfig.shuffle = %SHUFFLE_TRAIN_EXAMPLES
I0512 01:44:51.361759 139997397456896 gin_utils.py:85] train/utils.DatasetConfig.split = 'train'
I0512 01:44:51.361790 139997397456896 gin_utils.py:85] train/utils.DatasetConfig.task_feature_lengths = %TASK_FEATURE_LENGTHS
I0512 01:44:51.361823 139997397456896 gin_utils.py:85] train/utils.DatasetConfig.use_cached = %USE_CACHED_TASKS
I0512 01:44:51.361854 139997397456896 gin_utils.py:85] 
I0512 01:44:51.361886 139997397456896 gin_utils.py:85] # Parameters for train_eval/utils.DatasetConfig:
I0512 01:44:51.361918 139997397456896 gin_utils.py:85] # ==============================================================================
I0512 01:44:51.361950 139997397456896 gin_utils.py:85] train_eval/utils.DatasetConfig.batch_size = 128
I0512 01:44:51.361981 139997397456896 gin_utils.py:85] train_eval/utils.DatasetConfig.mixture_or_task_name = %MIXTURE_OR_TASK_NAME
I0512 01:44:51.362013 139997397456896 gin_utils.py:85] train_eval/utils.DatasetConfig.module = %MIXTURE_OR_TASK_MODULE
I0512 01:44:51.362044 139997397456896 gin_utils.py:85] train_eval/utils.DatasetConfig.pack = True
I0512 01:44:51.362076 139997397456896 gin_utils.py:85] train_eval/utils.DatasetConfig.seed = 42
I0512 01:44:51.362108 139997397456896 gin_utils.py:85] train_eval/utils.DatasetConfig.shuffle = False
I0512 01:44:51.362140 139997397456896 gin_utils.py:85] train_eval/utils.DatasetConfig.split = 'validation'
I0512 01:44:51.362171 139997397456896 gin_utils.py:85] train_eval/utils.DatasetConfig.task_feature_lengths = %TASK_FEATURE_LENGTHS
I0512 01:44:51.362203 139997397456896 gin_utils.py:85] train_eval/utils.DatasetConfig.use_cached = %USE_CACHED_TASKS
I0512 01:44:51.362235 139997397456896 gin_utils.py:85] 
I0512 01:44:51.362266 139997397456896 gin_utils.py:85] # Parameters for t5_architecture.DecoderLayer:
I0512 01:44:51.362298 139997397456896 gin_utils.py:85] # ==============================================================================
I0512 01:44:51.362337 139997397456896 gin_utils.py:85] t5_architecture.DecoderLayer.activation_partitioning_dims = \
I0512 01:44:51.362370 139997397456896 gin_utils.py:85]     %ACTIVATION_PARTITIONING_DIMS
I0512 01:44:51.362402 139997397456896 gin_utils.py:85] t5_architecture.DecoderLayer.dropout_factory = %DROPOUT_FACTORY
I0512 01:44:51.362434 139997397456896 gin_utils.py:85] t5_architecture.DecoderLayer.encoder_decoder_attention = None
I0512 01:44:51.362466 139997397456896 gin_utils.py:85] t5_architecture.DecoderLayer.layer_norm_factory = @layer_norm.T5LayerNorm
I0512 01:44:51.362498 139997397456896 gin_utils.py:85] t5_architecture.DecoderLayer.mlp = @dense.MlpBlock()
I0512 01:44:51.362529 139997397456896 gin_utils.py:85] t5_architecture.DecoderLayer.scanned = False
I0512 01:44:51.362562 139997397456896 gin_utils.py:85] t5_architecture.DecoderLayer.self_attention = \
I0512 01:44:51.362597 139997397456896 gin_utils.py:85]     @dense_attention.MultiHeadDotProductAttention()
I0512 01:44:51.362629 139997397456896 gin_utils.py:85] 
I0512 01:44:51.362661 139997397456896 gin_utils.py:85] # Parameters for t5_architecture.DecoderOnly:
I0512 01:44:51.362693 139997397456896 gin_utils.py:85] # ==============================================================================
I0512 01:44:51.362725 139997397456896 gin_utils.py:85] t5_architecture.DecoderOnly.decoder_factory = @moe_architecture.SparseDecoder
I0512 01:44:51.362756 139997397456896 gin_utils.py:85] t5_architecture.DecoderOnly.dtype = %ACTIVATION_DTYPE
I0512 01:44:51.362788 139997397456896 gin_utils.py:85] t5_architecture.DecoderOnly.shared_token_embedder_factory = @embedding.Embed
I0512 01:44:51.362820 139997397456896 gin_utils.py:85] 
I0512 01:44:51.362851 139997397456896 gin_utils.py:85] # Parameters for output_logits/dense.DenseGeneral:
I0512 01:44:51.362883 139997397456896 gin_utils.py:85] # ==============================================================================
I0512 01:44:51.362915 139997397456896 gin_utils.py:85] output_logits/dense.DenseGeneral.bias_init = %BIAS_INIT
I0512 01:44:51.362947 139997397456896 gin_utils.py:85] output_logits/dense.DenseGeneral.dtype = 'float32'
I0512 01:44:51.362979 139997397456896 gin_utils.py:85] output_logits/dense.DenseGeneral.features = %NUM_EMBEDDINGS
I0512 01:44:51.363011 139997397456896 gin_utils.py:85] output_logits/dense.DenseGeneral.kernel_axis_names = ['embed', 'vocab']
I0512 01:44:51.363043 139997397456896 gin_utils.py:85] output_logits/dense.DenseGeneral.kernel_init = \
I0512 01:44:51.363075 139997397456896 gin_utils.py:85]     @output_logits_kernel_init/linen.initializers.variance_scaling()
I0512 01:44:51.363107 139997397456896 gin_utils.py:85] output_logits/dense.DenseGeneral.use_bias = False
I0512 01:44:51.363139 139997397456896 gin_utils.py:85] 
I0512 01:44:51.363171 139997397456896 gin_utils.py:85] # Parameters for dropout_factory/linen.Dropout:
I0512 01:44:51.363202 139997397456896 gin_utils.py:85] # ==============================================================================
I0512 01:44:51.363234 139997397456896 gin_utils.py:85] dropout_factory/linen.Dropout.broadcast_dims = (-2,)
I0512 01:44:51.363265 139997397456896 gin_utils.py:85] dropout_factory/linen.Dropout.rate = %DROPOUT_RATE
I0512 01:44:51.363297 139997397456896 gin_utils.py:85] 
I0512 01:44:51.363338 139997397456896 gin_utils.py:85] # Parameters for embedding.Embed:
I0512 01:44:51.363371 139997397456896 gin_utils.py:85] # ==============================================================================
I0512 01:44:51.363403 139997397456896 gin_utils.py:85] embedding.Embed.attend_dtype = 'float32'
I0512 01:44:51.363435 139997397456896 gin_utils.py:85] embedding.Embed.cast_input_dtype = 'int32'
I0512 01:44:51.363467 139997397456896 gin_utils.py:85] embedding.Embed.dtype = %ACTIVATION_DTYPE
I0512 01:44:51.363498 139997397456896 gin_utils.py:85] embedding.Embed.embedding_init = @token_embedder_init/linen.initializers.normal()
I0512 01:44:51.363530 139997397456896 gin_utils.py:85] embedding.Embed.features = %EMBED_DIM
I0512 01:44:51.363563 139997397456896 gin_utils.py:85] embedding.Embed.name = 'token_embedder'
I0512 01:44:51.363629 139997397456896 gin_utils.py:85] embedding.Embed.num_embeddings = %NUM_EMBEDDINGS
I0512 01:44:51.363663 139997397456896 gin_utils.py:85] embedding.Embed.one_hot = True
I0512 01:44:51.363695 139997397456896 gin_utils.py:85] 
I0512 01:44:51.363728 139997397456896 gin_utils.py:85] # Parameters for dense.MlpBlock:
I0512 01:44:51.363760 139997397456896 gin_utils.py:85] # ==============================================================================
I0512 01:44:51.363792 139997397456896 gin_utils.py:85] dense.MlpBlock.activations = ('swiglu', 'linear')
I0512 01:44:51.363825 139997397456896 gin_utils.py:85] dense.MlpBlock.bias_init = %BIAS_INIT
I0512 01:44:51.363856 139997397456896 gin_utils.py:85] dense.MlpBlock.dtype = %ACTIVATION_DTYPE
I0512 01:44:51.363888 139997397456896 gin_utils.py:85] dense.MlpBlock.final_dropout_rate = 0
I0512 01:44:51.363920 139997397456896 gin_utils.py:85] dense.MlpBlock.input_axis_name = 'mlp_embed'
I0512 01:44:51.363952 139997397456896 gin_utils.py:85] dense.MlpBlock.intermediate_dim = %MLP_DIM
I0512 01:44:51.363984 139997397456896 gin_utils.py:85] dense.MlpBlock.intermediate_dropout_rate = %DROPOUT_RATE
I0512 01:44:51.328755 140392160929792 gin_utils.py:85] dense.MlpBlock.kernel_init = @mlp_kernel_init/linen.initializers.variance_scaling()
I0512 01:44:51.328788 140392160929792 gin_utils.py:85] dense.MlpBlock.output_axis_name = 'mlp_embed'
I0512 01:44:51.328819 140392160929792 gin_utils.py:85] dense.MlpBlock.use_bias = False
I0512 01:44:51.328851 140392160929792 gin_utils.py:85] 
I0512 01:44:51.328882 140392160929792 gin_utils.py:85] # Parameters for expert/dense.MlpBlock:
I0512 01:44:51.328913 140392160929792 gin_utils.py:85] # ==============================================================================
I0512 01:44:51.328945 140392160929792 gin_utils.py:85] expert/dense.MlpBlock.activation_partitioning_dims = 1
I0512 01:44:51.328977 140392160929792 gin_utils.py:85] expert/dense.MlpBlock.activations = ('swiglu', 'linear')
I0512 01:44:51.329011 140392160929792 gin_utils.py:85] expert/dense.MlpBlock.bias_init = %BIAS_INIT
I0512 01:44:51.329043 140392160929792 gin_utils.py:85] expert/dense.MlpBlock.data_sharding_constraints = ('expert_replicas', 'mlp')
I0512 01:44:51.329075 140392160929792 gin_utils.py:85] expert/dense.MlpBlock.dtype = %MOE_TRUNCATED_DTYPE
I0512 01:44:51.329106 140392160929792 gin_utils.py:85] expert/dense.MlpBlock.final_dropout_rate = 0.0
I0512 01:44:51.329138 140392160929792 gin_utils.py:85] expert/dense.MlpBlock.input_axis_name = 'mlp_embed'
I0512 01:44:51.329169 140392160929792 gin_utils.py:85] expert/dense.MlpBlock.intermediate_axis_name = 'expert_mlp'
I0512 01:44:51.329201 140392160929792 gin_utils.py:85] expert/dense.MlpBlock.intermediate_dim = %EXPERT_MLP_DIM
I0512 01:44:51.329232 140392160929792 gin_utils.py:85] expert/dense.MlpBlock.intermediate_dropout_rate = %EXPERT_DROPOUT_RATE
I0512 01:44:51.329264 140392160929792 gin_utils.py:85] expert/dense.MlpBlock.kernel_init = \
I0512 01:44:51.329295 140392160929792 gin_utils.py:85]     @expert_kernel_init/linen.initializers.variance_scaling()
I0512 01:44:51.329327 140392160929792 gin_utils.py:85] expert/dense.MlpBlock.output_axis_name = 'mlp_embed'
I0512 01:44:51.434311 139812357228544 resource_reader.py:50] system_path_file_exists:flaxformer/flaxformer/t5x/configs/moe/models/st_moe_decoder_only_large.gin
E0512 01:44:51.434733 139812357228544 resource_reader.py:55] Path not found: flaxformer/flaxformer/t5x/configs/moe/models/st_moe_decoder_only_large.gin
I0512 01:44:51.434796 139812357228544 resource_reader.py:50] system_path_file_exists:./flaxformer/flaxformer/t5x/configs/moe/models/st_moe_decoder_only_large.gin
E0512 01:44:51.434845 139812357228544 resource_reader.py:55] Path not found: ./flaxformer/flaxformer/t5x/configs/moe/models/st_moe_decoder_only_large.gin
I0512 01:44:51.379199 140577131780096 gin_utils.py:85] # Parameters for utils.CheckpointConfig:
I0512 01:44:51.379231 140577131780096 gin_utils.py:85] # ==============================================================================
I0512 01:44:51.379264 140577131780096 gin_utils.py:85] utils.CheckpointConfig.restore = @utils.RestoreCheckpointConfig()
I0512 01:44:51.379319 140577131780096 gin_utils.py:85] utils.CheckpointConfig.save = @utils.SaveCheckpointConfig()
I0512 01:44:51.379358 140577131780096 gin_utils.py:85] 
I0512 01:44:51.379391 140577131780096 gin_utils.py:85] # Parameters for partitioning.compute_num_model_partitions:
I0512 01:44:51.379423 140577131780096 gin_utils.py:85] # ==============================================================================
I0512 01:44:51.379456 140577131780096 gin_utils.py:85] partitioning.compute_num_model_partitions.model_parallel_submesh = \
I0512 01:44:51.379488 140577131780096 gin_utils.py:85]     %MODEL_PARALLEL_SUBMESH
I0512 01:44:51.379521 140577131780096 gin_utils.py:85] partitioning.compute_num_model_partitions.num_model_partitions = \
I0512 01:44:51.379554 140577131780096 gin_utils.py:85]     %NUM_MODEL_PARTITIONS
I0512 01:44:51.379586 140577131780096 gin_utils.py:85] 
I0512 01:44:51.379618 140577131780096 gin_utils.py:85] # Parameters for utils.create_learning_rate_scheduler:
I0512 01:44:51.379651 140577131780096 gin_utils.py:85] # ==============================================================================
I0512 01:44:51.379683 140577131780096 gin_utils.py:85] utils.create_learning_rate_scheduler.base_learning_rate = 1.0
I0512 01:44:51.379716 140577131780096 gin_utils.py:85] utils.create_learning_rate_scheduler.factors = 'constant * rsqrt_decay'
I0512 01:44:51.379748 140577131780096 gin_utils.py:85] utils.create_learning_rate_scheduler.warmup_steps = 10000
I0512 01:44:51.379786 140577131780096 gin_utils.py:85] 
I0512 01:44:51.379820 140577131780096 gin_utils.py:85] # Parameters for train/utils.DatasetConfig:
I0512 01:44:51.379853 140577131780096 gin_utils.py:85] # ==============================================================================
I0512 01:44:51.379888 140577131780096 gin_utils.py:85] train/utils.DatasetConfig.batch_size = %BATCH_SIZE
I0512 01:44:51.379922 140577131780096 gin_utils.py:85] train/utils.DatasetConfig.mixture_or_task_name = %MIXTURE_OR_TASK_NAME
I0512 01:44:51.379955 140577131780096 gin_utils.py:85] train/utils.DatasetConfig.module = %MIXTURE_OR_TASK_MODULE
I0512 01:44:51.379987 140577131780096 gin_utils.py:85] train/utils.DatasetConfig.pack = True
I0512 01:44:51.380019 140577131780096 gin_utils.py:85] train/utils.DatasetConfig.seed = 42
I0512 01:44:51.380054 140577131780096 gin_utils.py:85] train/utils.DatasetConfig.shuffle = %SHUFFLE_TRAIN_EXAMPLES
I0512 01:44:51.380088 140577131780096 gin_utils.py:85] train/utils.DatasetConfig.split = 'train'
I0512 01:44:51.380120 140577131780096 gin_utils.py:85] train/utils.DatasetConfig.task_feature_lengths = %TASK_FEATURE_LENGTHS
I0512 01:44:51.380152 140577131780096 gin_utils.py:85] train/utils.DatasetConfig.use_cached = %USE_CACHED_TASKS
I0512 01:44:51.380185 140577131780096 gin_utils.py:85] 
I0512 01:44:51.380217 140577131780096 gin_utils.py:85] # Parameters for train_eval/utils.DatasetConfig:
I0512 01:44:51.380250 140577131780096 gin_utils.py:85] # ==============================================================================
I0512 01:44:51.380282 140577131780096 gin_utils.py:85] train_eval/utils.DatasetConfig.batch_size = 128
I0512 01:44:51.380314 140577131780096 gin_utils.py:85] train_eval/utils.DatasetConfig.mixture_or_task_name = %MIXTURE_OR_TASK_NAME
I0512 01:44:51.380347 140577131780096 gin_utils.py:85] train_eval/utils.DatasetConfig.module = %MIXTURE_OR_TASK_MODULE
I0512 01:44:51.380380 140577131780096 gin_utils.py:85] train_eval/utils.DatasetConfig.pack = True
I0512 01:44:51.380412 140577131780096 gin_utils.py:85] train_eval/utils.DatasetConfig.seed = 42
I0512 01:44:51.380445 140577131780096 gin_utils.py:85] train_eval/utils.DatasetConfig.shuffle = False
I0512 01:44:51.380477 140577131780096 gin_utils.py:85] train_eval/utils.DatasetConfig.split = 'validation'
I0512 01:44:51.380509 140577131780096 gin_utils.py:85] train_eval/utils.DatasetConfig.task_feature_lengths = %TASK_FEATURE_LENGTHS
I0512 01:44:51.380542 140577131780096 gin_utils.py:85] train_eval/utils.DatasetConfig.use_cached = %USE_CACHED_TASKS
I0512 01:44:51.380574 140577131780096 gin_utils.py:85] 
I0512 01:44:51.380606 140577131780096 gin_utils.py:85] # Parameters for t5_architecture.DecoderLayer:
I0512 01:44:51.380639 140577131780096 gin_utils.py:85] # ==============================================================================
I0512 01:44:51.380671 140577131780096 gin_utils.py:85] t5_architecture.DecoderLayer.activation_partitioning_dims = \
I0512 01:44:51.380704 140577131780096 gin_utils.py:85]     %ACTIVATION_PARTITIONING_DIMS
I0512 01:44:51.380736 140577131780096 gin_utils.py:85] t5_architecture.DecoderLayer.dropout_factory = %DROPOUT_FACTORY
I0512 01:44:51.380768 140577131780096 gin_utils.py:85] t5_architecture.DecoderLayer.encoder_decoder_attention = None
I0512 01:44:51.380807 140577131780096 gin_utils.py:85] t5_architecture.DecoderLayer.layer_norm_factory = @layer_norm.T5LayerNorm
I0512 01:44:51.380840 140577131780096 gin_utils.py:85] t5_architecture.DecoderLayer.mlp = @dense.MlpBlock()
I0512 01:44:51.380873 140577131780096 gin_utils.py:85] t5_architecture.DecoderLayer.scanned = False
I0512 01:44:51.380909 140577131780096 gin_utils.py:85] t5_architecture.DecoderLayer.self_attention = \
I0512 01:44:51.380942 140577131780096 gin_utils.py:85]     @dense_attention.MultiHeadDotProductAttention()
I0512 01:44:51.380975 140577131780096 gin_utils.py:85] 
I0512 01:44:51.381007 140577131780096 gin_utils.py:85] # Parameters for t5_architecture.DecoderOnly:
I0512 01:44:51.381042 140577131780096 gin_utils.py:85] # ==============================================================================
I0512 01:44:51.381075 140577131780096 gin_utils.py:85] t5_architecture.DecoderOnly.decoder_factory = @moe_architecture.SparseDecoder
I0512 01:44:51.381108 140577131780096 gin_utils.py:85] t5_architecture.DecoderOnly.dtype = %ACTIVATION_DTYPE
I0512 01:44:51.381141 140577131780096 gin_utils.py:85] t5_architecture.DecoderOnly.shared_token_embedder_factory = @embedding.Embed
I0512 01:44:51.381173 140577131780096 gin_utils.py:85] 
I0512 01:44:51.381206 140577131780096 gin_utils.py:85] # Parameters for output_logits/dense.DenseGeneral:
I0512 01:44:51.381238 140577131780096 gin_utils.py:85] # ==============================================================================
I0512 01:44:51.381270 140577131780096 gin_utils.py:85] output_logits/dense.DenseGeneral.bias_init = %BIAS_INIT
I0512 01:44:51.381303 140577131780096 gin_utils.py:85] output_logits/dense.DenseGeneral.dtype = 'float32'
I0512 01:44:51.381335 140577131780096 gin_utils.py:85] output_logits/dense.DenseGeneral.features = %NUM_EMBEDDINGS
I0512 01:44:51.381368 140577131780096 gin_utils.py:85] output_logits/dense.DenseGeneral.kernel_axis_names = ['embed', 'vocab']
I0512 01:44:51.381400 140577131780096 gin_utils.py:85] output_logits/dense.DenseGeneral.kernel_init = \
I0512 01:44:51.381432 140577131780096 gin_utils.py:85]     @output_logits_kernel_init/linen.initializers.variance_scaling()
I0512 01:44:51.381465 140577131780096 gin_utils.py:85] output_logits/dense.DenseGeneral.use_bias = False
I0512 01:44:51.381498 140577131780096 gin_utils.py:85] 
I0512 01:44:51.381530 140577131780096 gin_utils.py:85] # Parameters for dropout_factory/linen.Dropout:
I0512 01:44:51.381562 140577131780096 gin_utils.py:85] # ==============================================================================
I0512 01:44:51.381595 140577131780096 gin_utils.py:85] dropout_factory/linen.Dropout.broadcast_dims = (-2,)
I0512 01:44:51.381627 140577131780096 gin_utils.py:85] dropout_factory/linen.Dropout.rate = %DROPOUT_RATE
I0512 01:44:51.381659 140577131780096 gin_utils.py:85] 
I0512 01:44:51.381692 140577131780096 gin_utils.py:85] # Parameters for embedding.Embed:
I0512 01:44:51.381724 140577131780096 gin_utils.py:85] # ==============================================================================
I0512 01:44:51.381756 140577131780096 gin_utils.py:85] embedding.Embed.attend_dtype = 'float32'
I0512 01:44:51.381795 140577131780096 gin_utils.py:85] embedding.Embed.cast_input_dtype = 'int32'
I0512 01:44:51.381828 140577131780096 gin_utils.py:85] embedding.Embed.dtype = %ACTIVATION_DTYPE
I0512 01:44:51.381860 140577131780096 gin_utils.py:85] embedding.Embed.embedding_init = @token_embedder_init/linen.initializers.normal()
I0512 01:44:51.381893 140577131780096 gin_utils.py:85] embedding.Embed.features = %EMBED_DIM
I0512 01:44:51.381925 140577131780096 gin_utils.py:85] embedding.Embed.name = 'token_embedder'
I0512 01:44:51.381957 140577131780096 gin_utils.py:85] embedding.Embed.num_embeddings = %NUM_EMBEDDINGS
I0512 01:44:51.381990 140577131780096 gin_utils.py:85] embedding.Embed.one_hot = True
I0512 01:44:51.382022 140577131780096 gin_utils.py:85] 
I0512 01:44:51.382057 140577131780096 gin_utils.py:85] # Parameters for dense.MlpBlock:
I0512 01:44:51.382090 140577131780096 gin_utils.py:85] # ==============================================================================
I0512 01:44:51.382123 140577131780096 gin_utils.py:85] dense.MlpBlock.activations = ('swiglu', 'linear')
I0512 01:44:51.382156 140577131780096 gin_utils.py:85] dense.MlpBlock.bias_init = %BIAS_INIT
I0512 01:44:51.382209 140577131780096 gin_utils.py:85] dense.MlpBlock.dtype = %ACTIVATION_DTYPE
I0512 01:44:51.382242 140577131780096 gin_utils.py:85] dense.MlpBlock.final_dropout_rate = 0
I0512 01:44:51.382274 140577131780096 gin_utils.py:85] dense.MlpBlock.input_axis_name = 'mlp_embed'
I0512 01:44:51.382307 140577131780096 gin_utils.py:85] dense.MlpBlock.intermediate_dim = %MLP_DIM
I0512 01:44:51.382339 140577131780096 gin_utils.py:85] dense.MlpBlock.intermediate_dropout_rate = %DROPOUT_RATE
I0512 01:44:51.471844 139676058949632 resource_reader.py:50] system_path_file_exists:flaxformer/flaxformer/t5x/configs/moe/models/st_moe_decoder_only_large.gin
E0512 01:44:51.472251 139676058949632 resource_reader.py:55] Path not found: flaxformer/flaxformer/t5x/configs/moe/models/st_moe_decoder_only_large.gin
I0512 01:44:51.472312 139676058949632 resource_reader.py:50] system_path_file_exists:./flaxformer/flaxformer/t5x/configs/moe/models/st_moe_decoder_only_large.gin
E0512 01:44:51.472359 139676058949632 resource_reader.py:55] Path not found: ./flaxformer/flaxformer/t5x/configs/moe/models/st_moe_decoder_only_large.gin
I0512 01:44:51.364015 139997397456896 gin_utils.py:85] dense.MlpBlock.kernel_init = @mlp_kernel_init/linen.initializers.variance_scaling()
I0512 01:44:51.364047 139997397456896 gin_utils.py:85] dense.MlpBlock.output_axis_name = 'mlp_embed'
I0512 01:44:51.364079 139997397456896 gin_utils.py:85] dense.MlpBlock.use_bias = False
I0512 01:44:51.364111 139997397456896 gin_utils.py:85] 
I0512 01:44:51.364143 139997397456896 gin_utils.py:85] # Parameters for expert/dense.MlpBlock:
I0512 01:44:51.364174 139997397456896 gin_utils.py:85] # ==============================================================================
I0512 01:44:51.364206 139997397456896 gin_utils.py:85] expert/dense.MlpBlock.activation_partitioning_dims = 1
I0512 01:44:51.364238 139997397456896 gin_utils.py:85] expert/dense.MlpBlock.activations = ('swiglu', 'linear')
I0512 01:44:51.364269 139997397456896 gin_utils.py:85] expert/dense.MlpBlock.bias_init = %BIAS_INIT
I0512 01:44:51.364301 139997397456896 gin_utils.py:85] expert/dense.MlpBlock.data_sharding_constraints = ('expert_replicas', 'mlp')
I0512 01:44:51.364340 139997397456896 gin_utils.py:85] expert/dense.MlpBlock.dtype = %MOE_TRUNCATED_DTYPE
I0512 01:44:51.364372 139997397456896 gin_utils.py:85] expert/dense.MlpBlock.final_dropout_rate = 0.0
I0512 01:44:51.364405 139997397456896 gin_utils.py:85] expert/dense.MlpBlock.input_axis_name = 'mlp_embed'
I0512 01:44:51.364437 139997397456896 gin_utils.py:85] expert/dense.MlpBlock.intermediate_axis_name = 'expert_mlp'
I0512 01:44:51.364469 139997397456896 gin_utils.py:85] expert/dense.MlpBlock.intermediate_dim = %EXPERT_MLP_DIM
I0512 01:44:51.364500 139997397456896 gin_utils.py:85] expert/dense.MlpBlock.intermediate_dropout_rate = %EXPERT_DROPOUT_RATE
I0512 01:44:51.364537 139997397456896 gin_utils.py:85] expert/dense.MlpBlock.kernel_init = \
I0512 01:44:51.364570 139997397456896 gin_utils.py:85]     @expert_kernel_init/linen.initializers.variance_scaling()
I0512 01:44:51.364604 139997397456896 gin_utils.py:85] expert/dense.MlpBlock.output_axis_name = 'mlp_embed'
I0512 01:44:51.412491 140188559153152 gin_utils.py:85] # Parameters for utils.CheckpointConfig:
I0512 01:44:51.412524 140188559153152 gin_utils.py:85] # ==============================================================================
I0512 01:44:51.412557 140188559153152 gin_utils.py:85] utils.CheckpointConfig.restore = @utils.RestoreCheckpointConfig()
I0512 01:44:51.412590 140188559153152 gin_utils.py:85] utils.CheckpointConfig.save = @utils.SaveCheckpointConfig()
I0512 01:44:51.412623 140188559153152 gin_utils.py:85] 
I0512 01:44:51.412656 140188559153152 gin_utils.py:85] # Parameters for moe_partitioning.compute_num_model_partitions:
I0512 01:44:51.412689 140188559153152 gin_utils.py:85] # ==============================================================================
I0512 01:44:51.412722 140188559153152 gin_utils.py:85] moe_partitioning.compute_num_model_partitions.model_parallel_submesh = \
I0512 01:44:51.412755 140188559153152 gin_utils.py:85]     %MODEL_PARALLEL_SUBMESH
I0512 01:44:51.412787 140188559153152 gin_utils.py:85] moe_partitioning.compute_num_model_partitions.num_model_partitions = \
I0512 01:44:51.412820 140188559153152 gin_utils.py:85]     %NUM_MODEL_PARTITIONS
I0512 01:44:51.412853 140188559153152 gin_utils.py:85] 
I0512 01:44:51.412886 140188559153152 gin_utils.py:85] # Parameters for utils.create_learning_rate_scheduler:
I0512 01:44:51.412919 140188559153152 gin_utils.py:85] # ==============================================================================
I0512 01:44:51.412951 140188559153152 gin_utils.py:85] utils.create_learning_rate_scheduler.base_learning_rate = 1.0
I0512 01:44:51.412984 140188559153152 gin_utils.py:85] utils.create_learning_rate_scheduler.factors = 'constant * rsqrt_decay'
I0512 01:44:51.413017 140188559153152 gin_utils.py:85] utils.create_learning_rate_scheduler.warmup_steps = 10000
I0512 01:44:51.413056 140188559153152 gin_utils.py:85] 
I0512 01:44:51.413089 140188559153152 gin_utils.py:85] # Parameters for train/utils.DatasetConfig:
I0512 01:44:51.413121 140188559153152 gin_utils.py:85] # ==============================================================================
I0512 01:44:51.413154 140188559153152 gin_utils.py:85] train/utils.DatasetConfig.batch_size = %BATCH_SIZE
I0512 01:44:51.413187 140188559153152 gin_utils.py:85] train/utils.DatasetConfig.mixture_or_task_name = %MIXTURE_OR_TASK_NAME
I0512 01:44:51.413220 140188559153152 gin_utils.py:85] train/utils.DatasetConfig.module = %MIXTURE_OR_TASK_MODULE
I0512 01:44:51.413253 140188559153152 gin_utils.py:85] train/utils.DatasetConfig.pack = True
I0512 01:44:51.413287 140188559153152 gin_utils.py:85] train/utils.DatasetConfig.seed = 42
I0512 01:44:51.413321 140188559153152 gin_utils.py:85] train/utils.DatasetConfig.shuffle = %SHUFFLE_TRAIN_EXAMPLES
I0512 01:44:51.413354 140188559153152 gin_utils.py:85] train/utils.DatasetConfig.split = 'train'
I0512 01:44:51.413387 140188559153152 gin_utils.py:85] train/utils.DatasetConfig.task_feature_lengths = %TASK_FEATURE_LENGTHS
I0512 01:44:51.413420 140188559153152 gin_utils.py:85] train/utils.DatasetConfig.use_cached = %USE_CACHED_TASKS
I0512 01:44:51.413453 140188559153152 gin_utils.py:85] 
I0512 01:44:51.413485 140188559153152 gin_utils.py:85] # Parameters for train_eval/utils.DatasetConfig:
I0512 01:44:51.413518 140188559153152 gin_utils.py:85] # ==============================================================================
I0512 01:44:51.413551 140188559153152 gin_utils.py:85] train_eval/utils.DatasetConfig.batch_size = 128
I0512 01:44:51.413584 140188559153152 gin_utils.py:85] train_eval/utils.DatasetConfig.mixture_or_task_name = %MIXTURE_OR_TASK_NAME
I0512 01:44:51.413617 140188559153152 gin_utils.py:85] train_eval/utils.DatasetConfig.module = %MIXTURE_OR_TASK_MODULE
I0512 01:44:51.413650 140188559153152 gin_utils.py:85] train_eval/utils.DatasetConfig.pack = True
I0512 01:44:51.413683 140188559153152 gin_utils.py:85] train_eval/utils.DatasetConfig.seed = 42
I0512 01:44:51.413716 140188559153152 gin_utils.py:85] train_eval/utils.DatasetConfig.shuffle = False
I0512 01:44:51.413748 140188559153152 gin_utils.py:85] train_eval/utils.DatasetConfig.split = 'validation'
I0512 01:44:51.413781 140188559153152 gin_utils.py:85] train_eval/utils.DatasetConfig.task_feature_lengths = %TASK_FEATURE_LENGTHS
I0512 01:44:51.413814 140188559153152 gin_utils.py:85] train_eval/utils.DatasetConfig.use_cached = %USE_CACHED_TASKS
I0512 01:44:51.413846 140188559153152 gin_utils.py:85] 
I0512 01:44:51.413879 140188559153152 gin_utils.py:85] # Parameters for t5_architecture.DecoderLayer:
I0512 01:44:51.413911 140188559153152 gin_utils.py:85] # ==============================================================================
I0512 01:44:51.413944 140188559153152 gin_utils.py:85] t5_architecture.DecoderLayer.activation_partitioning_dims = \
I0512 01:44:51.413977 140188559153152 gin_utils.py:85]     %ACTIVATION_PARTITIONING_DIMS
I0512 01:44:51.414010 140188559153152 gin_utils.py:85] t5_architecture.DecoderLayer.dropout_factory = %DROPOUT_FACTORY
I0512 01:44:51.414049 140188559153152 gin_utils.py:85] t5_architecture.DecoderLayer.encoder_decoder_attention = None
I0512 01:44:51.414083 140188559153152 gin_utils.py:85] t5_architecture.DecoderLayer.layer_norm_factory = @layer_norm.T5LayerNorm
I0512 01:44:51.414116 140188559153152 gin_utils.py:85] t5_architecture.DecoderLayer.mlp = @dense.MlpBlock()
I0512 01:44:51.414148 140188559153152 gin_utils.py:85] t5_architecture.DecoderLayer.scanned = False
I0512 01:44:51.414181 140188559153152 gin_utils.py:85] t5_architecture.DecoderLayer.self_attention = \
I0512 01:44:51.414214 140188559153152 gin_utils.py:85]     @dense_attention.MultiHeadDotProductAttention()
I0512 01:44:51.414247 140188559153152 gin_utils.py:85] 
I0512 01:44:51.414303 140188559153152 gin_utils.py:85] # Parameters for t5_architecture.DecoderOnly:
I0512 01:44:51.414338 140188559153152 gin_utils.py:85] # ==============================================================================
I0512 01:44:51.414371 140188559153152 gin_utils.py:85] t5_architecture.DecoderOnly.decoder_factory = @moe_architecture.SparseDecoder
I0512 01:44:51.414404 140188559153152 gin_utils.py:85] t5_architecture.DecoderOnly.dtype = %ACTIVATION_DTYPE
I0512 01:44:51.414457 140188559153152 gin_utils.py:85] t5_architecture.DecoderOnly.shared_token_embedder_factory = @embedding.Embed
I0512 01:44:51.414491 140188559153152 gin_utils.py:85] 
I0512 01:44:51.414524 140188559153152 gin_utils.py:85] # Parameters for output_logits/dense.DenseGeneral:
I0512 01:44:51.414557 140188559153152 gin_utils.py:85] # ==============================================================================
I0512 01:44:51.414589 140188559153152 gin_utils.py:85] output_logits/dense.DenseGeneral.bias_init = %BIAS_INIT
I0512 01:44:51.414622 140188559153152 gin_utils.py:85] output_logits/dense.DenseGeneral.dtype = 'float32'
I0512 01:44:51.414655 140188559153152 gin_utils.py:85] output_logits/dense.DenseGeneral.features = %NUM_EMBEDDINGS
I0512 01:44:51.414688 140188559153152 gin_utils.py:85] output_logits/dense.DenseGeneral.kernel_axis_names = ['embed', 'vocab']
I0512 01:44:51.414721 140188559153152 gin_utils.py:85] output_logits/dense.DenseGeneral.kernel_init = \
I0512 01:44:51.414754 140188559153152 gin_utils.py:85]     @output_logits_kernel_init/linen.initializers.variance_scaling()
I0512 01:44:51.414787 140188559153152 gin_utils.py:85] output_logits/dense.DenseGeneral.use_bias = False
I0512 01:44:51.414819 140188559153152 gin_utils.py:85] 
I0512 01:44:51.414852 140188559153152 gin_utils.py:85] # Parameters for dropout_factory/linen.Dropout:
I0512 01:44:51.414885 140188559153152 gin_utils.py:85] # ==============================================================================
I0512 01:44:51.414918 140188559153152 gin_utils.py:85] dropout_factory/linen.Dropout.broadcast_dims = (-2,)
I0512 01:44:51.414951 140188559153152 gin_utils.py:85] dropout_factory/linen.Dropout.rate = %DROPOUT_RATE
I0512 01:44:51.414984 140188559153152 gin_utils.py:85] 
I0512 01:44:51.415016 140188559153152 gin_utils.py:85] # Parameters for embedding.Embed:
I0512 01:44:51.415055 140188559153152 gin_utils.py:85] # ==============================================================================
I0512 01:44:51.415089 140188559153152 gin_utils.py:85] embedding.Embed.attend_dtype = 'float32'
I0512 01:44:51.415122 140188559153152 gin_utils.py:85] embedding.Embed.cast_input_dtype = 'int32'
I0512 01:44:51.415154 140188559153152 gin_utils.py:85] embedding.Embed.dtype = %ACTIVATION_DTYPE
I0512 01:44:51.415187 140188559153152 gin_utils.py:85] embedding.Embed.embedding_init = @token_embedder_init/linen.initializers.normal()
I0512 01:44:51.415220 140188559153152 gin_utils.py:85] embedding.Embed.features = %EMBED_DIM
I0512 01:44:51.415252 140188559153152 gin_utils.py:85] embedding.Embed.name = 'token_embedder'
I0512 01:44:51.415287 140188559153152 gin_utils.py:85] embedding.Embed.num_embeddings = %NUM_EMBEDDINGS
I0512 01:44:51.415320 140188559153152 gin_utils.py:85] embedding.Embed.one_hot = True
I0512 01:44:51.415353 140188559153152 gin_utils.py:85] 
I0512 01:44:51.415386 140188559153152 gin_utils.py:85] # Parameters for dense.MlpBlock:
I0512 01:44:51.415418 140188559153152 gin_utils.py:85] # ==============================================================================
I0512 01:44:51.415452 140188559153152 gin_utils.py:85] dense.MlpBlock.activations = ('swiglu', 'linear')
I0512 01:44:51.415485 140188559153152 gin_utils.py:85] dense.MlpBlock.bias_init = %BIAS_INIT
I0512 01:44:51.415518 140188559153152 gin_utils.py:85] dense.MlpBlock.dtype = %ACTIVATION_DTYPE
I0512 01:44:51.415550 140188559153152 gin_utils.py:85] dense.MlpBlock.final_dropout_rate = 0
I0512 01:44:51.415583 140188559153152 gin_utils.py:85] dense.MlpBlock.input_axis_name = 'mlp_embed'
I0512 01:44:51.415616 140188559153152 gin_utils.py:85] dense.MlpBlock.intermediate_dim = %MLP_DIM
I0512 01:44:51.415648 140188559153152 gin_utils.py:85] dense.MlpBlock.intermediate_dropout_rate = %DROPOUT_RATE
I0512 01:44:51.418072 140172769531904 gin_utils.py:85] # Parameters for utils.CheckpointConfig:
I0512 01:44:51.418103 140172769531904 gin_utils.py:85] # ==============================================================================
I0512 01:44:51.418134 140172769531904 gin_utils.py:85] utils.CheckpointConfig.restore = @utils.RestoreCheckpointConfig()
I0512 01:44:51.418165 140172769531904 gin_utils.py:85] utils.CheckpointConfig.save = @utils.SaveCheckpointConfig()
I0512 01:44:51.418196 140172769531904 gin_utils.py:85] 
I0512 01:44:51.418227 140172769531904 gin_utils.py:85] # Parameters for moe_partitioning.compute_num_model_partitions:
I0512 01:44:51.418258 140172769531904 gin_utils.py:85] # ==============================================================================
I0512 01:44:51.418288 140172769531904 gin_utils.py:85] moe_partitioning.compute_num_model_partitions.model_parallel_submesh = \
I0512 01:44:51.418321 140172769531904 gin_utils.py:85]     %MODEL_PARALLEL_SUBMESH
I0512 01:44:51.418353 140172769531904 gin_utils.py:85] moe_partitioning.compute_num_model_partitions.num_model_partitions = \
I0512 01:44:51.418384 140172769531904 gin_utils.py:85]     %NUM_MODEL_PARTITIONS
I0512 01:44:51.418415 140172769531904 gin_utils.py:85] 
I0512 01:44:51.418445 140172769531904 gin_utils.py:85] # Parameters for utils.create_learning_rate_scheduler:
I0512 01:44:51.418505 140172769531904 gin_utils.py:85] # ==============================================================================
I0512 01:44:51.418537 140172769531904 gin_utils.py:85] utils.create_learning_rate_scheduler.base_learning_rate = 1.0
I0512 01:44:51.418569 140172769531904 gin_utils.py:85] utils.create_learning_rate_scheduler.factors = 'constant * rsqrt_decay'
I0512 01:44:51.418600 140172769531904 gin_utils.py:85] utils.create_learning_rate_scheduler.warmup_steps = 10000
I0512 01:44:51.418631 140172769531904 gin_utils.py:85] 
I0512 01:44:51.418662 140172769531904 gin_utils.py:85] # Parameters for train/utils.DatasetConfig:
I0512 01:44:51.418693 140172769531904 gin_utils.py:85] # ==============================================================================
I0512 01:44:51.418723 140172769531904 gin_utils.py:85] train/utils.DatasetConfig.batch_size = %BATCH_SIZE
I0512 01:44:51.418754 140172769531904 gin_utils.py:85] train/utils.DatasetConfig.mixture_or_task_name = %MIXTURE_OR_TASK_NAME
I0512 01:44:51.418785 140172769531904 gin_utils.py:85] train/utils.DatasetConfig.module = %MIXTURE_OR_TASK_MODULE
I0512 01:44:51.418816 140172769531904 gin_utils.py:85] train/utils.DatasetConfig.pack = True
I0512 01:44:51.418846 140172769531904 gin_utils.py:85] train/utils.DatasetConfig.seed = 42
I0512 01:44:51.418876 140172769531904 gin_utils.py:85] train/utils.DatasetConfig.shuffle = %SHUFFLE_TRAIN_EXAMPLES
I0512 01:44:51.418907 140172769531904 gin_utils.py:85] train/utils.DatasetConfig.split = 'train'
I0512 01:44:51.418938 140172769531904 gin_utils.py:85] train/utils.DatasetConfig.task_feature_lengths = %TASK_FEATURE_LENGTHS
I0512 01:44:51.418969 140172769531904 gin_utils.py:85] train/utils.DatasetConfig.use_cached = %USE_CACHED_TASKS
I0512 01:44:51.419000 140172769531904 gin_utils.py:85] 
I0512 01:44:51.419031 140172769531904 gin_utils.py:85] # Parameters for train_eval/utils.DatasetConfig:
I0512 01:44:51.419067 140172769531904 gin_utils.py:85] # ==============================================================================
I0512 01:44:51.419099 140172769531904 gin_utils.py:85] train_eval/utils.DatasetConfig.batch_size = 128
I0512 01:44:51.419130 140172769531904 gin_utils.py:85] train_eval/utils.DatasetConfig.mixture_or_task_name = %MIXTURE_OR_TASK_NAME
I0512 01:44:51.419161 140172769531904 gin_utils.py:85] train_eval/utils.DatasetConfig.module = %MIXTURE_OR_TASK_MODULE
I0512 01:44:51.419192 140172769531904 gin_utils.py:85] train_eval/utils.DatasetConfig.pack = True
I0512 01:44:51.419222 140172769531904 gin_utils.py:85] train_eval/utils.DatasetConfig.seed = 42
I0512 01:44:51.419253 140172769531904 gin_utils.py:85] train_eval/utils.DatasetConfig.shuffle = False
I0512 01:44:51.419283 140172769531904 gin_utils.py:85] train_eval/utils.DatasetConfig.split = 'validation'
I0512 01:44:51.419316 140172769531904 gin_utils.py:85] train_eval/utils.DatasetConfig.task_feature_lengths = %TASK_FEATURE_LENGTHS
I0512 01:44:51.419347 140172769531904 gin_utils.py:85] train_eval/utils.DatasetConfig.use_cached = %USE_CACHED_TASKS
I0512 01:44:51.419379 140172769531904 gin_utils.py:85] 
I0512 01:44:51.419409 140172769531904 gin_utils.py:85] # Parameters for t5_architecture.DecoderLayer:
I0512 01:44:51.419440 140172769531904 gin_utils.py:85] # ==============================================================================
I0512 01:44:51.419471 140172769531904 gin_utils.py:85] t5_architecture.DecoderLayer.activation_partitioning_dims = \
I0512 01:44:51.419502 140172769531904 gin_utils.py:85]     %ACTIVATION_PARTITIONING_DIMS
I0512 01:44:51.419532 140172769531904 gin_utils.py:85] t5_architecture.DecoderLayer.dropout_factory = %DROPOUT_FACTORY
I0512 01:44:51.419563 140172769531904 gin_utils.py:85] t5_architecture.DecoderLayer.encoder_decoder_attention = None
I0512 01:44:51.419594 140172769531904 gin_utils.py:85] t5_architecture.DecoderLayer.layer_norm_factory = @layer_norm.T5LayerNorm
I0512 01:44:51.419625 140172769531904 gin_utils.py:85] t5_architecture.DecoderLayer.mlp = @dense.MlpBlock()
I0512 01:44:51.419655 140172769531904 gin_utils.py:85] t5_architecture.DecoderLayer.scanned = False
I0512 01:44:51.419686 140172769531904 gin_utils.py:85] t5_architecture.DecoderLayer.self_attention = \
I0512 01:44:51.419717 140172769531904 gin_utils.py:85]     @dense_attention.MultiHeadDotProductAttention()
I0512 01:44:51.419748 140172769531904 gin_utils.py:85] 
I0512 01:44:51.419779 140172769531904 gin_utils.py:85] # Parameters for t5_architecture.DecoderOnly:
I0512 01:44:51.419810 140172769531904 gin_utils.py:85] # ==============================================================================
I0512 01:44:51.419841 140172769531904 gin_utils.py:85] t5_architecture.DecoderOnly.decoder_factory = @moe_architecture.SparseDecoder
I0512 01:44:51.419871 140172769531904 gin_utils.py:85] t5_architecture.DecoderOnly.dtype = %ACTIVATION_DTYPE
I0512 01:44:51.419902 140172769531904 gin_utils.py:85] t5_architecture.DecoderOnly.shared_token_embedder_factory = @embedding.Embed
I0512 01:44:51.419933 140172769531904 gin_utils.py:85] 
I0512 01:44:51.419963 140172769531904 gin_utils.py:85] # Parameters for output_logits/dense.DenseGeneral:
I0512 01:44:51.419994 140172769531904 gin_utils.py:85] # ==============================================================================
I0512 01:44:51.420025 140172769531904 gin_utils.py:85] output_logits/dense.DenseGeneral.bias_init = %BIAS_INIT
I0512 01:44:51.420060 140172769531904 gin_utils.py:85] output_logits/dense.DenseGeneral.dtype = 'float32'
I0512 01:44:51.420092 140172769531904 gin_utils.py:85] output_logits/dense.DenseGeneral.features = %NUM_EMBEDDINGS
I0512 01:44:51.420124 140172769531904 gin_utils.py:85] output_logits/dense.DenseGeneral.kernel_axis_names = ['embed', 'vocab']
I0512 01:44:51.420155 140172769531904 gin_utils.py:85] output_logits/dense.DenseGeneral.kernel_init = \
I0512 01:44:51.420186 140172769531904 gin_utils.py:85]     @output_logits_kernel_init/linen.initializers.variance_scaling()
I0512 01:44:51.420217 140172769531904 gin_utils.py:85] output_logits/dense.DenseGeneral.use_bias = False
I0512 01:44:51.420248 140172769531904 gin_utils.py:85] 
I0512 01:44:51.420279 140172769531904 gin_utils.py:85] # Parameters for dropout_factory/linen.Dropout:
I0512 01:44:51.420311 140172769531904 gin_utils.py:85] # ==============================================================================
I0512 01:44:51.420343 140172769531904 gin_utils.py:85] dropout_factory/linen.Dropout.broadcast_dims = (-2,)
I0512 01:44:51.420374 140172769531904 gin_utils.py:85] dropout_factory/linen.Dropout.rate = %DROPOUT_RATE
I0512 01:44:51.420427 140172769531904 gin_utils.py:85] 
I0512 01:44:51.420466 140172769531904 gin_utils.py:85] # Parameters for embedding.Embed:
I0512 01:44:51.420498 140172769531904 gin_utils.py:85] # ==============================================================================
I0512 01:44:51.420529 140172769531904 gin_utils.py:85] embedding.Embed.attend_dtype = 'float32'
I0512 01:44:51.420560 140172769531904 gin_utils.py:85] embedding.Embed.cast_input_dtype = 'int32'
I0512 01:44:51.420591 140172769531904 gin_utils.py:85] embedding.Embed.dtype = %ACTIVATION_DTYPE
I0512 01:44:51.420622 140172769531904 gin_utils.py:85] embedding.Embed.embedding_init = @token_embedder_init/linen.initializers.normal()
I0512 01:44:51.420652 140172769531904 gin_utils.py:85] embedding.Embed.features = %EMBED_DIM
I0512 01:44:51.420683 140172769531904 gin_utils.py:85] embedding.Embed.name = 'token_embedder'
I0512 01:44:51.420713 140172769531904 gin_utils.py:85] embedding.Embed.num_embeddings = %NUM_EMBEDDINGS
I0512 01:44:51.420744 140172769531904 gin_utils.py:85] embedding.Embed.one_hot = True
I0512 01:44:51.420775 140172769531904 gin_utils.py:85] 
I0512 01:44:51.420806 140172769531904 gin_utils.py:85] # Parameters for dense.MlpBlock:
I0512 01:44:51.420836 140172769531904 gin_utils.py:85] # ==============================================================================
I0512 01:44:51.420868 140172769531904 gin_utils.py:85] dense.MlpBlock.activations = ('swiglu', 'linear')
I0512 01:44:51.420899 140172769531904 gin_utils.py:85] dense.MlpBlock.bias_init = %BIAS_INIT
I0512 01:44:51.420930 140172769531904 gin_utils.py:85] dense.MlpBlock.dtype = %ACTIVATION_DTYPE
I0512 01:44:51.420961 140172769531904 gin_utils.py:85] dense.MlpBlock.final_dropout_rate = 0
I0512 01:44:51.420991 140172769531904 gin_utils.py:85] dense.MlpBlock.input_axis_name = 'mlp_embed'
I0512 01:44:51.421022 140172769531904 gin_utils.py:85] dense.MlpBlock.intermediate_dim = %MLP_DIM
I0512 01:44:51.421059 140172769531904 gin_utils.py:85] dense.MlpBlock.intermediate_dropout_rate = %DROPOUT_RATE
I0512 01:44:51.434789 140316961150976 resource_reader.py:37] gin-config opened resource file:/home/rosinality/t5x/t5x/contrib/moe/configs/runs/pretrain.gin
I0512 01:44:51.435303 140316961150976 resource_reader.py:50] system_path_file_exists:t5x/configs/runs/pretrain.gin
I0512 01:44:51.435609 140316961150976 resource_reader.py:37] gin-config opened resource file:/home/rosinality/t5x/t5x/configs/runs/pretrain.gin
I0512 01:44:51.449518 140316961150976 gin_utils.py:83] Gin Configuration:
I0512 01:44:51.465303 140316961150976 gin_utils.py:85] from __gin__ import dynamic_registration
I0512 01:44:51.465373 140316961150976 gin_utils.py:85] import __main__ as train_script
I0512 01:44:51.465414 140316961150976 gin_utils.py:85] from flax import linen
I0512 01:44:51.465449 140316961150976 gin_utils.py:85] import flaxformer
I0512 01:44:51.465482 140316961150976 gin_utils.py:85] from flaxformer.architectures.moe import moe_architecture
I0512 01:44:51.465516 140316961150976 gin_utils.py:85] from flaxformer.architectures.moe import moe_enums
I0512 01:44:51.465569 140316961150976 gin_utils.py:85] from flaxformer.architectures.moe import moe_layers
I0512 01:44:51.465607 140316961150976 gin_utils.py:85] from flaxformer.architectures.moe import routing
I0512 01:44:51.465641 140316961150976 gin_utils.py:85] from flaxformer.architectures.t5 import t5_architecture
I0512 01:44:51.465673 140316961150976 gin_utils.py:85] from flaxformer.components.attention import dense_attention
I0512 01:44:51.465705 140316961150976 gin_utils.py:85] from flaxformer.components.attention import memory_efficient_attention
I0512 01:44:51.465737 140316961150976 gin_utils.py:85] from flaxformer.components import dense
I0512 01:44:51.465769 140316961150976 gin_utils.py:85] from flaxformer.components import embedding
I0512 01:44:51.465801 140316961150976 gin_utils.py:85] from flaxformer.components import layer_norm
I0512 01:44:51.465832 140316961150976 gin_utils.py:85] from gin import config
I0512 01:44:51.465871 140316961150976 gin_utils.py:85] import seqio
I0512 01:44:51.465904 140316961150976 gin_utils.py:85] import t5.data.mixtures
I0512 01:44:51.465936 140316961150976 gin_utils.py:85] from t5x import adafactor
I0512 01:44:51.465968 140316961150976 gin_utils.py:85] from t5x.contrib.moe import adafactor_utils
I0512 01:44:51.465999 140316961150976 gin_utils.py:85] from t5x.contrib.moe import models
I0512 01:44:51.466031 140316961150976 gin_utils.py:85] from t5x.contrib.moe import partitioning as moe_partitioning
I0512 01:44:51.466063 140316961150976 gin_utils.py:85] from t5x.contrib.moe import trainer as moe_trainer
I0512 01:44:51.466095 140316961150976 gin_utils.py:85] from t5x import gin_utils
I0512 01:44:51.466130 140316961150976 gin_utils.py:85] from t5x import partitioning
I0512 01:44:51.466162 140316961150976 gin_utils.py:85] from t5x import trainer
I0512 01:44:51.466194 140316961150976 gin_utils.py:85] from t5x import utils
I0512 01:44:51.466225 140316961150976 gin_utils.py:85] 
I0512 01:44:51.466258 140316961150976 gin_utils.py:85] # Macros:
I0512 01:44:51.466290 140316961150976 gin_utils.py:85] # ==============================================================================
I0512 01:44:51.466321 140316961150976 gin_utils.py:85] ACTIVATION_DTYPE = 'bfloat16'
I0512 01:44:51.466353 140316961150976 gin_utils.py:85] ACTIVATION_PARTITIONING_DIMS = 1
I0512 01:44:51.466385 140316961150976 gin_utils.py:85] ARCHITECTURE = @t5_architecture.DecoderOnly()
I0512 01:44:51.466416 140316961150976 gin_utils.py:85] AUX_LOSS_FACTOR = 0.01
I0512 01:44:51.466448 140316961150976 gin_utils.py:85] BATCH_SIZE = 384
I0512 01:44:51.466480 140316961150976 gin_utils.py:85] BIAS_INIT = @bias_init/linen.initializers.normal()
I0512 01:44:51.466511 140316961150976 gin_utils.py:85] DECODER_SPARSE_LAYOUT = %flaxformer.architectures.moe.moe_enums.LayerLayout.MIXED
I0512 01:44:51.466543 140316961150976 gin_utils.py:85] DROPOUT_FACTORY = @dropout_factory/linen.Dropout
I0512 01:44:51.466575 140316961150976 gin_utils.py:85] DROPOUT_RATE = 0.0
I0512 01:44:51.466607 140316961150976 gin_utils.py:85] EMBED_DIM = 2048
I0512 01:44:51.466639 140316961150976 gin_utils.py:85] EVAL_EXPERT_CAPACITY_FACTOR = 2.0
I0512 01:44:51.466670 140316961150976 gin_utils.py:85] EXPERT_DROPOUT_RATE = %DROPOUT_RATE
I0512 01:44:51.466702 140316961150976 gin_utils.py:85] EXPERT_MLP_DIM = %MLP_DIM
I0512 01:44:51.466734 140316961150976 gin_utils.py:85] GROUP_SIZE = 4096
I0512 01:44:51.466765 140316961150976 gin_utils.py:85] HEAD_DIM = 128
I0512 01:44:51.466797 140316961150976 gin_utils.py:85] JITTER_NOISE = 0.0
I0512 01:44:51.466828 140316961150976 gin_utils.py:85] LABEL_SMOOTHING = 0.0
I0512 01:44:51.466866 140316961150976 gin_utils.py:85] LOSS_NORMALIZING_FACTOR = 'NUM_REAL_TARGET_TOKENS'
I0512 01:44:51.466899 140316961150976 gin_utils.py:85] MIXTURE_OR_TASK_MODULE = None
I0512 01:44:51.466930 140316961150976 gin_utils.py:85] MIXTURE_OR_TASK_NAME = 'mix_full_lm_test'
I0512 01:44:51.466962 140316961150976 gin_utils.py:85] MLP_DIM = 8192
I0512 01:44:51.466994 140316961150976 gin_utils.py:85] MODEL = @models.MoeDecoderOnlyModel()
I0512 01:44:51.467025 140316961150976 gin_utils.py:85] MODEL_DIR = 'gs://rosinality-tpu-bucket/openmoe_8b/training'
I0512 01:44:51.467057 140316961150976 gin_utils.py:85] MODEL_PARALLEL_SUBMESH = None
I0512 01:44:51.467089 140316961150976 gin_utils.py:85] MOE_TRUNCATED_DTYPE = 'bfloat16'
I0512 01:44:51.467123 140316961150976 gin_utils.py:85] NUM_DECODER_LAYERS = 24
I0512 01:44:51.467155 140316961150976 gin_utils.py:85] NUM_DECODER_SPARSE_LAYERS = 4
I0512 01:44:51.467187 140316961150976 gin_utils.py:85] NUM_EMBEDDINGS = 256384
I0512 01:44:51.467219 140316961150976 gin_utils.py:85] NUM_EXPERT_PARTITIONS = 8
I0512 01:44:51.467251 140316961150976 gin_utils.py:85] NUM_EXPERTS = 8
I0512 01:44:51.467283 140316961150976 gin_utils.py:85] NUM_HEADS = 24
I0512 01:44:51.467314 140316961150976 gin_utils.py:85] NUM_MODEL_PARTITIONS = 4
I0512 01:44:51.467346 140316961150976 gin_utils.py:85] NUM_SELECTED_EXPERTS = 2
I0512 01:44:51.467377 140316961150976 gin_utils.py:85] OPTIMIZER = @adafactor.Adafactor()
I0512 01:44:51.467409 140316961150976 gin_utils.py:85] RANDOM_SEED = None
I0512 01:44:51.467441 140316961150976 gin_utils.py:85] ROUTER_Z_LOSS_FACTOR = 0.0001
I0512 01:44:51.467473 140316961150976 gin_utils.py:85] SCALE = 0.1
I0512 01:44:51.467504 140316961150976 gin_utils.py:85] SHUFFLE_TRAIN_EXAMPLES = True
I0512 01:44:51.467536 140316961150976 gin_utils.py:85] TASK_FEATURE_LENGTHS = {'targets': 2048}
I0512 01:44:51.467567 140316961150976 gin_utils.py:85] TRAIN_EXPERT_CAPACITY_FACTOR = 1.25
I0512 01:44:51.467599 140316961150976 gin_utils.py:85] TRAIN_STEPS = 500000
I0512 01:44:51.467631 140316961150976 gin_utils.py:85] USE_CACHED_TASKS = False
I0512 01:44:51.467662 140316961150976 gin_utils.py:85] USE_HARDWARE_RNG = False
I0512 01:44:51.467694 140316961150976 gin_utils.py:85] VOCABULARY = @seqio.SentencePieceVocabulary()
I0512 01:44:51.467725 140316961150976 gin_utils.py:85] Z_LOSS = 0.0001
I0512 01:44:51.467757 140316961150976 gin_utils.py:85] 
I0512 01:44:51.467789 140316961150976 gin_utils.py:85] # Parameters for adafactor.Adafactor:
I0512 01:44:51.467821 140316961150976 gin_utils.py:85] # ==============================================================================
I0512 01:44:51.467857 140316961150976 gin_utils.py:85] adafactor.Adafactor.decay_rate = 0.8
I0512 01:44:51.467890 140316961150976 gin_utils.py:85] adafactor.Adafactor.logical_factor_rules = @adafactor_utils.logical_factor_rules()
I0512 01:44:51.467923 140316961150976 gin_utils.py:85] adafactor.Adafactor.step_offset = 0
I0512 01:44:51.467954 140316961150976 gin_utils.py:85] 
I0512 01:44:51.490659 139812357228544 resource_reader.py:50] system_path_file_exists:flaxformer/flaxformer/t5x/configs/moe/models/tokens_choose_decoder_only_large.gin
E0512 01:44:51.490842 139812357228544 resource_reader.py:55] Path not found: flaxformer/flaxformer/t5x/configs/moe/models/tokens_choose_decoder_only_large.gin
I0512 01:44:51.490900 139812357228544 resource_reader.py:50] system_path_file_exists:./flaxformer/flaxformer/t5x/configs/moe/models/tokens_choose_decoder_only_large.gin
E0512 01:44:51.490944 139812357228544 resource_reader.py:55] Path not found: ./flaxformer/flaxformer/t5x/configs/moe/models/tokens_choose_decoder_only_large.gin
I0512 01:44:51.491809 139812357228544 resource_reader.py:50] system_path_file_exists:flaxformer/flaxformer/t5x/configs/moe/architectures/moe_decoder_only.gin
I0512 01:44:51.382372 140577131780096 gin_utils.py:85] dense.MlpBlock.kernel_init = @mlp_kernel_init/linen.initializers.variance_scaling()
I0512 01:44:51.382404 140577131780096 gin_utils.py:85] dense.MlpBlock.output_axis_name = 'mlp_embed'
I0512 01:44:51.382436 140577131780096 gin_utils.py:85] dense.MlpBlock.use_bias = False
I0512 01:44:51.382469 140577131780096 gin_utils.py:85] 
I0512 01:44:51.382501 140577131780096 gin_utils.py:85] # Parameters for expert/dense.MlpBlock:
I0512 01:44:51.382533 140577131780096 gin_utils.py:85] # ==============================================================================
I0512 01:44:51.382566 140577131780096 gin_utils.py:85] expert/dense.MlpBlock.activation_partitioning_dims = 1
I0512 01:44:51.382598 140577131780096 gin_utils.py:85] expert/dense.MlpBlock.activations = ('swiglu', 'linear')
I0512 01:44:51.382630 140577131780096 gin_utils.py:85] expert/dense.MlpBlock.bias_init = %BIAS_INIT
I0512 01:44:51.382663 140577131780096 gin_utils.py:85] expert/dense.MlpBlock.data_sharding_constraints = ('expert_replicas', 'mlp')
I0512 01:44:51.382695 140577131780096 gin_utils.py:85] expert/dense.MlpBlock.dtype = %MOE_TRUNCATED_DTYPE
I0512 01:44:51.382727 140577131780096 gin_utils.py:85] expert/dense.MlpBlock.final_dropout_rate = 0.0
I0512 01:44:51.382759 140577131780096 gin_utils.py:85] expert/dense.MlpBlock.input_axis_name = 'mlp_embed'
I0512 01:44:51.382798 140577131780096 gin_utils.py:85] expert/dense.MlpBlock.intermediate_axis_name = 'expert_mlp'
I0512 01:44:51.382831 140577131780096 gin_utils.py:85] expert/dense.MlpBlock.intermediate_dim = %EXPERT_MLP_DIM
I0512 01:44:51.382864 140577131780096 gin_utils.py:85] expert/dense.MlpBlock.intermediate_dropout_rate = %EXPERT_DROPOUT_RATE
I0512 01:44:51.382897 140577131780096 gin_utils.py:85] expert/dense.MlpBlock.kernel_init = \
I0512 01:44:51.382929 140577131780096 gin_utils.py:85]     @expert_kernel_init/linen.initializers.variance_scaling()
I0512 01:44:51.382962 140577131780096 gin_utils.py:85] expert/dense.MlpBlock.output_axis_name = 'mlp_embed'
E0512 01:44:51.491942 139812357228544 resource_reader.py:55] Path not found: flaxformer/flaxformer/t5x/configs/moe/architectures/moe_decoder_only.gin
I0512 01:44:51.491995 139812357228544 resource_reader.py:50] system_path_file_exists:./flaxformer/flaxformer/t5x/configs/moe/architectures/moe_decoder_only.gin
E0512 01:44:51.492035 139812357228544 resource_reader.py:55] Path not found: ./flaxformer/flaxformer/t5x/configs/moe/architectures/moe_decoder_only.gin
I0512 01:44:51.329359 140392160929792 gin_utils.py:85] expert/dense.MlpBlock.use_bias = False
I0512 01:44:51.329390 140392160929792 gin_utils.py:85] 
I0512 01:44:51.329421 140392160929792 gin_utils.py:85] # Parameters for models.MoeDecoderOnlyModel:
I0512 01:44:51.329453 140392160929792 gin_utils.py:85] # ==============================================================================
I0512 01:44:51.329484 140392160929792 gin_utils.py:85] models.MoeDecoderOnlyModel.aux_loss_factor = %AUX_LOSS_FACTOR
I0512 01:44:51.329515 140392160929792 gin_utils.py:85] models.MoeDecoderOnlyModel.label_smoothing = %LABEL_SMOOTHING
I0512 01:44:51.329547 140392160929792 gin_utils.py:85] models.MoeDecoderOnlyModel.loss_normalizing_factor = %LOSS_NORMALIZING_FACTOR
I0512 01:44:51.329578 140392160929792 gin_utils.py:85] models.MoeDecoderOnlyModel.module = %ARCHITECTURE
I0512 01:44:51.329609 140392160929792 gin_utils.py:85] models.MoeDecoderOnlyModel.optimizer_def = %OPTIMIZER
I0512 01:44:51.329641 140392160929792 gin_utils.py:85] models.MoeDecoderOnlyModel.router_z_loss_factor = %ROUTER_Z_LOSS_FACTOR
I0512 01:44:51.329673 140392160929792 gin_utils.py:85] models.MoeDecoderOnlyModel.vocabulary = %VOCABULARY
I0512 01:44:51.329704 140392160929792 gin_utils.py:85] models.MoeDecoderOnlyModel.z_loss = %Z_LOSS
I0512 01:44:51.329742 140392160929792 gin_utils.py:85] 
I0512 01:44:51.329775 140392160929792 gin_utils.py:85] # Parameters for moe_layers.MoeLayer:
I0512 01:44:51.329807 140392160929792 gin_utils.py:85] # ==============================================================================
I0512 01:44:51.329839 140392160929792 gin_utils.py:85] moe_layers.MoeLayer.dtype = %MOE_TRUNCATED_DTYPE
I0512 01:44:51.329871 140392160929792 gin_utils.py:85] moe_layers.MoeLayer.eval_capacity_factor = %EVAL_EXPERT_CAPACITY_FACTOR
I0512 01:44:51.329903 140392160929792 gin_utils.py:85] moe_layers.MoeLayer.expert = @expert/dense.MlpBlock()
I0512 01:44:51.329934 140392160929792 gin_utils.py:85] moe_layers.MoeLayer.max_group_size = %GROUP_SIZE
I0512 01:44:51.329966 140392160929792 gin_utils.py:85] moe_layers.MoeLayer.num_expert_partitions = %NUM_EXPERT_PARTITIONS
I0512 01:44:51.329999 140392160929792 gin_utils.py:85] moe_layers.MoeLayer.num_experts = %NUM_EXPERTS
I0512 01:44:51.330032 140392160929792 gin_utils.py:85] moe_layers.MoeLayer.num_model_partitions = \
I0512 01:44:51.330064 140392160929792 gin_utils.py:85]     @partitioning.compute_num_model_partitions()
I0512 01:44:51.330116 140392160929792 gin_utils.py:85] moe_layers.MoeLayer.train_capacity_factor = %TRAIN_EXPERT_CAPACITY_FACTOR
I0512 01:44:51.330152 140392160929792 gin_utils.py:85] 
I0512 01:44:51.330183 140392160929792 gin_utils.py:85] # Parameters for sparse_decoder/moe_layers.MoeLayer:
I0512 01:44:51.330215 140392160929792 gin_utils.py:85] # ==============================================================================
I0512 01:44:51.330248 140392160929792 gin_utils.py:85] sparse_decoder/moe_layers.MoeLayer.router = \
I0512 01:44:51.330279 140392160929792 gin_utils.py:85]     @sparse_decoder/routing.TokensChooseMaskedRouter()
I0512 01:44:51.330311 140392160929792 gin_utils.py:85] 
I0512 01:44:51.330343 140392160929792 gin_utils.py:85] # Parameters for partitioning.MoePjitPartitioner:
I0512 01:44:51.330374 140392160929792 gin_utils.py:85] # ==============================================================================
I0512 01:44:51.330406 140392160929792 gin_utils.py:85] partitioning.MoePjitPartitioner.model_parallel_submesh = %MODEL_PARALLEL_SUBMESH
I0512 01:44:51.330438 140392160929792 gin_utils.py:85] partitioning.MoePjitPartitioner.num_expert_partitions = %NUM_EXPERT_PARTITIONS
I0512 01:44:51.330469 140392160929792 gin_utils.py:85] partitioning.MoePjitPartitioner.num_partitions = %NUM_MODEL_PARTITIONS
I0512 01:44:51.330501 140392160929792 gin_utils.py:85] 
I0512 01:44:51.330533 140392160929792 gin_utils.py:85] # Parameters for moe_trainer.MoeTrainer:
I0512 01:44:51.330565 140392160929792 gin_utils.py:85] # ==============================================================================
I0512 01:44:51.330597 140392160929792 gin_utils.py:85] moe_trainer.MoeTrainer.learning_rate_fn = @utils.create_learning_rate_scheduler()
I0512 01:44:51.330628 140392160929792 gin_utils.py:85] moe_trainer.MoeTrainer.num_expert_partitions = %NUM_EXPERT_PARTITIONS
I0512 01:44:51.330660 140392160929792 gin_utils.py:85] moe_trainer.MoeTrainer.num_microbatches = 8
I0512 01:44:51.330691 140392160929792 gin_utils.py:85] 
I0512 01:44:51.330723 140392160929792 gin_utils.py:85] # Parameters for dense_attention.MultiHeadDotProductAttention:
I0512 01:44:51.330763 140392160929792 gin_utils.py:85] # ==============================================================================
I0512 01:44:51.330795 140392160929792 gin_utils.py:85] dense_attention.MultiHeadDotProductAttention.bias_init = %BIAS_INIT
I0512 01:44:51.330827 140392160929792 gin_utils.py:85] dense_attention.MultiHeadDotProductAttention.broadcast_dropout = True
I0512 01:44:51.330859 140392160929792 gin_utils.py:85] dense_attention.MultiHeadDotProductAttention.dropout_rate = %DROPOUT_RATE
I0512 01:44:51.330890 140392160929792 gin_utils.py:85] dense_attention.MultiHeadDotProductAttention.dtype = %ACTIVATION_DTYPE
I0512 01:44:51.330922 140392160929792 gin_utils.py:85] dense_attention.MultiHeadDotProductAttention.head_dim = %HEAD_DIM
I0512 01:44:51.330954 140392160929792 gin_utils.py:85] dense_attention.MultiHeadDotProductAttention.kernel_init = \
I0512 01:44:51.330986 140392160929792 gin_utils.py:85]     @attention_kernel_init/linen.initializers.variance_scaling()
I0512 01:44:51.331020 140392160929792 gin_utils.py:85] dense_attention.MultiHeadDotProductAttention.num_heads = %NUM_HEADS
I0512 01:44:51.331053 140392160929792 gin_utils.py:85] dense_attention.MultiHeadDotProductAttention.use_bias = False
I0512 01:44:51.331084 140392160929792 gin_utils.py:85] dense_attention.MultiHeadDotProductAttention.use_rotary_embedding = True
I0512 01:44:51.331117 140392160929792 gin_utils.py:85] 
I0512 01:44:51.331148 140392160929792 gin_utils.py:85] # Parameters for bias_init/linen.initializers.normal:
I0512 01:44:51.331180 140392160929792 gin_utils.py:85] # ==============================================================================
I0512 01:44:51.331212 140392160929792 gin_utils.py:85] bias_init/linen.initializers.normal.stddev = 1e-06
I0512 01:44:51.331243 140392160929792 gin_utils.py:85] 
I0512 01:44:51.331275 140392160929792 gin_utils.py:85] # Parameters for router_init/linen.initializers.normal:
I0512 01:44:51.331307 140392160929792 gin_utils.py:85] # ==============================================================================
I0512 01:44:51.331339 140392160929792 gin_utils.py:85] router_init/linen.initializers.normal.stddev = 0.02
I0512 01:44:51.331370 140392160929792 gin_utils.py:85] 
I0512 01:44:51.331401 140392160929792 gin_utils.py:85] # Parameters for token_embedder_init/linen.initializers.normal:
I0512 01:44:51.331433 140392160929792 gin_utils.py:85] # ==============================================================================
I0512 01:44:51.331464 140392160929792 gin_utils.py:85] token_embedder_init/linen.initializers.normal.stddev = 1.0
I0512 01:44:51.331496 140392160929792 gin_utils.py:85] 
I0512 01:44:51.331527 140392160929792 gin_utils.py:85] # Parameters for partitioning2.PjitPartitioner:
I0512 01:44:51.331559 140392160929792 gin_utils.py:85] # ==============================================================================
I0512 01:44:51.331590 140392160929792 gin_utils.py:85] partitioning2.PjitPartitioner.logical_axis_rules = \
I0512 01:44:51.331622 140392160929792 gin_utils.py:85]     @partitioning2.standard_logical_axis_rules()
I0512 01:44:51.331653 140392160929792 gin_utils.py:85] partitioning2.PjitPartitioner.model_parallel_submesh = None
I0512 01:44:51.331685 140392160929792 gin_utils.py:85] partitioning2.PjitPartitioner.num_partitions = 1
I0512 01:44:51.331716 140392160929792 gin_utils.py:85] 
I0512 01:44:51.331754 140392160929792 gin_utils.py:85] # Parameters for utils.RestoreCheckpointConfig:
I0512 01:44:51.331787 140392160929792 gin_utils.py:85] # ==============================================================================
I0512 01:44:51.331818 140392160929792 gin_utils.py:85] utils.RestoreCheckpointConfig.path = []
I0512 01:44:51.331850 140392160929792 gin_utils.py:85] 
I0512 01:44:51.331881 140392160929792 gin_utils.py:85] # Parameters for routing.RouterWeights:
I0512 01:44:51.331913 140392160929792 gin_utils.py:85] # ==============================================================================
I0512 01:44:51.331944 140392160929792 gin_utils.py:85] routing.RouterWeights.bias_init = %BIAS_INIT
I0512 01:44:51.331976 140392160929792 gin_utils.py:85] routing.RouterWeights.dtype = 'float32'
I0512 01:44:51.332010 140392160929792 gin_utils.py:85] routing.RouterWeights.kernel_init = @router_init/linen.initializers.normal()
I0512 01:44:51.332042 140392160929792 gin_utils.py:85] routing.RouterWeights.use_bias = False
I0512 01:44:51.332074 140392160929792 gin_utils.py:85] 
I0512 01:44:51.332105 140392160929792 gin_utils.py:85] # Parameters for utils.SaveCheckpointConfig:
I0512 01:44:51.332136 140392160929792 gin_utils.py:85] # ==============================================================================
I0512 01:44:51.332168 140392160929792 gin_utils.py:85] utils.SaveCheckpointConfig.dtype = 'float32'
I0512 01:44:51.332199 140392160929792 gin_utils.py:85] utils.SaveCheckpointConfig.keep = 20
I0512 01:44:51.332230 140392160929792 gin_utils.py:85] utils.SaveCheckpointConfig.period = 2500
I0512 01:44:51.332262 140392160929792 gin_utils.py:85] utils.SaveCheckpointConfig.save_dataset = True
I0512 01:44:51.332293 140392160929792 gin_utils.py:85] 
I0512 01:44:51.332325 140392160929792 gin_utils.py:85] # Parameters for seqio.SentencePieceVocabulary:
I0512 01:44:51.332356 140392160929792 gin_utils.py:85] # ==============================================================================
I0512 01:44:51.332387 140392160929792 gin_utils.py:85] seqio.SentencePieceVocabulary.extra_ids = 300
I0512 01:44:51.332419 140392160929792 gin_utils.py:85] seqio.SentencePieceVocabulary.sentencepiece_model_file = \
I0512 01:44:51.332450 140392160929792 gin_utils.py:85]     'gs://rosinality-tpu-bucket/sentencepiece.model'
I0512 01:44:51.332482 140392160929792 gin_utils.py:85] 
I0512 01:44:51.332513 140392160929792 gin_utils.py:85] # Parameters for moe_architecture.SparseDecoder:
I0512 01:44:51.332544 140392160929792 gin_utils.py:85] # ==============================================================================
I0512 01:44:51.332576 140392160929792 gin_utils.py:85] moe_architecture.SparseDecoder.dropout_factory = %DROPOUT_FACTORY
I0512 01:44:51.332607 140392160929792 gin_utils.py:85] moe_architecture.SparseDecoder.dtype = %ACTIVATION_DTYPE
I0512 01:44:51.332639 140392160929792 gin_utils.py:85] moe_architecture.SparseDecoder.layer_factory = @t5_architecture.DecoderLayer
I0512 01:44:51.332670 140392160929792 gin_utils.py:85] moe_architecture.SparseDecoder.layer_norm_factory = @layer_norm.T5LayerNorm
I0512 01:44:51.332701 140392160929792 gin_utils.py:85] moe_architecture.SparseDecoder.num_layers = %NUM_DECODER_LAYERS
I0512 01:44:51.332739 140392160929792 gin_utils.py:85] moe_architecture.SparseDecoder.num_sparse_layers = %NUM_DECODER_SPARSE_LAYERS
I0512 01:44:51.332772 140392160929792 gin_utils.py:85] moe_architecture.SparseDecoder.output_logits_factory = \
I0512 01:44:51.332804 140392160929792 gin_utils.py:85]     @output_logits/dense.DenseGeneral
I0512 01:44:51.332835 140392160929792 gin_utils.py:85] moe_architecture.SparseDecoder.sparse_layer_factory = \
I0512 01:44:51.332867 140392160929792 gin_utils.py:85]     @moe_architecture.SparseDecoderLayer
I0512 01:44:51.332898 140392160929792 gin_utils.py:85] moe_architecture.SparseDecoder.sparse_layout = %DECODER_SPARSE_LAYOUT
I0512 01:44:51.332930 140392160929792 gin_utils.py:85] 
I0512 01:44:51.332961 140392160929792 gin_utils.py:85] # Parameters for moe_architecture.SparseDecoderLayer:
I0512 01:44:51.332995 140392160929792 gin_utils.py:85] # ==============================================================================
I0512 01:44:51.333027 140392160929792 gin_utils.py:85] moe_architecture.SparseDecoderLayer.activation_partitioning_dims = \
I0512 01:44:51.333059 140392160929792 gin_utils.py:85]     %ACTIVATION_PARTITIONING_DIMS
I0512 01:44:51.333091 140392160929792 gin_utils.py:85] moe_architecture.SparseDecoderLayer.dropout_factory = %DROPOUT_FACTORY
I0512 01:44:51.333122 140392160929792 gin_utils.py:85] moe_architecture.SparseDecoderLayer.encoder_decoder_attention = None
I0512 01:44:51.333153 140392160929792 gin_utils.py:85] moe_architecture.SparseDecoderLayer.extra_mlp = @dense.MlpBlock()
I0512 01:44:51.333186 140392160929792 gin_utils.py:85] moe_architecture.SparseDecoderLayer.layer_norm_factory = @layer_norm.T5LayerNorm
I0512 01:44:51.333218 140392160929792 gin_utils.py:85] moe_architecture.SparseDecoderLayer.mlp = @sparse_decoder/moe_layers.MoeLayer()
I0512 01:44:51.333249 140392160929792 gin_utils.py:85] moe_architecture.SparseDecoderLayer.scanned = False
I0512 01:44:51.333281 140392160929792 gin_utils.py:85] moe_architecture.SparseDecoderLayer.self_attention = \
I0512 01:44:51.333313 140392160929792 gin_utils.py:85]     @dense_attention.MultiHeadDotProductAttention()
I0512 01:44:51.333344 140392160929792 gin_utils.py:85] 
I0512 01:44:51.333376 140392160929792 gin_utils.py:85] # Parameters for layer_norm.T5LayerNorm:
I0512 01:44:51.333408 140392160929792 gin_utils.py:85] # ==============================================================================
I0512 01:44:51.333439 140392160929792 gin_utils.py:85] layer_norm.T5LayerNorm.dtype = %ACTIVATION_DTYPE
I0512 01:44:51.333471 140392160929792 gin_utils.py:85] 
I0512 01:44:51.333502 140392160929792 gin_utils.py:85] # Parameters for routing.TokensChooseMaskedRouter:
I0512 01:44:51.333534 140392160929792 gin_utils.py:85] # ==============================================================================
I0512 01:44:51.333565 140392160929792 gin_utils.py:85] routing.TokensChooseMaskedRouter.dtype = 'float32'
I0512 01:44:51.333597 140392160929792 gin_utils.py:85] routing.TokensChooseMaskedRouter.ignore_padding_tokens = False
I0512 01:44:51.333629 140392160929792 gin_utils.py:85] routing.TokensChooseMaskedRouter.jitter_noise = %JITTER_NOISE
I0512 01:44:51.333661 140392160929792 gin_utils.py:85] routing.TokensChooseMaskedRouter.num_selected_experts = %NUM_SELECTED_EXPERTS
I0512 01:44:51.333693 140392160929792 gin_utils.py:85] routing.TokensChooseMaskedRouter.router_weights = @routing.RouterWeights()
I0512 01:44:51.333724 140392160929792 gin_utils.py:85] 
I0512 01:44:51.333762 140392160929792 gin_utils.py:85] # Parameters for sparse_decoder/routing.TokensChooseMaskedRouter:
I0512 01:44:51.333794 140392160929792 gin_utils.py:85] # ==============================================================================
I0512 01:44:51.333826 140392160929792 gin_utils.py:85] sparse_decoder/routing.TokensChooseMaskedRouter.batch_prioritized_routing = False
I0512 01:44:51.333858 140392160929792 gin_utils.py:85] 
I0512 01:44:51.333889 140392160929792 gin_utils.py:85] # Parameters for sparse_encoder/routing.TokensChooseMaskedRouter:
I0512 01:44:51.333920 140392160929792 gin_utils.py:85] # ==============================================================================
I0512 01:44:51.333952 140392160929792 gin_utils.py:85] sparse_encoder/routing.TokensChooseMaskedRouter.batch_prioritized_routing = False
I0512 01:44:51.333985 140392160929792 gin_utils.py:85] 
I0512 01:44:51.334018 140392160929792 gin_utils.py:85] # Parameters for train_script.train:
I0512 01:44:51.334050 140392160929792 gin_utils.py:85] # ==============================================================================
I0512 01:44:51.334100 140392160929792 gin_utils.py:85] train_script.train.checkpoint_cfg = @utils.CheckpointConfig()
I0512 01:44:51.334138 140392160929792 gin_utils.py:85] train_script.train.eval_period = 2500
I0512 01:44:51.334171 140392160929792 gin_utils.py:85] train_script.train.eval_steps = 20
I0512 01:44:51.415681 140188559153152 gin_utils.py:85] dense.MlpBlock.kernel_init = @mlp_kernel_init/linen.initializers.variance_scaling()
I0512 01:44:51.415714 140188559153152 gin_utils.py:85] dense.MlpBlock.output_axis_name = 'mlp_embed'
I0512 01:44:51.415747 140188559153152 gin_utils.py:85] dense.MlpBlock.use_bias = False
I0512 01:44:51.415780 140188559153152 gin_utils.py:85] 
I0512 01:44:51.415812 140188559153152 gin_utils.py:85] # Parameters for expert/dense.MlpBlock:
I0512 01:44:51.415845 140188559153152 gin_utils.py:85] # ==============================================================================
I0512 01:44:51.415878 140188559153152 gin_utils.py:85] expert/dense.MlpBlock.activation_partitioning_dims = 1
I0512 01:44:51.415911 140188559153152 gin_utils.py:85] expert/dense.MlpBlock.activations = ('swiglu', 'linear')
I0512 01:44:51.415944 140188559153152 gin_utils.py:85] expert/dense.MlpBlock.bias_init = %BIAS_INIT
I0512 01:44:51.415977 140188559153152 gin_utils.py:85] expert/dense.MlpBlock.data_sharding_constraints = ('expert_replicas', 'mlp')
I0512 01:44:51.416009 140188559153152 gin_utils.py:85] expert/dense.MlpBlock.dtype = %MOE_TRUNCATED_DTYPE
I0512 01:44:51.416048 140188559153152 gin_utils.py:85] expert/dense.MlpBlock.final_dropout_rate = 0.0
I0512 01:44:51.416082 140188559153152 gin_utils.py:85] expert/dense.MlpBlock.input_axis_name = 'mlp_embed'
I0512 01:44:51.416115 140188559153152 gin_utils.py:85] expert/dense.MlpBlock.intermediate_axis_name = 'expert_mlp'
I0512 01:44:51.416148 140188559153152 gin_utils.py:85] expert/dense.MlpBlock.intermediate_dim = %EXPERT_MLP_DIM
I0512 01:44:51.416180 140188559153152 gin_utils.py:85] expert/dense.MlpBlock.intermediate_dropout_rate = %EXPERT_DROPOUT_RATE
I0512 01:44:51.416213 140188559153152 gin_utils.py:85] expert/dense.MlpBlock.kernel_init = \
I0512 01:44:51.416245 140188559153152 gin_utils.py:85]     @expert_kernel_init/linen.initializers.variance_scaling()
I0512 01:44:51.416279 140188559153152 gin_utils.py:85] expert/dense.MlpBlock.output_axis_name = 'mlp_embed'
I0512 01:44:51.421091 140172769531904 gin_utils.py:85] dense.MlpBlock.kernel_init = @mlp_kernel_init/linen.initializers.variance_scaling()
I0512 01:44:51.421123 140172769531904 gin_utils.py:85] dense.MlpBlock.output_axis_name = 'mlp_embed'
I0512 01:44:51.421154 140172769531904 gin_utils.py:85] dense.MlpBlock.use_bias = False
I0512 01:44:51.421185 140172769531904 gin_utils.py:85] 
I0512 01:44:51.421216 140172769531904 gin_utils.py:85] # Parameters for expert/dense.MlpBlock:
I0512 01:44:51.421247 140172769531904 gin_utils.py:85] # ==============================================================================
I0512 01:44:51.421278 140172769531904 gin_utils.py:85] expert/dense.MlpBlock.activation_partitioning_dims = 1
I0512 01:44:51.421311 140172769531904 gin_utils.py:85] expert/dense.MlpBlock.activations = ('swiglu', 'linear')
I0512 01:44:51.421343 140172769531904 gin_utils.py:85] expert/dense.MlpBlock.bias_init = %BIAS_INIT
I0512 01:44:51.421374 140172769531904 gin_utils.py:85] expert/dense.MlpBlock.data_sharding_constraints = ('expert_replicas', 'mlp')
I0512 01:44:51.421406 140172769531904 gin_utils.py:85] expert/dense.MlpBlock.dtype = %MOE_TRUNCATED_DTYPE
I0512 01:44:51.421437 140172769531904 gin_utils.py:85] expert/dense.MlpBlock.final_dropout_rate = 0.0
I0512 01:44:51.421468 140172769531904 gin_utils.py:85] expert/dense.MlpBlock.input_axis_name = 'mlp_embed'
I0512 01:44:51.421498 140172769531904 gin_utils.py:85] expert/dense.MlpBlock.intermediate_axis_name = 'expert_mlp'
I0512 01:44:51.421530 140172769531904 gin_utils.py:85] expert/dense.MlpBlock.intermediate_dim = %EXPERT_MLP_DIM
I0512 01:44:51.421561 140172769531904 gin_utils.py:85] expert/dense.MlpBlock.intermediate_dropout_rate = %EXPERT_DROPOUT_RATE
I0512 01:44:51.421592 140172769531904 gin_utils.py:85] expert/dense.MlpBlock.kernel_init = \
I0512 01:44:51.421623 140172769531904 gin_utils.py:85]     @expert_kernel_init/linen.initializers.variance_scaling()
I0512 01:44:51.421654 140172769531904 gin_utils.py:85] expert/dense.MlpBlock.output_axis_name = 'mlp_embed'
I0512 01:44:51.528324 139676058949632 resource_reader.py:50] system_path_file_exists:flaxformer/flaxformer/t5x/configs/moe/models/tokens_choose_decoder_only_large.gin
E0512 01:44:51.528512 139676058949632 resource_reader.py:55] Path not found: flaxformer/flaxformer/t5x/configs/moe/models/tokens_choose_decoder_only_large.gin
I0512 01:44:51.528570 139676058949632 resource_reader.py:50] system_path_file_exists:./flaxformer/flaxformer/t5x/configs/moe/models/tokens_choose_decoder_only_large.gin
E0512 01:44:51.528613 139676058949632 resource_reader.py:55] Path not found: ./flaxformer/flaxformer/t5x/configs/moe/models/tokens_choose_decoder_only_large.gin
I0512 01:44:51.529479 139676058949632 resource_reader.py:50] system_path_file_exists:flaxformer/flaxformer/t5x/configs/moe/architectures/moe_decoder_only.gin
E0512 01:44:51.529610 139676058949632 resource_reader.py:55] Path not found: flaxformer/flaxformer/t5x/configs/moe/architectures/moe_decoder_only.gin
I0512 01:44:51.529664 139676058949632 resource_reader.py:50] system_path_file_exists:./flaxformer/flaxformer/t5x/configs/moe/architectures/moe_decoder_only.gin
E0512 01:44:51.529705 139676058949632 resource_reader.py:55] Path not found: ./flaxformer/flaxformer/t5x/configs/moe/architectures/moe_decoder_only.gin
I0512 01:44:51.536567 139812357228544 resource_reader.py:50] system_path_file_exists:t5x/contrib/moe/configs/runs/pretrain.gin
I0512 01:44:51.364636 139997397456896 gin_utils.py:85] expert/dense.MlpBlock.use_bias = False
I0512 01:44:51.364668 139997397456896 gin_utils.py:85] 
I0512 01:44:51.364700 139997397456896 gin_utils.py:85] # Parameters for models.MoeDecoderOnlyModel:
I0512 01:44:51.364732 139997397456896 gin_utils.py:85] # ==============================================================================
I0512 01:44:51.364764 139997397456896 gin_utils.py:85] models.MoeDecoderOnlyModel.aux_loss_factor = %AUX_LOSS_FACTOR
I0512 01:44:51.364796 139997397456896 gin_utils.py:85] models.MoeDecoderOnlyModel.label_smoothing = %LABEL_SMOOTHING
I0512 01:44:51.364828 139997397456896 gin_utils.py:85] models.MoeDecoderOnlyModel.loss_normalizing_factor = %LOSS_NORMALIZING_FACTOR
I0512 01:44:51.364859 139997397456896 gin_utils.py:85] models.MoeDecoderOnlyModel.module = %ARCHITECTURE
I0512 01:44:51.364892 139997397456896 gin_utils.py:85] models.MoeDecoderOnlyModel.optimizer_def = %OPTIMIZER
I0512 01:44:51.364924 139997397456896 gin_utils.py:85] models.MoeDecoderOnlyModel.router_z_loss_factor = %ROUTER_Z_LOSS_FACTOR
I0512 01:44:51.364974 139997397456896 gin_utils.py:85] models.MoeDecoderOnlyModel.vocabulary = %VOCABULARY
I0512 01:44:51.365013 139997397456896 gin_utils.py:85] models.MoeDecoderOnlyModel.z_loss = %Z_LOSS
I0512 01:44:51.365046 139997397456896 gin_utils.py:85] 
I0512 01:44:51.365079 139997397456896 gin_utils.py:85] # Parameters for moe_layers.MoeLayer:
I0512 01:44:51.365111 139997397456896 gin_utils.py:85] # ==============================================================================
I0512 01:44:51.365142 139997397456896 gin_utils.py:85] moe_layers.MoeLayer.dtype = %MOE_TRUNCATED_DTYPE
I0512 01:44:51.365174 139997397456896 gin_utils.py:85] moe_layers.MoeLayer.eval_capacity_factor = %EVAL_EXPERT_CAPACITY_FACTOR
I0512 01:44:51.365206 139997397456896 gin_utils.py:85] moe_layers.MoeLayer.expert = @expert/dense.MlpBlock()
I0512 01:44:51.365237 139997397456896 gin_utils.py:85] moe_layers.MoeLayer.max_group_size = %GROUP_SIZE
I0512 01:44:51.365268 139997397456896 gin_utils.py:85] moe_layers.MoeLayer.num_expert_partitions = %NUM_EXPERT_PARTITIONS
I0512 01:44:51.365299 139997397456896 gin_utils.py:85] moe_layers.MoeLayer.num_experts = %NUM_EXPERTS
I0512 01:44:51.365336 139997397456896 gin_utils.py:85] moe_layers.MoeLayer.num_model_partitions = \
I0512 01:44:51.365369 139997397456896 gin_utils.py:85]     @partitioning.compute_num_model_partitions()
I0512 01:44:51.365400 139997397456896 gin_utils.py:85] moe_layers.MoeLayer.train_capacity_factor = %TRAIN_EXPERT_CAPACITY_FACTOR
I0512 01:44:51.365431 139997397456896 gin_utils.py:85] 
I0512 01:44:51.365463 139997397456896 gin_utils.py:85] # Parameters for sparse_decoder/moe_layers.MoeLayer:
I0512 01:44:51.365494 139997397456896 gin_utils.py:85] # ==============================================================================
I0512 01:44:51.365525 139997397456896 gin_utils.py:85] sparse_decoder/moe_layers.MoeLayer.router = \
I0512 01:44:51.365556 139997397456896 gin_utils.py:85]     @sparse_decoder/routing.TokensChooseMaskedRouter()
I0512 01:44:51.365589 139997397456896 gin_utils.py:85] 
I0512 01:44:51.365621 139997397456896 gin_utils.py:85] # Parameters for partitioning.MoePjitPartitioner:
I0512 01:44:51.365653 139997397456896 gin_utils.py:85] # ==============================================================================
I0512 01:44:51.365684 139997397456896 gin_utils.py:85] partitioning.MoePjitPartitioner.model_parallel_submesh = %MODEL_PARALLEL_SUBMESH
I0512 01:44:51.365715 139997397456896 gin_utils.py:85] partitioning.MoePjitPartitioner.num_expert_partitions = %NUM_EXPERT_PARTITIONS
I0512 01:44:51.365746 139997397456896 gin_utils.py:85] partitioning.MoePjitPartitioner.num_partitions = %NUM_MODEL_PARTITIONS
I0512 01:44:51.365777 139997397456896 gin_utils.py:85] 
I0512 01:44:51.365808 139997397456896 gin_utils.py:85] # Parameters for moe_trainer.MoeTrainer:
I0512 01:44:51.365839 139997397456896 gin_utils.py:85] # ==============================================================================
I0512 01:44:51.365871 139997397456896 gin_utils.py:85] moe_trainer.MoeTrainer.learning_rate_fn = @utils.create_learning_rate_scheduler()
I0512 01:44:51.365902 139997397456896 gin_utils.py:85] moe_trainer.MoeTrainer.num_expert_partitions = %NUM_EXPERT_PARTITIONS
I0512 01:44:51.365932 139997397456896 gin_utils.py:85] moe_trainer.MoeTrainer.num_microbatches = 8
I0512 01:44:51.365963 139997397456896 gin_utils.py:85] 
I0512 01:44:51.365994 139997397456896 gin_utils.py:85] # Parameters for dense_attention.MultiHeadDotProductAttention:
I0512 01:44:51.366025 139997397456896 gin_utils.py:85] # ==============================================================================
I0512 01:44:51.366057 139997397456896 gin_utils.py:85] dense_attention.MultiHeadDotProductAttention.bias_init = %BIAS_INIT
I0512 01:44:51.366088 139997397456896 gin_utils.py:85] dense_attention.MultiHeadDotProductAttention.broadcast_dropout = True
I0512 01:44:51.366119 139997397456896 gin_utils.py:85] dense_attention.MultiHeadDotProductAttention.dropout_rate = %DROPOUT_RATE
I0512 01:44:51.366150 139997397456896 gin_utils.py:85] dense_attention.MultiHeadDotProductAttention.dtype = %ACTIVATION_DTYPE
I0512 01:44:51.366181 139997397456896 gin_utils.py:85] dense_attention.MultiHeadDotProductAttention.head_dim = %HEAD_DIM
I0512 01:44:51.366213 139997397456896 gin_utils.py:85] dense_attention.MultiHeadDotProductAttention.kernel_init = \
I0512 01:44:51.366244 139997397456896 gin_utils.py:85]     @attention_kernel_init/linen.initializers.variance_scaling()
I0512 01:44:51.366275 139997397456896 gin_utils.py:85] dense_attention.MultiHeadDotProductAttention.num_heads = %NUM_HEADS
I0512 01:44:51.366306 139997397456896 gin_utils.py:85] dense_attention.MultiHeadDotProductAttention.use_bias = False
I0512 01:44:51.366345 139997397456896 gin_utils.py:85] dense_attention.MultiHeadDotProductAttention.use_rotary_embedding = True
I0512 01:44:51.366376 139997397456896 gin_utils.py:85] 
I0512 01:44:51.366408 139997397456896 gin_utils.py:85] # Parameters for bias_init/linen.initializers.normal:
I0512 01:44:51.366440 139997397456896 gin_utils.py:85] # ==============================================================================
I0512 01:44:51.366471 139997397456896 gin_utils.py:85] bias_init/linen.initializers.normal.stddev = 1e-06
I0512 01:44:51.366502 139997397456896 gin_utils.py:85] 
I0512 01:44:51.366533 139997397456896 gin_utils.py:85] # Parameters for router_init/linen.initializers.normal:
I0512 01:44:51.366567 139997397456896 gin_utils.py:85] # ==============================================================================
I0512 01:44:51.366600 139997397456896 gin_utils.py:85] router_init/linen.initializers.normal.stddev = 0.02
I0512 01:44:51.366632 139997397456896 gin_utils.py:85] 
I0512 01:44:51.366664 139997397456896 gin_utils.py:85] # Parameters for token_embedder_init/linen.initializers.normal:
I0512 01:44:51.366697 139997397456896 gin_utils.py:85] # ==============================================================================
I0512 01:44:51.366728 139997397456896 gin_utils.py:85] token_embedder_init/linen.initializers.normal.stddev = 1.0
I0512 01:44:51.366760 139997397456896 gin_utils.py:85] 
I0512 01:44:51.366792 139997397456896 gin_utils.py:85] # Parameters for partitioning2.PjitPartitioner:
I0512 01:44:51.366823 139997397456896 gin_utils.py:85] # ==============================================================================
I0512 01:44:51.366855 139997397456896 gin_utils.py:85] partitioning2.PjitPartitioner.logical_axis_rules = \
I0512 01:44:51.366887 139997397456896 gin_utils.py:85]     @partitioning2.standard_logical_axis_rules()
I0512 01:44:51.366918 139997397456896 gin_utils.py:85] partitioning2.PjitPartitioner.model_parallel_submesh = None
I0512 01:44:51.366949 139997397456896 gin_utils.py:85] partitioning2.PjitPartitioner.num_partitions = 1
I0512 01:44:51.366981 139997397456896 gin_utils.py:85] 
I0512 01:44:51.367012 139997397456896 gin_utils.py:85] # Parameters for utils.RestoreCheckpointConfig:
I0512 01:44:51.367044 139997397456896 gin_utils.py:85] # ==============================================================================
I0512 01:44:51.367075 139997397456896 gin_utils.py:85] utils.RestoreCheckpointConfig.path = []
I0512 01:44:51.367106 139997397456896 gin_utils.py:85] 
I0512 01:44:51.367138 139997397456896 gin_utils.py:85] # Parameters for routing.RouterWeights:
I0512 01:44:51.367170 139997397456896 gin_utils.py:85] # ==============================================================================
I0512 01:44:51.367201 139997397456896 gin_utils.py:85] routing.RouterWeights.bias_init = %BIAS_INIT
I0512 01:44:51.367232 139997397456896 gin_utils.py:85] routing.RouterWeights.dtype = 'float32'
I0512 01:44:51.367264 139997397456896 gin_utils.py:85] routing.RouterWeights.kernel_init = @router_init/linen.initializers.normal()
I0512 01:44:51.367295 139997397456896 gin_utils.py:85] routing.RouterWeights.use_bias = False
I0512 01:44:51.367332 139997397456896 gin_utils.py:85] 
I0512 01:44:51.367365 139997397456896 gin_utils.py:85] # Parameters for utils.SaveCheckpointConfig:
I0512 01:44:51.367397 139997397456896 gin_utils.py:85] # ==============================================================================
I0512 01:44:51.367428 139997397456896 gin_utils.py:85] utils.SaveCheckpointConfig.dtype = 'float32'
I0512 01:44:51.367459 139997397456896 gin_utils.py:85] utils.SaveCheckpointConfig.keep = 20
I0512 01:44:51.367491 139997397456896 gin_utils.py:85] utils.SaveCheckpointConfig.period = 2500
I0512 01:44:51.367526 139997397456896 gin_utils.py:85] utils.SaveCheckpointConfig.save_dataset = True
I0512 01:44:51.367558 139997397456896 gin_utils.py:85] 
I0512 01:44:51.367592 139997397456896 gin_utils.py:85] # Parameters for seqio.SentencePieceVocabulary:
I0512 01:44:51.367624 139997397456896 gin_utils.py:85] # ==============================================================================
I0512 01:44:51.367655 139997397456896 gin_utils.py:85] seqio.SentencePieceVocabulary.extra_ids = 300
I0512 01:44:51.367687 139997397456896 gin_utils.py:85] seqio.SentencePieceVocabulary.sentencepiece_model_file = \
I0512 01:44:51.367718 139997397456896 gin_utils.py:85]     'gs://rosinality-tpu-bucket/sentencepiece.model'
I0512 01:44:51.367750 139997397456896 gin_utils.py:85] 
I0512 01:44:51.367781 139997397456896 gin_utils.py:85] # Parameters for moe_architecture.SparseDecoder:
I0512 01:44:51.367813 139997397456896 gin_utils.py:85] # ==============================================================================
I0512 01:44:51.367844 139997397456896 gin_utils.py:85] moe_architecture.SparseDecoder.dropout_factory = %DROPOUT_FACTORY
I0512 01:44:51.367876 139997397456896 gin_utils.py:85] moe_architecture.SparseDecoder.dtype = %ACTIVATION_DTYPE
I0512 01:44:51.367907 139997397456896 gin_utils.py:85] moe_architecture.SparseDecoder.layer_factory = @t5_architecture.DecoderLayer
I0512 01:44:51.367938 139997397456896 gin_utils.py:85] moe_architecture.SparseDecoder.layer_norm_factory = @layer_norm.T5LayerNorm
I0512 01:44:51.367970 139997397456896 gin_utils.py:85] moe_architecture.SparseDecoder.num_layers = %NUM_DECODER_LAYERS
I0512 01:44:51.368001 139997397456896 gin_utils.py:85] moe_architecture.SparseDecoder.num_sparse_layers = %NUM_DECODER_SPARSE_LAYERS
I0512 01:44:51.368033 139997397456896 gin_utils.py:85] moe_architecture.SparseDecoder.output_logits_factory = \
I0512 01:44:51.368064 139997397456896 gin_utils.py:85]     @output_logits/dense.DenseGeneral
I0512 01:44:51.368095 139997397456896 gin_utils.py:85] moe_architecture.SparseDecoder.sparse_layer_factory = \
I0512 01:44:51.368127 139997397456896 gin_utils.py:85]     @moe_architecture.SparseDecoderLayer
I0512 01:44:51.368158 139997397456896 gin_utils.py:85] moe_architecture.SparseDecoder.sparse_layout = %DECODER_SPARSE_LAYOUT
I0512 01:44:51.368190 139997397456896 gin_utils.py:85] 
I0512 01:44:51.368221 139997397456896 gin_utils.py:85] # Parameters for moe_architecture.SparseDecoderLayer:
I0512 01:44:51.368252 139997397456896 gin_utils.py:85] # ==============================================================================
I0512 01:44:51.368284 139997397456896 gin_utils.py:85] moe_architecture.SparseDecoderLayer.activation_partitioning_dims = \
I0512 01:44:51.368320 139997397456896 gin_utils.py:85]     %ACTIVATION_PARTITIONING_DIMS
I0512 01:44:51.368353 139997397456896 gin_utils.py:85] moe_architecture.SparseDecoderLayer.dropout_factory = %DROPOUT_FACTORY
I0512 01:44:51.368385 139997397456896 gin_utils.py:85] moe_architecture.SparseDecoderLayer.encoder_decoder_attention = None
I0512 01:44:51.368417 139997397456896 gin_utils.py:85] moe_architecture.SparseDecoderLayer.extra_mlp = @dense.MlpBlock()
I0512 01:44:51.368449 139997397456896 gin_utils.py:85] moe_architecture.SparseDecoderLayer.layer_norm_factory = @layer_norm.T5LayerNorm
I0512 01:44:51.368480 139997397456896 gin_utils.py:85] moe_architecture.SparseDecoderLayer.mlp = @sparse_decoder/moe_layers.MoeLayer()
I0512 01:44:51.368512 139997397456896 gin_utils.py:85] moe_architecture.SparseDecoderLayer.scanned = False
I0512 01:44:51.368544 139997397456896 gin_utils.py:85] moe_architecture.SparseDecoderLayer.self_attention = \
I0512 01:44:51.368578 139997397456896 gin_utils.py:85]     @dense_attention.MultiHeadDotProductAttention()
I0512 01:44:51.368611 139997397456896 gin_utils.py:85] 
I0512 01:44:51.368643 139997397456896 gin_utils.py:85] # Parameters for layer_norm.T5LayerNorm:
I0512 01:44:51.368675 139997397456896 gin_utils.py:85] # ==============================================================================
I0512 01:44:51.368707 139997397456896 gin_utils.py:85] layer_norm.T5LayerNorm.dtype = %ACTIVATION_DTYPE
I0512 01:44:51.368738 139997397456896 gin_utils.py:85] 
I0512 01:44:51.368770 139997397456896 gin_utils.py:85] # Parameters for routing.TokensChooseMaskedRouter:
I0512 01:44:51.368801 139997397456896 gin_utils.py:85] # ==============================================================================
I0512 01:44:51.368833 139997397456896 gin_utils.py:85] routing.TokensChooseMaskedRouter.dtype = 'float32'
I0512 01:44:51.368864 139997397456896 gin_utils.py:85] routing.TokensChooseMaskedRouter.ignore_padding_tokens = False
I0512 01:44:51.368896 139997397456896 gin_utils.py:85] routing.TokensChooseMaskedRouter.jitter_noise = %JITTER_NOISE
I0512 01:44:51.368927 139997397456896 gin_utils.py:85] routing.TokensChooseMaskedRouter.num_selected_experts = %NUM_SELECTED_EXPERTS
I0512 01:44:51.368979 139997397456896 gin_utils.py:85] routing.TokensChooseMaskedRouter.router_weights = @routing.RouterWeights()
I0512 01:44:51.369017 139997397456896 gin_utils.py:85] 
I0512 01:44:51.369050 139997397456896 gin_utils.py:85] # Parameters for sparse_decoder/routing.TokensChooseMaskedRouter:
I0512 01:44:51.369082 139997397456896 gin_utils.py:85] # ==============================================================================
I0512 01:44:51.369113 139997397456896 gin_utils.py:85] sparse_decoder/routing.TokensChooseMaskedRouter.batch_prioritized_routing = False
I0512 01:44:51.369144 139997397456896 gin_utils.py:85] 
I0512 01:44:51.369176 139997397456896 gin_utils.py:85] # Parameters for sparse_encoder/routing.TokensChooseMaskedRouter:
I0512 01:44:51.369207 139997397456896 gin_utils.py:85] # ==============================================================================
I0512 01:44:51.369238 139997397456896 gin_utils.py:85] sparse_encoder/routing.TokensChooseMaskedRouter.batch_prioritized_routing = False
I0512 01:44:51.369270 139997397456896 gin_utils.py:85] 
I0512 01:44:51.369301 139997397456896 gin_utils.py:85] # Parameters for train_script.train:
I0512 01:44:51.369339 139997397456896 gin_utils.py:85] # ==============================================================================
I0512 01:44:51.369372 139997397456896 gin_utils.py:85] train_script.train.checkpoint_cfg = @utils.CheckpointConfig()
I0512 01:44:51.369404 139997397456896 gin_utils.py:85] train_script.train.eval_period = 2500
I0512 01:44:51.369436 139997397456896 gin_utils.py:85] train_script.train.eval_steps = 20
I0512 01:44:51.467986 140316961150976 gin_utils.py:85] # Parameters for utils.CheckpointConfig:
I0512 01:44:51.468018 140316961150976 gin_utils.py:85] # ==============================================================================
I0512 01:44:51.468050 140316961150976 gin_utils.py:85] utils.CheckpointConfig.restore = @utils.RestoreCheckpointConfig()
I0512 01:44:51.468081 140316961150976 gin_utils.py:85] utils.CheckpointConfig.save = @utils.SaveCheckpointConfig()
I0512 01:44:51.468115 140316961150976 gin_utils.py:85] 
I0512 01:44:51.468147 140316961150976 gin_utils.py:85] # Parameters for moe_partitioning.compute_num_model_partitions:
I0512 01:44:51.468179 140316961150976 gin_utils.py:85] # ==============================================================================
I0512 01:44:51.468211 140316961150976 gin_utils.py:85] moe_partitioning.compute_num_model_partitions.model_parallel_submesh = \
I0512 01:44:51.468244 140316961150976 gin_utils.py:85]     %MODEL_PARALLEL_SUBMESH
I0512 01:44:51.468275 140316961150976 gin_utils.py:85] moe_partitioning.compute_num_model_partitions.num_model_partitions = \
I0512 01:44:51.468307 140316961150976 gin_utils.py:85]     %NUM_MODEL_PARTITIONS
I0512 01:44:51.468338 140316961150976 gin_utils.py:85] 
I0512 01:44:51.468370 140316961150976 gin_utils.py:85] # Parameters for utils.create_learning_rate_scheduler:
I0512 01:44:51.468402 140316961150976 gin_utils.py:85] # ==============================================================================
I0512 01:44:51.468434 140316961150976 gin_utils.py:85] utils.create_learning_rate_scheduler.base_learning_rate = 1.0
I0512 01:44:51.468466 140316961150976 gin_utils.py:85] utils.create_learning_rate_scheduler.factors = 'constant * rsqrt_decay'
I0512 01:44:51.468497 140316961150976 gin_utils.py:85] utils.create_learning_rate_scheduler.warmup_steps = 10000
I0512 01:44:51.468529 140316961150976 gin_utils.py:85] 
I0512 01:44:51.468561 140316961150976 gin_utils.py:85] # Parameters for train/utils.DatasetConfig:
I0512 01:44:51.468592 140316961150976 gin_utils.py:85] # ==============================================================================
I0512 01:44:51.468624 140316961150976 gin_utils.py:85] train/utils.DatasetConfig.batch_size = %BATCH_SIZE
I0512 01:44:51.468656 140316961150976 gin_utils.py:85] train/utils.DatasetConfig.mixture_or_task_name = %MIXTURE_OR_TASK_NAME
I0512 01:44:51.468688 140316961150976 gin_utils.py:85] train/utils.DatasetConfig.module = %MIXTURE_OR_TASK_MODULE
I0512 01:44:51.468720 140316961150976 gin_utils.py:85] train/utils.DatasetConfig.pack = True
I0512 01:44:51.468751 140316961150976 gin_utils.py:85] train/utils.DatasetConfig.seed = 42
I0512 01:44:51.468783 140316961150976 gin_utils.py:85] train/utils.DatasetConfig.shuffle = %SHUFFLE_TRAIN_EXAMPLES
I0512 01:44:51.468815 140316961150976 gin_utils.py:85] train/utils.DatasetConfig.split = 'train'
I0512 01:44:51.468846 140316961150976 gin_utils.py:85] train/utils.DatasetConfig.task_feature_lengths = %TASK_FEATURE_LENGTHS
I0512 01:44:51.468884 140316961150976 gin_utils.py:85] train/utils.DatasetConfig.use_cached = %USE_CACHED_TASKS
I0512 01:44:51.468917 140316961150976 gin_utils.py:85] 
I0512 01:44:51.468949 140316961150976 gin_utils.py:85] # Parameters for train_eval/utils.DatasetConfig:
I0512 01:44:51.468981 140316961150976 gin_utils.py:85] # ==============================================================================
I0512 01:44:51.469012 140316961150976 gin_utils.py:85] train_eval/utils.DatasetConfig.batch_size = 128
I0512 01:44:51.469044 140316961150976 gin_utils.py:85] train_eval/utils.DatasetConfig.mixture_or_task_name = %MIXTURE_OR_TASK_NAME
I0512 01:44:51.469076 140316961150976 gin_utils.py:85] train_eval/utils.DatasetConfig.module = %MIXTURE_OR_TASK_MODULE
I0512 01:44:51.469109 140316961150976 gin_utils.py:85] train_eval/utils.DatasetConfig.pack = True
I0512 01:44:51.469141 140316961150976 gin_utils.py:85] train_eval/utils.DatasetConfig.seed = 42
I0512 01:44:51.469173 140316961150976 gin_utils.py:85] train_eval/utils.DatasetConfig.shuffle = False
I0512 01:44:51.469205 140316961150976 gin_utils.py:85] train_eval/utils.DatasetConfig.split = 'validation'
I0512 01:44:51.469237 140316961150976 gin_utils.py:85] train_eval/utils.DatasetConfig.task_feature_lengths = %TASK_FEATURE_LENGTHS
I0512 01:44:51.469269 140316961150976 gin_utils.py:85] train_eval/utils.DatasetConfig.use_cached = %USE_CACHED_TASKS
I0512 01:44:51.469300 140316961150976 gin_utils.py:85] 
I0512 01:44:51.469332 140316961150976 gin_utils.py:85] # Parameters for t5_architecture.DecoderLayer:
I0512 01:44:51.469363 140316961150976 gin_utils.py:85] # ==============================================================================
I0512 01:44:51.469395 140316961150976 gin_utils.py:85] t5_architecture.DecoderLayer.activation_partitioning_dims = \
I0512 01:44:51.469427 140316961150976 gin_utils.py:85]     %ACTIVATION_PARTITIONING_DIMS
I0512 01:44:51.469459 140316961150976 gin_utils.py:85] t5_architecture.DecoderLayer.dropout_factory = %DROPOUT_FACTORY
I0512 01:44:51.469490 140316961150976 gin_utils.py:85] t5_architecture.DecoderLayer.encoder_decoder_attention = None
I0512 01:44:51.469522 140316961150976 gin_utils.py:85] t5_architecture.DecoderLayer.layer_norm_factory = @layer_norm.T5LayerNorm
I0512 01:44:51.469582 140316961150976 gin_utils.py:85] t5_architecture.DecoderLayer.mlp = @dense.MlpBlock()
I0512 01:44:51.469619 140316961150976 gin_utils.py:85] t5_architecture.DecoderLayer.scanned = False
I0512 01:44:51.469651 140316961150976 gin_utils.py:85] t5_architecture.DecoderLayer.self_attention = \
I0512 01:44:51.469683 140316961150976 gin_utils.py:85]     @dense_attention.MultiHeadDotProductAttention()
I0512 01:44:51.469715 140316961150976 gin_utils.py:85] 
I0512 01:44:51.469747 140316961150976 gin_utils.py:85] # Parameters for t5_architecture.DecoderOnly:
I0512 01:44:51.469779 140316961150976 gin_utils.py:85] # ==============================================================================
I0512 01:44:51.469811 140316961150976 gin_utils.py:85] t5_architecture.DecoderOnly.decoder_factory = @moe_architecture.SparseDecoder
I0512 01:44:51.469842 140316961150976 gin_utils.py:85] t5_architecture.DecoderOnly.dtype = %ACTIVATION_DTYPE
I0512 01:44:51.469880 140316961150976 gin_utils.py:85] t5_architecture.DecoderOnly.shared_token_embedder_factory = @embedding.Embed
I0512 01:44:51.469913 140316961150976 gin_utils.py:85] 
I0512 01:44:51.469945 140316961150976 gin_utils.py:85] # Parameters for output_logits/dense.DenseGeneral:
I0512 01:44:51.469977 140316961150976 gin_utils.py:85] # ==============================================================================
I0512 01:44:51.470008 140316961150976 gin_utils.py:85] output_logits/dense.DenseGeneral.bias_init = %BIAS_INIT
I0512 01:44:51.470040 140316961150976 gin_utils.py:85] output_logits/dense.DenseGeneral.dtype = 'float32'
I0512 01:44:51.470072 140316961150976 gin_utils.py:85] output_logits/dense.DenseGeneral.features = %NUM_EMBEDDINGS
I0512 01:44:51.470105 140316961150976 gin_utils.py:85] output_logits/dense.DenseGeneral.kernel_axis_names = ['embed', 'vocab']
I0512 01:44:51.470139 140316961150976 gin_utils.py:85] output_logits/dense.DenseGeneral.kernel_init = \
I0512 01:44:51.470171 140316961150976 gin_utils.py:85]     @output_logits_kernel_init/linen.initializers.variance_scaling()
I0512 01:44:51.470202 140316961150976 gin_utils.py:85] output_logits/dense.DenseGeneral.use_bias = False
I0512 01:44:51.470234 140316961150976 gin_utils.py:85] 
I0512 01:44:51.470266 140316961150976 gin_utils.py:85] # Parameters for dropout_factory/linen.Dropout:
I0512 01:44:51.470297 140316961150976 gin_utils.py:85] # ==============================================================================
I0512 01:44:51.470329 140316961150976 gin_utils.py:85] dropout_factory/linen.Dropout.broadcast_dims = (-2,)
I0512 01:44:51.470360 140316961150976 gin_utils.py:85] dropout_factory/linen.Dropout.rate = %DROPOUT_RATE
I0512 01:44:51.470392 140316961150976 gin_utils.py:85] 
I0512 01:44:51.470424 140316961150976 gin_utils.py:85] # Parameters for embedding.Embed:
I0512 01:44:51.470455 140316961150976 gin_utils.py:85] # ==============================================================================
I0512 01:44:51.470487 140316961150976 gin_utils.py:85] embedding.Embed.attend_dtype = 'float32'
I0512 01:44:51.470518 140316961150976 gin_utils.py:85] embedding.Embed.cast_input_dtype = 'int32'
I0512 01:44:51.470550 140316961150976 gin_utils.py:85] embedding.Embed.dtype = %ACTIVATION_DTYPE
I0512 01:44:51.470582 140316961150976 gin_utils.py:85] embedding.Embed.embedding_init = @token_embedder_init/linen.initializers.normal()
I0512 01:44:51.470613 140316961150976 gin_utils.py:85] embedding.Embed.features = %EMBED_DIM
I0512 01:44:51.470645 140316961150976 gin_utils.py:85] embedding.Embed.name = 'token_embedder'
I0512 01:44:51.470677 140316961150976 gin_utils.py:85] embedding.Embed.num_embeddings = %NUM_EMBEDDINGS
I0512 01:44:51.470709 140316961150976 gin_utils.py:85] embedding.Embed.one_hot = True
I0512 01:44:51.470741 140316961150976 gin_utils.py:85] 
I0512 01:44:51.470773 140316961150976 gin_utils.py:85] # Parameters for dense.MlpBlock:
I0512 01:44:51.470805 140316961150976 gin_utils.py:85] # ==============================================================================
I0512 01:44:51.470837 140316961150976 gin_utils.py:85] dense.MlpBlock.activations = ('swiglu', 'linear')
I0512 01:44:51.470875 140316961150976 gin_utils.py:85] dense.MlpBlock.bias_init = %BIAS_INIT
I0512 01:44:51.470908 140316961150976 gin_utils.py:85] dense.MlpBlock.dtype = %ACTIVATION_DTYPE
I0512 01:44:51.470939 140316961150976 gin_utils.py:85] dense.MlpBlock.final_dropout_rate = 0
I0512 01:44:51.470971 140316961150976 gin_utils.py:85] dense.MlpBlock.input_axis_name = 'mlp_embed'
I0512 01:44:51.471002 140316961150976 gin_utils.py:85] dense.MlpBlock.intermediate_dim = %MLP_DIM
I0512 01:44:51.471034 140316961150976 gin_utils.py:85] dense.MlpBlock.intermediate_dropout_rate = %DROPOUT_RATE
I0512 01:44:51.334202 140392160929792 gin_utils.py:85] train_script.train.infer_eval_dataset_cfg = None
I0512 01:44:51.334233 140392160929792 gin_utils.py:85] train_script.train.model = %MODEL
I0512 01:44:51.334264 140392160929792 gin_utils.py:85] train_script.train.model_dir = %MODEL_DIR
I0512 01:44:51.334296 140392160929792 gin_utils.py:85] train_script.train.partitioner = @partitioning.MoePjitPartitioner()
I0512 01:44:51.334328 140392160929792 gin_utils.py:85] train_script.train.random_seed = 42
I0512 01:44:51.334359 140392160929792 gin_utils.py:85] train_script.train.stats_period = 10
I0512 01:44:51.334390 140392160929792 gin_utils.py:85] train_script.train.summarize_config_fn = @gin_utils.summarize_gin_config
I0512 01:44:51.334422 140392160929792 gin_utils.py:85] train_script.train.total_steps = %TRAIN_STEPS
I0512 01:44:51.334454 140392160929792 gin_utils.py:85] train_script.train.train_dataset_cfg = @train/utils.DatasetConfig()
I0512 01:44:51.334485 140392160929792 gin_utils.py:85] train_script.train.train_eval_dataset_cfg = None
I0512 01:44:51.334516 140392160929792 gin_utils.py:85] train_script.train.trainer_cls = @moe_trainer.MoeTrainer
I0512 01:44:51.334547 140392160929792 gin_utils.py:85] train_script.train.use_hardware_rng = %USE_HARDWARE_RNG
I0512 01:44:51.334578 140392160929792 gin_utils.py:85] 
I0512 01:44:51.334610 140392160929792 gin_utils.py:85] # Parameters for trainer.Trainer:
I0512 01:44:51.334641 140392160929792 gin_utils.py:85] # ==============================================================================
I0512 01:44:51.334672 140392160929792 gin_utils.py:85] trainer.Trainer.learning_rate_fn = @utils.create_learning_rate_scheduler()
I0512 01:44:51.334704 140392160929792 gin_utils.py:85] trainer.Trainer.num_microbatches = None
I0512 01:44:51.334742 140392160929792 gin_utils.py:85] 
I0512 01:44:51.334774 140392160929792 gin_utils.py:85] # Parameters for attention_kernel_init/linen.initializers.variance_scaling:
I0512 01:44:51.334805 140392160929792 gin_utils.py:85] # ==============================================================================
I0512 01:44:51.334836 140392160929792 gin_utils.py:85] attention_kernel_init/linen.initializers.variance_scaling.distribution = 'normal'
I0512 01:44:51.334867 140392160929792 gin_utils.py:85] attention_kernel_init/linen.initializers.variance_scaling.mode = 'fan_in'
I0512 01:44:51.334899 140392160929792 gin_utils.py:85] attention_kernel_init/linen.initializers.variance_scaling.scale = %SCALE
I0512 01:44:51.334930 140392160929792 gin_utils.py:85] 
I0512 01:44:51.334961 140392160929792 gin_utils.py:85] # Parameters for expert_kernel_init/linen.initializers.variance_scaling:
I0512 01:44:51.334995 140392160929792 gin_utils.py:85] # ==============================================================================
I0512 01:44:51.335028 140392160929792 gin_utils.py:85] expert_kernel_init/linen.initializers.variance_scaling.distribution = 'normal'
I0512 01:44:51.335059 140392160929792 gin_utils.py:85] expert_kernel_init/linen.initializers.variance_scaling.mode = 'fan_in'
I0512 01:44:51.335091 140392160929792 gin_utils.py:85] expert_kernel_init/linen.initializers.variance_scaling.scale = %SCALE
I0512 01:44:51.335123 140392160929792 gin_utils.py:85] 
I0512 01:44:51.335154 140392160929792 gin_utils.py:85] # Parameters for mlp_kernel_init/linen.initializers.variance_scaling:
I0512 01:44:51.335186 140392160929792 gin_utils.py:85] # ==============================================================================
I0512 01:44:51.335217 140392160929792 gin_utils.py:85] mlp_kernel_init/linen.initializers.variance_scaling.distribution = \
I0512 01:44:51.335248 140392160929792 gin_utils.py:85]     'truncated_normal'
I0512 01:44:51.335279 140392160929792 gin_utils.py:85] mlp_kernel_init/linen.initializers.variance_scaling.mode = 'fan_in'
I0512 01:44:51.335311 140392160929792 gin_utils.py:85] mlp_kernel_init/linen.initializers.variance_scaling.scale = %SCALE
I0512 01:44:51.335342 140392160929792 gin_utils.py:85] 
I0512 01:44:51.335373 140392160929792 gin_utils.py:85] # Parameters for output_logits_kernel_init/linen.initializers.variance_scaling:
I0512 01:44:51.335405 140392160929792 gin_utils.py:85] # ==============================================================================
I0512 01:44:51.335436 140392160929792 gin_utils.py:85] output_logits_kernel_init/linen.initializers.variance_scaling.distribution = \
I0512 01:44:51.335467 140392160929792 gin_utils.py:85]     'truncated_normal'
I0512 01:44:51.335498 140392160929792 gin_utils.py:85] output_logits_kernel_init/linen.initializers.variance_scaling.mode = 'fan_in'
I0512 01:44:51.335529 140392160929792 gin_utils.py:85] output_logits_kernel_init/linen.initializers.variance_scaling.scale = %SCALE
I0512 01:44:51.336433 140392160929792 partitioning.py:559] `activation_partitioning_dims` = 2, `parameter_partitioning_dims` = 1
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
I0000 00:00:1715478291.400362  353284 pjrt_api.cc:98] GetPjrtApi was found for tpu at /home/rosinality/openmoe_venv/lib/python3.10/site-packages/libtpu/libtpu.so
I0000 00:00:1715478291.400411  353284 pjrt_api.cc:67] PJRT_Api is set for device type tpu
I0000 00:00:1715478291.400413  353284 pjrt_api.cc:72] PJRT plugin for tpu has PJRT API version 0.30. The framework PJRT API version is 0.30.
I0512 01:44:51.382994 140577131780096 gin_utils.py:85] expert/dense.MlpBlock.use_bias = False
I0512 01:44:51.383028 140577131780096 gin_utils.py:85] 
I0512 01:44:51.383062 140577131780096 gin_utils.py:85] # Parameters for models.MoeDecoderOnlyModel:
I0512 01:44:51.383095 140577131780096 gin_utils.py:85] # ==============================================================================
I0512 01:44:51.383127 140577131780096 gin_utils.py:85] models.MoeDecoderOnlyModel.aux_loss_factor = %AUX_LOSS_FACTOR
I0512 01:44:51.383160 140577131780096 gin_utils.py:85] models.MoeDecoderOnlyModel.label_smoothing = %LABEL_SMOOTHING
I0512 01:44:51.383192 140577131780096 gin_utils.py:85] models.MoeDecoderOnlyModel.loss_normalizing_factor = %LOSS_NORMALIZING_FACTOR
I0512 01:44:51.383224 140577131780096 gin_utils.py:85] models.MoeDecoderOnlyModel.module = %ARCHITECTURE
I0512 01:44:51.383257 140577131780096 gin_utils.py:85] models.MoeDecoderOnlyModel.optimizer_def = %OPTIMIZER
I0512 01:44:51.383309 140577131780096 gin_utils.py:85] models.MoeDecoderOnlyModel.router_z_loss_factor = %ROUTER_Z_LOSS_FACTOR
I0512 01:44:51.383348 140577131780096 gin_utils.py:85] models.MoeDecoderOnlyModel.vocabulary = %VOCABULARY
I0512 01:44:51.383382 140577131780096 gin_utils.py:85] models.MoeDecoderOnlyModel.z_loss = %Z_LOSS
I0512 01:44:51.383415 140577131780096 gin_utils.py:85] 
I0512 01:44:51.383447 140577131780096 gin_utils.py:85] # Parameters for moe_layers.MoeLayer:
I0512 01:44:51.383480 140577131780096 gin_utils.py:85] # ==============================================================================
I0512 01:44:51.383512 140577131780096 gin_utils.py:85] moe_layers.MoeLayer.dtype = %MOE_TRUNCATED_DTYPE
I0512 01:44:51.383545 140577131780096 gin_utils.py:85] moe_layers.MoeLayer.eval_capacity_factor = %EVAL_EXPERT_CAPACITY_FACTOR
I0512 01:44:51.383577 140577131780096 gin_utils.py:85] moe_layers.MoeLayer.expert = @expert/dense.MlpBlock()
I0512 01:44:51.383610 140577131780096 gin_utils.py:85] moe_layers.MoeLayer.max_group_size = %GROUP_SIZE
I0512 01:44:51.383642 140577131780096 gin_utils.py:85] moe_layers.MoeLayer.num_expert_partitions = %NUM_EXPERT_PARTITIONS
I0512 01:44:51.383674 140577131780096 gin_utils.py:85] moe_layers.MoeLayer.num_experts = %NUM_EXPERTS
I0512 01:44:51.383707 140577131780096 gin_utils.py:85] moe_layers.MoeLayer.num_model_partitions = \
I0512 01:44:51.383739 140577131780096 gin_utils.py:85]     @partitioning.compute_num_model_partitions()
I0512 01:44:51.383778 140577131780096 gin_utils.py:85] moe_layers.MoeLayer.train_capacity_factor = %TRAIN_EXPERT_CAPACITY_FACTOR
I0512 01:44:51.383812 140577131780096 gin_utils.py:85] 
I0512 01:44:51.383844 140577131780096 gin_utils.py:85] # Parameters for sparse_decoder/moe_layers.MoeLayer:
I0512 01:44:51.383877 140577131780096 gin_utils.py:85] # ==============================================================================
I0512 01:44:51.383909 140577131780096 gin_utils.py:85] sparse_decoder/moe_layers.MoeLayer.router = \
I0512 01:44:51.383941 140577131780096 gin_utils.py:85]     @sparse_decoder/routing.TokensChooseMaskedRouter()
I0512 01:44:51.383974 140577131780096 gin_utils.py:85] 
I0512 01:44:51.384006 140577131780096 gin_utils.py:85] # Parameters for partitioning.MoePjitPartitioner:
I0512 01:44:51.384040 140577131780096 gin_utils.py:85] # ==============================================================================
I0512 01:44:51.384073 140577131780096 gin_utils.py:85] partitioning.MoePjitPartitioner.model_parallel_submesh = %MODEL_PARALLEL_SUBMESH
I0512 01:44:51.384106 140577131780096 gin_utils.py:85] partitioning.MoePjitPartitioner.num_expert_partitions = %NUM_EXPERT_PARTITIONS
I0512 01:44:51.384139 140577131780096 gin_utils.py:85] partitioning.MoePjitPartitioner.num_partitions = %NUM_MODEL_PARTITIONS
I0512 01:44:51.384171 140577131780096 gin_utils.py:85] 
I0512 01:44:51.384204 140577131780096 gin_utils.py:85] # Parameters for moe_trainer.MoeTrainer:
I0512 01:44:51.384236 140577131780096 gin_utils.py:85] # ==============================================================================
I0512 01:44:51.384268 140577131780096 gin_utils.py:85] moe_trainer.MoeTrainer.learning_rate_fn = @utils.create_learning_rate_scheduler()
I0512 01:44:51.384301 140577131780096 gin_utils.py:85] moe_trainer.MoeTrainer.num_expert_partitions = %NUM_EXPERT_PARTITIONS
I0512 01:44:51.384334 140577131780096 gin_utils.py:85] moe_trainer.MoeTrainer.num_microbatches = 8
I0512 01:44:51.384366 140577131780096 gin_utils.py:85] 
I0512 01:44:51.384399 140577131780096 gin_utils.py:85] # Parameters for dense_attention.MultiHeadDotProductAttention:
I0512 01:44:51.384431 140577131780096 gin_utils.py:85] # ==============================================================================
I0512 01:44:51.384463 140577131780096 gin_utils.py:85] dense_attention.MultiHeadDotProductAttention.bias_init = %BIAS_INIT
I0512 01:44:51.384496 140577131780096 gin_utils.py:85] dense_attention.MultiHeadDotProductAttention.broadcast_dropout = True
I0512 01:44:51.384528 140577131780096 gin_utils.py:85] dense_attention.MultiHeadDotProductAttention.dropout_rate = %DROPOUT_RATE
I0512 01:44:51.384561 140577131780096 gin_utils.py:85] dense_attention.MultiHeadDotProductAttention.dtype = %ACTIVATION_DTYPE
I0512 01:44:51.384594 140577131780096 gin_utils.py:85] dense_attention.MultiHeadDotProductAttention.head_dim = %HEAD_DIM
I0512 01:44:51.384626 140577131780096 gin_utils.py:85] dense_attention.MultiHeadDotProductAttention.kernel_init = \
I0512 01:44:51.384658 140577131780096 gin_utils.py:85]     @attention_kernel_init/linen.initializers.variance_scaling()
I0512 01:44:51.384691 140577131780096 gin_utils.py:85] dense_attention.MultiHeadDotProductAttention.num_heads = %NUM_HEADS
I0512 01:44:51.384723 140577131780096 gin_utils.py:85] dense_attention.MultiHeadDotProductAttention.use_bias = False
I0512 01:44:51.384755 140577131780096 gin_utils.py:85] dense_attention.MultiHeadDotProductAttention.use_rotary_embedding = True
I0512 01:44:51.384794 140577131780096 gin_utils.py:85] 
I0512 01:44:51.384827 140577131780096 gin_utils.py:85] # Parameters for bias_init/linen.initializers.normal:
I0512 01:44:51.384860 140577131780096 gin_utils.py:85] # ==============================================================================
I0512 01:44:51.384892 140577131780096 gin_utils.py:85] bias_init/linen.initializers.normal.stddev = 1e-06
I0512 01:44:51.384925 140577131780096 gin_utils.py:85] 
I0512 01:44:51.384957 140577131780096 gin_utils.py:85] # Parameters for router_init/linen.initializers.normal:
I0512 01:44:51.384990 140577131780096 gin_utils.py:85] # ==============================================================================
I0512 01:44:51.385024 140577131780096 gin_utils.py:85] router_init/linen.initializers.normal.stddev = 0.02
I0512 01:44:51.385059 140577131780096 gin_utils.py:85] 
I0512 01:44:51.385092 140577131780096 gin_utils.py:85] # Parameters for token_embedder_init/linen.initializers.normal:
I0512 01:44:51.385125 140577131780096 gin_utils.py:85] # ==============================================================================
I0512 01:44:51.385158 140577131780096 gin_utils.py:85] token_embedder_init/linen.initializers.normal.stddev = 1.0
I0512 01:44:51.385191 140577131780096 gin_utils.py:85] 
I0512 01:44:51.385223 140577131780096 gin_utils.py:85] # Parameters for partitioning2.PjitPartitioner:
I0512 01:44:51.385256 140577131780096 gin_utils.py:85] # ==============================================================================
I0512 01:44:51.385288 140577131780096 gin_utils.py:85] partitioning2.PjitPartitioner.logical_axis_rules = \
I0512 01:44:51.385322 140577131780096 gin_utils.py:85]     @partitioning2.standard_logical_axis_rules()
I0512 01:44:51.385354 140577131780096 gin_utils.py:85] partitioning2.PjitPartitioner.model_parallel_submesh = None
I0512 01:44:51.385387 140577131780096 gin_utils.py:85] partitioning2.PjitPartitioner.num_partitions = 1
I0512 01:44:51.385420 140577131780096 gin_utils.py:85] 
I0512 01:44:51.385452 140577131780096 gin_utils.py:85] # Parameters for utils.RestoreCheckpointConfig:
I0512 01:44:51.385485 140577131780096 gin_utils.py:85] # ==============================================================================
I0512 01:44:51.385518 140577131780096 gin_utils.py:85] utils.RestoreCheckpointConfig.path = []
I0512 01:44:51.385551 140577131780096 gin_utils.py:85] 
I0512 01:44:51.385583 140577131780096 gin_utils.py:85] # Parameters for routing.RouterWeights:
I0512 01:44:51.385616 140577131780096 gin_utils.py:85] # ==============================================================================
I0512 01:44:51.385649 140577131780096 gin_utils.py:85] routing.RouterWeights.bias_init = %BIAS_INIT
I0512 01:44:51.385681 140577131780096 gin_utils.py:85] routing.RouterWeights.dtype = 'float32'
I0512 01:44:51.385714 140577131780096 gin_utils.py:85] routing.RouterWeights.kernel_init = @router_init/linen.initializers.normal()
I0512 01:44:51.385747 140577131780096 gin_utils.py:85] routing.RouterWeights.use_bias = False
I0512 01:44:51.385785 140577131780096 gin_utils.py:85] 
I0512 01:44:51.385819 140577131780096 gin_utils.py:85] # Parameters for utils.SaveCheckpointConfig:
I0512 01:44:51.385852 140577131780096 gin_utils.py:85] # ==============================================================================
I0512 01:44:51.385885 140577131780096 gin_utils.py:85] utils.SaveCheckpointConfig.dtype = 'float32'
I0512 01:44:51.385918 140577131780096 gin_utils.py:85] utils.SaveCheckpointConfig.keep = 20
I0512 01:44:51.385951 140577131780096 gin_utils.py:85] utils.SaveCheckpointConfig.period = 2500
I0512 01:44:51.385983 140577131780096 gin_utils.py:85] utils.SaveCheckpointConfig.save_dataset = True
I0512 01:44:51.386016 140577131780096 gin_utils.py:85] 
I0512 01:44:51.386051 140577131780096 gin_utils.py:85] # Parameters for seqio.SentencePieceVocabulary:
I0512 01:44:51.386084 140577131780096 gin_utils.py:85] # ==============================================================================
I0512 01:44:51.386117 140577131780096 gin_utils.py:85] seqio.SentencePieceVocabulary.extra_ids = 300
I0512 01:44:51.386150 140577131780096 gin_utils.py:85] seqio.SentencePieceVocabulary.sentencepiece_model_file = \
I0512 01:44:51.386183 140577131780096 gin_utils.py:85]     'gs://rosinality-tpu-bucket/sentencepiece.model'
I0512 01:44:51.386215 140577131780096 gin_utils.py:85] 
I0512 01:44:51.386248 140577131780096 gin_utils.py:85] # Parameters for moe_architecture.SparseDecoder:
I0512 01:44:51.386281 140577131780096 gin_utils.py:85] # ==============================================================================
I0512 01:44:51.386313 140577131780096 gin_utils.py:85] moe_architecture.SparseDecoder.dropout_factory = %DROPOUT_FACTORY
I0512 01:44:51.386346 140577131780096 gin_utils.py:85] moe_architecture.SparseDecoder.dtype = %ACTIVATION_DTYPE
I0512 01:44:51.386379 140577131780096 gin_utils.py:85] moe_architecture.SparseDecoder.layer_factory = @t5_architecture.DecoderLayer
I0512 01:44:51.386412 140577131780096 gin_utils.py:85] moe_architecture.SparseDecoder.layer_norm_factory = @layer_norm.T5LayerNorm
I0512 01:44:51.386445 140577131780096 gin_utils.py:85] moe_architecture.SparseDecoder.num_layers = %NUM_DECODER_LAYERS
I0512 01:44:51.386477 140577131780096 gin_utils.py:85] moe_architecture.SparseDecoder.num_sparse_layers = %NUM_DECODER_SPARSE_LAYERS
I0512 01:44:51.386510 140577131780096 gin_utils.py:85] moe_architecture.SparseDecoder.output_logits_factory = \
I0512 01:44:51.386542 140577131780096 gin_utils.py:85]     @output_logits/dense.DenseGeneral
I0512 01:44:51.386575 140577131780096 gin_utils.py:85] moe_architecture.SparseDecoder.sparse_layer_factory = \
I0512 01:44:51.386608 140577131780096 gin_utils.py:85]     @moe_architecture.SparseDecoderLayer
I0512 01:44:51.386641 140577131780096 gin_utils.py:85] moe_architecture.SparseDecoder.sparse_layout = %DECODER_SPARSE_LAYOUT
I0512 01:44:51.386674 140577131780096 gin_utils.py:85] 
I0512 01:44:51.386706 140577131780096 gin_utils.py:85] # Parameters for moe_architecture.SparseDecoderLayer:
I0512 01:44:51.386739 140577131780096 gin_utils.py:85] # ==============================================================================
I0512 01:44:51.386777 140577131780096 gin_utils.py:85] moe_architecture.SparseDecoderLayer.activation_partitioning_dims = \
I0512 01:44:51.386811 140577131780096 gin_utils.py:85]     %ACTIVATION_PARTITIONING_DIMS
I0512 01:44:51.386844 140577131780096 gin_utils.py:85] moe_architecture.SparseDecoderLayer.dropout_factory = %DROPOUT_FACTORY
I0512 01:44:51.386877 140577131780096 gin_utils.py:85] moe_architecture.SparseDecoderLayer.encoder_decoder_attention = None
I0512 01:44:51.386914 140577131780096 gin_utils.py:85] moe_architecture.SparseDecoderLayer.extra_mlp = @dense.MlpBlock()
I0512 01:44:51.386948 140577131780096 gin_utils.py:85] moe_architecture.SparseDecoderLayer.layer_norm_factory = @layer_norm.T5LayerNorm
I0512 01:44:51.386981 140577131780096 gin_utils.py:85] moe_architecture.SparseDecoderLayer.mlp = @sparse_decoder/moe_layers.MoeLayer()
I0512 01:44:51.387014 140577131780096 gin_utils.py:85] moe_architecture.SparseDecoderLayer.scanned = False
I0512 01:44:51.387049 140577131780096 gin_utils.py:85] moe_architecture.SparseDecoderLayer.self_attention = \
I0512 01:44:51.387083 140577131780096 gin_utils.py:85]     @dense_attention.MultiHeadDotProductAttention()
I0512 01:44:51.387116 140577131780096 gin_utils.py:85] 
I0512 01:44:51.387149 140577131780096 gin_utils.py:85] # Parameters for layer_norm.T5LayerNorm:
I0512 01:44:51.387183 140577131780096 gin_utils.py:85] # ==============================================================================
I0512 01:44:51.387216 140577131780096 gin_utils.py:85] layer_norm.T5LayerNorm.dtype = %ACTIVATION_DTYPE
I0512 01:44:51.387249 140577131780096 gin_utils.py:85] 
I0512 01:44:51.387281 140577131780096 gin_utils.py:85] # Parameters for routing.TokensChooseMaskedRouter:
I0512 01:44:51.387340 140577131780096 gin_utils.py:85] # ==============================================================================
I0512 01:44:51.387375 140577131780096 gin_utils.py:85] routing.TokensChooseMaskedRouter.dtype = 'float32'
I0512 01:44:51.387407 140577131780096 gin_utils.py:85] routing.TokensChooseMaskedRouter.ignore_padding_tokens = False
I0512 01:44:51.387440 140577131780096 gin_utils.py:85] routing.TokensChooseMaskedRouter.jitter_noise = %JITTER_NOISE
I0512 01:44:51.387473 140577131780096 gin_utils.py:85] routing.TokensChooseMaskedRouter.num_selected_experts = %NUM_SELECTED_EXPERTS
I0512 01:44:51.387506 140577131780096 gin_utils.py:85] routing.TokensChooseMaskedRouter.router_weights = @routing.RouterWeights()
I0512 01:44:51.387538 140577131780096 gin_utils.py:85] 
I0512 01:44:51.387571 140577131780096 gin_utils.py:85] # Parameters for sparse_decoder/routing.TokensChooseMaskedRouter:
I0512 01:44:51.387603 140577131780096 gin_utils.py:85] # ==============================================================================
I0512 01:44:51.387636 140577131780096 gin_utils.py:85] sparse_decoder/routing.TokensChooseMaskedRouter.batch_prioritized_routing = False
I0512 01:44:51.387669 140577131780096 gin_utils.py:85] 
I0512 01:44:51.387701 140577131780096 gin_utils.py:85] # Parameters for sparse_encoder/routing.TokensChooseMaskedRouter:
I0512 01:44:51.387733 140577131780096 gin_utils.py:85] # ==============================================================================
I0512 01:44:51.387766 140577131780096 gin_utils.py:85] sparse_encoder/routing.TokensChooseMaskedRouter.batch_prioritized_routing = False
I0512 01:44:51.387805 140577131780096 gin_utils.py:85] 
I0512 01:44:51.387838 140577131780096 gin_utils.py:85] # Parameters for train_script.train:
I0512 01:44:51.387871 140577131780096 gin_utils.py:85] # ==============================================================================
I0512 01:44:51.387905 140577131780096 gin_utils.py:85] train_script.train.checkpoint_cfg = @utils.CheckpointConfig()
I0512 01:44:51.387938 140577131780096 gin_utils.py:85] train_script.train.eval_period = 2500
I0512 01:44:51.387971 140577131780096 gin_utils.py:85] train_script.train.eval_steps = 20
I0512 01:44:51.573809 139676058949632 resource_reader.py:50] system_path_file_exists:t5x/contrib/moe/configs/runs/pretrain.gin
I0512 01:44:51.471066 140316961150976 gin_utils.py:85] dense.MlpBlock.kernel_init = @mlp_kernel_init/linen.initializers.variance_scaling()
I0512 01:44:51.471098 140316961150976 gin_utils.py:85] dense.MlpBlock.output_axis_name = 'mlp_embed'
I0512 01:44:51.471132 140316961150976 gin_utils.py:85] dense.MlpBlock.use_bias = False
I0512 01:44:51.471164 140316961150976 gin_utils.py:85] 
I0512 01:44:51.471196 140316961150976 gin_utils.py:85] # Parameters for expert/dense.MlpBlock:
I0512 01:44:51.471228 140316961150976 gin_utils.py:85] # ==============================================================================
I0512 01:44:51.471260 140316961150976 gin_utils.py:85] expert/dense.MlpBlock.activation_partitioning_dims = 1
I0512 01:44:51.471292 140316961150976 gin_utils.py:85] expert/dense.MlpBlock.activations = ('swiglu', 'linear')
I0512 01:44:51.471324 140316961150976 gin_utils.py:85] expert/dense.MlpBlock.bias_init = %BIAS_INIT
I0512 01:44:51.471356 140316961150976 gin_utils.py:85] expert/dense.MlpBlock.data_sharding_constraints = ('expert_replicas', 'mlp')
I0512 01:44:51.471388 140316961150976 gin_utils.py:85] expert/dense.MlpBlock.dtype = %MOE_TRUNCATED_DTYPE
I0512 01:44:51.471419 140316961150976 gin_utils.py:85] expert/dense.MlpBlock.final_dropout_rate = 0.0
I0512 01:44:51.471451 140316961150976 gin_utils.py:85] expert/dense.MlpBlock.input_axis_name = 'mlp_embed'
I0512 01:44:51.471482 140316961150976 gin_utils.py:85] expert/dense.MlpBlock.intermediate_axis_name = 'expert_mlp'
I0512 01:44:51.471513 140316961150976 gin_utils.py:85] expert/dense.MlpBlock.intermediate_dim = %EXPERT_MLP_DIM
I0512 01:44:51.471545 140316961150976 gin_utils.py:85] expert/dense.MlpBlock.intermediate_dropout_rate = %EXPERT_DROPOUT_RATE
I0512 01:44:51.471576 140316961150976 gin_utils.py:85] expert/dense.MlpBlock.kernel_init = \
I0512 01:44:51.471608 140316961150976 gin_utils.py:85]     @expert_kernel_init/linen.initializers.variance_scaling()
I0512 01:44:51.471639 140316961150976 gin_utils.py:85] expert/dense.MlpBlock.output_axis_name = 'mlp_embed'
I0512 01:44:51.421685 140172769531904 gin_utils.py:85] expert/dense.MlpBlock.use_bias = False
I0512 01:44:51.421716 140172769531904 gin_utils.py:85] 
I0512 01:44:51.421747 140172769531904 gin_utils.py:85] # Parameters for models.MoeDecoderOnlyModel:
I0512 01:44:51.421778 140172769531904 gin_utils.py:85] # ==============================================================================
I0512 01:44:51.421809 140172769531904 gin_utils.py:85] models.MoeDecoderOnlyModel.aux_loss_factor = %AUX_LOSS_FACTOR
I0512 01:44:51.421840 140172769531904 gin_utils.py:85] models.MoeDecoderOnlyModel.label_smoothing = %LABEL_SMOOTHING
I0512 01:44:51.421871 140172769531904 gin_utils.py:85] models.MoeDecoderOnlyModel.loss_normalizing_factor = %LOSS_NORMALIZING_FACTOR
I0512 01:44:51.421902 140172769531904 gin_utils.py:85] models.MoeDecoderOnlyModel.module = %ARCHITECTURE
I0512 01:44:51.421932 140172769531904 gin_utils.py:85] models.MoeDecoderOnlyModel.optimizer_def = %OPTIMIZER
I0512 01:44:51.421964 140172769531904 gin_utils.py:85] models.MoeDecoderOnlyModel.router_z_loss_factor = %ROUTER_Z_LOSS_FACTOR
I0512 01:44:51.421995 140172769531904 gin_utils.py:85] models.MoeDecoderOnlyModel.vocabulary = %VOCABULARY
I0512 01:44:51.422026 140172769531904 gin_utils.py:85] models.MoeDecoderOnlyModel.z_loss = %Z_LOSS
I0512 01:44:51.422062 140172769531904 gin_utils.py:85] 
I0512 01:44:51.422094 140172769531904 gin_utils.py:85] # Parameters for moe_layers.MoeLayer:
I0512 01:44:51.422125 140172769531904 gin_utils.py:85] # ==============================================================================
I0512 01:44:51.422156 140172769531904 gin_utils.py:85] moe_layers.MoeLayer.dtype = %MOE_TRUNCATED_DTYPE
I0512 01:44:51.422187 140172769531904 gin_utils.py:85] moe_layers.MoeLayer.eval_capacity_factor = %EVAL_EXPERT_CAPACITY_FACTOR
I0512 01:44:51.422218 140172769531904 gin_utils.py:85] moe_layers.MoeLayer.expert = @expert/dense.MlpBlock()
I0512 01:44:51.422249 140172769531904 gin_utils.py:85] moe_layers.MoeLayer.max_group_size = %GROUP_SIZE
I0512 01:44:51.422280 140172769531904 gin_utils.py:85] moe_layers.MoeLayer.num_expert_partitions = %NUM_EXPERT_PARTITIONS
I0512 01:44:51.422313 140172769531904 gin_utils.py:85] moe_layers.MoeLayer.num_experts = %NUM_EXPERTS
I0512 01:44:51.422345 140172769531904 gin_utils.py:85] moe_layers.MoeLayer.num_model_partitions = \
I0512 01:44:51.422376 140172769531904 gin_utils.py:85]     @moe_partitioning.compute_num_model_partitions()
I0512 01:44:51.422407 140172769531904 gin_utils.py:85] moe_layers.MoeLayer.train_capacity_factor = %TRAIN_EXPERT_CAPACITY_FACTOR
I0512 01:44:51.422438 140172769531904 gin_utils.py:85] 
I0512 01:44:51.422469 140172769531904 gin_utils.py:85] # Parameters for sparse_decoder/moe_layers.MoeLayer:
I0512 01:44:51.422500 140172769531904 gin_utils.py:85] # ==============================================================================
I0512 01:44:51.422531 140172769531904 gin_utils.py:85] sparse_decoder/moe_layers.MoeLayer.router = \
I0512 01:44:51.422562 140172769531904 gin_utils.py:85]     @sparse_decoder/routing.TokensChooseMaskedRouter()
I0512 01:44:51.422593 140172769531904 gin_utils.py:85] 
I0512 01:44:51.422625 140172769531904 gin_utils.py:85] # Parameters for moe_partitioning.MoePjitPartitioner:
I0512 01:44:51.422656 140172769531904 gin_utils.py:85] # ==============================================================================
I0512 01:44:51.422687 140172769531904 gin_utils.py:85] moe_partitioning.MoePjitPartitioner.model_parallel_submesh = \
I0512 01:44:51.422718 140172769531904 gin_utils.py:85]     %MODEL_PARALLEL_SUBMESH
I0512 01:44:51.422748 140172769531904 gin_utils.py:85] moe_partitioning.MoePjitPartitioner.num_expert_partitions = %NUM_EXPERT_PARTITIONS
I0512 01:44:51.422779 140172769531904 gin_utils.py:85] moe_partitioning.MoePjitPartitioner.num_partitions = %NUM_MODEL_PARTITIONS
I0512 01:44:51.422810 140172769531904 gin_utils.py:85] 
I0512 01:44:51.422841 140172769531904 gin_utils.py:85] # Parameters for moe_trainer.MoeTrainer:
I0512 01:44:51.422872 140172769531904 gin_utils.py:85] # ==============================================================================
I0512 01:44:51.422903 140172769531904 gin_utils.py:85] moe_trainer.MoeTrainer.learning_rate_fn = @utils.create_learning_rate_scheduler()
I0512 01:44:51.422935 140172769531904 gin_utils.py:85] moe_trainer.MoeTrainer.num_expert_partitions = %NUM_EXPERT_PARTITIONS
I0512 01:44:51.422965 140172769531904 gin_utils.py:85] moe_trainer.MoeTrainer.num_microbatches = 8
I0512 01:44:51.422996 140172769531904 gin_utils.py:85] 
I0512 01:44:51.423027 140172769531904 gin_utils.py:85] # Parameters for dense_attention.MultiHeadDotProductAttention:
I0512 01:44:51.423063 140172769531904 gin_utils.py:85] # ==============================================================================
I0512 01:44:51.423095 140172769531904 gin_utils.py:85] dense_attention.MultiHeadDotProductAttention.bias_init = %BIAS_INIT
I0512 01:44:51.423126 140172769531904 gin_utils.py:85] dense_attention.MultiHeadDotProductAttention.broadcast_dropout = True
I0512 01:44:51.423157 140172769531904 gin_utils.py:85] dense_attention.MultiHeadDotProductAttention.dropout_rate = %DROPOUT_RATE
I0512 01:44:51.423188 140172769531904 gin_utils.py:85] dense_attention.MultiHeadDotProductAttention.dtype = %ACTIVATION_DTYPE
I0512 01:44:51.423219 140172769531904 gin_utils.py:85] dense_attention.MultiHeadDotProductAttention.head_dim = %HEAD_DIM
I0512 01:44:51.423250 140172769531904 gin_utils.py:85] dense_attention.MultiHeadDotProductAttention.kernel_init = \
I0512 01:44:51.423281 140172769531904 gin_utils.py:85]     @attention_kernel_init/linen.initializers.variance_scaling()
I0512 01:44:51.423313 140172769531904 gin_utils.py:85] dense_attention.MultiHeadDotProductAttention.num_heads = %NUM_HEADS
I0512 01:44:51.423345 140172769531904 gin_utils.py:85] dense_attention.MultiHeadDotProductAttention.use_bias = False
I0512 01:44:51.423376 140172769531904 gin_utils.py:85] dense_attention.MultiHeadDotProductAttention.use_rotary_embedding = True
I0512 01:44:51.423407 140172769531904 gin_utils.py:85] 
I0512 01:44:51.423438 140172769531904 gin_utils.py:85] # Parameters for bias_init/linen.initializers.normal:
I0512 01:44:51.423469 140172769531904 gin_utils.py:85] # ==============================================================================
I0512 01:44:51.423500 140172769531904 gin_utils.py:85] bias_init/linen.initializers.normal.stddev = 1e-06
I0512 01:44:51.423531 140172769531904 gin_utils.py:85] 
I0512 01:44:51.423564 140172769531904 gin_utils.py:85] # Parameters for router_init/linen.initializers.normal:
I0512 01:44:51.423595 140172769531904 gin_utils.py:85] # ==============================================================================
I0512 01:44:51.423626 140172769531904 gin_utils.py:85] router_init/linen.initializers.normal.stddev = 0.02
I0512 01:44:51.423657 140172769531904 gin_utils.py:85] 
I0512 01:44:51.423687 140172769531904 gin_utils.py:85] # Parameters for token_embedder_init/linen.initializers.normal:
I0512 01:44:51.423718 140172769531904 gin_utils.py:85] # ==============================================================================
I0512 01:44:51.423749 140172769531904 gin_utils.py:85] token_embedder_init/linen.initializers.normal.stddev = 1.0
I0512 01:44:51.423780 140172769531904 gin_utils.py:85] 
I0512 01:44:51.423811 140172769531904 gin_utils.py:85] # Parameters for partitioning.PjitPartitioner:
I0512 01:44:51.423841 140172769531904 gin_utils.py:85] # ==============================================================================
I0512 01:44:51.423872 140172769531904 gin_utils.py:85] partitioning.PjitPartitioner.logical_axis_rules = \
I0512 01:44:51.423903 140172769531904 gin_utils.py:85]     @partitioning.standard_logical_axis_rules()
I0512 01:44:51.423934 140172769531904 gin_utils.py:85] partitioning.PjitPartitioner.model_parallel_submesh = None
I0512 01:44:51.423964 140172769531904 gin_utils.py:85] partitioning.PjitPartitioner.num_partitions = 1
I0512 01:44:51.423995 140172769531904 gin_utils.py:85] 
I0512 01:44:51.424026 140172769531904 gin_utils.py:85] # Parameters for utils.RestoreCheckpointConfig:
I0512 01:44:51.424062 140172769531904 gin_utils.py:85] # ==============================================================================
I0512 01:44:51.424093 140172769531904 gin_utils.py:85] utils.RestoreCheckpointConfig.path = []
I0512 01:44:51.424124 140172769531904 gin_utils.py:85] 
I0512 01:44:51.424155 140172769531904 gin_utils.py:85] # Parameters for routing.RouterWeights:
I0512 01:44:51.424185 140172769531904 gin_utils.py:85] # ==============================================================================
I0512 01:44:51.424216 140172769531904 gin_utils.py:85] routing.RouterWeights.bias_init = %BIAS_INIT
I0512 01:44:51.424247 140172769531904 gin_utils.py:85] routing.RouterWeights.dtype = 'float32'
I0512 01:44:51.424278 140172769531904 gin_utils.py:85] routing.RouterWeights.kernel_init = @router_init/linen.initializers.normal()
I0512 01:44:51.424310 140172769531904 gin_utils.py:85] routing.RouterWeights.use_bias = False
I0512 01:44:51.424342 140172769531904 gin_utils.py:85] 
I0512 01:44:51.424373 140172769531904 gin_utils.py:85] # Parameters for utils.SaveCheckpointConfig:
I0512 01:44:51.424421 140172769531904 gin_utils.py:85] # ==============================================================================
I0512 01:44:51.424458 140172769531904 gin_utils.py:85] utils.SaveCheckpointConfig.dtype = 'float32'
I0512 01:44:51.424490 140172769531904 gin_utils.py:85] utils.SaveCheckpointConfig.keep = 20
I0512 01:44:51.424521 140172769531904 gin_utils.py:85] utils.SaveCheckpointConfig.period = 2500
I0512 01:44:51.424552 140172769531904 gin_utils.py:85] utils.SaveCheckpointConfig.save_dataset = True
I0512 01:44:51.424583 140172769531904 gin_utils.py:85] 
I0512 01:44:51.424614 140172769531904 gin_utils.py:85] # Parameters for seqio.SentencePieceVocabulary:
I0512 01:44:51.424645 140172769531904 gin_utils.py:85] # ==============================================================================
I0512 01:44:51.424675 140172769531904 gin_utils.py:85] seqio.SentencePieceVocabulary.extra_ids = 300
I0512 01:44:51.424706 140172769531904 gin_utils.py:85] seqio.SentencePieceVocabulary.sentencepiece_model_file = \
I0512 01:44:51.424737 140172769531904 gin_utils.py:85]     'gs://rosinality-tpu-bucket/sentencepiece.model'
I0512 01:44:51.424768 140172769531904 gin_utils.py:85] 
I0512 01:44:51.424799 140172769531904 gin_utils.py:85] # Parameters for moe_architecture.SparseDecoder:
I0512 01:44:51.424829 140172769531904 gin_utils.py:85] # ==============================================================================
I0512 01:44:51.424860 140172769531904 gin_utils.py:85] moe_architecture.SparseDecoder.dropout_factory = %DROPOUT_FACTORY
I0512 01:44:51.424891 140172769531904 gin_utils.py:85] moe_architecture.SparseDecoder.dtype = %ACTIVATION_DTYPE
I0512 01:44:51.424922 140172769531904 gin_utils.py:85] moe_architecture.SparseDecoder.layer_factory = @t5_architecture.DecoderLayer
I0512 01:44:51.424952 140172769531904 gin_utils.py:85] moe_architecture.SparseDecoder.layer_norm_factory = @layer_norm.T5LayerNorm
I0512 01:44:51.424983 140172769531904 gin_utils.py:85] moe_architecture.SparseDecoder.num_layers = %NUM_DECODER_LAYERS
I0512 01:44:51.425013 140172769531904 gin_utils.py:85] moe_architecture.SparseDecoder.num_sparse_layers = %NUM_DECODER_SPARSE_LAYERS
I0512 01:44:51.425044 140172769531904 gin_utils.py:85] moe_architecture.SparseDecoder.output_logits_factory = \
I0512 01:44:51.425081 140172769531904 gin_utils.py:85]     @output_logits/dense.DenseGeneral
I0512 01:44:51.425112 140172769531904 gin_utils.py:85] moe_architecture.SparseDecoder.sparse_layer_factory = \
I0512 01:44:51.425143 140172769531904 gin_utils.py:85]     @moe_architecture.SparseDecoderLayer
I0512 01:44:51.425173 140172769531904 gin_utils.py:85] moe_architecture.SparseDecoder.sparse_layout = %DECODER_SPARSE_LAYOUT
I0512 01:44:51.425204 140172769531904 gin_utils.py:85] 
I0512 01:44:51.425234 140172769531904 gin_utils.py:85] # Parameters for moe_architecture.SparseDecoderLayer:
I0512 01:44:51.425266 140172769531904 gin_utils.py:85] # ==============================================================================
I0512 01:44:51.425296 140172769531904 gin_utils.py:85] moe_architecture.SparseDecoderLayer.activation_partitioning_dims = \
I0512 01:44:51.425330 140172769531904 gin_utils.py:85]     %ACTIVATION_PARTITIONING_DIMS
I0512 01:44:51.425361 140172769531904 gin_utils.py:85] moe_architecture.SparseDecoderLayer.dropout_factory = %DROPOUT_FACTORY
I0512 01:44:51.425391 140172769531904 gin_utils.py:85] moe_architecture.SparseDecoderLayer.encoder_decoder_attention = None
I0512 01:44:51.425423 140172769531904 gin_utils.py:85] moe_architecture.SparseDecoderLayer.extra_mlp = @dense.MlpBlock()
I0512 01:44:51.425454 140172769531904 gin_utils.py:85] moe_architecture.SparseDecoderLayer.layer_norm_factory = @layer_norm.T5LayerNorm
I0512 01:44:51.425485 140172769531904 gin_utils.py:85] moe_architecture.SparseDecoderLayer.mlp = @sparse_decoder/moe_layers.MoeLayer()
I0512 01:44:51.425516 140172769531904 gin_utils.py:85] moe_architecture.SparseDecoderLayer.scanned = False
I0512 01:44:51.425547 140172769531904 gin_utils.py:85] moe_architecture.SparseDecoderLayer.self_attention = \
I0512 01:44:51.425578 140172769531904 gin_utils.py:85]     @dense_attention.MultiHeadDotProductAttention()
I0512 01:44:51.425609 140172769531904 gin_utils.py:85] 
I0512 01:44:51.425641 140172769531904 gin_utils.py:85] # Parameters for layer_norm.T5LayerNorm:
I0512 01:44:51.425673 140172769531904 gin_utils.py:85] # ==============================================================================
I0512 01:44:51.425704 140172769531904 gin_utils.py:85] layer_norm.T5LayerNorm.dtype = %ACTIVATION_DTYPE
I0512 01:44:51.425735 140172769531904 gin_utils.py:85] 
I0512 01:44:51.425766 140172769531904 gin_utils.py:85] # Parameters for routing.TokensChooseMaskedRouter:
I0512 01:44:51.425797 140172769531904 gin_utils.py:85] # ==============================================================================
I0512 01:44:51.425829 140172769531904 gin_utils.py:85] routing.TokensChooseMaskedRouter.dtype = 'float32'
I0512 01:44:51.425860 140172769531904 gin_utils.py:85] routing.TokensChooseMaskedRouter.ignore_padding_tokens = False
I0512 01:44:51.425891 140172769531904 gin_utils.py:85] routing.TokensChooseMaskedRouter.jitter_noise = %JITTER_NOISE
I0512 01:44:51.425922 140172769531904 gin_utils.py:85] routing.TokensChooseMaskedRouter.num_selected_experts = %NUM_SELECTED_EXPERTS
I0512 01:44:51.425952 140172769531904 gin_utils.py:85] routing.TokensChooseMaskedRouter.router_weights = @routing.RouterWeights()
I0512 01:44:51.425983 140172769531904 gin_utils.py:85] 
I0512 01:44:51.426014 140172769531904 gin_utils.py:85] # Parameters for sparse_decoder/routing.TokensChooseMaskedRouter:
I0512 01:44:51.426053 140172769531904 gin_utils.py:85] # ==============================================================================
I0512 01:44:51.426086 140172769531904 gin_utils.py:85] sparse_decoder/routing.TokensChooseMaskedRouter.batch_prioritized_routing = False
I0512 01:44:51.426117 140172769531904 gin_utils.py:85] 
I0512 01:44:51.426148 140172769531904 gin_utils.py:85] # Parameters for sparse_encoder/routing.TokensChooseMaskedRouter:
I0512 01:44:51.426180 140172769531904 gin_utils.py:85] # ==============================================================================
I0512 01:44:51.426211 140172769531904 gin_utils.py:85] sparse_encoder/routing.TokensChooseMaskedRouter.batch_prioritized_routing = False
I0512 01:44:51.426242 140172769531904 gin_utils.py:85] 
I0512 01:44:51.426274 140172769531904 gin_utils.py:85] # Parameters for train_script.train:
I0512 01:44:51.426307 140172769531904 gin_utils.py:85] # ==============================================================================
I0512 01:44:51.426340 140172769531904 gin_utils.py:85] train_script.train.checkpoint_cfg = @utils.CheckpointConfig()
I0512 01:44:51.426371 140172769531904 gin_utils.py:85] train_script.train.eval_period = 2500
I0512 01:44:51.416314 140188559153152 gin_utils.py:85] expert/dense.MlpBlock.use_bias = False
I0512 01:44:51.416347 140188559153152 gin_utils.py:85] 
I0512 01:44:51.416380 140188559153152 gin_utils.py:85] # Parameters for models.MoeDecoderOnlyModel:
I0512 01:44:51.416412 140188559153152 gin_utils.py:85] # ==============================================================================
I0512 01:44:51.416445 140188559153152 gin_utils.py:85] models.MoeDecoderOnlyModel.aux_loss_factor = %AUX_LOSS_FACTOR
I0512 01:44:51.416478 140188559153152 gin_utils.py:85] models.MoeDecoderOnlyModel.label_smoothing = %LABEL_SMOOTHING
I0512 01:44:51.416511 140188559153152 gin_utils.py:85] models.MoeDecoderOnlyModel.loss_normalizing_factor = %LOSS_NORMALIZING_FACTOR
I0512 01:44:51.416544 140188559153152 gin_utils.py:85] models.MoeDecoderOnlyModel.module = %ARCHITECTURE
I0512 01:44:51.416577 140188559153152 gin_utils.py:85] models.MoeDecoderOnlyModel.optimizer_def = %OPTIMIZER
I0512 01:44:51.416610 140188559153152 gin_utils.py:85] models.MoeDecoderOnlyModel.router_z_loss_factor = %ROUTER_Z_LOSS_FACTOR
I0512 01:44:51.416643 140188559153152 gin_utils.py:85] models.MoeDecoderOnlyModel.vocabulary = %VOCABULARY
I0512 01:44:51.416676 140188559153152 gin_utils.py:85] models.MoeDecoderOnlyModel.z_loss = %Z_LOSS
I0512 01:44:51.416709 140188559153152 gin_utils.py:85] 
I0512 01:44:51.416742 140188559153152 gin_utils.py:85] # Parameters for moe_layers.MoeLayer:
I0512 01:44:51.416775 140188559153152 gin_utils.py:85] # ==============================================================================
I0512 01:44:51.416808 140188559153152 gin_utils.py:85] moe_layers.MoeLayer.dtype = %MOE_TRUNCATED_DTYPE
I0512 01:44:51.416852 140188559153152 gin_utils.py:85] moe_layers.MoeLayer.eval_capacity_factor = %EVAL_EXPERT_CAPACITY_FACTOR
I0512 01:44:51.416886 140188559153152 gin_utils.py:85] moe_layers.MoeLayer.expert = @expert/dense.MlpBlock()
I0512 01:44:51.416918 140188559153152 gin_utils.py:85] moe_layers.MoeLayer.max_group_size = %GROUP_SIZE
I0512 01:44:51.416952 140188559153152 gin_utils.py:85] moe_layers.MoeLayer.num_expert_partitions = %NUM_EXPERT_PARTITIONS
I0512 01:44:51.416984 140188559153152 gin_utils.py:85] moe_layers.MoeLayer.num_experts = %NUM_EXPERTS
I0512 01:44:51.417017 140188559153152 gin_utils.py:85] moe_layers.MoeLayer.num_model_partitions = \
I0512 01:44:51.417057 140188559153152 gin_utils.py:85]     @moe_partitioning.compute_num_model_partitions()
I0512 01:44:51.417089 140188559153152 gin_utils.py:85] moe_layers.MoeLayer.train_capacity_factor = %TRAIN_EXPERT_CAPACITY_FACTOR
I0512 01:44:51.417122 140188559153152 gin_utils.py:85] 
I0512 01:44:51.417155 140188559153152 gin_utils.py:85] # Parameters for sparse_decoder/moe_layers.MoeLayer:
I0512 01:44:51.417188 140188559153152 gin_utils.py:85] # ==============================================================================
I0512 01:44:51.417221 140188559153152 gin_utils.py:85] sparse_decoder/moe_layers.MoeLayer.router = \
I0512 01:44:51.417253 140188559153152 gin_utils.py:85]     @sparse_decoder/routing.TokensChooseMaskedRouter()
I0512 01:44:51.417288 140188559153152 gin_utils.py:85] 
I0512 01:44:51.417322 140188559153152 gin_utils.py:85] # Parameters for moe_partitioning.MoePjitPartitioner:
I0512 01:44:51.417355 140188559153152 gin_utils.py:85] # ==============================================================================
I0512 01:44:51.417388 140188559153152 gin_utils.py:85] moe_partitioning.MoePjitPartitioner.model_parallel_submesh = \
I0512 01:44:51.417421 140188559153152 gin_utils.py:85]     %MODEL_PARALLEL_SUBMESH
I0512 01:44:51.417454 140188559153152 gin_utils.py:85] moe_partitioning.MoePjitPartitioner.num_expert_partitions = %NUM_EXPERT_PARTITIONS
I0512 01:44:51.417486 140188559153152 gin_utils.py:85] moe_partitioning.MoePjitPartitioner.num_partitions = %NUM_MODEL_PARTITIONS
I0512 01:44:51.417519 140188559153152 gin_utils.py:85] 
I0512 01:44:51.417552 140188559153152 gin_utils.py:85] # Parameters for moe_trainer.MoeTrainer:
I0512 01:44:51.417584 140188559153152 gin_utils.py:85] # ==============================================================================
I0512 01:44:51.417617 140188559153152 gin_utils.py:85] moe_trainer.MoeTrainer.learning_rate_fn = @utils.create_learning_rate_scheduler()
I0512 01:44:51.417649 140188559153152 gin_utils.py:85] moe_trainer.MoeTrainer.num_expert_partitions = %NUM_EXPERT_PARTITIONS
I0512 01:44:51.417682 140188559153152 gin_utils.py:85] moe_trainer.MoeTrainer.num_microbatches = 8
I0512 01:44:51.417715 140188559153152 gin_utils.py:85] 
I0512 01:44:51.417747 140188559153152 gin_utils.py:85] # Parameters for dense_attention.MultiHeadDotProductAttention:
I0512 01:44:51.417780 140188559153152 gin_utils.py:85] # ==============================================================================
I0512 01:44:51.417813 140188559153152 gin_utils.py:85] dense_attention.MultiHeadDotProductAttention.bias_init = %BIAS_INIT
I0512 01:44:51.417845 140188559153152 gin_utils.py:85] dense_attention.MultiHeadDotProductAttention.broadcast_dropout = True
I0512 01:44:51.417879 140188559153152 gin_utils.py:85] dense_attention.MultiHeadDotProductAttention.dropout_rate = %DROPOUT_RATE
I0512 01:44:51.417912 140188559153152 gin_utils.py:85] dense_attention.MultiHeadDotProductAttention.dtype = %ACTIVATION_DTYPE
I0512 01:44:51.417945 140188559153152 gin_utils.py:85] dense_attention.MultiHeadDotProductAttention.head_dim = %HEAD_DIM
I0512 01:44:51.417977 140188559153152 gin_utils.py:85] dense_attention.MultiHeadDotProductAttention.kernel_init = \
I0512 01:44:51.418010 140188559153152 gin_utils.py:85]     @attention_kernel_init/linen.initializers.variance_scaling()
I0512 01:44:51.418051 140188559153152 gin_utils.py:85] dense_attention.MultiHeadDotProductAttention.num_heads = %NUM_HEADS
I0512 01:44:51.418085 140188559153152 gin_utils.py:85] dense_attention.MultiHeadDotProductAttention.use_bias = False
I0512 01:44:51.418118 140188559153152 gin_utils.py:85] dense_attention.MultiHeadDotProductAttention.use_rotary_embedding = True
I0512 01:44:51.418150 140188559153152 gin_utils.py:85] 
I0512 01:44:51.418183 140188559153152 gin_utils.py:85] # Parameters for bias_init/linen.initializers.normal:
I0512 01:44:51.418216 140188559153152 gin_utils.py:85] # ==============================================================================
I0512 01:44:51.418248 140188559153152 gin_utils.py:85] bias_init/linen.initializers.normal.stddev = 1e-06
I0512 01:44:51.418306 140188559153152 gin_utils.py:85] 
I0512 01:44:51.418341 140188559153152 gin_utils.py:85] # Parameters for router_init/linen.initializers.normal:
I0512 01:44:51.418375 140188559153152 gin_utils.py:85] # ==============================================================================
I0512 01:44:51.418408 140188559153152 gin_utils.py:85] router_init/linen.initializers.normal.stddev = 0.02
I0512 01:44:51.418441 140188559153152 gin_utils.py:85] 
I0512 01:44:51.418473 140188559153152 gin_utils.py:85] # Parameters for token_embedder_init/linen.initializers.normal:
I0512 01:44:51.418505 140188559153152 gin_utils.py:85] # ==============================================================================
I0512 01:44:51.418538 140188559153152 gin_utils.py:85] token_embedder_init/linen.initializers.normal.stddev = 1.0
I0512 01:44:51.418570 140188559153152 gin_utils.py:85] 
I0512 01:44:51.418603 140188559153152 gin_utils.py:85] # Parameters for partitioning.PjitPartitioner:
I0512 01:44:51.418635 140188559153152 gin_utils.py:85] # ==============================================================================
I0512 01:44:51.418668 140188559153152 gin_utils.py:85] partitioning.PjitPartitioner.logical_axis_rules = \
I0512 01:44:51.418700 140188559153152 gin_utils.py:85]     @partitioning.standard_logical_axis_rules()
I0512 01:44:51.418732 140188559153152 gin_utils.py:85] partitioning.PjitPartitioner.model_parallel_submesh = None
I0512 01:44:51.418764 140188559153152 gin_utils.py:85] partitioning.PjitPartitioner.num_partitions = 1
I0512 01:44:51.418796 140188559153152 gin_utils.py:85] 
I0512 01:44:51.418828 140188559153152 gin_utils.py:85] # Parameters for utils.RestoreCheckpointConfig:
I0512 01:44:51.418861 140188559153152 gin_utils.py:85] # ==============================================================================
I0512 01:44:51.418893 140188559153152 gin_utils.py:85] utils.RestoreCheckpointConfig.path = []
I0512 01:44:51.418925 140188559153152 gin_utils.py:85] 
I0512 01:44:51.418958 140188559153152 gin_utils.py:85] # Parameters for routing.RouterWeights:
I0512 01:44:51.418990 140188559153152 gin_utils.py:85] # ==============================================================================
I0512 01:44:51.419022 140188559153152 gin_utils.py:85] routing.RouterWeights.bias_init = %BIAS_INIT
I0512 01:44:51.419063 140188559153152 gin_utils.py:85] routing.RouterWeights.dtype = 'float32'
I0512 01:44:51.419095 140188559153152 gin_utils.py:85] routing.RouterWeights.kernel_init = @router_init/linen.initializers.normal()
I0512 01:44:51.419127 140188559153152 gin_utils.py:85] routing.RouterWeights.use_bias = False
I0512 01:44:51.419160 140188559153152 gin_utils.py:85] 
I0512 01:44:51.419192 140188559153152 gin_utils.py:85] # Parameters for utils.SaveCheckpointConfig:
I0512 01:44:51.419224 140188559153152 gin_utils.py:85] # ==============================================================================
I0512 01:44:51.419256 140188559153152 gin_utils.py:85] utils.SaveCheckpointConfig.dtype = 'float32'
I0512 01:44:51.419290 140188559153152 gin_utils.py:85] utils.SaveCheckpointConfig.keep = 20
I0512 01:44:51.419323 140188559153152 gin_utils.py:85] utils.SaveCheckpointConfig.period = 2500
I0512 01:44:51.419356 140188559153152 gin_utils.py:85] utils.SaveCheckpointConfig.save_dataset = True
I0512 01:44:51.419388 140188559153152 gin_utils.py:85] 
I0512 01:44:51.419420 140188559153152 gin_utils.py:85] # Parameters for seqio.SentencePieceVocabulary:
I0512 01:44:51.419453 140188559153152 gin_utils.py:85] # ==============================================================================
I0512 01:44:51.419486 140188559153152 gin_utils.py:85] seqio.SentencePieceVocabulary.extra_ids = 300
I0512 01:44:51.419518 140188559153152 gin_utils.py:85] seqio.SentencePieceVocabulary.sentencepiece_model_file = \
I0512 01:44:51.419550 140188559153152 gin_utils.py:85]     'gs://rosinality-tpu-bucket/sentencepiece.model'
I0512 01:44:51.419583 140188559153152 gin_utils.py:85] 
I0512 01:44:51.419615 140188559153152 gin_utils.py:85] # Parameters for moe_architecture.SparseDecoder:
I0512 01:44:51.419647 140188559153152 gin_utils.py:85] # ==============================================================================
I0512 01:44:51.419680 140188559153152 gin_utils.py:85] moe_architecture.SparseDecoder.dropout_factory = %DROPOUT_FACTORY
I0512 01:44:51.419713 140188559153152 gin_utils.py:85] moe_architecture.SparseDecoder.dtype = %ACTIVATION_DTYPE
I0512 01:44:51.419745 140188559153152 gin_utils.py:85] moe_architecture.SparseDecoder.layer_factory = @t5_architecture.DecoderLayer
I0512 01:44:51.419778 140188559153152 gin_utils.py:85] moe_architecture.SparseDecoder.layer_norm_factory = @layer_norm.T5LayerNorm
I0512 01:44:51.419811 140188559153152 gin_utils.py:85] moe_architecture.SparseDecoder.num_layers = %NUM_DECODER_LAYERS
I0512 01:44:51.419843 140188559153152 gin_utils.py:85] moe_architecture.SparseDecoder.num_sparse_layers = %NUM_DECODER_SPARSE_LAYERS
I0512 01:44:51.419875 140188559153152 gin_utils.py:85] moe_architecture.SparseDecoder.output_logits_factory = \
I0512 01:44:51.419907 140188559153152 gin_utils.py:85]     @output_logits/dense.DenseGeneral
I0512 01:44:51.419940 140188559153152 gin_utils.py:85] moe_architecture.SparseDecoder.sparse_layer_factory = \
I0512 01:44:51.419972 140188559153152 gin_utils.py:85]     @moe_architecture.SparseDecoderLayer
I0512 01:44:51.420004 140188559153152 gin_utils.py:85] moe_architecture.SparseDecoder.sparse_layout = %DECODER_SPARSE_LAYOUT
I0512 01:44:51.420042 140188559153152 gin_utils.py:85] 
I0512 01:44:51.420076 140188559153152 gin_utils.py:85] # Parameters for moe_architecture.SparseDecoderLayer:
I0512 01:44:51.420108 140188559153152 gin_utils.py:85] # ==============================================================================
I0512 01:44:51.420140 140188559153152 gin_utils.py:85] moe_architecture.SparseDecoderLayer.activation_partitioning_dims = \
I0512 01:44:51.420173 140188559153152 gin_utils.py:85]     %ACTIVATION_PARTITIONING_DIMS
I0512 01:44:51.420205 140188559153152 gin_utils.py:85] moe_architecture.SparseDecoderLayer.dropout_factory = %DROPOUT_FACTORY
I0512 01:44:51.420238 140188559153152 gin_utils.py:85] moe_architecture.SparseDecoderLayer.encoder_decoder_attention = None
I0512 01:44:51.420272 140188559153152 gin_utils.py:85] moe_architecture.SparseDecoderLayer.extra_mlp = @dense.MlpBlock()
I0512 01:44:51.420307 140188559153152 gin_utils.py:85] moe_architecture.SparseDecoderLayer.layer_norm_factory = @layer_norm.T5LayerNorm
I0512 01:44:51.420341 140188559153152 gin_utils.py:85] moe_architecture.SparseDecoderLayer.mlp = @sparse_decoder/moe_layers.MoeLayer()
I0512 01:44:51.369467 139997397456896 gin_utils.py:85] train_script.train.infer_eval_dataset_cfg = None
I0512 01:44:51.369498 139997397456896 gin_utils.py:85] train_script.train.model = %MODEL
I0512 01:44:51.369529 139997397456896 gin_utils.py:85] train_script.train.model_dir = %MODEL_DIR
I0512 01:44:51.369561 139997397456896 gin_utils.py:85] train_script.train.partitioner = @partitioning.MoePjitPartitioner()
I0512 01:44:51.369594 139997397456896 gin_utils.py:85] train_script.train.random_seed = 42
I0512 01:44:51.369626 139997397456896 gin_utils.py:85] train_script.train.stats_period = 10
I0512 01:44:51.369657 139997397456896 gin_utils.py:85] train_script.train.summarize_config_fn = @gin_utils.summarize_gin_config
I0512 01:44:51.369688 139997397456896 gin_utils.py:85] train_script.train.total_steps = %TRAIN_STEPS
I0512 01:44:51.369719 139997397456896 gin_utils.py:85] train_script.train.train_dataset_cfg = @train/utils.DatasetConfig()
I0512 01:44:51.369750 139997397456896 gin_utils.py:85] train_script.train.train_eval_dataset_cfg = None
I0512 01:44:51.369781 139997397456896 gin_utils.py:85] train_script.train.trainer_cls = @moe_trainer.MoeTrainer
I0512 01:44:51.369812 139997397456896 gin_utils.py:85] train_script.train.use_hardware_rng = %USE_HARDWARE_RNG
I0512 01:44:51.369843 139997397456896 gin_utils.py:85] 
I0512 01:44:51.369875 139997397456896 gin_utils.py:85] # Parameters for trainer.Trainer:
I0512 01:44:51.369906 139997397456896 gin_utils.py:85] # ==============================================================================
I0512 01:44:51.369937 139997397456896 gin_utils.py:85] trainer.Trainer.learning_rate_fn = @utils.create_learning_rate_scheduler()
I0512 01:44:51.369969 139997397456896 gin_utils.py:85] trainer.Trainer.num_microbatches = None
I0512 01:44:51.370000 139997397456896 gin_utils.py:85] 
I0512 01:44:51.370031 139997397456896 gin_utils.py:85] # Parameters for attention_kernel_init/linen.initializers.variance_scaling:
I0512 01:44:51.370062 139997397456896 gin_utils.py:85] # ==============================================================================
I0512 01:44:51.370094 139997397456896 gin_utils.py:85] attention_kernel_init/linen.initializers.variance_scaling.distribution = 'normal'
I0512 01:44:51.370125 139997397456896 gin_utils.py:85] attention_kernel_init/linen.initializers.variance_scaling.mode = 'fan_in'
I0512 01:44:51.370156 139997397456896 gin_utils.py:85] attention_kernel_init/linen.initializers.variance_scaling.scale = %SCALE
I0512 01:44:51.370187 139997397456896 gin_utils.py:85] 
I0512 01:44:51.370218 139997397456896 gin_utils.py:85] # Parameters for expert_kernel_init/linen.initializers.variance_scaling:
I0512 01:44:51.370249 139997397456896 gin_utils.py:85] # ==============================================================================
I0512 01:44:51.370280 139997397456896 gin_utils.py:85] expert_kernel_init/linen.initializers.variance_scaling.distribution = 'normal'
I0512 01:44:51.370316 139997397456896 gin_utils.py:85] expert_kernel_init/linen.initializers.variance_scaling.mode = 'fan_in'
I0512 01:44:51.370349 139997397456896 gin_utils.py:85] expert_kernel_init/linen.initializers.variance_scaling.scale = %SCALE
I0512 01:44:51.370381 139997397456896 gin_utils.py:85] 
I0512 01:44:51.370412 139997397456896 gin_utils.py:85] # Parameters for mlp_kernel_init/linen.initializers.variance_scaling:
I0512 01:44:51.370443 139997397456896 gin_utils.py:85] # ==============================================================================
I0512 01:44:51.370474 139997397456896 gin_utils.py:85] mlp_kernel_init/linen.initializers.variance_scaling.distribution = \
I0512 01:44:51.370505 139997397456896 gin_utils.py:85]     'truncated_normal'
I0512 01:44:51.370536 139997397456896 gin_utils.py:85] mlp_kernel_init/linen.initializers.variance_scaling.mode = 'fan_in'
I0512 01:44:51.370569 139997397456896 gin_utils.py:85] mlp_kernel_init/linen.initializers.variance_scaling.scale = %SCALE
I0512 01:44:51.370601 139997397456896 gin_utils.py:85] 
I0512 01:44:51.420373 140188559153152 gin_utils.py:85] moe_architecture.SparseDecoderLayer.scanned = False
I0512 01:44:51.420407 140188559153152 gin_utils.py:85] moe_architecture.SparseDecoderLayer.self_attention = \
I0512 01:44:51.420439 140188559153152 gin_utils.py:85]     @dense_attention.MultiHeadDotProductAttention()
I0512 01:44:51.420472 140188559153152 gin_utils.py:85] 
I0512 01:44:51.420505 140188559153152 gin_utils.py:85] # Parameters for layer_norm.T5LayerNorm:
I0512 01:44:51.420538 140188559153152 gin_utils.py:85] # ==============================================================================
I0512 01:44:51.420570 140188559153152 gin_utils.py:85] layer_norm.T5LayerNorm.dtype = %ACTIVATION_DTYPE
I0512 01:44:51.420603 140188559153152 gin_utils.py:85] 
I0512 01:44:51.420636 140188559153152 gin_utils.py:85] # Parameters for routing.TokensChooseMaskedRouter:
I0512 01:44:51.420668 140188559153152 gin_utils.py:85] # ==============================================================================
I0512 01:44:51.420700 140188559153152 gin_utils.py:85] routing.TokensChooseMaskedRouter.dtype = 'float32'
I0512 01:44:51.420733 140188559153152 gin_utils.py:85] routing.TokensChooseMaskedRouter.ignore_padding_tokens = False
I0512 01:44:51.420765 140188559153152 gin_utils.py:85] routing.TokensChooseMaskedRouter.jitter_noise = %JITTER_NOISE
I0512 01:44:51.420798 140188559153152 gin_utils.py:85] routing.TokensChooseMaskedRouter.num_selected_experts = %NUM_SELECTED_EXPERTS
I0512 01:44:51.420830 140188559153152 gin_utils.py:85] routing.TokensChooseMaskedRouter.router_weights = @routing.RouterWeights()
I0512 01:44:51.420862 140188559153152 gin_utils.py:85] 
I0512 01:44:51.420894 140188559153152 gin_utils.py:85] # Parameters for sparse_decoder/routing.TokensChooseMaskedRouter:
I0512 01:44:51.420929 140188559153152 gin_utils.py:85] # ==============================================================================
I0512 01:44:51.420964 140188559153152 gin_utils.py:85] sparse_decoder/routing.TokensChooseMaskedRouter.batch_prioritized_routing = False
I0512 01:44:51.420996 140188559153152 gin_utils.py:85] 
I0512 01:44:51.421034 140188559153152 gin_utils.py:85] # Parameters for sparse_encoder/routing.TokensChooseMaskedRouter:
I0512 01:44:51.421067 140188559153152 gin_utils.py:85] # ==============================================================================
I0512 01:44:51.421100 140188559153152 gin_utils.py:85] sparse_encoder/routing.TokensChooseMaskedRouter.batch_prioritized_routing = False
I0512 01:44:51.421132 140188559153152 gin_utils.py:85] 
I0512 01:44:51.421165 140188559153152 gin_utils.py:85] # Parameters for train_script.train:
I0512 01:44:51.421198 140188559153152 gin_utils.py:85] # ==============================================================================
I0512 01:44:51.421231 140188559153152 gin_utils.py:85] train_script.train.checkpoint_cfg = @utils.CheckpointConfig()
I0512 01:44:51.421264 140188559153152 gin_utils.py:85] train_script.train.eval_period = 2500
I0512 01:44:51.370633 139997397456896 gin_utils.py:85] # Parameters for output_logits_kernel_init/linen.initializers.variance_scaling:
I0512 01:44:51.370664 139997397456896 gin_utils.py:85] # ==============================================================================
I0512 01:44:51.370695 139997397456896 gin_utils.py:85] output_logits_kernel_init/linen.initializers.variance_scaling.distribution = \
I0512 01:44:51.370726 139997397456896 gin_utils.py:85]     'truncated_normal'
I0512 01:44:51.370758 139997397456896 gin_utils.py:85] output_logits_kernel_init/linen.initializers.variance_scaling.mode = 'fan_in'
I0512 01:44:51.370789 139997397456896 gin_utils.py:85] output_logits_kernel_init/linen.initializers.variance_scaling.scale = %SCALE
I0512 01:44:51.371695 139997397456896 partitioning.py:559] `activation_partitioning_dims` = 2, `parameter_partitioning_dims` = 1
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
I0000 00:00:1715478291.423198  352750 pjrt_api.cc:98] GetPjrtApi was found for tpu at /home/rosinality/openmoe_venv/lib/python3.10/site-packages/libtpu/libtpu.so
I0000 00:00:1715478291.423241  352750 pjrt_api.cc:67] PJRT_Api is set for device type tpu
I0000 00:00:1715478291.423243  352750 pjrt_api.cc:72] PJRT plugin for tpu has PJRT API version 0.30. The framework PJRT API version is 0.30.
I0512 01:44:51.536929 139812357228544 resource_reader.py:37] gin-config opened resource file:/home/rosinality/t5x/t5x/contrib/moe/configs/runs/pretrain.gin
I0512 01:44:51.537435 139812357228544 resource_reader.py:50] system_path_file_exists:t5x/configs/runs/pretrain.gin
I0512 01:44:51.537745 139812357228544 resource_reader.py:37] gin-config opened resource file:/home/rosinality/t5x/t5x/configs/runs/pretrain.gin
I0512 01:44:51.551709 139812357228544 gin_utils.py:83] Gin Configuration:
I0512 01:44:51.567441 139812357228544 gin_utils.py:85] from __gin__ import dynamic_registration
I0512 01:44:51.567510 139812357228544 gin_utils.py:85] import __main__ as train_script
I0512 01:44:51.567559 139812357228544 gin_utils.py:85] from flax import linen
I0512 01:44:51.567595 139812357228544 gin_utils.py:85] import flaxformer
I0512 01:44:51.567629 139812357228544 gin_utils.py:85] from flaxformer.architectures.moe import moe_architecture
I0512 01:44:51.567662 139812357228544 gin_utils.py:85] from flaxformer.architectures.moe import moe_enums
I0512 01:44:51.567695 139812357228544 gin_utils.py:85] from flaxformer.architectures.moe import moe_layers
I0512 01:44:51.567727 139812357228544 gin_utils.py:85] from flaxformer.architectures.moe import routing
I0512 01:44:51.567760 139812357228544 gin_utils.py:85] from flaxformer.architectures.t5 import t5_architecture
I0512 01:44:51.567793 139812357228544 gin_utils.py:85] from flaxformer.components.attention import dense_attention
I0512 01:44:51.567828 139812357228544 gin_utils.py:85] from flaxformer.components.attention import memory_efficient_attention
I0512 01:44:51.567861 139812357228544 gin_utils.py:85] from flaxformer.components import dense
I0512 01:44:51.567894 139812357228544 gin_utils.py:85] from flaxformer.components import embedding
I0512 01:44:51.567927 139812357228544 gin_utils.py:85] from flaxformer.components import layer_norm
I0512 01:44:51.567960 139812357228544 gin_utils.py:85] from gin import config
I0512 01:44:51.567992 139812357228544 gin_utils.py:85] import seqio
I0512 01:44:51.568024 139812357228544 gin_utils.py:85] import t5.data.mixtures
I0512 01:44:51.568056 139812357228544 gin_utils.py:85] from t5x import adafactor
I0512 01:44:51.568089 139812357228544 gin_utils.py:85] from t5x.contrib.moe import adafactor_utils
I0512 01:44:51.568121 139812357228544 gin_utils.py:85] from t5x.contrib.moe import models
I0512 01:44:51.568154 139812357228544 gin_utils.py:85] from t5x.contrib.moe import partitioning as moe_partitioning
I0512 01:44:51.568211 139812357228544 gin_utils.py:85] from t5x.contrib.moe import trainer as moe_trainer
I0512 01:44:51.568245 139812357228544 gin_utils.py:85] from t5x import gin_utils
I0512 01:44:51.568277 139812357228544 gin_utils.py:85] from t5x import partitioning
I0512 01:44:51.568309 139812357228544 gin_utils.py:85] from t5x import trainer
I0512 01:44:51.568341 139812357228544 gin_utils.py:85] from t5x import utils
I0512 01:44:51.568373 139812357228544 gin_utils.py:85] 
I0512 01:44:51.568405 139812357228544 gin_utils.py:85] # Macros:
I0512 01:44:51.568438 139812357228544 gin_utils.py:85] # ==============================================================================
I0512 01:44:51.568470 139812357228544 gin_utils.py:85] ACTIVATION_DTYPE = 'bfloat16'
I0512 01:44:51.568502 139812357228544 gin_utils.py:85] ACTIVATION_PARTITIONING_DIMS = 1
I0512 01:44:51.568534 139812357228544 gin_utils.py:85] ARCHITECTURE = @t5_architecture.DecoderOnly()
I0512 01:44:51.568573 139812357228544 gin_utils.py:85] AUX_LOSS_FACTOR = 0.01
I0512 01:44:51.568605 139812357228544 gin_utils.py:85] BATCH_SIZE = 384
I0512 01:44:51.568637 139812357228544 gin_utils.py:85] BIAS_INIT = @bias_init/linen.initializers.normal()
I0512 01:44:51.568669 139812357228544 gin_utils.py:85] DECODER_SPARSE_LAYOUT = %flaxformer.architectures.moe.moe_enums.LayerLayout.MIXED
I0512 01:44:51.568701 139812357228544 gin_utils.py:85] DROPOUT_FACTORY = @dropout_factory/linen.Dropout
I0512 01:44:51.568732 139812357228544 gin_utils.py:85] DROPOUT_RATE = 0.0
I0512 01:44:51.568764 139812357228544 gin_utils.py:85] EMBED_DIM = 2048
I0512 01:44:51.568797 139812357228544 gin_utils.py:85] EVAL_EXPERT_CAPACITY_FACTOR = 2.0
I0512 01:44:51.568831 139812357228544 gin_utils.py:85] EXPERT_DROPOUT_RATE = %DROPOUT_RATE
I0512 01:44:51.568863 139812357228544 gin_utils.py:85] EXPERT_MLP_DIM = %MLP_DIM
I0512 01:44:51.568895 139812357228544 gin_utils.py:85] GROUP_SIZE = 4096
I0512 01:44:51.568927 139812357228544 gin_utils.py:85] HEAD_DIM = 128
I0512 01:44:51.568958 139812357228544 gin_utils.py:85] JITTER_NOISE = 0.0
I0512 01:44:51.568990 139812357228544 gin_utils.py:85] LABEL_SMOOTHING = 0.0
I0512 01:44:51.569023 139812357228544 gin_utils.py:85] LOSS_NORMALIZING_FACTOR = 'NUM_REAL_TARGET_TOKENS'
I0512 01:44:51.569055 139812357228544 gin_utils.py:85] MIXTURE_OR_TASK_MODULE = None
I0512 01:44:51.569086 139812357228544 gin_utils.py:85] MIXTURE_OR_TASK_NAME = 'mix_full_lm_test'
I0512 01:44:51.569118 139812357228544 gin_utils.py:85] MLP_DIM = 8192
I0512 01:44:51.569150 139812357228544 gin_utils.py:85] MODEL = @models.MoeDecoderOnlyModel()
I0512 01:44:51.569181 139812357228544 gin_utils.py:85] MODEL_DIR = 'gs://rosinality-tpu-bucket/openmoe_8b/training'
I0512 01:44:51.569213 139812357228544 gin_utils.py:85] MODEL_PARALLEL_SUBMESH = None
I0512 01:44:51.569245 139812357228544 gin_utils.py:85] MOE_TRUNCATED_DTYPE = 'bfloat16'
I0512 01:44:51.569276 139812357228544 gin_utils.py:85] NUM_DECODER_LAYERS = 24
I0512 01:44:51.569308 139812357228544 gin_utils.py:85] NUM_DECODER_SPARSE_LAYERS = 4
I0512 01:44:51.569340 139812357228544 gin_utils.py:85] NUM_EMBEDDINGS = 256384
I0512 01:44:51.569372 139812357228544 gin_utils.py:85] NUM_EXPERT_PARTITIONS = 8
I0512 01:44:51.569403 139812357228544 gin_utils.py:85] NUM_EXPERTS = 8
I0512 01:44:51.569435 139812357228544 gin_utils.py:85] NUM_HEADS = 24
I0512 01:44:51.569467 139812357228544 gin_utils.py:85] NUM_MODEL_PARTITIONS = 4
I0512 01:44:51.569499 139812357228544 gin_utils.py:85] NUM_SELECTED_EXPERTS = 2
I0512 01:44:51.569530 139812357228544 gin_utils.py:85] OPTIMIZER = @adafactor.Adafactor()
I0512 01:44:51.569569 139812357228544 gin_utils.py:85] RANDOM_SEED = None
I0512 01:44:51.569602 139812357228544 gin_utils.py:85] ROUTER_Z_LOSS_FACTOR = 0.0001
I0512 01:44:51.569634 139812357228544 gin_utils.py:85] SCALE = 0.1
I0512 01:44:51.569666 139812357228544 gin_utils.py:85] SHUFFLE_TRAIN_EXAMPLES = True
I0512 01:44:51.569697 139812357228544 gin_utils.py:85] TASK_FEATURE_LENGTHS = {'targets': 2048}
I0512 01:44:51.569729 139812357228544 gin_utils.py:85] TRAIN_EXPERT_CAPACITY_FACTOR = 1.25
I0512 01:44:51.569761 139812357228544 gin_utils.py:85] TRAIN_STEPS = 500000
I0512 01:44:51.569793 139812357228544 gin_utils.py:85] USE_CACHED_TASKS = False
I0512 01:44:51.569828 139812357228544 gin_utils.py:85] USE_HARDWARE_RNG = False
I0512 01:44:51.569860 139812357228544 gin_utils.py:85] VOCABULARY = @seqio.SentencePieceVocabulary()
I0512 01:44:51.569892 139812357228544 gin_utils.py:85] Z_LOSS = 0.0001
I0512 01:44:51.569924 139812357228544 gin_utils.py:85] 
I0512 01:44:51.569956 139812357228544 gin_utils.py:85] # Parameters for adafactor.Adafactor:
I0512 01:44:51.569988 139812357228544 gin_utils.py:85] # ==============================================================================
I0512 01:44:51.570020 139812357228544 gin_utils.py:85] adafactor.Adafactor.decay_rate = 0.8
I0512 01:44:51.570052 139812357228544 gin_utils.py:85] adafactor.Adafactor.logical_factor_rules = @adafactor_utils.logical_factor_rules()
I0512 01:44:51.570084 139812357228544 gin_utils.py:85] adafactor.Adafactor.step_offset = 0
I0512 01:44:51.570115 139812357228544 gin_utils.py:85] 
I0512 01:44:51.426403 140172769531904 gin_utils.py:85] train_script.train.eval_steps = 20
I0512 01:44:51.426434 140172769531904 gin_utils.py:85] train_script.train.infer_eval_dataset_cfg = None
I0512 01:44:51.426465 140172769531904 gin_utils.py:85] train_script.train.model = %MODEL
I0512 01:44:51.426495 140172769531904 gin_utils.py:85] train_script.train.model_dir = %MODEL_DIR
I0512 01:44:51.426526 140172769531904 gin_utils.py:85] train_script.train.partitioner = @moe_partitioning.MoePjitPartitioner()
I0512 01:44:51.426558 140172769531904 gin_utils.py:85] train_script.train.random_seed = 42
I0512 01:44:51.426589 140172769531904 gin_utils.py:85] train_script.train.stats_period = 10
I0512 01:44:51.426620 140172769531904 gin_utils.py:85] train_script.train.summarize_config_fn = @gin_utils.summarize_gin_config
I0512 01:44:51.426651 140172769531904 gin_utils.py:85] train_script.train.total_steps = %TRAIN_STEPS
I0512 01:44:51.426681 140172769531904 gin_utils.py:85] train_script.train.train_dataset_cfg = @train/utils.DatasetConfig()
I0512 01:44:51.426712 140172769531904 gin_utils.py:85] train_script.train.train_eval_dataset_cfg = None
I0512 01:44:51.426743 140172769531904 gin_utils.py:85] train_script.train.trainer_cls = @moe_trainer.MoeTrainer
I0512 01:44:51.426774 140172769531904 gin_utils.py:85] train_script.train.use_hardware_rng = %USE_HARDWARE_RNG
I0512 01:44:51.426805 140172769531904 gin_utils.py:85] 
I0512 01:44:51.426836 140172769531904 gin_utils.py:85] # Parameters for trainer.Trainer:
I0512 01:44:51.426867 140172769531904 gin_utils.py:85] # ==============================================================================
I0512 01:44:51.426898 140172769531904 gin_utils.py:85] trainer.Trainer.learning_rate_fn = @utils.create_learning_rate_scheduler()
I0512 01:44:51.426928 140172769531904 gin_utils.py:85] trainer.Trainer.num_microbatches = None
I0512 01:44:51.426960 140172769531904 gin_utils.py:85] 
I0512 01:44:51.426990 140172769531904 gin_utils.py:85] # Parameters for attention_kernel_init/linen.initializers.variance_scaling:
I0512 01:44:51.427021 140172769531904 gin_utils.py:85] # ==============================================================================
I0512 01:44:51.427057 140172769531904 gin_utils.py:85] attention_kernel_init/linen.initializers.variance_scaling.distribution = 'normal'
I0512 01:44:51.427089 140172769531904 gin_utils.py:85] attention_kernel_init/linen.initializers.variance_scaling.mode = 'fan_in'
I0512 01:44:51.427120 140172769531904 gin_utils.py:85] attention_kernel_init/linen.initializers.variance_scaling.scale = %SCALE
I0512 01:44:51.427151 140172769531904 gin_utils.py:85] 
I0512 01:44:51.427182 140172769531904 gin_utils.py:85] # Parameters for expert_kernel_init/linen.initializers.variance_scaling:
I0512 01:44:51.427214 140172769531904 gin_utils.py:85] # ==============================================================================
I0512 01:44:51.427245 140172769531904 gin_utils.py:85] expert_kernel_init/linen.initializers.variance_scaling.distribution = 'normal'
I0512 01:44:51.427275 140172769531904 gin_utils.py:85] expert_kernel_init/linen.initializers.variance_scaling.mode = 'fan_in'
I0512 01:44:51.427307 140172769531904 gin_utils.py:85] expert_kernel_init/linen.initializers.variance_scaling.scale = %SCALE
I0512 01:44:51.427340 140172769531904 gin_utils.py:85] 
I0512 01:44:51.427371 140172769531904 gin_utils.py:85] # Parameters for mlp_kernel_init/linen.initializers.variance_scaling:
I0512 01:44:51.427402 140172769531904 gin_utils.py:85] # ==============================================================================
I0512 01:44:51.427433 140172769531904 gin_utils.py:85] mlp_kernel_init/linen.initializers.variance_scaling.distribution = \
I0512 01:44:51.427465 140172769531904 gin_utils.py:85]     'truncated_normal'
I0512 01:44:51.427496 140172769531904 gin_utils.py:85] mlp_kernel_init/linen.initializers.variance_scaling.mode = 'fan_in'
I0512 01:44:51.427527 140172769531904 gin_utils.py:85] mlp_kernel_init/linen.initializers.variance_scaling.scale = %SCALE
I0512 01:44:51.427558 140172769531904 gin_utils.py:85] 
I0512 01:44:51.427589 140172769531904 gin_utils.py:85] # Parameters for output_logits_kernel_init/linen.initializers.variance_scaling:
I0512 01:44:51.427620 140172769531904 gin_utils.py:85] # ==============================================================================
I0512 01:44:51.427651 140172769531904 gin_utils.py:85] output_logits_kernel_init/linen.initializers.variance_scaling.distribution = \
I0512 01:44:51.427682 140172769531904 gin_utils.py:85]     'truncated_normal'
I0512 01:44:51.427712 140172769531904 gin_utils.py:85] output_logits_kernel_init/linen.initializers.variance_scaling.mode = 'fan_in'
I0512 01:44:51.427743 140172769531904 gin_utils.py:85] output_logits_kernel_init/linen.initializers.variance_scaling.scale = %SCALE
I0512 01:44:51.428677 140172769531904 partitioning.py:559] `activation_partitioning_dims` = 2, `parameter_partitioning_dims` = 1
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
I0000 00:00:1715478291.510931  373603 pjrt_api.cc:98] GetPjrtApi was found for tpu at /home/rosinality/openmoe_venv/lib/python3.10/site-packages/libtpu/libtpu.so
I0000 00:00:1715478291.510986  373603 pjrt_api.cc:67] PJRT_Api is set for device type tpu
I0000 00:00:1715478291.510988  373603 pjrt_api.cc:72] PJRT plugin for tpu has PJRT API version 0.30. The framework PJRT API version is 0.30.
I0512 01:44:51.421298 140188559153152 gin_utils.py:85] train_script.train.eval_steps = 20
I0512 01:44:51.421332 140188559153152 gin_utils.py:85] train_script.train.infer_eval_dataset_cfg = None
I0512 01:44:51.421365 140188559153152 gin_utils.py:85] train_script.train.model = %MODEL
I0512 01:44:51.421397 140188559153152 gin_utils.py:85] train_script.train.model_dir = %MODEL_DIR
I0512 01:44:51.421430 140188559153152 gin_utils.py:85] train_script.train.partitioner = @moe_partitioning.MoePjitPartitioner()
I0512 01:44:51.421462 140188559153152 gin_utils.py:85] train_script.train.random_seed = 42
I0512 01:44:51.421495 140188559153152 gin_utils.py:85] train_script.train.stats_period = 10
I0512 01:44:51.421527 140188559153152 gin_utils.py:85] train_script.train.summarize_config_fn = @gin_utils.summarize_gin_config
I0512 01:44:51.421560 140188559153152 gin_utils.py:85] train_script.train.total_steps = %TRAIN_STEPS
I0512 01:44:51.421592 140188559153152 gin_utils.py:85] train_script.train.train_dataset_cfg = @train/utils.DatasetConfig()
I0512 01:44:51.421625 140188559153152 gin_utils.py:85] train_script.train.train_eval_dataset_cfg = None
I0512 01:44:51.421657 140188559153152 gin_utils.py:85] train_script.train.trainer_cls = @moe_trainer.MoeTrainer
I0512 01:44:51.421689 140188559153152 gin_utils.py:85] train_script.train.use_hardware_rng = %USE_HARDWARE_RNG
I0512 01:44:51.421722 140188559153152 gin_utils.py:85] 
I0512 01:44:51.421754 140188559153152 gin_utils.py:85] # Parameters for trainer.Trainer:
I0512 01:44:51.421786 140188559153152 gin_utils.py:85] # ==============================================================================
I0512 01:44:51.421819 140188559153152 gin_utils.py:85] trainer.Trainer.learning_rate_fn = @utils.create_learning_rate_scheduler()
I0512 01:44:51.421852 140188559153152 gin_utils.py:85] trainer.Trainer.num_microbatches = None
I0512 01:44:51.421884 140188559153152 gin_utils.py:85] 
I0512 01:44:51.421916 140188559153152 gin_utils.py:85] # Parameters for attention_kernel_init/linen.initializers.variance_scaling:
I0512 01:44:51.421949 140188559153152 gin_utils.py:85] # ==============================================================================
I0512 01:44:51.421982 140188559153152 gin_utils.py:85] attention_kernel_init/linen.initializers.variance_scaling.distribution = 'normal'
I0512 01:44:51.422014 140188559153152 gin_utils.py:85] attention_kernel_init/linen.initializers.variance_scaling.mode = 'fan_in'
I0512 01:44:51.422053 140188559153152 gin_utils.py:85] attention_kernel_init/linen.initializers.variance_scaling.scale = %SCALE
I0512 01:44:51.422087 140188559153152 gin_utils.py:85] 
I0512 01:44:51.422119 140188559153152 gin_utils.py:85] # Parameters for expert_kernel_init/linen.initializers.variance_scaling:
I0512 01:44:51.422152 140188559153152 gin_utils.py:85] # ==============================================================================
I0512 01:44:51.422184 140188559153152 gin_utils.py:85] expert_kernel_init/linen.initializers.variance_scaling.distribution = 'normal'
I0512 01:44:51.422217 140188559153152 gin_utils.py:85] expert_kernel_init/linen.initializers.variance_scaling.mode = 'fan_in'
I0512 01:44:51.422250 140188559153152 gin_utils.py:85] expert_kernel_init/linen.initializers.variance_scaling.scale = %SCALE
I0512 01:44:51.422308 140188559153152 gin_utils.py:85] 
I0512 01:44:51.422343 140188559153152 gin_utils.py:85] # Parameters for mlp_kernel_init/linen.initializers.variance_scaling:
I0512 01:44:51.422376 140188559153152 gin_utils.py:85] # ==============================================================================
I0512 01:44:51.422409 140188559153152 gin_utils.py:85] mlp_kernel_init/linen.initializers.variance_scaling.distribution = \
I0512 01:44:51.422441 140188559153152 gin_utils.py:85]     'truncated_normal'
I0512 01:44:51.422474 140188559153152 gin_utils.py:85] mlp_kernel_init/linen.initializers.variance_scaling.mode = 'fan_in'
I0512 01:44:51.422506 140188559153152 gin_utils.py:85] mlp_kernel_init/linen.initializers.variance_scaling.scale = %SCALE
I0512 01:44:51.422539 140188559153152 gin_utils.py:85] 
I0512 01:44:51.422571 140188559153152 gin_utils.py:85] # Parameters for output_logits_kernel_init/linen.initializers.variance_scaling:
I0512 01:44:51.422604 140188559153152 gin_utils.py:85] # ==============================================================================
I0512 01:44:51.422636 140188559153152 gin_utils.py:85] output_logits_kernel_init/linen.initializers.variance_scaling.distribution = \
I0512 01:44:51.422668 140188559153152 gin_utils.py:85]     'truncated_normal'
I0512 01:44:51.422701 140188559153152 gin_utils.py:85] output_logits_kernel_init/linen.initializers.variance_scaling.mode = 'fan_in'
I0512 01:44:51.422733 140188559153152 gin_utils.py:85] output_logits_kernel_init/linen.initializers.variance_scaling.scale = %SCALE
I0512 01:44:51.423634 140188559153152 partitioning.py:559] `activation_partitioning_dims` = 2, `parameter_partitioning_dims` = 1
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
I0000 00:00:1715478291.476781  351449 pjrt_api.cc:98] GetPjrtApi was found for tpu at /home/rosinality/openmoe_venv/lib/python3.10/site-packages/libtpu/libtpu.so
I0000 00:00:1715478291.476837  351449 pjrt_api.cc:67] PJRT_Api is set for device type tpu
I0000 00:00:1715478291.476840  351449 pjrt_api.cc:72] PJRT plugin for tpu has PJRT API version 0.30. The framework PJRT API version is 0.30.
I0512 01:44:51.471671 140316961150976 gin_utils.py:85] expert/dense.MlpBlock.use_bias = False
I0512 01:44:51.471702 140316961150976 gin_utils.py:85] 
I0512 01:44:51.471734 140316961150976 gin_utils.py:85] # Parameters for models.MoeDecoderOnlyModel:
I0512 01:44:51.471765 140316961150976 gin_utils.py:85] # ==============================================================================
I0512 01:44:51.471797 140316961150976 gin_utils.py:85] models.MoeDecoderOnlyModel.aux_loss_factor = %AUX_LOSS_FACTOR
I0512 01:44:51.471829 140316961150976 gin_utils.py:85] models.MoeDecoderOnlyModel.label_smoothing = %LABEL_SMOOTHING
I0512 01:44:51.471866 140316961150976 gin_utils.py:85] models.MoeDecoderOnlyModel.loss_normalizing_factor = %LOSS_NORMALIZING_FACTOR
I0512 01:44:51.471899 140316961150976 gin_utils.py:85] models.MoeDecoderOnlyModel.module = %ARCHITECTURE
I0512 01:44:51.471931 140316961150976 gin_utils.py:85] models.MoeDecoderOnlyModel.optimizer_def = %OPTIMIZER
I0512 01:44:51.471963 140316961150976 gin_utils.py:85] models.MoeDecoderOnlyModel.router_z_loss_factor = %ROUTER_Z_LOSS_FACTOR
I0512 01:44:51.471995 140316961150976 gin_utils.py:85] models.MoeDecoderOnlyModel.vocabulary = %VOCABULARY
I0512 01:44:51.472027 140316961150976 gin_utils.py:85] models.MoeDecoderOnlyModel.z_loss = %Z_LOSS
I0512 01:44:51.472059 140316961150976 gin_utils.py:85] 
I0512 01:44:51.472090 140316961150976 gin_utils.py:85] # Parameters for moe_layers.MoeLayer:
I0512 01:44:51.472126 140316961150976 gin_utils.py:85] # ==============================================================================
I0512 01:44:51.472162 140316961150976 gin_utils.py:85] moe_layers.MoeLayer.dtype = %MOE_TRUNCATED_DTYPE
I0512 01:44:51.472194 140316961150976 gin_utils.py:85] moe_layers.MoeLayer.eval_capacity_factor = %EVAL_EXPERT_CAPACITY_FACTOR
I0512 01:44:51.472226 140316961150976 gin_utils.py:85] moe_layers.MoeLayer.expert = @expert/dense.MlpBlock()
I0512 01:44:51.472258 140316961150976 gin_utils.py:85] moe_layers.MoeLayer.max_group_size = %GROUP_SIZE
I0512 01:44:51.472290 140316961150976 gin_utils.py:85] moe_layers.MoeLayer.num_expert_partitions = %NUM_EXPERT_PARTITIONS
I0512 01:44:51.472322 140316961150976 gin_utils.py:85] moe_layers.MoeLayer.num_experts = %NUM_EXPERTS
I0512 01:44:51.472353 140316961150976 gin_utils.py:85] moe_layers.MoeLayer.num_model_partitions = \
I0512 01:44:51.472386 140316961150976 gin_utils.py:85]     @moe_partitioning.compute_num_model_partitions()
I0512 01:44:51.472418 140316961150976 gin_utils.py:85] moe_layers.MoeLayer.train_capacity_factor = %TRAIN_EXPERT_CAPACITY_FACTOR
I0512 01:44:51.472450 140316961150976 gin_utils.py:85] 
I0512 01:44:51.472481 140316961150976 gin_utils.py:85] # Parameters for sparse_decoder/moe_layers.MoeLayer:
I0512 01:44:51.472513 140316961150976 gin_utils.py:85] # ==============================================================================
I0512 01:44:51.472545 140316961150976 gin_utils.py:85] sparse_decoder/moe_layers.MoeLayer.router = \
I0512 01:44:51.472577 140316961150976 gin_utils.py:85]     @sparse_decoder/routing.TokensChooseMaskedRouter()
I0512 01:44:51.472608 140316961150976 gin_utils.py:85] 
I0512 01:44:51.472640 140316961150976 gin_utils.py:85] # Parameters for moe_partitioning.MoePjitPartitioner:
I0512 01:44:51.472672 140316961150976 gin_utils.py:85] # ==============================================================================
I0512 01:44:51.472703 140316961150976 gin_utils.py:85] moe_partitioning.MoePjitPartitioner.model_parallel_submesh = \
I0512 01:44:51.472735 140316961150976 gin_utils.py:85]     %MODEL_PARALLEL_SUBMESH
I0512 01:44:51.472767 140316961150976 gin_utils.py:85] moe_partitioning.MoePjitPartitioner.num_expert_partitions = %NUM_EXPERT_PARTITIONS
I0512 01:44:51.472798 140316961150976 gin_utils.py:85] moe_partitioning.MoePjitPartitioner.num_partitions = %NUM_MODEL_PARTITIONS
I0512 01:44:51.472830 140316961150976 gin_utils.py:85] 
I0512 01:44:51.472867 140316961150976 gin_utils.py:85] # Parameters for moe_trainer.MoeTrainer:
I0512 01:44:51.472901 140316961150976 gin_utils.py:85] # ==============================================================================
I0512 01:44:51.472933 140316961150976 gin_utils.py:85] moe_trainer.MoeTrainer.learning_rate_fn = @utils.create_learning_rate_scheduler()
I0512 01:44:51.472965 140316961150976 gin_utils.py:85] moe_trainer.MoeTrainer.num_expert_partitions = %NUM_EXPERT_PARTITIONS
I0512 01:44:51.472997 140316961150976 gin_utils.py:85] moe_trainer.MoeTrainer.num_microbatches = 8
I0512 01:44:51.473029 140316961150976 gin_utils.py:85] 
I0512 01:44:51.473060 140316961150976 gin_utils.py:85] # Parameters for dense_attention.MultiHeadDotProductAttention:
I0512 01:44:51.473092 140316961150976 gin_utils.py:85] # ==============================================================================
I0512 01:44:51.473126 140316961150976 gin_utils.py:85] dense_attention.MultiHeadDotProductAttention.bias_init = %BIAS_INIT
I0512 01:44:51.473158 140316961150976 gin_utils.py:85] dense_attention.MultiHeadDotProductAttention.broadcast_dropout = True
I0512 01:44:51.473190 140316961150976 gin_utils.py:85] dense_attention.MultiHeadDotProductAttention.dropout_rate = %DROPOUT_RATE
I0512 01:44:51.473222 140316961150976 gin_utils.py:85] dense_attention.MultiHeadDotProductAttention.dtype = %ACTIVATION_DTYPE
I0512 01:44:51.473254 140316961150976 gin_utils.py:85] dense_attention.MultiHeadDotProductAttention.head_dim = %HEAD_DIM
I0512 01:44:51.473286 140316961150976 gin_utils.py:85] dense_attention.MultiHeadDotProductAttention.kernel_init = \
I0512 01:44:51.473317 140316961150976 gin_utils.py:85]     @attention_kernel_init/linen.initializers.variance_scaling()
I0512 01:44:51.473349 140316961150976 gin_utils.py:85] dense_attention.MultiHeadDotProductAttention.num_heads = %NUM_HEADS
I0512 01:44:51.473381 140316961150976 gin_utils.py:85] dense_attention.MultiHeadDotProductAttention.use_bias = False
I0512 01:44:51.473413 140316961150976 gin_utils.py:85] dense_attention.MultiHeadDotProductAttention.use_rotary_embedding = True
I0512 01:44:51.473445 140316961150976 gin_utils.py:85] 
I0512 01:44:51.473477 140316961150976 gin_utils.py:85] # Parameters for bias_init/linen.initializers.normal:
I0512 01:44:51.473509 140316961150976 gin_utils.py:85] # ==============================================================================
I0512 01:44:51.473557 140316961150976 gin_utils.py:85] bias_init/linen.initializers.normal.stddev = 1e-06
I0512 01:44:51.473597 140316961150976 gin_utils.py:85] 
I0512 01:44:51.473632 140316961150976 gin_utils.py:85] # Parameters for router_init/linen.initializers.normal:
I0512 01:44:51.473664 140316961150976 gin_utils.py:85] # ==============================================================================
I0512 01:44:51.473696 140316961150976 gin_utils.py:85] router_init/linen.initializers.normal.stddev = 0.02
I0512 01:44:51.473728 140316961150976 gin_utils.py:85] 
I0512 01:44:51.473759 140316961150976 gin_utils.py:85] # Parameters for token_embedder_init/linen.initializers.normal:
I0512 01:44:51.473791 140316961150976 gin_utils.py:85] # ==============================================================================
I0512 01:44:51.473823 140316961150976 gin_utils.py:85] token_embedder_init/linen.initializers.normal.stddev = 1.0
I0512 01:44:51.473860 140316961150976 gin_utils.py:85] 
I0512 01:44:51.473894 140316961150976 gin_utils.py:85] # Parameters for partitioning.PjitPartitioner:
I0512 01:44:51.473926 140316961150976 gin_utils.py:85] # ==============================================================================
I0512 01:44:51.473958 140316961150976 gin_utils.py:85] partitioning.PjitPartitioner.logical_axis_rules = \
I0512 01:44:51.473990 140316961150976 gin_utils.py:85]     @partitioning.standard_logical_axis_rules()
I0512 01:44:51.474021 140316961150976 gin_utils.py:85] partitioning.PjitPartitioner.model_parallel_submesh = None
I0512 01:44:51.474053 140316961150976 gin_utils.py:85] partitioning.PjitPartitioner.num_partitions = 1
I0512 01:44:51.474085 140316961150976 gin_utils.py:85] 
I0512 01:44:51.474118 140316961150976 gin_utils.py:85] # Parameters for utils.RestoreCheckpointConfig:
I0512 01:44:51.474151 140316961150976 gin_utils.py:85] # ==============================================================================
I0512 01:44:51.474183 140316961150976 gin_utils.py:85] utils.RestoreCheckpointConfig.path = []
I0512 01:44:51.474215 140316961150976 gin_utils.py:85] 
I0512 01:44:51.474247 140316961150976 gin_utils.py:85] # Parameters for routing.RouterWeights:
I0512 01:44:51.474278 140316961150976 gin_utils.py:85] # ==============================================================================
I0512 01:44:51.474310 140316961150976 gin_utils.py:85] routing.RouterWeights.bias_init = %BIAS_INIT
I0512 01:44:51.474342 140316961150976 gin_utils.py:85] routing.RouterWeights.dtype = 'float32'
I0512 01:44:51.474373 140316961150976 gin_utils.py:85] routing.RouterWeights.kernel_init = @router_init/linen.initializers.normal()
I0512 01:44:51.474405 140316961150976 gin_utils.py:85] routing.RouterWeights.use_bias = False
I0512 01:44:51.474436 140316961150976 gin_utils.py:85] 
I0512 01:44:51.474468 140316961150976 gin_utils.py:85] # Parameters for utils.SaveCheckpointConfig:
I0512 01:44:51.474500 140316961150976 gin_utils.py:85] # ==============================================================================
I0512 01:44:51.474531 140316961150976 gin_utils.py:85] utils.SaveCheckpointConfig.dtype = 'float32'
I0512 01:44:51.474563 140316961150976 gin_utils.py:85] utils.SaveCheckpointConfig.keep = 20
I0512 01:44:51.474594 140316961150976 gin_utils.py:85] utils.SaveCheckpointConfig.period = 2500
I0512 01:44:51.474626 140316961150976 gin_utils.py:85] utils.SaveCheckpointConfig.save_dataset = True
I0512 01:44:51.474657 140316961150976 gin_utils.py:85] 
I0512 01:44:51.474688 140316961150976 gin_utils.py:85] # Parameters for seqio.SentencePieceVocabulary:
I0512 01:44:51.474720 140316961150976 gin_utils.py:85] # ==============================================================================
I0512 01:44:51.474751 140316961150976 gin_utils.py:85] seqio.SentencePieceVocabulary.extra_ids = 300
I0512 01:44:51.474782 140316961150976 gin_utils.py:85] seqio.SentencePieceVocabulary.sentencepiece_model_file = \
I0512 01:44:51.474814 140316961150976 gin_utils.py:85]     'gs://rosinality-tpu-bucket/sentencepiece.model'
I0512 01:44:51.474845 140316961150976 gin_utils.py:85] 
I0512 01:44:51.474884 140316961150976 gin_utils.py:85] # Parameters for moe_architecture.SparseDecoder:
I0512 01:44:51.474916 140316961150976 gin_utils.py:85] # ==============================================================================
I0512 01:44:51.474948 140316961150976 gin_utils.py:85] moe_architecture.SparseDecoder.dropout_factory = %DROPOUT_FACTORY
I0512 01:44:51.474979 140316961150976 gin_utils.py:85] moe_architecture.SparseDecoder.dtype = %ACTIVATION_DTYPE
I0512 01:44:51.475011 140316961150976 gin_utils.py:85] moe_architecture.SparseDecoder.layer_factory = @t5_architecture.DecoderLayer
I0512 01:44:51.475042 140316961150976 gin_utils.py:85] moe_architecture.SparseDecoder.layer_norm_factory = @layer_norm.T5LayerNorm
I0512 01:44:51.475074 140316961150976 gin_utils.py:85] moe_architecture.SparseDecoder.num_layers = %NUM_DECODER_LAYERS
I0512 01:44:51.475106 140316961150976 gin_utils.py:85] moe_architecture.SparseDecoder.num_sparse_layers = %NUM_DECODER_SPARSE_LAYERS
I0512 01:44:51.475144 140316961150976 gin_utils.py:85] moe_architecture.SparseDecoder.output_logits_factory = \
I0512 01:44:51.475177 140316961150976 gin_utils.py:85]     @output_logits/dense.DenseGeneral
I0512 01:44:51.475208 140316961150976 gin_utils.py:85] moe_architecture.SparseDecoder.sparse_layer_factory = \
I0512 01:44:51.475240 140316961150976 gin_utils.py:85]     @moe_architecture.SparseDecoderLayer
I0512 01:44:51.475271 140316961150976 gin_utils.py:85] moe_architecture.SparseDecoder.sparse_layout = %DECODER_SPARSE_LAYOUT
I0512 01:44:51.475303 140316961150976 gin_utils.py:85] 
I0512 01:44:51.475335 140316961150976 gin_utils.py:85] # Parameters for moe_architecture.SparseDecoderLayer:
I0512 01:44:51.475367 140316961150976 gin_utils.py:85] # ==============================================================================
I0512 01:44:51.475399 140316961150976 gin_utils.py:85] moe_architecture.SparseDecoderLayer.activation_partitioning_dims = \
I0512 01:44:51.475430 140316961150976 gin_utils.py:85]     %ACTIVATION_PARTITIONING_DIMS
I0512 01:44:51.475461 140316961150976 gin_utils.py:85] moe_architecture.SparseDecoderLayer.dropout_factory = %DROPOUT_FACTORY
I0512 01:44:51.475493 140316961150976 gin_utils.py:85] moe_architecture.SparseDecoderLayer.encoder_decoder_attention = None
I0512 01:44:51.475525 140316961150976 gin_utils.py:85] moe_architecture.SparseDecoderLayer.extra_mlp = @dense.MlpBlock()
I0512 01:44:51.475557 140316961150976 gin_utils.py:85] moe_architecture.SparseDecoderLayer.layer_norm_factory = @layer_norm.T5LayerNorm
I0512 01:44:51.475588 140316961150976 gin_utils.py:85] moe_architecture.SparseDecoderLayer.mlp = @sparse_decoder/moe_layers.MoeLayer()
I0512 01:44:51.475621 140316961150976 gin_utils.py:85] moe_architecture.SparseDecoderLayer.scanned = False
I0512 01:44:51.475652 140316961150976 gin_utils.py:85] moe_architecture.SparseDecoderLayer.self_attention = \
I0512 01:44:51.475684 140316961150976 gin_utils.py:85]     @dense_attention.MultiHeadDotProductAttention()
I0512 01:44:51.475716 140316961150976 gin_utils.py:85] 
I0512 01:44:51.475748 140316961150976 gin_utils.py:85] # Parameters for layer_norm.T5LayerNorm:
I0512 01:44:51.475781 140316961150976 gin_utils.py:85] # ==============================================================================
I0512 01:44:51.475812 140316961150976 gin_utils.py:85] layer_norm.T5LayerNorm.dtype = %ACTIVATION_DTYPE
I0512 01:44:51.475844 140316961150976 gin_utils.py:85] 
I0512 01:44:51.475882 140316961150976 gin_utils.py:85] # Parameters for routing.TokensChooseMaskedRouter:
I0512 01:44:51.475914 140316961150976 gin_utils.py:85] # ==============================================================================
I0512 01:44:51.475946 140316961150976 gin_utils.py:85] routing.TokensChooseMaskedRouter.dtype = 'float32'
I0512 01:44:51.475997 140316961150976 gin_utils.py:85] routing.TokensChooseMaskedRouter.ignore_padding_tokens = False
I0512 01:44:51.476031 140316961150976 gin_utils.py:85] routing.TokensChooseMaskedRouter.jitter_noise = %JITTER_NOISE
I0512 01:44:51.476063 140316961150976 gin_utils.py:85] routing.TokensChooseMaskedRouter.num_selected_experts = %NUM_SELECTED_EXPERTS
I0512 01:44:51.476095 140316961150976 gin_utils.py:85] routing.TokensChooseMaskedRouter.router_weights = @routing.RouterWeights()
I0512 01:44:51.476129 140316961150976 gin_utils.py:85] 
I0512 01:44:51.476161 140316961150976 gin_utils.py:85] # Parameters for sparse_decoder/routing.TokensChooseMaskedRouter:
I0512 01:44:51.476194 140316961150976 gin_utils.py:85] # ==============================================================================
I0512 01:44:51.476225 140316961150976 gin_utils.py:85] sparse_decoder/routing.TokensChooseMaskedRouter.batch_prioritized_routing = False
I0512 01:44:51.476257 140316961150976 gin_utils.py:85] 
I0512 01:44:51.476288 140316961150976 gin_utils.py:85] # Parameters for sparse_encoder/routing.TokensChooseMaskedRouter:
I0512 01:44:51.476320 140316961150976 gin_utils.py:85] # ==============================================================================
I0512 01:44:51.476351 140316961150976 gin_utils.py:85] sparse_encoder/routing.TokensChooseMaskedRouter.batch_prioritized_routing = False
I0512 01:44:51.476383 140316961150976 gin_utils.py:85] 
I0512 01:44:51.476415 140316961150976 gin_utils.py:85] # Parameters for train_script.train:
I0512 01:44:51.476448 140316961150976 gin_utils.py:85] # ==============================================================================
I0512 01:44:51.476480 140316961150976 gin_utils.py:85] train_script.train.checkpoint_cfg = @utils.CheckpointConfig()
I0512 01:44:51.476512 140316961150976 gin_utils.py:85] train_script.train.eval_period = 2500
I0512 01:44:51.574169 139676058949632 resource_reader.py:37] gin-config opened resource file:/home/rosinality/t5x/t5x/contrib/moe/configs/runs/pretrain.gin
I0512 01:44:51.574679 139676058949632 resource_reader.py:50] system_path_file_exists:t5x/configs/runs/pretrain.gin
I0512 01:44:51.574982 139676058949632 resource_reader.py:37] gin-config opened resource file:/home/rosinality/t5x/t5x/configs/runs/pretrain.gin
I0512 01:44:51.588959 139676058949632 gin_utils.py:83] Gin Configuration:
I0512 01:44:51.604780 139676058949632 gin_utils.py:85] from __gin__ import dynamic_registration
I0512 01:44:51.604849 139676058949632 gin_utils.py:85] import __main__ as train_script
I0512 01:44:51.604890 139676058949632 gin_utils.py:85] from flax import linen
I0512 01:44:51.604924 139676058949632 gin_utils.py:85] import flaxformer
I0512 01:44:51.604957 139676058949632 gin_utils.py:85] from flaxformer.architectures.moe import moe_architecture
I0512 01:44:51.604990 139676058949632 gin_utils.py:85] from flaxformer.architectures.moe import moe_enums
I0512 01:44:51.605022 139676058949632 gin_utils.py:85] from flaxformer.architectures.moe import moe_layers
I0512 01:44:51.605054 139676058949632 gin_utils.py:85] from flaxformer.architectures.moe import routing
I0512 01:44:51.605086 139676058949632 gin_utils.py:85] from flaxformer.architectures.t5 import t5_architecture
I0512 01:44:51.605118 139676058949632 gin_utils.py:85] from flaxformer.components.attention import dense_attention
I0512 01:44:51.605150 139676058949632 gin_utils.py:85] from flaxformer.components.attention import memory_efficient_attention
I0512 01:44:51.605182 139676058949632 gin_utils.py:85] from flaxformer.components import dense
I0512 01:44:51.605214 139676058949632 gin_utils.py:85] from flaxformer.components import embedding
I0512 01:44:51.605246 139676058949632 gin_utils.py:85] from flaxformer.components import layer_norm
I0512 01:44:51.605277 139676058949632 gin_utils.py:85] from gin import config
I0512 01:44:51.605309 139676058949632 gin_utils.py:85] import seqio
I0512 01:44:51.605340 139676058949632 gin_utils.py:85] import t5.data.mixtures
I0512 01:44:51.605378 139676058949632 gin_utils.py:85] from t5x import adafactor
I0512 01:44:51.605417 139676058949632 gin_utils.py:85] from t5x.contrib.moe import adafactor_utils
I0512 01:44:51.605449 139676058949632 gin_utils.py:85] from t5x.contrib.moe import models
I0512 01:44:51.605480 139676058949632 gin_utils.py:85] from t5x.contrib.moe import partitioning
I0512 01:44:51.605512 139676058949632 gin_utils.py:85] from t5x.contrib.moe import trainer as moe_trainer
I0512 01:44:51.605543 139676058949632 gin_utils.py:85] from t5x import gin_utils
I0512 01:44:51.605574 139676058949632 gin_utils.py:85] from t5x import partitioning as partitioning2
I0512 01:44:51.605606 139676058949632 gin_utils.py:85] from t5x import trainer
I0512 01:44:51.605639 139676058949632 gin_utils.py:85] from t5x import utils
I0512 01:44:51.605672 139676058949632 gin_utils.py:85] 
I0512 01:44:51.605704 139676058949632 gin_utils.py:85] # Macros:
I0512 01:44:51.605736 139676058949632 gin_utils.py:85] # ==============================================================================
I0512 01:44:51.605794 139676058949632 gin_utils.py:85] ACTIVATION_DTYPE = 'bfloat16'
I0512 01:44:51.605833 139676058949632 gin_utils.py:85] ACTIVATION_PARTITIONING_DIMS = 1
I0512 01:44:51.605865 139676058949632 gin_utils.py:85] ARCHITECTURE = @t5_architecture.DecoderOnly()
I0512 01:44:51.605897 139676058949632 gin_utils.py:85] AUX_LOSS_FACTOR = 0.01
I0512 01:44:51.605928 139676058949632 gin_utils.py:85] BATCH_SIZE = 384
I0512 01:44:51.605960 139676058949632 gin_utils.py:85] BIAS_INIT = @bias_init/linen.initializers.normal()
I0512 01:44:51.605991 139676058949632 gin_utils.py:85] DECODER_SPARSE_LAYOUT = %flaxformer.architectures.moe.moe_enums.LayerLayout.MIXED
I0512 01:44:51.606023 139676058949632 gin_utils.py:85] DROPOUT_FACTORY = @dropout_factory/linen.Dropout
I0512 01:44:51.606055 139676058949632 gin_utils.py:85] DROPOUT_RATE = 0.0
I0512 01:44:51.606086 139676058949632 gin_utils.py:85] EMBED_DIM = 2048
I0512 01:44:51.606118 139676058949632 gin_utils.py:85] EVAL_EXPERT_CAPACITY_FACTOR = 2.0
I0512 01:44:51.606149 139676058949632 gin_utils.py:85] EXPERT_DROPOUT_RATE = %DROPOUT_RATE
I0512 01:44:51.606180 139676058949632 gin_utils.py:85] EXPERT_MLP_DIM = %MLP_DIM
I0512 01:44:51.606212 139676058949632 gin_utils.py:85] GROUP_SIZE = 4096
I0512 01:44:51.606243 139676058949632 gin_utils.py:85] HEAD_DIM = 128
I0512 01:44:51.606274 139676058949632 gin_utils.py:85] JITTER_NOISE = 0.0
I0512 01:44:51.606305 139676058949632 gin_utils.py:85] LABEL_SMOOTHING = 0.0
I0512 01:44:51.606337 139676058949632 gin_utils.py:85] LOSS_NORMALIZING_FACTOR = 'NUM_REAL_TARGET_TOKENS'
I0512 01:44:51.606368 139676058949632 gin_utils.py:85] MIXTURE_OR_TASK_MODULE = None
I0512 01:44:51.606408 139676058949632 gin_utils.py:85] MIXTURE_OR_TASK_NAME = 'mix_full_lm_test'
I0512 01:44:51.606440 139676058949632 gin_utils.py:85] MLP_DIM = 8192
I0512 01:44:51.606472 139676058949632 gin_utils.py:85] MODEL = @models.MoeDecoderOnlyModel()
I0512 01:44:51.606503 139676058949632 gin_utils.py:85] MODEL_DIR = 'gs://rosinality-tpu-bucket/openmoe_8b/training'
I0512 01:44:51.606535 139676058949632 gin_utils.py:85] MODEL_PARALLEL_SUBMESH = None
I0512 01:44:51.606566 139676058949632 gin_utils.py:85] MOE_TRUNCATED_DTYPE = 'bfloat16'
I0512 01:44:51.606597 139676058949632 gin_utils.py:85] NUM_DECODER_LAYERS = 24
I0512 01:44:51.606630 139676058949632 gin_utils.py:85] NUM_DECODER_SPARSE_LAYERS = 4
I0512 01:44:51.606663 139676058949632 gin_utils.py:85] NUM_EMBEDDINGS = 256384
I0512 01:44:51.606695 139676058949632 gin_utils.py:85] NUM_EXPERT_PARTITIONS = 8
I0512 01:44:51.606726 139676058949632 gin_utils.py:85] NUM_EXPERTS = 8
I0512 01:44:51.606758 139676058949632 gin_utils.py:85] NUM_HEADS = 24
I0512 01:44:51.606789 139676058949632 gin_utils.py:85] NUM_MODEL_PARTITIONS = 4
I0512 01:44:51.606821 139676058949632 gin_utils.py:85] NUM_SELECTED_EXPERTS = 2
I0512 01:44:51.606852 139676058949632 gin_utils.py:85] OPTIMIZER = @adafactor.Adafactor()
I0512 01:44:51.606883 139676058949632 gin_utils.py:85] RANDOM_SEED = None
I0512 01:44:51.606915 139676058949632 gin_utils.py:85] ROUTER_Z_LOSS_FACTOR = 0.0001
I0512 01:44:51.606946 139676058949632 gin_utils.py:85] SCALE = 0.1
I0512 01:44:51.606978 139676058949632 gin_utils.py:85] SHUFFLE_TRAIN_EXAMPLES = True
I0512 01:44:51.607010 139676058949632 gin_utils.py:85] TASK_FEATURE_LENGTHS = {'targets': 2048}
I0512 01:44:51.607041 139676058949632 gin_utils.py:85] TRAIN_EXPERT_CAPACITY_FACTOR = 1.25
I0512 01:44:51.607072 139676058949632 gin_utils.py:85] TRAIN_STEPS = 500000
I0512 01:44:51.607104 139676058949632 gin_utils.py:85] USE_CACHED_TASKS = False
I0512 01:44:51.607135 139676058949632 gin_utils.py:85] USE_HARDWARE_RNG = False
I0512 01:44:51.607167 139676058949632 gin_utils.py:85] VOCABULARY = @seqio.SentencePieceVocabulary()
I0512 01:44:51.607198 139676058949632 gin_utils.py:85] Z_LOSS = 0.0001
I0512 01:44:51.607229 139676058949632 gin_utils.py:85] 
I0512 01:44:51.607261 139676058949632 gin_utils.py:85] # Parameters for adafactor.Adafactor:
I0512 01:44:51.607292 139676058949632 gin_utils.py:85] # ==============================================================================
I0512 01:44:51.607324 139676058949632 gin_utils.py:85] adafactor.Adafactor.decay_rate = 0.8
I0512 01:44:51.607356 139676058949632 gin_utils.py:85] adafactor.Adafactor.logical_factor_rules = @adafactor_utils.logical_factor_rules()
I0512 01:44:51.607393 139676058949632 gin_utils.py:85] adafactor.Adafactor.step_offset = 0
I0512 01:44:51.607426 139676058949632 gin_utils.py:85] 
I0512 01:44:51.570147 139812357228544 gin_utils.py:85] # Parameters for utils.CheckpointConfig:
I0512 01:44:51.570179 139812357228544 gin_utils.py:85] # ==============================================================================
I0512 01:44:51.570211 139812357228544 gin_utils.py:85] utils.CheckpointConfig.restore = @utils.RestoreCheckpointConfig()
I0512 01:44:51.570244 139812357228544 gin_utils.py:85] utils.CheckpointConfig.save = @utils.SaveCheckpointConfig()
I0512 01:44:51.570275 139812357228544 gin_utils.py:85] 
I0512 01:44:51.570307 139812357228544 gin_utils.py:85] # Parameters for moe_partitioning.compute_num_model_partitions:
I0512 01:44:51.570339 139812357228544 gin_utils.py:85] # ==============================================================================
I0512 01:44:51.570371 139812357228544 gin_utils.py:85] moe_partitioning.compute_num_model_partitions.model_parallel_submesh = \
I0512 01:44:51.570403 139812357228544 gin_utils.py:85]     %MODEL_PARALLEL_SUBMESH
I0512 01:44:51.570435 139812357228544 gin_utils.py:85] moe_partitioning.compute_num_model_partitions.num_model_partitions = \
I0512 01:44:51.570467 139812357228544 gin_utils.py:85]     %NUM_MODEL_PARTITIONS
I0512 01:44:51.570499 139812357228544 gin_utils.py:85] 
I0512 01:44:51.570531 139812357228544 gin_utils.py:85] # Parameters for utils.create_learning_rate_scheduler:
I0512 01:44:51.570569 139812357228544 gin_utils.py:85] # ==============================================================================
I0512 01:44:51.570602 139812357228544 gin_utils.py:85] utils.create_learning_rate_scheduler.base_learning_rate = 1.0
I0512 01:44:51.570634 139812357228544 gin_utils.py:85] utils.create_learning_rate_scheduler.factors = 'constant * rsqrt_decay'
I0512 01:44:51.570666 139812357228544 gin_utils.py:85] utils.create_learning_rate_scheduler.warmup_steps = 10000
I0512 01:44:51.570698 139812357228544 gin_utils.py:85] 
I0512 01:44:51.570729 139812357228544 gin_utils.py:85] # Parameters for train/utils.DatasetConfig:
I0512 01:44:51.570761 139812357228544 gin_utils.py:85] # ==============================================================================
I0512 01:44:51.570794 139812357228544 gin_utils.py:85] train/utils.DatasetConfig.batch_size = %BATCH_SIZE
I0512 01:44:51.570827 139812357228544 gin_utils.py:85] train/utils.DatasetConfig.mixture_or_task_name = %MIXTURE_OR_TASK_NAME
I0512 01:44:51.570860 139812357228544 gin_utils.py:85] train/utils.DatasetConfig.module = %MIXTURE_OR_TASK_MODULE
I0512 01:44:51.570892 139812357228544 gin_utils.py:85] train/utils.DatasetConfig.pack = True
I0512 01:44:51.570923 139812357228544 gin_utils.py:85] train/utils.DatasetConfig.seed = 42
I0512 01:44:51.570955 139812357228544 gin_utils.py:85] train/utils.DatasetConfig.shuffle = %SHUFFLE_TRAIN_EXAMPLES
I0512 01:44:51.570987 139812357228544 gin_utils.py:85] train/utils.DatasetConfig.split = 'train'
I0512 01:44:51.571019 139812357228544 gin_utils.py:85] train/utils.DatasetConfig.task_feature_lengths = %TASK_FEATURE_LENGTHS
I0512 01:44:51.571051 139812357228544 gin_utils.py:85] train/utils.DatasetConfig.use_cached = %USE_CACHED_TASKS
I0512 01:44:51.571083 139812357228544 gin_utils.py:85] 
I0512 01:44:51.571115 139812357228544 gin_utils.py:85] # Parameters for train_eval/utils.DatasetConfig:
I0512 01:44:51.571147 139812357228544 gin_utils.py:85] # ==============================================================================
I0512 01:44:51.571179 139812357228544 gin_utils.py:85] train_eval/utils.DatasetConfig.batch_size = 128
I0512 01:44:51.571211 139812357228544 gin_utils.py:85] train_eval/utils.DatasetConfig.mixture_or_task_name = %MIXTURE_OR_TASK_NAME
I0512 01:44:51.571242 139812357228544 gin_utils.py:85] train_eval/utils.DatasetConfig.module = %MIXTURE_OR_TASK_MODULE
I0512 01:44:51.571274 139812357228544 gin_utils.py:85] train_eval/utils.DatasetConfig.pack = True
I0512 01:44:51.571306 139812357228544 gin_utils.py:85] train_eval/utils.DatasetConfig.seed = 42
I0512 01:44:51.571338 139812357228544 gin_utils.py:85] train_eval/utils.DatasetConfig.shuffle = False
I0512 01:44:51.571369 139812357228544 gin_utils.py:85] train_eval/utils.DatasetConfig.split = 'validation'
I0512 01:44:51.571401 139812357228544 gin_utils.py:85] train_eval/utils.DatasetConfig.task_feature_lengths = %TASK_FEATURE_LENGTHS
I0512 01:44:51.571434 139812357228544 gin_utils.py:85] train_eval/utils.DatasetConfig.use_cached = %USE_CACHED_TASKS
I0512 01:44:51.571466 139812357228544 gin_utils.py:85] 
I0512 01:44:51.571497 139812357228544 gin_utils.py:85] # Parameters for t5_architecture.DecoderLayer:
I0512 01:44:51.571529 139812357228544 gin_utils.py:85] # ==============================================================================
I0512 01:44:51.571568 139812357228544 gin_utils.py:85] t5_architecture.DecoderLayer.activation_partitioning_dims = \
I0512 01:44:51.571600 139812357228544 gin_utils.py:85]     %ACTIVATION_PARTITIONING_DIMS
I0512 01:44:51.571631 139812357228544 gin_utils.py:85] t5_architecture.DecoderLayer.dropout_factory = %DROPOUT_FACTORY
I0512 01:44:51.571663 139812357228544 gin_utils.py:85] t5_architecture.DecoderLayer.encoder_decoder_attention = None
I0512 01:44:51.571695 139812357228544 gin_utils.py:85] t5_architecture.DecoderLayer.layer_norm_factory = @layer_norm.T5LayerNorm
I0512 01:44:51.571727 139812357228544 gin_utils.py:85] t5_architecture.DecoderLayer.mlp = @dense.MlpBlock()
I0512 01:44:51.571758 139812357228544 gin_utils.py:85] t5_architecture.DecoderLayer.scanned = False
I0512 01:44:51.571791 139812357228544 gin_utils.py:85] t5_architecture.DecoderLayer.self_attention = \
I0512 01:44:51.571825 139812357228544 gin_utils.py:85]     @dense_attention.MultiHeadDotProductAttention()
I0512 01:44:51.571857 139812357228544 gin_utils.py:85] 
I0512 01:44:51.571889 139812357228544 gin_utils.py:85] # Parameters for t5_architecture.DecoderOnly:
I0512 01:44:51.571921 139812357228544 gin_utils.py:85] # ==============================================================================
I0512 01:44:51.571954 139812357228544 gin_utils.py:85] t5_architecture.DecoderOnly.decoder_factory = @moe_architecture.SparseDecoder
I0512 01:44:51.571985 139812357228544 gin_utils.py:85] t5_architecture.DecoderOnly.dtype = %ACTIVATION_DTYPE
I0512 01:44:51.572017 139812357228544 gin_utils.py:85] t5_architecture.DecoderOnly.shared_token_embedder_factory = @embedding.Embed
I0512 01:44:51.572049 139812357228544 gin_utils.py:85] 
I0512 01:44:51.572081 139812357228544 gin_utils.py:85] # Parameters for output_logits/dense.DenseGeneral:
I0512 01:44:51.572113 139812357228544 gin_utils.py:85] # ==============================================================================
I0512 01:44:51.572145 139812357228544 gin_utils.py:85] output_logits/dense.DenseGeneral.bias_init = %BIAS_INIT
I0512 01:44:51.572198 139812357228544 gin_utils.py:85] output_logits/dense.DenseGeneral.dtype = 'float32'
I0512 01:44:51.572233 139812357228544 gin_utils.py:85] output_logits/dense.DenseGeneral.features = %NUM_EMBEDDINGS
I0512 01:44:51.572266 139812357228544 gin_utils.py:85] output_logits/dense.DenseGeneral.kernel_axis_names = ['embed', 'vocab']
I0512 01:44:51.572298 139812357228544 gin_utils.py:85] output_logits/dense.DenseGeneral.kernel_init = \
I0512 01:44:51.572330 139812357228544 gin_utils.py:85]     @output_logits_kernel_init/linen.initializers.variance_scaling()
I0512 01:44:51.572362 139812357228544 gin_utils.py:85] output_logits/dense.DenseGeneral.use_bias = False
I0512 01:44:51.572393 139812357228544 gin_utils.py:85] 
I0512 01:44:51.572425 139812357228544 gin_utils.py:85] # Parameters for dropout_factory/linen.Dropout:
I0512 01:44:51.572457 139812357228544 gin_utils.py:85] # ==============================================================================
I0512 01:44:51.572489 139812357228544 gin_utils.py:85] dropout_factory/linen.Dropout.broadcast_dims = (-2,)
I0512 01:44:51.572521 139812357228544 gin_utils.py:85] dropout_factory/linen.Dropout.rate = %DROPOUT_RATE
I0512 01:44:51.572558 139812357228544 gin_utils.py:85] 
I0512 01:44:51.572592 139812357228544 gin_utils.py:85] # Parameters for embedding.Embed:
I0512 01:44:51.572624 139812357228544 gin_utils.py:85] # ==============================================================================
I0512 01:44:51.572656 139812357228544 gin_utils.py:85] embedding.Embed.attend_dtype = 'float32'
I0512 01:44:51.572687 139812357228544 gin_utils.py:85] embedding.Embed.cast_input_dtype = 'int32'
I0512 01:44:51.572719 139812357228544 gin_utils.py:85] embedding.Embed.dtype = %ACTIVATION_DTYPE
I0512 01:44:51.572751 139812357228544 gin_utils.py:85] embedding.Embed.embedding_init = @token_embedder_init/linen.initializers.normal()
I0512 01:44:51.572783 139812357228544 gin_utils.py:85] embedding.Embed.features = %EMBED_DIM
I0512 01:44:51.572817 139812357228544 gin_utils.py:85] embedding.Embed.name = 'token_embedder'
I0512 01:44:51.572849 139812357228544 gin_utils.py:85] embedding.Embed.num_embeddings = %NUM_EMBEDDINGS
I0512 01:44:51.572881 139812357228544 gin_utils.py:85] embedding.Embed.one_hot = True
I0512 01:44:51.572913 139812357228544 gin_utils.py:85] 
I0512 01:44:51.572945 139812357228544 gin_utils.py:85] # Parameters for dense.MlpBlock:
I0512 01:44:51.572978 139812357228544 gin_utils.py:85] # ==============================================================================
I0512 01:44:51.573010 139812357228544 gin_utils.py:85] dense.MlpBlock.activations = ('swiglu', 'linear')
I0512 01:44:51.573042 139812357228544 gin_utils.py:85] dense.MlpBlock.bias_init = %BIAS_INIT
I0512 01:44:51.573074 139812357228544 gin_utils.py:85] dense.MlpBlock.dtype = %ACTIVATION_DTYPE
I0512 01:44:51.573106 139812357228544 gin_utils.py:85] dense.MlpBlock.final_dropout_rate = 0
I0512 01:44:51.573138 139812357228544 gin_utils.py:85] dense.MlpBlock.input_axis_name = 'mlp_embed'
I0512 01:44:51.573170 139812357228544 gin_utils.py:85] dense.MlpBlock.intermediate_dim = %MLP_DIM
I0512 01:44:51.573201 139812357228544 gin_utils.py:85] dense.MlpBlock.intermediate_dropout_rate = %DROPOUT_RATE
I0512 01:44:51.476544 140316961150976 gin_utils.py:85] train_script.train.eval_steps = 20
I0512 01:44:51.476576 140316961150976 gin_utils.py:85] train_script.train.infer_eval_dataset_cfg = None
I0512 01:44:51.476607 140316961150976 gin_utils.py:85] train_script.train.model = %MODEL
I0512 01:44:51.476639 140316961150976 gin_utils.py:85] train_script.train.model_dir = %MODEL_DIR
I0512 01:44:51.476670 140316961150976 gin_utils.py:85] train_script.train.partitioner = @moe_partitioning.MoePjitPartitioner()
I0512 01:44:51.476701 140316961150976 gin_utils.py:85] train_script.train.random_seed = 42
I0512 01:44:51.476733 140316961150976 gin_utils.py:85] train_script.train.stats_period = 10
I0512 01:44:51.476764 140316961150976 gin_utils.py:85] train_script.train.summarize_config_fn = @gin_utils.summarize_gin_config
I0512 01:44:51.476796 140316961150976 gin_utils.py:85] train_script.train.total_steps = %TRAIN_STEPS
I0512 01:44:51.476828 140316961150976 gin_utils.py:85] train_script.train.train_dataset_cfg = @train/utils.DatasetConfig()
I0512 01:44:51.476866 140316961150976 gin_utils.py:85] train_script.train.train_eval_dataset_cfg = None
I0512 01:44:51.476899 140316961150976 gin_utils.py:85] train_script.train.trainer_cls = @moe_trainer.MoeTrainer
I0512 01:44:51.476931 140316961150976 gin_utils.py:85] train_script.train.use_hardware_rng = %USE_HARDWARE_RNG
I0512 01:44:51.476962 140316961150976 gin_utils.py:85] 
I0512 01:44:51.476994 140316961150976 gin_utils.py:85] # Parameters for trainer.Trainer:
I0512 01:44:51.477026 140316961150976 gin_utils.py:85] # ==============================================================================
I0512 01:44:51.477057 140316961150976 gin_utils.py:85] trainer.Trainer.learning_rate_fn = @utils.create_learning_rate_scheduler()
I0512 01:44:51.477089 140316961150976 gin_utils.py:85] trainer.Trainer.num_microbatches = None
I0512 01:44:51.477123 140316961150976 gin_utils.py:85] 
I0512 01:44:51.477155 140316961150976 gin_utils.py:85] # Parameters for attention_kernel_init/linen.initializers.variance_scaling:
I0512 01:44:51.477187 140316961150976 gin_utils.py:85] # ==============================================================================
I0512 01:44:51.477219 140316961150976 gin_utils.py:85] attention_kernel_init/linen.initializers.variance_scaling.distribution = 'normal'
I0512 01:44:51.477251 140316961150976 gin_utils.py:85] attention_kernel_init/linen.initializers.variance_scaling.mode = 'fan_in'
I0512 01:44:51.477282 140316961150976 gin_utils.py:85] attention_kernel_init/linen.initializers.variance_scaling.scale = %SCALE
I0512 01:44:51.477314 140316961150976 gin_utils.py:85] 
I0512 01:44:51.477345 140316961150976 gin_utils.py:85] # Parameters for expert_kernel_init/linen.initializers.variance_scaling:
I0512 01:44:51.477377 140316961150976 gin_utils.py:85] # ==============================================================================
I0512 01:44:51.477408 140316961150976 gin_utils.py:85] expert_kernel_init/linen.initializers.variance_scaling.distribution = 'normal'
I0512 01:44:51.477440 140316961150976 gin_utils.py:85] expert_kernel_init/linen.initializers.variance_scaling.mode = 'fan_in'
I0512 01:44:51.477471 140316961150976 gin_utils.py:85] expert_kernel_init/linen.initializers.variance_scaling.scale = %SCALE
I0512 01:44:51.477503 140316961150976 gin_utils.py:85] 
I0512 01:44:51.477535 140316961150976 gin_utils.py:85] # Parameters for mlp_kernel_init/linen.initializers.variance_scaling:
I0512 01:44:51.477590 140316961150976 gin_utils.py:85] # ==============================================================================
I0512 01:44:51.477624 140316961150976 gin_utils.py:85] mlp_kernel_init/linen.initializers.variance_scaling.distribution = \
I0512 01:44:51.477656 140316961150976 gin_utils.py:85]     'truncated_normal'
I0512 01:44:51.477688 140316961150976 gin_utils.py:85] mlp_kernel_init/linen.initializers.variance_scaling.mode = 'fan_in'
I0512 01:44:51.477720 140316961150976 gin_utils.py:85] mlp_kernel_init/linen.initializers.variance_scaling.scale = %SCALE
I0512 01:44:51.477752 140316961150976 gin_utils.py:85] 
I0512 01:44:51.477783 140316961150976 gin_utils.py:85] # Parameters for output_logits_kernel_init/linen.initializers.variance_scaling:
I0512 01:44:51.477815 140316961150976 gin_utils.py:85] # ==============================================================================
I0512 01:44:51.477847 140316961150976 gin_utils.py:85] output_logits_kernel_init/linen.initializers.variance_scaling.distribution = \
I0512 01:44:51.477886 140316961150976 gin_utils.py:85]     'truncated_normal'
I0512 01:44:51.477918 140316961150976 gin_utils.py:85] output_logits_kernel_init/linen.initializers.variance_scaling.mode = 'fan_in'
I0512 01:44:51.477949 140316961150976 gin_utils.py:85] output_logits_kernel_init/linen.initializers.variance_scaling.scale = %SCALE
I0512 01:44:51.478858 140316961150976 partitioning.py:559] `activation_partitioning_dims` = 2, `parameter_partitioning_dims` = 1
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
I0000 00:00:1715478291.560076  382697 pjrt_api.cc:98] GetPjrtApi was found for tpu at /home/rosinality/openmoe_venv/lib/python3.10/site-packages/libtpu/libtpu.so
I0000 00:00:1715478291.560136  382697 pjrt_api.cc:67] PJRT_Api is set for device type tpu
I0000 00:00:1715478291.560138  382697 pjrt_api.cc:72] PJRT plugin for tpu has PJRT API version 0.30. The framework PJRT API version is 0.30.
I0512 01:44:51.607458 139676058949632 gin_utils.py:85] # Parameters for utils.CheckpointConfig:
I0512 01:44:51.607489 139676058949632 gin_utils.py:85] # ==============================================================================
I0512 01:44:51.607521 139676058949632 gin_utils.py:85] utils.CheckpointConfig.restore = @utils.RestoreCheckpointConfig()
I0512 01:44:51.607553 139676058949632 gin_utils.py:85] utils.CheckpointConfig.save = @utils.SaveCheckpointConfig()
I0512 01:44:51.607585 139676058949632 gin_utils.py:85] 
I0512 01:44:51.607616 139676058949632 gin_utils.py:85] # Parameters for partitioning.compute_num_model_partitions:
I0512 01:44:51.607650 139676058949632 gin_utils.py:85] # ==============================================================================
I0512 01:44:51.607683 139676058949632 gin_utils.py:85] partitioning.compute_num_model_partitions.model_parallel_submesh = \
I0512 01:44:51.607714 139676058949632 gin_utils.py:85]     %MODEL_PARALLEL_SUBMESH
I0512 01:44:51.607746 139676058949632 gin_utils.py:85] partitioning.compute_num_model_partitions.num_model_partitions = \
I0512 01:44:51.607778 139676058949632 gin_utils.py:85]     %NUM_MODEL_PARTITIONS
I0512 01:44:51.607809 139676058949632 gin_utils.py:85] 
I0512 01:44:51.607840 139676058949632 gin_utils.py:85] # Parameters for utils.create_learning_rate_scheduler:
I0512 01:44:51.607872 139676058949632 gin_utils.py:85] # ==============================================================================
I0512 01:44:51.607903 139676058949632 gin_utils.py:85] utils.create_learning_rate_scheduler.base_learning_rate = 1.0
I0512 01:44:51.607935 139676058949632 gin_utils.py:85] utils.create_learning_rate_scheduler.factors = 'constant * rsqrt_decay'
I0512 01:44:51.607967 139676058949632 gin_utils.py:85] utils.create_learning_rate_scheduler.warmup_steps = 10000
I0512 01:44:51.607998 139676058949632 gin_utils.py:85] 
I0512 01:44:51.608030 139676058949632 gin_utils.py:85] # Parameters for train/utils.DatasetConfig:
I0512 01:44:51.608061 139676058949632 gin_utils.py:85] # ==============================================================================
I0512 01:44:51.608093 139676058949632 gin_utils.py:85] train/utils.DatasetConfig.batch_size = %BATCH_SIZE
I0512 01:44:51.608124 139676058949632 gin_utils.py:85] train/utils.DatasetConfig.mixture_or_task_name = %MIXTURE_OR_TASK_NAME
I0512 01:44:51.608155 139676058949632 gin_utils.py:85] train/utils.DatasetConfig.module = %MIXTURE_OR_TASK_MODULE
I0512 01:44:51.608187 139676058949632 gin_utils.py:85] train/utils.DatasetConfig.pack = True
I0512 01:44:51.608218 139676058949632 gin_utils.py:85] train/utils.DatasetConfig.seed = 42
I0512 01:44:51.608250 139676058949632 gin_utils.py:85] train/utils.DatasetConfig.shuffle = %SHUFFLE_TRAIN_EXAMPLES
I0512 01:44:51.608281 139676058949632 gin_utils.py:85] train/utils.DatasetConfig.split = 'train'
I0512 01:44:51.608313 139676058949632 gin_utils.py:85] train/utils.DatasetConfig.task_feature_lengths = %TASK_FEATURE_LENGTHS
I0512 01:44:51.608344 139676058949632 gin_utils.py:85] train/utils.DatasetConfig.use_cached = %USE_CACHED_TASKS
I0512 01:44:51.608381 139676058949632 gin_utils.py:85] 
I0512 01:44:51.608414 139676058949632 gin_utils.py:85] # Parameters for train_eval/utils.DatasetConfig:
I0512 01:44:51.608446 139676058949632 gin_utils.py:85] # ==============================================================================
I0512 01:44:51.608478 139676058949632 gin_utils.py:85] train_eval/utils.DatasetConfig.batch_size = 128
I0512 01:44:51.608510 139676058949632 gin_utils.py:85] train_eval/utils.DatasetConfig.mixture_or_task_name = %MIXTURE_OR_TASK_NAME
I0512 01:44:51.608541 139676058949632 gin_utils.py:85] train_eval/utils.DatasetConfig.module = %MIXTURE_OR_TASK_MODULE
I0512 01:44:51.608573 139676058949632 gin_utils.py:85] train_eval/utils.DatasetConfig.pack = True
I0512 01:44:51.608604 139676058949632 gin_utils.py:85] train_eval/utils.DatasetConfig.seed = 42
I0512 01:44:51.608637 139676058949632 gin_utils.py:85] train_eval/utils.DatasetConfig.shuffle = False
I0512 01:44:51.608670 139676058949632 gin_utils.py:85] train_eval/utils.DatasetConfig.split = 'validation'
I0512 01:44:51.608701 139676058949632 gin_utils.py:85] train_eval/utils.DatasetConfig.task_feature_lengths = %TASK_FEATURE_LENGTHS
I0512 01:44:51.608733 139676058949632 gin_utils.py:85] train_eval/utils.DatasetConfig.use_cached = %USE_CACHED_TASKS
I0512 01:44:51.608764 139676058949632 gin_utils.py:85] 
I0512 01:44:51.608796 139676058949632 gin_utils.py:85] # Parameters for t5_architecture.DecoderLayer:
I0512 01:44:51.608828 139676058949632 gin_utils.py:85] # ==============================================================================
I0512 01:44:51.608859 139676058949632 gin_utils.py:85] t5_architecture.DecoderLayer.activation_partitioning_dims = \
I0512 01:44:51.608891 139676058949632 gin_utils.py:85]     %ACTIVATION_PARTITIONING_DIMS
I0512 01:44:51.608922 139676058949632 gin_utils.py:85] t5_architecture.DecoderLayer.dropout_factory = %DROPOUT_FACTORY
I0512 01:44:51.608954 139676058949632 gin_utils.py:85] t5_architecture.DecoderLayer.encoder_decoder_attention = None
I0512 01:44:51.608985 139676058949632 gin_utils.py:85] t5_architecture.DecoderLayer.layer_norm_factory = @layer_norm.T5LayerNorm
I0512 01:44:51.609017 139676058949632 gin_utils.py:85] t5_architecture.DecoderLayer.mlp = @dense.MlpBlock()
I0512 01:44:51.609048 139676058949632 gin_utils.py:85] t5_architecture.DecoderLayer.scanned = False
I0512 01:44:51.609079 139676058949632 gin_utils.py:85] t5_architecture.DecoderLayer.self_attention = \
I0512 01:44:51.609111 139676058949632 gin_utils.py:85]     @dense_attention.MultiHeadDotProductAttention()
I0512 01:44:51.609142 139676058949632 gin_utils.py:85] 
I0512 01:44:51.609174 139676058949632 gin_utils.py:85] # Parameters for t5_architecture.DecoderOnly:
I0512 01:44:51.609205 139676058949632 gin_utils.py:85] # ==============================================================================
I0512 01:44:51.609236 139676058949632 gin_utils.py:85] t5_architecture.DecoderOnly.decoder_factory = @moe_architecture.SparseDecoder
I0512 01:44:51.609268 139676058949632 gin_utils.py:85] t5_architecture.DecoderOnly.dtype = %ACTIVATION_DTYPE
I0512 01:44:51.609299 139676058949632 gin_utils.py:85] t5_architecture.DecoderOnly.shared_token_embedder_factory = @embedding.Embed
I0512 01:44:51.609331 139676058949632 gin_utils.py:85] 
I0512 01:44:51.609362 139676058949632 gin_utils.py:85] # Parameters for output_logits/dense.DenseGeneral:
I0512 01:44:51.609400 139676058949632 gin_utils.py:85] # ==============================================================================
I0512 01:44:51.609432 139676058949632 gin_utils.py:85] output_logits/dense.DenseGeneral.bias_init = %BIAS_INIT
I0512 01:44:51.609464 139676058949632 gin_utils.py:85] output_logits/dense.DenseGeneral.dtype = 'float32'
I0512 01:44:51.609495 139676058949632 gin_utils.py:85] output_logits/dense.DenseGeneral.features = %NUM_EMBEDDINGS
I0512 01:44:51.609527 139676058949632 gin_utils.py:85] output_logits/dense.DenseGeneral.kernel_axis_names = ['embed', 'vocab']
I0512 01:44:51.609558 139676058949632 gin_utils.py:85] output_logits/dense.DenseGeneral.kernel_init = \
I0512 01:44:51.609590 139676058949632 gin_utils.py:85]     @output_logits_kernel_init/linen.initializers.variance_scaling()
I0512 01:44:51.609622 139676058949632 gin_utils.py:85] output_logits/dense.DenseGeneral.use_bias = False
I0512 01:44:51.609655 139676058949632 gin_utils.py:85] 
I0512 01:44:51.609687 139676058949632 gin_utils.py:85] # Parameters for dropout_factory/linen.Dropout:
I0512 01:44:51.609719 139676058949632 gin_utils.py:85] # ==============================================================================
I0512 01:44:51.609750 139676058949632 gin_utils.py:85] dropout_factory/linen.Dropout.broadcast_dims = (-2,)
I0512 01:44:51.609808 139676058949632 gin_utils.py:85] dropout_factory/linen.Dropout.rate = %DROPOUT_RATE
I0512 01:44:51.609842 139676058949632 gin_utils.py:85] 
I0512 01:44:51.609874 139676058949632 gin_utils.py:85] # Parameters for embedding.Embed:
I0512 01:44:51.609906 139676058949632 gin_utils.py:85] # ==============================================================================
I0512 01:44:51.609938 139676058949632 gin_utils.py:85] embedding.Embed.attend_dtype = 'float32'
I0512 01:44:51.609969 139676058949632 gin_utils.py:85] embedding.Embed.cast_input_dtype = 'int32'
I0512 01:44:51.610001 139676058949632 gin_utils.py:85] embedding.Embed.dtype = %ACTIVATION_DTYPE
I0512 01:44:51.610033 139676058949632 gin_utils.py:85] embedding.Embed.embedding_init = @token_embedder_init/linen.initializers.normal()
I0512 01:44:51.610065 139676058949632 gin_utils.py:85] embedding.Embed.features = %EMBED_DIM
I0512 01:44:51.610096 139676058949632 gin_utils.py:85] embedding.Embed.name = 'token_embedder'
I0512 01:44:51.610128 139676058949632 gin_utils.py:85] embedding.Embed.num_embeddings = %NUM_EMBEDDINGS
I0512 01:44:51.610160 139676058949632 gin_utils.py:85] embedding.Embed.one_hot = True
I0512 01:44:51.610192 139676058949632 gin_utils.py:85] 
I0512 01:44:51.610223 139676058949632 gin_utils.py:85] # Parameters for dense.MlpBlock:
I0512 01:44:51.610255 139676058949632 gin_utils.py:85] # ==============================================================================
I0512 01:44:51.610287 139676058949632 gin_utils.py:85] dense.MlpBlock.activations = ('swiglu', 'linear')
I0512 01:44:51.610319 139676058949632 gin_utils.py:85] dense.MlpBlock.bias_init = %BIAS_INIT
I0512 01:44:51.610351 139676058949632 gin_utils.py:85] dense.MlpBlock.dtype = %ACTIVATION_DTYPE
I0512 01:44:51.610388 139676058949632 gin_utils.py:85] dense.MlpBlock.final_dropout_rate = 0
I0512 01:44:51.610420 139676058949632 gin_utils.py:85] dense.MlpBlock.input_axis_name = 'mlp_embed'
I0512 01:44:51.610452 139676058949632 gin_utils.py:85] dense.MlpBlock.intermediate_dim = %MLP_DIM
I0512 01:44:51.610483 139676058949632 gin_utils.py:85] dense.MlpBlock.intermediate_dropout_rate = %DROPOUT_RATE
I0512 01:44:51.388003 140577131780096 gin_utils.py:85] train_script.train.infer_eval_dataset_cfg = None
I0512 01:44:51.388037 140577131780096 gin_utils.py:85] train_script.train.model = %MODEL
I0512 01:44:51.388072 140577131780096 gin_utils.py:85] train_script.train.model_dir = %MODEL_DIR
I0512 01:44:51.388105 140577131780096 gin_utils.py:85] train_script.train.partitioner = @partitioning.MoePjitPartitioner()
I0512 01:44:51.388138 140577131780096 gin_utils.py:85] train_script.train.random_seed = 42
I0512 01:44:51.388170 140577131780096 gin_utils.py:85] train_script.train.stats_period = 10
I0512 01:44:51.388203 140577131780096 gin_utils.py:85] train_script.train.summarize_config_fn = @gin_utils.summarize_gin_config
I0512 01:44:51.388236 140577131780096 gin_utils.py:85] train_script.train.total_steps = %TRAIN_STEPS
I0512 01:44:51.388269 140577131780096 gin_utils.py:85] train_script.train.train_dataset_cfg = @train/utils.DatasetConfig()
I0512 01:44:51.388301 140577131780096 gin_utils.py:85] train_script.train.train_eval_dataset_cfg = None
I0512 01:44:51.388334 140577131780096 gin_utils.py:85] train_script.train.trainer_cls = @moe_trainer.MoeTrainer
I0512 01:44:51.388366 140577131780096 gin_utils.py:85] train_script.train.use_hardware_rng = %USE_HARDWARE_RNG
I0512 01:44:51.388399 140577131780096 gin_utils.py:85] 
I0512 01:44:51.388431 140577131780096 gin_utils.py:85] # Parameters for trainer.Trainer:
I0512 01:44:51.388464 140577131780096 gin_utils.py:85] # ==============================================================================
I0512 01:44:51.388496 140577131780096 gin_utils.py:85] trainer.Trainer.learning_rate_fn = @utils.create_learning_rate_scheduler()
I0512 01:44:51.388529 140577131780096 gin_utils.py:85] trainer.Trainer.num_microbatches = None
I0512 01:44:51.388561 140577131780096 gin_utils.py:85] 
I0512 01:44:51.388594 140577131780096 gin_utils.py:85] # Parameters for attention_kernel_init/linen.initializers.variance_scaling:
I0512 01:44:51.573233 139812357228544 gin_utils.py:85] dense.MlpBlock.kernel_init = @mlp_kernel_init/linen.initializers.variance_scaling()
I0512 01:44:51.573266 139812357228544 gin_utils.py:85] dense.MlpBlock.output_axis_name = 'mlp_embed'
I0512 01:44:51.573297 139812357228544 gin_utils.py:85] dense.MlpBlock.use_bias = False
I0512 01:44:51.573329 139812357228544 gin_utils.py:85] 
I0512 01:44:51.573361 139812357228544 gin_utils.py:85] # Parameters for expert/dense.MlpBlock:
I0512 01:44:51.573393 139812357228544 gin_utils.py:85] # ==============================================================================
I0512 01:44:51.573426 139812357228544 gin_utils.py:85] expert/dense.MlpBlock.activation_partitioning_dims = 1
I0512 01:44:51.573457 139812357228544 gin_utils.py:85] expert/dense.MlpBlock.activations = ('swiglu', 'linear')
I0512 01:44:51.573489 139812357228544 gin_utils.py:85] expert/dense.MlpBlock.bias_init = %BIAS_INIT
I0512 01:44:51.573521 139812357228544 gin_utils.py:85] expert/dense.MlpBlock.data_sharding_constraints = ('expert_replicas', 'mlp')
I0512 01:44:51.573558 139812357228544 gin_utils.py:85] expert/dense.MlpBlock.dtype = %MOE_TRUNCATED_DTYPE
I0512 01:44:51.573591 139812357228544 gin_utils.py:85] expert/dense.MlpBlock.final_dropout_rate = 0.0
I0512 01:44:51.573624 139812357228544 gin_utils.py:85] expert/dense.MlpBlock.input_axis_name = 'mlp_embed'
I0512 01:44:51.573656 139812357228544 gin_utils.py:85] expert/dense.MlpBlock.intermediate_axis_name = 'expert_mlp'
I0512 01:44:51.573688 139812357228544 gin_utils.py:85] expert/dense.MlpBlock.intermediate_dim = %EXPERT_MLP_DIM
I0512 01:44:51.573719 139812357228544 gin_utils.py:85] expert/dense.MlpBlock.intermediate_dropout_rate = %EXPERT_DROPOUT_RATE
I0512 01:44:51.573752 139812357228544 gin_utils.py:85] expert/dense.MlpBlock.kernel_init = \
I0512 01:44:51.573784 139812357228544 gin_utils.py:85]     @expert_kernel_init/linen.initializers.variance_scaling()
I0512 01:44:51.573818 139812357228544 gin_utils.py:85] expert/dense.MlpBlock.output_axis_name = 'mlp_embed'
I0512 01:44:51.388626 140577131780096 gin_utils.py:85] # ==============================================================================
I0512 01:44:51.388659 140577131780096 gin_utils.py:85] attention_kernel_init/linen.initializers.variance_scaling.distribution = 'normal'
I0512 01:44:51.388691 140577131780096 gin_utils.py:85] attention_kernel_init/linen.initializers.variance_scaling.mode = 'fan_in'
I0512 01:44:51.388723 140577131780096 gin_utils.py:85] attention_kernel_init/linen.initializers.variance_scaling.scale = %SCALE
I0512 01:44:51.388756 140577131780096 gin_utils.py:85] 
I0512 01:44:51.388794 140577131780096 gin_utils.py:85] # Parameters for expert_kernel_init/linen.initializers.variance_scaling:
I0512 01:44:51.388828 140577131780096 gin_utils.py:85] # ==============================================================================
I0512 01:44:51.388860 140577131780096 gin_utils.py:85] expert_kernel_init/linen.initializers.variance_scaling.distribution = 'normal'
I0512 01:44:51.388893 140577131780096 gin_utils.py:85] expert_kernel_init/linen.initializers.variance_scaling.mode = 'fan_in'
I0512 01:44:51.388926 140577131780096 gin_utils.py:85] expert_kernel_init/linen.initializers.variance_scaling.scale = %SCALE
I0512 01:44:51.388959 140577131780096 gin_utils.py:85] 
I0512 01:44:51.388991 140577131780096 gin_utils.py:85] # Parameters for mlp_kernel_init/linen.initializers.variance_scaling:
I0512 01:44:51.389024 140577131780096 gin_utils.py:85] # ==============================================================================
I0512 01:44:51.389059 140577131780096 gin_utils.py:85] mlp_kernel_init/linen.initializers.variance_scaling.distribution = \
I0512 01:44:51.389092 140577131780096 gin_utils.py:85]     'truncated_normal'
I0512 01:44:51.389125 140577131780096 gin_utils.py:85] mlp_kernel_init/linen.initializers.variance_scaling.mode = 'fan_in'
I0512 01:44:51.389157 140577131780096 gin_utils.py:85] mlp_kernel_init/linen.initializers.variance_scaling.scale = %SCALE
I0512 01:44:51.389190 140577131780096 gin_utils.py:85] 
I0512 01:44:51.389222 140577131780096 gin_utils.py:85] # Parameters for output_logits_kernel_init/linen.initializers.variance_scaling:
I0512 01:44:51.389255 140577131780096 gin_utils.py:85] # ==============================================================================
I0512 01:44:51.389287 140577131780096 gin_utils.py:85] output_logits_kernel_init/linen.initializers.variance_scaling.distribution = \
I0512 01:44:51.389320 140577131780096 gin_utils.py:85]     'truncated_normal'
I0512 01:44:51.389352 140577131780096 gin_utils.py:85] output_logits_kernel_init/linen.initializers.variance_scaling.mode = 'fan_in'
I0512 01:44:51.389384 140577131780096 gin_utils.py:85] output_logits_kernel_init/linen.initializers.variance_scaling.scale = %SCALE
I0512 01:44:51.390297 140577131780096 partitioning.py:559] `activation_partitioning_dims` = 2, `parameter_partitioning_dims` = 1
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
I0000 00:00:1715478291.433789  355987 pjrt_api.cc:98] GetPjrtApi was found for tpu at /home/rosinality/openmoe_venv/lib/python3.10/site-packages/libtpu/libtpu.so
I0000 00:00:1715478291.433841  355987 pjrt_api.cc:67] PJRT_Api is set for device type tpu
I0000 00:00:1715478291.433845  355987 pjrt_api.cc:72] PJRT plugin for tpu has PJRT API version 0.30. The framework PJRT API version is 0.30.
I0512 01:44:51.610515 139676058949632 gin_utils.py:85] dense.MlpBlock.kernel_init = @mlp_kernel_init/linen.initializers.variance_scaling()
I0512 01:44:51.610546 139676058949632 gin_utils.py:85] dense.MlpBlock.output_axis_name = 'mlp_embed'
I0512 01:44:51.610578 139676058949632 gin_utils.py:85] dense.MlpBlock.use_bias = False
I0512 01:44:51.610610 139676058949632 gin_utils.py:85] 
I0512 01:44:51.610643 139676058949632 gin_utils.py:85] # Parameters for expert/dense.MlpBlock:
I0512 01:44:51.610676 139676058949632 gin_utils.py:85] # ==============================================================================
I0512 01:44:51.610707 139676058949632 gin_utils.py:85] expert/dense.MlpBlock.activation_partitioning_dims = 1
I0512 01:44:51.610739 139676058949632 gin_utils.py:85] expert/dense.MlpBlock.activations = ('swiglu', 'linear')
I0512 01:44:51.610771 139676058949632 gin_utils.py:85] expert/dense.MlpBlock.bias_init = %BIAS_INIT
I0512 01:44:51.610802 139676058949632 gin_utils.py:85] expert/dense.MlpBlock.data_sharding_constraints = ('expert_replicas', 'mlp')
I0512 01:44:51.610834 139676058949632 gin_utils.py:85] expert/dense.MlpBlock.dtype = %MOE_TRUNCATED_DTYPE
I0512 01:44:51.610865 139676058949632 gin_utils.py:85] expert/dense.MlpBlock.final_dropout_rate = 0.0
I0512 01:44:51.610897 139676058949632 gin_utils.py:85] expert/dense.MlpBlock.input_axis_name = 'mlp_embed'
I0512 01:44:51.610928 139676058949632 gin_utils.py:85] expert/dense.MlpBlock.intermediate_axis_name = 'expert_mlp'
I0512 01:44:51.610960 139676058949632 gin_utils.py:85] expert/dense.MlpBlock.intermediate_dim = %EXPERT_MLP_DIM
I0512 01:44:51.610991 139676058949632 gin_utils.py:85] expert/dense.MlpBlock.intermediate_dropout_rate = %EXPERT_DROPOUT_RATE
I0512 01:44:51.611022 139676058949632 gin_utils.py:85] expert/dense.MlpBlock.kernel_init = \
I0512 01:44:51.611054 139676058949632 gin_utils.py:85]     @expert_kernel_init/linen.initializers.variance_scaling()
I0512 01:44:51.611085 139676058949632 gin_utils.py:85] expert/dense.MlpBlock.output_axis_name = 'mlp_embed'
I0512 01:44:51.573851 139812357228544 gin_utils.py:85] expert/dense.MlpBlock.use_bias = False
I0512 01:44:51.573884 139812357228544 gin_utils.py:85] 
I0512 01:44:51.573915 139812357228544 gin_utils.py:85] # Parameters for models.MoeDecoderOnlyModel:
I0512 01:44:51.573947 139812357228544 gin_utils.py:85] # ==============================================================================
I0512 01:44:51.573979 139812357228544 gin_utils.py:85] models.MoeDecoderOnlyModel.aux_loss_factor = %AUX_LOSS_FACTOR
I0512 01:44:51.574011 139812357228544 gin_utils.py:85] models.MoeDecoderOnlyModel.label_smoothing = %LABEL_SMOOTHING
I0512 01:44:51.574044 139812357228544 gin_utils.py:85] models.MoeDecoderOnlyModel.loss_normalizing_factor = %LOSS_NORMALIZING_FACTOR
I0512 01:44:51.574075 139812357228544 gin_utils.py:85] models.MoeDecoderOnlyModel.module = %ARCHITECTURE
I0512 01:44:51.574107 139812357228544 gin_utils.py:85] models.MoeDecoderOnlyModel.optimizer_def = %OPTIMIZER
I0512 01:44:51.574140 139812357228544 gin_utils.py:85] models.MoeDecoderOnlyModel.router_z_loss_factor = %ROUTER_Z_LOSS_FACTOR
I0512 01:44:51.574172 139812357228544 gin_utils.py:85] models.MoeDecoderOnlyModel.vocabulary = %VOCABULARY
I0512 01:44:51.574204 139812357228544 gin_utils.py:85] models.MoeDecoderOnlyModel.z_loss = %Z_LOSS
I0512 01:44:51.574236 139812357228544 gin_utils.py:85] 
I0512 01:44:51.574268 139812357228544 gin_utils.py:85] # Parameters for moe_layers.MoeLayer:
I0512 01:44:51.574300 139812357228544 gin_utils.py:85] # ==============================================================================
I0512 01:44:51.574332 139812357228544 gin_utils.py:85] moe_layers.MoeLayer.dtype = %MOE_TRUNCATED_DTYPE
I0512 01:44:51.574365 139812357228544 gin_utils.py:85] moe_layers.MoeLayer.eval_capacity_factor = %EVAL_EXPERT_CAPACITY_FACTOR
I0512 01:44:51.574397 139812357228544 gin_utils.py:85] moe_layers.MoeLayer.expert = @expert/dense.MlpBlock()
I0512 01:44:51.574428 139812357228544 gin_utils.py:85] moe_layers.MoeLayer.max_group_size = %GROUP_SIZE
I0512 01:44:51.574460 139812357228544 gin_utils.py:85] moe_layers.MoeLayer.num_expert_partitions = %NUM_EXPERT_PARTITIONS
I0512 01:44:51.574492 139812357228544 gin_utils.py:85] moe_layers.MoeLayer.num_experts = %NUM_EXPERTS
I0512 01:44:51.574524 139812357228544 gin_utils.py:85] moe_layers.MoeLayer.num_model_partitions = \
I0512 01:44:51.574561 139812357228544 gin_utils.py:85]     @moe_partitioning.compute_num_model_partitions()
I0512 01:44:51.574594 139812357228544 gin_utils.py:85] moe_layers.MoeLayer.train_capacity_factor = %TRAIN_EXPERT_CAPACITY_FACTOR
I0512 01:44:51.574626 139812357228544 gin_utils.py:85] 
I0512 01:44:51.574658 139812357228544 gin_utils.py:85] # Parameters for sparse_decoder/moe_layers.MoeLayer:
I0512 01:44:51.574690 139812357228544 gin_utils.py:85] # ==============================================================================
I0512 01:44:51.574722 139812357228544 gin_utils.py:85] sparse_decoder/moe_layers.MoeLayer.router = \
I0512 01:44:51.574753 139812357228544 gin_utils.py:85]     @sparse_decoder/routing.TokensChooseMaskedRouter()
I0512 01:44:51.574785 139812357228544 gin_utils.py:85] 
I0512 01:44:51.574820 139812357228544 gin_utils.py:85] # Parameters for moe_partitioning.MoePjitPartitioner:
I0512 01:44:51.574853 139812357228544 gin_utils.py:85] # ==============================================================================
I0512 01:44:51.574885 139812357228544 gin_utils.py:85] moe_partitioning.MoePjitPartitioner.model_parallel_submesh = \
I0512 01:44:51.574917 139812357228544 gin_utils.py:85]     %MODEL_PARALLEL_SUBMESH
I0512 01:44:51.574948 139812357228544 gin_utils.py:85] moe_partitioning.MoePjitPartitioner.num_expert_partitions = %NUM_EXPERT_PARTITIONS
I0512 01:44:51.574980 139812357228544 gin_utils.py:85] moe_partitioning.MoePjitPartitioner.num_partitions = %NUM_MODEL_PARTITIONS
I0512 01:44:51.575012 139812357228544 gin_utils.py:85] 
I0512 01:44:51.575044 139812357228544 gin_utils.py:85] # Parameters for moe_trainer.MoeTrainer:
I0512 01:44:51.575077 139812357228544 gin_utils.py:85] # ==============================================================================
I0512 01:44:51.575108 139812357228544 gin_utils.py:85] moe_trainer.MoeTrainer.learning_rate_fn = @utils.create_learning_rate_scheduler()
I0512 01:44:51.575140 139812357228544 gin_utils.py:85] moe_trainer.MoeTrainer.num_expert_partitions = %NUM_EXPERT_PARTITIONS
I0512 01:44:51.575172 139812357228544 gin_utils.py:85] moe_trainer.MoeTrainer.num_microbatches = 8
I0512 01:44:51.575204 139812357228544 gin_utils.py:85] 
I0512 01:44:51.575236 139812357228544 gin_utils.py:85] # Parameters for dense_attention.MultiHeadDotProductAttention:
I0512 01:44:51.575267 139812357228544 gin_utils.py:85] # ==============================================================================
I0512 01:44:51.575299 139812357228544 gin_utils.py:85] dense_attention.MultiHeadDotProductAttention.bias_init = %BIAS_INIT
I0512 01:44:51.575331 139812357228544 gin_utils.py:85] dense_attention.MultiHeadDotProductAttention.broadcast_dropout = True
I0512 01:44:51.575363 139812357228544 gin_utils.py:85] dense_attention.MultiHeadDotProductAttention.dropout_rate = %DROPOUT_RATE
I0512 01:44:51.575395 139812357228544 gin_utils.py:85] dense_attention.MultiHeadDotProductAttention.dtype = %ACTIVATION_DTYPE
I0512 01:44:51.575426 139812357228544 gin_utils.py:85] dense_attention.MultiHeadDotProductAttention.head_dim = %HEAD_DIM
I0512 01:44:51.575458 139812357228544 gin_utils.py:85] dense_attention.MultiHeadDotProductAttention.kernel_init = \
I0512 01:44:51.575491 139812357228544 gin_utils.py:85]     @attention_kernel_init/linen.initializers.variance_scaling()
I0512 01:44:51.575522 139812357228544 gin_utils.py:85] dense_attention.MultiHeadDotProductAttention.num_heads = %NUM_HEADS
I0512 01:44:51.575560 139812357228544 gin_utils.py:85] dense_attention.MultiHeadDotProductAttention.use_bias = False
I0512 01:44:51.575593 139812357228544 gin_utils.py:85] dense_attention.MultiHeadDotProductAttention.use_rotary_embedding = True
I0512 01:44:51.575625 139812357228544 gin_utils.py:85] 
I0512 01:44:51.575657 139812357228544 gin_utils.py:85] # Parameters for bias_init/linen.initializers.normal:
I0512 01:44:51.575689 139812357228544 gin_utils.py:85] # ==============================================================================
I0512 01:44:51.575721 139812357228544 gin_utils.py:85] bias_init/linen.initializers.normal.stddev = 1e-06
I0512 01:44:51.575752 139812357228544 gin_utils.py:85] 
I0512 01:44:51.575785 139812357228544 gin_utils.py:85] # Parameters for router_init/linen.initializers.normal:
I0512 01:44:51.575819 139812357228544 gin_utils.py:85] # ==============================================================================
I0512 01:44:51.575852 139812357228544 gin_utils.py:85] router_init/linen.initializers.normal.stddev = 0.02
I0512 01:44:51.575884 139812357228544 gin_utils.py:85] 
I0512 01:44:51.575916 139812357228544 gin_utils.py:85] # Parameters for token_embedder_init/linen.initializers.normal:
I0512 01:44:51.575947 139812357228544 gin_utils.py:85] # ==============================================================================
I0512 01:44:51.575979 139812357228544 gin_utils.py:85] token_embedder_init/linen.initializers.normal.stddev = 1.0
I0512 01:44:51.576011 139812357228544 gin_utils.py:85] 
I0512 01:44:51.576042 139812357228544 gin_utils.py:85] # Parameters for partitioning.PjitPartitioner:
I0512 01:44:51.576074 139812357228544 gin_utils.py:85] # ==============================================================================
I0512 01:44:51.576106 139812357228544 gin_utils.py:85] partitioning.PjitPartitioner.logical_axis_rules = \
I0512 01:44:51.576137 139812357228544 gin_utils.py:85]     @partitioning.standard_logical_axis_rules()
I0512 01:44:51.576189 139812357228544 gin_utils.py:85] partitioning.PjitPartitioner.model_parallel_submesh = None
I0512 01:44:51.576228 139812357228544 gin_utils.py:85] partitioning.PjitPartitioner.num_partitions = 1
I0512 01:44:51.576260 139812357228544 gin_utils.py:85] 
I0512 01:44:51.611117 139676058949632 gin_utils.py:85] expert/dense.MlpBlock.use_bias = False
I0512 01:44:51.611148 139676058949632 gin_utils.py:85] 
I0512 01:44:51.611179 139676058949632 gin_utils.py:85] # Parameters for models.MoeDecoderOnlyModel:
I0512 01:44:51.611211 139676058949632 gin_utils.py:85] # ==============================================================================
I0512 01:44:51.611242 139676058949632 gin_utils.py:85] models.MoeDecoderOnlyModel.aux_loss_factor = %AUX_LOSS_FACTOR
I0512 01:44:51.611274 139676058949632 gin_utils.py:85] models.MoeDecoderOnlyModel.label_smoothing = %LABEL_SMOOTHING
I0512 01:44:51.611305 139676058949632 gin_utils.py:85] models.MoeDecoderOnlyModel.loss_normalizing_factor = %LOSS_NORMALIZING_FACTOR
I0512 01:44:51.611337 139676058949632 gin_utils.py:85] models.MoeDecoderOnlyModel.module = %ARCHITECTURE
I0512 01:44:51.611373 139676058949632 gin_utils.py:85] models.MoeDecoderOnlyModel.optimizer_def = %OPTIMIZER
I0512 01:44:51.611407 139676058949632 gin_utils.py:85] models.MoeDecoderOnlyModel.router_z_loss_factor = %ROUTER_Z_LOSS_FACTOR
I0512 01:44:51.611439 139676058949632 gin_utils.py:85] models.MoeDecoderOnlyModel.vocabulary = %VOCABULARY
I0512 01:44:51.611470 139676058949632 gin_utils.py:85] models.MoeDecoderOnlyModel.z_loss = %Z_LOSS
I0512 01:44:51.611501 139676058949632 gin_utils.py:85] 
I0512 01:44:51.611531 139676058949632 gin_utils.py:85] # Parameters for moe_layers.MoeLayer:
I0512 01:44:51.611563 139676058949632 gin_utils.py:85] # ==============================================================================
I0512 01:44:51.611594 139676058949632 gin_utils.py:85] moe_layers.MoeLayer.dtype = %MOE_TRUNCATED_DTYPE
I0512 01:44:51.611627 139676058949632 gin_utils.py:85] moe_layers.MoeLayer.eval_capacity_factor = %EVAL_EXPERT_CAPACITY_FACTOR
I0512 01:44:51.611661 139676058949632 gin_utils.py:85] moe_layers.MoeLayer.expert = @expert/dense.MlpBlock()
I0512 01:44:51.611693 139676058949632 gin_utils.py:85] moe_layers.MoeLayer.max_group_size = %GROUP_SIZE
I0512 01:44:51.611725 139676058949632 gin_utils.py:85] moe_layers.MoeLayer.num_expert_partitions = %NUM_EXPERT_PARTITIONS
I0512 01:44:51.611757 139676058949632 gin_utils.py:85] moe_layers.MoeLayer.num_experts = %NUM_EXPERTS
I0512 01:44:51.611790 139676058949632 gin_utils.py:85] moe_layers.MoeLayer.num_model_partitions = \
I0512 01:44:51.611822 139676058949632 gin_utils.py:85]     @partitioning.compute_num_model_partitions()
I0512 01:44:51.611854 139676058949632 gin_utils.py:85] moe_layers.MoeLayer.train_capacity_factor = %TRAIN_EXPERT_CAPACITY_FACTOR
I0512 01:44:51.611886 139676058949632 gin_utils.py:85] 
I0512 01:44:51.611917 139676058949632 gin_utils.py:85] # Parameters for sparse_decoder/moe_layers.MoeLayer:
I0512 01:44:51.611950 139676058949632 gin_utils.py:85] # ==============================================================================
I0512 01:44:51.611982 139676058949632 gin_utils.py:85] sparse_decoder/moe_layers.MoeLayer.router = \
I0512 01:44:51.612014 139676058949632 gin_utils.py:85]     @sparse_decoder/routing.TokensChooseMaskedRouter()
I0512 01:44:51.612046 139676058949632 gin_utils.py:85] 
I0512 01:44:51.612078 139676058949632 gin_utils.py:85] # Parameters for partitioning.MoePjitPartitioner:
I0512 01:44:51.612110 139676058949632 gin_utils.py:85] # ==============================================================================
I0512 01:44:51.612142 139676058949632 gin_utils.py:85] partitioning.MoePjitPartitioner.model_parallel_submesh = %MODEL_PARALLEL_SUBMESH
I0512 01:44:51.612174 139676058949632 gin_utils.py:85] partitioning.MoePjitPartitioner.num_expert_partitions = %NUM_EXPERT_PARTITIONS
I0512 01:44:51.612206 139676058949632 gin_utils.py:85] partitioning.MoePjitPartitioner.num_partitions = %NUM_MODEL_PARTITIONS
I0512 01:44:51.612238 139676058949632 gin_utils.py:85] 
I0512 01:44:51.612270 139676058949632 gin_utils.py:85] # Parameters for moe_trainer.MoeTrainer:
I0512 01:44:51.612302 139676058949632 gin_utils.py:85] # ==============================================================================
I0512 01:44:51.612334 139676058949632 gin_utils.py:85] moe_trainer.MoeTrainer.learning_rate_fn = @utils.create_learning_rate_scheduler()
I0512 01:44:51.612365 139676058949632 gin_utils.py:85] moe_trainer.MoeTrainer.num_expert_partitions = %NUM_EXPERT_PARTITIONS
I0512 01:44:51.612404 139676058949632 gin_utils.py:85] moe_trainer.MoeTrainer.num_microbatches = 8
I0512 01:44:51.612437 139676058949632 gin_utils.py:85] 
I0512 01:44:51.612468 139676058949632 gin_utils.py:85] # Parameters for dense_attention.MultiHeadDotProductAttention:
I0512 01:44:51.612500 139676058949632 gin_utils.py:85] # ==============================================================================
I0512 01:44:51.612533 139676058949632 gin_utils.py:85] dense_attention.MultiHeadDotProductAttention.bias_init = %BIAS_INIT
I0512 01:44:51.612565 139676058949632 gin_utils.py:85] dense_attention.MultiHeadDotProductAttention.broadcast_dropout = True
I0512 01:44:51.612597 139676058949632 gin_utils.py:85] dense_attention.MultiHeadDotProductAttention.dropout_rate = %DROPOUT_RATE
I0512 01:44:51.612630 139676058949632 gin_utils.py:85] dense_attention.MultiHeadDotProductAttention.dtype = %ACTIVATION_DTYPE
I0512 01:44:51.612663 139676058949632 gin_utils.py:85] dense_attention.MultiHeadDotProductAttention.head_dim = %HEAD_DIM
I0512 01:44:51.612696 139676058949632 gin_utils.py:85] dense_attention.MultiHeadDotProductAttention.kernel_init = \
I0512 01:44:51.612728 139676058949632 gin_utils.py:85]     @attention_kernel_init/linen.initializers.variance_scaling()
I0512 01:44:51.612760 139676058949632 gin_utils.py:85] dense_attention.MultiHeadDotProductAttention.num_heads = %NUM_HEADS
I0512 01:44:51.612792 139676058949632 gin_utils.py:85] dense_attention.MultiHeadDotProductAttention.use_bias = False
I0512 01:44:51.612824 139676058949632 gin_utils.py:85] dense_attention.MultiHeadDotProductAttention.use_rotary_embedding = True
I0512 01:44:51.612856 139676058949632 gin_utils.py:85] 
I0512 01:44:51.612888 139676058949632 gin_utils.py:85] # Parameters for bias_init/linen.initializers.normal:
I0512 01:44:51.612920 139676058949632 gin_utils.py:85] # ==============================================================================
I0512 01:44:51.612952 139676058949632 gin_utils.py:85] bias_init/linen.initializers.normal.stddev = 1e-06
I0512 01:44:51.612984 139676058949632 gin_utils.py:85] 
I0512 01:44:51.613016 139676058949632 gin_utils.py:85] # Parameters for router_init/linen.initializers.normal:
I0512 01:44:51.613049 139676058949632 gin_utils.py:85] # ==============================================================================
I0512 01:44:51.613082 139676058949632 gin_utils.py:85] router_init/linen.initializers.normal.stddev = 0.02
I0512 01:44:51.613115 139676058949632 gin_utils.py:85] 
I0512 01:44:51.613147 139676058949632 gin_utils.py:85] # Parameters for token_embedder_init/linen.initializers.normal:
I0512 01:44:51.613179 139676058949632 gin_utils.py:85] # ==============================================================================
I0512 01:44:51.613211 139676058949632 gin_utils.py:85] token_embedder_init/linen.initializers.normal.stddev = 1.0
I0512 01:44:51.613243 139676058949632 gin_utils.py:85] 
I0512 01:44:51.613275 139676058949632 gin_utils.py:85] # Parameters for partitioning2.PjitPartitioner:
I0512 01:44:51.613307 139676058949632 gin_utils.py:85] # ==============================================================================
I0512 01:44:51.613339 139676058949632 gin_utils.py:85] partitioning2.PjitPartitioner.logical_axis_rules = \
I0512 01:44:51.613376 139676058949632 gin_utils.py:85]     @partitioning2.standard_logical_axis_rules()
I0512 01:44:51.613410 139676058949632 gin_utils.py:85] partitioning2.PjitPartitioner.model_parallel_submesh = None
I0512 01:44:51.613443 139676058949632 gin_utils.py:85] partitioning2.PjitPartitioner.num_partitions = 1
I0512 01:44:51.613475 139676058949632 gin_utils.py:85] 
I0512 01:44:51.613506 139676058949632 gin_utils.py:85] # Parameters for utils.RestoreCheckpointConfig:
I0512 01:44:51.613538 139676058949632 gin_utils.py:85] # ==============================================================================
I0512 01:44:51.613570 139676058949632 gin_utils.py:85] utils.RestoreCheckpointConfig.path = []
I0512 01:44:51.613602 139676058949632 gin_utils.py:85] 
I0512 01:44:51.613635 139676058949632 gin_utils.py:85] # Parameters for routing.RouterWeights:
I0512 01:44:51.613668 139676058949632 gin_utils.py:85] # ==============================================================================
I0512 01:44:51.613699 139676058949632 gin_utils.py:85] routing.RouterWeights.bias_init = %BIAS_INIT
I0512 01:44:51.613761 139676058949632 gin_utils.py:85] routing.RouterWeights.dtype = 'float32'
I0512 01:44:51.613822 139676058949632 gin_utils.py:85] routing.RouterWeights.kernel_init = @router_init/linen.initializers.normal()
I0512 01:44:51.613857 139676058949632 gin_utils.py:85] routing.RouterWeights.use_bias = False
I0512 01:44:51.613888 139676058949632 gin_utils.py:85] 
I0512 01:44:51.613921 139676058949632 gin_utils.py:85] # Parameters for utils.SaveCheckpointConfig:
I0512 01:44:51.613953 139676058949632 gin_utils.py:85] # ==============================================================================
I0512 01:44:51.613985 139676058949632 gin_utils.py:85] utils.SaveCheckpointConfig.dtype = 'float32'
I0512 01:44:51.614017 139676058949632 gin_utils.py:85] utils.SaveCheckpointConfig.keep = 20
I0512 01:44:51.614048 139676058949632 gin_utils.py:85] utils.SaveCheckpointConfig.period = 2500
I0512 01:44:51.614080 139676058949632 gin_utils.py:85] utils.SaveCheckpointConfig.save_dataset = True
I0512 01:44:51.614112 139676058949632 gin_utils.py:85] 
I0512 01:44:51.614143 139676058949632 gin_utils.py:85] # Parameters for seqio.SentencePieceVocabulary:
I0512 01:44:51.614175 139676058949632 gin_utils.py:85] # ==============================================================================
I0512 01:44:51.614207 139676058949632 gin_utils.py:85] seqio.SentencePieceVocabulary.extra_ids = 300
I0512 01:44:51.614238 139676058949632 gin_utils.py:85] seqio.SentencePieceVocabulary.sentencepiece_model_file = \
I0512 01:44:51.614270 139676058949632 gin_utils.py:85]     'gs://rosinality-tpu-bucket/sentencepiece.model'
I0512 01:44:51.614302 139676058949632 gin_utils.py:85] 
I0512 01:44:51.614334 139676058949632 gin_utils.py:85] # Parameters for moe_architecture.SparseDecoder:
I0512 01:44:51.614366 139676058949632 gin_utils.py:85] # ==============================================================================
I0512 01:44:51.614405 139676058949632 gin_utils.py:85] moe_architecture.SparseDecoder.dropout_factory = %DROPOUT_FACTORY
I0512 01:44:51.614437 139676058949632 gin_utils.py:85] moe_architecture.SparseDecoder.dtype = %ACTIVATION_DTYPE
I0512 01:44:51.614469 139676058949632 gin_utils.py:85] moe_architecture.SparseDecoder.layer_factory = @t5_architecture.DecoderLayer
I0512 01:44:51.614501 139676058949632 gin_utils.py:85] moe_architecture.SparseDecoder.layer_norm_factory = @layer_norm.T5LayerNorm
I0512 01:44:51.614533 139676058949632 gin_utils.py:85] moe_architecture.SparseDecoder.num_layers = %NUM_DECODER_LAYERS
I0512 01:44:51.614564 139676058949632 gin_utils.py:85] moe_architecture.SparseDecoder.num_sparse_layers = %NUM_DECODER_SPARSE_LAYERS
I0512 01:44:51.614596 139676058949632 gin_utils.py:85] moe_architecture.SparseDecoder.output_logits_factory = \
I0512 01:44:51.614629 139676058949632 gin_utils.py:85]     @output_logits/dense.DenseGeneral
I0512 01:44:51.614663 139676058949632 gin_utils.py:85] moe_architecture.SparseDecoder.sparse_layer_factory = \
I0512 01:44:51.614696 139676058949632 gin_utils.py:85]     @moe_architecture.SparseDecoderLayer
I0512 01:44:51.614727 139676058949632 gin_utils.py:85] moe_architecture.SparseDecoder.sparse_layout = %DECODER_SPARSE_LAYOUT
I0512 01:44:51.614759 139676058949632 gin_utils.py:85] 
I0512 01:44:51.614790 139676058949632 gin_utils.py:85] # Parameters for moe_architecture.SparseDecoderLayer:
I0512 01:44:51.614822 139676058949632 gin_utils.py:85] # ==============================================================================
I0512 01:44:51.614855 139676058949632 gin_utils.py:85] moe_architecture.SparseDecoderLayer.activation_partitioning_dims = \
I0512 01:44:51.614886 139676058949632 gin_utils.py:85]     %ACTIVATION_PARTITIONING_DIMS
I0512 01:44:51.614918 139676058949632 gin_utils.py:85] moe_architecture.SparseDecoderLayer.dropout_factory = %DROPOUT_FACTORY
I0512 01:44:51.614950 139676058949632 gin_utils.py:85] moe_architecture.SparseDecoderLayer.encoder_decoder_attention = None
I0512 01:44:51.614982 139676058949632 gin_utils.py:85] moe_architecture.SparseDecoderLayer.extra_mlp = @dense.MlpBlock()
I0512 01:44:51.615015 139676058949632 gin_utils.py:85] moe_architecture.SparseDecoderLayer.layer_norm_factory = @layer_norm.T5LayerNorm
I0512 01:44:51.615047 139676058949632 gin_utils.py:85] moe_architecture.SparseDecoderLayer.mlp = @sparse_decoder/moe_layers.MoeLayer()
I0512 01:44:51.615079 139676058949632 gin_utils.py:85] moe_architecture.SparseDecoderLayer.scanned = False
I0512 01:44:51.615112 139676058949632 gin_utils.py:85] moe_architecture.SparseDecoderLayer.self_attention = \
I0512 01:44:51.615144 139676058949632 gin_utils.py:85]     @dense_attention.MultiHeadDotProductAttention()
I0512 01:44:51.615176 139676058949632 gin_utils.py:85] 
I0512 01:44:51.615208 139676058949632 gin_utils.py:85] # Parameters for layer_norm.T5LayerNorm:
I0512 01:44:51.615240 139676058949632 gin_utils.py:85] # ==============================================================================
I0512 01:44:51.615272 139676058949632 gin_utils.py:85] layer_norm.T5LayerNorm.dtype = %ACTIVATION_DTYPE
I0512 01:44:51.615304 139676058949632 gin_utils.py:85] 
I0512 01:44:51.615336 139676058949632 gin_utils.py:85] # Parameters for routing.TokensChooseMaskedRouter:
I0512 01:44:51.615368 139676058949632 gin_utils.py:85] # ==============================================================================
I0512 01:44:51.615406 139676058949632 gin_utils.py:85] routing.TokensChooseMaskedRouter.dtype = 'float32'
I0512 01:44:51.615439 139676058949632 gin_utils.py:85] routing.TokensChooseMaskedRouter.ignore_padding_tokens = False
I0512 01:44:51.615471 139676058949632 gin_utils.py:85] routing.TokensChooseMaskedRouter.jitter_noise = %JITTER_NOISE
I0512 01:44:51.615503 139676058949632 gin_utils.py:85] routing.TokensChooseMaskedRouter.num_selected_experts = %NUM_SELECTED_EXPERTS
I0512 01:44:51.615535 139676058949632 gin_utils.py:85] routing.TokensChooseMaskedRouter.router_weights = @routing.RouterWeights()
I0512 01:44:51.615567 139676058949632 gin_utils.py:85] 
I0512 01:44:51.615599 139676058949632 gin_utils.py:85] # Parameters for sparse_decoder/routing.TokensChooseMaskedRouter:
I0512 01:44:51.615633 139676058949632 gin_utils.py:85] # ==============================================================================
I0512 01:44:51.615665 139676058949632 gin_utils.py:85] sparse_decoder/routing.TokensChooseMaskedRouter.batch_prioritized_routing = False
I0512 01:44:51.615698 139676058949632 gin_utils.py:85] 
I0512 01:44:51.615730 139676058949632 gin_utils.py:85] # Parameters for sparse_encoder/routing.TokensChooseMaskedRouter:
I0512 01:44:51.615761 139676058949632 gin_utils.py:85] # ==============================================================================
I0512 01:44:51.615793 139676058949632 gin_utils.py:85] sparse_encoder/routing.TokensChooseMaskedRouter.batch_prioritized_routing = False
I0512 01:44:51.615825 139676058949632 gin_utils.py:85] 
I0512 01:44:51.615857 139676058949632 gin_utils.py:85] # Parameters for train_script.train:
I0512 01:44:51.615890 139676058949632 gin_utils.py:85] # ==============================================================================
I0512 01:44:51.615923 139676058949632 gin_utils.py:85] train_script.train.checkpoint_cfg = @utils.CheckpointConfig()
I0512 01:44:51.615955 139676058949632 gin_utils.py:85] train_script.train.eval_period = 2500
I0512 01:44:51.615988 139676058949632 gin_utils.py:85] train_script.train.eval_steps = 20
I0512 01:44:51.616019 139676058949632 gin_utils.py:85] train_script.train.infer_eval_dataset_cfg = None
I0512 01:44:51.576292 139812357228544 gin_utils.py:85] # Parameters for utils.RestoreCheckpointConfig:
I0512 01:44:51.576323 139812357228544 gin_utils.py:85] # ==============================================================================
I0512 01:44:51.576355 139812357228544 gin_utils.py:85] utils.RestoreCheckpointConfig.path = []
I0512 01:44:51.576387 139812357228544 gin_utils.py:85] 
I0512 01:44:51.576419 139812357228544 gin_utils.py:85] # Parameters for routing.RouterWeights:
I0512 01:44:51.576450 139812357228544 gin_utils.py:85] # ==============================================================================
I0512 01:44:51.576482 139812357228544 gin_utils.py:85] routing.RouterWeights.bias_init = %BIAS_INIT
I0512 01:44:51.576513 139812357228544 gin_utils.py:85] routing.RouterWeights.dtype = 'float32'
I0512 01:44:51.576552 139812357228544 gin_utils.py:85] routing.RouterWeights.kernel_init = @router_init/linen.initializers.normal()
I0512 01:44:51.576586 139812357228544 gin_utils.py:85] routing.RouterWeights.use_bias = False
I0512 01:44:51.576618 139812357228544 gin_utils.py:85] 
I0512 01:44:51.576649 139812357228544 gin_utils.py:85] # Parameters for utils.SaveCheckpointConfig:
I0512 01:44:51.576681 139812357228544 gin_utils.py:85] # ==============================================================================
I0512 01:44:51.576712 139812357228544 gin_utils.py:85] utils.SaveCheckpointConfig.dtype = 'float32'
I0512 01:44:51.576743 139812357228544 gin_utils.py:85] utils.SaveCheckpointConfig.keep = 20
I0512 01:44:51.576775 139812357228544 gin_utils.py:85] utils.SaveCheckpointConfig.period = 2500
I0512 01:44:51.576808 139812357228544 gin_utils.py:85] utils.SaveCheckpointConfig.save_dataset = True
I0512 01:44:51.576841 139812357228544 gin_utils.py:85] 
I0512 01:44:51.576873 139812357228544 gin_utils.py:85] # Parameters for seqio.SentencePieceVocabulary:
I0512 01:44:51.576904 139812357228544 gin_utils.py:85] # ==============================================================================
I0512 01:44:51.576936 139812357228544 gin_utils.py:85] seqio.SentencePieceVocabulary.extra_ids = 300
I0512 01:44:51.576967 139812357228544 gin_utils.py:85] seqio.SentencePieceVocabulary.sentencepiece_model_file = \
I0512 01:44:51.576999 139812357228544 gin_utils.py:85]     'gs://rosinality-tpu-bucket/sentencepiece.model'
I0512 01:44:51.577030 139812357228544 gin_utils.py:85] 
I0512 01:44:51.577062 139812357228544 gin_utils.py:85] # Parameters for moe_architecture.SparseDecoder:
I0512 01:44:51.577093 139812357228544 gin_utils.py:85] # ==============================================================================
I0512 01:44:51.577125 139812357228544 gin_utils.py:85] moe_architecture.SparseDecoder.dropout_factory = %DROPOUT_FACTORY
I0512 01:44:51.577157 139812357228544 gin_utils.py:85] moe_architecture.SparseDecoder.dtype = %ACTIVATION_DTYPE
I0512 01:44:51.577189 139812357228544 gin_utils.py:85] moe_architecture.SparseDecoder.layer_factory = @t5_architecture.DecoderLayer
I0512 01:44:51.577220 139812357228544 gin_utils.py:85] moe_architecture.SparseDecoder.layer_norm_factory = @layer_norm.T5LayerNorm
I0512 01:44:51.577252 139812357228544 gin_utils.py:85] moe_architecture.SparseDecoder.num_layers = %NUM_DECODER_LAYERS
I0512 01:44:51.577283 139812357228544 gin_utils.py:85] moe_architecture.SparseDecoder.num_sparse_layers = %NUM_DECODER_SPARSE_LAYERS
I0512 01:44:51.577315 139812357228544 gin_utils.py:85] moe_architecture.SparseDecoder.output_logits_factory = \
I0512 01:44:51.577346 139812357228544 gin_utils.py:85]     @output_logits/dense.DenseGeneral
I0512 01:44:51.577378 139812357228544 gin_utils.py:85] moe_architecture.SparseDecoder.sparse_layer_factory = \
I0512 01:44:51.577409 139812357228544 gin_utils.py:85]     @moe_architecture.SparseDecoderLayer
I0512 01:44:51.577441 139812357228544 gin_utils.py:85] moe_architecture.SparseDecoder.sparse_layout = %DECODER_SPARSE_LAYOUT
I0512 01:44:51.577473 139812357228544 gin_utils.py:85] 
I0512 01:44:51.577504 139812357228544 gin_utils.py:85] # Parameters for moe_architecture.SparseDecoderLayer:
I0512 01:44:51.577536 139812357228544 gin_utils.py:85] # ==============================================================================
I0512 01:44:51.577574 139812357228544 gin_utils.py:85] moe_architecture.SparseDecoderLayer.activation_partitioning_dims = \
I0512 01:44:51.577607 139812357228544 gin_utils.py:85]     %ACTIVATION_PARTITIONING_DIMS
I0512 01:44:51.577638 139812357228544 gin_utils.py:85] moe_architecture.SparseDecoderLayer.dropout_factory = %DROPOUT_FACTORY
I0512 01:44:51.577670 139812357228544 gin_utils.py:85] moe_architecture.SparseDecoderLayer.encoder_decoder_attention = None
I0512 01:44:51.577703 139812357228544 gin_utils.py:85] moe_architecture.SparseDecoderLayer.extra_mlp = @dense.MlpBlock()
I0512 01:44:51.577735 139812357228544 gin_utils.py:85] moe_architecture.SparseDecoderLayer.layer_norm_factory = @layer_norm.T5LayerNorm
I0512 01:44:51.577766 139812357228544 gin_utils.py:85] moe_architecture.SparseDecoderLayer.mlp = @sparse_decoder/moe_layers.MoeLayer()
I0512 01:44:51.616051 139676058949632 gin_utils.py:85] train_script.train.model = %MODEL
I0512 01:44:51.616083 139676058949632 gin_utils.py:85] train_script.train.model_dir = %MODEL_DIR
I0512 01:44:51.616115 139676058949632 gin_utils.py:85] train_script.train.partitioner = @partitioning.MoePjitPartitioner()
I0512 01:44:51.616147 139676058949632 gin_utils.py:85] train_script.train.random_seed = 42
I0512 01:44:51.616178 139676058949632 gin_utils.py:85] train_script.train.stats_period = 10
I0512 01:44:51.616209 139676058949632 gin_utils.py:85] train_script.train.summarize_config_fn = @gin_utils.summarize_gin_config
I0512 01:44:51.616241 139676058949632 gin_utils.py:85] train_script.train.total_steps = %TRAIN_STEPS
I0512 01:44:51.616272 139676058949632 gin_utils.py:85] train_script.train.train_dataset_cfg = @train/utils.DatasetConfig()
I0512 01:44:51.616304 139676058949632 gin_utils.py:85] train_script.train.train_eval_dataset_cfg = None
I0512 01:44:51.616336 139676058949632 gin_utils.py:85] train_script.train.trainer_cls = @moe_trainer.MoeTrainer
I0512 01:44:51.577799 139812357228544 gin_utils.py:85] moe_architecture.SparseDecoderLayer.scanned = False
I0512 01:44:51.577833 139812357228544 gin_utils.py:85] moe_architecture.SparseDecoderLayer.self_attention = \
I0512 01:44:51.577865 139812357228544 gin_utils.py:85]     @dense_attention.MultiHeadDotProductAttention()
I0512 01:44:51.577896 139812357228544 gin_utils.py:85] 
I0512 01:44:51.577929 139812357228544 gin_utils.py:85] # Parameters for layer_norm.T5LayerNorm:
I0512 01:44:51.577961 139812357228544 gin_utils.py:85] # ==============================================================================
I0512 01:44:51.577993 139812357228544 gin_utils.py:85] layer_norm.T5LayerNorm.dtype = %ACTIVATION_DTYPE
I0512 01:44:51.578024 139812357228544 gin_utils.py:85] 
I0512 01:44:51.578056 139812357228544 gin_utils.py:85] # Parameters for routing.TokensChooseMaskedRouter:
I0512 01:44:51.578088 139812357228544 gin_utils.py:85] # ==============================================================================
I0512 01:44:51.578119 139812357228544 gin_utils.py:85] routing.TokensChooseMaskedRouter.dtype = 'float32'
I0512 01:44:51.578151 139812357228544 gin_utils.py:85] routing.TokensChooseMaskedRouter.ignore_padding_tokens = False
I0512 01:44:51.578183 139812357228544 gin_utils.py:85] routing.TokensChooseMaskedRouter.jitter_noise = %JITTER_NOISE
I0512 01:44:51.578214 139812357228544 gin_utils.py:85] routing.TokensChooseMaskedRouter.num_selected_experts = %NUM_SELECTED_EXPERTS
I0512 01:44:51.578246 139812357228544 gin_utils.py:85] routing.TokensChooseMaskedRouter.router_weights = @routing.RouterWeights()
I0512 01:44:51.578277 139812357228544 gin_utils.py:85] 
I0512 01:44:51.578308 139812357228544 gin_utils.py:85] # Parameters for sparse_decoder/routing.TokensChooseMaskedRouter:
I0512 01:44:51.578340 139812357228544 gin_utils.py:85] # ==============================================================================
I0512 01:44:51.578372 139812357228544 gin_utils.py:85] sparse_decoder/routing.TokensChooseMaskedRouter.batch_prioritized_routing = False
I0512 01:44:51.578403 139812357228544 gin_utils.py:85] 
I0512 01:44:51.578434 139812357228544 gin_utils.py:85] # Parameters for sparse_encoder/routing.TokensChooseMaskedRouter:
I0512 01:44:51.578466 139812357228544 gin_utils.py:85] # ==============================================================================
I0512 01:44:51.578497 139812357228544 gin_utils.py:85] sparse_encoder/routing.TokensChooseMaskedRouter.batch_prioritized_routing = False
I0512 01:44:51.578529 139812357228544 gin_utils.py:85] 
I0512 01:44:51.578567 139812357228544 gin_utils.py:85] # Parameters for train_script.train:
I0512 01:44:51.578600 139812357228544 gin_utils.py:85] # ==============================================================================
I0512 01:44:51.578633 139812357228544 gin_utils.py:85] train_script.train.checkpoint_cfg = @utils.CheckpointConfig()
I0512 01:44:51.578664 139812357228544 gin_utils.py:85] train_script.train.eval_period = 2500
I0512 01:44:51.578696 139812357228544 gin_utils.py:85] train_script.train.eval_steps = 20
I0512 01:44:51.578727 139812357228544 gin_utils.py:85] train_script.train.infer_eval_dataset_cfg = None
I0512 01:44:51.578758 139812357228544 gin_utils.py:85] train_script.train.model = %MODEL
I0512 01:44:51.578790 139812357228544 gin_utils.py:85] train_script.train.model_dir = %MODEL_DIR
I0512 01:44:51.578824 139812357228544 gin_utils.py:85] train_script.train.partitioner = @moe_partitioning.MoePjitPartitioner()
I0512 01:44:51.578855 139812357228544 gin_utils.py:85] train_script.train.random_seed = 42
I0512 01:44:51.578887 139812357228544 gin_utils.py:85] train_script.train.stats_period = 10
I0512 01:44:51.578918 139812357228544 gin_utils.py:85] train_script.train.summarize_config_fn = @gin_utils.summarize_gin_config
I0512 01:44:51.578950 139812357228544 gin_utils.py:85] train_script.train.total_steps = %TRAIN_STEPS
I0512 01:44:51.578981 139812357228544 gin_utils.py:85] train_script.train.train_dataset_cfg = @train/utils.DatasetConfig()
I0512 01:44:51.579013 139812357228544 gin_utils.py:85] train_script.train.train_eval_dataset_cfg = None
I0512 01:44:51.579044 139812357228544 gin_utils.py:85] train_script.train.trainer_cls = @moe_trainer.MoeTrainer
I0512 01:44:51.579076 139812357228544 gin_utils.py:85] train_script.train.use_hardware_rng = %USE_HARDWARE_RNG
I0512 01:44:51.579107 139812357228544 gin_utils.py:85] 
I0512 01:44:51.579138 139812357228544 gin_utils.py:85] # Parameters for trainer.Trainer:
I0512 01:44:51.579170 139812357228544 gin_utils.py:85] # ==============================================================================
I0512 01:44:51.579202 139812357228544 gin_utils.py:85] trainer.Trainer.learning_rate_fn = @utils.create_learning_rate_scheduler()
I0512 01:44:51.579234 139812357228544 gin_utils.py:85] trainer.Trainer.num_microbatches = None
I0512 01:44:51.579265 139812357228544 gin_utils.py:85] 
I0512 01:44:51.579297 139812357228544 gin_utils.py:85] # Parameters for attention_kernel_init/linen.initializers.variance_scaling:
I0512 01:44:51.579328 139812357228544 gin_utils.py:85] # ==============================================================================
I0512 01:44:51.579360 139812357228544 gin_utils.py:85] attention_kernel_init/linen.initializers.variance_scaling.distribution = 'normal'
I0512 01:44:51.579391 139812357228544 gin_utils.py:85] attention_kernel_init/linen.initializers.variance_scaling.mode = 'fan_in'
I0512 01:44:51.579422 139812357228544 gin_utils.py:85] attention_kernel_init/linen.initializers.variance_scaling.scale = %SCALE
I0512 01:44:51.579454 139812357228544 gin_utils.py:85] 
I0512 01:44:51.579485 139812357228544 gin_utils.py:85] # Parameters for expert_kernel_init/linen.initializers.variance_scaling:
I0512 01:44:51.579517 139812357228544 gin_utils.py:85] # ==============================================================================
I0512 01:44:51.579554 139812357228544 gin_utils.py:85] expert_kernel_init/linen.initializers.variance_scaling.distribution = 'normal'
I0512 01:44:51.579587 139812357228544 gin_utils.py:85] expert_kernel_init/linen.initializers.variance_scaling.mode = 'fan_in'
I0512 01:44:51.579619 139812357228544 gin_utils.py:85] expert_kernel_init/linen.initializers.variance_scaling.scale = %SCALE
I0512 01:44:51.579651 139812357228544 gin_utils.py:85] 
I0512 01:44:51.579682 139812357228544 gin_utils.py:85] # Parameters for mlp_kernel_init/linen.initializers.variance_scaling:
I0512 01:44:51.579714 139812357228544 gin_utils.py:85] # ==============================================================================
I0512 01:44:51.579745 139812357228544 gin_utils.py:85] mlp_kernel_init/linen.initializers.variance_scaling.distribution = \
I0512 01:44:51.579777 139812357228544 gin_utils.py:85]     'truncated_normal'
I0512 01:44:51.579810 139812357228544 gin_utils.py:85] mlp_kernel_init/linen.initializers.variance_scaling.mode = 'fan_in'
I0512 01:44:51.579843 139812357228544 gin_utils.py:85] mlp_kernel_init/linen.initializers.variance_scaling.scale = %SCALE
I0512 01:44:51.579874 139812357228544 gin_utils.py:85] 
I0512 01:44:51.579906 139812357228544 gin_utils.py:85] # Parameters for output_logits_kernel_init/linen.initializers.variance_scaling:
I0512 01:44:51.579937 139812357228544 gin_utils.py:85] # ==============================================================================
I0512 01:44:51.579969 139812357228544 gin_utils.py:85] output_logits_kernel_init/linen.initializers.variance_scaling.distribution = \
I0512 01:44:51.580000 139812357228544 gin_utils.py:85]     'truncated_normal'
I0512 01:44:51.580031 139812357228544 gin_utils.py:85] output_logits_kernel_init/linen.initializers.variance_scaling.mode = 'fan_in'
I0512 01:44:51.580063 139812357228544 gin_utils.py:85] output_logits_kernel_init/linen.initializers.variance_scaling.scale = %SCALE
I0512 01:44:51.580995 139812357228544 partitioning.py:559] `activation_partitioning_dims` = 2, `parameter_partitioning_dims` = 1
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
I0000 00:00:1715478291.630387  349683 pjrt_api.cc:98] GetPjrtApi was found for tpu at /home/rosinality/openmoe_venv/lib/python3.10/site-packages/libtpu/libtpu.so
I0000 00:00:1715478291.630440  349683 pjrt_api.cc:67] PJRT_Api is set for device type tpu
I0000 00:00:1715478291.630442  349683 pjrt_api.cc:72] PJRT plugin for tpu has PJRT API version 0.30. The framework PJRT API version is 0.30.
I0512 01:44:51.616367 139676058949632 gin_utils.py:85] train_script.train.use_hardware_rng = %USE_HARDWARE_RNG
I0512 01:44:51.616405 139676058949632 gin_utils.py:85] 
I0512 01:44:51.616437 139676058949632 gin_utils.py:85] # Parameters for trainer.Trainer:
I0512 01:44:51.616470 139676058949632 gin_utils.py:85] # ==============================================================================
I0512 01:44:51.616501 139676058949632 gin_utils.py:85] trainer.Trainer.learning_rate_fn = @utils.create_learning_rate_scheduler()
I0512 01:44:51.616533 139676058949632 gin_utils.py:85] trainer.Trainer.num_microbatches = None
I0512 01:44:51.616565 139676058949632 gin_utils.py:85] 
I0512 01:44:51.616596 139676058949632 gin_utils.py:85] # Parameters for attention_kernel_init/linen.initializers.variance_scaling:
I0512 01:44:51.616629 139676058949632 gin_utils.py:85] # ==============================================================================
I0512 01:44:51.616662 139676058949632 gin_utils.py:85] attention_kernel_init/linen.initializers.variance_scaling.distribution = 'normal'
I0512 01:44:51.616695 139676058949632 gin_utils.py:85] attention_kernel_init/linen.initializers.variance_scaling.mode = 'fan_in'
I0512 01:44:51.616727 139676058949632 gin_utils.py:85] attention_kernel_init/linen.initializers.variance_scaling.scale = %SCALE
I0512 01:44:51.616759 139676058949632 gin_utils.py:85] 
I0512 01:44:51.616791 139676058949632 gin_utils.py:85] # Parameters for expert_kernel_init/linen.initializers.variance_scaling:
I0512 01:44:51.616822 139676058949632 gin_utils.py:85] # ==============================================================================
I0512 01:44:51.616854 139676058949632 gin_utils.py:85] expert_kernel_init/linen.initializers.variance_scaling.distribution = 'normal'
I0512 01:44:51.616886 139676058949632 gin_utils.py:85] expert_kernel_init/linen.initializers.variance_scaling.mode = 'fan_in'
I0512 01:44:51.616918 139676058949632 gin_utils.py:85] expert_kernel_init/linen.initializers.variance_scaling.scale = %SCALE
I0512 01:44:51.616950 139676058949632 gin_utils.py:85] 
I0512 01:44:51.616981 139676058949632 gin_utils.py:85] # Parameters for mlp_kernel_init/linen.initializers.variance_scaling:
I0512 01:44:51.617013 139676058949632 gin_utils.py:85] # ==============================================================================
I0512 01:44:51.617044 139676058949632 gin_utils.py:85] mlp_kernel_init/linen.initializers.variance_scaling.distribution = \
I0512 01:44:51.617076 139676058949632 gin_utils.py:85]     'truncated_normal'
I0512 01:44:51.617108 139676058949632 gin_utils.py:85] mlp_kernel_init/linen.initializers.variance_scaling.mode = 'fan_in'
I0512 01:44:51.617139 139676058949632 gin_utils.py:85] mlp_kernel_init/linen.initializers.variance_scaling.scale = %SCALE
I0512 01:44:51.617171 139676058949632 gin_utils.py:85] 
I0512 01:44:51.617203 139676058949632 gin_utils.py:85] # Parameters for output_logits_kernel_init/linen.initializers.variance_scaling:
I0512 01:44:51.617235 139676058949632 gin_utils.py:85] # ==============================================================================
I0512 01:44:51.617266 139676058949632 gin_utils.py:85] output_logits_kernel_init/linen.initializers.variance_scaling.distribution = \
I0512 01:44:51.617298 139676058949632 gin_utils.py:85]     'truncated_normal'
I0512 01:44:51.617330 139676058949632 gin_utils.py:85] output_logits_kernel_init/linen.initializers.variance_scaling.mode = 'fan_in'
I0512 01:44:51.617362 139676058949632 gin_utils.py:85] output_logits_kernel_init/linen.initializers.variance_scaling.scale = %SCALE
I0512 01:44:51.618283 139676058949632 partitioning.py:559] `activation_partitioning_dims` = 2, `parameter_partitioning_dims` = 1
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
I0000 00:00:1715478291.675985  384045 pjrt_api.cc:98] GetPjrtApi was found for tpu at /home/rosinality/openmoe_venv/lib/python3.10/site-packages/libtpu/libtpu.so
I0000 00:00:1715478291.676030  384045 pjrt_api.cc:67] PJRT_Api is set for device type tpu
I0000 00:00:1715478291.676033  384045 pjrt_api.cc:72] PJRT plugin for tpu has PJRT API version 0.30. The framework PJRT API version is 0.30.
I0000 00:00:1715478294.486998  355987 tfrt_cpu_pjrt_client.cc:349] TfrtCpuClient created.
I0512 01:44:54.487224 140577131780096 train.py:196] Process ID: 2
I0000 00:00:1715478294.493169  373603 tfrt_cpu_pjrt_client.cc:349] TfrtCpuClient created.
I0512 01:44:54.493406 140172769531904 train.py:196] Process ID: 7
I0000 00:00:1715478294.499158  352750 tfrt_cpu_pjrt_client.cc:349] TfrtCpuClient created.
I0512 01:44:54.499398 139997397456896 train.py:196] Process ID: 1
I0000 00:00:1715478294.496634  382697 tfrt_cpu_pjrt_client.cc:349] TfrtCpuClient created.
I0512 01:44:54.496876 140316961150976 train.py:196] Process ID: 0
I0000 00:00:1715478294.486628  349683 tfrt_cpu_pjrt_client.cc:349] TfrtCpuClient created.
I0512 01:44:54.486825 139812357228544 train.py:196] Process ID: 5
I0000 00:00:1715478294.561740  351449 tfrt_cpu_pjrt_client.cc:349] TfrtCpuClient created.
I0512 01:44:54.561964 140188559153152 train.py:196] Process ID: 3
I0000 00:00:1715478294.574898  353284 tfrt_cpu_pjrt_client.cc:349] TfrtCpuClient created.
I0512 01:44:54.575100 140392160929792 train.py:196] Process ID: 6
I0000 00:00:1715478294.586603  384045 tfrt_cpu_pjrt_client.cc:349] TfrtCpuClient created.
I0512 01:44:54.586817 139676058949632 train.py:196] Process ID: 4
I0512 01:44:54.917580 140577131780096 train.py:266] Using seed for initialization and dropout RNG: 42
I0512 01:44:54.920687 139812357228544 train.py:266] Using seed for initialization and dropout RNG: 42
I0512 01:44:54.943891 139997397456896 train.py:266] Using seed for initialization and dropout RNG: 42
I0512 01:44:54.950223 140316961150976 train.py:266] Using seed for initialization and dropout RNG: 42
I0512 01:44:54.937206 139676058949632 train.py:266] Using seed for initialization and dropout RNG: 42
I0512 01:44:54.962098 140172769531904 train.py:266] Using seed for initialization and dropout RNG: 42
I0512 01:44:54.969537 140392160929792 train.py:266] Using seed for initialization and dropout RNG: 42
I0512 01:44:54.979285 140188559153152 train.py:266] Using seed for initialization and dropout RNG: 42
I0512 01:44:55.219038 140577131780096 partitioning.py:94] For MoE, first construct vanilla T5X (data, model) mesh.
I0512 01:44:55.219434 140577131780096 partitioning.py:151] last device coords : (1, 3, 3, 0)
last local device coords: (1, 1, 0, 0)
W0512 01:44:55.219630 140577131780096 partitioning.py:197] Tiling device assignment mesh by hosts, which may lead to reduced XLA collective performance. To avoid this, modify the model parallel submesh or run with more tasks per host.
I0512 01:44:55.219802 140577131780096 partitioning.py:255] global_mesh axis_names: ('data', 'model')
I0512 01:44:55.219856 140577131780096 partitioning.py:256] global_mesh devices: [[TpuDevice(id=0, process_index=0, coords=(0,0,0), core_on_chip=0)
  TpuDevice(id=2, process_index=0, coords=(0,1,0), core_on_chip=0)
  TpuDevice(id=1, process_index=0, coords=(1,0,0), core_on_chip=0)
  TpuDevice(id=3, process_index=0, coords=(1,1,0), core_on_chip=0)]
 [TpuDevice(id=8, process_index=2, coords=(0,0,1), core_on_chip=0)
  TpuDevice(id=10, process_index=2, coords=(0,1,1), core_on_chip=0)
  TpuDevice(id=9, process_index=2, coords=(1,0,1), core_on_chip=0)
  TpuDevice(id=11, process_index=2, coords=(1,1,1), core_on_chip=0)]
 [TpuDevice(id=16, process_index=4, coords=(0,0,2), core_on_chip=0)
  TpuDevice(id=18, process_index=4, coords=(0,1,2), core_on_chip=0)
  TpuDevice(id=17, process_index=4, coords=(1,0,2), core_on_chip=0)
  TpuDevice(id=19, process_index=4, coords=(1,1,2), core_on_chip=0)]
 [TpuDevice(id=24, process_index=6, coords=(0,0,3), core_on_chip=0)
  TpuDevice(id=26, process_index=6, coords=(0,1,3), core_on_chip=0)
  TpuDevice(id=25, process_index=6, coords=(1,0,3), core_on_chip=0)
  TpuDevice(id=27, process_index=6, coords=(1,1,3), core_on_chip=0)]
 [TpuDevice(id=4, process_index=1, coords=(0,2,0), core_on_chip=0)
  TpuDevice(id=6, process_index=1, coords=(0,3,0), core_on_chip=0)
  TpuDevice(id=5, process_index=1, coords=(1,2,0), core_on_chip=0)
  TpuDevice(id=7, process_index=1, coords=(1,3,0), core_on_chip=0)]
 [TpuDevice(id=12, process_index=3, coords=(0,2,1), core_on_chip=0)
  TpuDevice(id=14, process_index=3, coords=(0,3,1), core_on_chip=0)
  TpuDevice(id=13, process_index=3, coords=(1,2,1), core_on_chip=0)
  TpuDevice(id=15, process_index=3, coords=(1,3,1), core_on_chip=0)]
 [TpuDevice(id=20, process_index=5, coords=(0,2,2), core_on_chip=0)
  TpuDevice(id=22, process_index=5, coords=(0,3,2), core_on_chip=0)
  TpuDevice(id=21, process_index=5, coords=(1,2,2), core_on_chip=0)
  TpuDevice(id=23, process_index=5, coords=(1,3,2), core_on_chip=0)]
 [TpuDevice(id=28, process_index=7, coords=(0,2,3), core_on_chip=0)
  TpuDevice(id=30, process_index=7, coords=(0,3,3), core_on_chip=0)
  TpuDevice(id=29, process_index=7, coords=(1,2,3), core_on_chip=0)
  TpuDevice(id=31, process_index=7, coords=(1,3,3), core_on_chip=0)]]
I0512 01:44:55.220290 140577131780096 partitioning.py:257] global_mesh devices shape: (8, 4)
I0512 01:44:55.220356 140577131780096 partitioning.py:111] Overridden MoE global_mesh axes_names: ('data', 'expert', 'model')
I0512 01:44:55.220399 140577131780096 partitioning.py:114] Overridden MoE global_mesh devices: [[[TpuDevice(id=0, process_index=0, coords=(0,0,0), core_on_chip=0)
   TpuDevice(id=2, process_index=0, coords=(0,1,0), core_on_chip=0)
   TpuDevice(id=1, process_index=0, coords=(1,0,0), core_on_chip=0)
   TpuDevice(id=3, process_index=0, coords=(1,1,0), core_on_chip=0)]
  [TpuDevice(id=8, process_index=2, coords=(0,0,1), core_on_chip=0)
   TpuDevice(id=10, process_index=2, coords=(0,1,1), core_on_chip=0)
   TpuDevice(id=9, process_index=2, coords=(1,0,1), core_on_chip=0)
   TpuDevice(id=11, process_index=2, coords=(1,1,1), core_on_chip=0)]
  [TpuDevice(id=16, process_index=4, coords=(0,0,2), core_on_chip=0)
   TpuDevice(id=18, process_index=4, coords=(0,1,2), core_on_chip=0)
   TpuDevice(id=17, process_index=4, coords=(1,0,2), core_on_chip=0)
   TpuDevice(id=19, process_index=4, coords=(1,1,2), core_on_chip=0)]
  [TpuDevice(id=24, process_index=6, coords=(0,0,3), core_on_chip=0)
   TpuDevice(id=26, process_index=6, coords=(0,1,3), core_on_chip=0)
   TpuDevice(id=25, process_index=6, coords=(1,0,3), core_on_chip=0)
   TpuDevice(id=27, process_index=6, coords=(1,1,3), core_on_chip=0)]
  [TpuDevice(id=4, process_index=1, coords=(0,2,0), core_on_chip=0)
   TpuDevice(id=6, process_index=1, coords=(0,3,0), core_on_chip=0)
   TpuDevice(id=5, process_index=1, coords=(1,2,0), core_on_chip=0)
   TpuDevice(id=7, process_index=1, coords=(1,3,0), core_on_chip=0)]
  [TpuDevice(id=12, process_index=3, coords=(0,2,1), core_on_chip=0)
   TpuDevice(id=14, process_index=3, coords=(0,3,1), core_on_chip=0)
   TpuDevice(id=13, process_index=3, coords=(1,2,1), core_on_chip=0)
   TpuDevice(id=15, process_index=3, coords=(1,3,1), core_on_chip=0)]
  [TpuDevice(id=20, process_index=5, coords=(0,2,2), core_on_chip=0)
   TpuDevice(id=22, process_index=5, coords=(0,3,2), core_on_chip=0)
   TpuDevice(id=21, process_index=5, coords=(1,2,2), core_on_chip=0)
   TpuDevice(id=23, process_index=5, coords=(1,3,2), core_on_chip=0)]
  [TpuDevice(id=28, process_index=7, coords=(0,2,3), core_on_chip=0)
   TpuDevice(id=30, process_index=7, coords=(0,3,3), core_on_chip=0)
   TpuDevice(id=29, process_index=7, coords=(1,2,3), core_on_chip=0)
   TpuDevice(id=31, process_index=7, coords=(1,3,3), core_on_chip=0)]]]
I0512 01:44:55.220586 140577131780096 partitioning.py:115] Overridden MoE global_mesh shape: (1, 8, 4)
I0512 01:44:55.222745 139812357228544 partitioning.py:94] For MoE, first construct vanilla T5X (data, model) mesh.
I0512 01:44:55.223115 139812357228544 partitioning.py:151] last device coords : (1, 3, 3, 0)
last local device coords: (1, 1, 0, 0)
W0512 01:44:55.223317 139812357228544 partitioning.py:197] Tiling device assignment mesh by hosts, which may lead to reduced XLA collective performance. To avoid this, modify the model parallel submesh or run with more tasks per host.
I0512 01:44:55.223481 139812357228544 partitioning.py:255] global_mesh axis_names: ('data', 'model')
I0512 01:44:55.223558 139812357228544 partitioning.py:256] global_mesh devices: [[TpuDevice(id=0, process_index=0, coords=(0,0,0), core_on_chip=0)
  TpuDevice(id=2, process_index=0, coords=(0,1,0), core_on_chip=0)
  TpuDevice(id=1, process_index=0, coords=(1,0,0), core_on_chip=0)
  TpuDevice(id=3, process_index=0, coords=(1,1,0), core_on_chip=0)]
 [TpuDevice(id=8, process_index=2, coords=(0,0,1), core_on_chip=0)
  TpuDevice(id=10, process_index=2, coords=(0,1,1), core_on_chip=0)
  TpuDevice(id=9, process_index=2, coords=(1,0,1), core_on_chip=0)
  TpuDevice(id=11, process_index=2, coords=(1,1,1), core_on_chip=0)]
 [TpuDevice(id=16, process_index=4, coords=(0,0,2), core_on_chip=0)
  TpuDevice(id=18, process_index=4, coords=(0,1,2), core_on_chip=0)
  TpuDevice(id=17, process_index=4, coords=(1,0,2), core_on_chip=0)
  TpuDevice(id=19, process_index=4, coords=(1,1,2), core_on_chip=0)]
 [TpuDevice(id=24, process_index=6, coords=(0,0,3), core_on_chip=0)
  TpuDevice(id=26, process_index=6, coords=(0,1,3), core_on_chip=0)
  TpuDevice(id=25, process_index=6, coords=(1,0,3), core_on_chip=0)
  TpuDevice(id=27, process_index=6, coords=(1,1,3), core_on_chip=0)]
 [TpuDevice(id=4, process_index=1, coords=(0,2,0), core_on_chip=0)
  TpuDevice(id=6, process_index=1, coords=(0,3,0), core_on_chip=0)
  TpuDevice(id=5, process_index=1, coords=(1,2,0), core_on_chip=0)
  TpuDevice(id=7, process_index=1, coords=(1,3,0), core_on_chip=0)]
 [TpuDevice(id=12, process_index=3, coords=(0,2,1), core_on_chip=0)
  TpuDevice(id=14, process_index=3, coords=(0,3,1), core_on_chip=0)
  TpuDevice(id=13, process_index=3, coords=(1,2,1), core_on_chip=0)
  TpuDevice(id=15, process_index=3, coords=(1,3,1), core_on_chip=0)]
 [TpuDevice(id=20, process_index=5, coords=(0,2,2), core_on_chip=0)
  TpuDevice(id=22, process_index=5, coords=(0,3,2), core_on_chip=0)
  TpuDevice(id=21, process_index=5, coords=(1,2,2), core_on_chip=0)
  TpuDevice(id=23, process_index=5, coords=(1,3,2), core_on_chip=0)]
 [TpuDevice(id=28, process_index=7, coords=(0,2,3), core_on_chip=0)
  TpuDevice(id=30, process_index=7, coords=(0,3,3), core_on_chip=0)
  TpuDevice(id=29, process_index=7, coords=(1,2,3), core_on_chip=0)
  TpuDevice(id=31, process_index=7, coords=(1,3,3), core_on_chip=0)]]
I0512 01:44:55.224127 139812357228544 partitioning.py:257] global_mesh devices shape: (8, 4)
I0512 01:44:55.224219 139812357228544 partitioning.py:111] Overridden MoE global_mesh axes_names: ('data', 'expert', 'model')
I0512 01:44:55.224267 139812357228544 partitioning.py:114] Overridden MoE global_mesh devices: [[[TpuDevice(id=0, process_index=0, coords=(0,0,0), core_on_chip=0)
   TpuDevice(id=2, process_index=0, coords=(0,1,0), core_on_chip=0)
   TpuDevice(id=1, process_index=0, coords=(1,0,0), core_on_chip=0)
   TpuDevice(id=3, process_index=0, coords=(1,1,0), core_on_chip=0)]
  [TpuDevice(id=8, process_index=2, coords=(0,0,1), core_on_chip=0)
   TpuDevice(id=10, process_index=2, coords=(0,1,1), core_on_chip=0)
   TpuDevice(id=9, process_index=2, coords=(1,0,1), core_on_chip=0)
   TpuDevice(id=11, process_index=2, coords=(1,1,1), core_on_chip=0)]
  [TpuDevice(id=16, process_index=4, coords=(0,0,2), core_on_chip=0)
   TpuDevice(id=18, process_index=4, coords=(0,1,2), core_on_chip=0)
   TpuDevice(id=17, process_index=4, coords=(1,0,2), core_on_chip=0)
   TpuDevice(id=19, process_index=4, coords=(1,1,2), core_on_chip=0)]
  [TpuDevice(id=24, process_index=6, coords=(0,0,3), core_on_chip=0)
   TpuDevice(id=26, process_index=6, coords=(0,1,3), core_on_chip=0)
   TpuDevice(id=25, process_index=6, coords=(1,0,3), core_on_chip=0)
   TpuDevice(id=27, process_index=6, coords=(1,1,3), core_on_chip=0)]
  [TpuDevice(id=4, process_index=1, coords=(0,2,0), core_on_chip=0)
   TpuDevice(id=6, process_index=1, coords=(0,3,0), core_on_chip=0)
   TpuDevice(id=5, process_index=1, coords=(1,2,0), core_on_chip=0)
   TpuDevice(id=7, process_index=1, coords=(1,3,0), core_on_chip=0)]
  [TpuDevice(id=12, process_index=3, coords=(0,2,1), core_on_chip=0)
   TpuDevice(id=14, process_index=3, coords=(0,3,1), core_on_chip=0)
   TpuDevice(id=13, process_index=3, coords=(1,2,1), core_on_chip=0)
   TpuDevice(id=15, process_index=3, coords=(1,3,1), core_on_chip=0)]
  [TpuDevice(id=20, process_index=5, coords=(0,2,2), core_on_chip=0)
   TpuDevice(id=22, process_index=5, coords=(0,3,2), core_on_chip=0)
   TpuDevice(id=21, process_index=5, coords=(1,2,2), core_on_chip=0)
   TpuDevice(id=23, process_index=5, coords=(1,3,2), core_on_chip=0)]
  [TpuDevice(id=28, process_index=7, coords=(0,2,3), core_on_chip=0)
   TpuDevice(id=30, process_index=7, coords=(0,3,3), core_on_chip=0)
   TpuDevice(id=29, process_index=7, coords=(1,2,3), core_on_chip=0)
   TpuDevice(id=31, process_index=7, coords=(1,3,3), core_on_chip=0)]]]
I0512 01:44:55.224453 139812357228544 partitioning.py:115] Overridden MoE global_mesh shape: (1, 8, 4)
I0512 01:44:55.246429 139997397456896 partitioning.py:94] For MoE, first construct vanilla T5X (data, model) mesh.
I0512 01:44:55.246861 139997397456896 partitioning.py:151] last device coords : (1, 3, 3, 0)
last local device coords: (1, 1, 0, 0)
W0512 01:44:55.247077 139997397456896 partitioning.py:197] Tiling device assignment mesh by hosts, which may lead to reduced XLA collective performance. To avoid this, modify the model parallel submesh or run with more tasks per host.
I0512 01:44:55.247251 139997397456896 partitioning.py:255] global_mesh axis_names: ('data', 'model')
I0512 01:44:55.247318 139997397456896 partitioning.py:256] global_mesh devices: [[TpuDevice(id=0, process_index=0, coords=(0,0,0), core_on_chip=0)
  TpuDevice(id=2, process_index=0, coords=(0,1,0), core_on_chip=0)
  TpuDevice(id=1, process_index=0, coords=(1,0,0), core_on_chip=0)
  TpuDevice(id=3, process_index=0, coords=(1,1,0), core_on_chip=0)]
 [TpuDevice(id=8, process_index=2, coords=(0,0,1), core_on_chip=0)
  TpuDevice(id=10, process_index=2, coords=(0,1,1), core_on_chip=0)
  TpuDevice(id=9, process_index=2, coords=(1,0,1), core_on_chip=0)
  TpuDevice(id=11, process_index=2, coords=(1,1,1), core_on_chip=0)]
 [TpuDevice(id=16, process_index=4, coords=(0,0,2), core_on_chip=0)
  TpuDevice(id=18, process_index=4, coords=(0,1,2), core_on_chip=0)
  TpuDevice(id=17, process_index=4, coords=(1,0,2), core_on_chip=0)
  TpuDevice(id=19, process_index=4, coords=(1,1,2), core_on_chip=0)]
 [TpuDevice(id=24, process_index=6, coords=(0,0,3), core_on_chip=0)
  TpuDevice(id=26, process_index=6, coords=(0,1,3), core_on_chip=0)
  TpuDevice(id=25, process_index=6, coords=(1,0,3), core_on_chip=0)
  TpuDevice(id=27, process_index=6, coords=(1,1,3), core_on_chip=0)]
 [TpuDevice(id=4, process_index=1, coords=(0,2,0), core_on_chip=0)
  TpuDevice(id=6, process_index=1, coords=(0,3,0), core_on_chip=0)
  TpuDevice(id=5, process_index=1, coords=(1,2,0), core_on_chip=0)
  TpuDevice(id=7, process_index=1, coords=(1,3,0), core_on_chip=0)]
 [TpuDevice(id=12, process_index=3, coords=(0,2,1), core_on_chip=0)
  TpuDevice(id=14, process_index=3, coords=(0,3,1), core_on_chip=0)
  TpuDevice(id=13, process_index=3, coords=(1,2,1), core_on_chip=0)
  TpuDevice(id=15, process_index=3, coords=(1,3,1), core_on_chip=0)]
 [TpuDevice(id=20, process_index=5, coords=(0,2,2), core_on_chip=0)
  TpuDevice(id=22, process_index=5, coords=(0,3,2), core_on_chip=0)
  TpuDevice(id=21, process_index=5, coords=(1,2,2), core_on_chip=0)
  TpuDevice(id=23, process_index=5, coords=(1,3,2), core_on_chip=0)]
 [TpuDevice(id=28, process_index=7, coords=(0,2,3), core_on_chip=0)
  TpuDevice(id=30, process_index=7, coords=(0,3,3), core_on_chip=0)
  TpuDevice(id=29, process_index=7, coords=(1,2,3), core_on_chip=0)
  TpuDevice(id=31, process_index=7, coords=(1,3,3), core_on_chip=0)]]
I0512 01:44:55.247920 139997397456896 partitioning.py:257] global_mesh devices shape: (8, 4)
I0512 01:44:55.247987 139997397456896 partitioning.py:111] Overridden MoE global_mesh axes_names: ('data', 'expert', 'model')
I0512 01:44:55.248031 139997397456896 partitioning.py:114] Overridden MoE global_mesh devices: [[[TpuDevice(id=0, process_index=0, coords=(0,0,0), core_on_chip=0)
   TpuDevice(id=2, process_index=0, coords=(0,1,0), core_on_chip=0)
   TpuDevice(id=1, process_index=0, coords=(1,0,0), core_on_chip=0)
   TpuDevice(id=3, process_index=0, coords=(1,1,0), core_on_chip=0)]
  [TpuDevice(id=8, process_index=2, coords=(0,0,1), core_on_chip=0)
   TpuDevice(id=10, process_index=2, coords=(0,1,1), core_on_chip=0)
   TpuDevice(id=9, process_index=2, coords=(1,0,1), core_on_chip=0)
   TpuDevice(id=11, process_index=2, coords=(1,1,1), core_on_chip=0)]
  [TpuDevice(id=16, process_index=4, coords=(0,0,2), core_on_chip=0)
   TpuDevice(id=18, process_index=4, coords=(0,1,2), core_on_chip=0)
   TpuDevice(id=17, process_index=4, coords=(1,0,2), core_on_chip=0)
   TpuDevice(id=19, process_index=4, coords=(1,1,2), core_on_chip=0)]
  [TpuDevice(id=24, process_index=6, coords=(0,0,3), core_on_chip=0)
   TpuDevice(id=26, process_index=6, coords=(0,1,3), core_on_chip=0)
   TpuDevice(id=25, process_index=6, coords=(1,0,3), core_on_chip=0)
   TpuDevice(id=27, process_index=6, coords=(1,1,3), core_on_chip=0)]
  [TpuDevice(id=4, process_index=1, coords=(0,2,0), core_on_chip=0)
   TpuDevice(id=6, process_index=1, coords=(0,3,0), core_on_chip=0)
   TpuDevice(id=5, process_index=1, coords=(1,2,0), core_on_chip=0)
   TpuDevice(id=7, process_index=1, coords=(1,3,0), core_on_chip=0)]
  [TpuDevice(id=12, process_index=3, coords=(0,2,1), core_on_chip=0)
   TpuDevice(id=14, process_index=3, coords=(0,3,1), core_on_chip=0)
   TpuDevice(id=13, process_index=3, coords=(1,2,1), core_on_chip=0)
   TpuDevice(id=15, process_index=3, coords=(1,3,1), core_on_chip=0)]
  [TpuDevice(id=20, process_index=5, coords=(0,2,2), core_on_chip=0)
   TpuDevice(id=22, process_index=5, coords=(0,3,2), core_on_chip=0)
   TpuDevice(id=21, process_index=5, coords=(1,2,2), core_on_chip=0)
   TpuDevice(id=23, process_index=5, coords=(1,3,2), core_on_chip=0)]
  [TpuDevice(id=28, process_index=7, coords=(0,2,3), core_on_chip=0)
   TpuDevice(id=30, process_index=7, coords=(0,3,3), core_on_chip=0)
   TpuDevice(id=29, process_index=7, coords=(1,2,3), core_on_chip=0)
   TpuDevice(id=31, process_index=7, coords=(1,3,3), core_on_chip=0)]]]
I0512 01:44:55.248216 139997397456896 partitioning.py:115] Overridden MoE global_mesh shape: (1, 8, 4)
I0512 01:44:55.231046 139676058949632 partitioning.py:94] For MoE, first construct vanilla T5X (data, model) mesh.
I0512 01:44:55.231400 139676058949632 partitioning.py:151] last device coords : (1, 3, 3, 0)
last local device coords: (1, 1, 0, 0)
W0512 01:44:55.231611 139676058949632 partitioning.py:197] Tiling device assignment mesh by hosts, which may lead to reduced XLA collective performance. To avoid this, modify the model parallel submesh or run with more tasks per host.
I0512 01:44:55.231791 139676058949632 partitioning.py:255] global_mesh axis_names: ('data', 'model')
I0512 01:44:55.231846 139676058949632 partitioning.py:256] global_mesh devices: [[TpuDevice(id=0, process_index=0, coords=(0,0,0), core_on_chip=0)
  TpuDevice(id=2, process_index=0, coords=(0,1,0), core_on_chip=0)
  TpuDevice(id=1, process_index=0, coords=(1,0,0), core_on_chip=0)
  TpuDevice(id=3, process_index=0, coords=(1,1,0), core_on_chip=0)]
 [TpuDevice(id=8, process_index=2, coords=(0,0,1), core_on_chip=0)
  TpuDevice(id=10, process_index=2, coords=(0,1,1), core_on_chip=0)
  TpuDevice(id=9, process_index=2, coords=(1,0,1), core_on_chip=0)
  TpuDevice(id=11, process_index=2, coords=(1,1,1), core_on_chip=0)]
 [TpuDevice(id=16, process_index=4, coords=(0,0,2), core_on_chip=0)
  TpuDevice(id=18, process_index=4, coords=(0,1,2), core_on_chip=0)
  TpuDevice(id=17, process_index=4, coords=(1,0,2), core_on_chip=0)
  TpuDevice(id=19, process_index=4, coords=(1,1,2), core_on_chip=0)]
 [TpuDevice(id=24, process_index=6, coords=(0,0,3), core_on_chip=0)
  TpuDevice(id=26, process_index=6, coords=(0,1,3), core_on_chip=0)
  TpuDevice(id=25, process_index=6, coords=(1,0,3), core_on_chip=0)
  TpuDevice(id=27, process_index=6, coords=(1,1,3), core_on_chip=0)]
 [TpuDevice(id=4, process_index=1, coords=(0,2,0), core_on_chip=0)
  TpuDevice(id=6, process_index=1, coords=(0,3,0), core_on_chip=0)
  TpuDevice(id=5, process_index=1, coords=(1,2,0), core_on_chip=0)
  TpuDevice(id=7, process_index=1, coords=(1,3,0), core_on_chip=0)]
 [TpuDevice(id=12, process_index=3, coords=(0,2,1), core_on_chip=0)
  TpuDevice(id=14, process_index=3, coords=(0,3,1), core_on_chip=0)
  TpuDevice(id=13, process_index=3, coords=(1,2,1), core_on_chip=0)
  TpuDevice(id=15, process_index=3, coords=(1,3,1), core_on_chip=0)]
 [TpuDevice(id=20, process_index=5, coords=(0,2,2), core_on_chip=0)
  TpuDevice(id=22, process_index=5, coords=(0,3,2), core_on_chip=0)
  TpuDevice(id=21, process_index=5, coords=(1,2,2), core_on_chip=0)
  TpuDevice(id=23, process_index=5, coords=(1,3,2), core_on_chip=0)]
 [TpuDevice(id=28, process_index=7, coords=(0,2,3), core_on_chip=0)
  TpuDevice(id=30, process_index=7, coords=(0,3,3), core_on_chip=0)
  TpuDevice(id=29, process_index=7, coords=(1,2,3), core_on_chip=0)
  TpuDevice(id=31, process_index=7, coords=(1,3,3), core_on_chip=0)]]
I0512 01:44:55.232378 139676058949632 partitioning.py:257] global_mesh devices shape: (8, 4)
I0512 01:44:55.232444 139676058949632 partitioning.py:111] Overridden MoE global_mesh axes_names: ('data', 'expert', 'model')
I0512 01:44:55.232494 139676058949632 partitioning.py:114] Overridden MoE global_mesh devices: [[[TpuDevice(id=0, process_index=0, coords=(0,0,0), core_on_chip=0)
   TpuDevice(id=2, process_index=0, coords=(0,1,0), core_on_chip=0)
   TpuDevice(id=1, process_index=0, coords=(1,0,0), core_on_chip=0)
   TpuDevice(id=3, process_index=0, coords=(1,1,0), core_on_chip=0)]
  [TpuDevice(id=8, process_index=2, coords=(0,0,1), core_on_chip=0)
   TpuDevice(id=10, process_index=2, coords=(0,1,1), core_on_chip=0)
   TpuDevice(id=9, process_index=2, coords=(1,0,1), core_on_chip=0)
   TpuDevice(id=11, process_index=2, coords=(1,1,1), core_on_chip=0)]
  [TpuDevice(id=16, process_index=4, coords=(0,0,2), core_on_chip=0)
   TpuDevice(id=18, process_index=4, coords=(0,1,2), core_on_chip=0)
   TpuDevice(id=17, process_index=4, coords=(1,0,2), core_on_chip=0)
   TpuDevice(id=19, process_index=4, coords=(1,1,2), core_on_chip=0)]
  [TpuDevice(id=24, process_index=6, coords=(0,0,3), core_on_chip=0)
   TpuDevice(id=26, process_index=6, coords=(0,1,3), core_on_chip=0)
   TpuDevice(id=25, process_index=6, coords=(1,0,3), core_on_chip=0)
   TpuDevice(id=27, process_index=6, coords=(1,1,3), core_on_chip=0)]
  [TpuDevice(id=4, process_index=1, coords=(0,2,0), core_on_chip=0)
   TpuDevice(id=6, process_index=1, coords=(0,3,0), core_on_chip=0)
   TpuDevice(id=5, process_index=1, coords=(1,2,0), core_on_chip=0)
   TpuDevice(id=7, process_index=1, coords=(1,3,0), core_on_chip=0)]
  [TpuDevice(id=12, process_index=3, coords=(0,2,1), core_on_chip=0)
   TpuDevice(id=14, process_index=3, coords=(0,3,1), core_on_chip=0)
   TpuDevice(id=13, process_index=3, coords=(1,2,1), core_on_chip=0)
   TpuDevice(id=15, process_index=3, coords=(1,3,1), core_on_chip=0)]
  [TpuDevice(id=20, process_index=5, coords=(0,2,2), core_on_chip=0)
   TpuDevice(id=22, process_index=5, coords=(0,3,2), core_on_chip=0)
   TpuDevice(id=21, process_index=5, coords=(1,2,2), core_on_chip=0)
   TpuDevice(id=23, process_index=5, coords=(1,3,2), core_on_chip=0)]
  [TpuDevice(id=28, process_index=7, coords=(0,2,3), core_on_chip=0)
   TpuDevice(id=30, process_index=7, coords=(0,3,3), core_on_chip=0)
   TpuDevice(id=29, process_index=7, coords=(1,2,3), core_on_chip=0)
   TpuDevice(id=31, process_index=7, coords=(1,3,3), core_on_chip=0)]]]
I0512 01:44:55.232681 139676058949632 partitioning.py:115] Overridden MoE global_mesh shape: (1, 8, 4)
I0512 01:44:55.258334 140316961150976 partitioning.py:94] For MoE, first construct vanilla T5X (data, model) mesh.
I0512 01:44:55.258727 140316961150976 partitioning.py:151] last device coords : (1, 3, 3, 0)
last local device coords: (1, 1, 0, 0)
W0512 01:44:55.258945 140316961150976 partitioning.py:197] Tiling device assignment mesh by hosts, which may lead to reduced XLA collective performance. To avoid this, modify the model parallel submesh or run with more tasks per host.
I0512 01:44:55.264873 140392160929792 partitioning.py:94] For MoE, first construct vanilla T5X (data, model) mesh.
I0512 01:44:55.259156 140316961150976 partitioning.py:255] global_mesh axis_names: ('data', 'model')
I0512 01:44:55.259213 140316961150976 partitioning.py:256] global_mesh devices: [[TpuDevice(id=0, process_index=0, coords=(0,0,0), core_on_chip=0)
  TpuDevice(id=2, process_index=0, coords=(0,1,0), core_on_chip=0)
  TpuDevice(id=1, process_index=0, coords=(1,0,0), core_on_chip=0)
  TpuDevice(id=3, process_index=0, coords=(1,1,0), core_on_chip=0)]
 [TpuDevice(id=8, process_index=2, coords=(0,0,1), core_on_chip=0)
  TpuDevice(id=10, process_index=2, coords=(0,1,1), core_on_chip=0)
  TpuDevice(id=9, process_index=2, coords=(1,0,1), core_on_chip=0)
  TpuDevice(id=11, process_index=2, coords=(1,1,1), core_on_chip=0)]
 [TpuDevice(id=16, process_index=4, coords=(0,0,2), core_on_chip=0)
  TpuDevice(id=18, process_index=4, coords=(0,1,2), core_on_chip=0)
  TpuDevice(id=17, process_index=4, coords=(1,0,2), core_on_chip=0)
  TpuDevice(id=19, process_index=4, coords=(1,1,2), core_on_chip=0)]
 [TpuDevice(id=24, process_index=6, coords=(0,0,3), core_on_chip=0)
  TpuDevice(id=26, process_index=6, coords=(0,1,3), core_on_chip=0)
  TpuDevice(id=25, process_index=6, coords=(1,0,3), core_on_chip=0)
  TpuDevice(id=27, process_index=6, coords=(1,1,3), core_on_chip=0)]
 [TpuDevice(id=4, process_index=1, coords=(0,2,0), core_on_chip=0)
  TpuDevice(id=6, process_index=1, coords=(0,3,0), core_on_chip=0)
  TpuDevice(id=5, process_index=1, coords=(1,2,0), core_on_chip=0)
  TpuDevice(id=7, process_index=1, coords=(1,3,0), core_on_chip=0)]
 [TpuDevice(id=12, process_index=3, coords=(0,2,1), core_on_chip=0)
  TpuDevice(id=14, process_index=3, coords=(0,3,1), core_on_chip=0)
  TpuDevice(id=13, process_index=3, coords=(1,2,1), core_on_chip=0)
  TpuDevice(id=15, process_index=3, coords=(1,3,1), core_on_chip=0)]
 [TpuDevice(id=20, process_index=5, coords=(0,2,2), core_on_chip=0)
  TpuDevice(id=22, process_index=5, coords=(0,3,2), core_on_chip=0)
  TpuDevice(id=21, process_index=5, coords=(1,2,2), core_on_chip=0)
  TpuDevice(id=23, process_index=5, coords=(1,3,2), core_on_chip=0)]
 [TpuDevice(id=28, process_index=7, coords=(0,2,3), core_on_chip=0)
  TpuDevice(id=30, process_index=7, coords=(0,3,3), core_on_chip=0)
  TpuDevice(id=29, process_index=7, coords=(1,2,3), core_on_chip=0)
  TpuDevice(id=31, process_index=7, coords=(1,3,3), core_on_chip=0)]]
I0512 01:44:55.265230 140392160929792 partitioning.py:151] last device coords : (1, 3, 3, 0)
last local device coords: (1, 1, 0, 0)
W0512 01:44:55.265417 140392160929792 partitioning.py:197] Tiling device assignment mesh by hosts, which may lead to reduced XLA collective performance. To avoid this, modify the model parallel submesh or run with more tasks per host.
I0512 01:44:55.259793 140316961150976 partitioning.py:257] global_mesh devices shape: (8, 4)
I0512 01:44:55.259863 140316961150976 partitioning.py:111] Overridden MoE global_mesh axes_names: ('data', 'expert', 'model')
I0512 01:44:55.259907 140316961150976 partitioning.py:114] Overridden MoE global_mesh devices: [[[TpuDevice(id=0, process_index=0, coords=(0,0,0), core_on_chip=0)
   TpuDevice(id=2, process_index=0, coords=(0,1,0), core_on_chip=0)
   TpuDevice(id=1, process_index=0, coords=(1,0,0), core_on_chip=0)
   TpuDevice(id=3, process_index=0, coords=(1,1,0), core_on_chip=0)]
  [TpuDevice(id=8, process_index=2, coords=(0,0,1), core_on_chip=0)
   TpuDevice(id=10, process_index=2, coords=(0,1,1), core_on_chip=0)
   TpuDevice(id=9, process_index=2, coords=(1,0,1), core_on_chip=0)
   TpuDevice(id=11, process_index=2, coords=(1,1,1), core_on_chip=0)]
  [TpuDevice(id=16, process_index=4, coords=(0,0,2), core_on_chip=0)
   TpuDevice(id=18, process_index=4, coords=(0,1,2), core_on_chip=0)
   TpuDevice(id=17, process_index=4, coords=(1,0,2), core_on_chip=0)
   TpuDevice(id=19, process_index=4, coords=(1,1,2), core_on_chip=0)]
  [TpuDevice(id=24, process_index=6, coords=(0,0,3), core_on_chip=0)
   TpuDevice(id=26, process_index=6, coords=(0,1,3), core_on_chip=0)
   TpuDevice(id=25, process_index=6, coords=(1,0,3), core_on_chip=0)
   TpuDevice(id=27, process_index=6, coords=(1,1,3), core_on_chip=0)]
  [TpuDevice(id=4, process_index=1, coords=(0,2,0), core_on_chip=0)
   TpuDevice(id=6, process_index=1, coords=(0,3,0), core_on_chip=0)
   TpuDevice(id=5, process_index=1, coords=(1,2,0), core_on_chip=0)
   TpuDevice(id=7, process_index=1, coords=(1,3,0), core_on_chip=0)]
  [TpuDevice(id=12, process_index=3, coords=(0,2,1), core_on_chip=0)
   TpuDevice(id=14, process_index=3, coords=(0,3,1), core_on_chip=0)
   TpuDevice(id=13, process_index=3, coords=(1,2,1), core_on_chip=0)
   TpuDevice(id=15, process_index=3, coords=(1,3,1), core_on_chip=0)]
  [TpuDevice(id=20, process_index=5, coords=(0,2,2), core_on_chip=0)
   TpuDevice(id=22, process_index=5, coords=(0,3,2), core_on_chip=0)
   TpuDevice(id=21, process_index=5, coords=(1,2,2), core_on_chip=0)
   TpuDevice(id=23, process_index=5, coords=(1,3,2), core_on_chip=0)]
  [TpuDevice(id=28, process_index=7, coords=(0,2,3), core_on_chip=0)
   TpuDevice(id=30, process_index=7, coords=(0,3,3), core_on_chip=0)
   TpuDevice(id=29, process_index=7, coords=(1,2,3), core_on_chip=0)
   TpuDevice(id=31, process_index=7, coords=(1,3,3), core_on_chip=0)]]]
I0512 01:44:55.260097 140316961150976 partitioning.py:115] Overridden MoE global_mesh shape: (1, 8, 4)
I0512 01:44:55.265576 140392160929792 partitioning.py:255] global_mesh axis_names: ('data', 'model')
I0512 01:44:55.265630 140392160929792 partitioning.py:256] global_mesh devices: [[TpuDevice(id=0, process_index=0, coords=(0,0,0), core_on_chip=0)
  TpuDevice(id=2, process_index=0, coords=(0,1,0), core_on_chip=0)
  TpuDevice(id=1, process_index=0, coords=(1,0,0), core_on_chip=0)
  TpuDevice(id=3, process_index=0, coords=(1,1,0), core_on_chip=0)]
 [TpuDevice(id=8, process_index=2, coords=(0,0,1), core_on_chip=0)
  TpuDevice(id=10, process_index=2, coords=(0,1,1), core_on_chip=0)
  TpuDevice(id=9, process_index=2, coords=(1,0,1), core_on_chip=0)
  TpuDevice(id=11, process_index=2, coords=(1,1,1), core_on_chip=0)]
 [TpuDevice(id=16, process_index=4, coords=(0,0,2), core_on_chip=0)
  TpuDevice(id=18, process_index=4, coords=(0,1,2), core_on_chip=0)
  TpuDevice(id=17, process_index=4, coords=(1,0,2), core_on_chip=0)
  TpuDevice(id=19, process_index=4, coords=(1,1,2), core_on_chip=0)]
 [TpuDevice(id=24, process_index=6, coords=(0,0,3), core_on_chip=0)
  TpuDevice(id=26, process_index=6, coords=(0,1,3), core_on_chip=0)
  TpuDevice(id=25, process_index=6, coords=(1,0,3), core_on_chip=0)
  TpuDevice(id=27, process_index=6, coords=(1,1,3), core_on_chip=0)]
 [TpuDevice(id=4, process_index=1, coords=(0,2,0), core_on_chip=0)
  TpuDevice(id=6, process_index=1, coords=(0,3,0), core_on_chip=0)
  TpuDevice(id=5, process_index=1, coords=(1,2,0), core_on_chip=0)
  TpuDevice(id=7, process_index=1, coords=(1,3,0), core_on_chip=0)]
 [TpuDevice(id=12, process_index=3, coords=(0,2,1), core_on_chip=0)
  TpuDevice(id=14, process_index=3, coords=(0,3,1), core_on_chip=0)
  TpuDevice(id=13, process_index=3, coords=(1,2,1), core_on_chip=0)
  TpuDevice(id=15, process_index=3, coords=(1,3,1), core_on_chip=0)]
 [TpuDevice(id=20, process_index=5, coords=(0,2,2), core_on_chip=0)
  TpuDevice(id=22, process_index=5, coords=(0,3,2), core_on_chip=0)
  TpuDevice(id=21, process_index=5, coords=(1,2,2), core_on_chip=0)
  TpuDevice(id=23, process_index=5, coords=(1,3,2), core_on_chip=0)]
 [TpuDevice(id=28, process_index=7, coords=(0,2,3), core_on_chip=0)
  TpuDevice(id=30, process_index=7, coords=(0,3,3), core_on_chip=0)
  TpuDevice(id=29, process_index=7, coords=(1,2,3), core_on_chip=0)
  TpuDevice(id=31, process_index=7, coords=(1,3,3), core_on_chip=0)]]
I0512 01:44:55.266051 140392160929792 partitioning.py:257] global_mesh devices shape: (8, 4)
I0512 01:44:55.266145 140392160929792 partitioning.py:111] Overridden MoE global_mesh axes_names: ('data', 'expert', 'model')
I0512 01:44:55.266191 140392160929792 partitioning.py:114] Overridden MoE global_mesh devices: [[[TpuDevice(id=0, process_index=0, coords=(0,0,0), core_on_chip=0)
   TpuDevice(id=2, process_index=0, coords=(0,1,0), core_on_chip=0)
   TpuDevice(id=1, process_index=0, coords=(1,0,0), core_on_chip=0)
   TpuDevice(id=3, process_index=0, coords=(1,1,0), core_on_chip=0)]
  [TpuDevice(id=8, process_index=2, coords=(0,0,1), core_on_chip=0)
   TpuDevice(id=10, process_index=2, coords=(0,1,1), core_on_chip=0)
   TpuDevice(id=9, process_index=2, coords=(1,0,1), core_on_chip=0)
   TpuDevice(id=11, process_index=2, coords=(1,1,1), core_on_chip=0)]
  [TpuDevice(id=16, process_index=4, coords=(0,0,2), core_on_chip=0)
   TpuDevice(id=18, process_index=4, coords=(0,1,2), core_on_chip=0)
   TpuDevice(id=17, process_index=4, coords=(1,0,2), core_on_chip=0)
   TpuDevice(id=19, process_index=4, coords=(1,1,2), core_on_chip=0)]
  [TpuDevice(id=24, process_index=6, coords=(0,0,3), core_on_chip=0)
   TpuDevice(id=26, process_index=6, coords=(0,1,3), core_on_chip=0)
   TpuDevice(id=25, process_index=6, coords=(1,0,3), core_on_chip=0)
   TpuDevice(id=27, process_index=6, coords=(1,1,3), core_on_chip=0)]
  [TpuDevice(id=4, process_index=1, coords=(0,2,0), core_on_chip=0)
   TpuDevice(id=6, process_index=1, coords=(0,3,0), core_on_chip=0)
   TpuDevice(id=5, process_index=1, coords=(1,2,0), core_on_chip=0)
   TpuDevice(id=7, process_index=1, coords=(1,3,0), core_on_chip=0)]
  [TpuDevice(id=12, process_index=3, coords=(0,2,1), core_on_chip=0)
   TpuDevice(id=14, process_index=3, coords=(0,3,1), core_on_chip=0)
   TpuDevice(id=13, process_index=3, coords=(1,2,1), core_on_chip=0)
   TpuDevice(id=15, process_index=3, coords=(1,3,1), core_on_chip=0)]
  [TpuDevice(id=20, process_index=5, coords=(0,2,2), core_on_chip=0)
   TpuDevice(id=22, process_index=5, coords=(0,3,2), core_on_chip=0)
   TpuDevice(id=21, process_index=5, coords=(1,2,2), core_on_chip=0)
   TpuDevice(id=23, process_index=5, coords=(1,3,2), core_on_chip=0)]
  [TpuDevice(id=28, process_index=7, coords=(0,2,3), core_on_chip=0)
   TpuDevice(id=30, process_index=7, coords=(0,3,3), core_on_chip=0)
   TpuDevice(id=29, process_index=7, coords=(1,2,3), core_on_chip=0)
   TpuDevice(id=31, process_index=7, coords=(1,3,3), core_on_chip=0)]]]
I0512 01:44:55.266379 140392160929792 partitioning.py:115] Overridden MoE global_mesh shape: (1, 8, 4)
I0512 01:44:55.274237 140172769531904 partitioning.py:94] For MoE, first construct vanilla T5X (data, model) mesh.
I0512 01:44:55.274687 140172769531904 partitioning.py:151] last device coords : (1, 3, 3, 0)
last local device coords: (1, 1, 0, 0)
W0512 01:44:55.274908 140172769531904 partitioning.py:197] Tiling device assignment mesh by hosts, which may lead to reduced XLA collective performance. To avoid this, modify the model parallel submesh or run with more tasks per host.
I0512 01:44:55.275098 140172769531904 partitioning.py:255] global_mesh axis_names: ('data', 'model')
I0512 01:44:55.275155 140172769531904 partitioning.py:256] global_mesh devices: [[TpuDevice(id=0, process_index=0, coords=(0,0,0), core_on_chip=0)
  TpuDevice(id=2, process_index=0, coords=(0,1,0), core_on_chip=0)
  TpuDevice(id=1, process_index=0, coords=(1,0,0), core_on_chip=0)
  TpuDevice(id=3, process_index=0, coords=(1,1,0), core_on_chip=0)]
 [TpuDevice(id=8, process_index=2, coords=(0,0,1), core_on_chip=0)
  TpuDevice(id=10, process_index=2, coords=(0,1,1), core_on_chip=0)
  TpuDevice(id=9, process_index=2, coords=(1,0,1), core_on_chip=0)
  TpuDevice(id=11, process_index=2, coords=(1,1,1), core_on_chip=0)]
 [TpuDevice(id=16, process_index=4, coords=(0,0,2), core_on_chip=0)
  TpuDevice(id=18, process_index=4, coords=(0,1,2), core_on_chip=0)
  TpuDevice(id=17, process_index=4, coords=(1,0,2), core_on_chip=0)
  TpuDevice(id=19, process_index=4, coords=(1,1,2), core_on_chip=0)]
 [TpuDevice(id=24, process_index=6, coords=(0,0,3), core_on_chip=0)
  TpuDevice(id=26, process_index=6, coords=(0,1,3), core_on_chip=0)
  TpuDevice(id=25, process_index=6, coords=(1,0,3), core_on_chip=0)
  TpuDevice(id=27, process_index=6, coords=(1,1,3), core_on_chip=0)]
 [TpuDevice(id=4, process_index=1, coords=(0,2,0), core_on_chip=0)
  TpuDevice(id=6, process_index=1, coords=(0,3,0), core_on_chip=0)
  TpuDevice(id=5, process_index=1, coords=(1,2,0), core_on_chip=0)
  TpuDevice(id=7, process_index=1, coords=(1,3,0), core_on_chip=0)]
 [TpuDevice(id=12, process_index=3, coords=(0,2,1), core_on_chip=0)
  TpuDevice(id=14, process_index=3, coords=(0,3,1), core_on_chip=0)
  TpuDevice(id=13, process_index=3, coords=(1,2,1), core_on_chip=0)
  TpuDevice(id=15, process_index=3, coords=(1,3,1), core_on_chip=0)]
 [TpuDevice(id=20, process_index=5, coords=(0,2,2), core_on_chip=0)
  TpuDevice(id=22, process_index=5, coords=(0,3,2), core_on_chip=0)
  TpuDevice(id=21, process_index=5, coords=(1,2,2), core_on_chip=0)
  TpuDevice(id=23, process_index=5, coords=(1,3,2), core_on_chip=0)]
 [TpuDevice(id=28, process_index=7, coords=(0,2,3), core_on_chip=0)
  TpuDevice(id=30, process_index=7, coords=(0,3,3), core_on_chip=0)
  TpuDevice(id=29, process_index=7, coords=(1,2,3), core_on_chip=0)
  TpuDevice(id=31, process_index=7, coords=(1,3,3), core_on_chip=0)]]
I0512 01:44:55.275761 140172769531904 partitioning.py:257] global_mesh devices shape: (8, 4)
I0512 01:44:55.275828 140172769531904 partitioning.py:111] Overridden MoE global_mesh axes_names: ('data', 'expert', 'model')
I0512 01:44:55.275872 140172769531904 partitioning.py:114] Overridden MoE global_mesh devices: [[[TpuDevice(id=0, process_index=0, coords=(0,0,0), core_on_chip=0)
   TpuDevice(id=2, process_index=0, coords=(0,1,0), core_on_chip=0)
   TpuDevice(id=1, process_index=0, coords=(1,0,0), core_on_chip=0)
   TpuDevice(id=3, process_index=0, coords=(1,1,0), core_on_chip=0)]
  [TpuDevice(id=8, process_index=2, coords=(0,0,1), core_on_chip=0)
   TpuDevice(id=10, process_index=2, coords=(0,1,1), core_on_chip=0)
   TpuDevice(id=9, process_index=2, coords=(1,0,1), core_on_chip=0)
   TpuDevice(id=11, process_index=2, coords=(1,1,1), core_on_chip=0)]
  [TpuDevice(id=16, process_index=4, coords=(0,0,2), core_on_chip=0)
   TpuDevice(id=18, process_index=4, coords=(0,1,2), core_on_chip=0)
   TpuDevice(id=17, process_index=4, coords=(1,0,2), core_on_chip=0)
   TpuDevice(id=19, process_index=4, coords=(1,1,2), core_on_chip=0)]
  [TpuDevice(id=24, process_index=6, coords=(0,0,3), core_on_chip=0)
   TpuDevice(id=26, process_index=6, coords=(0,1,3), core_on_chip=0)
   TpuDevice(id=25, process_index=6, coords=(1,0,3), core_on_chip=0)
   TpuDevice(id=27, process_index=6, coords=(1,1,3), core_on_chip=0)]
  [TpuDevice(id=4, process_index=1, coords=(0,2,0), core_on_chip=0)
   TpuDevice(id=6, process_index=1, coords=(0,3,0), core_on_chip=0)
   TpuDevice(id=5, process_index=1, coords=(1,2,0), core_on_chip=0)
   TpuDevice(id=7, process_index=1, coords=(1,3,0), core_on_chip=0)]
  [TpuDevice(id=12, process_index=3, coords=(0,2,1), core_on_chip=0)
   TpuDevice(id=14, process_index=3, coords=(0,3,1), core_on_chip=0)
   TpuDevice(id=13, process_index=3, coords=(1,2,1), core_on_chip=0)
   TpuDevice(id=15, process_index=3, coords=(1,3,1), core_on_chip=0)]
  [TpuDevice(id=20, process_index=5, coords=(0,2,2), core_on_chip=0)
   TpuDevice(id=22, process_index=5, coords=(0,3,2), core_on_chip=0)
   TpuDevice(id=21, process_index=5, coords=(1,2,2), core_on_chip=0)
   TpuDevice(id=23, process_index=5, coords=(1,3,2), core_on_chip=0)]
  [TpuDevice(id=28, process_index=7, coords=(0,2,3), core_on_chip=0)
   TpuDevice(id=30, process_index=7, coords=(0,3,3), core_on_chip=0)
   TpuDevice(id=29, process_index=7, coords=(1,2,3), core_on_chip=0)
   TpuDevice(id=31, process_index=7, coords=(1,3,3), core_on_chip=0)]]]
I0512 01:44:55.276062 140172769531904 partitioning.py:115] Overridden MoE global_mesh shape: (1, 8, 4)
I0512 01:44:55.288803 140188559153152 partitioning.py:94] For MoE, first construct vanilla T5X (data, model) mesh.
I0512 01:44:55.289191 140188559153152 partitioning.py:151] last device coords : (1, 3, 3, 0)
last local device coords: (1, 1, 0, 0)
W0512 01:44:55.289400 140188559153152 partitioning.py:197] Tiling device assignment mesh by hosts, which may lead to reduced XLA collective performance. To avoid this, modify the model parallel submesh or run with more tasks per host.
I0512 01:44:55.289579 140188559153152 partitioning.py:255] global_mesh axis_names: ('data', 'model')
I0512 01:44:55.289633 140188559153152 partitioning.py:256] global_mesh devices: [[TpuDevice(id=0, process_index=0, coords=(0,0,0), core_on_chip=0)
  TpuDevice(id=2, process_index=0, coords=(0,1,0), core_on_chip=0)
  TpuDevice(id=1, process_index=0, coords=(1,0,0), core_on_chip=0)
  TpuDevice(id=3, process_index=0, coords=(1,1,0), core_on_chip=0)]
 [TpuDevice(id=8, process_index=2, coords=(0,0,1), core_on_chip=0)
  TpuDevice(id=10, process_index=2, coords=(0,1,1), core_on_chip=0)
  TpuDevice(id=9, process_index=2, coords=(1,0,1), core_on_chip=0)
  TpuDevice(id=11, process_index=2, coords=(1,1,1), core_on_chip=0)]
 [TpuDevice(id=16, process_index=4, coords=(0,0,2), core_on_chip=0)
  TpuDevice(id=18, process_index=4, coords=(0,1,2), core_on_chip=0)
  TpuDevice(id=17, process_index=4, coords=(1,0,2), core_on_chip=0)
  TpuDevice(id=19, process_index=4, coords=(1,1,2), core_on_chip=0)]
 [TpuDevice(id=24, process_index=6, coords=(0,0,3), core_on_chip=0)
  TpuDevice(id=26, process_index=6, coords=(0,1,3), core_on_chip=0)
  TpuDevice(id=25, process_index=6, coords=(1,0,3), core_on_chip=0)
  TpuDevice(id=27, process_index=6, coords=(1,1,3), core_on_chip=0)]
 [TpuDevice(id=4, process_index=1, coords=(0,2,0), core_on_chip=0)
  TpuDevice(id=6, process_index=1, coords=(0,3,0), core_on_chip=0)
  TpuDevice(id=5, process_index=1, coords=(1,2,0), core_on_chip=0)
  TpuDevice(id=7, process_index=1, coords=(1,3,0), core_on_chip=0)]
 [TpuDevice(id=12, process_index=3, coords=(0,2,1), core_on_chip=0)
  TpuDevice(id=14, process_index=3, coords=(0,3,1), core_on_chip=0)
  TpuDevice(id=13, process_index=3, coords=(1,2,1), core_on_chip=0)
  TpuDevice(id=15, process_index=3, coords=(1,3,1), core_on_chip=0)]
 [TpuDevice(id=20, process_index=5, coords=(0,2,2), core_on_chip=0)
  TpuDevice(id=22, process_index=5, coords=(0,3,2), core_on_chip=0)
  TpuDevice(id=21, process_index=5, coords=(1,2,2), core_on_chip=0)
  TpuDevice(id=23, process_index=5, coords=(1,3,2), core_on_chip=0)]
 [TpuDevice(id=28, process_index=7, coords=(0,2,3), core_on_chip=0)
  TpuDevice(id=30, process_index=7, coords=(0,3,3), core_on_chip=0)
  TpuDevice(id=29, process_index=7, coords=(1,2,3), core_on_chip=0)
  TpuDevice(id=31, process_index=7, coords=(1,3,3), core_on_chip=0)]]
I0512 01:44:55.290287 140188559153152 partitioning.py:257] global_mesh devices shape: (8, 4)
I0512 01:44:55.290364 140188559153152 partitioning.py:111] Overridden MoE global_mesh axes_names: ('data', 'expert', 'model')
I0512 01:44:55.290412 140188559153152 partitioning.py:114] Overridden MoE global_mesh devices: [[[TpuDevice(id=0, process_index=0, coords=(0,0,0), core_on_chip=0)
   TpuDevice(id=2, process_index=0, coords=(0,1,0), core_on_chip=0)
   TpuDevice(id=1, process_index=0, coords=(1,0,0), core_on_chip=0)
   TpuDevice(id=3, process_index=0, coords=(1,1,0), core_on_chip=0)]
  [TpuDevice(id=8, process_index=2, coords=(0,0,1), core_on_chip=0)
   TpuDevice(id=10, process_index=2, coords=(0,1,1), core_on_chip=0)
   TpuDevice(id=9, process_index=2, coords=(1,0,1), core_on_chip=0)
   TpuDevice(id=11, process_index=2, coords=(1,1,1), core_on_chip=0)]
  [TpuDevice(id=16, process_index=4, coords=(0,0,2), core_on_chip=0)
   TpuDevice(id=18, process_index=4, coords=(0,1,2), core_on_chip=0)
   TpuDevice(id=17, process_index=4, coords=(1,0,2), core_on_chip=0)
   TpuDevice(id=19, process_index=4, coords=(1,1,2), core_on_chip=0)]
  [TpuDevice(id=24, process_index=6, coords=(0,0,3), core_on_chip=0)
   TpuDevice(id=26, process_index=6, coords=(0,1,3), core_on_chip=0)
   TpuDevice(id=25, process_index=6, coords=(1,0,3), core_on_chip=0)
   TpuDevice(id=27, process_index=6, coords=(1,1,3), core_on_chip=0)]
  [TpuDevice(id=4, process_index=1, coords=(0,2,0), core_on_chip=0)
   TpuDevice(id=6, process_index=1, coords=(0,3,0), core_on_chip=0)
   TpuDevice(id=5, process_index=1, coords=(1,2,0), core_on_chip=0)
   TpuDevice(id=7, process_index=1, coords=(1,3,0), core_on_chip=0)]
  [TpuDevice(id=12, process_index=3, coords=(0,2,1), core_on_chip=0)
   TpuDevice(id=14, process_index=3, coords=(0,3,1), core_on_chip=0)
   TpuDevice(id=13, process_index=3, coords=(1,2,1), core_on_chip=0)
   TpuDevice(id=15, process_index=3, coords=(1,3,1), core_on_chip=0)]
  [TpuDevice(id=20, process_index=5, coords=(0,2,2), core_on_chip=0)
   TpuDevice(id=22, process_index=5, coords=(0,3,2), core_on_chip=0)
   TpuDevice(id=21, process_index=5, coords=(1,2,2), core_on_chip=0)
   TpuDevice(id=23, process_index=5, coords=(1,3,2), core_on_chip=0)]
  [TpuDevice(id=28, process_index=7, coords=(0,2,3), core_on_chip=0)
   TpuDevice(id=30, process_index=7, coords=(0,3,3), core_on_chip=0)
   TpuDevice(id=29, process_index=7, coords=(1,2,3), core_on_chip=0)
   TpuDevice(id=31, process_index=7, coords=(1,3,3), core_on_chip=0)]]]
I0512 01:44:55.290596 140188559153152 partitioning.py:115] Overridden MoE global_mesh shape: (1, 8, 4)
I0512 01:44:56.242931 140392160929792 utils.py:1979] Initializing dataset for task 'mix_full_lm_test' with a replica batch size of 48 and a seed of 42
I0512 01:44:56.243013 140172769531904 utils.py:1979] Initializing dataset for task 'mix_full_lm_test' with a replica batch size of 48 and a seed of 42
I0512 01:44:56.242902 139997397456896 utils.py:1979] Initializing dataset for task 'mix_full_lm_test' with a replica batch size of 48 and a seed of 42
I0512 01:44:56.243283 140577131780096 utils.py:1979] Initializing dataset for task 'mix_full_lm_test' with a replica batch size of 48 and a seed of 42
I0512 01:44:56.243092 140188559153152 utils.py:1979] Initializing dataset for task 'mix_full_lm_test' with a replica batch size of 48 and a seed of 42
I0512 01:44:56.243202 140316961150976 utils.py:1979] Initializing dataset for task 'mix_full_lm_test' with a replica batch size of 48 and a seed of 42
I0512 01:44:56.243053 139812357228544 utils.py:1979] Initializing dataset for task 'mix_full_lm_test' with a replica batch size of 48 and a seed of 42
I0512 01:44:56.242934 139676058949632 utils.py:1979] Initializing dataset for task 'mix_full_lm_test' with a replica batch size of 48 and a seed of 42
I0512 01:44:56.584680 140392160929792 dataset_info.py:540] Load dataset info from gs://rosinality-tpu-bucket/redpajama_arxiv/1.0.0
I0512 01:44:56.584439 140188559153152 dataset_info.py:540] Load dataset info from gs://rosinality-tpu-bucket/redpajama_arxiv/1.0.0
I0512 01:44:56.604377 140172769531904 dataset_info.py:540] Load dataset info from gs://rosinality-tpu-bucket/redpajama_arxiv/1.0.0
I0512 01:44:56.607599 140577131780096 dataset_info.py:540] Load dataset info from gs://rosinality-tpu-bucket/redpajama_arxiv/1.0.0
I0512 01:44:56.604519 140316961150976 dataset_info.py:540] Load dataset info from gs://rosinality-tpu-bucket/redpajama_arxiv/1.0.0
I0512 01:44:56.608417 139997397456896 dataset_info.py:540] Load dataset info from gs://rosinality-tpu-bucket/redpajama_arxiv/1.0.0
I0512 01:44:56.588039 139676058949632 dataset_info.py:540] Load dataset info from gs://rosinality-tpu-bucket/redpajama_arxiv/1.0.0
I0512 01:44:56.605549 139812357228544 dataset_info.py:540] Load dataset info from gs://rosinality-tpu-bucket/redpajama_arxiv/1.0.0
I0512 01:44:56.827846 140316961150976 dataset_providers.py:1597] Sharding at the data source: 1 of 8
I0512 01:44:56.840030 140188559153152 dataset_providers.py:1597] Sharding at the data source: 6 of 8
I0512 01:44:56.852883 139997397456896 dataset_providers.py:1597] Sharding at the data source: 5 of 8
I0512 01:44:56.859814 140392160929792 dataset_providers.py:1597] Sharding at the data source: 4 of 8
I0512 01:44:56.853986 139812357228544 dataset_providers.py:1597] Sharding at the data source: 7 of 8
I0512 01:44:56.882412 140172769531904 dataset_providers.py:1597] Sharding at the data source: 8 of 8
I0512 01:44:56.870045 139676058949632 dataset_providers.py:1597] Sharding at the data source: 3 of 8
I0512 01:44:56.900813 140577131780096 dataset_providers.py:1597] Sharding at the data source: 2 of 8
I0512 01:44:56.901670 140316961150976 dataset_info.py:540] Load dataset info from gs://rosinality-tpu-bucket/redpajama_arxiv/1.0.0
I0512 01:44:56.906091 140188559153152 dataset_info.py:540] Load dataset info from gs://rosinality-tpu-bucket/redpajama_arxiv/1.0.0
I0512 01:44:56.931934 139997397456896 dataset_info.py:540] Load dataset info from gs://rosinality-tpu-bucket/redpajama_arxiv/1.0.0
I0512 01:44:56.919472 139812357228544 dataset_info.py:540] Load dataset info from gs://rosinality-tpu-bucket/redpajama_arxiv/1.0.0
I0512 01:44:56.943948 140172769531904 dataset_info.py:540] Load dataset info from gs://rosinality-tpu-bucket/redpajama_arxiv/1.0.0
I0512 01:44:56.945156 140392160929792 dataset_info.py:540] Load dataset info from gs://rosinality-tpu-bucket/redpajama_arxiv/1.0.0
I0512 01:44:56.930626 139676058949632 dataset_info.py:540] Load dataset info from gs://rosinality-tpu-bucket/redpajama_arxiv/1.0.0
I0512 01:44:56.974286 140577131780096 dataset_info.py:540] Load dataset info from gs://rosinality-tpu-bucket/redpajama_arxiv/1.0.0
I0512 01:44:57.144121 140188559153152 dataset_builder.py:421] Reusing dataset redpajama_arxiv (gs://rosinality-tpu-bucket/redpajama_arxiv/1.0.0)
I0512 01:44:57.162302 140392160929792 dataset_builder.py:421] Reusing dataset redpajama_arxiv (gs://rosinality-tpu-bucket/redpajama_arxiv/1.0.0)
I0512 01:44:57.157651 140316961150976 dataset_builder.py:421] Reusing dataset redpajama_arxiv (gs://rosinality-tpu-bucket/redpajama_arxiv/1.0.0)
I0512 01:44:57.162046 139997397456896 dataset_builder.py:421] Reusing dataset redpajama_arxiv (gs://rosinality-tpu-bucket/redpajama_arxiv/1.0.0)
I0512 01:44:57.192020 140172769531904 dataset_builder.py:421] Reusing dataset redpajama_arxiv (gs://rosinality-tpu-bucket/redpajama_arxiv/1.0.0)
I0512 01:44:57.175013 139812357228544 dataset_builder.py:421] Reusing dataset redpajama_arxiv (gs://rosinality-tpu-bucket/redpajama_arxiv/1.0.0)
I0512 01:44:57.181334 139676058949632 dataset_builder.py:421] Reusing dataset redpajama_arxiv (gs://rosinality-tpu-bucket/redpajama_arxiv/1.0.0)
I0512 01:44:57.202902 140577131780096 dataset_builder.py:421] Reusing dataset redpajama_arxiv (gs://rosinality-tpu-bucket/redpajama_arxiv/1.0.0)
I0512 01:44:57.311049 140188559153152 logging_logger.py:49] Constructing tf.data.Dataset redpajama_arxiv for split train, from gs://rosinality-tpu-bucket/redpajama_arxiv/1.0.0
I0512 01:44:57.318936 140392160929792 logging_logger.py:49] Constructing tf.data.Dataset redpajama_arxiv for split train, from gs://rosinality-tpu-bucket/redpajama_arxiv/1.0.0
I0512 01:44:57.320528 140316961150976 logging_logger.py:49] Constructing tf.data.Dataset redpajama_arxiv for split train, from gs://rosinality-tpu-bucket/redpajama_arxiv/1.0.0
I0512 01:44:57.330426 139997397456896 logging_logger.py:49] Constructing tf.data.Dataset redpajama_arxiv for split train, from gs://rosinality-tpu-bucket/redpajama_arxiv/1.0.0
I0512 01:44:57.361506 140172769531904 logging_logger.py:49] Constructing tf.data.Dataset redpajama_arxiv for split train, from gs://rosinality-tpu-bucket/redpajama_arxiv/1.0.0
I0512 01:44:57.339321 139812357228544 logging_logger.py:49] Constructing tf.data.Dataset redpajama_arxiv for split train, from gs://rosinality-tpu-bucket/redpajama_arxiv/1.0.0
I0512 01:44:57.342407 139676058949632 logging_logger.py:49] Constructing tf.data.Dataset redpajama_arxiv for split train, from gs://rosinality-tpu-bucket/redpajama_arxiv/1.0.0
I0512 01:44:57.365546 140577131780096 logging_logger.py:49] Constructing tf.data.Dataset redpajama_arxiv for split train, from gs://rosinality-tpu-bucket/redpajama_arxiv/1.0.0
WARNING:tensorflow:From /home/rosinality/openmoe_venv/lib/python3.10/site-packages/tensorflow/python/util/nest_util.py:1105: _RandomDataset.__init__ (from tensorflow.python.data.experimental.ops.random_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.random(...)`.
W0512 01:44:58.278461 140392160929792 deprecation.py:50] From /home/rosinality/openmoe_venv/lib/python3.10/site-packages/tensorflow/python/util/nest_util.py:1105: _RandomDataset.__init__ (from tensorflow.python.data.experimental.ops.random_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.random(...)`.
WARNING:tensorflow:From /home/rosinality/openmoe_venv/lib/python3.10/site-packages/tensorflow/python/util/nest_util.py:1105: _RandomDataset.__init__ (from tensorflow.python.data.experimental.ops.random_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.random(...)`.
W0512 01:44:58.297824 140188559153152 deprecation.py:50] From /home/rosinality/openmoe_venv/lib/python3.10/site-packages/tensorflow/python/util/nest_util.py:1105: _RandomDataset.__init__ (from tensorflow.python.data.experimental.ops.random_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.random(...)`.
WARNING:tensorflow:From /home/rosinality/openmoe_venv/lib/python3.10/site-packages/tensorflow/python/util/nest_util.py:1105: _RandomDataset.__init__ (from tensorflow.python.data.experimental.ops.random_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.random(...)`.
W0512 01:44:58.311688 139997397456896 deprecation.py:50] From /home/rosinality/openmoe_venv/lib/python3.10/site-packages/tensorflow/python/util/nest_util.py:1105: _RandomDataset.__init__ (from tensorflow.python.data.experimental.ops.random_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.random(...)`.
WARNING:tensorflow:From /home/rosinality/openmoe_venv/lib/python3.10/site-packages/tensorflow/python/util/nest_util.py:1105: _RandomDataset.__init__ (from tensorflow.python.data.experimental.ops.random_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.random(...)`.
W0512 01:44:58.314694 140316961150976 deprecation.py:50] From /home/rosinality/openmoe_venv/lib/python3.10/site-packages/tensorflow/python/util/nest_util.py:1105: _RandomDataset.__init__ (from tensorflow.python.data.experimental.ops.random_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.random(...)`.
WARNING:tensorflow:From /home/rosinality/openmoe_venv/lib/python3.10/site-packages/tensorflow/python/util/nest_util.py:1105: _RandomDataset.__init__ (from tensorflow.python.data.experimental.ops.random_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.random(...)`.
W0512 01:44:58.303627 139676058949632 deprecation.py:50] From /home/rosinality/openmoe_venv/lib/python3.10/site-packages/tensorflow/python/util/nest_util.py:1105: _RandomDataset.__init__ (from tensorflow.python.data.experimental.ops.random_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.random(...)`.
WARNING:tensorflow:From /home/rosinality/openmoe_venv/lib/python3.10/site-packages/tensorflow/python/util/nest_util.py:1105: _RandomDataset.__init__ (from tensorflow.python.data.experimental.ops.random_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.random(...)`.
W0512 01:44:58.335016 140577131780096 deprecation.py:50] From /home/rosinality/openmoe_venv/lib/python3.10/site-packages/tensorflow/python/util/nest_util.py:1105: _RandomDataset.__init__ (from tensorflow.python.data.experimental.ops.random_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.random(...)`.
WARNING:tensorflow:From /home/rosinality/openmoe_venv/lib/python3.10/site-packages/tensorflow/python/util/nest_util.py:1105: _RandomDataset.__init__ (from tensorflow.python.data.experimental.ops.random_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.random(...)`.
W0512 01:44:58.342764 140172769531904 deprecation.py:50] From /home/rosinality/openmoe_venv/lib/python3.10/site-packages/tensorflow/python/util/nest_util.py:1105: _RandomDataset.__init__ (from tensorflow.python.data.experimental.ops.random_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.random(...)`.
WARNING:tensorflow:From /home/rosinality/openmoe_venv/lib/python3.10/site-packages/tensorflow/python/util/nest_util.py:1105: _RandomDataset.__init__ (from tensorflow.python.data.experimental.ops.random_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.random(...)`.
W0512 01:44:58.321294 139812357228544 deprecation.py:50] From /home/rosinality/openmoe_venv/lib/python3.10/site-packages/tensorflow/python/util/nest_util.py:1105: _RandomDataset.__init__ (from tensorflow.python.data.experimental.ops.random_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.random(...)`.
I0512 01:45:00.406443 140392160929792 dataset_info.py:540] Load dataset info from gs://rosinality-tpu-bucket/redpajama_c4/1.0.0
I0512 01:45:00.456738 139997397456896 dataset_info.py:540] Load dataset info from gs://rosinality-tpu-bucket/redpajama_c4/1.0.0
I0512 01:45:00.511784 140172769531904 dataset_info.py:540] Load dataset info from gs://rosinality-tpu-bucket/redpajama_c4/1.0.0
I0512 01:45:00.509703 140188559153152 dataset_info.py:540] Load dataset info from gs://rosinality-tpu-bucket/redpajama_c4/1.0.0
I0512 01:45:00.511184 140316961150976 dataset_info.py:540] Load dataset info from gs://rosinality-tpu-bucket/redpajama_c4/1.0.0
I0512 01:45:00.516760 140577131780096 dataset_info.py:540] Load dataset info from gs://rosinality-tpu-bucket/redpajama_c4/1.0.0
I0512 01:45:00.501977 139676058949632 dataset_info.py:540] Load dataset info from gs://rosinality-tpu-bucket/redpajama_c4/1.0.0
I0512 01:45:00.535234 139812357228544 dataset_info.py:540] Load dataset info from gs://rosinality-tpu-bucket/redpajama_c4/1.0.0
I0512 01:45:00.658562 139997397456896 dataset_providers.py:1597] Sharding at the data source: 5 of 8
I0512 01:45:00.688055 140392160929792 dataset_providers.py:1597] Sharding at the data source: 4 of 8
I0512 01:45:00.722273 139997397456896 dataset_info.py:540] Load dataset info from gs://rosinality-tpu-bucket/redpajama_c4/1.0.0
I0512 01:45:00.737366 140172769531904 dataset_providers.py:1597] Sharding at the data source: 8 of 8
I0512 01:45:00.738512 140577131780096 dataset_providers.py:1597] Sharding at the data source: 2 of 8
I0512 01:45:00.744266 140392160929792 dataset_info.py:540] Load dataset info from gs://rosinality-tpu-bucket/redpajama_c4/1.0.0
I0512 01:45:00.736621 139676058949632 dataset_providers.py:1597] Sharding at the data source: 3 of 8
I0512 01:45:00.781158 140188559153152 dataset_providers.py:1597] Sharding at the data source: 6 of 8
I0512 01:45:00.761936 139812357228544 dataset_providers.py:1597] Sharding at the data source: 7 of 8
I0512 01:45:00.797075 140172769531904 dataset_info.py:540] Load dataset info from gs://rosinality-tpu-bucket/redpajama_c4/1.0.0
I0512 01:45:00.799854 140577131780096 dataset_info.py:540] Load dataset info from gs://rosinality-tpu-bucket/redpajama_c4/1.0.0
I0512 01:45:00.800121 139676058949632 dataset_info.py:540] Load dataset info from gs://rosinality-tpu-bucket/redpajama_c4/1.0.0
I0512 01:45:00.836499 140316961150976 dataset_providers.py:1597] Sharding at the data source: 1 of 8
I0512 01:45:00.845259 140188559153152 dataset_info.py:540] Load dataset info from gs://rosinality-tpu-bucket/redpajama_c4/1.0.0
I0512 01:45:00.830490 139812357228544 dataset_info.py:540] Load dataset info from gs://rosinality-tpu-bucket/redpajama_c4/1.0.0
I0512 01:45:00.895830 140316961150976 dataset_info.py:540] Load dataset info from gs://rosinality-tpu-bucket/redpajama_c4/1.0.0
I0512 01:45:00.966552 139997397456896 dataset_builder.py:421] Reusing dataset redpajama_c4 (gs://rosinality-tpu-bucket/redpajama_c4/1.0.0)
I0512 01:45:00.973040 140392160929792 dataset_builder.py:421] Reusing dataset redpajama_c4 (gs://rosinality-tpu-bucket/redpajama_c4/1.0.0)
I0512 01:45:01.032983 140172769531904 dataset_builder.py:421] Reusing dataset redpajama_c4 (gs://rosinality-tpu-bucket/redpajama_c4/1.0.0)
I0512 01:45:01.045520 139997397456896 logging_logger.py:49] Constructing tf.data.Dataset redpajama_c4 for split train[:-2000], from gs://rosinality-tpu-bucket/redpajama_c4/1.0.0
I0512 01:45:01.048264 140392160929792 logging_logger.py:49] Constructing tf.data.Dataset redpajama_c4 for split train[:-2000], from gs://rosinality-tpu-bucket/redpajama_c4/1.0.0
I0512 01:45:01.053024 140577131780096 dataset_builder.py:421] Reusing dataset redpajama_c4 (gs://rosinality-tpu-bucket/redpajama_c4/1.0.0)
I0512 01:45:01.045015 139812357228544 dataset_builder.py:421] Reusing dataset redpajama_c4 (gs://rosinality-tpu-bucket/redpajama_c4/1.0.0)
I0512 01:45:01.049640 139676058949632 dataset_builder.py:421] Reusing dataset redpajama_c4 (gs://rosinality-tpu-bucket/redpajama_c4/1.0.0)
I0512 01:45:01.099333 140188559153152 dataset_builder.py:421] Reusing dataset redpajama_c4 (gs://rosinality-tpu-bucket/redpajama_c4/1.0.0)
I0512 01:45:01.101171 140316961150976 dataset_builder.py:421] Reusing dataset redpajama_c4 (gs://rosinality-tpu-bucket/redpajama_c4/1.0.0)
I0512 01:45:01.110604 140172769531904 logging_logger.py:49] Constructing tf.data.Dataset redpajama_c4 for split train[:-2000], from gs://rosinality-tpu-bucket/redpajama_c4/1.0.0
I0512 01:45:01.134143 140577131780096 logging_logger.py:49] Constructing tf.data.Dataset redpajama_c4 for split train[:-2000], from gs://rosinality-tpu-bucket/redpajama_c4/1.0.0
I0512 01:45:01.120071 139812357228544 logging_logger.py:49] Constructing tf.data.Dataset redpajama_c4 for split train[:-2000], from gs://rosinality-tpu-bucket/redpajama_c4/1.0.0
I0512 01:45:01.122855 139676058949632 logging_logger.py:49] Constructing tf.data.Dataset redpajama_c4 for split train[:-2000], from gs://rosinality-tpu-bucket/redpajama_c4/1.0.0
I0512 01:45:01.174464 140188559153152 logging_logger.py:49] Constructing tf.data.Dataset redpajama_c4 for split train[:-2000], from gs://rosinality-tpu-bucket/redpajama_c4/1.0.0
I0512 01:45:01.175591 140316961150976 logging_logger.py:49] Constructing tf.data.Dataset redpajama_c4 for split train[:-2000], from gs://rosinality-tpu-bucket/redpajama_c4/1.0.0
I0512 01:45:01.469170 140392160929792 dataset_info.py:540] Load dataset info from gs://rosinality-tpu-bucket/redpajama_common_crawl/1.0.0
I0512 01:45:01.526502 139997397456896 dataset_info.py:540] Load dataset info from gs://rosinality-tpu-bucket/redpajama_common_crawl/1.0.0
I0512 01:45:01.552312 140172769531904 dataset_info.py:540] Load dataset info from gs://rosinality-tpu-bucket/redpajama_common_crawl/1.0.0
I0512 01:45:01.557306 139812357228544 dataset_info.py:540] Load dataset info from gs://rosinality-tpu-bucket/redpajama_common_crawl/1.0.0
I0512 01:45:01.586275 140577131780096 dataset_info.py:540] Load dataset info from gs://rosinality-tpu-bucket/redpajama_common_crawl/1.0.0
I0512 01:45:01.604334 139676058949632 dataset_info.py:540] Load dataset info from gs://rosinality-tpu-bucket/redpajama_common_crawl/1.0.0
I0512 01:45:01.634696 140188559153152 dataset_info.py:540] Load dataset info from gs://rosinality-tpu-bucket/redpajama_common_crawl/1.0.0
I0512 01:45:01.675066 140316961150976 dataset_info.py:540] Load dataset info from gs://rosinality-tpu-bucket/redpajama_common_crawl/1.0.0
I0512 01:45:01.777375 140392160929792 dataset_providers.py:1597] Sharding at the data source: 4 of 8
I0512 01:45:01.804057 139997397456896 dataset_providers.py:1597] Sharding at the data source: 5 of 8
I0512 01:45:01.838633 140392160929792 dataset_info.py:540] Load dataset info from gs://rosinality-tpu-bucket/redpajama_common_crawl/1.0.0
I0512 01:45:01.822745 139812357228544 dataset_providers.py:1597] Sharding at the data source: 7 of 8
I0512 01:45:01.850876 140172769531904 dataset_providers.py:1597] Sharding at the data source: 8 of 8
I0512 01:45:01.907527 140577131780096 dataset_providers.py:1597] Sharding at the data source: 2 of 8
I0512 01:45:01.913625 140172769531904 dataset_info.py:540] Load dataset info from gs://rosinality-tpu-bucket/redpajama_common_crawl/1.0.0
I0512 01:45:01.893013 139812357228544 dataset_info.py:540] Load dataset info from gs://rosinality-tpu-bucket/redpajama_common_crawl/1.0.0
I0512 01:45:01.915203 139997397456896 dataset_info.py:540] Load dataset info from gs://rosinality-tpu-bucket/redpajama_common_crawl/1.0.0
I0512 01:45:01.919142 140188559153152 dataset_providers.py:1597] Sharding at the data source: 6 of 8
I0512 01:45:01.902486 139676058949632 dataset_providers.py:1597] Sharding at the data source: 3 of 8
I0512 01:45:01.973673 140577131780096 dataset_info.py:540] Load dataset info from gs://rosinality-tpu-bucket/redpajama_common_crawl/1.0.0
I0512 01:45:01.984138 140316961150976 dataset_providers.py:1597] Sharding at the data source: 1 of 8
I0512 01:45:01.988871 140188559153152 dataset_info.py:540] Load dataset info from gs://rosinality-tpu-bucket/redpajama_common_crawl/1.0.0
I0512 01:45:01.976584 139676058949632 dataset_info.py:540] Load dataset info from gs://rosinality-tpu-bucket/redpajama_common_crawl/1.0.0
I0512 01:45:02.055021 140316961150976 dataset_info.py:540] Load dataset info from gs://rosinality-tpu-bucket/redpajama_common_crawl/1.0.0
I0512 01:45:02.146134 140392160929792 dataset_builder.py:421] Reusing dataset redpajama_common_crawl (gs://rosinality-tpu-bucket/redpajama_common_crawl/1.0.0)
I0512 01:45:02.165093 139812357228544 dataset_builder.py:421] Reusing dataset redpajama_common_crawl (gs://rosinality-tpu-bucket/redpajama_common_crawl/1.0.0)
I0512 01:45:02.196941 140172769531904 dataset_builder.py:421] Reusing dataset redpajama_common_crawl (gs://rosinality-tpu-bucket/redpajama_common_crawl/1.0.0)
I0512 01:45:02.199548 139997397456896 dataset_builder.py:421] Reusing dataset redpajama_common_crawl (gs://rosinality-tpu-bucket/redpajama_common_crawl/1.0.0)
I0512 01:45:02.240698 140392160929792 logging_logger.py:49] Constructing tf.data.Dataset redpajama_common_crawl for split train, from gs://rosinality-tpu-bucket/redpajama_common_crawl/1.0.0
I0512 01:45:02.255476 140188559153152 dataset_builder.py:421] Reusing dataset redpajama_common_crawl (gs://rosinality-tpu-bucket/redpajama_common_crawl/1.0.0)
I0512 01:45:02.267513 140577131780096 dataset_builder.py:421] Reusing dataset redpajama_common_crawl (gs://rosinality-tpu-bucket/redpajama_common_crawl/1.0.0)
I0512 01:45:02.257217 139812357228544 logging_logger.py:49] Constructing tf.data.Dataset redpajama_common_crawl for split train, from gs://rosinality-tpu-bucket/redpajama_common_crawl/1.0.0
I0512 01:45:02.292821 139997397456896 logging_logger.py:49] Constructing tf.data.Dataset redpajama_common_crawl for split train, from gs://rosinality-tpu-bucket/redpajama_common_crawl/1.0.0
I0512 01:45:02.296060 140172769531904 logging_logger.py:49] Constructing tf.data.Dataset redpajama_common_crawl for split train, from gs://rosinality-tpu-bucket/redpajama_common_crawl/1.0.0
I0512 01:45:02.277537 139676058949632 dataset_builder.py:421] Reusing dataset redpajama_common_crawl (gs://rosinality-tpu-bucket/redpajama_common_crawl/1.0.0)
I0512 01:45:02.332332 140316961150976 dataset_builder.py:421] Reusing dataset redpajama_common_crawl (gs://rosinality-tpu-bucket/redpajama_common_crawl/1.0.0)
I0512 01:45:02.358716 140188559153152 logging_logger.py:49] Constructing tf.data.Dataset redpajama_common_crawl for split train, from gs://rosinality-tpu-bucket/redpajama_common_crawl/1.0.0
I0512 01:45:02.369976 140577131780096 logging_logger.py:49] Constructing tf.data.Dataset redpajama_common_crawl for split train, from gs://rosinality-tpu-bucket/redpajama_common_crawl/1.0.0
I0512 01:45:02.375052 139676058949632 logging_logger.py:49] Constructing tf.data.Dataset redpajama_common_crawl for split train, from gs://rosinality-tpu-bucket/redpajama_common_crawl/1.0.0
I0512 01:45:02.426088 140316961150976 logging_logger.py:49] Constructing tf.data.Dataset redpajama_common_crawl for split train, from gs://rosinality-tpu-bucket/redpajama_common_crawl/1.0.0
I0512 01:45:02.655875 140392160929792 dataset_info.py:540] Load dataset info from gs://rosinality-tpu-bucket/redpajama_github/1.0.0
I0512 01:45:02.656445 139812357228544 dataset_info.py:540] Load dataset info from gs://rosinality-tpu-bucket/redpajama_github/1.0.0
I0512 01:45:02.687278 139997397456896 dataset_info.py:540] Load dataset info from gs://rosinality-tpu-bucket/redpajama_github/1.0.0
I0512 01:45:02.742116 140172769531904 dataset_info.py:540] Load dataset info from gs://rosinality-tpu-bucket/redpajama_github/1.0.0
I0512 01:45:02.758810 140188559153152 dataset_info.py:540] Load dataset info from gs://rosinality-tpu-bucket/redpajama_github/1.0.0
I0512 01:45:02.771364 139676058949632 dataset_info.py:540] Load dataset info from gs://rosinality-tpu-bucket/redpajama_github/1.0.0
I0512 01:45:02.798856 140577131780096 dataset_info.py:540] Load dataset info from gs://rosinality-tpu-bucket/redpajama_github/1.0.0
I0512 01:45:02.811952 140316961150976 dataset_info.py:540] Load dataset info from gs://rosinality-tpu-bucket/redpajama_github/1.0.0
I0512 01:45:02.877159 140392160929792 dataset_providers.py:1597] Sharding at the data source: 4 of 8
I0512 01:45:02.913383 139812357228544 dataset_providers.py:1597] Sharding at the data source: 7 of 8
I0512 01:45:02.955683 140392160929792 dataset_info.py:540] Load dataset info from gs://rosinality-tpu-bucket/redpajama_github/1.0.0
I0512 01:45:02.963101 140172769531904 dataset_providers.py:1597] Sharding at the data source: 8 of 8
I0512 01:45:02.987321 139997397456896 dataset_providers.py:1597] Sharding at the data source: 5 of 8
I0512 01:45:02.989286 140188559153152 dataset_providers.py:1597] Sharding at the data source: 6 of 8
I0512 01:45:02.976694 139812357228544 dataset_info.py:540] Load dataset info from gs://rosinality-tpu-bucket/redpajama_github/1.0.0
I0512 01:45:03.011789 140577131780096 dataset_providers.py:1597] Sharding at the data source: 2 of 8
I0512 01:45:02.998130 139676058949632 dataset_providers.py:1597] Sharding at the data source: 3 of 8
I0512 01:45:03.034866 140172769531904 dataset_info.py:540] Load dataset info from gs://rosinality-tpu-bucket/redpajama_github/1.0.0
I0512 01:45:03.033677 140316961150976 dataset_providers.py:1597] Sharding at the data source: 1 of 8
I0512 01:45:03.050802 139997397456896 dataset_info.py:540] Load dataset info from gs://rosinality-tpu-bucket/redpajama_github/1.0.0
I0512 01:45:03.078364 140577131780096 dataset_info.py:540] Load dataset info from gs://rosinality-tpu-bucket/redpajama_github/1.0.0
I0512 01:45:03.065382 139676058949632 dataset_info.py:540] Load dataset info from gs://rosinality-tpu-bucket/redpajama_github/1.0.0
I0512 01:45:03.089375 140188559153152 dataset_info.py:540] Load dataset info from gs://rosinality-tpu-bucket/redpajama_github/1.0.0
I0512 01:45:03.097221 140316961150976 dataset_info.py:540] Load dataset info from gs://rosinality-tpu-bucket/redpajama_github/1.0.0
I0512 01:45:03.215001 139812357228544 dataset_builder.py:421] Reusing dataset redpajama_github (gs://rosinality-tpu-bucket/redpajama_github/1.0.0)
I0512 01:45:03.267093 140392160929792 dataset_builder.py:421] Reusing dataset redpajama_github (gs://rosinality-tpu-bucket/redpajama_github/1.0.0)
I0512 01:45:03.270549 140172769531904 dataset_builder.py:421] Reusing dataset redpajama_github (gs://rosinality-tpu-bucket/redpajama_github/1.0.0)
I0512 01:45:03.284720 139997397456896 dataset_builder.py:421] Reusing dataset redpajama_github (gs://rosinality-tpu-bucket/redpajama_github/1.0.0)
I0512 01:45:03.300198 140188559153152 dataset_builder.py:421] Reusing dataset redpajama_github (gs://rosinality-tpu-bucket/redpajama_github/1.0.0)
I0512 01:45:03.304132 140577131780096 dataset_builder.py:421] Reusing dataset redpajama_github (gs://rosinality-tpu-bucket/redpajama_github/1.0.0)
I0512 01:45:03.308719 139812357228544 logging_logger.py:49] Constructing tf.data.Dataset redpajama_github for split train, from gs://rosinality-tpu-bucket/redpajama_github/1.0.0
I0512 01:45:03.326590 140316961150976 dataset_builder.py:421] Reusing dataset redpajama_github (gs://rosinality-tpu-bucket/redpajama_github/1.0.0)
I0512 01:45:03.337442 140392160929792 logging_logger.py:49] Constructing tf.data.Dataset redpajama_github for split train, from gs://rosinality-tpu-bucket/redpajama_github/1.0.0
I0512 01:45:03.318224 139676058949632 dataset_builder.py:421] Reusing dataset redpajama_github (gs://rosinality-tpu-bucket/redpajama_github/1.0.0)
I0512 01:45:03.348663 140172769531904 logging_logger.py:49] Constructing tf.data.Dataset redpajama_github for split train, from gs://rosinality-tpu-bucket/redpajama_github/1.0.0
I0512 01:45:03.359808 139997397456896 logging_logger.py:49] Constructing tf.data.Dataset redpajama_github for split train, from gs://rosinality-tpu-bucket/redpajama_github/1.0.0
I0512 01:45:03.369747 140188559153152 logging_logger.py:49] Constructing tf.data.Dataset redpajama_github for split train, from gs://rosinality-tpu-bucket/redpajama_github/1.0.0
I0512 01:45:03.383241 140577131780096 logging_logger.py:49] Constructing tf.data.Dataset redpajama_github for split train, from gs://rosinality-tpu-bucket/redpajama_github/1.0.0
I0512 01:45:03.393475 139676058949632 logging_logger.py:49] Constructing tf.data.Dataset redpajama_github for split train, from gs://rosinality-tpu-bucket/redpajama_github/1.0.0
I0512 01:45:03.415321 140316961150976 logging_logger.py:49] Constructing tf.data.Dataset redpajama_github for split train, from gs://rosinality-tpu-bucket/redpajama_github/1.0.0
I0512 01:45:03.736940 140392160929792 dataset_info.py:540] Load dataset info from gs://rosinality-tpu-bucket/redpajama_stackexchange/1.0.0
I0512 01:45:03.725066 139812357228544 dataset_info.py:540] Load dataset info from gs://rosinality-tpu-bucket/redpajama_stackexchange/1.0.0
I0512 01:45:03.759444 139997397456896 dataset_info.py:540] Load dataset info from gs://rosinality-tpu-bucket/redpajama_stackexchange/1.0.0
I0512 01:45:03.762921 140172769531904 dataset_info.py:540] Load dataset info from gs://rosinality-tpu-bucket/redpajama_stackexchange/1.0.0
I0512 01:45:03.781873 140188559153152 dataset_info.py:540] Load dataset info from gs://rosinality-tpu-bucket/redpajama_stackexchange/1.0.0
I0512 01:45:03.813662 140316961150976 dataset_info.py:540] Load dataset info from gs://rosinality-tpu-bucket/redpajama_stackexchange/1.0.0
I0512 01:45:03.798044 139676058949632 dataset_info.py:540] Load dataset info from gs://rosinality-tpu-bucket/redpajama_stackexchange/1.0.0
I0512 01:45:03.819821 140577131780096 dataset_info.py:540] Load dataset info from gs://rosinality-tpu-bucket/redpajama_stackexchange/1.0.0
I0512 01:45:03.990923 140392160929792 dataset_providers.py:1597] Sharding at the data source: 4 of 8
I0512 01:45:03.972556 139812357228544 dataset_providers.py:1597] Sharding at the data source: 7 of 8
I0512 01:45:04.021686 140188559153152 dataset_providers.py:1597] Sharding at the data source: 6 of 8
I0512 01:45:04.025923 140316961150976 dataset_providers.py:1597] Sharding at the data source: 1 of 8
I0512 01:45:04.034762 139997397456896 dataset_providers.py:1597] Sharding at the data source: 5 of 8
I0512 01:45:04.018787 139676058949632 dataset_providers.py:1597] Sharding at the data source: 3 of 8
I0512 01:45:04.052514 140392160929792 dataset_info.py:540] Load dataset info from gs://rosinality-tpu-bucket/redpajama_stackexchange/1.0.0
I0512 01:45:04.034830 139812357228544 dataset_info.py:540] Load dataset info from gs://rosinality-tpu-bucket/redpajama_stackexchange/1.0.0
I0512 01:45:04.057517 140577131780096 dataset_providers.py:1597] Sharding at the data source: 2 of 8
I0512 01:45:04.079141 140188559153152 dataset_info.py:540] Load dataset info from gs://rosinality-tpu-bucket/redpajama_stackexchange/1.0.0
I0512 01:45:04.085653 140172769531904 dataset_providers.py:1597] Sharding at the data source: 8 of 8
I0512 01:45:04.081945 139676058949632 dataset_info.py:540] Load dataset info from gs://rosinality-tpu-bucket/redpajama_stackexchange/1.0.0
I0512 01:45:04.107048 139997397456896 dataset_info.py:540] Load dataset info from gs://rosinality-tpu-bucket/redpajama_stackexchange/1.0.0
I0512 01:45:04.121512 140577131780096 dataset_info.py:540] Load dataset info from gs://rosinality-tpu-bucket/redpajama_stackexchange/1.0.0
I0512 01:45:04.126355 140316961150976 dataset_info.py:540] Load dataset info from gs://rosinality-tpu-bucket/redpajama_stackexchange/1.0.0
I0512 01:45:04.151302 140172769531904 dataset_info.py:540] Load dataset info from gs://rosinality-tpu-bucket/redpajama_stackexchange/1.0.0
I0512 01:45:04.295808 140392160929792 dataset_builder.py:421] Reusing dataset redpajama_stackexchange (gs://rosinality-tpu-bucket/redpajama_stackexchange/1.0.0)
I0512 01:45:04.299756 140188559153152 dataset_builder.py:421] Reusing dataset redpajama_stackexchange (gs://rosinality-tpu-bucket/redpajama_stackexchange/1.0.0)
I0512 01:45:04.285358 139812357228544 dataset_builder.py:421] Reusing dataset redpajama_stackexchange (gs://rosinality-tpu-bucket/redpajama_stackexchange/1.0.0)
I0512 01:45:04.323842 139676058949632 dataset_builder.py:421] Reusing dataset redpajama_stackexchange (gs://rosinality-tpu-bucket/redpajama_stackexchange/1.0.0)
I0512 01:45:04.346528 140577131780096 dataset_builder.py:421] Reusing dataset redpajama_stackexchange (gs://rosinality-tpu-bucket/redpajama_stackexchange/1.0.0)
I0512 01:45:04.359886 139997397456896 dataset_builder.py:421] Reusing dataset redpajama_stackexchange (gs://rosinality-tpu-bucket/redpajama_stackexchange/1.0.0)
I0512 01:45:04.368800 140392160929792 logging_logger.py:49] Constructing tf.data.Dataset redpajama_stackexchange for split train, from gs://rosinality-tpu-bucket/redpajama_stackexchange/1.0.0
I0512 01:45:04.373009 140172769531904 dataset_builder.py:421] Reusing dataset redpajama_stackexchange (gs://rosinality-tpu-bucket/redpajama_stackexchange/1.0.0)
I0512 01:45:04.373132 140188559153152 logging_logger.py:49] Constructing tf.data.Dataset redpajama_stackexchange for split train, from gs://rosinality-tpu-bucket/redpajama_stackexchange/1.0.0
I0512 01:45:04.370856 140316961150976 dataset_builder.py:421] Reusing dataset redpajama_stackexchange (gs://rosinality-tpu-bucket/redpajama_stackexchange/1.0.0)
I0512 01:45:04.360649 139812357228544 logging_logger.py:49] Constructing tf.data.Dataset redpajama_stackexchange for split train, from gs://rosinality-tpu-bucket/redpajama_stackexchange/1.0.0
I0512 01:45:04.396106 139676058949632 logging_logger.py:49] Constructing tf.data.Dataset redpajama_stackexchange for split train, from gs://rosinality-tpu-bucket/redpajama_stackexchange/1.0.0
I0512 01:45:04.421413 140577131780096 logging_logger.py:49] Constructing tf.data.Dataset redpajama_stackexchange for split train, from gs://rosinality-tpu-bucket/redpajama_stackexchange/1.0.0
I0512 01:45:04.431861 139997397456896 logging_logger.py:49] Constructing tf.data.Dataset redpajama_stackexchange for split train, from gs://rosinality-tpu-bucket/redpajama_stackexchange/1.0.0
I0512 01:45:04.445534 140172769531904 logging_logger.py:49] Constructing tf.data.Dataset redpajama_stackexchange for split train, from gs://rosinality-tpu-bucket/redpajama_stackexchange/1.0.0
I0512 01:45:04.451308 140316961150976 logging_logger.py:49] Constructing tf.data.Dataset redpajama_stackexchange for split train, from gs://rosinality-tpu-bucket/redpajama_stackexchange/1.0.0
I0512 01:45:04.758671 140392160929792 dataset_info.py:540] Load dataset info from gs://rosinality-tpu-bucket/redpajama_wikipedia/1.0.0
I0512 01:45:04.770484 140188559153152 dataset_info.py:540] Load dataset info from gs://rosinality-tpu-bucket/redpajama_wikipedia/1.0.0
I0512 01:45:04.759166 139812357228544 dataset_info.py:540] Load dataset info from gs://rosinality-tpu-bucket/redpajama_wikipedia/1.0.0
I0512 01:45:04.826497 140577131780096 dataset_info.py:540] Load dataset info from gs://rosinality-tpu-bucket/redpajama_wikipedia/1.0.0
I0512 01:45:04.836336 139997397456896 dataset_info.py:540] Load dataset info from gs://rosinality-tpu-bucket/redpajama_wikipedia/1.0.0
I0512 01:45:04.852735 140172769531904 dataset_info.py:540] Load dataset info from gs://rosinality-tpu-bucket/redpajama_wikipedia/1.0.0
I0512 01:45:04.833201 139676058949632 dataset_info.py:540] Load dataset info from gs://rosinality-tpu-bucket/redpajama_wikipedia/1.0.0
I0512 01:45:04.895690 140316961150976 dataset_info.py:540] Load dataset info from gs://rosinality-tpu-bucket/redpajama_wikipedia/1.0.0
I0512 01:45:04.992417 140188559153152 dataset_providers.py:1597] Sharding at the data source: 6 of 8
I0512 01:45:04.986114 139812357228544 dataset_providers.py:1597] Sharding at the data source: 7 of 8
I0512 01:45:05.022794 140392160929792 dataset_providers.py:1597] Sharding at the data source: 4 of 8
I0512 01:45:05.043399 140577131780096 dataset_providers.py:1597] Sharding at the data source: 2 of 8
I0512 01:45:05.033192 139676058949632 dataset_providers.py:1597] Sharding at the data source: 3 of 8
I0512 01:45:05.057467 140188559153152 dataset_info.py:540] Load dataset info from gs://rosinality-tpu-bucket/redpajama_wikipedia/1.0.0
I0512 01:45:05.067595 140172769531904 dataset_providers.py:1597] Sharding at the data source: 8 of 8
I0512 01:45:05.048903 139812357228544 dataset_info.py:540] Load dataset info from gs://rosinality-tpu-bucket/redpajama_wikipedia/1.0.0
I0512 01:45:05.090804 140392160929792 dataset_info.py:540] Load dataset info from gs://rosinality-tpu-bucket/redpajama_wikipedia/1.0.0
I0512 01:45:05.116440 139997397456896 dataset_providers.py:1597] Sharding at the data source: 5 of 8
I0512 01:45:05.101367 139676058949632 dataset_info.py:540] Load dataset info from gs://rosinality-tpu-bucket/redpajama_wikipedia/1.0.0
I0512 01:45:05.131311 140172769531904 dataset_info.py:540] Load dataset info from gs://rosinality-tpu-bucket/redpajama_wikipedia/1.0.0
I0512 01:45:05.141714 140316961150976 dataset_providers.py:1597] Sharding at the data source: 1 of 8
I0512 01:45:05.145841 140577131780096 dataset_info.py:540] Load dataset info from gs://rosinality-tpu-bucket/redpajama_wikipedia/1.0.0
I0512 01:45:05.172892 139997397456896 dataset_info.py:540] Load dataset info from gs://rosinality-tpu-bucket/redpajama_wikipedia/1.0.0
I0512 01:45:05.207476 140316961150976 dataset_info.py:540] Load dataset info from gs://rosinality-tpu-bucket/redpajama_wikipedia/1.0.0
I0512 01:45:05.303396 140188559153152 dataset_builder.py:421] Reusing dataset redpajama_wikipedia (gs://rosinality-tpu-bucket/redpajama_wikipedia/1.0.0)
I0512 01:45:05.297884 139812357228544 dataset_builder.py:421] Reusing dataset redpajama_wikipedia (gs://rosinality-tpu-bucket/redpajama_wikipedia/1.0.0)
I0512 01:45:05.322443 140392160929792 dataset_builder.py:421] Reusing dataset redpajama_wikipedia (gs://rosinality-tpu-bucket/redpajama_wikipedia/1.0.0)
I0512 01:45:05.362845 140577131780096 dataset_builder.py:421] Reusing dataset redpajama_wikipedia (gs://rosinality-tpu-bucket/redpajama_wikipedia/1.0.0)
I0512 01:45:05.378373 140172769531904 dataset_builder.py:421] Reusing dataset redpajama_wikipedia (gs://rosinality-tpu-bucket/redpajama_wikipedia/1.0.0)
I0512 01:45:05.378515 140188559153152 logging_logger.py:49] Constructing tf.data.Dataset redpajama_wikipedia for split train, from gs://rosinality-tpu-bucket/redpajama_wikipedia/1.0.0
I0512 01:45:05.362869 139676058949632 dataset_builder.py:421] Reusing dataset redpajama_wikipedia (gs://rosinality-tpu-bucket/redpajama_wikipedia/1.0.0)
I0512 01:45:05.389517 139997397456896 dataset_builder.py:421] Reusing dataset redpajama_wikipedia (gs://rosinality-tpu-bucket/redpajama_wikipedia/1.0.0)
I0512 01:45:05.371141 139812357228544 logging_logger.py:49] Constructing tf.data.Dataset redpajama_wikipedia for split train, from gs://rosinality-tpu-bucket/redpajama_wikipedia/1.0.0
I0512 01:45:05.397343 140392160929792 logging_logger.py:49] Constructing tf.data.Dataset redpajama_wikipedia for split train, from gs://rosinality-tpu-bucket/redpajama_wikipedia/1.0.0
I0512 01:45:05.437702 140577131780096 logging_logger.py:49] Constructing tf.data.Dataset redpajama_wikipedia for split train, from gs://rosinality-tpu-bucket/redpajama_wikipedia/1.0.0
I0512 01:45:05.444937 140316961150976 dataset_builder.py:421] Reusing dataset redpajama_wikipedia (gs://rosinality-tpu-bucket/redpajama_wikipedia/1.0.0)
I0512 01:45:05.452803 140172769531904 logging_logger.py:49] Constructing tf.data.Dataset redpajama_wikipedia for split train, from gs://rosinality-tpu-bucket/redpajama_wikipedia/1.0.0
I0512 01:45:05.434570 139676058949632 logging_logger.py:49] Constructing tf.data.Dataset redpajama_wikipedia for split train, from gs://rosinality-tpu-bucket/redpajama_wikipedia/1.0.0
I0512 01:45:05.465313 139997397456896 logging_logger.py:49] Constructing tf.data.Dataset redpajama_wikipedia for split train, from gs://rosinality-tpu-bucket/redpajama_wikipedia/1.0.0
I0512 01:45:05.517296 140316961150976 logging_logger.py:49] Constructing tf.data.Dataset redpajama_wikipedia for split train, from gs://rosinality-tpu-bucket/redpajama_wikipedia/1.0.0
I0512 01:45:05.773736 140188559153152 dataset_info.py:540] Load dataset info from gs://rosinality-tpu-bucket/the_stack_dedup/1.0.0
I0512 01:45:05.808187 140392160929792 dataset_info.py:540] Load dataset info from gs://rosinality-tpu-bucket/the_stack_dedup/1.0.0
I0512 01:45:05.787249 139812357228544 dataset_info.py:540] Load dataset info from gs://rosinality-tpu-bucket/the_stack_dedup/1.0.0
I0512 01:45:05.831533 140577131780096 dataset_info.py:540] Load dataset info from gs://rosinality-tpu-bucket/the_stack_dedup/1.0.0
I0512 01:45:05.855012 139997397456896 dataset_info.py:540] Load dataset info from gs://rosinality-tpu-bucket/the_stack_dedup/1.0.0
I0512 01:45:05.859536 140172769531904 dataset_info.py:540] Load dataset info from gs://rosinality-tpu-bucket/the_stack_dedup/1.0.0
I0512 01:45:05.839373 139676058949632 dataset_info.py:540] Load dataset info from gs://rosinality-tpu-bucket/the_stack_dedup/1.0.0
I0512 01:45:05.933340 140316961150976 dataset_info.py:540] Load dataset info from gs://rosinality-tpu-bucket/the_stack_dedup/1.0.0
I0512 01:45:06.184582 140188559153152 dataset_providers.py:1597] Sharding at the data source: 6 of 8
I0512 01:45:06.190874 140392160929792 dataset_providers.py:1597] Sharding at the data source: 4 of 8
I0512 01:45:06.196387 140577131780096 dataset_providers.py:1597] Sharding at the data source: 2 of 8
I0512 01:45:06.190624 139676058949632 dataset_providers.py:1597] Sharding at the data source: 3 of 8
I0512 01:45:06.211619 139997397456896 dataset_providers.py:1597] Sharding at the data source: 5 of 8
I0512 01:45:06.209807 139812357228544 dataset_providers.py:1597] Sharding at the data source: 7 of 8
I0512 01:45:06.250379 140172769531904 dataset_providers.py:1597] Sharding at the data source: 8 of 8
I0512 01:45:06.250154 140188559153152 dataset_info.py:540] Load dataset info from gs://rosinality-tpu-bucket/the_stack_dedup/1.0.0
I0512 01:45:06.260749 140392160929792 dataset_info.py:540] Load dataset info from gs://rosinality-tpu-bucket/the_stack_dedup/1.0.0
I0512 01:45:06.265140 140577131780096 dataset_info.py:540] Load dataset info from gs://rosinality-tpu-bucket/the_stack_dedup/1.0.0
I0512 01:45:06.258478 139676058949632 dataset_info.py:540] Load dataset info from gs://rosinality-tpu-bucket/the_stack_dedup/1.0.0
I0512 01:45:06.272349 139812357228544 dataset_info.py:540] Load dataset info from gs://rosinality-tpu-bucket/the_stack_dedup/1.0.0
I0512 01:45:06.294286 139997397456896 dataset_info.py:540] Load dataset info from gs://rosinality-tpu-bucket/the_stack_dedup/1.0.0
I0512 01:45:06.317834 140172769531904 dataset_info.py:540] Load dataset info from gs://rosinality-tpu-bucket/the_stack_dedup/1.0.0
I0512 01:45:06.348322 140316961150976 dataset_providers.py:1597] Sharding at the data source: 1 of 8
I0512 01:45:06.426212 140316961150976 dataset_info.py:540] Load dataset info from gs://rosinality-tpu-bucket/the_stack_dedup/1.0.0
I0512 01:45:06.576891 140188559153152 dataset_builder.py:421] Reusing dataset the_stack_dedup (gs://rosinality-tpu-bucket/the_stack_dedup/1.0.0)
I0512 01:45:06.585322 140392160929792 dataset_builder.py:421] Reusing dataset the_stack_dedup (gs://rosinality-tpu-bucket/the_stack_dedup/1.0.0)
I0512 01:45:06.587295 139676058949632 dataset_builder.py:421] Reusing dataset the_stack_dedup (gs://rosinality-tpu-bucket/the_stack_dedup/1.0.0)
I0512 01:45:06.611196 140577131780096 dataset_builder.py:421] Reusing dataset the_stack_dedup (gs://rosinality-tpu-bucket/the_stack_dedup/1.0.0)
I0512 01:45:06.603159 139812357228544 dataset_builder.py:421] Reusing dataset the_stack_dedup (gs://rosinality-tpu-bucket/the_stack_dedup/1.0.0)
I0512 01:45:06.634588 139997397456896 dataset_builder.py:421] Reusing dataset the_stack_dedup (gs://rosinality-tpu-bucket/the_stack_dedup/1.0.0)
I0512 01:45:06.657040 140172769531904 dataset_builder.py:421] Reusing dataset the_stack_dedup (gs://rosinality-tpu-bucket/the_stack_dedup/1.0.0)
I0512 01:45:06.695740 140392160929792 logging_logger.py:49] Constructing tf.data.Dataset the_stack_dedup for split train, from gs://rosinality-tpu-bucket/the_stack_dedup/1.0.0
I0512 01:45:06.696884 140188559153152 logging_logger.py:49] Constructing tf.data.Dataset the_stack_dedup for split train, from gs://rosinality-tpu-bucket/the_stack_dedup/1.0.0
I0512 01:45:06.701935 139676058949632 logging_logger.py:49] Constructing tf.data.Dataset the_stack_dedup for split train, from gs://rosinality-tpu-bucket/the_stack_dedup/1.0.0
I0512 01:45:06.723298 139812357228544 logging_logger.py:49] Constructing tf.data.Dataset the_stack_dedup for split train, from gs://rosinality-tpu-bucket/the_stack_dedup/1.0.0
I0512 01:45:06.745629 140577131780096 logging_logger.py:49] Constructing tf.data.Dataset the_stack_dedup for split train, from gs://rosinality-tpu-bucket/the_stack_dedup/1.0.0
I0512 01:45:06.747897 139997397456896 logging_logger.py:49] Constructing tf.data.Dataset the_stack_dedup for split train, from gs://rosinality-tpu-bucket/the_stack_dedup/1.0.0
I0512 01:45:06.772685 140172769531904 logging_logger.py:49] Constructing tf.data.Dataset the_stack_dedup for split train, from gs://rosinality-tpu-bucket/the_stack_dedup/1.0.0
I0512 01:45:06.771278 140316961150976 dataset_builder.py:421] Reusing dataset the_stack_dedup (gs://rosinality-tpu-bucket/the_stack_dedup/1.0.0)
I0512 01:45:06.885904 140316961150976 logging_logger.py:49] Constructing tf.data.Dataset the_stack_dedup for split train, from gs://rosinality-tpu-bucket/the_stack_dedup/1.0.0
I0512 01:45:08.444971 140188559153152 dataset_providers.py:2432] The output dataset from seqio.get_dataset has the following features
I0512 01:45:08.445157 140188559153152 dataset_providers.py:2438] feature: decoder_target_tokens 	 shape: [48, 2048] 	 dtype: int32
I0512 01:45:08.445214 140188559153152 dataset_providers.py:2438] feature: decoder_input_tokens 	 shape: [48, 2048] 	 dtype: int32
I0512 01:45:08.445256 140188559153152 dataset_providers.py:2438] feature: decoder_loss_weights 	 shape: [48, 2048] 	 dtype: int32
I0512 01:45:08.445296 140188559153152 dataset_providers.py:2438] feature: decoder_segment_ids 	 shape: [48, 2048] 	 dtype: int32
I0512 01:45:08.445335 140188559153152 dataset_providers.py:2438] feature: decoder_positions 	 shape: [48, 2048] 	 dtype: int32
I0512 01:45:08.428815 139676058949632 dataset_providers.py:2432] The output dataset from seqio.get_dataset has the following features
I0512 01:45:08.428962 139676058949632 dataset_providers.py:2438] feature: decoder_target_tokens 	 shape: [48, 2048] 	 dtype: int32
I0512 01:45:08.429017 139676058949632 dataset_providers.py:2438] feature: decoder_input_tokens 	 shape: [48, 2048] 	 dtype: int32
I0512 01:45:08.429059 139676058949632 dataset_providers.py:2438] feature: decoder_loss_weights 	 shape: [48, 2048] 	 dtype: int32
I0512 01:45:08.429099 139676058949632 dataset_providers.py:2438] feature: decoder_segment_ids 	 shape: [48, 2048] 	 dtype: int32
I0512 01:45:08.429138 139676058949632 dataset_providers.py:2438] feature: decoder_positions 	 shape: [48, 2048] 	 dtype: int32
I0512 01:45:08.455918 140392160929792 dataset_providers.py:2432] The output dataset from seqio.get_dataset has the following features
I0512 01:45:08.456074 140392160929792 dataset_providers.py:2438] feature: decoder_target_tokens 	 shape: [48, 2048] 	 dtype: int32
I0512 01:45:08.456127 140392160929792 dataset_providers.py:2438] feature: decoder_input_tokens 	 shape: [48, 2048] 	 dtype: int32
I0512 01:45:08.456168 140392160929792 dataset_providers.py:2438] feature: decoder_loss_weights 	 shape: [48, 2048] 	 dtype: int32
I0512 01:45:08.456208 140392160929792 dataset_providers.py:2438] feature: decoder_segment_ids 	 shape: [48, 2048] 	 dtype: int32
I0512 01:45:08.456246 140392160929792 dataset_providers.py:2438] feature: decoder_positions 	 shape: [48, 2048] 	 dtype: int32
I0512 01:45:08.472675 139812357228544 dataset_providers.py:2432] The output dataset from seqio.get_dataset has the following features
I0512 01:45:08.472859 139812357228544 dataset_providers.py:2438] feature: decoder_target_tokens 	 shape: [48, 2048] 	 dtype: int32
I0512 01:45:08.472910 139812357228544 dataset_providers.py:2438] feature: decoder_input_tokens 	 shape: [48, 2048] 	 dtype: int32
I0512 01:45:08.472948 139812357228544 dataset_providers.py:2438] feature: decoder_loss_weights 	 shape: [48, 2048] 	 dtype: int32
I0512 01:45:08.472985 139812357228544 dataset_providers.py:2438] feature: decoder_segment_ids 	 shape: [48, 2048] 	 dtype: int32
I0512 01:45:08.473021 139812357228544 dataset_providers.py:2438] feature: decoder_positions 	 shape: [48, 2048] 	 dtype: int32
I0512 01:45:08.517504 139997397456896 dataset_providers.py:2432] The output dataset from seqio.get_dataset has the following features
I0512 01:45:08.517669 139997397456896 dataset_providers.py:2438] feature: decoder_target_tokens 	 shape: [48, 2048] 	 dtype: int32
I0512 01:45:08.517721 139997397456896 dataset_providers.py:2438] feature: decoder_input_tokens 	 shape: [48, 2048] 	 dtype: int32
I0512 01:45:08.517762 139997397456896 dataset_providers.py:2438] feature: decoder_loss_weights 	 shape: [48, 2048] 	 dtype: int32
I0512 01:45:08.517801 139997397456896 dataset_providers.py:2438] feature: decoder_segment_ids 	 shape: [48, 2048] 	 dtype: int32
I0512 01:45:08.517839 139997397456896 dataset_providers.py:2438] feature: decoder_positions 	 shape: [48, 2048] 	 dtype: int32
I0512 01:45:08.523506 140172769531904 dataset_providers.py:2432] The output dataset from seqio.get_dataset has the following features
I0512 01:45:08.523683 140172769531904 dataset_providers.py:2438] feature: decoder_target_tokens 	 shape: [48, 2048] 	 dtype: int32
I0512 01:45:08.523735 140172769531904 dataset_providers.py:2438] feature: decoder_input_tokens 	 shape: [48, 2048] 	 dtype: int32
I0512 01:45:08.523776 140172769531904 dataset_providers.py:2438] feature: decoder_loss_weights 	 shape: [48, 2048] 	 dtype: int32
I0512 01:45:08.523815 140172769531904 dataset_providers.py:2438] feature: decoder_segment_ids 	 shape: [48, 2048] 	 dtype: int32
I0512 01:45:08.523853 140172769531904 dataset_providers.py:2438] feature: decoder_positions 	 shape: [48, 2048] 	 dtype: int32
I0512 01:45:08.555336 140577131780096 dataset_providers.py:2432] The output dataset from seqio.get_dataset has the following features
I0512 01:45:08.555527 140577131780096 dataset_providers.py:2438] feature: decoder_target_tokens 	 shape: [48, 2048] 	 dtype: int32
I0512 01:45:08.555578 140577131780096 dataset_providers.py:2438] feature: decoder_input_tokens 	 shape: [48, 2048] 	 dtype: int32
I0512 01:45:08.555616 140577131780096 dataset_providers.py:2438] feature: decoder_loss_weights 	 shape: [48, 2048] 	 dtype: int32
I0512 01:45:08.555652 140577131780096 dataset_providers.py:2438] feature: decoder_segment_ids 	 shape: [48, 2048] 	 dtype: int32
I0512 01:45:08.555694 140577131780096 dataset_providers.py:2438] feature: decoder_positions 	 shape: [48, 2048] 	 dtype: int32
I0512 01:45:08.617590 140316961150976 dataset_providers.py:2432] The output dataset from seqio.get_dataset has the following features
I0512 01:45:08.617752 140316961150976 dataset_providers.py:2438] feature: decoder_target_tokens 	 shape: [48, 2048] 	 dtype: int32
I0512 01:45:08.617807 140316961150976 dataset_providers.py:2438] feature: decoder_input_tokens 	 shape: [48, 2048] 	 dtype: int32
I0512 01:45:08.617850 140316961150976 dataset_providers.py:2438] feature: decoder_loss_weights 	 shape: [48, 2048] 	 dtype: int32
I0512 01:45:08.617892 140316961150976 dataset_providers.py:2438] feature: decoder_segment_ids 	 shape: [48, 2048] 	 dtype: int32
I0512 01:45:08.617932 140316961150976 dataset_providers.py:2438] feature: decoder_positions 	 shape: [48, 2048] 	 dtype: int32
W0512 01:45:09.362681 139676058949632 activation_partitioning.py:145] In /home/rosinality/t5x/flaxformer/flaxformer/architectures/t5/t5_architecture.py:520, activation_partitioning_dims was set, but it is deprecated and will be removed soon.
W0512 01:45:09.388996 140392160929792 activation_partitioning.py:145] In /home/rosinality/t5x/flaxformer/flaxformer/architectures/t5/t5_architecture.py:520, activation_partitioning_dims was set, but it is deprecated and will be removed soon.
W0512 01:45:09.370495 139676058949632 activation_partitioning.py:145] In /home/rosinality/t5x/flaxformer/flaxformer/architectures/t5/t5_architecture.py:586, activation_partitioning_dims was set, but it is deprecated and will be removed soon.
W0512 01:45:09.391771 140188559153152 activation_partitioning.py:145] In /home/rosinality/t5x/flaxformer/flaxformer/architectures/t5/t5_architecture.py:520, activation_partitioning_dims was set, but it is deprecated and will be removed soon.
W0512 01:45:09.396829 140392160929792 activation_partitioning.py:145] In /home/rosinality/t5x/flaxformer/flaxformer/architectures/t5/t5_architecture.py:586, activation_partitioning_dims was set, but it is deprecated and will be removed soon.
W0512 01:45:09.399559 140188559153152 activation_partitioning.py:145] In /home/rosinality/t5x/flaxformer/flaxformer/architectures/t5/t5_architecture.py:586, activation_partitioning_dims was set, but it is deprecated and will be removed soon.
W0512 01:45:09.413864 139812357228544 activation_partitioning.py:145] In /home/rosinality/t5x/flaxformer/flaxformer/architectures/t5/t5_architecture.py:520, activation_partitioning_dims was set, but it is deprecated and will be removed soon.
W0512 01:45:09.421969 139812357228544 activation_partitioning.py:145] In /home/rosinality/t5x/flaxformer/flaxformer/architectures/t5/t5_architecture.py:586, activation_partitioning_dims was set, but it is deprecated and will be removed soon.
W0512 01:45:09.421948 139676058949632 activation_partitioning.py:145] In /home/rosinality/t5x/flaxformer/flaxformer/architectures/t5/t5_architecture.py:623, activation_partitioning_dims was set, but it is deprecated and will be removed soon.
W0512 01:45:09.425127 139676058949632 activation_partitioning.py:145] In /home/rosinality/t5x/flaxformer/flaxformer/architectures/t5/t5_architecture.py:663, activation_partitioning_dims was set, but it is deprecated and will be removed soon.
W0512 01:45:09.448544 140392160929792 activation_partitioning.py:145] In /home/rosinality/t5x/flaxformer/flaxformer/architectures/t5/t5_architecture.py:623, activation_partitioning_dims was set, but it is deprecated and will be removed soon.
W0512 01:45:09.451770 140392160929792 activation_partitioning.py:145] In /home/rosinality/t5x/flaxformer/flaxformer/architectures/t5/t5_architecture.py:663, activation_partitioning_dims was set, but it is deprecated and will be removed soon.
W0512 01:45:09.451541 140188559153152 activation_partitioning.py:145] In /home/rosinality/t5x/flaxformer/flaxformer/architectures/t5/t5_architecture.py:623, activation_partitioning_dims was set, but it is deprecated and will be removed soon.
W0512 01:45:09.454977 140188559153152 activation_partitioning.py:145] In /home/rosinality/t5x/flaxformer/flaxformer/architectures/t5/t5_architecture.py:663, activation_partitioning_dims was set, but it is deprecated and will be removed soon.
W0512 01:45:09.474212 140172769531904 activation_partitioning.py:145] In /home/rosinality/t5x/flaxformer/flaxformer/architectures/t5/t5_architecture.py:520, activation_partitioning_dims was set, but it is deprecated and will be removed soon.
W0512 01:45:09.474912 139997397456896 activation_partitioning.py:145] In /home/rosinality/t5x/flaxformer/flaxformer/architectures/t5/t5_architecture.py:520, activation_partitioning_dims was set, but it is deprecated and will be removed soon.
W0512 01:45:09.482177 140172769531904 activation_partitioning.py:145] In /home/rosinality/t5x/flaxformer/flaxformer/architectures/t5/t5_architecture.py:586, activation_partitioning_dims was set, but it is deprecated and will be removed soon.
W0512 01:45:09.482968 139997397456896 activation_partitioning.py:145] In /home/rosinality/t5x/flaxformer/flaxformer/architectures/t5/t5_architecture.py:586, activation_partitioning_dims was set, but it is deprecated and will be removed soon.
W0512 01:45:09.470502 139676058949632 activation_partitioning.py:145] In /home/rosinality/t5x/flaxformer/flaxformer/architectures/t5/t5_architecture.py:678, activation_partitioning_dims was set, but it is deprecated and will be removed soon.
W0512 01:45:09.473308 139812357228544 activation_partitioning.py:145] In /home/rosinality/t5x/flaxformer/flaxformer/architectures/t5/t5_architecture.py:623, activation_partitioning_dims was set, but it is deprecated and will be removed soon.
W0512 01:45:09.497378 140392160929792 activation_partitioning.py:145] In /home/rosinality/t5x/flaxformer/flaxformer/architectures/t5/t5_architecture.py:678, activation_partitioning_dims was set, but it is deprecated and will be removed soon.
W0512 01:45:09.474927 139676058949632 activation_partitioning.py:145] In /home/rosinality/t5x/flaxformer/flaxformer/architectures/t5/t5_architecture.py:520, activation_partitioning_dims was set, but it is deprecated and will be removed soon.
W0512 01:45:09.476592 139812357228544 activation_partitioning.py:145] In /home/rosinality/t5x/flaxformer/flaxformer/architectures/t5/t5_architecture.py:663, activation_partitioning_dims was set, but it is deprecated and will be removed soon.
W0512 01:45:09.478125 139676058949632 activation_partitioning.py:145] In /home/rosinality/t5x/flaxformer/flaxformer/architectures/t5/t5_architecture.py:586, activation_partitioning_dims was set, but it is deprecated and will be removed soon.
W0512 01:45:09.501747 140392160929792 activation_partitioning.py:145] In /home/rosinality/t5x/flaxformer/flaxformer/architectures/t5/t5_architecture.py:520, activation_partitioning_dims was set, but it is deprecated and will be removed soon.
W0512 01:45:09.501441 140188559153152 activation_partitioning.py:145] In /home/rosinality/t5x/flaxformer/flaxformer/architectures/t5/t5_architecture.py:678, activation_partitioning_dims was set, but it is deprecated and will be removed soon.
W0512 01:45:09.505183 140392160929792 activation_partitioning.py:145] In /home/rosinality/t5x/flaxformer/flaxformer/architectures/t5/t5_architecture.py:586, activation_partitioning_dims was set, but it is deprecated and will be removed soon.
W0512 01:45:09.503192 140577131780096 activation_partitioning.py:145] In /home/rosinality/t5x/flaxformer/flaxformer/architectures/t5/t5_architecture.py:520, activation_partitioning_dims was set, but it is deprecated and will be removed soon.
W0512 01:45:09.505844 140188559153152 activation_partitioning.py:145] In /home/rosinality/t5x/flaxformer/flaxformer/architectures/t5/t5_architecture.py:520, activation_partitioning_dims was set, but it is deprecated and will be removed soon.
W0512 01:45:09.510918 140577131780096 activation_partitioning.py:145] In /home/rosinality/t5x/flaxformer/flaxformer/architectures/t5/t5_architecture.py:586, activation_partitioning_dims was set, but it is deprecated and will be removed soon.
W0512 01:45:09.509063 140188559153152 activation_partitioning.py:145] In /home/rosinality/t5x/flaxformer/flaxformer/architectures/t5/t5_architecture.py:586, activation_partitioning_dims was set, but it is deprecated and will be removed soon.
W0512 01:45:09.500313 139676058949632 activation_partitioning.py:145] In /home/rosinality/t5x/flaxformer/flaxformer/architectures/t5/t5_architecture.py:623, activation_partitioning_dims was set, but it is deprecated and will be removed soon.
W0512 01:45:09.503767 139676058949632 activation_partitioning.py:145] In /home/rosinality/t5x/flaxformer/flaxformer/architectures/t5/t5_architecture.py:663, activation_partitioning_dims was set, but it is deprecated and will be removed soon.
W0512 01:45:09.527286 140392160929792 activation_partitioning.py:145] In /home/rosinality/t5x/flaxformer/flaxformer/architectures/t5/t5_architecture.py:623, activation_partitioning_dims was set, but it is deprecated and will be removed soon.
W0512 01:45:09.530520 140392160929792 activation_partitioning.py:145] In /home/rosinality/t5x/flaxformer/flaxformer/architectures/t5/t5_architecture.py:663, activation_partitioning_dims was set, but it is deprecated and will be removed soon.
W0512 01:45:09.530992 140188559153152 activation_partitioning.py:145] In /home/rosinality/t5x/flaxformer/flaxformer/architectures/t5/t5_architecture.py:623, activation_partitioning_dims was set, but it is deprecated and will be removed soon.
W0512 01:45:09.533896 140172769531904 activation_partitioning.py:145] In /home/rosinality/t5x/flaxformer/flaxformer/architectures/t5/t5_architecture.py:623, activation_partitioning_dims was set, but it is deprecated and will be removed soon.
W0512 01:45:09.534168 140188559153152 activation_partitioning.py:145] In /home/rosinality/t5x/flaxformer/flaxformer/architectures/t5/t5_architecture.py:663, activation_partitioning_dims was set, but it is deprecated and will be removed soon.
W0512 01:45:09.535169 139997397456896 activation_partitioning.py:145] In /home/rosinality/t5x/flaxformer/flaxformer/architectures/t5/t5_architecture.py:623, activation_partitioning_dims was set, but it is deprecated and will be removed soon.
W0512 01:45:09.537148 140172769531904 activation_partitioning.py:145] In /home/rosinality/t5x/flaxformer/flaxformer/architectures/t5/t5_architecture.py:663, activation_partitioning_dims was set, but it is deprecated and will be removed soon.
W0512 01:45:09.538422 139997397456896 activation_partitioning.py:145] In /home/rosinality/t5x/flaxformer/flaxformer/architectures/t5/t5_architecture.py:663, activation_partitioning_dims was set, but it is deprecated and will be removed soon.
W0512 01:45:09.517739 139676058949632 activation_partitioning.py:145] In /home/rosinality/t5x/flaxformer/flaxformer/architectures/t5/t5_architecture.py:678, activation_partitioning_dims was set, but it is deprecated and will be removed soon.
W0512 01:45:09.543662 140392160929792 activation_partitioning.py:145] In /home/rosinality/t5x/flaxformer/flaxformer/architectures/t5/t5_architecture.py:678, activation_partitioning_dims was set, but it is deprecated and will be removed soon.
W0512 01:45:09.522169 139812357228544 activation_partitioning.py:145] In /home/rosinality/t5x/flaxformer/flaxformer/architectures/t5/t5_architecture.py:678, activation_partitioning_dims was set, but it is deprecated and will be removed soon.
W0512 01:45:09.526611 139812357228544 activation_partitioning.py:145] In /home/rosinality/t5x/flaxformer/flaxformer/architectures/t5/t5_architecture.py:520, activation_partitioning_dims was set, but it is deprecated and will be removed soon.
W0512 01:45:09.547216 140188559153152 activation_partitioning.py:145] In /home/rosinality/t5x/flaxformer/flaxformer/architectures/t5/t5_architecture.py:678, activation_partitioning_dims was set, but it is deprecated and will be removed soon.
W0512 01:45:09.529841 139812357228544 activation_partitioning.py:145] In /home/rosinality/t5x/flaxformer/flaxformer/architectures/t5/t5_architecture.py:586, activation_partitioning_dims was set, but it is deprecated and will be removed soon.
W0512 01:45:09.562255 140577131780096 activation_partitioning.py:145] In /home/rosinality/t5x/flaxformer/flaxformer/architectures/t5/t5_architecture.py:623, activation_partitioning_dims was set, but it is deprecated and will be removed soon.
W0512 01:45:09.565476 140577131780096 activation_partitioning.py:145] In /home/rosinality/t5x/flaxformer/flaxformer/architectures/t5/t5_architecture.py:663, activation_partitioning_dims was set, but it is deprecated and will be removed soon.
W0512 01:45:09.564671 140316961150976 activation_partitioning.py:145] In /home/rosinality/t5x/flaxformer/flaxformer/architectures/t5/t5_architecture.py:520, activation_partitioning_dims was set, but it is deprecated and will be removed soon.
W0512 01:45:09.552090 139812357228544 activation_partitioning.py:145] In /home/rosinality/t5x/flaxformer/flaxformer/architectures/t5/t5_architecture.py:623, activation_partitioning_dims was set, but it is deprecated and will be removed soon.
W0512 01:45:09.572229 140316961150976 activation_partitioning.py:145] In /home/rosinality/t5x/flaxformer/flaxformer/architectures/t5/t5_architecture.py:586, activation_partitioning_dims was set, but it is deprecated and will be removed soon.
W0512 01:45:09.555299 139812357228544 activation_partitioning.py:145] In /home/rosinality/t5x/flaxformer/flaxformer/architectures/t5/t5_architecture.py:663, activation_partitioning_dims was set, but it is deprecated and will be removed soon.
W0512 01:45:09.582488 140172769531904 activation_partitioning.py:145] In /home/rosinality/t5x/flaxformer/flaxformer/architectures/t5/t5_architecture.py:678, activation_partitioning_dims was set, but it is deprecated and will be removed soon.
W0512 01:45:09.584057 139997397456896 activation_partitioning.py:145] In /home/rosinality/t5x/flaxformer/flaxformer/architectures/t5/t5_architecture.py:678, activation_partitioning_dims was set, but it is deprecated and will be removed soon.
W0512 01:45:09.586853 140172769531904 activation_partitioning.py:145] In /home/rosinality/t5x/flaxformer/flaxformer/architectures/t5/t5_architecture.py:520, activation_partitioning_dims was set, but it is deprecated and will be removed soon.
W0512 01:45:09.590043 140172769531904 activation_partitioning.py:145] In /home/rosinality/t5x/flaxformer/flaxformer/architectures/t5/t5_architecture.py:586, activation_partitioning_dims was set, but it is deprecated and will be removed soon.
W0512 01:45:09.588402 139997397456896 activation_partitioning.py:145] In /home/rosinality/t5x/flaxformer/flaxformer/architectures/t5/t5_architecture.py:520, activation_partitioning_dims was set, but it is deprecated and will be removed soon.
W0512 01:45:09.568543 139812357228544 activation_partitioning.py:145] In /home/rosinality/t5x/flaxformer/flaxformer/architectures/t5/t5_architecture.py:678, activation_partitioning_dims was set, but it is deprecated and will be removed soon.
W0512 01:45:09.591618 139997397456896 activation_partitioning.py:145] In /home/rosinality/t5x/flaxformer/flaxformer/architectures/t5/t5_architecture.py:586, activation_partitioning_dims was set, but it is deprecated and will be removed soon.
W0512 01:45:09.612062 140172769531904 activation_partitioning.py:145] In /home/rosinality/t5x/flaxformer/flaxformer/architectures/t5/t5_architecture.py:623, activation_partitioning_dims was set, but it is deprecated and will be removed soon.
W0512 01:45:09.611052 140577131780096 activation_partitioning.py:145] In /home/rosinality/t5x/flaxformer/flaxformer/architectures/t5/t5_architecture.py:678, activation_partitioning_dims was set, but it is deprecated and will be removed soon.
W0512 01:45:09.615286 140172769531904 activation_partitioning.py:145] In /home/rosinality/t5x/flaxformer/flaxformer/architectures/t5/t5_architecture.py:663, activation_partitioning_dims was set, but it is deprecated and will be removed soon.
W0512 01:45:09.613876 139997397456896 activation_partitioning.py:145] In /home/rosinality/t5x/flaxformer/flaxformer/architectures/t5/t5_architecture.py:623, activation_partitioning_dims was set, but it is deprecated and will be removed soon.
W0512 01:45:09.615471 140577131780096 activation_partitioning.py:145] In /home/rosinality/t5x/flaxformer/flaxformer/architectures/t5/t5_architecture.py:520, activation_partitioning_dims was set, but it is deprecated and will be removed soon.
W0512 01:45:09.617107 139997397456896 activation_partitioning.py:145] In /home/rosinality/t5x/flaxformer/flaxformer/architectures/t5/t5_architecture.py:663, activation_partitioning_dims was set, but it is deprecated and will be removed soon.
W0512 01:45:09.618659 140577131780096 activation_partitioning.py:145] In /home/rosinality/t5x/flaxformer/flaxformer/architectures/t5/t5_architecture.py:586, activation_partitioning_dims was set, but it is deprecated and will be removed soon.
W0512 01:45:09.628490 140172769531904 activation_partitioning.py:145] In /home/rosinality/t5x/flaxformer/flaxformer/architectures/t5/t5_architecture.py:678, activation_partitioning_dims was set, but it is deprecated and will be removed soon.
W0512 01:45:09.624844 140316961150976 activation_partitioning.py:145] In /home/rosinality/t5x/flaxformer/flaxformer/architectures/t5/t5_architecture.py:623, activation_partitioning_dims was set, but it is deprecated and will be removed soon.
W0512 01:45:09.630281 139997397456896 activation_partitioning.py:145] In /home/rosinality/t5x/flaxformer/flaxformer/architectures/t5/t5_architecture.py:678, activation_partitioning_dims was set, but it is deprecated and will be removed soon.
W0512 01:45:09.628021 140316961150976 activation_partitioning.py:145] In /home/rosinality/t5x/flaxformer/flaxformer/architectures/t5/t5_architecture.py:663, activation_partitioning_dims was set, but it is deprecated and will be removed soon.
W0512 01:45:09.640782 140577131780096 activation_partitioning.py:145] In /home/rosinality/t5x/flaxformer/flaxformer/architectures/t5/t5_architecture.py:623, activation_partitioning_dims was set, but it is deprecated and will be removed soon.
W0512 01:45:09.644037 140577131780096 activation_partitioning.py:145] In /home/rosinality/t5x/flaxformer/flaxformer/architectures/t5/t5_architecture.py:663, activation_partitioning_dims was set, but it is deprecated and will be removed soon.
W0512 01:45:09.657207 140577131780096 activation_partitioning.py:145] In /home/rosinality/t5x/flaxformer/flaxformer/architectures/t5/t5_architecture.py:678, activation_partitioning_dims was set, but it is deprecated and will be removed soon.
W0512 01:45:09.672771 140316961150976 activation_partitioning.py:145] In /home/rosinality/t5x/flaxformer/flaxformer/architectures/t5/t5_architecture.py:678, activation_partitioning_dims was set, but it is deprecated and will be removed soon.
W0512 01:45:09.677107 140316961150976 activation_partitioning.py:145] In /home/rosinality/t5x/flaxformer/flaxformer/architectures/t5/t5_architecture.py:520, activation_partitioning_dims was set, but it is deprecated and will be removed soon.
W0512 01:45:09.680268 140316961150976 activation_partitioning.py:145] In /home/rosinality/t5x/flaxformer/flaxformer/architectures/t5/t5_architecture.py:586, activation_partitioning_dims was set, but it is deprecated and will be removed soon.
W0512 01:45:09.702108 140316961150976 activation_partitioning.py:145] In /home/rosinality/t5x/flaxformer/flaxformer/architectures/t5/t5_architecture.py:623, activation_partitioning_dims was set, but it is deprecated and will be removed soon.
W0512 01:45:09.705265 140316961150976 activation_partitioning.py:145] In /home/rosinality/t5x/flaxformer/flaxformer/architectures/t5/t5_architecture.py:663, activation_partitioning_dims was set, but it is deprecated and will be removed soon.
I0512 01:45:09.690817 139676058949632 moe_layers.py:777] Selected group_size=4096 and num_groups=192 for input num_tokens=786432, max_group_size=4096, num_experts=8 and num_expert_replicas=1
I0512 01:45:09.715344 140392160929792 moe_layers.py:777] Selected group_size=4096 and num_groups=192 for input num_tokens=786432, max_group_size=4096, num_experts=8 and num_expert_replicas=1
I0512 01:45:09.717303 140188559153152 moe_layers.py:777] Selected group_size=4096 and num_groups=192 for input num_tokens=786432, max_group_size=4096, num_experts=8 and num_expert_replicas=1
W0512 01:45:09.718326 140316961150976 activation_partitioning.py:145] In /home/rosinality/t5x/flaxformer/flaxformer/architectures/t5/t5_architecture.py:678, activation_partitioning_dims was set, but it is deprecated and will be removed soon.
I0512 01:45:09.741178 139812357228544 moe_layers.py:777] Selected group_size=4096 and num_groups=192 for input num_tokens=786432, max_group_size=4096, num_experts=8 and num_expert_replicas=1
I0512 01:45:09.801691 140172769531904 moe_layers.py:777] Selected group_size=4096 and num_groups=192 for input num_tokens=786432, max_group_size=4096, num_experts=8 and num_expert_replicas=1
I0512 01:45:09.801224 139997397456896 moe_layers.py:777] Selected group_size=4096 and num_groups=192 for input num_tokens=786432, max_group_size=4096, num_experts=8 and num_expert_replicas=1
I0512 01:45:09.830040 140577131780096 moe_layers.py:777] Selected group_size=4096 and num_groups=192 for input num_tokens=786432, max_group_size=4096, num_experts=8 and num_expert_replicas=1
I0512 01:45:09.887713 140316961150976 moe_layers.py:777] Selected group_size=4096 and num_groups=192 for input num_tokens=786432, max_group_size=4096, num_experts=8 and num_expert_replicas=1
I0512 01:45:10.074989 139676058949632 moe_layers.py:777] Selected group_size=4096 and num_groups=192 for input num_tokens=786432, max_group_size=4096, num_experts=8 and num_expert_replicas=1
I0512 01:45:10.099928 140392160929792 moe_layers.py:777] Selected group_size=4096 and num_groups=192 for input num_tokens=786432, max_group_size=4096, num_experts=8 and num_expert_replicas=1
I0512 01:45:10.097428 140188559153152 moe_layers.py:777] Selected group_size=4096 and num_groups=192 for input num_tokens=786432, max_group_size=4096, num_experts=8 and num_expert_replicas=1
I0512 01:45:10.128396 139812357228544 moe_layers.py:777] Selected group_size=4096 and num_groups=192 for input num_tokens=786432, max_group_size=4096, num_experts=8 and num_expert_replicas=1
I0512 01:45:10.184055 139997397456896 moe_layers.py:777] Selected group_size=4096 and num_groups=192 for input num_tokens=786432, max_group_size=4096, num_experts=8 and num_expert_replicas=1
I0512 01:45:10.189919 140172769531904 moe_layers.py:777] Selected group_size=4096 and num_groups=192 for input num_tokens=786432, max_group_size=4096, num_experts=8 and num_expert_replicas=1
I0512 01:45:10.215599 140577131780096 moe_layers.py:777] Selected group_size=4096 and num_groups=192 for input num_tokens=786432, max_group_size=4096, num_experts=8 and num_expert_replicas=1
I0512 01:45:10.263949 140316961150976 moe_layers.py:777] Selected group_size=4096 and num_groups=192 for input num_tokens=786432, max_group_size=4096, num_experts=8 and num_expert_replicas=1
I0512 01:45:10.396939 139676058949632 moe_layers.py:777] Selected group_size=4096 and num_groups=192 for input num_tokens=786432, max_group_size=4096, num_experts=8 and num_expert_replicas=1
I0512 01:45:10.422623 140392160929792 moe_layers.py:777] Selected group_size=4096 and num_groups=192 for input num_tokens=786432, max_group_size=4096, num_experts=8 and num_expert_replicas=1
I0512 01:45:10.422614 140188559153152 moe_layers.py:777] Selected group_size=4096 and num_groups=192 for input num_tokens=786432, max_group_size=4096, num_experts=8 and num_expert_replicas=1
I0512 01:45:10.452078 139812357228544 moe_layers.py:777] Selected group_size=4096 and num_groups=192 for input num_tokens=786432, max_group_size=4096, num_experts=8 and num_expert_replicas=1
I0512 01:45:10.508753 139997397456896 moe_layers.py:777] Selected group_size=4096 and num_groups=192 for input num_tokens=786432, max_group_size=4096, num_experts=8 and num_expert_replicas=1
I0512 01:45:10.515839 140172769531904 moe_layers.py:777] Selected group_size=4096 and num_groups=192 for input num_tokens=786432, max_group_size=4096, num_experts=8 and num_expert_replicas=1
I0512 01:45:10.540217 140577131780096 moe_layers.py:777] Selected group_size=4096 and num_groups=192 for input num_tokens=786432, max_group_size=4096, num_experts=8 and num_expert_replicas=1
I0512 01:45:10.581947 140316961150976 moe_layers.py:777] Selected group_size=4096 and num_groups=192 for input num_tokens=786432, max_group_size=4096, num_experts=8 and num_expert_replicas=1
I0512 01:45:10.898105 140316961150976 moe_layers.py:777] Selected group_size=4096 and num_groups=192 for input num_tokens=786432, max_group_size=4096, num_experts=8 and num_expert_replicas=1
I0512 01:45:10.952748 139676058949632 moe_layers.py:777] Selected group_size=4096 and num_groups=192 for input num_tokens=786432, max_group_size=4096, num_experts=8 and num_expert_replicas=1
I0512 01:45:10.976897 140392160929792 moe_layers.py:777] Selected group_size=4096 and num_groups=192 for input num_tokens=786432, max_group_size=4096, num_experts=8 and num_expert_replicas=1
I0512 01:45:10.976912 140188559153152 moe_layers.py:777] Selected group_size=4096 and num_groups=192 for input num_tokens=786432, max_group_size=4096, num_experts=8 and num_expert_replicas=1
W0512 01:45:10.994837 140316961150976 adafactor.py:358] Since rank of parameter decoder/decoder_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:10.995847 140316961150976 adafactor.py:358] Since rank of parameter decoder/layers_0/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:10.997028 140316961150976 adafactor.py:358] Since rank of parameter decoder/layers_0/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:10.998327 140316961150976 adafactor.py:358] Since rank of parameter decoder/layers_0/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:10.999549 140316961150976 adafactor.py:358] Since rank of parameter decoder/layers_0/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:11.000450 140316961150976 adafactor.py:358] Since rank of parameter decoder/layers_0/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:11.001332 140316961150976 adafactor.py:358] Since rank of parameter decoder/layers_0/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.002444 140316961150976 adafactor.py:358] Since rank of parameter decoder/layers_0/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:11.003538 140316961150976 adafactor.py:358] Since rank of parameter decoder/layers_0/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.004676 140316961150976 adafactor.py:358] Since rank of parameter decoder/layers_0/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.005780 140316961150976 adafactor.py:358] Since rank of parameter decoder/layers_1/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.006837 140316961150976 adafactor.py:358] Since rank of parameter decoder/layers_1/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.007900 140316961150976 adafactor.py:358] Since rank of parameter decoder/layers_1/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:11.009078 140316961150976 adafactor.py:358] Since rank of parameter decoder/layers_1/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:11.009987 140316961150976 adafactor.py:358] Since rank of parameter decoder/layers_1/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:11.010873 140316961150976 adafactor.py:358] Since rank of parameter decoder/layers_1/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.011939 140316961150976 adafactor.py:358] Since rank of parameter decoder/layers_1/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:11.013743 140316961150976 adafactor.py:358] Since rank of parameter decoder/layers_1/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.014817 140316961150976 adafactor.py:358] Since rank of parameter decoder/layers_1/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.015887 140316961150976 adafactor.py:358] Since rank of parameter decoder/layers_10/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.017028 140316961150976 adafactor.py:358] Since rank of parameter decoder/layers_10/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
I0512 01:45:11.012146 139812357228544 moe_layers.py:777] Selected group_size=4096 and num_groups=192 for input num_tokens=786432, max_group_size=4096, num_experts=8 and num_expert_replicas=1
I0512 01:45:11.061099 139997397456896 moe_layers.py:777] Selected group_size=4096 and num_groups=192 for input num_tokens=786432, max_group_size=4096, num_experts=8 and num_expert_replicas=1
I0512 01:45:11.071563 140172769531904 moe_layers.py:777] Selected group_size=4096 and num_groups=192 for input num_tokens=786432, max_group_size=4096, num_experts=8 and num_expert_replicas=1
W0512 01:45:11.052272 139676058949632 adafactor.py:358] Since rank of parameter decoder/decoder_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:11.075806 140392160929792 adafactor.py:358] Since rank of parameter decoder/decoder_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:11.053301 139676058949632 adafactor.py:358] Since rank of parameter decoder/layers_0/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.076832 140392160929792 adafactor.py:358] Since rank of parameter decoder/layers_0/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.054559 139676058949632 adafactor.py:358] Since rank of parameter decoder/layers_0/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.074640 140188559153152 adafactor.py:358] Since rank of parameter decoder/decoder_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:11.078037 140392160929792 adafactor.py:358] Since rank of parameter decoder/layers_0/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.075665 140188559153152 adafactor.py:358] Since rank of parameter decoder/layers_0/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.055731 139676058949632 adafactor.py:358] Since rank of parameter decoder/layers_0/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:11.079224 140392160929792 adafactor.py:358] Since rank of parameter decoder/layers_0/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:11.076845 140188559153152 adafactor.py:358] Since rank of parameter decoder/layers_0/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.056994 139676058949632 adafactor.py:358] Since rank of parameter decoder/layers_0/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:11.080487 140392160929792 adafactor.py:358] Since rank of parameter decoder/layers_0/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:11.077986 140188559153152 adafactor.py:358] Since rank of parameter decoder/layers_0/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:11.057983 139676058949632 adafactor.py:358] Since rank of parameter decoder/layers_0/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:11.081443 140392160929792 adafactor.py:358] Since rank of parameter decoder/layers_0/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:11.058897 139676058949632 adafactor.py:358] Since rank of parameter decoder/layers_0/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.079211 140188559153152 adafactor.py:358] Since rank of parameter decoder/layers_0/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:11.082388 140392160929792 adafactor.py:358] Since rank of parameter decoder/layers_0/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.080100 140188559153152 adafactor.py:358] Since rank of parameter decoder/layers_0/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:11.060003 139676058949632 adafactor.py:358] Since rank of parameter decoder/layers_0/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:11.083507 140392160929792 adafactor.py:358] Since rank of parameter decoder/layers_0/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:11.080996 140188559153152 adafactor.py:358] Since rank of parameter decoder/layers_0/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.061106 139676058949632 adafactor.py:358] Since rank of parameter decoder/layers_0/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.084642 140392160929792 adafactor.py:358] Since rank of parameter decoder/layers_0/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.082070 140188559153152 adafactor.py:358] Since rank of parameter decoder/layers_0/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:11.062312 139676058949632 adafactor.py:358] Since rank of parameter decoder/layers_0/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.085820 140392160929792 adafactor.py:358] Since rank of parameter decoder/layers_0/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.083196 140188559153152 adafactor.py:358] Since rank of parameter decoder/layers_0/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.063437 139676058949632 adafactor.py:358] Since rank of parameter decoder/layers_1/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.086979 140392160929792 adafactor.py:358] Since rank of parameter decoder/layers_1/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.064544 139676058949632 adafactor.py:358] Since rank of parameter decoder/layers_1/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.084620 140188559153152 adafactor.py:358] Since rank of parameter decoder/layers_0/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.088079 140392160929792 adafactor.py:358] Since rank of parameter decoder/layers_1/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.065640 139676058949632 adafactor.py:358] Since rank of parameter decoder/layers_1/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:11.085763 140188559153152 adafactor.py:358] Since rank of parameter decoder/layers_1/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.089185 140392160929792 adafactor.py:358] Since rank of parameter decoder/layers_1/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:11.086905 140188559153152 adafactor.py:358] Since rank of parameter decoder/layers_1/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.066847 139676058949632 adafactor.py:358] Since rank of parameter decoder/layers_1/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:11.090385 140392160929792 adafactor.py:358] Since rank of parameter decoder/layers_1/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:11.067758 139676058949632 adafactor.py:358] Since rank of parameter decoder/layers_1/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:11.087994 140188559153152 adafactor.py:358] Since rank of parameter decoder/layers_1/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:11.091277 140392160929792 adafactor.py:358] Since rank of parameter decoder/layers_1/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:11.068646 139676058949632 adafactor.py:358] Since rank of parameter decoder/layers_1/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.092173 140392160929792 adafactor.py:358] Since rank of parameter decoder/layers_1/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.089150 140188559153152 adafactor.py:358] Since rank of parameter decoder/layers_1/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:11.069752 139676058949632 adafactor.py:358] Since rank of parameter decoder/layers_1/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:11.090037 140188559153152 adafactor.py:358] Since rank of parameter decoder/layers_1/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:11.093265 140392160929792 adafactor.py:358] Since rank of parameter decoder/layers_1/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:11.090950 140188559153152 adafactor.py:358] Since rank of parameter decoder/layers_1/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.070946 139676058949632 adafactor.py:358] Since rank of parameter decoder/layers_1/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.094460 140392160929792 adafactor.py:358] Since rank of parameter decoder/layers_1/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.092030 140188559153152 adafactor.py:358] Since rank of parameter decoder/layers_1/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:11.072048 139676058949632 adafactor.py:358] Since rank of parameter decoder/layers_1/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.095564 140392160929792 adafactor.py:358] Since rank of parameter decoder/layers_1/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.093185 140188559153152 adafactor.py:358] Since rank of parameter decoder/layers_1/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.073147 139676058949632 adafactor.py:358] Since rank of parameter decoder/layers_10/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.096654 140392160929792 adafactor.py:358] Since rank of parameter decoder/layers_10/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.094291 140188559153152 adafactor.py:358] Since rank of parameter decoder/layers_1/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.074326 139676058949632 adafactor.py:358] Since rank of parameter decoder/layers_10/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.097775 140392160929792 adafactor.py:358] Since rank of parameter decoder/layers_10/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.075511 139676058949632 adafactor.py:358] Since rank of parameter decoder/layers_10/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:11.095409 140188559153152 adafactor.py:358] Since rank of parameter decoder/layers_10/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.098989 140392160929792 adafactor.py:358] Since rank of parameter decoder/layers_10/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:11.096508 140188559153152 adafactor.py:358] Since rank of parameter decoder/layers_10/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.076624 139676058949632 adafactor.py:358] Since rank of parameter decoder/layers_10/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:11.100096 140392160929792 adafactor.py:358] Since rank of parameter decoder/layers_10/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:11.077519 139676058949632 adafactor.py:358] Since rank of parameter decoder/layers_10/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:11.097670 140188559153152 adafactor.py:358] Since rank of parameter decoder/layers_10/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:11.100991 140392160929792 adafactor.py:358] Since rank of parameter decoder/layers_10/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:11.078438 139676058949632 adafactor.py:358] Since rank of parameter decoder/layers_10/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.098779 140188559153152 adafactor.py:358] Since rank of parameter decoder/layers_10/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:11.101873 140392160929792 adafactor.py:358] Since rank of parameter decoder/layers_10/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.099663 140188559153152 adafactor.py:358] Since rank of parameter decoder/layers_10/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:11.079599 139676058949632 adafactor.py:358] Since rank of parameter decoder/layers_10/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:11.103081 140392160929792 adafactor.py:358] Since rank of parameter decoder/layers_10/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:11.100568 140188559153152 adafactor.py:358] Since rank of parameter decoder/layers_10/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.080695 139676058949632 adafactor.py:358] Since rank of parameter decoder/layers_10/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.104198 140392160929792 adafactor.py:358] Since rank of parameter decoder/layers_10/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.101711 140188559153152 adafactor.py:358] Since rank of parameter decoder/layers_10/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:11.081822 139676058949632 adafactor.py:358] Since rank of parameter decoder/layers_10/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.105298 140392160929792 adafactor.py:358] Since rank of parameter decoder/layers_10/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.102827 140188559153152 adafactor.py:358] Since rank of parameter decoder/layers_10/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.082928 139676058949632 adafactor.py:358] Since rank of parameter decoder/layers_11/extra_mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
I0512 01:45:11.104273 140577131780096 moe_layers.py:777] Selected group_size=4096 and num_groups=192 for input num_tokens=786432, max_group_size=4096, num_experts=8 and num_expert_replicas=1
W0512 01:45:11.106436 140392160929792 adafactor.py:358] Since rank of parameter decoder/layers_11/extra_mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.103908 140188559153152 adafactor.py:358] Since rank of parameter decoder/layers_10/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.084089 139676058949632 adafactor.py:358] Since rank of parameter decoder/layers_11/extra_mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.107602 140392160929792 adafactor.py:358] Since rank of parameter decoder/layers_11/extra_mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.104994 140188559153152 adafactor.py:358] Since rank of parameter decoder/layers_11/extra_mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.085206 139676058949632 adafactor.py:358] Since rank of parameter decoder/layers_11/extra_mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:11.108708 140392160929792 adafactor.py:358] Since rank of parameter decoder/layers_11/extra_mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:11.106143 140188559153152 adafactor.py:358] Since rank of parameter decoder/layers_11/extra_mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.107262 140188559153152 adafactor.py:358] Since rank of parameter decoder/layers_11/extra_mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:11.089383 139676058949632 adafactor.py:358] Since rank of parameter decoder/layers_11/mlp/router/router_weights/w/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>, <FactorDim.NONE: None>) is ignored.
W0512 01:45:11.112893 140392160929792 adafactor.py:358] Since rank of parameter decoder/layers_11/mlp/router/router_weights/w/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>, <FactorDim.NONE: None>) is ignored.
W0512 01:45:11.090332 139676058949632 adafactor.py:358] Since rank of parameter decoder/layers_11/pre_extra_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:11.113791 140392160929792 adafactor.py:358] Since rank of parameter decoder/layers_11/pre_extra_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:11.111350 140188559153152 adafactor.py:358] Since rank of parameter decoder/layers_11/mlp/router/router_weights/w/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>, <FactorDim.NONE: None>) is ignored.
W0512 01:45:11.091231 139676058949632 adafactor.py:358] Since rank of parameter decoder/layers_11/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:11.114720 140392160929792 adafactor.py:358] Since rank of parameter decoder/layers_11/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:11.092137 139676058949632 adafactor.py:358] Since rank of parameter decoder/layers_11/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:11.112246 140188559153152 adafactor.py:358] Since rank of parameter decoder/layers_11/pre_extra_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:11.115617 140392160929792 adafactor.py:358] Since rank of parameter decoder/layers_11/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:11.093089 139676058949632 adafactor.py:358] Since rank of parameter decoder/layers_11/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.113125 140188559153152 adafactor.py:358] Since rank of parameter decoder/layers_11/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:11.116567 140392160929792 adafactor.py:358] Since rank of parameter decoder/layers_11/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.114023 140188559153152 adafactor.py:358] Since rank of parameter decoder/layers_11/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:11.094248 139676058949632 adafactor.py:358] Since rank of parameter decoder/layers_11/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:11.117666 140392160929792 adafactor.py:358] Since rank of parameter decoder/layers_11/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:11.115018 140188559153152 adafactor.py:358] Since rank of parameter decoder/layers_11/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.095359 139676058949632 adafactor.py:358] Since rank of parameter decoder/layers_11/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.118796 140392160929792 adafactor.py:358] Since rank of parameter decoder/layers_11/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.116100 140188559153152 adafactor.py:358] Since rank of parameter decoder/layers_11/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:11.096458 139676058949632 adafactor.py:358] Since rank of parameter decoder/layers_11/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.119898 140392160929792 adafactor.py:358] Since rank of parameter decoder/layers_11/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.117190 140188559153152 adafactor.py:358] Since rank of parameter decoder/layers_11/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.097682 139676058949632 adafactor.py:358] Since rank of parameter decoder/layers_12/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.121171 140392160929792 adafactor.py:358] Since rank of parameter decoder/layers_12/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.118300 140188559153152 adafactor.py:358] Since rank of parameter decoder/layers_11/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.098811 139676058949632 adafactor.py:358] Since rank of parameter decoder/layers_12/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.122291 140392160929792 adafactor.py:358] Since rank of parameter decoder/layers_12/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.119492 140188559153152 adafactor.py:358] Since rank of parameter decoder/layers_12/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.099923 139676058949632 adafactor.py:358] Since rank of parameter decoder/layers_12/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:11.123391 140392160929792 adafactor.py:358] Since rank of parameter decoder/layers_12/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:11.120575 140188559153152 adafactor.py:358] Since rank of parameter decoder/layers_12/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.101011 139676058949632 adafactor.py:358] Since rank of parameter decoder/layers_12/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:11.124511 140392160929792 adafactor.py:358] Since rank of parameter decoder/layers_12/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:11.121661 140188559153152 adafactor.py:358] Since rank of parameter decoder/layers_12/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:11.101994 139676058949632 adafactor.py:358] Since rank of parameter decoder/layers_12/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:11.125459 140392160929792 adafactor.py:358] Since rank of parameter decoder/layers_12/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:11.102898 139676058949632 adafactor.py:358] Since rank of parameter decoder/layers_12/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.122757 140188559153152 adafactor.py:358] Since rank of parameter decoder/layers_12/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:11.126401 140392160929792 adafactor.py:358] Since rank of parameter decoder/layers_12/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.123699 140188559153152 adafactor.py:358] Since rank of parameter decoder/layers_12/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:11.103980 139676058949632 adafactor.py:358] Since rank of parameter decoder/layers_12/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:11.124601 140188559153152 adafactor.py:358] Since rank of parameter decoder/layers_12/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.127498 140392160929792 adafactor.py:358] Since rank of parameter decoder/layers_12/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:11.105073 139676058949632 adafactor.py:358] Since rank of parameter decoder/layers_12/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.125773 140188559153152 adafactor.py:358] Since rank of parameter decoder/layers_12/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:11.128595 140392160929792 adafactor.py:358] Since rank of parameter decoder/layers_12/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.106916 139676058949632 adafactor.py:358] Since rank of parameter decoder/layers_12/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.127006 140188559153152 adafactor.py:358] Since rank of parameter decoder/layers_12/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.130463 140392160929792 adafactor.py:358] Since rank of parameter decoder/layers_12/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.108015 139676058949632 adafactor.py:358] Since rank of parameter decoder/layers_13/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.131560 140392160929792 adafactor.py:358] Since rank of parameter decoder/layers_13/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.128867 140188559153152 adafactor.py:358] Since rank of parameter decoder/layers_12/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.109106 139676058949632 adafactor.py:358] Since rank of parameter decoder/layers_13/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.132654 140392160929792 adafactor.py:358] Since rank of parameter decoder/layers_13/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.129952 140188559153152 adafactor.py:358] Since rank of parameter decoder/layers_13/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.110700 139676058949632 adafactor.py:358] Since rank of parameter decoder/layers_13/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:11.131043 140188559153152 adafactor.py:358] Since rank of parameter decoder/layers_13/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.134523 140392160929792 adafactor.py:358] Since rank of parameter decoder/layers_13/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:11.111854 139676058949632 adafactor.py:358] Since rank of parameter decoder/layers_13/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:11.112514 139812357228544 adafactor.py:358] Since rank of parameter decoder/decoder_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:11.132680 140188559153152 adafactor.py:358] Since rank of parameter decoder/layers_13/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:11.112756 139676058949632 adafactor.py:358] Since rank of parameter decoder/layers_13/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:11.135669 140392160929792 adafactor.py:358] Since rank of parameter decoder/layers_13/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:11.113549 139812357228544 adafactor.py:358] Since rank of parameter decoder/layers_0/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.113652 139676058949632 adafactor.py:358] Since rank of parameter decoder/layers_13/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.136571 140392160929792 adafactor.py:358] Since rank of parameter decoder/layers_13/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:11.133791 140188559153152 adafactor.py:358] Since rank of parameter decoder/layers_13/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:11.114729 139812357228544 adafactor.py:358] Since rank of parameter decoder/layers_0/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.137467 140392160929792 adafactor.py:358] Since rank of parameter decoder/layers_13/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.134728 140188559153152 adafactor.py:358] Since rank of parameter decoder/layers_13/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:11.114809 139676058949632 adafactor.py:358] Since rank of parameter decoder/layers_13/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:11.115884 139812357228544 adafactor.py:358] Since rank of parameter decoder/layers_0/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:11.135600 140188559153152 adafactor.py:358] Since rank of parameter decoder/layers_13/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.138589 140392160929792 adafactor.py:358] Since rank of parameter decoder/layers_13/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:11.115947 139676058949632 adafactor.py:358] Since rank of parameter decoder/layers_13/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.136670 140188559153152 adafactor.py:358] Since rank of parameter decoder/layers_13/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:11.117161 139812357228544 adafactor.py:358] Since rank of parameter decoder/layers_0/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:11.139738 140392160929792 adafactor.py:358] Since rank of parameter decoder/layers_13/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.118103 139812357228544 adafactor.py:358] Since rank of parameter decoder/layers_0/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:11.137777 140188559153152 adafactor.py:358] Since rank of parameter decoder/layers_13/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.140863 140392160929792 adafactor.py:358] Since rank of parameter decoder/layers_13/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.119013 139812357228544 adafactor.py:358] Since rank of parameter decoder/layers_0/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.138888 140188559153152 adafactor.py:358] Since rank of parameter decoder/layers_13/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.141961 140392160929792 adafactor.py:358] Since rank of parameter decoder/layers_14/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.120127 139812357228544 adafactor.py:358] Since rank of parameter decoder/layers_0/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:11.139957 140188559153152 adafactor.py:358] Since rank of parameter decoder/layers_14/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.143076 140392160929792 adafactor.py:358] Since rank of parameter decoder/layers_14/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.121247 139812357228544 adafactor.py:358] Since rank of parameter decoder/layers_0/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.141022 140188559153152 adafactor.py:358] Since rank of parameter decoder/layers_14/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.144255 140392160929792 adafactor.py:358] Since rank of parameter decoder/layers_14/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:11.122407 139812357228544 adafactor.py:358] Since rank of parameter decoder/layers_0/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.142150 140188559153152 adafactor.py:358] Since rank of parameter decoder/layers_14/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:11.145352 140392160929792 adafactor.py:358] Since rank of parameter decoder/layers_14/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:11.123523 139812357228544 adafactor.py:358] Since rank of parameter decoder/layers_1/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.146288 140392160929792 adafactor.py:358] Since rank of parameter decoder/layers_14/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:11.143232 140188559153152 adafactor.py:358] Since rank of parameter decoder/layers_14/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:11.144114 140188559153152 adafactor.py:358] Since rank of parameter decoder/layers_14/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:11.124645 139812357228544 adafactor.py:358] Since rank of parameter decoder/layers_1/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.144990 140188559153152 adafactor.py:358] Since rank of parameter decoder/layers_14/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.125990 139812357228544 adafactor.py:358] Since rank of parameter decoder/layers_1/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:11.127151 139812357228544 adafactor.py:358] Since rank of parameter decoder/layers_1/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:11.128043 139812357228544 adafactor.py:358] Since rank of parameter decoder/layers_1/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:11.128968 139812357228544 adafactor.py:358] Since rank of parameter decoder/layers_1/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.130066 139812357228544 adafactor.py:358] Since rank of parameter decoder/layers_1/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:11.131221 139812357228544 adafactor.py:358] Since rank of parameter decoder/layers_1/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.132333 139812357228544 adafactor.py:358] Since rank of parameter decoder/layers_1/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.133416 139812357228544 adafactor.py:358] Since rank of parameter decoder/layers_10/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.134522 139812357228544 adafactor.py:358] Since rank of parameter decoder/layers_10/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.135690 139812357228544 adafactor.py:358] Since rank of parameter decoder/layers_10/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:11.136823 139812357228544 adafactor.py:358] Since rank of parameter decoder/layers_10/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:11.137698 139812357228544 adafactor.py:358] Since rank of parameter decoder/layers_10/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:11.138582 139812357228544 adafactor.py:358] Since rank of parameter decoder/layers_10/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.159213 139997397456896 adafactor.py:358] Since rank of parameter decoder/decoder_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:11.139736 139812357228544 adafactor.py:358] Since rank of parameter decoder/layers_10/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:11.160206 139997397456896 adafactor.py:358] Since rank of parameter decoder/layers_0/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.117048 139676058949632 adafactor.py:358] Since rank of parameter decoder/layers_13/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.118169 139676058949632 adafactor.py:358] Since rank of parameter decoder/layers_14/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.119261 139676058949632 adafactor.py:358] Since rank of parameter decoder/layers_14/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.120401 139676058949632 adafactor.py:358] Since rank of parameter decoder/layers_14/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:11.121481 139676058949632 adafactor.py:358] Since rank of parameter decoder/layers_14/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:11.122404 139676058949632 adafactor.py:358] Since rank of parameter decoder/layers_14/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:11.123297 139676058949632 adafactor.py:358] Since rank of parameter decoder/layers_14/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.124388 139676058949632 adafactor.py:358] Since rank of parameter decoder/layers_14/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:11.140864 139812357228544 adafactor.py:358] Since rank of parameter decoder/layers_10/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.161387 139997397456896 adafactor.py:358] Since rank of parameter decoder/layers_0/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.141967 139812357228544 adafactor.py:358] Since rank of parameter decoder/layers_10/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.162514 139997397456896 adafactor.py:358] Since rank of parameter decoder/layers_0/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:11.143060 139812357228544 adafactor.py:358] Since rank of parameter decoder/layers_11/extra_mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.163777 139997397456896 adafactor.py:358] Since rank of parameter decoder/layers_0/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:11.164695 139997397456896 adafactor.py:358] Since rank of parameter decoder/layers_0/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:11.144248 139812357228544 adafactor.py:358] Since rank of parameter decoder/layers_11/extra_mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.165610 139997397456896 adafactor.py:358] Since rank of parameter decoder/layers_0/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.145355 139812357228544 adafactor.py:358] Since rank of parameter decoder/layers_11/extra_mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:11.166702 139997397456896 adafactor.py:358] Since rank of parameter decoder/layers_0/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:11.167775 139997397456896 adafactor.py:358] Since rank of parameter decoder/layers_0/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.168928 139997397456896 adafactor.py:358] Since rank of parameter decoder/layers_0/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.149515 139812357228544 adafactor.py:358] Since rank of parameter decoder/layers_11/mlp/router/router_weights/w/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>, <FactorDim.NONE: None>) is ignored.
W0512 01:45:11.170037 139997397456896 adafactor.py:358] Since rank of parameter decoder/layers_1/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.172054 140172769531904 adafactor.py:358] Since rank of parameter decoder/decoder_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:11.150427 139812357228544 adafactor.py:358] Since rank of parameter decoder/layers_11/pre_extra_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:11.171113 139997397456896 adafactor.py:358] Since rank of parameter decoder/layers_1/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.173133 140172769531904 adafactor.py:358] Since rank of parameter decoder/layers_0/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.151318 139812357228544 adafactor.py:358] Since rank of parameter decoder/layers_11/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:11.172184 139997397456896 adafactor.py:358] Since rank of parameter decoder/layers_1/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:11.174345 140172769531904 adafactor.py:358] Since rank of parameter decoder/layers_0/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.152241 139812357228544 adafactor.py:358] Since rank of parameter decoder/layers_11/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:11.173528 139997397456896 adafactor.py:358] Since rank of parameter decoder/layers_1/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:11.153206 139812357228544 adafactor.py:358] Since rank of parameter decoder/layers_11/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.175481 140172769531904 adafactor.py:358] Since rank of parameter decoder/layers_0/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:11.174410 139997397456896 adafactor.py:358] Since rank of parameter decoder/layers_1/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:11.154291 139812357228544 adafactor.py:358] Since rank of parameter decoder/layers_11/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:11.176740 140172769531904 adafactor.py:358] Since rank of parameter decoder/layers_0/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:11.175269 139997397456896 adafactor.py:358] Since rank of parameter decoder/layers_1/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.177689 140172769531904 adafactor.py:358] Since rank of parameter decoder/layers_0/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:11.155391 139812357228544 adafactor.py:358] Since rank of parameter decoder/layers_11/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.176348 139997397456896 adafactor.py:358] Since rank of parameter decoder/layers_1/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:11.178651 140172769531904 adafactor.py:358] Since rank of parameter decoder/layers_0/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.156519 139812357228544 adafactor.py:358] Since rank of parameter decoder/layers_11/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.177514 139997397456896 adafactor.py:358] Since rank of parameter decoder/layers_1/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.179759 140172769531904 adafactor.py:358] Since rank of parameter decoder/layers_0/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:11.157733 139812357228544 adafactor.py:358] Since rank of parameter decoder/layers_12/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.178590 139997397456896 adafactor.py:358] Since rank of parameter decoder/layers_1/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.180897 140172769531904 adafactor.py:358] Since rank of parameter decoder/layers_0/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.158829 139812357228544 adafactor.py:358] Since rank of parameter decoder/layers_12/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.179650 139997397456896 adafactor.py:358] Since rank of parameter decoder/layers_10/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.182056 140172769531904 adafactor.py:358] Since rank of parameter decoder/layers_0/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.159924 139812357228544 adafactor.py:358] Since rank of parameter decoder/layers_12/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:11.180717 139997397456896 adafactor.py:358] Since rank of parameter decoder/layers_10/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.183163 140172769531904 adafactor.py:358] Since rank of parameter decoder/layers_1/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.161034 139812357228544 adafactor.py:358] Since rank of parameter decoder/layers_12/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:11.181900 139997397456896 adafactor.py:358] Since rank of parameter decoder/layers_10/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:11.161981 139812357228544 adafactor.py:358] Since rank of parameter decoder/layers_12/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:11.184253 140172769531904 adafactor.py:358] Since rank of parameter decoder/layers_1/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.182971 139997397456896 adafactor.py:358] Since rank of parameter decoder/layers_10/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:11.162885 139812357228544 adafactor.py:358] Since rank of parameter decoder/layers_12/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.185369 140172769531904 adafactor.py:358] Since rank of parameter decoder/layers_1/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:11.183880 139997397456896 adafactor.py:358] Since rank of parameter decoder/layers_10/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:11.163996 139812357228544 adafactor.py:358] Since rank of parameter decoder/layers_12/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:11.186540 140172769531904 adafactor.py:358] Since rank of parameter decoder/layers_1/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:11.184750 139997397456896 adafactor.py:358] Since rank of parameter decoder/layers_10/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.165110 139812357228544 adafactor.py:358] Since rank of parameter decoder/layers_12/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.187435 140172769531904 adafactor.py:358] Since rank of parameter decoder/layers_1/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:11.185917 139997397456896 adafactor.py:358] Since rank of parameter decoder/layers_10/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:11.188322 140172769531904 adafactor.py:358] Since rank of parameter decoder/layers_1/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.186995 139997397456896 adafactor.py:358] Since rank of parameter decoder/layers_10/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.167021 139812357228544 adafactor.py:358] Since rank of parameter decoder/layers_12/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.189449 140172769531904 adafactor.py:358] Since rank of parameter decoder/layers_1/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:11.188070 139997397456896 adafactor.py:358] Since rank of parameter decoder/layers_10/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.168138 139812357228544 adafactor.py:358] Since rank of parameter decoder/layers_13/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.190599 140172769531904 adafactor.py:358] Since rank of parameter decoder/layers_1/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.189179 139997397456896 adafactor.py:358] Since rank of parameter decoder/layers_11/extra_mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.169273 139812357228544 adafactor.py:358] Since rank of parameter decoder/layers_13/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.191679 140172769531904 adafactor.py:358] Since rank of parameter decoder/layers_1/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.190322 139997397456896 adafactor.py:358] Since rank of parameter decoder/layers_11/extra_mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.192801 140172769531904 adafactor.py:358] Since rank of parameter decoder/layers_10/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.170934 139812357228544 adafactor.py:358] Since rank of parameter decoder/layers_13/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:11.191411 139997397456896 adafactor.py:358] Since rank of parameter decoder/layers_11/extra_mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:11.193896 140172769531904 adafactor.py:358] Since rank of parameter decoder/layers_10/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.172106 139812357228544 adafactor.py:358] Since rank of parameter decoder/layers_13/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:11.195059 140172769531904 adafactor.py:358] Since rank of parameter decoder/layers_10/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:11.196156 140172769531904 adafactor.py:358] Since rank of parameter decoder/layers_10/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:11.197071 140172769531904 adafactor.py:358] Since rank of parameter decoder/layers_10/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:11.195509 139997397456896 adafactor.py:358] Since rank of parameter decoder/layers_11/mlp/router/router_weights/w/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>, <FactorDim.NONE: None>) is ignored.
W0512 01:45:11.197952 140172769531904 adafactor.py:358] Since rank of parameter decoder/layers_10/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.196411 139997397456896 adafactor.py:358] Since rank of parameter decoder/layers_11/pre_extra_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:11.199125 140172769531904 adafactor.py:358] Since rank of parameter decoder/layers_10/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:11.197320 139997397456896 adafactor.py:358] Since rank of parameter decoder/layers_11/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:11.198196 139997397456896 adafactor.py:358] Since rank of parameter decoder/layers_11/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:11.200253 140172769531904 adafactor.py:358] Since rank of parameter decoder/layers_10/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.199125 139997397456896 adafactor.py:358] Since rank of parameter decoder/layers_11/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.201410 140172769531904 adafactor.py:358] Since rank of parameter decoder/layers_10/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.200212 139997397456896 adafactor.py:358] Since rank of parameter decoder/layers_11/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:11.202548 140172769531904 adafactor.py:358] Since rank of parameter decoder/layers_11/extra_mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.201332 139997397456896 adafactor.py:358] Since rank of parameter decoder/layers_11/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.203741 140172769531904 adafactor.py:358] Since rank of parameter decoder/layers_11/extra_mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.202413 139997397456896 adafactor.py:358] Since rank of parameter decoder/layers_11/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.204910 140172769531904 adafactor.py:358] Since rank of parameter decoder/layers_11/extra_mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:11.203635 139997397456896 adafactor.py:358] Since rank of parameter decoder/layers_12/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.204717 139997397456896 adafactor.py:358] Since rank of parameter decoder/layers_12/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.205850 139997397456896 adafactor.py:358] Since rank of parameter decoder/layers_12/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:11.206916 139997397456896 adafactor.py:358] Since rank of parameter decoder/layers_12/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:11.209117 140172769531904 adafactor.py:358] Since rank of parameter decoder/layers_11/mlp/router/router_weights/w/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>, <FactorDim.NONE: None>) is ignored.
W0512 01:45:11.207846 139997397456896 adafactor.py:358] Since rank of parameter decoder/layers_12/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:11.207861 140577131780096 adafactor.py:358] Since rank of parameter decoder/decoder_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:11.210017 140172769531904 adafactor.py:358] Since rank of parameter decoder/layers_11/pre_extra_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:11.208730 139997397456896 adafactor.py:358] Since rank of parameter decoder/layers_12/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.208886 140577131780096 adafactor.py:358] Since rank of parameter decoder/layers_0/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.210894 140172769531904 adafactor.py:358] Since rank of parameter decoder/layers_11/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:11.209836 139997397456896 adafactor.py:358] Since rank of parameter decoder/layers_12/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:11.211785 140172769531904 adafactor.py:358] Since rank of parameter decoder/layers_11/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:11.210050 140577131780096 adafactor.py:358] Since rank of parameter decoder/layers_0/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.212758 140172769531904 adafactor.py:358] Since rank of parameter decoder/layers_11/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.210899 139997397456896 adafactor.py:358] Since rank of parameter decoder/layers_12/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.211161 140577131780096 adafactor.py:358] Since rank of parameter decoder/layers_0/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:11.213835 140172769531904 adafactor.py:358] Since rank of parameter decoder/layers_11/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:11.212388 140577131780096 adafactor.py:358] Since rank of parameter decoder/layers_0/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:11.212755 139997397456896 adafactor.py:358] Since rank of parameter decoder/layers_12/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.213297 140577131780096 adafactor.py:358] Since rank of parameter decoder/layers_0/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:11.214910 140172769531904 adafactor.py:358] Since rank of parameter decoder/layers_11/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.213863 139997397456896 adafactor.py:358] Since rank of parameter decoder/layers_13/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.214205 140577131780096 adafactor.py:358] Since rank of parameter decoder/layers_0/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.215984 140172769531904 adafactor.py:358] Since rank of parameter decoder/layers_11/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.214934 139997397456896 adafactor.py:358] Since rank of parameter decoder/layers_13/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.215338 140577131780096 adafactor.py:358] Since rank of parameter decoder/layers_0/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:11.217202 140172769531904 adafactor.py:358] Since rank of parameter decoder/layers_12/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.218282 140172769531904 adafactor.py:358] Since rank of parameter decoder/layers_12/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.216455 140577131780096 adafactor.py:358] Since rank of parameter decoder/layers_0/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.216656 139997397456896 adafactor.py:358] Since rank of parameter decoder/layers_13/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:11.219390 140172769531904 adafactor.py:358] Since rank of parameter decoder/layers_12/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:11.217628 140577131780096 adafactor.py:358] Since rank of parameter decoder/layers_0/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.217843 139997397456896 adafactor.py:358] Since rank of parameter decoder/layers_13/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:11.220496 140172769531904 adafactor.py:358] Since rank of parameter decoder/layers_12/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:11.218746 139997397456896 adafactor.py:358] Since rank of parameter decoder/layers_13/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:11.218747 140577131780096 adafactor.py:358] Since rank of parameter decoder/layers_1/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.173051 139812357228544 adafactor.py:358] Since rank of parameter decoder/layers_13/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:11.173976 139812357228544 adafactor.py:358] Since rank of parameter decoder/layers_13/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.175108 139812357228544 adafactor.py:358] Since rank of parameter decoder/layers_13/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:11.176307 139812357228544 adafactor.py:358] Since rank of parameter decoder/layers_13/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.177423 139812357228544 adafactor.py:358] Since rank of parameter decoder/layers_13/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.178524 139812357228544 adafactor.py:358] Since rank of parameter decoder/layers_14/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.179612 139812357228544 adafactor.py:358] Since rank of parameter decoder/layers_14/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.180798 139812357228544 adafactor.py:358] Since rank of parameter decoder/layers_14/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:11.221451 140172769531904 adafactor.py:358] Since rank of parameter decoder/layers_12/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:11.219625 139997397456896 adafactor.py:358] Since rank of parameter decoder/layers_13/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.219882 140577131780096 adafactor.py:358] Since rank of parameter decoder/layers_1/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.222345 140172769531904 adafactor.py:358] Since rank of parameter decoder/layers_12/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.220706 139997397456896 adafactor.py:358] Since rank of parameter decoder/layers_13/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:11.220985 140577131780096 adafactor.py:358] Since rank of parameter decoder/layers_1/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:11.223419 140172769531904 adafactor.py:358] Since rank of parameter decoder/layers_12/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:11.221866 139997397456896 adafactor.py:358] Since rank of parameter decoder/layers_13/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.222155 140577131780096 adafactor.py:358] Since rank of parameter decoder/layers_1/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:11.224522 140172769531904 adafactor.py:358] Since rank of parameter decoder/layers_12/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.223081 140577131780096 adafactor.py:358] Since rank of parameter decoder/layers_1/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:11.222937 139997397456896 adafactor.py:358] Since rank of parameter decoder/layers_13/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.224023 140577131780096 adafactor.py:358] Since rank of parameter decoder/layers_1/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.224030 139997397456896 adafactor.py:358] Since rank of parameter decoder/layers_14/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.226332 140172769531904 adafactor.py:358] Since rank of parameter decoder/layers_12/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.225134 140577131780096 adafactor.py:358] Since rank of parameter decoder/layers_1/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:11.225121 139997397456896 adafactor.py:358] Since rank of parameter decoder/layers_14/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.227405 140172769531904 adafactor.py:358] Since rank of parameter decoder/layers_13/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.226307 140577131780096 adafactor.py:358] Since rank of parameter decoder/layers_1/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.226244 139997397456896 adafactor.py:358] Since rank of parameter decoder/layers_14/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:11.228517 140172769531904 adafactor.py:358] Since rank of parameter decoder/layers_13/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.227433 140577131780096 adafactor.py:358] Since rank of parameter decoder/layers_1/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.227315 139997397456896 adafactor.py:358] Since rank of parameter decoder/layers_14/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:11.230077 140172769531904 adafactor.py:358] Since rank of parameter decoder/layers_13/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:11.228179 139997397456896 adafactor.py:358] Since rank of parameter decoder/layers_14/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:11.228533 140577131780096 adafactor.py:358] Since rank of parameter decoder/layers_10/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.229080 139997397456896 adafactor.py:358] Since rank of parameter decoder/layers_14/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.231251 140172769531904 adafactor.py:358] Since rank of parameter decoder/layers_13/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:11.229654 140577131780096 adafactor.py:358] Since rank of parameter decoder/layers_10/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.232161 140172769531904 adafactor.py:358] Since rank of parameter decoder/layers_13/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:11.230157 139997397456896 adafactor.py:358] Since rank of parameter decoder/layers_14/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:11.230844 140577131780096 adafactor.py:358] Since rank of parameter decoder/layers_10/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:11.233108 140172769531904 adafactor.py:358] Since rank of parameter decoder/layers_13/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.231983 140577131780096 adafactor.py:358] Since rank of parameter decoder/layers_10/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:11.234240 140172769531904 adafactor.py:358] Since rank of parameter decoder/layers_13/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:11.232873 140577131780096 adafactor.py:358] Since rank of parameter decoder/layers_10/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:11.235409 140172769531904 adafactor.py:358] Since rank of parameter decoder/layers_13/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.233759 140577131780096 adafactor.py:358] Since rank of parameter decoder/layers_10/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.236592 140172769531904 adafactor.py:358] Since rank of parameter decoder/layers_13/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.234933 140577131780096 adafactor.py:358] Since rank of parameter decoder/layers_10/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:11.237667 140172769531904 adafactor.py:358] Since rank of parameter decoder/layers_14/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.236067 140577131780096 adafactor.py:358] Since rank of parameter decoder/layers_10/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.238777 140172769531904 adafactor.py:358] Since rank of parameter decoder/layers_14/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.237185 140577131780096 adafactor.py:358] Since rank of parameter decoder/layers_10/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.239917 140172769531904 adafactor.py:358] Since rank of parameter decoder/layers_14/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:11.238275 140577131780096 adafactor.py:358] Since rank of parameter decoder/layers_11/extra_mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.147197 140392160929792 adafactor.py:358] Since rank of parameter decoder/layers_14/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.148280 140392160929792 adafactor.py:358] Since rank of parameter decoder/layers_14/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:11.149422 140392160929792 adafactor.py:358] Since rank of parameter decoder/layers_14/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.150559 140392160929792 adafactor.py:358] Since rank of parameter decoder/layers_14/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.151650 140392160929792 adafactor.py:358] Since rank of parameter decoder/layers_15/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.152746 140392160929792 adafactor.py:358] Since rank of parameter decoder/layers_15/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.153894 140392160929792 adafactor.py:358] Since rank of parameter decoder/layers_15/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:11.155037 140392160929792 adafactor.py:358] Since rank of parameter decoder/layers_15/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:11.155934 140392160929792 adafactor.py:358] Since rank of parameter decoder/layers_15/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:11.156838 140392160929792 adafactor.py:358] Since rank of parameter decoder/layers_15/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.158009 140392160929792 adafactor.py:358] Since rank of parameter decoder/layers_15/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:11.159138 140392160929792 adafactor.py:358] Since rank of parameter decoder/layers_15/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.160231 140392160929792 adafactor.py:358] Since rank of parameter decoder/layers_15/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.161316 140392160929792 adafactor.py:358] Since rank of parameter decoder/layers_16/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.162504 140392160929792 adafactor.py:358] Since rank of parameter decoder/layers_16/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.163609 140392160929792 adafactor.py:358] Since rank of parameter decoder/layers_16/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:11.164738 140392160929792 adafactor.py:358] Since rank of parameter decoder/layers_16/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:11.241024 140172769531904 adafactor.py:358] Since rank of parameter decoder/layers_14/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:11.165625 140392160929792 adafactor.py:358] Since rank of parameter decoder/layers_16/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:11.166622 140392160929792 adafactor.py:358] Since rank of parameter decoder/layers_16/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.167726 140392160929792 adafactor.py:358] Since rank of parameter decoder/layers_16/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:11.168825 140392160929792 adafactor.py:358] Since rank of parameter decoder/layers_16/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.169935 140392160929792 adafactor.py:358] Since rank of parameter decoder/layers_16/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.171107 140392160929792 adafactor.py:358] Since rank of parameter decoder/layers_17/extra_mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.172191 140392160929792 adafactor.py:358] Since rank of parameter decoder/layers_17/extra_mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.173312 140392160929792 adafactor.py:358] Since rank of parameter decoder/layers_17/extra_mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:11.239475 140577131780096 adafactor.py:358] Since rank of parameter decoder/layers_11/extra_mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.241910 140172769531904 adafactor.py:358] Since rank of parameter decoder/layers_14/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:11.177449 140392160929792 adafactor.py:358] Since rank of parameter decoder/layers_17/mlp/router/router_weights/w/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>, <FactorDim.NONE: None>) is ignored.
W0512 01:45:11.178392 140392160929792 adafactor.py:358] Since rank of parameter decoder/layers_17/pre_extra_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:11.179341 140392160929792 adafactor.py:358] Since rank of parameter decoder/layers_17/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:11.180243 140392160929792 adafactor.py:358] Since rank of parameter decoder/layers_17/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:11.181139 140392160929792 adafactor.py:358] Since rank of parameter decoder/layers_17/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.182250 140392160929792 adafactor.py:358] Since rank of parameter decoder/layers_17/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:11.183350 140392160929792 adafactor.py:358] Since rank of parameter decoder/layers_17/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.184978 140392160929792 adafactor.py:358] Since rank of parameter decoder/layers_17/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.186096 140392160929792 adafactor.py:358] Since rank of parameter decoder/layers_18/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.240578 140577131780096 adafactor.py:358] Since rank of parameter decoder/layers_11/extra_mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:11.242801 140172769531904 adafactor.py:358] Since rank of parameter decoder/layers_14/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.187214 140392160929792 adafactor.py:358] Since rank of parameter decoder/layers_18/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.188360 140392160929792 adafactor.py:358] Since rank of parameter decoder/layers_18/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:11.189453 140392160929792 adafactor.py:358] Since rank of parameter decoder/layers_18/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:11.190386 140392160929792 adafactor.py:358] Since rank of parameter decoder/layers_18/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:11.191291 140392160929792 adafactor.py:358] Since rank of parameter decoder/layers_18/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.192399 140392160929792 adafactor.py:358] Since rank of parameter decoder/layers_18/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:11.193568 140392160929792 adafactor.py:358] Since rank of parameter decoder/layers_18/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.194707 140392160929792 adafactor.py:358] Since rank of parameter decoder/layers_18/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.195803 140392160929792 adafactor.py:358] Since rank of parameter decoder/layers_19/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.243876 140172769531904 adafactor.py:358] Since rank of parameter decoder/layers_14/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:11.196916 140392160929792 adafactor.py:358] Since rank of parameter decoder/layers_19/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.198063 140392160929792 adafactor.py:358] Since rank of parameter decoder/layers_19/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:11.199188 140392160929792 adafactor.py:358] Since rank of parameter decoder/layers_19/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:11.200092 140392160929792 adafactor.py:358] Since rank of parameter decoder/layers_19/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:11.200991 140392160929792 adafactor.py:358] Since rank of parameter decoder/layers_19/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.202160 140392160929792 adafactor.py:358] Since rank of parameter decoder/layers_19/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:11.203263 140392160929792 adafactor.py:358] Since rank of parameter decoder/layers_19/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.204384 140392160929792 adafactor.py:358] Since rank of parameter decoder/layers_19/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.146056 140188559153152 adafactor.py:358] Since rank of parameter decoder/layers_14/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:11.147206 140188559153152 adafactor.py:358] Since rank of parameter decoder/layers_14/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.148288 140188559153152 adafactor.py:358] Since rank of parameter decoder/layers_14/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.149370 140188559153152 adafactor.py:358] Since rank of parameter decoder/layers_15/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.150462 140188559153152 adafactor.py:358] Since rank of parameter decoder/layers_15/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.151587 140188559153152 adafactor.py:358] Since rank of parameter decoder/layers_15/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:11.152654 140188559153152 adafactor.py:358] Since rank of parameter decoder/layers_15/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:11.153533 140188559153152 adafactor.py:358] Since rank of parameter decoder/layers_15/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:11.205473 140392160929792 adafactor.py:358] Since rank of parameter decoder/layers_2/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.206651 140392160929792 adafactor.py:358] Since rank of parameter decoder/layers_2/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.207739 140392160929792 adafactor.py:358] Since rank of parameter decoder/layers_2/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:11.208821 140392160929792 adafactor.py:358] Since rank of parameter decoder/layers_2/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:11.209718 140392160929792 adafactor.py:358] Since rank of parameter decoder/layers_2/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:11.210694 140392160929792 adafactor.py:358] Since rank of parameter decoder/layers_2/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.211786 140392160929792 adafactor.py:358] Since rank of parameter decoder/layers_2/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:11.212891 140392160929792 adafactor.py:358] Since rank of parameter decoder/layers_2/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.213986 140392160929792 adafactor.py:358] Since rank of parameter decoder/layers_2/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.154470 140188559153152 adafactor.py:358] Since rank of parameter decoder/layers_15/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.155603 140188559153152 adafactor.py:358] Since rank of parameter decoder/layers_15/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:11.156671 140188559153152 adafactor.py:358] Since rank of parameter decoder/layers_15/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.157748 140188559153152 adafactor.py:358] Since rank of parameter decoder/layers_15/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.158847 140188559153152 adafactor.py:358] Since rank of parameter decoder/layers_16/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.159972 140188559153152 adafactor.py:358] Since rank of parameter decoder/layers_16/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.161040 140188559153152 adafactor.py:358] Since rank of parameter decoder/layers_16/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:11.162107 140188559153152 adafactor.py:358] Since rank of parameter decoder/layers_16/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:11.163010 140188559153152 adafactor.py:358] Since rank of parameter decoder/layers_16/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:11.215171 140392160929792 adafactor.py:358] Since rank of parameter decoder/layers_20/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.216268 140392160929792 adafactor.py:358] Since rank of parameter decoder/layers_20/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.217368 140392160929792 adafactor.py:358] Since rank of parameter decoder/layers_20/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:11.218495 140392160929792 adafactor.py:358] Since rank of parameter decoder/layers_20/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:11.219464 140392160929792 adafactor.py:358] Since rank of parameter decoder/layers_20/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:11.220373 140392160929792 adafactor.py:358] Since rank of parameter decoder/layers_20/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.221454 140392160929792 adafactor.py:358] Since rank of parameter decoder/layers_20/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:11.222575 140392160929792 adafactor.py:358] Since rank of parameter decoder/layers_20/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.223722 140392160929792 adafactor.py:358] Since rank of parameter decoder/layers_20/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.244796 140577131780096 adafactor.py:358] Since rank of parameter decoder/layers_11/mlp/router/router_weights/w/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>, <FactorDim.NONE: None>) is ignored.
W0512 01:45:11.163943 140188559153152 adafactor.py:358] Since rank of parameter decoder/layers_16/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.165027 140188559153152 adafactor.py:358] Since rank of parameter decoder/layers_16/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:11.166082 140188559153152 adafactor.py:358] Since rank of parameter decoder/layers_16/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.167196 140188559153152 adafactor.py:358] Since rank of parameter decoder/layers_16/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.168323 140188559153152 adafactor.py:358] Since rank of parameter decoder/layers_17/extra_mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.169409 140188559153152 adafactor.py:358] Since rank of parameter decoder/layers_17/extra_mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.170526 140188559153152 adafactor.py:358] Since rank of parameter decoder/layers_17/extra_mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:11.174602 140188559153152 adafactor.py:358] Since rank of parameter decoder/layers_17/mlp/router/router_weights/w/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>, <FactorDim.NONE: None>) is ignored.
W0512 01:45:11.224834 140392160929792 adafactor.py:358] Since rank of parameter decoder/layers_21/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.225958 140392160929792 adafactor.py:358] Since rank of parameter decoder/layers_21/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.227087 140392160929792 adafactor.py:358] Since rank of parameter decoder/layers_21/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:11.228243 140392160929792 adafactor.py:358] Since rank of parameter decoder/layers_21/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:11.229158 140392160929792 adafactor.py:358] Since rank of parameter decoder/layers_21/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:11.230060 140392160929792 adafactor.py:358] Since rank of parameter decoder/layers_21/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.231192 140392160929792 adafactor.py:358] Since rank of parameter decoder/layers_21/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:11.232294 140392160929792 adafactor.py:358] Since rank of parameter decoder/layers_21/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.245707 140577131780096 adafactor.py:358] Since rank of parameter decoder/layers_11/pre_extra_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:11.175494 140188559153152 adafactor.py:358] Since rank of parameter decoder/layers_17/pre_extra_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:11.176428 140188559153152 adafactor.py:358] Since rank of parameter decoder/layers_17/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:11.177304 140188559153152 adafactor.py:358] Since rank of parameter decoder/layers_17/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:11.178168 140188559153152 adafactor.py:358] Since rank of parameter decoder/layers_17/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.179275 140188559153152 adafactor.py:358] Since rank of parameter decoder/layers_17/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:11.180347 140188559153152 adafactor.py:358] Since rank of parameter decoder/layers_17/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.181886 140188559153152 adafactor.py:358] Since rank of parameter decoder/layers_17/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.182989 140188559153152 adafactor.py:358] Since rank of parameter decoder/layers_18/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.184051 140188559153152 adafactor.py:358] Since rank of parameter decoder/layers_18/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.233453 140392160929792 adafactor.py:358] Since rank of parameter decoder/layers_21/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.234579 140392160929792 adafactor.py:358] Since rank of parameter decoder/layers_22/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.235681 140392160929792 adafactor.py:358] Since rank of parameter decoder/layers_22/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.236777 140392160929792 adafactor.py:358] Since rank of parameter decoder/layers_22/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:11.238390 140392160929792 adafactor.py:358] Since rank of parameter decoder/layers_22/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:11.239308 140392160929792 adafactor.py:358] Since rank of parameter decoder/layers_22/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:11.240199 140392160929792 adafactor.py:358] Since rank of parameter decoder/layers_22/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.241287 140392160929792 adafactor.py:358] Since rank of parameter decoder/layers_22/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:11.242460 140392160929792 adafactor.py:358] Since rank of parameter decoder/layers_22/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.246585 140577131780096 adafactor.py:358] Since rank of parameter decoder/layers_11/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:11.185174 140188559153152 adafactor.py:358] Since rank of parameter decoder/layers_18/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:11.186236 140188559153152 adafactor.py:358] Since rank of parameter decoder/layers_18/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:11.187125 140188559153152 adafactor.py:358] Since rank of parameter decoder/layers_18/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:11.188006 140188559153152 adafactor.py:358] Since rank of parameter decoder/layers_18/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.189056 140188559153152 adafactor.py:358] Since rank of parameter decoder/layers_18/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:11.190188 140188559153152 adafactor.py:358] Since rank of parameter decoder/layers_18/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.191265 140188559153152 adafactor.py:358] Since rank of parameter decoder/layers_18/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.192329 140188559153152 adafactor.py:358] Since rank of parameter decoder/layers_19/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.193402 140188559153152 adafactor.py:358] Since rank of parameter decoder/layers_19/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.247516 140577131780096 adafactor.py:358] Since rank of parameter decoder/layers_11/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:11.243535 140392160929792 adafactor.py:358] Since rank of parameter decoder/layers_22/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.244648 140392160929792 adafactor.py:358] Since rank of parameter decoder/layers_23/extra_mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.245749 140392160929792 adafactor.py:358] Since rank of parameter decoder/layers_23/extra_mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.246937 140392160929792 adafactor.py:358] Since rank of parameter decoder/layers_23/extra_mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:11.194580 140188559153152 adafactor.py:358] Since rank of parameter decoder/layers_19/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:11.195661 140188559153152 adafactor.py:358] Since rank of parameter decoder/layers_19/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:11.196527 140188559153152 adafactor.py:358] Since rank of parameter decoder/layers_19/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:11.197400 140188559153152 adafactor.py:358] Since rank of parameter decoder/layers_19/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.198554 140188559153152 adafactor.py:358] Since rank of parameter decoder/layers_19/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:11.199630 140188559153152 adafactor.py:358] Since rank of parameter decoder/layers_19/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.200699 140188559153152 adafactor.py:358] Since rank of parameter decoder/layers_19/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.201773 140188559153152 adafactor.py:358] Since rank of parameter decoder/layers_2/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.248467 140577131780096 adafactor.py:358] Since rank of parameter decoder/layers_11/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.245273 140316961150976 adafactor.py:358] Since rank of parameter decoder/layers_10/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:11.251101 140392160929792 adafactor.py:358] Since rank of parameter decoder/layers_23/mlp/router/router_weights/w/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>, <FactorDim.NONE: None>) is ignored.
W0512 01:45:11.202925 140188559153152 adafactor.py:358] Since rank of parameter decoder/layers_2/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.203999 140188559153152 adafactor.py:358] Since rank of parameter decoder/layers_2/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:11.205057 140188559153152 adafactor.py:358] Since rank of parameter decoder/layers_2/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:11.205933 140188559153152 adafactor.py:358] Since rank of parameter decoder/layers_2/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:11.206875 140188559153152 adafactor.py:358] Since rank of parameter decoder/layers_2/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.207939 140188559153152 adafactor.py:358] Since rank of parameter decoder/layers_2/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:11.209012 140188559153152 adafactor.py:358] Since rank of parameter decoder/layers_2/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.210061 140188559153152 adafactor.py:358] Since rank of parameter decoder/layers_2/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.211215 140188559153152 adafactor.py:358] Since rank of parameter decoder/layers_20/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.249557 140577131780096 adafactor.py:358] Since rank of parameter decoder/layers_11/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:11.246603 140316961150976 adafactor.py:358] Since rank of parameter decoder/layers_10/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:11.252020 140392160929792 adafactor.py:358] Since rank of parameter decoder/layers_23/pre_extra_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:11.212281 140188559153152 adafactor.py:358] Since rank of parameter decoder/layers_20/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.213346 140188559153152 adafactor.py:358] Since rank of parameter decoder/layers_20/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:11.214443 140188559153152 adafactor.py:358] Since rank of parameter decoder/layers_20/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:11.215395 140188559153152 adafactor.py:358] Since rank of parameter decoder/layers_20/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:11.216267 140188559153152 adafactor.py:358] Since rank of parameter decoder/layers_20/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.217330 140188559153152 adafactor.py:358] Since rank of parameter decoder/layers_20/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:11.218423 140188559153152 adafactor.py:358] Since rank of parameter decoder/layers_20/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.219556 140188559153152 adafactor.py:358] Since rank of parameter decoder/layers_20/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.220611 140188559153152 adafactor.py:358] Since rank of parameter decoder/layers_21/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.250713 140577131780096 adafactor.py:358] Since rank of parameter decoder/layers_11/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.247538 140316961150976 adafactor.py:358] Since rank of parameter decoder/layers_10/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:11.252895 140392160929792 adafactor.py:358] Since rank of parameter decoder/layers_23/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:11.221678 140188559153152 adafactor.py:358] Since rank of parameter decoder/layers_21/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.222769 140188559153152 adafactor.py:358] Since rank of parameter decoder/layers_21/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:11.223890 140188559153152 adafactor.py:358] Since rank of parameter decoder/layers_21/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:11.224766 140188559153152 adafactor.py:358] Since rank of parameter decoder/layers_21/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:11.225632 140188559153152 adafactor.py:358] Since rank of parameter decoder/layers_21/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.226722 140188559153152 adafactor.py:358] Since rank of parameter decoder/layers_21/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:11.227780 140188559153152 adafactor.py:358] Since rank of parameter decoder/layers_21/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.228900 140188559153152 adafactor.py:358] Since rank of parameter decoder/layers_21/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.253768 140392160929792 adafactor.py:358] Since rank of parameter decoder/layers_23/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:11.251872 140577131780096 adafactor.py:358] Since rank of parameter decoder/layers_11/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.248455 140316961150976 adafactor.py:358] Since rank of parameter decoder/layers_10/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.229971 140188559153152 adafactor.py:358] Since rank of parameter decoder/layers_22/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.231068 140188559153152 adafactor.py:358] Since rank of parameter decoder/layers_22/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.232141 140188559153152 adafactor.py:358] Since rank of parameter decoder/layers_22/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:11.233681 140188559153152 adafactor.py:358] Since rank of parameter decoder/layers_22/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:11.234631 140188559153152 adafactor.py:358] Since rank of parameter decoder/layers_22/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:11.235515 140188559153152 adafactor.py:358] Since rank of parameter decoder/layers_22/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.236583 140188559153152 adafactor.py:358] Since rank of parameter decoder/layers_22/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:11.237715 140188559153152 adafactor.py:358] Since rank of parameter decoder/layers_22/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.238819 140188559153152 adafactor.py:358] Since rank of parameter decoder/layers_22/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.254733 140392160929792 adafactor.py:358] Since rank of parameter decoder/layers_23/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.249658 140316961150976 adafactor.py:358] Since rank of parameter decoder/layers_10/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:11.253122 140577131780096 adafactor.py:358] Since rank of parameter decoder/layers_12/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.255820 140392160929792 adafactor.py:358] Since rank of parameter decoder/layers_23/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:11.239898 140188559153152 adafactor.py:358] Since rank of parameter decoder/layers_23/extra_mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.240975 140188559153152 adafactor.py:358] Since rank of parameter decoder/layers_23/extra_mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.242100 140188559153152 adafactor.py:358] Since rank of parameter decoder/layers_23/extra_mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:11.254251 140577131780096 adafactor.py:358] Since rank of parameter decoder/layers_12/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.250780 140316961150976 adafactor.py:358] Since rank of parameter decoder/layers_10/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.257054 140392160929792 adafactor.py:358] Since rank of parameter decoder/layers_23/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.246121 140188559153152 adafactor.py:358] Since rank of parameter decoder/layers_23/mlp/router/router_weights/w/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>, <FactorDim.NONE: None>) is ignored.
W0512 01:45:11.247039 140188559153152 adafactor.py:358] Since rank of parameter decoder/layers_23/pre_extra_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:11.247901 140188559153152 adafactor.py:358] Since rank of parameter decoder/layers_23/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:11.248768 140188559153152 adafactor.py:358] Since rank of parameter decoder/layers_23/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:11.251879 140316961150976 adafactor.py:358] Since rank of parameter decoder/layers_10/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.249683 140188559153152 adafactor.py:358] Since rank of parameter decoder/layers_23/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.250765 140188559153152 adafactor.py:358] Since rank of parameter decoder/layers_23/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:11.251993 140188559153152 adafactor.py:358] Since rank of parameter decoder/layers_23/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.253059 140188559153152 adafactor.py:358] Since rank of parameter decoder/layers_23/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.254160 140188559153152 adafactor.py:358] Since rank of parameter decoder/layers_3/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.255389 140577131780096 adafactor.py:358] Since rank of parameter decoder/layers_12/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:11.258209 140392160929792 adafactor.py:358] Since rank of parameter decoder/layers_23/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.253032 140316961150976 adafactor.py:358] Since rank of parameter decoder/layers_11/extra_mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.256505 140577131780096 adafactor.py:358] Since rank of parameter decoder/layers_12/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:11.255302 140188559153152 adafactor.py:358] Since rank of parameter decoder/layers_3/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.259375 140392160929792 adafactor.py:358] Since rank of parameter decoder/layers_3/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.257483 140577131780096 adafactor.py:358] Since rank of parameter decoder/layers_12/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:11.254182 140316961150976 adafactor.py:358] Since rank of parameter decoder/layers_11/extra_mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.256392 140188559153152 adafactor.py:358] Since rank of parameter decoder/layers_3/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:11.257451 140188559153152 adafactor.py:358] Since rank of parameter decoder/layers_3/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:11.258399 140577131780096 adafactor.py:358] Since rank of parameter decoder/layers_12/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.260501 140392160929792 adafactor.py:358] Since rank of parameter decoder/layers_3/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.255300 140316961150976 adafactor.py:358] Since rank of parameter decoder/layers_11/extra_mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:11.258371 140188559153152 adafactor.py:358] Since rank of parameter decoder/layers_3/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:11.259547 140577131780096 adafactor.py:358] Since rank of parameter decoder/layers_12/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:11.261624 140392160929792 adafactor.py:358] Since rank of parameter decoder/layers_3/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:11.259280 140188559153152 adafactor.py:358] Since rank of parameter decoder/layers_3/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.260667 140577131780096 adafactor.py:358] Since rank of parameter decoder/layers_12/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.125521 139676058949632 adafactor.py:358] Since rank of parameter decoder/layers_14/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.126629 139676058949632 adafactor.py:358] Since rank of parameter decoder/layers_14/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.127725 139676058949632 adafactor.py:358] Since rank of parameter decoder/layers_15/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.128813 139676058949632 adafactor.py:358] Since rank of parameter decoder/layers_15/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.130000 139676058949632 adafactor.py:358] Since rank of parameter decoder/layers_15/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:11.131084 139676058949632 adafactor.py:358] Since rank of parameter decoder/layers_15/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:11.131971 139676058949632 adafactor.py:358] Since rank of parameter decoder/layers_15/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:11.132871 139676058949632 adafactor.py:358] Since rank of parameter decoder/layers_15/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.134090 139676058949632 adafactor.py:358] Since rank of parameter decoder/layers_15/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:11.262788 140392160929792 adafactor.py:358] Since rank of parameter decoder/layers_3/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:11.260346 140188559153152 adafactor.py:358] Since rank of parameter decoder/layers_3/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:11.135191 139676058949632 adafactor.py:358] Since rank of parameter decoder/layers_15/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.136288 139676058949632 adafactor.py:358] Since rank of parameter decoder/layers_15/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.137380 139676058949632 adafactor.py:358] Since rank of parameter decoder/layers_16/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.138550 139676058949632 adafactor.py:358] Since rank of parameter decoder/layers_16/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.139643 139676058949632 adafactor.py:358] Since rank of parameter decoder/layers_16/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:11.263717 140392160929792 adafactor.py:358] Since rank of parameter decoder/layers_3/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:11.261407 140188559153152 adafactor.py:358] Since rank of parameter decoder/layers_3/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.264718 140392160929792 adafactor.py:358] Since rank of parameter decoder/layers_3/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.140731 139676058949632 adafactor.py:358] Since rank of parameter decoder/layers_16/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:11.141608 139676058949632 adafactor.py:358] Since rank of parameter decoder/layers_16/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:11.142576 139676058949632 adafactor.py:358] Since rank of parameter decoder/layers_16/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.143667 139676058949632 adafactor.py:358] Since rank of parameter decoder/layers_16/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:11.144752 139676058949632 adafactor.py:358] Since rank of parameter decoder/layers_16/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.145882 139676058949632 adafactor.py:358] Since rank of parameter decoder/layers_16/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.147027 139676058949632 adafactor.py:358] Since rank of parameter decoder/layers_17/extra_mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.148108 139676058949632 adafactor.py:358] Since rank of parameter decoder/layers_17/extra_mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.259527 140316961150976 adafactor.py:358] Since rank of parameter decoder/layers_11/mlp/router/router_weights/w/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>, <FactorDim.NONE: None>) is ignored.
W0512 01:45:11.263099 140577131780096 adafactor.py:358] Since rank of parameter decoder/layers_12/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.262498 140188559153152 adafactor.py:358] Since rank of parameter decoder/layers_3/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.260445 140316961150976 adafactor.py:358] Since rank of parameter decoder/layers_11/pre_extra_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:11.265883 140392160929792 adafactor.py:358] Since rank of parameter decoder/layers_3/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:11.264254 140577131780096 adafactor.py:358] Since rank of parameter decoder/layers_13/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.149221 139676058949632 adafactor.py:358] Since rank of parameter decoder/layers_17/extra_mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:11.153360 139676058949632 adafactor.py:358] Since rank of parameter decoder/layers_17/mlp/router/router_weights/w/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>, <FactorDim.NONE: None>) is ignored.
W0512 01:45:11.154321 139676058949632 adafactor.py:358] Since rank of parameter decoder/layers_17/pre_extra_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:11.155275 139676058949632 adafactor.py:358] Since rank of parameter decoder/layers_17/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:11.156168 139676058949632 adafactor.py:358] Since rank of parameter decoder/layers_17/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:11.157053 139676058949632 adafactor.py:358] Since rank of parameter decoder/layers_17/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.158184 139676058949632 adafactor.py:358] Since rank of parameter decoder/layers_17/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:11.159282 139676058949632 adafactor.py:358] Since rank of parameter decoder/layers_17/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.160878 139676058949632 adafactor.py:358] Since rank of parameter decoder/layers_17/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.263612 140188559153152 adafactor.py:358] Since rank of parameter decoder/layers_4/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.267071 140392160929792 adafactor.py:358] Since rank of parameter decoder/layers_3/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.261408 140316961150976 adafactor.py:358] Since rank of parameter decoder/layers_11/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:11.265372 140577131780096 adafactor.py:358] Since rank of parameter decoder/layers_13/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.162007 139676058949632 adafactor.py:358] Since rank of parameter decoder/layers_18/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.163086 139676058949632 adafactor.py:358] Since rank of parameter decoder/layers_18/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.164242 139676058949632 adafactor.py:358] Since rank of parameter decoder/layers_18/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:11.165330 139676058949632 adafactor.py:358] Since rank of parameter decoder/layers_18/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:11.166254 139676058949632 adafactor.py:358] Since rank of parameter decoder/layers_18/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:11.167149 139676058949632 adafactor.py:358] Since rank of parameter decoder/layers_18/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.168267 139676058949632 adafactor.py:358] Since rank of parameter decoder/layers_18/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:11.169427 139676058949632 adafactor.py:358] Since rank of parameter decoder/layers_18/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.264714 140188559153152 adafactor.py:358] Since rank of parameter decoder/layers_4/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.262338 140316961150976 adafactor.py:358] Since rank of parameter decoder/layers_11/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:11.268212 140392160929792 adafactor.py:358] Since rank of parameter decoder/layers_3/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.170558 139676058949632 adafactor.py:358] Since rank of parameter decoder/layers_18/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.171662 139676058949632 adafactor.py:358] Since rank of parameter decoder/layers_19/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.172776 139676058949632 adafactor.py:358] Since rank of parameter decoder/layers_19/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.173990 139676058949632 adafactor.py:358] Since rank of parameter decoder/layers_19/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:11.175107 139676058949632 adafactor.py:358] Since rank of parameter decoder/layers_19/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:11.176019 139676058949632 adafactor.py:358] Since rank of parameter decoder/layers_19/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:11.176921 139676058949632 adafactor.py:358] Since rank of parameter decoder/layers_19/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.178102 139676058949632 adafactor.py:358] Since rank of parameter decoder/layers_19/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:11.179203 139676058949632 adafactor.py:358] Since rank of parameter decoder/layers_19/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.263240 140316961150976 adafactor.py:358] Since rank of parameter decoder/layers_11/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.265827 140188559153152 adafactor.py:358] Since rank of parameter decoder/layers_4/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:11.267090 140577131780096 adafactor.py:358] Since rank of parameter decoder/layers_13/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:11.180302 139676058949632 adafactor.py:358] Since rank of parameter decoder/layers_19/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.181395 139676058949632 adafactor.py:358] Since rank of parameter decoder/layers_2/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.182581 139676058949632 adafactor.py:358] Since rank of parameter decoder/layers_2/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.183669 139676058949632 adafactor.py:358] Since rank of parameter decoder/layers_2/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:11.184765 139676058949632 adafactor.py:358] Since rank of parameter decoder/layers_2/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:11.185638 139676058949632 adafactor.py:358] Since rank of parameter decoder/layers_2/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:11.186604 139676058949632 adafactor.py:358] Since rank of parameter decoder/layers_2/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.187709 139676058949632 adafactor.py:358] Since rank of parameter decoder/layers_2/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:11.188810 139676058949632 adafactor.py:358] Since rank of parameter decoder/layers_2/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.269376 140392160929792 adafactor.py:358] Since rank of parameter decoder/layers_4/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.264330 140316961150976 adafactor.py:358] Since rank of parameter decoder/layers_11/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:11.266980 140188559153152 adafactor.py:358] Since rank of parameter decoder/layers_4/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:11.268273 140577131780096 adafactor.py:358] Since rank of parameter decoder/layers_13/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:11.189921 139676058949632 adafactor.py:358] Since rank of parameter decoder/layers_2/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.191071 139676058949632 adafactor.py:358] Since rank of parameter decoder/layers_20/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.192161 139676058949632 adafactor.py:358] Since rank of parameter decoder/layers_20/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.193251 139676058949632 adafactor.py:358] Since rank of parameter decoder/layers_20/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:11.194396 139676058949632 adafactor.py:358] Since rank of parameter decoder/layers_20/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:11.195349 139676058949632 adafactor.py:358] Since rank of parameter decoder/layers_20/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:11.196243 139676058949632 adafactor.py:358] Since rank of parameter decoder/layers_20/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.197329 139676058949632 adafactor.py:358] Since rank of parameter decoder/layers_20/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:11.270505 140392160929792 adafactor.py:358] Since rank of parameter decoder/layers_4/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.267884 140188559153152 adafactor.py:358] Since rank of parameter decoder/layers_4/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:11.265442 140316961150976 adafactor.py:358] Since rank of parameter decoder/layers_11/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.269161 140577131780096 adafactor.py:358] Since rank of parameter decoder/layers_13/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:11.198450 139676058949632 adafactor.py:358] Since rank of parameter decoder/layers_20/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.199583 139676058949632 adafactor.py:358] Since rank of parameter decoder/layers_20/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.200668 139676058949632 adafactor.py:358] Since rank of parameter decoder/layers_21/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.201785 139676058949632 adafactor.py:358] Since rank of parameter decoder/layers_21/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.202882 139676058949632 adafactor.py:358] Since rank of parameter decoder/layers_21/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:11.204028 139676058949632 adafactor.py:358] Since rank of parameter decoder/layers_21/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:11.204924 139676058949632 adafactor.py:358] Since rank of parameter decoder/layers_21/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:11.205830 139676058949632 adafactor.py:358] Since rank of parameter decoder/layers_21/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.206930 139676058949632 adafactor.py:358] Since rank of parameter decoder/layers_21/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:11.268748 140188559153152 adafactor.py:358] Since rank of parameter decoder/layers_4/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.271614 140392160929792 adafactor.py:358] Since rank of parameter decoder/layers_4/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:11.270046 140577131780096 adafactor.py:358] Since rank of parameter decoder/layers_13/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.266549 140316961150976 adafactor.py:358] Since rank of parameter decoder/layers_11/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.208019 139676058949632 adafactor.py:358] Since rank of parameter decoder/layers_21/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.209159 139676058949632 adafactor.py:358] Since rank of parameter decoder/layers_21/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.210300 139676058949632 adafactor.py:358] Since rank of parameter decoder/layers_22/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.211401 139676058949632 adafactor.py:358] Since rank of parameter decoder/layers_22/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.212492 139676058949632 adafactor.py:358] Since rank of parameter decoder/layers_22/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:11.214157 139676058949632 adafactor.py:358] Since rank of parameter decoder/layers_22/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:11.215065 139676058949632 adafactor.py:358] Since rank of parameter decoder/layers_22/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:11.215984 139676058949632 adafactor.py:358] Since rank of parameter decoder/layers_22/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.217092 139676058949632 adafactor.py:358] Since rank of parameter decoder/layers_22/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:11.269793 140188559153152 adafactor.py:358] Since rank of parameter decoder/layers_4/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:11.272719 140392160929792 adafactor.py:358] Since rank of parameter decoder/layers_4/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:11.267714 140316961150976 adafactor.py:358] Since rank of parameter decoder/layers_12/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.271128 140577131780096 adafactor.py:358] Since rank of parameter decoder/layers_13/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:11.273654 140392160929792 adafactor.py:358] Since rank of parameter decoder/layers_4/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:11.270886 140188559153152 adafactor.py:358] Since rank of parameter decoder/layers_4/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.218285 139676058949632 adafactor.py:358] Since rank of parameter decoder/layers_22/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.219381 139676058949632 adafactor.py:358] Since rank of parameter decoder/layers_22/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.220475 139676058949632 adafactor.py:358] Since rank of parameter decoder/layers_23/extra_mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.221567 139676058949632 adafactor.py:358] Since rank of parameter decoder/layers_23/extra_mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.222755 139676058949632 adafactor.py:358] Since rank of parameter decoder/layers_23/extra_mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:11.226840 139676058949632 adafactor.py:358] Since rank of parameter decoder/layers_23/mlp/router/router_weights/w/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>, <FactorDim.NONE: None>) is ignored.
W0512 01:45:11.227753 139676058949632 adafactor.py:358] Since rank of parameter decoder/layers_23/pre_extra_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:11.228625 139676058949632 adafactor.py:358] Since rank of parameter decoder/layers_23/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:11.268821 140316961150976 adafactor.py:358] Since rank of parameter decoder/layers_12/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.272313 140577131780096 adafactor.py:358] Since rank of parameter decoder/layers_13/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.274579 140392160929792 adafactor.py:358] Since rank of parameter decoder/layers_4/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.229511 139676058949632 adafactor.py:358] Since rank of parameter decoder/layers_23/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:11.230467 139676058949632 adafactor.py:358] Since rank of parameter decoder/layers_23/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.231541 139676058949632 adafactor.py:358] Since rank of parameter decoder/layers_23/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:11.232794 139676058949632 adafactor.py:358] Since rank of parameter decoder/layers_23/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.233930 139676058949632 adafactor.py:358] Since rank of parameter decoder/layers_23/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.235058 139676058949632 adafactor.py:358] Since rank of parameter decoder/layers_3/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.236133 139676058949632 adafactor.py:358] Since rank of parameter decoder/layers_3/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.237215 139676058949632 adafactor.py:358] Since rank of parameter decoder/layers_3/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:11.238338 139676058949632 adafactor.py:358] Since rank of parameter decoder/layers_3/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:11.271996 140188559153152 adafactor.py:358] Since rank of parameter decoder/layers_4/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.269912 140316961150976 adafactor.py:358] Since rank of parameter decoder/layers_12/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:11.273422 140577131780096 adafactor.py:358] Since rank of parameter decoder/layers_13/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.239243 139676058949632 adafactor.py:358] Since rank of parameter decoder/layers_3/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:11.240184 139676058949632 adafactor.py:358] Since rank of parameter decoder/layers_3/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.275664 140392160929792 adafactor.py:358] Since rank of parameter decoder/layers_4/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:11.273065 140188559153152 adafactor.py:358] Since rank of parameter decoder/layers_5/extra_mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.270963 140316961150976 adafactor.py:358] Since rank of parameter decoder/layers_12/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:11.274520 140577131780096 adafactor.py:358] Since rank of parameter decoder/layers_14/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.276771 140392160929792 adafactor.py:358] Since rank of parameter decoder/layers_4/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.274126 140188559153152 adafactor.py:358] Since rank of parameter decoder/layers_5/extra_mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.241267 139676058949632 adafactor.py:358] Since rank of parameter decoder/layers_3/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:11.242374 139676058949632 adafactor.py:358] Since rank of parameter decoder/layers_3/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.243460 139676058949632 adafactor.py:358] Since rank of parameter decoder/layers_3/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.244585 139676058949632 adafactor.py:358] Since rank of parameter decoder/layers_4/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.271833 140316961150976 adafactor.py:358] Since rank of parameter decoder/layers_12/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:11.245662 139676058949632 adafactor.py:358] Since rank of parameter decoder/layers_4/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.246770 139676058949632 adafactor.py:358] Since rank of parameter decoder/layers_4/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:11.247843 139676058949632 adafactor.py:358] Since rank of parameter decoder/layers_4/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:11.248772 139676058949632 adafactor.py:358] Since rank of parameter decoder/layers_4/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:11.249636 139676058949632 adafactor.py:358] Since rank of parameter decoder/layers_4/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.250752 139676058949632 adafactor.py:358] Since rank of parameter decoder/layers_4/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:11.251852 139676058949632 adafactor.py:358] Since rank of parameter decoder/layers_4/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.252995 139676058949632 adafactor.py:358] Since rank of parameter decoder/layers_4/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.275638 140577131780096 adafactor.py:358] Since rank of parameter decoder/layers_14/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.277912 140392160929792 adafactor.py:358] Since rank of parameter decoder/layers_4/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.254137 139676058949632 adafactor.py:358] Since rank of parameter decoder/layers_5/extra_mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.272748 140316961150976 adafactor.py:358] Since rank of parameter decoder/layers_12/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.255243 139676058949632 adafactor.py:358] Since rank of parameter decoder/layers_5/extra_mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.275258 140188559153152 adafactor.py:358] Since rank of parameter decoder/layers_5/extra_mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:11.279052 140392160929792 adafactor.py:358] Since rank of parameter decoder/layers_5/extra_mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.276794 140577131780096 adafactor.py:358] Since rank of parameter decoder/layers_14/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:11.256329 139676058949632 adafactor.py:358] Since rank of parameter decoder/layers_5/extra_mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:11.273845 140316961150976 adafactor.py:358] Since rank of parameter decoder/layers_12/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:11.274901 140316961150976 adafactor.py:358] Since rank of parameter decoder/layers_12/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.277897 140577131780096 adafactor.py:358] Since rank of parameter decoder/layers_14/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:11.280140 140392160929792 adafactor.py:358] Since rank of parameter decoder/layers_5/extra_mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.278795 140577131780096 adafactor.py:358] Since rank of parameter decoder/layers_14/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:11.281227 140392160929792 adafactor.py:358] Since rank of parameter decoder/layers_5/extra_mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:11.275942 140316961150976 adafactor.py:358] Since rank of parameter decoder/layers_12/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.279729 140577131780096 adafactor.py:358] Since rank of parameter decoder/layers_14/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.279274 140188559153152 adafactor.py:358] Since rank of parameter decoder/layers_5/mlp/router/router_weights/w/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>, <FactorDim.NONE: None>) is ignored.
W0512 01:45:11.277069 140316961150976 adafactor.py:358] Since rank of parameter decoder/layers_13/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.280217 140188559153152 adafactor.py:358] Since rank of parameter decoder/layers_5/pre_extra_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:11.260461 139676058949632 adafactor.py:358] Since rank of parameter decoder/layers_5/mlp/router/router_weights/w/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>, <FactorDim.NONE: None>) is ignored.
W0512 01:45:11.278150 140316961150976 adafactor.py:358] Since rank of parameter decoder/layers_13/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.281084 140188559153152 adafactor.py:358] Since rank of parameter decoder/layers_5/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:11.261425 139676058949632 adafactor.py:358] Since rank of parameter decoder/layers_5/pre_extra_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:11.281948 140188559153152 adafactor.py:358] Since rank of parameter decoder/layers_5/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:11.279712 140316961150976 adafactor.py:358] Since rank of parameter decoder/layers_13/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:11.285358 140392160929792 adafactor.py:358] Since rank of parameter decoder/layers_5/mlp/router/router_weights/w/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>, <FactorDim.NONE: None>) is ignored.
W0512 01:45:11.262335 139676058949632 adafactor.py:358] Since rank of parameter decoder/layers_5/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:11.282844 140188559153152 adafactor.py:358] Since rank of parameter decoder/layers_5/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.263224 139676058949632 adafactor.py:358] Since rank of parameter decoder/layers_5/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:11.280826 140316961150976 adafactor.py:358] Since rank of parameter decoder/layers_13/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:11.286353 140392160929792 adafactor.py:358] Since rank of parameter decoder/layers_5/pre_extra_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:11.264116 139676058949632 adafactor.py:358] Since rank of parameter decoder/layers_5/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.281711 140316961150976 adafactor.py:358] Since rank of parameter decoder/layers_13/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:11.284372 140188559153152 adafactor.py:358] Since rank of parameter decoder/layers_5/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:11.287239 140392160929792 adafactor.py:358] Since rank of parameter decoder/layers_5/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:11.282579 140316961150976 adafactor.py:358] Since rank of parameter decoder/layers_13/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.288123 140392160929792 adafactor.py:358] Since rank of parameter decoder/layers_5/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:11.285437 140188559153152 adafactor.py:358] Since rank of parameter decoder/layers_5/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.265647 139676058949632 adafactor.py:358] Since rank of parameter decoder/layers_5/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:11.283634 140316961150976 adafactor.py:358] Since rank of parameter decoder/layers_13/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:11.289016 140392160929792 adafactor.py:358] Since rank of parameter decoder/layers_5/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.286529 140188559153152 adafactor.py:358] Since rank of parameter decoder/layers_5/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.266770 139676058949632 adafactor.py:358] Since rank of parameter decoder/layers_5/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.284736 140316961150976 adafactor.py:358] Since rank of parameter decoder/layers_13/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.287589 140188559153152 adafactor.py:358] Since rank of parameter decoder/layers_6/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.290557 140392160929792 adafactor.py:358] Since rank of parameter decoder/layers_5/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:11.267851 139676058949632 adafactor.py:358] Since rank of parameter decoder/layers_5/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.285868 140316961150976 adafactor.py:358] Since rank of parameter decoder/layers_13/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.288699 140188559153152 adafactor.py:358] Since rank of parameter decoder/layers_6/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.291664 140392160929792 adafactor.py:358] Since rank of parameter decoder/layers_5/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.268933 139676058949632 adafactor.py:358] Since rank of parameter decoder/layers_6/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.286971 140316961150976 adafactor.py:358] Since rank of parameter decoder/layers_14/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.289758 140188559153152 adafactor.py:358] Since rank of parameter decoder/layers_6/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:11.292751 140392160929792 adafactor.py:358] Since rank of parameter decoder/layers_5/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.270104 139676058949632 adafactor.py:358] Since rank of parameter decoder/layers_6/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.271197 139676058949632 adafactor.py:358] Since rank of parameter decoder/layers_6/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:11.288079 140316961150976 adafactor.py:358] Since rank of parameter decoder/layers_14/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.293835 140392160929792 adafactor.py:358] Since rank of parameter decoder/layers_6/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.290847 140188559153152 adafactor.py:358] Since rank of parameter decoder/layers_6/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:11.291740 140188559153152 adafactor.py:358] Since rank of parameter decoder/layers_6/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:11.289241 140316961150976 adafactor.py:358] Since rank of parameter decoder/layers_14/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:11.295009 140392160929792 adafactor.py:358] Since rank of parameter decoder/layers_6/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.272292 139676058949632 adafactor.py:358] Since rank of parameter decoder/layers_6/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:11.292611 140188559153152 adafactor.py:358] Since rank of parameter decoder/layers_6/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.290369 140316961150976 adafactor.py:358] Since rank of parameter decoder/layers_14/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:11.273176 139676058949632 adafactor.py:358] Since rank of parameter decoder/layers_6/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:11.296102 140392160929792 adafactor.py:358] Since rank of parameter decoder/layers_6/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:11.293723 140188559153152 adafactor.py:358] Since rank of parameter decoder/layers_6/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:11.291283 140316961150976 adafactor.py:358] Since rank of parameter decoder/layers_14/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:11.274136 139676058949632 adafactor.py:358] Since rank of parameter decoder/layers_6/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.297189 140392160929792 adafactor.py:358] Since rank of parameter decoder/layers_6/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:11.292151 140316961150976 adafactor.py:358] Since rank of parameter decoder/layers_14/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.294986 140188559153152 adafactor.py:358] Since rank of parameter decoder/layers_6/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.275274 139676058949632 adafactor.py:358] Since rank of parameter decoder/layers_6/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:11.298105 140392160929792 adafactor.py:358] Since rank of parameter decoder/layers_6/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:11.296059 140188559153152 adafactor.py:358] Since rank of parameter decoder/layers_6/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.299003 140392160929792 adafactor.py:358] Since rank of parameter decoder/layers_6/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.293900 140316961150976 adafactor.py:358] Since rank of parameter decoder/layers_14/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:11.276367 139676058949632 adafactor.py:358] Since rank of parameter decoder/layers_6/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.297124 140188559153152 adafactor.py:358] Since rank of parameter decoder/layers_7/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.300142 140392160929792 adafactor.py:358] Since rank of parameter decoder/layers_6/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:11.294964 140316961150976 adafactor.py:358] Since rank of parameter decoder/layers_14/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.277459 139676058949632 adafactor.py:358] Since rank of parameter decoder/layers_6/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.298228 140188559153152 adafactor.py:358] Since rank of parameter decoder/layers_7/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.301225 140392160929792 adafactor.py:358] Since rank of parameter decoder/layers_6/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.278573 139676058949632 adafactor.py:358] Since rank of parameter decoder/layers_7/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.296043 140316961150976 adafactor.py:358] Since rank of parameter decoder/layers_14/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.299324 140188559153152 adafactor.py:358] Since rank of parameter decoder/layers_7/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:11.302403 140392160929792 adafactor.py:358] Since rank of parameter decoder/layers_6/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.279708 139676058949632 adafactor.py:358] Since rank of parameter decoder/layers_7/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.297149 140316961150976 adafactor.py:358] Since rank of parameter decoder/layers_15/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.300373 140188559153152 adafactor.py:358] Since rank of parameter decoder/layers_7/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:11.303533 140392160929792 adafactor.py:358] Since rank of parameter decoder/layers_7/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.280838 139676058949632 adafactor.py:358] Since rank of parameter decoder/layers_7/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:11.298241 140316961150976 adafactor.py:358] Since rank of parameter decoder/layers_15/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.301243 140188559153152 adafactor.py:358] Since rank of parameter decoder/layers_7/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:11.304731 140392160929792 adafactor.py:358] Since rank of parameter decoder/layers_7/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.281935 139676058949632 adafactor.py:358] Since rank of parameter decoder/layers_7/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:11.299304 140316961150976 adafactor.py:358] Since rank of parameter decoder/layers_15/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:11.302155 140188559153152 adafactor.py:358] Since rank of parameter decoder/layers_7/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.282855 139676058949632 adafactor.py:358] Since rank of parameter decoder/layers_7/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:11.305863 140392160929792 adafactor.py:358] Since rank of parameter decoder/layers_7/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:11.300366 140316961150976 adafactor.py:358] Since rank of parameter decoder/layers_15/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:11.303234 140188559153152 adafactor.py:358] Since rank of parameter decoder/layers_7/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:11.301270 140316961150976 adafactor.py:358] Since rank of parameter decoder/layers_15/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:11.283808 139676058949632 adafactor.py:358] Since rank of parameter decoder/layers_7/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.307021 140392160929792 adafactor.py:358] Since rank of parameter decoder/layers_7/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:11.304290 140188559153152 adafactor.py:358] Since rank of parameter decoder/layers_7/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.302158 140316961150976 adafactor.py:358] Since rank of parameter decoder/layers_15/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.284881 139676058949632 adafactor.py:358] Since rank of parameter decoder/layers_7/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:11.307912 140392160929792 adafactor.py:358] Since rank of parameter decoder/layers_7/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:11.305335 140188559153152 adafactor.py:358] Since rank of parameter decoder/layers_7/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.303221 140316961150976 adafactor.py:358] Since rank of parameter decoder/layers_15/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:11.308838 140392160929792 adafactor.py:358] Since rank of parameter decoder/layers_7/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.286003 139676058949632 adafactor.py:358] Since rank of parameter decoder/layers_7/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.306481 140188559153152 adafactor.py:358] Since rank of parameter decoder/layers_8/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.304270 140316961150976 adafactor.py:358] Since rank of parameter decoder/layers_15/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.287094 139676058949632 adafactor.py:358] Since rank of parameter decoder/layers_7/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.309920 140392160929792 adafactor.py:358] Since rank of parameter decoder/layers_7/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:11.307533 140188559153152 adafactor.py:358] Since rank of parameter decoder/layers_8/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.305368 140316961150976 adafactor.py:358] Since rank of parameter decoder/layers_15/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.311040 140392160929792 adafactor.py:358] Since rank of parameter decoder/layers_7/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.288235 139676058949632 adafactor.py:358] Since rank of parameter decoder/layers_8/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.308596 140188559153152 adafactor.py:358] Since rank of parameter decoder/layers_8/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:11.306441 140316961150976 adafactor.py:358] Since rank of parameter decoder/layers_16/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.289311 139676058949632 adafactor.py:358] Since rank of parameter decoder/layers_8/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.312134 140392160929792 adafactor.py:358] Since rank of parameter decoder/layers_7/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.309653 140188559153152 adafactor.py:358] Since rank of parameter decoder/layers_8/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:11.290429 139676058949632 adafactor.py:358] Since rank of parameter decoder/layers_8/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:11.313273 140392160929792 adafactor.py:358] Since rank of parameter decoder/layers_8/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.310628 140188559153152 adafactor.py:358] Since rank of parameter decoder/layers_8/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:11.307482 140316961150976 adafactor.py:358] Since rank of parameter decoder/layers_16/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.308570 140316961150976 adafactor.py:358] Since rank of parameter decoder/layers_16/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:11.311497 140188559153152 adafactor.py:358] Since rank of parameter decoder/layers_8/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.291514 139676058949632 adafactor.py:358] Since rank of parameter decoder/layers_8/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:11.314399 140392160929792 adafactor.py:358] Since rank of parameter decoder/layers_8/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.292469 139676058949632 adafactor.py:358] Since rank of parameter decoder/layers_8/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:11.309638 140316961150976 adafactor.py:358] Since rank of parameter decoder/layers_16/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:11.315486 140392160929792 adafactor.py:358] Since rank of parameter decoder/layers_8/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:11.312568 140188559153152 adafactor.py:358] Since rank of parameter decoder/layers_8/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:11.310507 140316961150976 adafactor.py:358] Since rank of parameter decoder/layers_16/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:11.293370 139676058949632 adafactor.py:358] Since rank of parameter decoder/layers_8/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.313628 140188559153152 adafactor.py:358] Since rank of parameter decoder/layers_8/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.311397 140316961150976 adafactor.py:358] Since rank of parameter decoder/layers_16/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.316571 140392160929792 adafactor.py:358] Since rank of parameter decoder/layers_8/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:11.294556 139676058949632 adafactor.py:358] Since rank of parameter decoder/layers_8/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:11.317530 140392160929792 adafactor.py:358] Since rank of parameter decoder/layers_8/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:11.314803 140188559153152 adafactor.py:358] Since rank of parameter decoder/layers_8/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.312490 140316961150976 adafactor.py:358] Since rank of parameter decoder/layers_16/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:11.318466 140392160929792 adafactor.py:358] Since rank of parameter decoder/layers_8/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.295655 139676058949632 adafactor.py:358] Since rank of parameter decoder/layers_8/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.315892 140188559153152 adafactor.py:358] Since rank of parameter decoder/layers_9/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.313572 140316961150976 adafactor.py:358] Since rank of parameter decoder/layers_16/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.319558 140392160929792 adafactor.py:358] Since rank of parameter decoder/layers_8/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:11.296806 139676058949632 adafactor.py:358] Since rank of parameter decoder/layers_8/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.316981 140188559153152 adafactor.py:358] Since rank of parameter decoder/layers_9/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.314653 140316961150976 adafactor.py:358] Since rank of parameter decoder/layers_16/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.320647 140392160929792 adafactor.py:358] Since rank of parameter decoder/layers_8/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.298105 139676058949632 adafactor.py:358] Since rank of parameter decoder/layers_9/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.318044 140188559153152 adafactor.py:358] Since rank of parameter decoder/layers_9/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:11.321811 140392160929792 adafactor.py:358] Since rank of parameter decoder/layers_8/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.181882 139812357228544 adafactor.py:358] Since rank of parameter decoder/layers_14/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:11.182778 139812357228544 adafactor.py:358] Since rank of parameter decoder/layers_14/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:11.183694 139812357228544 adafactor.py:358] Since rank of parameter decoder/layers_14/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.184820 139812357228544 adafactor.py:358] Since rank of parameter decoder/layers_14/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:11.185950 139812357228544 adafactor.py:358] Since rank of parameter decoder/layers_14/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.187058 139812357228544 adafactor.py:358] Since rank of parameter decoder/layers_14/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.188153 139812357228544 adafactor.py:358] Since rank of parameter decoder/layers_15/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.189262 139812357228544 adafactor.py:358] Since rank of parameter decoder/layers_15/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.190398 139812357228544 adafactor.py:358] Since rank of parameter decoder/layers_15/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:11.319190 140188559153152 adafactor.py:358] Since rank of parameter decoder/layers_9/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:11.299209 139676058949632 adafactor.py:358] Since rank of parameter decoder/layers_9/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.191466 139812357228544 adafactor.py:358] Since rank of parameter decoder/layers_15/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:11.192380 139812357228544 adafactor.py:358] Since rank of parameter decoder/layers_15/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:11.193269 139812357228544 adafactor.py:358] Since rank of parameter decoder/layers_15/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.194397 139812357228544 adafactor.py:358] Since rank of parameter decoder/layers_15/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:11.195477 139812357228544 adafactor.py:358] Since rank of parameter decoder/layers_15/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.196598 139812357228544 adafactor.py:358] Since rank of parameter decoder/layers_15/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.197690 139812357228544 adafactor.py:358] Since rank of parameter decoder/layers_16/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.198832 139812357228544 adafactor.py:358] Since rank of parameter decoder/layers_16/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.320063 140188559153152 adafactor.py:358] Since rank of parameter decoder/layers_9/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:11.322964 140392160929792 adafactor.py:358] Since rank of parameter decoder/layers_9/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.300292 139676058949632 adafactor.py:358] Since rank of parameter decoder/layers_9/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:11.320926 140188559153152 adafactor.py:358] Since rank of parameter decoder/layers_9/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.199919 139812357228544 adafactor.py:358] Since rank of parameter decoder/layers_16/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:11.201617 139812357228544 adafactor.py:358] Since rank of parameter decoder/layers_16/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:11.202547 139812357228544 adafactor.py:358] Since rank of parameter decoder/layers_16/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:11.203540 139812357228544 adafactor.py:358] Since rank of parameter decoder/layers_16/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.204702 139812357228544 adafactor.py:358] Since rank of parameter decoder/layers_16/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:11.205801 139812357228544 adafactor.py:358] Since rank of parameter decoder/layers_16/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.206894 139812357228544 adafactor.py:358] Since rank of parameter decoder/layers_16/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.208027 139812357228544 adafactor.py:358] Since rank of parameter decoder/layers_17/extra_mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.209135 139812357228544 adafactor.py:358] Since rank of parameter decoder/layers_17/extra_mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.324115 140392160929792 adafactor.py:358] Since rank of parameter decoder/layers_9/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.301440 139676058949632 adafactor.py:358] Since rank of parameter decoder/layers_9/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:11.321981 140188559153152 adafactor.py:358] Since rank of parameter decoder/layers_9/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:11.210228 139812357228544 adafactor.py:358] Since rank of parameter decoder/layers_17/extra_mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:11.214358 139812357228544 adafactor.py:358] Since rank of parameter decoder/layers_17/mlp/router/router_weights/w/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>, <FactorDim.NONE: None>) is ignored.
W0512 01:45:11.215261 139812357228544 adafactor.py:358] Since rank of parameter decoder/layers_17/pre_extra_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:11.216221 139812357228544 adafactor.py:358] Since rank of parameter decoder/layers_17/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:11.217111 139812357228544 adafactor.py:358] Since rank of parameter decoder/layers_17/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:11.218002 139812357228544 adafactor.py:358] Since rank of parameter decoder/layers_17/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.219084 139812357228544 adafactor.py:358] Since rank of parameter decoder/layers_17/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:11.220192 139812357228544 adafactor.py:358] Since rank of parameter decoder/layers_17/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.221778 139812357228544 adafactor.py:358] Since rank of parameter decoder/layers_17/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.302356 139676058949632 adafactor.py:358] Since rank of parameter decoder/layers_9/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:11.325212 140392160929792 adafactor.py:358] Since rank of parameter decoder/layers_9/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:11.323115 140188559153152 adafactor.py:358] Since rank of parameter decoder/layers_9/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.303246 139676058949632 adafactor.py:358] Since rank of parameter decoder/layers_9/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.326387 140392160929792 adafactor.py:358] Since rank of parameter decoder/layers_9/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:11.222865 139812357228544 adafactor.py:358] Since rank of parameter decoder/layers_18/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.223962 139812357228544 adafactor.py:358] Since rank of parameter decoder/layers_18/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.225127 139812357228544 adafactor.py:358] Since rank of parameter decoder/layers_18/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:11.226213 139812357228544 adafactor.py:358] Since rank of parameter decoder/layers_18/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:11.227090 139812357228544 adafactor.py:358] Since rank of parameter decoder/layers_18/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:11.227972 139812357228544 adafactor.py:358] Since rank of parameter decoder/layers_18/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.229074 139812357228544 adafactor.py:358] Since rank of parameter decoder/layers_18/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:11.230221 139812357228544 adafactor.py:358] Since rank of parameter decoder/layers_18/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.231304 139812357228544 adafactor.py:358] Since rank of parameter decoder/layers_18/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.232410 139812357228544 adafactor.py:358] Since rank of parameter decoder/layers_19/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.233489 139812357228544 adafactor.py:358] Since rank of parameter decoder/layers_19/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.234615 139812357228544 adafactor.py:358] Since rank of parameter decoder/layers_19/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:11.235697 139812357228544 adafactor.py:358] Since rank of parameter decoder/layers_19/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:11.236625 139812357228544 adafactor.py:358] Since rank of parameter decoder/layers_19/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:11.237507 139812357228544 adafactor.py:358] Since rank of parameter decoder/layers_19/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.238649 139812357228544 adafactor.py:358] Since rank of parameter decoder/layers_19/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:11.239751 139812357228544 adafactor.py:358] Since rank of parameter decoder/layers_19/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.324174 140188559153152 adafactor.py:358] Since rank of parameter decoder/layers_9/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.304320 139676058949632 adafactor.py:358] Since rank of parameter decoder/layers_9/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:11.327282 140392160929792 adafactor.py:358] Since rank of parameter decoder/layers_9/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:11.240869 139812357228544 adafactor.py:358] Since rank of parameter decoder/layers_19/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.241955 139812357228544 adafactor.py:358] Since rank of parameter decoder/layers_2/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.243091 139812357228544 adafactor.py:358] Since rank of parameter decoder/layers_2/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.244220 139812357228544 adafactor.py:358] Since rank of parameter decoder/layers_2/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:11.245300 139812357228544 adafactor.py:358] Since rank of parameter decoder/layers_2/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:11.246186 139812357228544 adafactor.py:358] Since rank of parameter decoder/layers_2/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:11.247118 139812357228544 adafactor.py:358] Since rank of parameter decoder/layers_2/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.248212 139812357228544 adafactor.py:358] Since rank of parameter decoder/layers_2/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:11.249297 139812357228544 adafactor.py:358] Since rank of parameter decoder/layers_2/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.328184 140392160929792 adafactor.py:358] Since rank of parameter decoder/layers_9/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.325220 140188559153152 adafactor.py:358] Since rank of parameter decoder/logits_dense/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.305447 139676058949632 adafactor.py:358] Since rank of parameter decoder/layers_9/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.231278 139997397456896 adafactor.py:358] Since rank of parameter decoder/layers_14/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.232346 139997397456896 adafactor.py:358] Since rank of parameter decoder/layers_14/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.233441 139997397456896 adafactor.py:358] Since rank of parameter decoder/layers_15/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.234497 139997397456896 adafactor.py:358] Since rank of parameter decoder/layers_15/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.235621 139997397456896 adafactor.py:358] Since rank of parameter decoder/layers_15/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:11.236671 139997397456896 adafactor.py:358] Since rank of parameter decoder/layers_15/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:11.237575 139997397456896 adafactor.py:358] Since rank of parameter decoder/layers_15/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:11.238447 139997397456896 adafactor.py:358] Since rank of parameter decoder/layers_15/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.250363 139812357228544 adafactor.py:358] Since rank of parameter decoder/layers_2/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.251501 139812357228544 adafactor.py:358] Since rank of parameter decoder/layers_20/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.252624 139812357228544 adafactor.py:358] Since rank of parameter decoder/layers_20/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.253703 139812357228544 adafactor.py:358] Since rank of parameter decoder/layers_20/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:11.254779 139812357228544 adafactor.py:358] Since rank of parameter decoder/layers_20/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:11.255726 139812357228544 adafactor.py:358] Since rank of parameter decoder/layers_20/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:11.256642 139812357228544 adafactor.py:358] Since rank of parameter decoder/layers_20/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.257705 139812357228544 adafactor.py:358] Since rank of parameter decoder/layers_20/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:11.239576 139997397456896 adafactor.py:358] Since rank of parameter decoder/layers_15/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:11.240646 139997397456896 adafactor.py:358] Since rank of parameter decoder/layers_15/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.241757 139997397456896 adafactor.py:358] Since rank of parameter decoder/layers_15/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.242823 139997397456896 adafactor.py:358] Since rank of parameter decoder/layers_16/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.243990 139997397456896 adafactor.py:358] Since rank of parameter decoder/layers_16/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.245110 139997397456896 adafactor.py:358] Since rank of parameter decoder/layers_16/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:11.246178 139997397456896 adafactor.py:358] Since rank of parameter decoder/layers_16/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:11.247045 139997397456896 adafactor.py:358] Since rank of parameter decoder/layers_16/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:11.247970 139997397456896 adafactor.py:358] Since rank of parameter decoder/layers_16/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.329263 140392160929792 adafactor.py:358] Since rank of parameter decoder/layers_9/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:11.306553 139676058949632 adafactor.py:358] Since rank of parameter decoder/layers_9/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.326297 140188559153152 adafactor.py:358] Since rank of parameter token_embedder/embedding 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:11.258788 139812357228544 adafactor.py:358] Since rank of parameter decoder/layers_20/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.259899 139812357228544 adafactor.py:358] Since rank of parameter decoder/layers_20/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.261010 139812357228544 adafactor.py:358] Since rank of parameter decoder/layers_21/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.262100 139812357228544 adafactor.py:358] Since rank of parameter decoder/layers_21/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.263172 139812357228544 adafactor.py:358] Since rank of parameter decoder/layers_21/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:11.264356 139812357228544 adafactor.py:358] Since rank of parameter decoder/layers_21/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:11.265262 139812357228544 adafactor.py:358] Since rank of parameter decoder/layers_21/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:11.266155 139812357228544 adafactor.py:358] Since rank of parameter decoder/layers_21/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.267239 139812357228544 adafactor.py:358] Since rank of parameter decoder/layers_21/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:11.249082 139997397456896 adafactor.py:358] Since rank of parameter decoder/layers_16/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:11.250152 139997397456896 adafactor.py:358] Since rank of parameter decoder/layers_16/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.251226 139997397456896 adafactor.py:358] Since rank of parameter decoder/layers_16/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.252353 139997397456896 adafactor.py:358] Since rank of parameter decoder/layers_17/extra_mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.253513 139997397456896 adafactor.py:358] Since rank of parameter decoder/layers_17/extra_mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.254641 139997397456896 adafactor.py:358] Since rank of parameter decoder/layers_17/extra_mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:11.258882 139997397456896 adafactor.py:358] Since rank of parameter decoder/layers_17/mlp/router/router_weights/w/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>, <FactorDim.NONE: None>) is ignored.
W0512 01:45:11.259812 139997397456896 adafactor.py:358] Since rank of parameter decoder/layers_17/pre_extra_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:11.330434 140392160929792 adafactor.py:358] Since rank of parameter decoder/layers_9/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.307618 139676058949632 adafactor.py:358] Since rank of parameter decoder/logits_dense/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.268343 139812357228544 adafactor.py:358] Since rank of parameter decoder/layers_21/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.269487 139812357228544 adafactor.py:358] Since rank of parameter decoder/layers_21/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.270561 139812357228544 adafactor.py:358] Since rank of parameter decoder/layers_22/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.271625 139812357228544 adafactor.py:358] Since rank of parameter decoder/layers_22/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.272727 139812357228544 adafactor.py:358] Since rank of parameter decoder/layers_22/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:11.274283 139812357228544 adafactor.py:358] Since rank of parameter decoder/layers_22/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:11.275172 139812357228544 adafactor.py:358] Since rank of parameter decoder/layers_22/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:11.276048 139812357228544 adafactor.py:358] Since rank of parameter decoder/layers_22/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.277159 139812357228544 adafactor.py:358] Since rank of parameter decoder/layers_22/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:11.331529 140392160929792 adafactor.py:358] Since rank of parameter decoder/layers_9/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.308701 139676058949632 adafactor.py:358] Since rank of parameter token_embedder/embedding 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:11.260780 139997397456896 adafactor.py:358] Since rank of parameter decoder/layers_17/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:11.261732 139997397456896 adafactor.py:358] Since rank of parameter decoder/layers_17/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:11.262654 139997397456896 adafactor.py:358] Since rank of parameter decoder/layers_17/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.263809 139997397456896 adafactor.py:358] Since rank of parameter decoder/layers_17/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:11.264929 139997397456896 adafactor.py:358] Since rank of parameter decoder/layers_17/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.266563 139997397456896 adafactor.py:358] Since rank of parameter decoder/layers_17/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.267681 139997397456896 adafactor.py:358] Since rank of parameter decoder/layers_18/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.268745 139997397456896 adafactor.py:358] Since rank of parameter decoder/layers_18/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.269939 139997397456896 adafactor.py:358] Since rank of parameter decoder/layers_18/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:11.278278 139812357228544 adafactor.py:358] Since rank of parameter decoder/layers_22/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.279348 139812357228544 adafactor.py:358] Since rank of parameter decoder/layers_22/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.280445 139812357228544 adafactor.py:358] Since rank of parameter decoder/layers_23/extra_mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.281534 139812357228544 adafactor.py:358] Since rank of parameter decoder/layers_23/extra_mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.282668 139812357228544 adafactor.py:358] Since rank of parameter decoder/layers_23/extra_mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:11.286806 139812357228544 adafactor.py:358] Since rank of parameter decoder/layers_23/mlp/router/router_weights/w/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>, <FactorDim.NONE: None>) is ignored.
W0512 01:45:11.287694 139812357228544 adafactor.py:358] Since rank of parameter decoder/layers_23/pre_extra_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:11.288609 139812357228544 adafactor.py:358] Since rank of parameter decoder/layers_23/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:11.332604 140392160929792 adafactor.py:358] Since rank of parameter decoder/logits_dense/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.271052 139997397456896 adafactor.py:358] Since rank of parameter decoder/layers_18/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:11.271959 139997397456896 adafactor.py:358] Since rank of parameter decoder/layers_18/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:11.272871 139997397456896 adafactor.py:358] Since rank of parameter decoder/layers_18/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.274007 139997397456896 adafactor.py:358] Since rank of parameter decoder/layers_18/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:11.275170 139997397456896 adafactor.py:358] Since rank of parameter decoder/layers_18/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.276292 139997397456896 adafactor.py:358] Since rank of parameter decoder/layers_18/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.277443 139997397456896 adafactor.py:358] Since rank of parameter decoder/layers_19/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.278570 139997397456896 adafactor.py:358] Since rank of parameter decoder/layers_19/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.279728 139997397456896 adafactor.py:358] Since rank of parameter decoder/layers_19/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:11.333704 140392160929792 adafactor.py:358] Since rank of parameter token_embedder/embedding 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:11.289482 139812357228544 adafactor.py:358] Since rank of parameter decoder/layers_23/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:11.290405 139812357228544 adafactor.py:358] Since rank of parameter decoder/layers_23/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.291479 139812357228544 adafactor.py:358] Since rank of parameter decoder/layers_23/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:11.292737 139812357228544 adafactor.py:358] Since rank of parameter decoder/layers_23/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.293858 139812357228544 adafactor.py:358] Since rank of parameter decoder/layers_23/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.295008 139812357228544 adafactor.py:358] Since rank of parameter decoder/layers_3/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.296126 139812357228544 adafactor.py:358] Since rank of parameter decoder/layers_3/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.297267 139812357228544 adafactor.py:358] Since rank of parameter decoder/layers_3/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:11.298342 139812357228544 adafactor.py:358] Since rank of parameter decoder/layers_3/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:11.280836 139997397456896 adafactor.py:358] Since rank of parameter decoder/layers_19/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:11.281748 139997397456896 adafactor.py:358] Since rank of parameter decoder/layers_19/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:11.282622 139997397456896 adafactor.py:358] Since rank of parameter decoder/layers_19/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.283795 139997397456896 adafactor.py:358] Since rank of parameter decoder/layers_19/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:11.284867 139997397456896 adafactor.py:358] Since rank of parameter decoder/layers_19/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.285965 139997397456896 adafactor.py:358] Since rank of parameter decoder/layers_19/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.287042 139997397456896 adafactor.py:358] Since rank of parameter decoder/layers_2/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.288164 139997397456896 adafactor.py:358] Since rank of parameter decoder/layers_2/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.299230 139812357228544 adafactor.py:358] Since rank of parameter decoder/layers_3/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:11.289271 139997397456896 adafactor.py:358] Since rank of parameter decoder/layers_2/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:11.290344 139997397456896 adafactor.py:358] Since rank of parameter decoder/layers_2/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:11.291199 139997397456896 adafactor.py:358] Since rank of parameter decoder/layers_2/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:11.292125 139997397456896 adafactor.py:358] Since rank of parameter decoder/layers_2/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.293229 139997397456896 adafactor.py:358] Since rank of parameter decoder/layers_2/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:11.294314 139997397456896 adafactor.py:358] Since rank of parameter decoder/layers_2/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.295386 139997397456896 adafactor.py:358] Since rank of parameter decoder/layers_2/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.296501 139997397456896 adafactor.py:358] Since rank of parameter decoder/layers_20/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.297610 139997397456896 adafactor.py:358] Since rank of parameter decoder/layers_20/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.298689 139997397456896 adafactor.py:358] Since rank of parameter decoder/layers_20/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:11.299748 139997397456896 adafactor.py:358] Since rank of parameter decoder/layers_20/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:11.300688 139997397456896 adafactor.py:358] Since rank of parameter decoder/layers_20/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:11.301599 139997397456896 adafactor.py:358] Since rank of parameter decoder/layers_20/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.302662 139997397456896 adafactor.py:358] Since rank of parameter decoder/layers_20/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:11.303776 139997397456896 adafactor.py:358] Since rank of parameter decoder/layers_20/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.304894 139997397456896 adafactor.py:358] Since rank of parameter decoder/layers_20/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.305988 139997397456896 adafactor.py:358] Since rank of parameter decoder/layers_21/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.307078 139997397456896 adafactor.py:358] Since rank of parameter decoder/layers_21/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.300189 139812357228544 adafactor.py:358] Since rank of parameter decoder/layers_3/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.301277 139812357228544 adafactor.py:358] Since rank of parameter decoder/layers_3/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:11.302350 139812357228544 adafactor.py:358] Since rank of parameter decoder/layers_3/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.303414 139812357228544 adafactor.py:358] Since rank of parameter decoder/layers_3/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.304583 139812357228544 adafactor.py:358] Since rank of parameter decoder/layers_4/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.305660 139812357228544 adafactor.py:358] Since rank of parameter decoder/layers_4/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.306732 139812357228544 adafactor.py:358] Since rank of parameter decoder/layers_4/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:11.307806 139812357228544 adafactor.py:358] Since rank of parameter decoder/layers_4/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:11.308147 139997397456896 adafactor.py:358] Since rank of parameter decoder/layers_21/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:11.309307 139997397456896 adafactor.py:358] Since rank of parameter decoder/layers_21/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:11.310190 139997397456896 adafactor.py:358] Since rank of parameter decoder/layers_21/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:11.311064 139997397456896 adafactor.py:358] Since rank of parameter decoder/layers_21/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.312143 139997397456896 adafactor.py:358] Since rank of parameter decoder/layers_21/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:11.313254 139997397456896 adafactor.py:358] Since rank of parameter decoder/layers_21/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.314387 139997397456896 adafactor.py:358] Since rank of parameter decoder/layers_21/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.315441 139997397456896 adafactor.py:358] Since rank of parameter decoder/layers_22/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.308772 139812357228544 adafactor.py:358] Since rank of parameter decoder/layers_4/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:11.309637 139812357228544 adafactor.py:358] Since rank of parameter decoder/layers_4/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.310713 139812357228544 adafactor.py:358] Since rank of parameter decoder/layers_4/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:11.311826 139812357228544 adafactor.py:358] Since rank of parameter decoder/layers_4/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.313009 139812357228544 adafactor.py:358] Since rank of parameter decoder/layers_4/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.314101 139812357228544 adafactor.py:358] Since rank of parameter decoder/layers_5/extra_mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.245048 140172769531904 adafactor.py:358] Since rank of parameter decoder/layers_14/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.246128 140172769531904 adafactor.py:358] Since rank of parameter decoder/layers_14/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.247221 140172769531904 adafactor.py:358] Since rank of parameter decoder/layers_15/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.248291 140172769531904 adafactor.py:358] Since rank of parameter decoder/layers_15/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.249453 140172769531904 adafactor.py:358] Since rank of parameter decoder/layers_15/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:11.250533 140172769531904 adafactor.py:358] Since rank of parameter decoder/layers_15/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:11.251414 140172769531904 adafactor.py:358] Since rank of parameter decoder/layers_15/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:11.252309 140172769531904 adafactor.py:358] Since rank of parameter decoder/layers_15/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.315178 139812357228544 adafactor.py:358] Since rank of parameter decoder/layers_5/extra_mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.316507 139997397456896 adafactor.py:358] Since rank of parameter decoder/layers_22/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.317600 139997397456896 adafactor.py:358] Since rank of parameter decoder/layers_22/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:11.319190 139997397456896 adafactor.py:358] Since rank of parameter decoder/layers_22/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:11.320080 139997397456896 adafactor.py:358] Since rank of parameter decoder/layers_22/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:11.320976 139997397456896 adafactor.py:358] Since rank of parameter decoder/layers_22/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.322054 139997397456896 adafactor.py:358] Since rank of parameter decoder/layers_22/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:11.323186 139997397456896 adafactor.py:358] Since rank of parameter decoder/layers_22/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.324277 139997397456896 adafactor.py:358] Since rank of parameter decoder/layers_22/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.325389 139997397456896 adafactor.py:358] Since rank of parameter decoder/layers_23/extra_mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.253470 140172769531904 adafactor.py:358] Since rank of parameter decoder/layers_15/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:11.254561 140172769531904 adafactor.py:358] Since rank of parameter decoder/layers_15/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.255654 140172769531904 adafactor.py:358] Since rank of parameter decoder/layers_15/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.256778 140172769531904 adafactor.py:358] Since rank of parameter decoder/layers_16/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.257916 140172769531904 adafactor.py:358] Since rank of parameter decoder/layers_16/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.259039 140172769531904 adafactor.py:358] Since rank of parameter decoder/layers_16/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:11.260124 140172769531904 adafactor.py:358] Since rank of parameter decoder/layers_16/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:11.261046 140172769531904 adafactor.py:358] Since rank of parameter decoder/layers_16/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:11.261995 140172769531904 adafactor.py:358] Since rank of parameter decoder/layers_16/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.316279 139812357228544 adafactor.py:358] Since rank of parameter decoder/layers_5/extra_mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:11.326470 139997397456896 adafactor.py:358] Since rank of parameter decoder/layers_23/extra_mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.263081 140172769531904 adafactor.py:358] Since rank of parameter decoder/layers_16/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:11.264159 140172769531904 adafactor.py:358] Since rank of parameter decoder/layers_16/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.265293 140172769531904 adafactor.py:358] Since rank of parameter decoder/layers_16/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.266428 140172769531904 adafactor.py:358] Since rank of parameter decoder/layers_17/extra_mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.267513 140172769531904 adafactor.py:358] Since rank of parameter decoder/layers_17/extra_mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.268624 140172769531904 adafactor.py:358] Since rank of parameter decoder/layers_17/extra_mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:11.272712 140172769531904 adafactor.py:358] Since rank of parameter decoder/layers_17/mlp/router/router_weights/w/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>, <FactorDim.NONE: None>) is ignored.
W0512 01:45:11.273616 140172769531904 adafactor.py:358] Since rank of parameter decoder/layers_17/pre_extra_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:11.327598 139997397456896 adafactor.py:358] Since rank of parameter decoder/layers_23/extra_mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:11.331640 139997397456896 adafactor.py:358] Since rank of parameter decoder/layers_23/mlp/router/router_weights/w/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>, <FactorDim.NONE: None>) is ignored.
W0512 01:45:11.332543 139997397456896 adafactor.py:358] Since rank of parameter decoder/layers_23/pre_extra_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:11.333441 139997397456896 adafactor.py:358] Since rank of parameter decoder/layers_23/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:11.334303 139997397456896 adafactor.py:358] Since rank of parameter decoder/layers_23/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:11.335226 139997397456896 adafactor.py:358] Since rank of parameter decoder/layers_23/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.336295 139997397456896 adafactor.py:358] Since rank of parameter decoder/layers_23/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:11.337548 139997397456896 adafactor.py:358] Since rank of parameter decoder/layers_23/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.274590 140172769531904 adafactor.py:358] Since rank of parameter decoder/layers_17/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:11.275516 140172769531904 adafactor.py:358] Since rank of parameter decoder/layers_17/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:11.276466 140172769531904 adafactor.py:358] Since rank of parameter decoder/layers_17/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.277604 140172769531904 adafactor.py:358] Since rank of parameter decoder/layers_17/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:11.278744 140172769531904 adafactor.py:358] Since rank of parameter decoder/layers_17/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.280325 140172769531904 adafactor.py:358] Since rank of parameter decoder/layers_17/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.281445 140172769531904 adafactor.py:358] Since rank of parameter decoder/layers_18/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.282522 140172769531904 adafactor.py:358] Since rank of parameter decoder/layers_18/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.283654 140172769531904 adafactor.py:358] Since rank of parameter decoder/layers_18/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:11.338600 139997397456896 adafactor.py:358] Since rank of parameter decoder/layers_23/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.284783 140172769531904 adafactor.py:358] Since rank of parameter decoder/layers_18/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:11.285665 140172769531904 adafactor.py:358] Since rank of parameter decoder/layers_18/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:11.286548 140172769531904 adafactor.py:358] Since rank of parameter decoder/layers_18/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.287619 140172769531904 adafactor.py:358] Since rank of parameter decoder/layers_18/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:11.288787 140172769531904 adafactor.py:358] Since rank of parameter decoder/layers_18/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.289873 140172769531904 adafactor.py:358] Since rank of parameter decoder/layers_18/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.290942 140172769531904 adafactor.py:358] Since rank of parameter decoder/layers_19/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.292016 140172769531904 adafactor.py:358] Since rank of parameter decoder/layers_19/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.293170 140172769531904 adafactor.py:358] Since rank of parameter decoder/layers_19/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:11.339689 139997397456896 adafactor.py:358] Since rank of parameter decoder/layers_3/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.294265 140172769531904 adafactor.py:358] Since rank of parameter decoder/layers_19/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:11.295129 140172769531904 adafactor.py:358] Since rank of parameter decoder/layers_19/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:11.296004 140172769531904 adafactor.py:358] Since rank of parameter decoder/layers_19/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.297163 140172769531904 adafactor.py:358] Since rank of parameter decoder/layers_19/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:11.298254 140172769531904 adafactor.py:358] Since rank of parameter decoder/layers_19/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.299363 140172769531904 adafactor.py:358] Since rank of parameter decoder/layers_19/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.300461 140172769531904 adafactor.py:358] Since rank of parameter decoder/layers_2/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.301598 140172769531904 adafactor.py:358] Since rank of parameter decoder/layers_2/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.320332 139812357228544 adafactor.py:358] Since rank of parameter decoder/layers_5/mlp/router/router_weights/w/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>, <FactorDim.NONE: None>) is ignored.
W0512 01:45:11.340751 139997397456896 adafactor.py:358] Since rank of parameter decoder/layers_3/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.302674 140172769531904 adafactor.py:358] Since rank of parameter decoder/layers_2/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:11.303755 140172769531904 adafactor.py:358] Since rank of parameter decoder/layers_2/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:11.304680 140172769531904 adafactor.py:358] Since rank of parameter decoder/layers_2/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:11.305627 140172769531904 adafactor.py:358] Since rank of parameter decoder/layers_2/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.306709 140172769531904 adafactor.py:358] Since rank of parameter decoder/layers_2/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:11.307800 140172769531904 adafactor.py:358] Since rank of parameter decoder/layers_2/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.308909 140172769531904 adafactor.py:358] Since rank of parameter decoder/layers_2/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.310046 140172769531904 adafactor.py:358] Since rank of parameter decoder/layers_20/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.311122 140172769531904 adafactor.py:358] Since rank of parameter decoder/layers_20/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.321286 139812357228544 adafactor.py:358] Since rank of parameter decoder/layers_5/pre_extra_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:11.341832 139997397456896 adafactor.py:358] Since rank of parameter decoder/layers_3/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:11.312220 140172769531904 adafactor.py:358] Since rank of parameter decoder/layers_20/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:11.313321 140172769531904 adafactor.py:358] Since rank of parameter decoder/layers_20/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:11.314276 140172769531904 adafactor.py:358] Since rank of parameter decoder/layers_20/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:11.315151 140172769531904 adafactor.py:358] Since rank of parameter decoder/layers_20/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.316251 140172769531904 adafactor.py:358] Since rank of parameter decoder/layers_20/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:11.317413 140172769531904 adafactor.py:358] Since rank of parameter decoder/layers_20/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.318610 140172769531904 adafactor.py:358] Since rank of parameter decoder/layers_20/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.319693 140172769531904 adafactor.py:358] Since rank of parameter decoder/layers_21/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.320823 140172769531904 adafactor.py:358] Since rank of parameter decoder/layers_21/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.322165 139812357228544 adafactor.py:358] Since rank of parameter decoder/layers_5/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:11.342907 139997397456896 adafactor.py:358] Since rank of parameter decoder/layers_3/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:11.323041 139812357228544 adafactor.py:358] Since rank of parameter decoder/layers_5/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:11.321901 140172769531904 adafactor.py:358] Since rank of parameter decoder/layers_21/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:11.323051 140172769531904 adafactor.py:358] Since rank of parameter decoder/layers_21/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:11.323979 140172769531904 adafactor.py:358] Since rank of parameter decoder/layers_21/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:11.324920 140172769531904 adafactor.py:358] Since rank of parameter decoder/layers_21/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.326060 140172769531904 adafactor.py:358] Since rank of parameter decoder/layers_21/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:11.327158 140172769531904 adafactor.py:358] Since rank of parameter decoder/layers_21/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.328304 140172769531904 adafactor.py:358] Since rank of parameter decoder/layers_21/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.329441 140172769531904 adafactor.py:358] Since rank of parameter decoder/layers_22/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.343813 139997397456896 adafactor.py:358] Since rank of parameter decoder/layers_3/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:11.323944 139812357228544 adafactor.py:358] Since rank of parameter decoder/layers_5/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.344757 139997397456896 adafactor.py:358] Since rank of parameter decoder/layers_3/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.330535 140172769531904 adafactor.py:358] Since rank of parameter decoder/layers_22/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.331624 140172769531904 adafactor.py:358] Since rank of parameter decoder/layers_22/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:11.333229 140172769531904 adafactor.py:358] Since rank of parameter decoder/layers_22/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:11.334114 140172769531904 adafactor.py:358] Since rank of parameter decoder/layers_22/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:11.334988 140172769531904 adafactor.py:358] Since rank of parameter decoder/layers_22/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.336075 140172769531904 adafactor.py:358] Since rank of parameter decoder/layers_22/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:11.337231 140172769531904 adafactor.py:358] Since rank of parameter decoder/layers_22/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.338308 140172769531904 adafactor.py:358] Since rank of parameter decoder/layers_22/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.339429 140172769531904 adafactor.py:358] Since rank of parameter decoder/layers_23/extra_mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.325536 139812357228544 adafactor.py:358] Since rank of parameter decoder/layers_5/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:11.345857 139997397456896 adafactor.py:358] Since rank of parameter decoder/layers_3/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:11.340592 140172769531904 adafactor.py:358] Since rank of parameter decoder/layers_23/extra_mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.341766 140172769531904 adafactor.py:358] Since rank of parameter decoder/layers_23/extra_mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:11.345868 140172769531904 adafactor.py:358] Since rank of parameter decoder/layers_23/mlp/router/router_weights/w/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>, <FactorDim.NONE: None>) is ignored.
W0512 01:45:11.346776 140172769531904 adafactor.py:358] Since rank of parameter decoder/layers_23/pre_extra_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:11.347666 140172769531904 adafactor.py:358] Since rank of parameter decoder/layers_23/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:11.326614 139812357228544 adafactor.py:358] Since rank of parameter decoder/layers_5/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.346928 139997397456896 adafactor.py:358] Since rank of parameter decoder/layers_3/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.348577 140172769531904 adafactor.py:358] Since rank of parameter decoder/layers_23/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:11.349526 140172769531904 adafactor.py:358] Since rank of parameter decoder/layers_23/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.327685 139812357228544 adafactor.py:358] Since rank of parameter decoder/layers_5/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.348003 139997397456896 adafactor.py:358] Since rank of parameter decoder/layers_3/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.350604 140172769531904 adafactor.py:358] Since rank of parameter decoder/layers_23/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:11.328781 139812357228544 adafactor.py:358] Since rank of parameter decoder/layers_6/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.349138 139997397456896 adafactor.py:358] Since rank of parameter decoder/layers_4/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.351839 140172769531904 adafactor.py:358] Since rank of parameter decoder/layers_23/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.350210 139997397456896 adafactor.py:358] Since rank of parameter decoder/layers_4/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.329905 139812357228544 adafactor.py:358] Since rank of parameter decoder/layers_6/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.352936 140172769531904 adafactor.py:358] Since rank of parameter decoder/layers_23/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.351282 139997397456896 adafactor.py:358] Since rank of parameter decoder/layers_4/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:11.330976 139812357228544 adafactor.py:358] Since rank of parameter decoder/layers_6/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:11.354092 140172769531904 adafactor.py:358] Since rank of parameter decoder/layers_3/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.352349 139997397456896 adafactor.py:358] Since rank of parameter decoder/layers_4/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:11.332051 139812357228544 adafactor.py:358] Since rank of parameter decoder/layers_6/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:11.355193 140172769531904 adafactor.py:358] Since rank of parameter decoder/layers_3/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.332979 139812357228544 adafactor.py:358] Since rank of parameter decoder/layers_6/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:11.353291 139997397456896 adafactor.py:358] Since rank of parameter decoder/layers_4/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:11.354153 139997397456896 adafactor.py:358] Since rank of parameter decoder/layers_4/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.333862 139812357228544 adafactor.py:358] Since rank of parameter decoder/layers_6/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.356304 140172769531904 adafactor.py:358] Since rank of parameter decoder/layers_3/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:11.334982 139812357228544 adafactor.py:358] Since rank of parameter decoder/layers_6/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:11.355221 139997397456896 adafactor.py:358] Since rank of parameter decoder/layers_4/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:11.357445 140172769531904 adafactor.py:358] Since rank of parameter decoder/layers_3/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:11.336062 139812357228544 adafactor.py:358] Since rank of parameter decoder/layers_6/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.356380 139997397456896 adafactor.py:358] Since rank of parameter decoder/layers_4/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.358353 140172769531904 adafactor.py:358] Since rank of parameter decoder/layers_3/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:11.359300 140172769531904 adafactor.py:358] Since rank of parameter decoder/layers_3/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.357633 139997397456896 adafactor.py:358] Since rank of parameter decoder/layers_4/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.337692 139812357228544 adafactor.py:358] Since rank of parameter decoder/layers_6/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.360371 140172769531904 adafactor.py:358] Since rank of parameter decoder/layers_3/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:11.358722 139997397456896 adafactor.py:358] Since rank of parameter decoder/layers_5/extra_mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.338760 139812357228544 adafactor.py:358] Since rank of parameter decoder/layers_7/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.361455 140172769531904 adafactor.py:358] Since rank of parameter decoder/layers_3/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.359783 139997397456896 adafactor.py:358] Since rank of parameter decoder/layers_5/extra_mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.339884 139812357228544 adafactor.py:358] Since rank of parameter decoder/layers_7/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.362526 140172769531904 adafactor.py:358] Since rank of parameter decoder/layers_3/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.360862 139997397456896 adafactor.py:358] Since rank of parameter decoder/layers_5/extra_mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:11.340978 139812357228544 adafactor.py:358] Since rank of parameter decoder/layers_7/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:11.363637 140172769531904 adafactor.py:358] Since rank of parameter decoder/layers_4/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.342040 139812357228544 adafactor.py:358] Since rank of parameter decoder/layers_7/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:11.364737 140172769531904 adafactor.py:358] Since rank of parameter decoder/layers_4/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.342923 139812357228544 adafactor.py:358] Since rank of parameter decoder/layers_7/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:11.365826 140172769531904 adafactor.py:358] Since rank of parameter decoder/layers_4/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:11.343880 139812357228544 adafactor.py:358] Since rank of parameter decoder/layers_7/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.364931 139997397456896 adafactor.py:358] Since rank of parameter decoder/layers_5/mlp/router/router_weights/w/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>, <FactorDim.NONE: None>) is ignored.
W0512 01:45:11.366899 140172769531904 adafactor.py:358] Since rank of parameter decoder/layers_4/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:11.344981 139812357228544 adafactor.py:358] Since rank of parameter decoder/layers_7/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:11.367827 140172769531904 adafactor.py:358] Since rank of parameter decoder/layers_4/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:11.365917 139997397456896 adafactor.py:358] Since rank of parameter decoder/layers_5/pre_extra_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:11.346063 139812357228544 adafactor.py:358] Since rank of parameter decoder/layers_7/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.368737 140172769531904 adafactor.py:358] Since rank of parameter decoder/layers_4/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.366791 139997397456896 adafactor.py:358] Since rank of parameter decoder/layers_5/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:11.347126 139812357228544 adafactor.py:358] Since rank of parameter decoder/layers_7/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.369805 140172769531904 adafactor.py:358] Since rank of parameter decoder/layers_4/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:11.367676 139997397456896 adafactor.py:358] Since rank of parameter decoder/layers_5/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:11.348283 139812357228544 adafactor.py:358] Since rank of parameter decoder/layers_8/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.368553 139997397456896 adafactor.py:358] Since rank of parameter decoder/layers_5/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.370881 140172769531904 adafactor.py:358] Since rank of parameter decoder/layers_4/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.349367 139812357228544 adafactor.py:358] Since rank of parameter decoder/layers_8/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.372004 140172769531904 adafactor.py:358] Since rank of parameter decoder/layers_4/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.370245 139997397456896 adafactor.py:358] Since rank of parameter decoder/layers_5/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:11.350440 139812357228544 adafactor.py:358] Since rank of parameter decoder/layers_8/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:11.373116 140172769531904 adafactor.py:358] Since rank of parameter decoder/layers_5/extra_mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.371326 139997397456896 adafactor.py:358] Since rank of parameter decoder/layers_5/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.351510 139812357228544 adafactor.py:358] Since rank of parameter decoder/layers_8/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:11.374178 140172769531904 adafactor.py:358] Since rank of parameter decoder/layers_5/extra_mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.372378 139997397456896 adafactor.py:358] Since rank of parameter decoder/layers_5/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.352494 139812357228544 adafactor.py:358] Since rank of parameter decoder/layers_8/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:11.375253 140172769531904 adafactor.py:358] Since rank of parameter decoder/layers_5/extra_mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:11.353384 139812357228544 adafactor.py:358] Since rank of parameter decoder/layers_8/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.373459 139997397456896 adafactor.py:358] Since rank of parameter decoder/layers_6/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.280819 140577131780096 adafactor.py:358] Since rank of parameter decoder/layers_14/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:11.281955 140577131780096 adafactor.py:358] Since rank of parameter decoder/layers_14/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.283074 140577131780096 adafactor.py:358] Since rank of parameter decoder/layers_14/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.284214 140577131780096 adafactor.py:358] Since rank of parameter decoder/layers_15/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.285294 140577131780096 adafactor.py:358] Since rank of parameter decoder/layers_15/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.286431 140577131780096 adafactor.py:358] Since rank of parameter decoder/layers_15/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:11.287555 140577131780096 adafactor.py:358] Since rank of parameter decoder/layers_15/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:11.288461 140577131780096 adafactor.py:358] Since rank of parameter decoder/layers_15/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:11.374570 139997397456896 adafactor.py:358] Since rank of parameter decoder/layers_6/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.354451 139812357228544 adafactor.py:358] Since rank of parameter decoder/layers_8/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:11.289365 140577131780096 adafactor.py:358] Since rank of parameter decoder/layers_15/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.290515 140577131780096 adafactor.py:358] Since rank of parameter decoder/layers_15/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:11.291643 140577131780096 adafactor.py:358] Since rank of parameter decoder/layers_15/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.292744 140577131780096 adafactor.py:358] Since rank of parameter decoder/layers_15/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.293833 140577131780096 adafactor.py:358] Since rank of parameter decoder/layers_16/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.295010 140577131780096 adafactor.py:358] Since rank of parameter decoder/layers_16/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.296139 140577131780096 adafactor.py:358] Since rank of parameter decoder/layers_16/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:11.297233 140577131780096 adafactor.py:358] Since rank of parameter decoder/layers_16/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:11.298135 140577131780096 adafactor.py:358] Since rank of parameter decoder/layers_16/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:11.375625 139997397456896 adafactor.py:358] Since rank of parameter decoder/layers_6/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:11.355528 139812357228544 adafactor.py:358] Since rank of parameter decoder/layers_8/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.299090 140577131780096 adafactor.py:358] Since rank of parameter decoder/layers_16/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.300218 140577131780096 adafactor.py:358] Since rank of parameter decoder/layers_16/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:11.301319 140577131780096 adafactor.py:358] Since rank of parameter decoder/layers_16/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.302457 140577131780096 adafactor.py:358] Since rank of parameter decoder/layers_16/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.303644 140577131780096 adafactor.py:358] Since rank of parameter decoder/layers_17/extra_mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.304748 140577131780096 adafactor.py:358] Since rank of parameter decoder/layers_17/extra_mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.305860 140577131780096 adafactor.py:358] Since rank of parameter decoder/layers_17/extra_mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:11.310014 140577131780096 adafactor.py:358] Since rank of parameter decoder/layers_17/mlp/router/router_weights/w/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>, <FactorDim.NONE: None>) is ignored.
W0512 01:45:11.376668 139997397456896 adafactor.py:358] Since rank of parameter decoder/layers_6/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:11.356690 139812357228544 adafactor.py:358] Since rank of parameter decoder/layers_8/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.379348 140172769531904 adafactor.py:358] Since rank of parameter decoder/layers_5/mlp/router/router_weights/w/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>, <FactorDim.NONE: None>) is ignored.
W0512 01:45:11.310926 140577131780096 adafactor.py:358] Since rank of parameter decoder/layers_17/pre_extra_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:11.311913 140577131780096 adafactor.py:358] Since rank of parameter decoder/layers_17/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:11.312811 140577131780096 adafactor.py:358] Since rank of parameter decoder/layers_17/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:11.313706 140577131780096 adafactor.py:358] Since rank of parameter decoder/layers_17/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.314797 140577131780096 adafactor.py:358] Since rank of parameter decoder/layers_17/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:11.315946 140577131780096 adafactor.py:358] Since rank of parameter decoder/layers_17/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.317517 140577131780096 adafactor.py:358] Since rank of parameter decoder/layers_17/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.318611 140577131780096 adafactor.py:358] Since rank of parameter decoder/layers_18/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.319749 140577131780096 adafactor.py:358] Since rank of parameter decoder/layers_18/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.377568 139997397456896 adafactor.py:358] Since rank of parameter decoder/layers_6/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:11.380296 140172769531904 adafactor.py:358] Since rank of parameter decoder/layers_5/pre_extra_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:11.357786 139812357228544 adafactor.py:358] Since rank of parameter decoder/layers_9/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.378438 139997397456896 adafactor.py:358] Since rank of parameter decoder/layers_6/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.320899 140577131780096 adafactor.py:358] Since rank of parameter decoder/layers_18/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:11.321997 140577131780096 adafactor.py:358] Since rank of parameter decoder/layers_18/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:11.322922 140577131780096 adafactor.py:358] Since rank of parameter decoder/layers_18/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:11.323854 140577131780096 adafactor.py:358] Since rank of parameter decoder/layers_18/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.324946 140577131780096 adafactor.py:358] Since rank of parameter decoder/layers_18/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:11.326095 140577131780096 adafactor.py:358] Since rank of parameter decoder/layers_18/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.327195 140577131780096 adafactor.py:358] Since rank of parameter decoder/layers_18/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.328314 140577131780096 adafactor.py:358] Since rank of parameter decoder/layers_19/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.329415 140577131780096 adafactor.py:358] Since rank of parameter decoder/layers_19/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.381187 140172769531904 adafactor.py:358] Since rank of parameter decoder/layers_5/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:11.358867 139812357228544 adafactor.py:358] Since rank of parameter decoder/layers_9/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.379542 139997397456896 adafactor.py:358] Since rank of parameter decoder/layers_6/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:11.330555 140577131780096 adafactor.py:358] Since rank of parameter decoder/layers_19/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:11.331678 140577131780096 adafactor.py:358] Since rank of parameter decoder/layers_19/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:11.332576 140577131780096 adafactor.py:358] Since rank of parameter decoder/layers_19/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:11.333469 140577131780096 adafactor.py:358] Since rank of parameter decoder/layers_19/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.334624 140577131780096 adafactor.py:358] Since rank of parameter decoder/layers_19/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:11.335757 140577131780096 adafactor.py:358] Since rank of parameter decoder/layers_19/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.336861 140577131780096 adafactor.py:358] Since rank of parameter decoder/layers_19/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.337961 140577131780096 adafactor.py:358] Since rank of parameter decoder/layers_2/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.382073 140172769531904 adafactor.py:358] Since rank of parameter decoder/layers_5/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:11.339107 140577131780096 adafactor.py:358] Since rank of parameter decoder/layers_2/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.340228 140577131780096 adafactor.py:358] Since rank of parameter decoder/layers_2/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:11.341304 140577131780096 adafactor.py:358] Since rank of parameter decoder/layers_2/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:11.342219 140577131780096 adafactor.py:358] Since rank of parameter decoder/layers_2/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:11.343168 140577131780096 adafactor.py:358] Since rank of parameter decoder/layers_2/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.344275 140577131780096 adafactor.py:358] Since rank of parameter decoder/layers_2/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:11.345375 140577131780096 adafactor.py:358] Since rank of parameter decoder/layers_2/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.346465 140577131780096 adafactor.py:358] Since rank of parameter decoder/layers_2/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.347635 140577131780096 adafactor.py:358] Since rank of parameter decoder/layers_20/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.359932 139812357228544 adafactor.py:358] Since rank of parameter decoder/layers_9/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:11.380600 139997397456896 adafactor.py:358] Since rank of parameter decoder/layers_6/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.382945 140172769531904 adafactor.py:358] Since rank of parameter decoder/layers_5/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.348743 140577131780096 adafactor.py:358] Since rank of parameter decoder/layers_20/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.349837 140577131780096 adafactor.py:358] Since rank of parameter decoder/layers_20/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:11.350921 140577131780096 adafactor.py:358] Since rank of parameter decoder/layers_20/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:11.351910 140577131780096 adafactor.py:358] Since rank of parameter decoder/layers_20/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:11.352806 140577131780096 adafactor.py:358] Since rank of parameter decoder/layers_20/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.353912 140577131780096 adafactor.py:358] Since rank of parameter decoder/layers_20/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:11.355047 140577131780096 adafactor.py:358] Since rank of parameter decoder/layers_20/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.356212 140577131780096 adafactor.py:358] Since rank of parameter decoder/layers_20/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.357301 140577131780096 adafactor.py:358] Since rank of parameter decoder/layers_21/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.361083 139812357228544 adafactor.py:358] Since rank of parameter decoder/layers_9/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:11.381688 139997397456896 adafactor.py:358] Since rank of parameter decoder/layers_6/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.361971 139812357228544 adafactor.py:358] Since rank of parameter decoder/layers_9/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:11.384477 140172769531904 adafactor.py:358] Since rank of parameter decoder/layers_5/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:11.382778 139997397456896 adafactor.py:358] Since rank of parameter decoder/layers_7/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.362849 139812357228544 adafactor.py:358] Since rank of parameter decoder/layers_9/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.358387 140577131780096 adafactor.py:358] Since rank of parameter decoder/layers_21/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.359499 140577131780096 adafactor.py:358] Since rank of parameter decoder/layers_21/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:11.360637 140577131780096 adafactor.py:358] Since rank of parameter decoder/layers_21/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:11.361544 140577131780096 adafactor.py:358] Since rank of parameter decoder/layers_21/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:11.362474 140577131780096 adafactor.py:358] Since rank of parameter decoder/layers_21/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.363605 140577131780096 adafactor.py:358] Since rank of parameter decoder/layers_21/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:11.364695 140577131780096 adafactor.py:358] Since rank of parameter decoder/layers_21/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.365841 140577131780096 adafactor.py:358] Since rank of parameter decoder/layers_21/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.385572 140172769531904 adafactor.py:358] Since rank of parameter decoder/layers_5/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.383968 139997397456896 adafactor.py:358] Since rank of parameter decoder/layers_7/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.366934 140577131780096 adafactor.py:358] Since rank of parameter decoder/layers_22/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.368057 140577131780096 adafactor.py:358] Since rank of parameter decoder/layers_22/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.369157 140577131780096 adafactor.py:358] Since rank of parameter decoder/layers_22/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:11.370731 140577131780096 adafactor.py:358] Since rank of parameter decoder/layers_22/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:11.371664 140577131780096 adafactor.py:358] Since rank of parameter decoder/layers_22/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:11.372552 140577131780096 adafactor.py:358] Since rank of parameter decoder/layers_22/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.373628 140577131780096 adafactor.py:358] Since rank of parameter decoder/layers_22/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:11.374790 140577131780096 adafactor.py:358] Since rank of parameter decoder/layers_22/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.375910 140577131780096 adafactor.py:358] Since rank of parameter decoder/layers_22/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.363956 139812357228544 adafactor.py:358] Since rank of parameter decoder/layers_9/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:11.386641 140172769531904 adafactor.py:358] Since rank of parameter decoder/layers_5/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.377006 140577131780096 adafactor.py:358] Since rank of parameter decoder/layers_23/extra_mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.378111 140577131780096 adafactor.py:358] Since rank of parameter decoder/layers_23/extra_mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.379260 140577131780096 adafactor.py:358] Since rank of parameter decoder/layers_23/extra_mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:11.383407 140577131780096 adafactor.py:358] Since rank of parameter decoder/layers_23/mlp/router/router_weights/w/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>, <FactorDim.NONE: None>) is ignored.
W0512 01:45:11.384317 140577131780096 adafactor.py:358] Since rank of parameter decoder/layers_23/pre_extra_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:11.385195 140577131780096 adafactor.py:358] Since rank of parameter decoder/layers_23/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:11.385111 139997397456896 adafactor.py:358] Since rank of parameter decoder/layers_7/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:11.365117 139812357228544 adafactor.py:358] Since rank of parameter decoder/layers_9/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.387709 140172769531904 adafactor.py:358] Since rank of parameter decoder/layers_6/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.386210 139997397456896 adafactor.py:358] Since rank of parameter decoder/layers_7/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:11.386074 140577131780096 adafactor.py:358] Since rank of parameter decoder/layers_23/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:11.366191 139812357228544 adafactor.py:358] Since rank of parameter decoder/layers_9/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.386998 140577131780096 adafactor.py:358] Since rank of parameter decoder/layers_23/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.388875 140172769531904 adafactor.py:358] Since rank of parameter decoder/layers_6/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.387096 139997397456896 adafactor.py:358] Since rank of parameter decoder/layers_7/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:11.367258 139812357228544 adafactor.py:358] Since rank of parameter decoder/logits_dense/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.388019 139997397456896 adafactor.py:358] Since rank of parameter decoder/layers_7/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.388122 140577131780096 adafactor.py:358] Since rank of parameter decoder/layers_23/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:11.389938 140172769531904 adafactor.py:358] Since rank of parameter decoder/layers_6/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:11.368368 139812357228544 adafactor.py:358] Since rank of parameter token_embedder/embedding 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:11.391014 140172769531904 adafactor.py:358] Since rank of parameter decoder/layers_6/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:11.389109 139997397456896 adafactor.py:358] Since rank of parameter decoder/layers_7/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:11.389371 140577131780096 adafactor.py:358] Since rank of parameter decoder/layers_23/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.391897 140172769531904 adafactor.py:358] Since rank of parameter decoder/layers_6/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:11.390184 139997397456896 adafactor.py:358] Since rank of parameter decoder/layers_7/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.390450 140577131780096 adafactor.py:358] Since rank of parameter decoder/layers_23/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.392807 140172769531904 adafactor.py:358] Since rank of parameter decoder/layers_6/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.391237 139997397456896 adafactor.py:358] Since rank of parameter decoder/layers_7/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.391604 140577131780096 adafactor.py:358] Since rank of parameter decoder/layers_3/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.393927 140172769531904 adafactor.py:358] Since rank of parameter decoder/layers_6/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:11.392688 140577131780096 adafactor.py:358] Since rank of parameter decoder/layers_3/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.392358 139997397456896 adafactor.py:358] Since rank of parameter decoder/layers_8/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.395002 140172769531904 adafactor.py:358] Since rank of parameter decoder/layers_6/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.393763 140577131780096 adafactor.py:358] Since rank of parameter decoder/layers_3/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:11.393449 139997397456896 adafactor.py:358] Since rank of parameter decoder/layers_8/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.396077 140172769531904 adafactor.py:358] Since rank of parameter decoder/layers_6/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.394500 139997397456896 adafactor.py:358] Since rank of parameter decoder/layers_8/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:11.394843 140577131780096 adafactor.py:358] Since rank of parameter decoder/layers_3/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:11.397173 140172769531904 adafactor.py:358] Since rank of parameter decoder/layers_7/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.395749 140577131780096 adafactor.py:358] Since rank of parameter decoder/layers_3/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:11.395548 139997397456896 adafactor.py:358] Since rank of parameter decoder/layers_8/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:11.396474 139997397456896 adafactor.py:358] Since rank of parameter decoder/layers_8/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:11.396674 140577131780096 adafactor.py:358] Since rank of parameter decoder/layers_3/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.398319 140172769531904 adafactor.py:358] Since rank of parameter decoder/layers_7/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.397373 139997397456896 adafactor.py:358] Since rank of parameter decoder/layers_8/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.397772 140577131780096 adafactor.py:358] Since rank of parameter decoder/layers_3/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:11.399414 140172769531904 adafactor.py:358] Since rank of parameter decoder/layers_7/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:11.398434 139997397456896 adafactor.py:358] Since rank of parameter decoder/layers_8/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:11.398844 140577131780096 adafactor.py:358] Since rank of parameter decoder/layers_3/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.400509 140172769531904 adafactor.py:358] Since rank of parameter decoder/layers_7/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:11.401399 140172769531904 adafactor.py:358] Since rank of parameter decoder/layers_7/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:11.399497 139997397456896 adafactor.py:358] Since rank of parameter decoder/layers_8/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.399948 140577131780096 adafactor.py:358] Since rank of parameter decoder/layers_3/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.402327 140172769531904 adafactor.py:358] Since rank of parameter decoder/layers_7/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.400608 139997397456896 adafactor.py:358] Since rank of parameter decoder/layers_8/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.401073 140577131780096 adafactor.py:358] Since rank of parameter decoder/layers_4/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.403384 140172769531904 adafactor.py:358] Since rank of parameter decoder/layers_7/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:11.401717 139997397456896 adafactor.py:358] Since rank of parameter decoder/layers_9/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.402142 140577131780096 adafactor.py:358] Since rank of parameter decoder/layers_4/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.404491 140172769531904 adafactor.py:358] Since rank of parameter decoder/layers_7/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.402814 139997397456896 adafactor.py:358] Since rank of parameter decoder/layers_9/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.403264 140577131780096 adafactor.py:358] Since rank of parameter decoder/layers_4/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:11.405557 140172769531904 adafactor.py:358] Since rank of parameter decoder/layers_7/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.403914 139997397456896 adafactor.py:358] Since rank of parameter decoder/layers_9/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:11.404376 140577131780096 adafactor.py:358] Since rank of parameter decoder/layers_4/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:11.406683 140172769531904 adafactor.py:358] Since rank of parameter decoder/layers_8/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.405067 139997397456896 adafactor.py:358] Since rank of parameter decoder/layers_9/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:11.405312 140577131780096 adafactor.py:358] Since rank of parameter decoder/layers_4/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:11.407769 140172769531904 adafactor.py:358] Since rank of parameter decoder/layers_8/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.405950 139997397456896 adafactor.py:358] Since rank of parameter decoder/layers_9/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:11.406197 140577131780096 adafactor.py:358] Since rank of parameter decoder/layers_4/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.406823 139997397456896 adafactor.py:358] Since rank of parameter decoder/layers_9/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.407274 140577131780096 adafactor.py:358] Since rank of parameter decoder/layers_4/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:11.408885 140172769531904 adafactor.py:358] Since rank of parameter decoder/layers_8/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:11.407877 139997397456896 adafactor.py:358] Since rank of parameter decoder/layers_9/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:11.409958 140172769531904 adafactor.py:358] Since rank of parameter decoder/layers_8/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:11.408393 140577131780096 adafactor.py:358] Since rank of parameter decoder/layers_4/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.410902 140172769531904 adafactor.py:358] Since rank of parameter decoder/layers_8/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:11.409003 139997397456896 adafactor.py:358] Since rank of parameter decoder/layers_9/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.409518 140577131780096 adafactor.py:358] Since rank of parameter decoder/layers_4/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.411808 140172769531904 adafactor.py:358] Since rank of parameter decoder/layers_8/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.410076 139997397456896 adafactor.py:358] Since rank of parameter decoder/layers_9/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.410604 140577131780096 adafactor.py:358] Since rank of parameter decoder/layers_5/extra_mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.412913 140172769531904 adafactor.py:358] Since rank of parameter decoder/layers_8/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:11.411137 139997397456896 adafactor.py:358] Since rank of parameter decoder/logits_dense/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.411719 140577131780096 adafactor.py:358] Since rank of parameter decoder/layers_5/extra_mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.413999 140172769531904 adafactor.py:358] Since rank of parameter decoder/layers_8/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.412193 139997397456896 adafactor.py:358] Since rank of parameter token_embedder/embedding 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:11.412803 140577131780096 adafactor.py:358] Since rank of parameter decoder/layers_5/extra_mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:11.415126 140172769531904 adafactor.py:358] Since rank of parameter decoder/layers_8/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.416229 140172769531904 adafactor.py:358] Since rank of parameter decoder/layers_9/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.417338 140172769531904 adafactor.py:358] Since rank of parameter decoder/layers_9/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.418411 140172769531904 adafactor.py:358] Since rank of parameter decoder/layers_9/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:11.416893 140577131780096 adafactor.py:358] Since rank of parameter decoder/layers_5/mlp/router/router_weights/w/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>, <FactorDim.NONE: None>) is ignored.
W0512 01:45:11.417851 140577131780096 adafactor.py:358] Since rank of parameter decoder/layers_5/pre_extra_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:11.419558 140172769531904 adafactor.py:358] Since rank of parameter decoder/layers_9/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:11.420477 140172769531904 adafactor.py:358] Since rank of parameter decoder/layers_9/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:11.315722 140316961150976 adafactor.py:358] Since rank of parameter decoder/layers_17/extra_mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.316858 140316961150976 adafactor.py:358] Since rank of parameter decoder/layers_17/extra_mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.317967 140316961150976 adafactor.py:358] Since rank of parameter decoder/layers_17/extra_mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:11.321975 140316961150976 adafactor.py:358] Since rank of parameter decoder/layers_17/mlp/router/router_weights/w/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>, <FactorDim.NONE: None>) is ignored.
W0512 01:45:11.322865 140316961150976 adafactor.py:358] Since rank of parameter decoder/layers_17/pre_extra_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:11.323783 140316961150976 adafactor.py:358] Since rank of parameter decoder/layers_17/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:11.324658 140316961150976 adafactor.py:358] Since rank of parameter decoder/layers_17/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:11.325522 140316961150976 adafactor.py:358] Since rank of parameter decoder/layers_17/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.418735 140577131780096 adafactor.py:358] Since rank of parameter decoder/layers_5/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:11.421364 140172769531904 adafactor.py:358] Since rank of parameter decoder/layers_9/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.419656 140577131780096 adafactor.py:358] Since rank of parameter decoder/layers_5/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:11.326622 140316961150976 adafactor.py:358] Since rank of parameter decoder/layers_17/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:11.327738 140316961150976 adafactor.py:358] Since rank of parameter decoder/layers_17/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.328796 140316961150976 adafactor.py:358] Since rank of parameter decoder/layers_17/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.329884 140316961150976 adafactor.py:358] Since rank of parameter decoder/layers_18/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.330947 140316961150976 adafactor.py:358] Since rank of parameter decoder/layers_18/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.332067 140316961150976 adafactor.py:358] Since rank of parameter decoder/layers_18/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:11.333128 140316961150976 adafactor.py:358] Since rank of parameter decoder/layers_18/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:11.334054 140316961150976 adafactor.py:358] Since rank of parameter decoder/layers_18/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:11.334927 140316961150976 adafactor.py:358] Since rank of parameter decoder/layers_18/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.420569 140577131780096 adafactor.py:358] Since rank of parameter decoder/layers_5/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.336056 140316961150976 adafactor.py:358] Since rank of parameter decoder/layers_18/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:11.337133 140316961150976 adafactor.py:358] Since rank of parameter decoder/layers_18/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.338222 140316961150976 adafactor.py:358] Since rank of parameter decoder/layers_18/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.339793 140316961150976 adafactor.py:358] Since rank of parameter decoder/layers_19/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.340863 140316961150976 adafactor.py:358] Since rank of parameter decoder/layers_19/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.341951 140316961150976 adafactor.py:358] Since rank of parameter decoder/layers_19/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:11.343023 140316961150976 adafactor.py:358] Since rank of parameter decoder/layers_19/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:11.343946 140316961150976 adafactor.py:358] Since rank of parameter decoder/layers_19/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:11.344821 140316961150976 adafactor.py:358] Since rank of parameter decoder/layers_19/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.345930 140316961150976 adafactor.py:358] Since rank of parameter decoder/layers_19/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:11.347003 140316961150976 adafactor.py:358] Since rank of parameter decoder/layers_19/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.348136 140316961150976 adafactor.py:358] Since rank of parameter decoder/layers_19/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.349205 140316961150976 adafactor.py:358] Since rank of parameter decoder/layers_2/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.350453 140316961150976 adafactor.py:358] Since rank of parameter decoder/layers_2/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.351566 140316961150976 adafactor.py:358] Since rank of parameter decoder/layers_2/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:11.352636 140316961150976 adafactor.py:358] Since rank of parameter decoder/layers_2/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:11.353520 140316961150976 adafactor.py:358] Since rank of parameter decoder/layers_2/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:11.422139 140577131780096 adafactor.py:358] Since rank of parameter decoder/layers_5/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:11.354416 140316961150976 adafactor.py:358] Since rank of parameter decoder/layers_2/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.355523 140316961150976 adafactor.py:358] Since rank of parameter decoder/layers_2/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:11.356609 140316961150976 adafactor.py:358] Since rank of parameter decoder/layers_2/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.357697 140316961150976 adafactor.py:358] Since rank of parameter decoder/layers_2/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.358758 140316961150976 adafactor.py:358] Since rank of parameter decoder/layers_20/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.359869 140316961150976 adafactor.py:358] Since rank of parameter decoder/layers_20/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.360940 140316961150976 adafactor.py:358] Since rank of parameter decoder/layers_20/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:11.362034 140316961150976 adafactor.py:358] Since rank of parameter decoder/layers_20/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:11.362939 140316961150976 adafactor.py:358] Since rank of parameter decoder/layers_20/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:11.423263 140577131780096 adafactor.py:358] Since rank of parameter decoder/layers_5/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.363859 140316961150976 adafactor.py:358] Since rank of parameter decoder/layers_20/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.364917 140316961150976 adafactor.py:358] Since rank of parameter decoder/layers_20/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:11.366015 140316961150976 adafactor.py:358] Since rank of parameter decoder/layers_20/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.367074 140316961150976 adafactor.py:358] Since rank of parameter decoder/layers_20/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.368206 140316961150976 adafactor.py:358] Since rank of parameter decoder/layers_21/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.369263 140316961150976 adafactor.py:358] Since rank of parameter decoder/layers_21/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.370360 140316961150976 adafactor.py:358] Since rank of parameter decoder/layers_21/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:11.371425 140316961150976 adafactor.py:358] Since rank of parameter decoder/layers_21/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:11.372349 140316961150976 adafactor.py:358] Since rank of parameter decoder/layers_21/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:11.424386 140577131780096 adafactor.py:358] Since rank of parameter decoder/layers_5/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.373219 140316961150976 adafactor.py:358] Since rank of parameter decoder/layers_21/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.374299 140316961150976 adafactor.py:358] Since rank of parameter decoder/layers_21/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:11.375358 140316961150976 adafactor.py:358] Since rank of parameter decoder/layers_21/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.376494 140316961150976 adafactor.py:358] Since rank of parameter decoder/layers_21/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.377576 140316961150976 adafactor.py:358] Since rank of parameter decoder/layers_22/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.378644 140316961150976 adafactor.py:358] Since rank of parameter decoder/layers_22/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.379707 140316961150976 adafactor.py:358] Since rank of parameter decoder/layers_22/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:11.380833 140316961150976 adafactor.py:358] Since rank of parameter decoder/layers_22/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:11.425484 140577131780096 adafactor.py:358] Since rank of parameter decoder/layers_6/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.381759 140316961150976 adafactor.py:358] Since rank of parameter decoder/layers_22/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:11.382625 140316961150976 adafactor.py:358] Since rank of parameter decoder/layers_22/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.383701 140316961150976 adafactor.py:358] Since rank of parameter decoder/layers_22/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:11.384763 140316961150976 adafactor.py:358] Since rank of parameter decoder/layers_22/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.385909 140316961150976 adafactor.py:358] Since rank of parameter decoder/layers_22/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.386969 140316961150976 adafactor.py:358] Since rank of parameter decoder/layers_23/extra_mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.388024 140316961150976 adafactor.py:358] Since rank of parameter decoder/layers_23/extra_mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.389077 140316961150976 adafactor.py:358] Since rank of parameter decoder/layers_23/extra_mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:11.393563 140316961150976 adafactor.py:358] Since rank of parameter decoder/layers_23/mlp/router/router_weights/w/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>, <FactorDim.NONE: None>) is ignored.
W0512 01:45:11.426628 140577131780096 adafactor.py:358] Since rank of parameter decoder/layers_6/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.394456 140316961150976 adafactor.py:358] Since rank of parameter decoder/layers_23/pre_extra_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:11.395327 140316961150976 adafactor.py:358] Since rank of parameter decoder/layers_23/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:11.396227 140316961150976 adafactor.py:358] Since rank of parameter decoder/layers_23/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:11.397091 140316961150976 adafactor.py:358] Since rank of parameter decoder/layers_23/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.398247 140316961150976 adafactor.py:358] Since rank of parameter decoder/layers_23/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:11.399452 140316961150976 adafactor.py:358] Since rank of parameter decoder/layers_23/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.400507 140316961150976 adafactor.py:358] Since rank of parameter decoder/layers_23/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.401574 140316961150976 adafactor.py:358] Since rank of parameter decoder/layers_3/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.428006 140577131780096 adafactor.py:358] Since rank of parameter decoder/layers_6/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:11.402679 140316961150976 adafactor.py:358] Since rank of parameter decoder/layers_3/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.403759 140316961150976 adafactor.py:358] Since rank of parameter decoder/layers_3/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:11.404810 140316961150976 adafactor.py:358] Since rank of parameter decoder/layers_3/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:11.405701 140316961150976 adafactor.py:358] Since rank of parameter decoder/layers_3/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:11.406619 140316961150976 adafactor.py:358] Since rank of parameter decoder/layers_3/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.407687 140316961150976 adafactor.py:358] Since rank of parameter decoder/layers_3/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:11.408756 140316961150976 adafactor.py:358] Since rank of parameter decoder/layers_3/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.409832 140316961150976 adafactor.py:358] Since rank of parameter decoder/layers_3/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.410935 140316961150976 adafactor.py:358] Since rank of parameter decoder/layers_4/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.429097 140577131780096 adafactor.py:358] Since rank of parameter decoder/layers_6/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:11.429994 140577131780096 adafactor.py:358] Since rank of parameter decoder/layers_6/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:11.430880 140577131780096 adafactor.py:358] Since rank of parameter decoder/layers_6/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.432055 140577131780096 adafactor.py:358] Since rank of parameter decoder/layers_6/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:11.412002 140316961150976 adafactor.py:358] Since rank of parameter decoder/layers_4/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.413057 140316961150976 adafactor.py:358] Since rank of parameter decoder/layers_4/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:11.414157 140316961150976 adafactor.py:358] Since rank of parameter decoder/layers_4/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:11.415053 140316961150976 adafactor.py:358] Since rank of parameter decoder/layers_4/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:11.415907 140316961150976 adafactor.py:358] Since rank of parameter decoder/layers_4/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.416991 140316961150976 adafactor.py:358] Since rank of parameter decoder/layers_4/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:11.418076 140316961150976 adafactor.py:358] Since rank of parameter decoder/layers_4/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.419182 140316961150976 adafactor.py:358] Since rank of parameter decoder/layers_4/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.420242 140316961150976 adafactor.py:358] Since rank of parameter decoder/layers_5/extra_mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.433145 140577131780096 adafactor.py:358] Since rank of parameter decoder/layers_6/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.421282 140316961150976 adafactor.py:358] Since rank of parameter decoder/layers_5/extra_mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.422361 140316961150976 adafactor.py:358] Since rank of parameter decoder/layers_5/extra_mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:11.426366 140316961150976 adafactor.py:358] Since rank of parameter decoder/layers_5/mlp/router/router_weights/w/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>, <FactorDim.NONE: None>) is ignored.
W0512 01:45:11.427287 140316961150976 adafactor.py:358] Since rank of parameter decoder/layers_5/pre_extra_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:11.428148 140316961150976 adafactor.py:358] Since rank of parameter decoder/layers_5/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:11.429012 140316961150976 adafactor.py:358] Since rank of parameter decoder/layers_5/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:11.429908 140316961150976 adafactor.py:358] Since rank of parameter decoder/layers_5/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.434228 140577131780096 adafactor.py:358] Since rank of parameter decoder/layers_6/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.430967 140316961150976 adafactor.py:358] Since rank of parameter decoder/layers_5/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:11.435335 140577131780096 adafactor.py:358] Since rank of parameter decoder/layers_7/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.432092 140316961150976 adafactor.py:358] Since rank of parameter decoder/layers_5/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.433170 140316961150976 adafactor.py:358] Since rank of parameter decoder/layers_5/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.436476 140577131780096 adafactor.py:358] Since rank of parameter decoder/layers_7/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.434267 140316961150976 adafactor.py:358] Since rank of parameter decoder/layers_6/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.437558 140577131780096 adafactor.py:358] Since rank of parameter decoder/layers_7/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:11.438633 140577131780096 adafactor.py:358] Since rank of parameter decoder/layers_7/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:11.435318 140316961150976 adafactor.py:358] Since rank of parameter decoder/layers_6/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.439570 140577131780096 adafactor.py:358] Since rank of parameter decoder/layers_7/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:11.436481 140316961150976 adafactor.py:358] Since rank of parameter decoder/layers_6/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:11.440514 140577131780096 adafactor.py:358] Since rank of parameter decoder/layers_7/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.437561 140316961150976 adafactor.py:358] Since rank of parameter decoder/layers_6/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:11.441590 140577131780096 adafactor.py:358] Since rank of parameter decoder/layers_7/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:11.438438 140316961150976 adafactor.py:358] Since rank of parameter decoder/layers_6/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:11.439304 140316961150976 adafactor.py:358] Since rank of parameter decoder/layers_6/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.442723 140577131780096 adafactor.py:358] Since rank of parameter decoder/layers_7/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.443827 140577131780096 adafactor.py:358] Since rank of parameter decoder/layers_7/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.440821 140316961150976 adafactor.py:358] Since rank of parameter decoder/layers_6/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:11.444955 140577131780096 adafactor.py:358] Since rank of parameter decoder/layers_8/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.441919 140316961150976 adafactor.py:358] Since rank of parameter decoder/layers_6/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.446041 140577131780096 adafactor.py:358] Since rank of parameter decoder/layers_8/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.442981 140316961150976 adafactor.py:358] Since rank of parameter decoder/layers_6/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.447114 140577131780096 adafactor.py:358] Since rank of parameter decoder/layers_8/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:11.444028 140316961150976 adafactor.py:358] Since rank of parameter decoder/layers_7/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.445139 140316961150976 adafactor.py:358] Since rank of parameter decoder/layers_7/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.448212 140577131780096 adafactor.py:358] Since rank of parameter decoder/layers_8/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:11.449154 140577131780096 adafactor.py:358] Since rank of parameter decoder/layers_8/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:11.446225 140316961150976 adafactor.py:358] Since rank of parameter decoder/layers_7/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:11.450041 140577131780096 adafactor.py:358] Since rank of parameter decoder/layers_8/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.447281 140316961150976 adafactor.py:358] Since rank of parameter decoder/layers_7/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:11.451112 140577131780096 adafactor.py:358] Since rank of parameter decoder/layers_8/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:11.448155 140316961150976 adafactor.py:358] Since rank of parameter decoder/layers_7/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:11.452229 140577131780096 adafactor.py:358] Since rank of parameter decoder/layers_8/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.449055 140316961150976 adafactor.py:358] Since rank of parameter decoder/layers_7/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.453359 140577131780096 adafactor.py:358] Since rank of parameter decoder/layers_8/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.450149 140316961150976 adafactor.py:358] Since rank of parameter decoder/layers_7/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:11.454461 140577131780096 adafactor.py:358] Since rank of parameter decoder/layers_9/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.451208 140316961150976 adafactor.py:358] Since rank of parameter decoder/layers_7/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.452261 140316961150976 adafactor.py:358] Since rank of parameter decoder/layers_7/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.455595 140577131780096 adafactor.py:358] Since rank of parameter decoder/layers_9/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.453359 140316961150976 adafactor.py:358] Since rank of parameter decoder/layers_8/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.456689 140577131780096 adafactor.py:358] Since rank of parameter decoder/layers_9/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:11.454460 140316961150976 adafactor.py:358] Since rank of parameter decoder/layers_8/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.457828 140577131780096 adafactor.py:358] Since rank of parameter decoder/layers_9/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:11.458720 140577131780096 adafactor.py:358] Since rank of parameter decoder/layers_9/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:11.455520 140316961150976 adafactor.py:358] Since rank of parameter decoder/layers_8/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:11.459628 140577131780096 adafactor.py:358] Since rank of parameter decoder/layers_9/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.456597 140316961150976 adafactor.py:358] Since rank of parameter decoder/layers_8/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:11.460724 140577131780096 adafactor.py:358] Since rank of parameter decoder/layers_9/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:11.457479 140316961150976 adafactor.py:358] Since rank of parameter decoder/layers_8/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:11.458425 140316961150976 adafactor.py:358] Since rank of parameter decoder/layers_8/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.461851 140577131780096 adafactor.py:358] Since rank of parameter decoder/layers_9/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.459489 140316961150976 adafactor.py:358] Since rank of parameter decoder/layers_8/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:11.462964 140577131780096 adafactor.py:358] Since rank of parameter decoder/layers_9/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.460552 140316961150976 adafactor.py:358] Since rank of parameter decoder/layers_8/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.464073 140577131780096 adafactor.py:358] Since rank of parameter decoder/logits_dense/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.461642 140316961150976 adafactor.py:358] Since rank of parameter decoder/layers_8/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.465155 140577131780096 adafactor.py:358] Since rank of parameter token_embedder/embedding 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:11.462781 140316961150976 adafactor.py:358] Since rank of parameter decoder/layers_9/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.463845 140316961150976 adafactor.py:358] Since rank of parameter decoder/layers_9/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.464902 140316961150976 adafactor.py:358] Since rank of parameter decoder/layers_9/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:11.465993 140316961150976 adafactor.py:358] Since rank of parameter decoder/layers_9/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:11.466905 140316961150976 adafactor.py:358] Since rank of parameter decoder/layers_9/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:11.467782 140316961150976 adafactor.py:358] Since rank of parameter decoder/layers_9/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.468825 140316961150976 adafactor.py:358] Since rank of parameter decoder/layers_9/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:11.469909 140316961150976 adafactor.py:358] Since rank of parameter decoder/layers_9/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.471019 140316961150976 adafactor.py:358] Since rank of parameter decoder/layers_9/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.472059 140316961150976 adafactor.py:358] Since rank of parameter decoder/logits_dense/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.473115 140316961150976 adafactor.py:358] Since rank of parameter token_embedder/embedding 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:11.422427 140172769531904 adafactor.py:358] Since rank of parameter decoder/layers_9/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:11.423550 140172769531904 adafactor.py:358] Since rank of parameter decoder/layers_9/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.424659 140172769531904 adafactor.py:358] Since rank of parameter decoder/layers_9/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.425729 140172769531904 adafactor.py:358] Since rank of parameter decoder/logits_dense/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:11.426802 140172769531904 adafactor.py:358] Since rank of parameter token_embedder/embedding 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
I0512 01:45:11.672619 139676058949632 utils.py:947] No checkpoints found in specified directory: gs://rosinality-tpu-bucket/openmoe_8b/training
I0512 01:45:11.693177 140188559153152 utils.py:947] No checkpoints found in specified directory: gs://rosinality-tpu-bucket/openmoe_8b/training
I0512 01:45:11.702020 140392160929792 utils.py:947] No checkpoints found in specified directory: gs://rosinality-tpu-bucket/openmoe_8b/training
I0512 01:45:11.742002 139812357228544 utils.py:947] No checkpoints found in specified directory: gs://rosinality-tpu-bucket/openmoe_8b/training
I0512 01:45:11.775591 139997397456896 utils.py:947] No checkpoints found in specified directory: gs://rosinality-tpu-bucket/openmoe_8b/training
I0512 01:45:11.787564 140172769531904 utils.py:947] No checkpoints found in specified directory: gs://rosinality-tpu-bucket/openmoe_8b/training
I0512 01:45:11.840825 140316961150976 utils.py:947] No checkpoints found in specified directory: gs://rosinality-tpu-bucket/openmoe_8b/training
I0512 01:45:11.848398 140577131780096 utils.py:947] No checkpoints found in specified directory: gs://rosinality-tpu-bucket/openmoe_8b/training
I0512 01:45:12.138103 140188559153152 utils.py:1065] Initializing parameters from scratch.
I0512 01:45:12.143794 140392160929792 utils.py:1065] Initializing parameters from scratch.
I0512 01:45:12.140264 139676058949632 utils.py:1065] Initializing parameters from scratch.
I0512 01:45:12.196219 139812357228544 utils.py:1065] Initializing parameters from scratch.
I0512 01:45:12.216677 139997397456896 utils.py:1065] Initializing parameters from scratch.
I0512 01:45:12.234788 140172769531904 utils.py:1065] Initializing parameters from scratch.
I0512 01:45:12.296226 140577131780096 utils.py:1065] Initializing parameters from scratch.
I0512 01:45:12.299184 140316961150976 utils.py:1065] Initializing parameters from scratch.
I0512 01:45:12.612529 140188559153152 moe_layers.py:777] Selected group_size=4096 and num_groups=192 for input num_tokens=786432, max_group_size=4096, num_experts=8 and num_expert_replicas=1
I0512 01:45:12.611695 139676058949632 moe_layers.py:777] Selected group_size=4096 and num_groups=192 for input num_tokens=786432, max_group_size=4096, num_experts=8 and num_expert_replicas=1
I0512 01:45:12.664657 140392160929792 moe_layers.py:777] Selected group_size=4096 and num_groups=192 for input num_tokens=786432, max_group_size=4096, num_experts=8 and num_expert_replicas=1
I0512 01:45:12.693013 139997397456896 moe_layers.py:777] Selected group_size=4096 and num_groups=192 for input num_tokens=786432, max_group_size=4096, num_experts=8 and num_expert_replicas=1
I0512 01:45:12.682036 139812357228544 moe_layers.py:777] Selected group_size=4096 and num_groups=192 for input num_tokens=786432, max_group_size=4096, num_experts=8 and num_expert_replicas=1
I0512 01:45:12.715961 140172769531904 moe_layers.py:777] Selected group_size=4096 and num_groups=192 for input num_tokens=786432, max_group_size=4096, num_experts=8 and num_expert_replicas=1
I0512 01:45:12.774131 140316961150976 moe_layers.py:777] Selected group_size=4096 and num_groups=192 for input num_tokens=786432, max_group_size=4096, num_experts=8 and num_expert_replicas=1
I0512 01:45:12.817083 140577131780096 moe_layers.py:777] Selected group_size=4096 and num_groups=192 for input num_tokens=786432, max_group_size=4096, num_experts=8 and num_expert_replicas=1
I0512 01:45:13.180932 140316961150976 moe_layers.py:777] Selected group_size=4096 and num_groups=192 for input num_tokens=786432, max_group_size=4096, num_experts=8 and num_expert_replicas=1
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
I0512 01:45:13.273214 140188559153152 moe_layers.py:777] Selected group_size=4096 and num_groups=192 for input num_tokens=786432, max_group_size=4096, num_experts=8 and num_expert_replicas=1
I0512 01:45:13.262891 139676058949632 moe_layers.py:777] Selected group_size=4096 and num_groups=192 for input num_tokens=786432, max_group_size=4096, num_experts=8 and num_expert_replicas=1
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
I0512 01:45:13.345348 139997397456896 moe_layers.py:777] Selected group_size=4096 and num_groups=192 for input num_tokens=786432, max_group_size=4096, num_experts=8 and num_expert_replicas=1
I0512 01:45:13.375725 140172769531904 moe_layers.py:777] Selected group_size=4096 and num_groups=192 for input num_tokens=786432, max_group_size=4096, num_experts=8 and num_expert_replicas=1
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
I0512 01:45:13.386671 140392160929792 moe_layers.py:777] Selected group_size=4096 and num_groups=192 for input num_tokens=786432, max_group_size=4096, num_experts=8 and num_expert_replicas=1
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
I0512 01:45:13.385193 139812357228544 moe_layers.py:777] Selected group_size=4096 and num_groups=192 for input num_tokens=786432, max_group_size=4096, num_experts=8 and num_expert_replicas=1
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
I0512 01:45:13.493026 140577131780096 moe_layers.py:777] Selected group_size=4096 and num_groups=192 for input num_tokens=786432, max_group_size=4096, num_experts=8 and num_expert_replicas=1
I0512 01:45:13.621886 140188559153152 moe_layers.py:777] Selected group_size=4096 and num_groups=192 for input num_tokens=786432, max_group_size=4096, num_experts=8 and num_expert_replicas=1
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
I0512 01:45:13.622303 139676058949632 moe_layers.py:777] Selected group_size=4096 and num_groups=192 for input num_tokens=786432, max_group_size=4096, num_experts=8 and num_expert_replicas=1
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
I0512 01:45:13.696944 139997397456896 moe_layers.py:777] Selected group_size=4096 and num_groups=192 for input num_tokens=786432, max_group_size=4096, num_experts=8 and num_expert_replicas=1
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
I0512 01:45:13.722720 140172769531904 moe_layers.py:777] Selected group_size=4096 and num_groups=192 for input num_tokens=786432, max_group_size=4096, num_experts=8 and num_expert_replicas=1
I0512 01:45:13.753311 140392160929792 moe_layers.py:777] Selected group_size=4096 and num_groups=192 for input num_tokens=786432, max_group_size=4096, num_experts=8 and num_expert_replicas=1
I0512 01:45:13.739902 139812357228544 moe_layers.py:777] Selected group_size=4096 and num_groups=192 for input num_tokens=786432, max_group_size=4096, num_experts=8 and num_expert_replicas=1
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
I0512 01:45:13.796537 140316961150976 moe_layers.py:777] Selected group_size=4096 and num_groups=192 for input num_tokens=786432, max_group_size=4096, num_experts=8 and num_expert_replicas=1
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
I0512 01:45:13.861927 140577131780096 moe_layers.py:777] Selected group_size=4096 and num_groups=192 for input num_tokens=786432, max_group_size=4096, num_experts=8 and num_expert_replicas=1
I0512 01:45:13.967819 140188559153152 moe_layers.py:777] Selected group_size=4096 and num_groups=192 for input num_tokens=786432, max_group_size=4096, num_experts=8 and num_expert_replicas=1
I0512 01:45:13.974320 139676058949632 moe_layers.py:777] Selected group_size=4096 and num_groups=192 for input num_tokens=786432, max_group_size=4096, num_experts=8 and num_expert_replicas=1
I0512 01:45:14.048773 139997397456896 moe_layers.py:777] Selected group_size=4096 and num_groups=192 for input num_tokens=786432, max_group_size=4096, num_experts=8 and num_expert_replicas=1
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
W0512 01:45:14.073449 140188559153152 adafactor.py:358] Since rank of parameter decoder/decoder_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:14.074578 140188559153152 adafactor.py:358] Since rank of parameter decoder/layers_0/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.075770 140188559153152 adafactor.py:358] Since rank of parameter decoder/layers_0/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.076916 140188559153152 adafactor.py:358] Since rank of parameter decoder/layers_0/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:14.078172 140188559153152 adafactor.py:358] Since rank of parameter decoder/layers_0/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:14.079252 140188559153152 adafactor.py:358] Since rank of parameter decoder/layers_0/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:14.080158 140188559153152 adafactor.py:358] Since rank of parameter decoder/layers_0/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.081255 140188559153152 adafactor.py:358] Since rank of parameter decoder/layers_0/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:14.082369 140188559153152 adafactor.py:358] Since rank of parameter decoder/layers_0/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.083553 140188559153152 adafactor.py:358] Since rank of parameter decoder/layers_0/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.084635 140188559153152 adafactor.py:358] Since rank of parameter decoder/layers_1/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.085753 140188559153152 adafactor.py:358] Since rank of parameter decoder/layers_1/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.086898 140188559153152 adafactor.py:358] Since rank of parameter decoder/layers_1/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:14.088066 140188559153152 adafactor.py:358] Since rank of parameter decoder/layers_1/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:14.089056 140188559153152 adafactor.py:358] Since rank of parameter decoder/layers_1/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:14.089947 140188559153152 adafactor.py:358] Since rank of parameter decoder/layers_1/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.091104 140188559153152 adafactor.py:358] Since rank of parameter decoder/layers_1/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:14.092549 140188559153152 adafactor.py:358] Since rank of parameter decoder/layers_1/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.094945 140188559153152 adafactor.py:358] Since rank of parameter decoder/layers_1/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.096079 140188559153152 adafactor.py:358] Since rank of parameter decoder/layers_10/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
I0512 01:45:14.099359 140172769531904 moe_layers.py:777] Selected group_size=4096 and num_groups=192 for input num_tokens=786432, max_group_size=4096, num_experts=8 and num_expert_replicas=1
W0512 01:45:14.097349 140188559153152 adafactor.py:358] Since rank of parameter decoder/layers_10/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.098612 140188559153152 adafactor.py:358] Since rank of parameter decoder/layers_10/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:14.080688 139676058949632 adafactor.py:358] Since rank of parameter decoder/decoder_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:14.081757 139676058949632 adafactor.py:358] Since rank of parameter decoder/layers_0/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.102186 140188559153152 adafactor.py:358] Since rank of parameter decoder/layers_10/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:14.084663 139676058949632 adafactor.py:358] Since rank of parameter decoder/layers_0/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.105340 140188559153152 adafactor.py:358] Since rank of parameter decoder/layers_10/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:14.085858 139676058949632 adafactor.py:358] Since rank of parameter decoder/layers_0/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:14.087222 139676058949632 adafactor.py:358] Since rank of parameter decoder/layers_0/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:14.107815 140188559153152 adafactor.py:358] Since rank of parameter decoder/layers_10/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.088327 139676058949632 adafactor.py:358] Since rank of parameter decoder/layers_0/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:14.089546 139676058949632 adafactor.py:358] Since rank of parameter decoder/layers_0/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.110749 140188559153152 adafactor.py:358] Since rank of parameter decoder/layers_10/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:14.112076 140188559153152 adafactor.py:358] Since rank of parameter decoder/layers_10/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.113229 140188559153152 adafactor.py:358] Since rank of parameter decoder/layers_10/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.093457 139676058949632 adafactor.py:358] Since rank of parameter decoder/layers_0/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:14.095275 139676058949632 adafactor.py:358] Since rank of parameter decoder/layers_0/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
I0512 01:45:14.095656 139812357228544 moe_layers.py:777] Selected group_size=4096 and num_groups=192 for input num_tokens=786432, max_group_size=4096, num_experts=8 and num_expert_replicas=1
W0512 01:45:14.097206 139676058949632 adafactor.py:358] Since rank of parameter decoder/layers_0/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.099046 139676058949632 adafactor.py:358] Since rank of parameter decoder/layers_1/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.119462 140188559153152 adafactor.py:358] Since rank of parameter decoder/layers_11/extra_mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.100286 139676058949632 adafactor.py:358] Since rank of parameter decoder/layers_1/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.120914 140188559153152 adafactor.py:358] Since rank of parameter decoder/layers_11/extra_mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.101729 139676058949632 adafactor.py:358] Since rank of parameter decoder/layers_1/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:14.122058 140188559153152 adafactor.py:358] Since rank of parameter decoder/layers_11/extra_mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:14.103094 139676058949632 adafactor.py:358] Since rank of parameter decoder/layers_1/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
I0512 01:45:14.125948 140392160929792 moe_layers.py:777] Selected group_size=4096 and num_groups=192 for input num_tokens=786432, max_group_size=4096, num_experts=8 and num_expert_replicas=1
W0512 01:45:14.104109 139676058949632 adafactor.py:358] Since rank of parameter decoder/layers_1/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:14.105133 139676058949632 adafactor.py:358] Since rank of parameter decoder/layers_1/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.126439 140188559153152 adafactor.py:358] Since rank of parameter decoder/layers_11/mlp/router/router_weights/w/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>, <FactorDim.NONE: None>) is ignored.
W0512 01:45:14.106559 139676058949632 adafactor.py:358] Since rank of parameter decoder/layers_1/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:14.127377 140188559153152 adafactor.py:358] Since rank of parameter decoder/layers_11/pre_extra_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:14.107877 139676058949632 adafactor.py:358] Since rank of parameter decoder/layers_1/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.128305 140188559153152 adafactor.py:358] Since rank of parameter decoder/layers_11/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:14.109114 139676058949632 adafactor.py:358] Since rank of parameter decoder/layers_1/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.129198 140188559153152 adafactor.py:358] Since rank of parameter decoder/layers_11/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:14.110393 139676058949632 adafactor.py:358] Since rank of parameter decoder/layers_10/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.131404 140188559153152 adafactor.py:358] Since rank of parameter decoder/layers_11/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.111635 139676058949632 adafactor.py:358] Since rank of parameter decoder/layers_10/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.132510 140188559153152 adafactor.py:358] Since rank of parameter decoder/layers_11/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:14.112960 139676058949632 adafactor.py:358] Since rank of parameter decoder/layers_10/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:14.133626 140188559153152 adafactor.py:358] Since rank of parameter decoder/layers_11/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.114260 139676058949632 adafactor.py:358] Since rank of parameter decoder/layers_10/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:14.134801 140188559153152 adafactor.py:358] Since rank of parameter decoder/layers_11/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.115269 139676058949632 adafactor.py:358] Since rank of parameter decoder/layers_10/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:14.136134 140188559153152 adafactor.py:358] Since rank of parameter decoder/layers_12/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.116265 139676058949632 adafactor.py:358] Since rank of parameter decoder/layers_10/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.137235 140188559153152 adafactor.py:358] Since rank of parameter decoder/layers_12/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.117565 139676058949632 adafactor.py:358] Since rank of parameter decoder/layers_10/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:14.138380 140188559153152 adafactor.py:358] Since rank of parameter decoder/layers_12/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:14.118867 139676058949632 adafactor.py:358] Since rank of parameter decoder/layers_10/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.139516 140188559153152 adafactor.py:358] Since rank of parameter decoder/layers_12/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:14.120116 139676058949632 adafactor.py:358] Since rank of parameter decoder/layers_10/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.140469 140188559153152 adafactor.py:358] Since rank of parameter decoder/layers_12/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:14.121369 139676058949632 adafactor.py:358] Since rank of parameter decoder/layers_11/extra_mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.141370 140188559153152 adafactor.py:358] Since rank of parameter decoder/layers_12/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.142558 140188559153152 adafactor.py:358] Since rank of parameter decoder/layers_12/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:14.122715 139676058949632 adafactor.py:358] Since rank of parameter decoder/layers_11/extra_mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.143694 140188559153152 adafactor.py:358] Since rank of parameter decoder/layers_12/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.124043 139676058949632 adafactor.py:358] Since rank of parameter decoder/layers_11/extra_mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
I0512 01:45:14.141958 140316961150976 moe_layers.py:777] Selected group_size=4096 and num_groups=192 for input num_tokens=786432, max_group_size=4096, num_experts=8 and num_expert_replicas=1
W0512 01:45:14.144869 140188559153152 adafactor.py:358] Since rank of parameter decoder/layers_12/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.145956 140188559153152 adafactor.py:358] Since rank of parameter decoder/layers_13/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.147093 140188559153152 adafactor.py:358] Since rank of parameter decoder/layers_13/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.148222 140188559153152 adafactor.py:358] Since rank of parameter decoder/layers_13/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:14.128775 139676058949632 adafactor.py:358] Since rank of parameter decoder/layers_11/mlp/router/router_weights/w/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>, <FactorDim.NONE: None>) is ignored.
W0512 01:45:14.149371 140188559153152 adafactor.py:358] Since rank of parameter decoder/layers_13/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:14.129869 139676058949632 adafactor.py:358] Since rank of parameter decoder/layers_11/pre_extra_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:14.150314 140188559153152 adafactor.py:358] Since rank of parameter decoder/layers_13/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:14.130881 139676058949632 adafactor.py:358] Since rank of parameter decoder/layers_11/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:14.151208 140188559153152 adafactor.py:358] Since rank of parameter decoder/layers_13/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.131884 139676058949632 adafactor.py:358] Since rank of parameter decoder/layers_11/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:14.152308 140188559153152 adafactor.py:358] Since rank of parameter decoder/layers_13/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:14.153446 140188559153152 adafactor.py:358] Since rank of parameter decoder/layers_13/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.154602 140188559153152 adafactor.py:358] Since rank of parameter decoder/layers_13/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.134677 139676058949632 adafactor.py:358] Since rank of parameter decoder/layers_11/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.155679 140188559153152 adafactor.py:358] Since rank of parameter decoder/layers_14/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.135962 139676058949632 adafactor.py:358] Since rank of parameter decoder/layers_11/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:14.156762 140188559153152 adafactor.py:358] Since rank of parameter decoder/layers_14/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.137199 139676058949632 adafactor.py:358] Since rank of parameter decoder/layers_11/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.157908 140188559153152 adafactor.py:358] Since rank of parameter decoder/layers_14/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:14.138477 139676058949632 adafactor.py:358] Since rank of parameter decoder/layers_11/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.159052 140188559153152 adafactor.py:358] Since rank of parameter decoder/layers_14/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:14.139925 139676058949632 adafactor.py:358] Since rank of parameter decoder/layers_12/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.159956 140188559153152 adafactor.py:358] Since rank of parameter decoder/layers_14/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:14.160861 140188559153152 adafactor.py:358] Since rank of parameter decoder/layers_14/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.141168 139676058949632 adafactor.py:358] Since rank of parameter decoder/layers_12/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.161969 140188559153152 adafactor.py:358] Since rank of parameter decoder/layers_14/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:14.142426 139676058949632 adafactor.py:358] Since rank of parameter decoder/layers_12/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:14.163162 140188559153152 adafactor.py:358] Since rank of parameter decoder/layers_14/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.143757 139676058949632 adafactor.py:358] Since rank of parameter decoder/layers_12/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:14.164435 139997397456896 adafactor.py:358] Since rank of parameter decoder/decoder_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:14.164277 140188559153152 adafactor.py:358] Since rank of parameter decoder/layers_14/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.165541 139997397456896 adafactor.py:358] Since rank of parameter decoder/layers_0/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.144824 139676058949632 adafactor.py:358] Since rank of parameter decoder/layers_12/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:14.165377 140188559153152 adafactor.py:358] Since rank of parameter decoder/layers_15/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.145910 139676058949632 adafactor.py:358] Since rank of parameter decoder/layers_12/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.166687 139997397456896 adafactor.py:358] Since rank of parameter decoder/layers_0/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.166641 140188559153152 adafactor.py:358] Since rank of parameter decoder/layers_15/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.167803 139997397456896 adafactor.py:358] Since rank of parameter decoder/layers_0/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:14.147119 139676058949632 adafactor.py:358] Since rank of parameter decoder/layers_12/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:14.167877 140188559153152 adafactor.py:358] Since rank of parameter decoder/layers_15/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:14.169062 139997397456896 adafactor.py:358] Since rank of parameter decoder/layers_0/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:14.148322 139676058949632 adafactor.py:358] Since rank of parameter decoder/layers_12/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.168967 140188559153152 adafactor.py:358] Since rank of parameter decoder/layers_15/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:14.169979 139997397456896 adafactor.py:358] Since rank of parameter decoder/layers_0/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:14.149610 139676058949632 adafactor.py:358] Since rank of parameter decoder/layers_12/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.169939 140188559153152 adafactor.py:358] Since rank of parameter decoder/layers_15/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:14.170895 139997397456896 adafactor.py:358] Since rank of parameter decoder/layers_0/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.170863 140188559153152 adafactor.py:358] Since rank of parameter decoder/layers_15/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.150863 139676058949632 adafactor.py:358] Since rank of parameter decoder/layers_13/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.172072 139997397456896 adafactor.py:358] Since rank of parameter decoder/layers_0/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:14.172012 140188559153152 adafactor.py:358] Since rank of parameter decoder/layers_15/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:14.152074 139676058949632 adafactor.py:358] Since rank of parameter decoder/layers_13/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.173259 139997397456896 adafactor.py:358] Since rank of parameter decoder/layers_0/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.173119 140188559153152 adafactor.py:358] Since rank of parameter decoder/layers_15/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.153277 139676058949632 adafactor.py:358] Since rank of parameter decoder/layers_13/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:14.174512 139997397456896 adafactor.py:358] Since rank of parameter decoder/layers_0/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.174220 140188559153152 adafactor.py:358] Since rank of parameter decoder/layers_15/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.154646 139676058949632 adafactor.py:358] Since rank of parameter decoder/layers_13/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:14.175653 139997397456896 adafactor.py:358] Since rank of parameter decoder/layers_1/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.175511 140188559153152 adafactor.py:358] Since rank of parameter decoder/layers_16/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.155659 139676058949632 adafactor.py:358] Since rank of parameter decoder/layers_13/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:14.176836 139997397456896 adafactor.py:358] Since rank of parameter decoder/layers_1/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.156646 139676058949632 adafactor.py:358] Since rank of parameter decoder/layers_13/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.176667 140188559153152 adafactor.py:358] Since rank of parameter decoder/layers_16/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.178008 139997397456896 adafactor.py:358] Since rank of parameter decoder/layers_1/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:14.157901 139676058949632 adafactor.py:358] Since rank of parameter decoder/layers_13/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:14.177740 140188559153152 adafactor.py:358] Since rank of parameter decoder/layers_16/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:14.179190 139997397456896 adafactor.py:358] Since rank of parameter decoder/layers_1/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:14.178864 140188559153152 adafactor.py:358] Since rank of parameter decoder/layers_16/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:14.159161 139676058949632 adafactor.py:358] Since rank of parameter decoder/layers_13/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.180140 139997397456896 adafactor.py:358] Since rank of parameter decoder/layers_1/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:14.179774 140188559153152 adafactor.py:358] Since rank of parameter decoder/layers_16/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:14.181079 139997397456896 adafactor.py:358] Since rank of parameter decoder/layers_1/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.160355 139676058949632 adafactor.py:358] Since rank of parameter decoder/layers_13/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.180715 140188559153152 adafactor.py:358] Since rank of parameter decoder/layers_16/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.161567 139676058949632 adafactor.py:358] Since rank of parameter decoder/layers_14/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.182174 139997397456896 adafactor.py:358] Since rank of parameter decoder/layers_1/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:14.181784 140188559153152 adafactor.py:358] Since rank of parameter decoder/layers_16/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:14.183334 139997397456896 adafactor.py:358] Since rank of parameter decoder/layers_1/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.162810 139676058949632 adafactor.py:358] Since rank of parameter decoder/layers_14/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.182933 140188559153152 adafactor.py:358] Since rank of parameter decoder/layers_16/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.184454 139997397456896 adafactor.py:358] Since rank of parameter decoder/layers_1/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.164086 139676058949632 adafactor.py:358] Since rank of parameter decoder/layers_14/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:14.184099 140188559153152 adafactor.py:358] Since rank of parameter decoder/layers_16/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.185575 139997397456896 adafactor.py:358] Since rank of parameter decoder/layers_10/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.165322 139676058949632 adafactor.py:358] Since rank of parameter decoder/layers_14/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:14.186694 139997397456896 adafactor.py:358] Since rank of parameter decoder/layers_10/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.185928 140188559153152 adafactor.py:358] Since rank of parameter decoder/layers_17/extra_mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.166355 139676058949632 adafactor.py:358] Since rank of parameter decoder/layers_14/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:14.187840 139997397456896 adafactor.py:358] Since rank of parameter decoder/layers_10/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:14.187142 140188559153152 adafactor.py:358] Since rank of parameter decoder/layers_17/extra_mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.167347 139676058949632 adafactor.py:358] Since rank of parameter decoder/layers_14/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.188928 139997397456896 adafactor.py:358] Since rank of parameter decoder/layers_10/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:14.188283 140188559153152 adafactor.py:358] Since rank of parameter decoder/layers_17/extra_mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:14.168587 139676058949632 adafactor.py:358] Since rank of parameter decoder/layers_14/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:14.189865 139997397456896 adafactor.py:358] Since rank of parameter decoder/layers_10/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:14.169929 139676058949632 adafactor.py:358] Since rank of parameter decoder/layers_14/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.190763 139997397456896 adafactor.py:358] Since rank of parameter decoder/layers_10/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.191915 139997397456896 adafactor.py:358] Since rank of parameter decoder/layers_10/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:14.171137 139676058949632 adafactor.py:358] Since rank of parameter decoder/layers_14/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.193033 139997397456896 adafactor.py:358] Since rank of parameter decoder/layers_10/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.172342 139676058949632 adafactor.py:358] Since rank of parameter decoder/layers_15/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.192529 140188559153152 adafactor.py:358] Since rank of parameter decoder/layers_17/mlp/router/router_weights/w/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>, <FactorDim.NONE: None>) is ignored.
W0512 01:45:14.194112 139997397456896 adafactor.py:358] Since rank of parameter decoder/layers_10/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.193433 140188559153152 adafactor.py:358] Since rank of parameter decoder/layers_17/pre_extra_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:14.173581 139676058949632 adafactor.py:358] Since rank of parameter decoder/layers_15/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.195188 139997397456896 adafactor.py:358] Since rank of parameter decoder/layers_11/extra_mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.194525 140188559153152 adafactor.py:358] Since rank of parameter decoder/layers_17/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:14.174928 139676058949632 adafactor.py:358] Since rank of parameter decoder/layers_15/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:14.195415 140188559153152 adafactor.py:358] Since rank of parameter decoder/layers_17/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:14.196342 139997397456896 adafactor.py:358] Since rank of parameter decoder/layers_11/extra_mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.176145 139676058949632 adafactor.py:358] Since rank of parameter decoder/layers_15/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:14.196320 140188559153152 adafactor.py:358] Since rank of parameter decoder/layers_17/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.197469 139997397456896 adafactor.py:358] Since rank of parameter decoder/layers_11/extra_mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:14.177130 139676058949632 adafactor.py:358] Since rank of parameter decoder/layers_15/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:14.197409 140188559153152 adafactor.py:358] Since rank of parameter decoder/layers_17/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:14.178150 139676058949632 adafactor.py:358] Since rank of parameter decoder/layers_15/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.198530 140188559153152 adafactor.py:358] Since rank of parameter decoder/layers_17/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.179417 139676058949632 adafactor.py:358] Since rank of parameter decoder/layers_15/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:14.199665 140188559153152 adafactor.py:358] Since rank of parameter decoder/layers_17/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.180650 139676058949632 adafactor.py:358] Since rank of parameter decoder/layers_15/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.201701 139997397456896 adafactor.py:358] Since rank of parameter decoder/layers_11/mlp/router/router_weights/w/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>, <FactorDim.NONE: None>) is ignored.
W0512 01:45:14.200899 140188559153152 adafactor.py:358] Since rank of parameter decoder/layers_18/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.181903 139676058949632 adafactor.py:358] Since rank of parameter decoder/layers_15/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.202605 139997397456896 adafactor.py:358] Since rank of parameter decoder/layers_11/pre_extra_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:14.202001 140188559153152 adafactor.py:358] Since rank of parameter decoder/layers_18/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.203503 139997397456896 adafactor.py:358] Since rank of parameter decoder/layers_11/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:14.183110 139676058949632 adafactor.py:358] Since rank of parameter decoder/layers_16/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.203127 140188559153152 adafactor.py:358] Since rank of parameter decoder/layers_18/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:14.204425 139997397456896 adafactor.py:358] Since rank of parameter decoder/layers_11/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:14.184381 139676058949632 adafactor.py:358] Since rank of parameter decoder/layers_16/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.204280 140188559153152 adafactor.py:358] Since rank of parameter decoder/layers_18/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:14.205159 140188559153152 adafactor.py:358] Since rank of parameter decoder/layers_18/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:14.206161 139997397456896 adafactor.py:358] Since rank of parameter decoder/layers_11/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.185599 139676058949632 adafactor.py:358] Since rank of parameter decoder/layers_16/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:14.206037 140188559153152 adafactor.py:358] Since rank of parameter decoder/layers_18/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.207281 139997397456896 adafactor.py:358] Since rank of parameter decoder/layers_11/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:14.186837 139676058949632 adafactor.py:358] Since rank of parameter decoder/layers_16/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:14.207170 140188559153152 adafactor.py:358] Since rank of parameter decoder/layers_18/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:14.208412 139997397456896 adafactor.py:358] Since rank of parameter decoder/layers_11/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.187818 139676058949632 adafactor.py:358] Since rank of parameter decoder/layers_16/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:14.208321 140188559153152 adafactor.py:358] Since rank of parameter decoder/layers_18/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.209573 139997397456896 adafactor.py:358] Since rank of parameter decoder/layers_11/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.188862 139676058949632 adafactor.py:358] Since rank of parameter decoder/layers_16/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.209407 140188559153152 adafactor.py:358] Since rank of parameter decoder/layers_18/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.190129 139676058949632 adafactor.py:358] Since rank of parameter decoder/layers_16/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:14.210817 139997397456896 adafactor.py:358] Since rank of parameter decoder/layers_12/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.210684 140188559153152 adafactor.py:358] Since rank of parameter decoder/layers_19/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.211939 139997397456896 adafactor.py:358] Since rank of parameter decoder/layers_12/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.191339 139676058949632 adafactor.py:358] Since rank of parameter decoder/layers_16/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.211753 140188559153152 adafactor.py:358] Since rank of parameter decoder/layers_19/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.213073 139997397456896 adafactor.py:358] Since rank of parameter decoder/layers_12/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:14.192617 139676058949632 adafactor.py:358] Since rank of parameter decoder/layers_16/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.213003 140188559153152 adafactor.py:358] Since rank of parameter decoder/layers_19/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:14.214264 139997397456896 adafactor.py:358] Since rank of parameter decoder/layers_12/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:14.215202 139997397456896 adafactor.py:358] Since rank of parameter decoder/layers_12/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:14.214122 140188559153152 adafactor.py:358] Since rank of parameter decoder/layers_19/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:14.194997 139676058949632 adafactor.py:358] Since rank of parameter decoder/layers_17/extra_mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.215095 140188559153152 adafactor.py:358] Since rank of parameter decoder/layers_19/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:14.216087 139997397456896 adafactor.py:358] Since rank of parameter decoder/layers_12/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.215961 140188559153152 adafactor.py:358] Since rank of parameter decoder/layers_19/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.196201 139676058949632 adafactor.py:358] Since rank of parameter decoder/layers_17/extra_mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.217229 139997397456896 adafactor.py:358] Since rank of parameter decoder/layers_12/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:14.217116 140188559153152 adafactor.py:358] Since rank of parameter decoder/layers_19/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:14.197448 139676058949632 adafactor.py:358] Since rank of parameter decoder/layers_17/extra_mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:14.220153 140172769531904 adafactor.py:358] Since rank of parameter decoder/decoder_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:14.218217 140188559153152 adafactor.py:358] Since rank of parameter decoder/layers_19/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.218332 139997397456896 adafactor.py:358] Since rank of parameter decoder/layers_12/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.221246 140172769531904 adafactor.py:358] Since rank of parameter decoder/layers_0/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.219356 140188559153152 adafactor.py:358] Since rank of parameter decoder/layers_19/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.219497 139997397456896 adafactor.py:358] Since rank of parameter decoder/layers_12/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.222415 140172769531904 adafactor.py:358] Since rank of parameter decoder/layers_0/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.220446 140188559153152 adafactor.py:358] Since rank of parameter decoder/layers_2/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.220597 139997397456896 adafactor.py:358] Since rank of parameter decoder/layers_13/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.221583 140188559153152 adafactor.py:358] Since rank of parameter decoder/layers_2/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.221735 139997397456896 adafactor.py:358] Since rank of parameter decoder/layers_13/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.224595 140172769531904 adafactor.py:358] Since rank of parameter decoder/layers_0/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:14.202125 139676058949632 adafactor.py:358] Since rank of parameter decoder/layers_17/mlp/router/router_weights/w/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>, <FactorDim.NONE: None>) is ignored.
W0512 01:45:14.202869 139812357228544 adafactor.py:358] Since rank of parameter decoder/decoder_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:14.222729 140188559153152 adafactor.py:358] Since rank of parameter decoder/layers_2/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:14.222823 139997397456896 adafactor.py:358] Since rank of parameter decoder/layers_13/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:14.203125 139676058949632 adafactor.py:358] Since rank of parameter decoder/layers_17/pre_extra_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:14.203986 139812357228544 adafactor.py:358] Since rank of parameter decoder/layers_0/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.226416 140172769531904 adafactor.py:358] Since rank of parameter decoder/layers_0/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:14.223871 140188559153152 adafactor.py:358] Since rank of parameter decoder/layers_2/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:14.224039 139997397456896 adafactor.py:358] Since rank of parameter decoder/layers_13/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:14.224944 139997397456896 adafactor.py:358] Since rank of parameter decoder/layers_13/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:14.204165 139676058949632 adafactor.py:358] Since rank of parameter decoder/layers_17/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:14.205196 139812357228544 adafactor.py:358] Since rank of parameter decoder/layers_0/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.227541 140172769531904 adafactor.py:358] Since rank of parameter decoder/layers_0/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:14.224863 140188559153152 adafactor.py:358] Since rank of parameter decoder/layers_2/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:14.205139 139676058949632 adafactor.py:358] Since rank of parameter decoder/layers_17/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:14.225861 139997397456896 adafactor.py:358] Since rank of parameter decoder/layers_13/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.225862 140188559153152 adafactor.py:358] Since rank of parameter decoder/layers_2/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.228519 140172769531904 adafactor.py:358] Since rank of parameter decoder/layers_0/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.206300 139812357228544 adafactor.py:358] Since rank of parameter decoder/layers_0/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:14.226941 139997397456896 adafactor.py:358] Since rank of parameter decoder/layers_13/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:14.206166 139676058949632 adafactor.py:358] Since rank of parameter decoder/layers_17/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.227086 140188559153152 adafactor.py:358] Since rank of parameter decoder/layers_2/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:14.207519 139812357228544 adafactor.py:358] Since rank of parameter decoder/layers_0/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:14.228048 139997397456896 adafactor.py:358] Since rank of parameter decoder/layers_13/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.207346 139676058949632 adafactor.py:358] Since rank of parameter decoder/layers_17/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:14.208490 139812357228544 adafactor.py:358] Since rank of parameter decoder/layers_0/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:14.230908 140172769531904 adafactor.py:358] Since rank of parameter decoder/layers_0/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:14.228307 140188559153152 adafactor.py:358] Since rank of parameter decoder/layers_2/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.229208 139997397456896 adafactor.py:358] Since rank of parameter decoder/layers_13/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.208556 139676058949632 adafactor.py:358] Since rank of parameter decoder/layers_17/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.209400 139812357228544 adafactor.py:358] Since rank of parameter decoder/layers_0/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.232053 140172769531904 adafactor.py:358] Since rank of parameter decoder/layers_0/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.229514 140188559153152 adafactor.py:358] Since rank of parameter decoder/layers_2/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.230277 139997397456896 adafactor.py:358] Since rank of parameter decoder/layers_14/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.209848 139676058949632 adafactor.py:358] Since rank of parameter decoder/layers_17/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.210528 139812357228544 adafactor.py:358] Since rank of parameter decoder/layers_0/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:14.231377 139997397456896 adafactor.py:358] Since rank of parameter decoder/layers_14/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.233315 140172769531904 adafactor.py:358] Since rank of parameter decoder/layers_0/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.230785 140188559153152 adafactor.py:358] Since rank of parameter decoder/layers_20/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.211067 139676058949632 adafactor.py:358] Since rank of parameter decoder/layers_18/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.211630 139812357228544 adafactor.py:358] Since rank of parameter decoder/layers_0/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.232498 139997397456896 adafactor.py:358] Since rank of parameter decoder/layers_14/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:14.234460 140172769531904 adafactor.py:358] Since rank of parameter decoder/layers_1/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.231974 140188559153152 adafactor.py:358] Since rank of parameter decoder/layers_20/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.212853 139812357228544 adafactor.py:358] Since rank of parameter decoder/layers_0/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.212294 139676058949632 adafactor.py:358] Since rank of parameter decoder/layers_18/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.235578 140172769531904 adafactor.py:358] Since rank of parameter decoder/layers_1/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.233612 139997397456896 adafactor.py:358] Since rank of parameter decoder/layers_14/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:14.233139 140188559153152 adafactor.py:358] Since rank of parameter decoder/layers_20/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:14.213972 139812357228544 adafactor.py:358] Since rank of parameter decoder/layers_1/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.234515 139997397456896 adafactor.py:358] Since rank of parameter decoder/layers_14/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:14.213490 139676058949632 adafactor.py:358] Since rank of parameter decoder/layers_18/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:14.236723 140172769531904 adafactor.py:358] Since rank of parameter decoder/layers_1/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:14.234381 140188559153152 adafactor.py:358] Since rank of parameter decoder/layers_20/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:14.235432 139997397456896 adafactor.py:358] Since rank of parameter decoder/layers_14/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.215080 139812357228544 adafactor.py:358] Since rank of parameter decoder/layers_1/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.214844 139676058949632 adafactor.py:358] Since rank of parameter decoder/layers_18/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:14.237893 140172769531904 adafactor.py:358] Since rank of parameter decoder/layers_1/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:14.235398 140188559153152 adafactor.py:358] Since rank of parameter decoder/layers_20/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:14.236502 139997397456896 adafactor.py:358] Since rank of parameter decoder/layers_14/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:14.215833 139676058949632 adafactor.py:358] Since rank of parameter decoder/layers_18/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:14.216231 139812357228544 adafactor.py:358] Since rank of parameter decoder/layers_1/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:14.238856 140172769531904 adafactor.py:358] Since rank of parameter decoder/layers_1/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:14.236372 140188559153152 adafactor.py:358] Since rank of parameter decoder/layers_20/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.216829 139676058949632 adafactor.py:358] Since rank of parameter decoder/layers_18/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.237688 139997397456896 adafactor.py:358] Since rank of parameter decoder/layers_14/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.239784 140172769531904 adafactor.py:358] Since rank of parameter decoder/layers_1/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.217465 139812357228544 adafactor.py:358] Since rank of parameter decoder/layers_1/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:14.237482 140188559153152 adafactor.py:358] Since rank of parameter decoder/layers_20/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:14.218408 139812357228544 adafactor.py:358] Since rank of parameter decoder/layers_1/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:14.238771 139997397456896 adafactor.py:358] Since rank of parameter decoder/layers_14/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.240940 140172769531904 adafactor.py:358] Since rank of parameter decoder/layers_1/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:14.218475 139676058949632 adafactor.py:358] Since rank of parameter decoder/layers_18/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:14.238617 140188559153152 adafactor.py:358] Since rank of parameter decoder/layers_20/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.219344 139812357228544 adafactor.py:358] Since rank of parameter decoder/layers_1/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.239842 139997397456896 adafactor.py:358] Since rank of parameter decoder/layers_15/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.242123 140172769531904 adafactor.py:358] Since rank of parameter decoder/layers_1/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.239723 140188559153152 adafactor.py:358] Since rank of parameter decoder/layers_20/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.220507 139812357228544 adafactor.py:358] Since rank of parameter decoder/layers_1/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:14.219836 139676058949632 adafactor.py:358] Since rank of parameter decoder/layers_18/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.240920 139997397456896 adafactor.py:358] Since rank of parameter decoder/layers_15/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.243220 140172769531904 adafactor.py:358] Since rank of parameter decoder/layers_1/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.221074 139676058949632 adafactor.py:358] Since rank of parameter decoder/layers_18/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.221662 139812357228544 adafactor.py:358] Since rank of parameter decoder/layers_1/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.242111 139997397456896 adafactor.py:358] Since rank of parameter decoder/layers_15/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:14.241754 140188559153152 adafactor.py:358] Since rank of parameter decoder/layers_21/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.244335 140172769531904 adafactor.py:358] Since rank of parameter decoder/layers_10/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.222753 139812357228544 adafactor.py:358] Since rank of parameter decoder/layers_1/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.222395 139676058949632 adafactor.py:358] Since rank of parameter decoder/layers_19/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.243234 139997397456896 adafactor.py:358] Since rank of parameter decoder/layers_15/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:14.245480 140172769531904 adafactor.py:358] Since rank of parameter decoder/layers_10/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.242922 140188559153152 adafactor.py:358] Since rank of parameter decoder/layers_21/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.244177 139997397456896 adafactor.py:358] Since rank of parameter decoder/layers_15/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:14.223901 139812357228544 adafactor.py:358] Since rank of parameter decoder/layers_10/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.223721 139676058949632 adafactor.py:358] Since rank of parameter decoder/layers_19/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.246656 140172769531904 adafactor.py:358] Since rank of parameter decoder/layers_10/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:14.246675 140392160929792 adafactor.py:358] Since rank of parameter decoder/decoder_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:14.244030 140188559153152 adafactor.py:358] Since rank of parameter decoder/layers_21/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:14.245094 139997397456896 adafactor.py:358] Since rank of parameter decoder/layers_15/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.225136 139676058949632 adafactor.py:358] Since rank of parameter decoder/layers_19/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:14.247912 140172769531904 adafactor.py:358] Since rank of parameter decoder/layers_10/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:14.248159 140392160929792 adafactor.py:358] Since rank of parameter decoder/layers_0/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.225627 139812357228544 adafactor.py:358] Since rank of parameter decoder/layers_10/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.245139 140188559153152 adafactor.py:358] Since rank of parameter decoder/layers_21/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:14.246234 139997397456896 adafactor.py:358] Since rank of parameter decoder/layers_15/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:14.246130 140188559153152 adafactor.py:358] Since rank of parameter decoder/layers_21/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:14.248864 140172769531904 adafactor.py:358] Since rank of parameter decoder/layers_10/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:14.226546 139676058949632 adafactor.py:358] Since rank of parameter decoder/layers_19/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:14.247326 139997397456896 adafactor.py:358] Since rank of parameter decoder/layers_15/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.249501 140392160929792 adafactor.py:358] Since rank of parameter decoder/layers_0/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.247071 140188559153152 adafactor.py:358] Since rank of parameter decoder/layers_21/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.249755 140172769531904 adafactor.py:358] Since rank of parameter decoder/layers_10/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.227093 139812357228544 adafactor.py:358] Since rank of parameter decoder/layers_10/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:14.248391 139997397456896 adafactor.py:358] Since rank of parameter decoder/layers_15/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.227674 139676058949632 adafactor.py:358] Since rank of parameter decoder/layers_19/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:14.250813 140392160929792 adafactor.py:358] Since rank of parameter decoder/layers_0/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:14.248168 140188559153152 adafactor.py:358] Since rank of parameter decoder/layers_21/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:14.250892 140172769531904 adafactor.py:358] Since rank of parameter decoder/layers_10/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:14.228286 139812357228544 adafactor.py:358] Since rank of parameter decoder/layers_10/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:14.249529 139997397456896 adafactor.py:358] Since rank of parameter decoder/layers_16/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.229300 139812357228544 adafactor.py:358] Since rank of parameter decoder/layers_10/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:14.228776 139676058949632 adafactor.py:358] Since rank of parameter decoder/layers_19/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.252155 140392160929792 adafactor.py:358] Since rank of parameter decoder/layers_0/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:14.251976 140172769531904 adafactor.py:358] Since rank of parameter decoder/layers_10/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.246690 140316961150976 adafactor.py:358] Since rank of parameter decoder/decoder_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:14.249565 140188559153152 adafactor.py:358] Since rank of parameter decoder/layers_21/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.230231 139812357228544 adafactor.py:358] Since rank of parameter decoder/layers_10/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.250711 139997397456896 adafactor.py:358] Since rank of parameter decoder/layers_16/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.230228 139676058949632 adafactor.py:358] Since rank of parameter decoder/layers_19/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:14.247766 140316961150976 adafactor.py:358] Since rank of parameter decoder/layers_0/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.253142 140392160929792 adafactor.py:358] Since rank of parameter decoder/layers_0/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:14.253083 140172769531904 adafactor.py:358] Since rank of parameter decoder/layers_10/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.250750 140188559153152 adafactor.py:358] Since rank of parameter decoder/layers_21/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.231498 139812357228544 adafactor.py:358] Since rank of parameter decoder/layers_10/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:14.251831 139997397456896 adafactor.py:358] Since rank of parameter decoder/layers_16/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:14.254127 140392160929792 adafactor.py:358] Since rank of parameter decoder/layers_0/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.254175 140172769531904 adafactor.py:358] Since rank of parameter decoder/layers_11/extra_mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.231634 139676058949632 adafactor.py:358] Since rank of parameter decoder/layers_19/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.249346 140316961150976 adafactor.py:358] Since rank of parameter decoder/layers_0/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.252118 140188559153152 adafactor.py:358] Since rank of parameter decoder/layers_22/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.252981 139997397456896 adafactor.py:358] Since rank of parameter decoder/layers_16/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:14.232694 139812357228544 adafactor.py:358] Since rank of parameter decoder/layers_10/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.255311 140392160929792 adafactor.py:358] Since rank of parameter decoder/layers_0/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:14.255345 140172769531904 adafactor.py:358] Since rank of parameter decoder/layers_11/extra_mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.233090 139676058949632 adafactor.py:358] Since rank of parameter decoder/layers_19/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.253892 139997397456896 adafactor.py:358] Since rank of parameter decoder/layers_16/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:14.253277 140188559153152 adafactor.py:358] Since rank of parameter decoder/layers_22/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.233900 139812357228544 adafactor.py:358] Since rank of parameter decoder/layers_10/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.256482 140392160929792 adafactor.py:358] Since rank of parameter decoder/layers_0/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.256501 140172769531904 adafactor.py:358] Since rank of parameter decoder/layers_11/extra_mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:14.254858 139997397456896 adafactor.py:358] Since rank of parameter decoder/layers_16/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.235022 139812357228544 adafactor.py:358] Since rank of parameter decoder/layers_11/extra_mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.254498 140188559153152 adafactor.py:358] Since rank of parameter decoder/layers_22/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:14.234658 139676058949632 adafactor.py:358] Since rank of parameter decoder/layers_2/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.257743 140392160929792 adafactor.py:358] Since rank of parameter decoder/layers_0/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.255978 139997397456896 adafactor.py:358] Since rank of parameter decoder/layers_16/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:14.255874 140188559153152 adafactor.py:358] Since rank of parameter decoder/layers_22/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:14.236231 139812357228544 adafactor.py:358] Since rank of parameter decoder/layers_11/extra_mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.235949 139676058949632 adafactor.py:358] Since rank of parameter decoder/layers_2/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.258944 140392160929792 adafactor.py:358] Since rank of parameter decoder/layers_1/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.257170 139997397456896 adafactor.py:358] Since rank of parameter decoder/layers_16/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.254088 140316961150976 adafactor.py:358] Since rank of parameter decoder/layers_0/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:14.256781 140188559153152 adafactor.py:358] Since rank of parameter decoder/layers_22/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:14.237358 139812357228544 adafactor.py:358] Since rank of parameter decoder/layers_11/extra_mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:14.237058 139676058949632 adafactor.py:358] Since rank of parameter decoder/layers_2/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:14.260133 140392160929792 adafactor.py:358] Since rank of parameter decoder/layers_1/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.258310 139997397456896 adafactor.py:358] Since rank of parameter decoder/layers_16/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.257702 140188559153152 adafactor.py:358] Since rank of parameter decoder/layers_22/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.255434 140316961150976 adafactor.py:358] Since rank of parameter decoder/layers_0/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:14.260722 140172769531904 adafactor.py:358] Since rank of parameter decoder/layers_11/mlp/router/router_weights/w/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>, <FactorDim.NONE: None>) is ignored.
W0512 01:45:14.238216 139676058949632 adafactor.py:358] Since rank of parameter decoder/layers_2/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:14.261335 140392160929792 adafactor.py:358] Since rank of parameter decoder/layers_1/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:14.258850 140188559153152 adafactor.py:358] Since rank of parameter decoder/layers_22/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:14.256462 140316961150976 adafactor.py:358] Since rank of parameter decoder/layers_0/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:14.261630 140172769531904 adafactor.py:358] Since rank of parameter decoder/layers_11/pre_extra_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:14.239125 139676058949632 adafactor.py:358] Since rank of parameter decoder/layers_2/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:14.260011 139997397456896 adafactor.py:358] Since rank of parameter decoder/layers_17/extra_mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.262632 140392160929792 adafactor.py:358] Since rank of parameter decoder/layers_1/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:14.257409 140316961150976 adafactor.py:358] Since rank of parameter decoder/layers_0/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.262544 140172769531904 adafactor.py:358] Since rank of parameter decoder/layers_11/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:14.240075 139676058949632 adafactor.py:358] Since rank of parameter decoder/layers_2/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.260140 140188559153152 adafactor.py:358] Since rank of parameter decoder/layers_22/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.261167 139997397456896 adafactor.py:358] Since rank of parameter decoder/layers_17/extra_mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.263451 140172769531904 adafactor.py:358] Since rank of parameter decoder/layers_11/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:14.263613 140392160929792 adafactor.py:358] Since rank of parameter decoder/layers_1/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:14.241656 139812357228544 adafactor.py:358] Since rank of parameter decoder/layers_11/mlp/router/router_weights/w/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>, <FactorDim.NONE: None>) is ignored.
W0512 01:45:14.258607 140316961150976 adafactor.py:358] Since rank of parameter decoder/layers_0/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:14.261235 140188559153152 adafactor.py:358] Since rank of parameter decoder/layers_22/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.241228 139676058949632 adafactor.py:358] Since rank of parameter decoder/layers_2/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:14.262275 139997397456896 adafactor.py:358] Since rank of parameter decoder/layers_17/extra_mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:14.264616 140392160929792 adafactor.py:358] Since rank of parameter decoder/layers_1/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.242585 139812357228544 adafactor.py:358] Since rank of parameter decoder/layers_11/pre_extra_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:14.259771 140316961150976 adafactor.py:358] Since rank of parameter decoder/layers_0/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.242399 139676058949632 adafactor.py:358] Since rank of parameter decoder/layers_2/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.262374 140188559153152 adafactor.py:358] Since rank of parameter decoder/layers_23/extra_mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.265540 140172769531904 adafactor.py:358] Since rank of parameter decoder/layers_11/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.265807 140392160929792 adafactor.py:358] Since rank of parameter decoder/layers_1/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:14.243588 139812357228544 adafactor.py:358] Since rank of parameter decoder/layers_11/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:14.261008 140316961150976 adafactor.py:358] Since rank of parameter decoder/layers_0/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.243545 139676058949632 adafactor.py:358] Since rank of parameter decoder/layers_2/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.263462 140188559153152 adafactor.py:358] Since rank of parameter decoder/layers_23/extra_mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.244544 139812357228544 adafactor.py:358] Since rank of parameter decoder/layers_11/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:14.266640 140172769531904 adafactor.py:358] Since rank of parameter decoder/layers_11/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:14.267099 140392160929792 adafactor.py:358] Since rank of parameter decoder/layers_1/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.264622 140188559153152 adafactor.py:358] Since rank of parameter decoder/layers_23/extra_mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:14.262183 140316961150976 adafactor.py:358] Since rank of parameter decoder/layers_1/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.244748 139676058949632 adafactor.py:358] Since rank of parameter decoder/layers_20/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.267749 140172769531904 adafactor.py:358] Since rank of parameter decoder/layers_11/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.268296 140392160929792 adafactor.py:358] Since rank of parameter decoder/layers_1/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.266569 139997397456896 adafactor.py:358] Since rank of parameter decoder/layers_17/mlp/router/router_weights/w/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>, <FactorDim.NONE: None>) is ignored.
W0512 01:45:14.263488 140316961150976 adafactor.py:358] Since rank of parameter decoder/layers_1/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.245924 139676058949632 adafactor.py:358] Since rank of parameter decoder/layers_20/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.268915 140172769531904 adafactor.py:358] Since rank of parameter decoder/layers_11/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.246758 139812357228544 adafactor.py:358] Since rank of parameter decoder/layers_11/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.269493 140392160929792 adafactor.py:358] Since rank of parameter decoder/layers_10/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.267464 139997397456896 adafactor.py:358] Since rank of parameter decoder/layers_17/pre_extra_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:14.247059 139676058949632 adafactor.py:358] Since rank of parameter decoder/layers_20/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:14.264729 140316961150976 adafactor.py:358] Since rank of parameter decoder/layers_1/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:14.247883 139812357228544 adafactor.py:358] Since rank of parameter decoder/layers_11/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:14.270246 140172769531904 adafactor.py:358] Since rank of parameter decoder/layers_12/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.268404 139997397456896 adafactor.py:358] Since rank of parameter decoder/layers_17/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:14.270726 140392160929792 adafactor.py:358] Since rank of parameter decoder/layers_10/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.248196 139676058949632 adafactor.py:358] Since rank of parameter decoder/layers_20/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:14.266006 140316961150976 adafactor.py:358] Since rank of parameter decoder/layers_1/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:14.269334 139997397456896 adafactor.py:358] Since rank of parameter decoder/layers_17/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:14.271355 140172769531904 adafactor.py:358] Since rank of parameter decoder/layers_12/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.268814 140188559153152 adafactor.py:358] Since rank of parameter decoder/layers_23/mlp/router/router_weights/w/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>, <FactorDim.NONE: None>) is ignored.
W0512 01:45:14.249290 139812357228544 adafactor.py:358] Since rank of parameter decoder/layers_11/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.271984 140392160929792 adafactor.py:358] Since rank of parameter decoder/layers_10/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:14.249123 139676058949632 adafactor.py:358] Since rank of parameter decoder/layers_20/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:14.270210 139997397456896 adafactor.py:358] Since rank of parameter decoder/layers_17/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.266954 140316961150976 adafactor.py:358] Since rank of parameter decoder/layers_1/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:14.269739 140188559153152 adafactor.py:358] Since rank of parameter decoder/layers_23/pre_extra_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:14.272477 140172769531904 adafactor.py:358] Since rank of parameter decoder/layers_12/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:14.250396 139812357228544 adafactor.py:358] Since rank of parameter decoder/layers_11/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.250157 139676058949632 adafactor.py:358] Since rank of parameter decoder/layers_20/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.273180 140392160929792 adafactor.py:358] Since rank of parameter decoder/layers_10/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:14.267872 140316961150976 adafactor.py:358] Since rank of parameter decoder/layers_1/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.271327 139997397456896 adafactor.py:358] Since rank of parameter decoder/layers_17/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:14.273573 140172769531904 adafactor.py:358] Since rank of parameter decoder/layers_12/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:14.270706 140188559153152 adafactor.py:358] Since rank of parameter decoder/layers_23/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:14.251729 139812357228544 adafactor.py:358] Since rank of parameter decoder/layers_12/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.251303 139676058949632 adafactor.py:358] Since rank of parameter decoder/layers_20/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:14.274184 140392160929792 adafactor.py:358] Since rank of parameter decoder/layers_10/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:14.271632 140188559153152 adafactor.py:358] Since rank of parameter decoder/layers_23/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:14.269062 140316961150976 adafactor.py:358] Since rank of parameter decoder/layers_1/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:14.272426 139997397456896 adafactor.py:358] Since rank of parameter decoder/layers_17/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.274522 140172769531904 adafactor.py:358] Since rank of parameter decoder/layers_12/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:14.275162 140392160929792 adafactor.py:358] Since rank of parameter decoder/layers_10/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.252895 139812357228544 adafactor.py:358] Since rank of parameter decoder/layers_12/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.252428 139676058949632 adafactor.py:358] Since rank of parameter decoder/layers_20/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.272558 140188559153152 adafactor.py:358] Since rank of parameter decoder/layers_23/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.275430 140172769531904 adafactor.py:358] Since rank of parameter decoder/layers_12/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.270334 140316961150976 adafactor.py:358] Since rank of parameter decoder/layers_1/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.273649 139997397456896 adafactor.py:358] Since rank of parameter decoder/layers_17/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.254000 139812357228544 adafactor.py:358] Since rank of parameter decoder/layers_12/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:14.276409 140392160929792 adafactor.py:358] Since rank of parameter decoder/layers_10/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:14.253585 139676058949632 adafactor.py:358] Since rank of parameter decoder/layers_20/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.273732 140188559153152 adafactor.py:358] Since rank of parameter decoder/layers_23/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:14.276565 140172769531904 adafactor.py:358] Since rank of parameter decoder/layers_12/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:14.271510 140316961150976 adafactor.py:358] Since rank of parameter decoder/layers_1/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.274781 139997397456896 adafactor.py:358] Since rank of parameter decoder/layers_18/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
I0512 01:45:14.274873 140577131780096 moe_layers.py:777] Selected group_size=4096 and num_groups=192 for input num_tokens=786432, max_group_size=4096, num_experts=8 and num_expert_replicas=1
W0512 01:45:14.277630 140392160929792 adafactor.py:358] Since rank of parameter decoder/layers_10/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.274921 140188559153152 adafactor.py:358] Since rank of parameter decoder/layers_23/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.277660 140172769531904 adafactor.py:358] Since rank of parameter decoder/layers_12/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.272628 140316961150976 adafactor.py:358] Since rank of parameter decoder/layers_10/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.275897 139997397456896 adafactor.py:358] Since rank of parameter decoder/layers_18/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.255107 139812357228544 adafactor.py:358] Since rank of parameter decoder/layers_12/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:14.256069 139812357228544 adafactor.py:358] Since rank of parameter decoder/layers_12/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:14.255690 139676058949632 adafactor.py:358] Since rank of parameter decoder/layers_21/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.278787 140392160929792 adafactor.py:358] Since rank of parameter decoder/layers_10/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.276026 140188559153152 adafactor.py:358] Since rank of parameter decoder/layers_23/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.278846 140172769531904 adafactor.py:358] Since rank of parameter decoder/layers_12/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.277041 139997397456896 adafactor.py:358] Since rank of parameter decoder/layers_18/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:14.273813 140316961150976 adafactor.py:358] Since rank of parameter decoder/layers_10/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.257016 139812357228544 adafactor.py:358] Since rank of parameter decoder/layers_12/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.256817 139676058949632 adafactor.py:358] Since rank of parameter decoder/layers_21/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.279918 140392160929792 adafactor.py:358] Since rank of parameter decoder/layers_11/extra_mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.279938 140172769531904 adafactor.py:358] Since rank of parameter decoder/layers_13/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.277096 140188559153152 adafactor.py:358] Since rank of parameter decoder/layers_3/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.278219 139997397456896 adafactor.py:358] Since rank of parameter decoder/layers_18/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:14.275011 140316961150976 adafactor.py:358] Since rank of parameter decoder/layers_10/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:14.258198 139812357228544 adafactor.py:358] Since rank of parameter decoder/layers_12/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:14.257966 139676058949632 adafactor.py:358] Since rank of parameter decoder/layers_21/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:14.281110 140392160929792 adafactor.py:358] Since rank of parameter decoder/layers_11/extra_mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.278229 140188559153152 adafactor.py:358] Since rank of parameter decoder/layers_3/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.281074 140172769531904 adafactor.py:358] Since rank of parameter decoder/layers_13/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.279152 139997397456896 adafactor.py:358] Since rank of parameter decoder/layers_18/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:14.276197 140316961150976 adafactor.py:358] Since rank of parameter decoder/layers_10/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:14.259321 139812357228544 adafactor.py:358] Since rank of parameter decoder/layers_12/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.259101 139676058949632 adafactor.py:358] Since rank of parameter decoder/layers_21/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:14.280065 139997397456896 adafactor.py:358] Since rank of parameter decoder/layers_18/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.282286 140392160929792 adafactor.py:358] Since rank of parameter decoder/layers_11/extra_mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:14.279357 140188559153152 adafactor.py:358] Since rank of parameter decoder/layers_3/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:14.282155 140172769531904 adafactor.py:358] Since rank of parameter decoder/layers_13/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:14.277118 140316961150976 adafactor.py:358] Since rank of parameter decoder/layers_10/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:14.260506 139812357228544 adafactor.py:358] Since rank of parameter decoder/layers_12/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.260061 139676058949632 adafactor.py:358] Since rank of parameter decoder/layers_21/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:14.280418 140188559153152 adafactor.py:358] Since rank of parameter decoder/layers_3/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:14.281257 139997397456896 adafactor.py:358] Since rank of parameter decoder/layers_18/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:14.278106 140316961150976 adafactor.py:358] Since rank of parameter decoder/layers_10/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.283299 140172769531904 adafactor.py:358] Since rank of parameter decoder/layers_13/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:14.260981 139676058949632 adafactor.py:358] Since rank of parameter decoder/layers_21/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.281341 140188559153152 adafactor.py:358] Since rank of parameter decoder/layers_3/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:14.261589 139812357228544 adafactor.py:358] Since rank of parameter decoder/layers_13/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.284176 140172769531904 adafactor.py:358] Since rank of parameter decoder/layers_13/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:14.285131 140172769531904 adafactor.py:358] Since rank of parameter decoder/layers_13/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.282357 140188559153152 adafactor.py:358] Since rank of parameter decoder/layers_3/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.262131 139676058949632 adafactor.py:358] Since rank of parameter decoder/layers_21/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:14.262669 139812357228544 adafactor.py:358] Since rank of parameter decoder/layers_13/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.282427 139997397456896 adafactor.py:358] Since rank of parameter decoder/layers_18/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.283510 139997397456896 adafactor.py:358] Since rank of parameter decoder/layers_18/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.280738 140316961150976 adafactor.py:358] Since rank of parameter decoder/layers_10/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:14.283470 140188559153152 adafactor.py:358] Since rank of parameter decoder/layers_3/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:14.286226 140172769531904 adafactor.py:358] Since rank of parameter decoder/layers_13/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:14.263789 139812357228544 adafactor.py:358] Since rank of parameter decoder/layers_13/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:14.286605 140392160929792 adafactor.py:358] Since rank of parameter decoder/layers_11/mlp/router/router_weights/w/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>, <FactorDim.NONE: None>) is ignored.
W0512 01:45:14.284612 139997397456896 adafactor.py:358] Since rank of parameter decoder/layers_19/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.263261 139676058949632 adafactor.py:358] Since rank of parameter decoder/layers_21/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.281955 140316961150976 adafactor.py:358] Since rank of parameter decoder/layers_10/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.287539 140392160929792 adafactor.py:358] Since rank of parameter decoder/layers_11/pre_extra_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:14.264975 139812357228544 adafactor.py:358] Since rank of parameter decoder/layers_13/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:14.287379 140172769531904 adafactor.py:358] Since rank of parameter decoder/layers_13/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.285721 139997397456896 adafactor.py:358] Since rank of parameter decoder/layers_19/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.284598 140188559153152 adafactor.py:358] Since rank of parameter decoder/layers_3/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.265860 139812357228544 adafactor.py:358] Since rank of parameter decoder/layers_13/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:14.264408 139676058949632 adafactor.py:358] Since rank of parameter decoder/layers_21/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.283124 140316961150976 adafactor.py:358] Since rank of parameter decoder/layers_10/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.288443 140392160929792 adafactor.py:358] Since rank of parameter decoder/layers_11/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:14.288642 140172769531904 adafactor.py:358] Since rank of parameter decoder/layers_13/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.266744 139812357228544 adafactor.py:358] Since rank of parameter decoder/layers_13/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.286852 139997397456896 adafactor.py:358] Since rank of parameter decoder/layers_19/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:14.289363 140392160929792 adafactor.py:358] Since rank of parameter decoder/layers_11/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:14.265493 139676058949632 adafactor.py:358] Since rank of parameter decoder/layers_22/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.266598 139676058949632 adafactor.py:358] Since rank of parameter decoder/layers_22/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.285708 140188559153152 adafactor.py:358] Since rank of parameter decoder/layers_3/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.284227 140316961150976 adafactor.py:358] Since rank of parameter decoder/layers_11/extra_mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.289740 140172769531904 adafactor.py:358] Since rank of parameter decoder/layers_14/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.287946 139997397456896 adafactor.py:358] Since rank of parameter decoder/layers_19/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:14.267858 139812357228544 adafactor.py:358] Since rank of parameter decoder/layers_13/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:14.267693 139676058949632 adafactor.py:358] Since rank of parameter decoder/layers_22/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:14.286890 140188559153152 adafactor.py:358] Since rank of parameter decoder/layers_4/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.288006 140188559153152 adafactor.py:358] Since rank of parameter decoder/layers_4/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.288855 139997397456896 adafactor.py:358] Since rank of parameter decoder/layers_19/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:14.285505 140316961150976 adafactor.py:358] Since rank of parameter decoder/layers_11/extra_mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.291125 140392160929792 adafactor.py:358] Since rank of parameter decoder/layers_11/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.290813 140172769531904 adafactor.py:358] Since rank of parameter decoder/layers_14/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.269040 139812357228544 adafactor.py:358] Since rank of parameter decoder/layers_13/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.268839 139676058949632 adafactor.py:358] Since rank of parameter decoder/layers_22/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:14.289769 139997397456896 adafactor.py:358] Since rank of parameter decoder/layers_19/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.286672 140316961150976 adafactor.py:358] Since rank of parameter decoder/layers_11/extra_mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:14.291954 140172769531904 adafactor.py:358] Since rank of parameter decoder/layers_14/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:14.289118 140188559153152 adafactor.py:358] Since rank of parameter decoder/layers_4/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:14.270146 139812357228544 adafactor.py:358] Since rank of parameter decoder/layers_13/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.269739 139676058949632 adafactor.py:358] Since rank of parameter decoder/layers_22/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:14.292283 140392160929792 adafactor.py:358] Since rank of parameter decoder/layers_11/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:14.291008 139997397456896 adafactor.py:358] Since rank of parameter decoder/layers_19/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:14.270694 139676058949632 adafactor.py:358] Since rank of parameter decoder/layers_22/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.293115 140172769531904 adafactor.py:358] Since rank of parameter decoder/layers_14/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:14.271225 139812357228544 adafactor.py:358] Since rank of parameter decoder/layers_14/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.290233 140188559153152 adafactor.py:358] Since rank of parameter decoder/layers_4/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:14.293447 140392160929792 adafactor.py:358] Since rank of parameter decoder/layers_11/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.291310 140188559153152 adafactor.py:358] Since rank of parameter decoder/layers_4/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:14.294001 140172769531904 adafactor.py:358] Since rank of parameter decoder/layers_14/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:14.292088 139997397456896 adafactor.py:358] Since rank of parameter decoder/layers_19/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.271831 139676058949632 adafactor.py:358] Since rank of parameter decoder/layers_22/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:14.272349 139812357228544 adafactor.py:358] Since rank of parameter decoder/layers_14/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.292199 140188559153152 adafactor.py:358] Since rank of parameter decoder/layers_4/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.294879 140172769531904 adafactor.py:358] Since rank of parameter decoder/layers_14/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.293231 139997397456896 adafactor.py:358] Since rank of parameter decoder/layers_19/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.294623 140392160929792 adafactor.py:358] Since rank of parameter decoder/layers_11/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.273021 139676058949632 adafactor.py:358] Since rank of parameter decoder/layers_22/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.273649 139812357228544 adafactor.py:358] Since rank of parameter decoder/layers_14/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:14.295970 140172769531904 adafactor.py:358] Since rank of parameter decoder/layers_14/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:14.293305 140188559153152 adafactor.py:358] Since rank of parameter decoder/layers_4/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:14.290923 140316961150976 adafactor.py:358] Since rank of parameter decoder/layers_11/mlp/router/router_weights/w/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>, <FactorDim.NONE: None>) is ignored.
W0512 01:45:14.294308 139997397456896 adafactor.py:358] Since rank of parameter decoder/layers_2/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.295945 140392160929792 adafactor.py:358] Since rank of parameter decoder/layers_12/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.291860 140316961150976 adafactor.py:358] Since rank of parameter decoder/layers_11/pre_extra_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:14.294450 140188559153152 adafactor.py:358] Since rank of parameter decoder/layers_4/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.297136 140172769531904 adafactor.py:358] Since rank of parameter decoder/layers_14/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.274234 139676058949632 adafactor.py:358] Since rank of parameter decoder/layers_22/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.295434 139997397456896 adafactor.py:358] Since rank of parameter decoder/layers_2/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.275658 139812357228544 adafactor.py:358] Since rank of parameter decoder/layers_14/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:14.297049 140392160929792 adafactor.py:358] Since rank of parameter decoder/layers_12/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.298189 140392160929792 adafactor.py:358] Since rank of parameter decoder/layers_12/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:14.292760 140316961150976 adafactor.py:358] Since rank of parameter decoder/layers_11/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:14.298231 140172769531904 adafactor.py:358] Since rank of parameter decoder/layers_14/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.275367 139676058949632 adafactor.py:358] Since rank of parameter decoder/layers_23/extra_mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.296505 139997397456896 adafactor.py:358] Since rank of parameter decoder/layers_2/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:14.296001 140188559153152 adafactor.py:358] Since rank of parameter decoder/layers_4/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.293718 140316961150976 adafactor.py:358] Since rank of parameter decoder/layers_11/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:14.276782 139812357228544 adafactor.py:358] Since rank of parameter decoder/layers_14/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:14.299281 140392160929792 adafactor.py:358] Since rank of parameter decoder/layers_12/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:14.299385 140172769531904 adafactor.py:358] Since rank of parameter decoder/layers_15/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.297618 139997397456896 adafactor.py:358] Since rank of parameter decoder/layers_2/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:14.276521 139676058949632 adafactor.py:358] Since rank of parameter decoder/layers_23/extra_mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.277679 139812357228544 adafactor.py:358] Since rank of parameter decoder/layers_14/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.297092 140188559153152 adafactor.py:358] Since rank of parameter decoder/layers_5/extra_mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.294691 140316961150976 adafactor.py:358] Since rank of parameter decoder/layers_11/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.300237 140392160929792 adafactor.py:358] Since rank of parameter decoder/layers_12/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:14.298507 139997397456896 adafactor.py:358] Since rank of parameter decoder/layers_2/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:14.300503 140172769531904 adafactor.py:358] Since rank of parameter decoder/layers_15/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.298162 140188559153152 adafactor.py:358] Since rank of parameter decoder/layers_5/extra_mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.295789 140316961150976 adafactor.py:358] Since rank of parameter decoder/layers_11/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:14.301135 140392160929792 adafactor.py:358] Since rank of parameter decoder/layers_12/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.278810 139812357228544 adafactor.py:358] Since rank of parameter decoder/layers_14/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:14.277696 139676058949632 adafactor.py:358] Since rank of parameter decoder/layers_23/extra_mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:14.299463 139997397456896 adafactor.py:358] Since rank of parameter decoder/layers_2/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.301651 140172769531904 adafactor.py:358] Since rank of parameter decoder/layers_15/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:14.299279 140188559153152 adafactor.py:358] Since rank of parameter decoder/layers_5/extra_mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:14.302264 140392160929792 adafactor.py:358] Since rank of parameter decoder/layers_12/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:14.279967 139812357228544 adafactor.py:358] Since rank of parameter decoder/layers_14/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.296957 140316961150976 adafactor.py:358] Since rank of parameter decoder/layers_11/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.300559 139997397456896 adafactor.py:358] Since rank of parameter decoder/layers_2/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:14.302744 140172769531904 adafactor.py:358] Since rank of parameter decoder/layers_15/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:14.298099 140316961150976 adafactor.py:358] Since rank of parameter decoder/layers_11/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.281214 139812357228544 adafactor.py:358] Since rank of parameter decoder/layers_14/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.303354 140392160929792 adafactor.py:358] Since rank of parameter decoder/layers_12/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.301703 139997397456896 adafactor.py:358] Since rank of parameter decoder/layers_2/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.303655 140172769531904 adafactor.py:358] Since rank of parameter decoder/layers_15/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:14.299372 140316961150976 adafactor.py:358] Since rank of parameter decoder/layers_12/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.282310 139812357228544 adafactor.py:358] Since rank of parameter decoder/layers_15/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.304587 140172769531904 adafactor.py:358] Since rank of parameter decoder/layers_15/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.302810 139997397456896 adafactor.py:358] Since rank of parameter decoder/layers_2/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.282080 139676058949632 adafactor.py:358] Since rank of parameter decoder/layers_23/mlp/router/router_weights/w/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>, <FactorDim.NONE: None>) is ignored.
W0512 01:45:14.304570 140392160929792 adafactor.py:358] Since rank of parameter decoder/layers_12/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.305734 140172769531904 adafactor.py:358] Since rank of parameter decoder/layers_15/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:14.300472 140316961150976 adafactor.py:358] Since rank of parameter decoder/layers_12/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.283037 139676058949632 adafactor.py:358] Since rank of parameter decoder/layers_23/pre_extra_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:14.283439 139812357228544 adafactor.py:358] Since rank of parameter decoder/layers_15/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.303981 139997397456896 adafactor.py:358] Since rank of parameter decoder/layers_20/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.303390 140188559153152 adafactor.py:358] Since rank of parameter decoder/layers_5/mlp/router/router_weights/w/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>, <FactorDim.NONE: None>) is ignored.
W0512 01:45:14.305735 140392160929792 adafactor.py:358] Since rank of parameter decoder/layers_13/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.301623 140316961150976 adafactor.py:358] Since rank of parameter decoder/layers_12/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:14.306827 140172769531904 adafactor.py:358] Since rank of parameter decoder/layers_15/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.284719 139812357228544 adafactor.py:358] Since rank of parameter decoder/layers_15/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:14.304346 140188559153152 adafactor.py:358] Since rank of parameter decoder/layers_5/pre_extra_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:14.305136 139997397456896 adafactor.py:358] Since rank of parameter decoder/layers_20/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.284056 139676058949632 adafactor.py:358] Since rank of parameter decoder/layers_23/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:14.284991 139676058949632 adafactor.py:358] Since rank of parameter decoder/layers_23/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:14.302728 140316961150976 adafactor.py:358] Since rank of parameter decoder/layers_12/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:14.306919 140392160929792 adafactor.py:358] Since rank of parameter decoder/layers_13/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.308068 140392160929792 adafactor.py:358] Since rank of parameter decoder/layers_13/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:14.307910 140172769531904 adafactor.py:358] Since rank of parameter decoder/layers_15/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.305226 140188559153152 adafactor.py:358] Since rank of parameter decoder/layers_5/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:14.306210 139997397456896 adafactor.py:358] Since rank of parameter decoder/layers_20/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:14.285963 139676058949632 adafactor.py:358] Since rank of parameter decoder/layers_23/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.286470 139812357228544 adafactor.py:358] Since rank of parameter decoder/layers_15/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:14.306094 140188559153152 adafactor.py:358] Since rank of parameter decoder/layers_5/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:14.303693 140316961150976 adafactor.py:358] Since rank of parameter decoder/layers_12/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:14.309043 140172769531904 adafactor.py:358] Since rank of parameter decoder/layers_16/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.307309 139997397456896 adafactor.py:358] Since rank of parameter decoder/layers_20/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:14.287468 139812357228544 adafactor.py:358] Since rank of parameter decoder/layers_15/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:14.306997 140188559153152 adafactor.py:358] Since rank of parameter decoder/layers_5/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.309257 140392160929792 adafactor.py:358] Since rank of parameter decoder/layers_13/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:14.304594 140316961150976 adafactor.py:358] Since rank of parameter decoder/layers_12/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.287154 139676058949632 adafactor.py:358] Since rank of parameter decoder/layers_23/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:14.310234 140392160929792 adafactor.py:358] Since rank of parameter decoder/layers_13/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:14.308201 139997397456896 adafactor.py:358] Since rank of parameter decoder/layers_20/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:14.310187 140172769531904 adafactor.py:358] Since rank of parameter decoder/layers_16/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.308153 140188559153152 adafactor.py:358] Since rank of parameter decoder/layers_5/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:14.305754 140316961150976 adafactor.py:358] Since rank of parameter decoder/layers_12/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:14.311154 140392160929792 adafactor.py:358] Since rank of parameter decoder/layers_13/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.288841 139812357228544 adafactor.py:358] Since rank of parameter decoder/layers_15/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.309176 139997397456896 adafactor.py:358] Since rank of parameter decoder/layers_20/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.311282 140172769531904 adafactor.py:358] Since rank of parameter decoder/layers_16/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:14.288299 139676058949632 adafactor.py:358] Since rank of parameter decoder/layers_23/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.306845 140316961150976 adafactor.py:358] Since rank of parameter decoder/layers_12/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.309319 140188559153152 adafactor.py:358] Since rank of parameter decoder/layers_5/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.310275 139997397456896 adafactor.py:358] Since rank of parameter decoder/layers_20/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:14.312294 140392160929792 adafactor.py:358] Since rank of parameter decoder/layers_13/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:14.290162 139812357228544 adafactor.py:358] Since rank of parameter decoder/layers_15/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:14.312362 140172769531904 adafactor.py:358] Since rank of parameter decoder/layers_16/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:14.310461 140188559153152 adafactor.py:358] Since rank of parameter decoder/layers_5/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.308012 140316961150976 adafactor.py:358] Since rank of parameter decoder/layers_12/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.289439 139676058949632 adafactor.py:358] Since rank of parameter decoder/layers_23/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.311369 139997397456896 adafactor.py:358] Since rank of parameter decoder/layers_20/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.313305 140172769531904 adafactor.py:358] Since rank of parameter decoder/layers_16/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:14.313552 140392160929792 adafactor.py:358] Since rank of parameter decoder/layers_13/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.311570 140188559153152 adafactor.py:358] Since rank of parameter decoder/layers_6/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.314229 140172769531904 adafactor.py:358] Since rank of parameter decoder/layers_16/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.309126 140316961150976 adafactor.py:358] Since rank of parameter decoder/layers_13/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.290604 139676058949632 adafactor.py:358] Since rank of parameter decoder/layers_3/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.312451 139997397456896 adafactor.py:358] Since rank of parameter decoder/layers_20/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.314707 140392160929792 adafactor.py:358] Since rank of parameter decoder/layers_13/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.312636 140188559153152 adafactor.py:358] Since rank of parameter decoder/layers_6/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.291791 139676058949632 adafactor.py:358] Since rank of parameter decoder/layers_3/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.315330 140172769531904 adafactor.py:358] Since rank of parameter decoder/layers_16/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:14.310291 140316961150976 adafactor.py:358] Since rank of parameter decoder/layers_13/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.314315 139997397456896 adafactor.py:358] Since rank of parameter decoder/layers_21/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.313751 140188559153152 adafactor.py:358] Since rank of parameter decoder/layers_6/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:14.315884 140392160929792 adafactor.py:358] Since rank of parameter decoder/layers_14/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.316438 140172769531904 adafactor.py:358] Since rank of parameter decoder/layers_16/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.311416 140316961150976 adafactor.py:358] Since rank of parameter decoder/layers_13/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:14.292928 139676058949632 adafactor.py:358] Since rank of parameter decoder/layers_3/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:14.294803 139812357228544 adafactor.py:358] Since rank of parameter decoder/layers_15/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.294142 139676058949632 adafactor.py:358] Since rank of parameter decoder/layers_3/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:14.314884 140188559153152 adafactor.py:358] Since rank of parameter decoder/layers_6/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:14.315438 139997397456896 adafactor.py:358] Since rank of parameter decoder/layers_21/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.317636 140172769531904 adafactor.py:358] Since rank of parameter decoder/layers_16/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.295083 139676058949632 adafactor.py:358] Since rank of parameter decoder/layers_3/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:14.312559 140316961150976 adafactor.py:358] Since rank of parameter decoder/layers_13/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:14.317051 140392160929792 adafactor.py:358] Since rank of parameter decoder/layers_14/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.318285 140392160929792 adafactor.py:358] Since rank of parameter decoder/layers_14/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:14.315765 140188559153152 adafactor.py:358] Since rank of parameter decoder/layers_6/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:14.316547 139997397456896 adafactor.py:358] Since rank of parameter decoder/layers_21/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:14.313459 140316961150976 adafactor.py:358] Since rank of parameter decoder/layers_13/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:14.296506 139812357228544 adafactor.py:358] Since rank of parameter decoder/layers_15/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.296069 139676058949632 adafactor.py:358] Since rank of parameter decoder/layers_3/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.316634 140188559153152 adafactor.py:358] Since rank of parameter decoder/layers_6/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.319436 140392160929792 adafactor.py:358] Since rank of parameter decoder/layers_14/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:14.314393 140316961150976 adafactor.py:358] Since rank of parameter decoder/layers_13/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.319607 140172769531904 adafactor.py:358] Since rank of parameter decoder/layers_17/extra_mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.317691 139997397456896 adafactor.py:358] Since rank of parameter decoder/layers_21/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:14.297628 139812357228544 adafactor.py:358] Since rank of parameter decoder/layers_16/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.297222 139676058949632 adafactor.py:358] Since rank of parameter decoder/layers_3/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:14.320367 140392160929792 adafactor.py:358] Since rank of parameter decoder/layers_14/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:14.317743 140188559153152 adafactor.py:358] Since rank of parameter decoder/layers_6/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:14.318726 139997397456896 adafactor.py:358] Since rank of parameter decoder/layers_21/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:14.315500 140316961150976 adafactor.py:358] Since rank of parameter decoder/layers_13/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:14.320725 140172769531904 adafactor.py:358] Since rank of parameter decoder/layers_17/extra_mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.321274 140392160929792 adafactor.py:358] Since rank of parameter decoder/layers_14/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.298400 139676058949632 adafactor.py:358] Since rank of parameter decoder/layers_3/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.299338 139812357228544 adafactor.py:358] Since rank of parameter decoder/layers_16/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.319615 139997397456896 adafactor.py:358] Since rank of parameter decoder/layers_21/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.318845 140188559153152 adafactor.py:358] Since rank of parameter decoder/layers_6/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.316696 140316961150976 adafactor.py:358] Since rank of parameter decoder/layers_13/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.321858 140172769531904 adafactor.py:358] Since rank of parameter decoder/layers_17/extra_mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:14.322438 140392160929792 adafactor.py:358] Since rank of parameter decoder/layers_14/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:14.299553 139676058949632 adafactor.py:358] Since rank of parameter decoder/layers_3/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.319907 140188559153152 adafactor.py:358] Since rank of parameter decoder/layers_6/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.320693 139997397456896 adafactor.py:358] Since rank of parameter decoder/layers_21/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:14.300605 139812357228544 adafactor.py:358] Since rank of parameter decoder/layers_16/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:14.317859 140316961150976 adafactor.py:358] Since rank of parameter decoder/layers_13/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.323598 140392160929792 adafactor.py:358] Since rank of parameter decoder/layers_14/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.300743 139676058949632 adafactor.py:358] Since rank of parameter decoder/layers_4/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.320983 140188559153152 adafactor.py:358] Since rank of parameter decoder/layers_7/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.321829 139997397456896 adafactor.py:358] Since rank of parameter decoder/layers_21/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.318949 140316961150976 adafactor.py:358] Since rank of parameter decoder/layers_14/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.302249 139812357228544 adafactor.py:358] Since rank of parameter decoder/layers_16/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:14.324764 140392160929792 adafactor.py:358] Since rank of parameter decoder/layers_14/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.301909 139676058949632 adafactor.py:358] Since rank of parameter decoder/layers_4/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.322094 140188559153152 adafactor.py:358] Since rank of parameter decoder/layers_7/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.322963 139997397456896 adafactor.py:358] Since rank of parameter decoder/layers_21/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.320050 140316961150976 adafactor.py:358] Since rank of parameter decoder/layers_14/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.303167 139812357228544 adafactor.py:358] Since rank of parameter decoder/layers_16/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:14.303043 139676058949632 adafactor.py:358] Since rank of parameter decoder/layers_4/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:14.323207 140188559153152 adafactor.py:358] Since rank of parameter decoder/layers_7/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:14.325948 140172769531904 adafactor.py:358] Since rank of parameter decoder/layers_17/mlp/router/router_weights/w/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>, <FactorDim.NONE: None>) is ignored.
W0512 01:45:14.324109 139997397456896 adafactor.py:358] Since rank of parameter decoder/layers_22/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.321191 140316961150976 adafactor.py:358] Since rank of parameter decoder/layers_14/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:14.304261 139812357228544 adafactor.py:358] Since rank of parameter decoder/layers_16/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.325891 140392160929792 adafactor.py:358] Since rank of parameter decoder/layers_15/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.304169 139676058949632 adafactor.py:358] Since rank of parameter decoder/layers_4/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:14.326848 140172769531904 adafactor.py:358] Since rank of parameter decoder/layers_17/pre_extra_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:14.324356 140188559153152 adafactor.py:358] Since rank of parameter decoder/layers_7/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:14.325232 139997397456896 adafactor.py:358] Since rank of parameter decoder/layers_22/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.322336 140316961150976 adafactor.py:358] Since rank of parameter decoder/layers_14/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:14.305380 139812357228544 adafactor.py:358] Since rank of parameter decoder/layers_16/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:14.305142 139676058949632 adafactor.py:358] Since rank of parameter decoder/layers_4/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:14.325244 140188559153152 adafactor.py:358] Since rank of parameter decoder/layers_7/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:14.327783 140172769531904 adafactor.py:358] Since rank of parameter decoder/layers_17/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:14.327059 140392160929792 adafactor.py:358] Since rank of parameter decoder/layers_15/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.326313 139997397456896 adafactor.py:358] Since rank of parameter decoder/layers_22/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:14.323238 140316961150976 adafactor.py:358] Since rank of parameter decoder/layers_14/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:14.328706 140172769531904 adafactor.py:358] Since rank of parameter decoder/layers_17/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:14.306139 139676058949632 adafactor.py:358] Since rank of parameter decoder/layers_4/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.306476 139812357228544 adafactor.py:358] Since rank of parameter decoder/layers_16/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.326167 140188559153152 adafactor.py:358] Since rank of parameter decoder/layers_7/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.328258 140392160929792 adafactor.py:358] Since rank of parameter decoder/layers_15/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:14.329438 140392160929792 adafactor.py:358] Since rank of parameter decoder/layers_15/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:14.324156 140316961150976 adafactor.py:358] Since rank of parameter decoder/layers_14/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.327456 139997397456896 adafactor.py:358] Since rank of parameter decoder/layers_22/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:14.329591 140172769531904 adafactor.py:358] Since rank of parameter decoder/layers_17/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.307268 139676058949632 adafactor.py:358] Since rank of parameter decoder/layers_4/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:14.307645 139812357228544 adafactor.py:358] Since rank of parameter decoder/layers_16/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.327307 140188559153152 adafactor.py:358] Since rank of parameter decoder/layers_7/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:14.328353 139997397456896 adafactor.py:358] Since rank of parameter decoder/layers_22/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:14.330500 140392160929792 adafactor.py:358] Since rank of parameter decoder/layers_15/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:14.325268 140316961150976 adafactor.py:358] Since rank of parameter decoder/layers_14/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:14.330685 140172769531904 adafactor.py:358] Since rank of parameter decoder/layers_17/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:14.328426 140188559153152 adafactor.py:358] Since rank of parameter decoder/layers_7/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.308398 139676058949632 adafactor.py:358] Since rank of parameter decoder/layers_4/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.329274 139997397456896 adafactor.py:358] Since rank of parameter decoder/layers_22/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.331602 140392160929792 adafactor.py:358] Since rank of parameter decoder/layers_15/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.326447 140316961150976 adafactor.py:358] Since rank of parameter decoder/layers_14/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.331782 140172769531904 adafactor.py:358] Since rank of parameter decoder/layers_17/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.309722 139812357228544 adafactor.py:358] Since rank of parameter decoder/layers_17/extra_mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.329563 140188559153152 adafactor.py:358] Since rank of parameter decoder/layers_7/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.330385 139997397456896 adafactor.py:358] Since rank of parameter decoder/layers_22/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:14.309981 139676058949632 adafactor.py:358] Since rank of parameter decoder/layers_4/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.327554 140316961150976 adafactor.py:358] Since rank of parameter decoder/layers_14/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.333007 140392160929792 adafactor.py:358] Since rank of parameter decoder/layers_15/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:14.332970 140172769531904 adafactor.py:358] Since rank of parameter decoder/layers_17/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.310851 139812357228544 adafactor.py:358] Since rank of parameter decoder/layers_17/extra_mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.331544 139997397456896 adafactor.py:358] Since rank of parameter decoder/layers_22/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.330766 140188559153152 adafactor.py:358] Since rank of parameter decoder/layers_8/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.311123 139676058949632 adafactor.py:358] Since rank of parameter decoder/layers_5/extra_mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.328655 140316961150976 adafactor.py:358] Since rank of parameter decoder/layers_15/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.334101 140172769531904 adafactor.py:358] Since rank of parameter decoder/layers_18/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.334424 140392160929792 adafactor.py:358] Since rank of parameter decoder/layers_15/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.311949 139812357228544 adafactor.py:358] Since rank of parameter decoder/layers_17/extra_mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:14.332662 139997397456896 adafactor.py:358] Since rank of parameter decoder/layers_22/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.331878 140188559153152 adafactor.py:358] Since rank of parameter decoder/layers_8/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.312261 139676058949632 adafactor.py:358] Since rank of parameter decoder/layers_5/extra_mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.329918 140316961150976 adafactor.py:358] Since rank of parameter decoder/layers_15/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.335186 140172769531904 adafactor.py:358] Since rank of parameter decoder/layers_18/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.333000 140188559153152 adafactor.py:358] Since rank of parameter decoder/layers_8/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:14.335800 140392160929792 adafactor.py:358] Since rank of parameter decoder/layers_15/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.333834 139997397456896 adafactor.py:358] Since rank of parameter decoder/layers_23/extra_mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.313361 139676058949632 adafactor.py:358] Since rank of parameter decoder/layers_5/extra_mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:14.331072 140316961150976 adafactor.py:358] Since rank of parameter decoder/layers_15/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:14.336283 140172769531904 adafactor.py:358] Since rank of parameter decoder/layers_18/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:14.334118 140188559153152 adafactor.py:358] Since rank of parameter decoder/layers_8/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:14.334983 139997397456896 adafactor.py:358] Since rank of parameter decoder/layers_23/extra_mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.337084 140392160929792 adafactor.py:358] Since rank of parameter decoder/layers_16/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.332243 140316961150976 adafactor.py:358] Since rank of parameter decoder/layers_15/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:14.337536 140172769531904 adafactor.py:358] Since rank of parameter decoder/layers_18/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:14.335136 140188559153152 adafactor.py:358] Since rank of parameter decoder/layers_8/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:14.336165 139997397456896 adafactor.py:358] Since rank of parameter decoder/layers_23/extra_mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:14.338437 140392160929792 adafactor.py:358] Since rank of parameter decoder/layers_16/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.316100 139812357228544 adafactor.py:358] Since rank of parameter decoder/layers_17/mlp/router/router_weights/w/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>, <FactorDim.NONE: None>) is ignored.
W0512 01:45:14.333147 140316961150976 adafactor.py:358] Since rank of parameter decoder/layers_15/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:14.338521 140172769531904 adafactor.py:358] Since rank of parameter decoder/layers_18/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:14.336019 140188559153152 adafactor.py:358] Since rank of parameter decoder/layers_8/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.334123 140316961150976 adafactor.py:358] Since rank of parameter decoder/layers_15/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.317066 139812357228544 adafactor.py:358] Since rank of parameter decoder/layers_17/pre_extra_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:14.339485 140172769531904 adafactor.py:358] Since rank of parameter decoder/layers_18/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.339753 140392160929792 adafactor.py:358] Since rank of parameter decoder/layers_16/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:14.337091 140188559153152 adafactor.py:358] Since rank of parameter decoder/layers_8/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:14.318005 139812357228544 adafactor.py:358] Since rank of parameter decoder/layers_17/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:14.340691 140172769531904 adafactor.py:358] Since rank of parameter decoder/layers_18/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:14.338174 140188559153152 adafactor.py:358] Since rank of parameter decoder/layers_8/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.341098 140392160929792 adafactor.py:358] Since rank of parameter decoder/layers_16/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:14.318903 139812357228544 adafactor.py:358] Since rank of parameter decoder/layers_17/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:14.318607 139676058949632 adafactor.py:358] Since rank of parameter decoder/layers_5/mlp/router/router_weights/w/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>, <FactorDim.NONE: None>) is ignored.
W0512 01:45:14.336345 140316961150976 adafactor.py:358] Since rank of parameter decoder/layers_15/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:14.341897 140172769531904 adafactor.py:358] Since rank of parameter decoder/layers_18/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.319796 139812357228544 adafactor.py:358] Since rank of parameter decoder/layers_17/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.342241 140392160929792 adafactor.py:358] Since rank of parameter decoder/layers_16/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:14.339382 140188559153152 adafactor.py:358] Since rank of parameter decoder/layers_8/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.340433 139997397456896 adafactor.py:358] Since rank of parameter decoder/layers_23/mlp/router/router_weights/w/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>, <FactorDim.NONE: None>) is ignored.
W0512 01:45:14.319733 139676058949632 adafactor.py:358] Since rank of parameter decoder/layers_5/pre_extra_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:14.337459 140316961150976 adafactor.py:358] Since rank of parameter decoder/layers_15/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.343046 140172769531904 adafactor.py:358] Since rank of parameter decoder/layers_18/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.320969 139812357228544 adafactor.py:358] Since rank of parameter decoder/layers_17/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:14.340460 140188559153152 adafactor.py:358] Since rank of parameter decoder/layers_9/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.343393 140392160929792 adafactor.py:358] Since rank of parameter decoder/layers_16/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.341383 139997397456896 adafactor.py:358] Since rank of parameter decoder/layers_23/pre_extra_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:14.320681 139676058949632 adafactor.py:358] Since rank of parameter decoder/layers_5/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:14.338612 140316961150976 adafactor.py:358] Since rank of parameter decoder/layers_15/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.344209 140172769531904 adafactor.py:358] Since rank of parameter decoder/layers_19/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.341556 140188559153152 adafactor.py:358] Since rank of parameter decoder/layers_9/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.321589 139676058949632 adafactor.py:358] Since rank of parameter decoder/layers_5/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:14.322071 139812357228544 adafactor.py:358] Since rank of parameter decoder/layers_17/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.344588 140392160929792 adafactor.py:358] Since rank of parameter decoder/layers_16/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:14.342272 139997397456896 adafactor.py:358] Since rank of parameter decoder/layers_23/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:14.339720 140316961150976 adafactor.py:358] Since rank of parameter decoder/layers_16/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.343157 139997397456896 adafactor.py:358] Since rank of parameter decoder/layers_23/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:14.322521 139676058949632 adafactor.py:358] Since rank of parameter decoder/layers_5/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.342672 140188559153152 adafactor.py:358] Since rank of parameter decoder/layers_9/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:14.345403 140172769531904 adafactor.py:358] Since rank of parameter decoder/layers_19/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.323210 139812357228544 adafactor.py:358] Since rank of parameter decoder/layers_17/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.345703 140392160929792 adafactor.py:358] Since rank of parameter decoder/layers_16/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.344092 139997397456896 adafactor.py:358] Since rank of parameter decoder/layers_23/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.340891 140316961150976 adafactor.py:358] Since rank of parameter decoder/layers_16/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.323673 139676058949632 adafactor.py:358] Since rank of parameter decoder/layers_5/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:14.343787 140188559153152 adafactor.py:358] Since rank of parameter decoder/layers_9/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:14.346629 140172769531904 adafactor.py:358] Since rank of parameter decoder/layers_19/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:14.324441 139812357228544 adafactor.py:358] Since rank of parameter decoder/layers_18/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.347041 140392160929792 adafactor.py:358] Since rank of parameter decoder/layers_16/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.342024 140316961150976 adafactor.py:358] Since rank of parameter decoder/layers_16/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:14.345269 139997397456896 adafactor.py:358] Since rank of parameter decoder/layers_23/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:14.344671 140188559153152 adafactor.py:358] Since rank of parameter decoder/layers_9/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:14.324810 139676058949632 adafactor.py:358] Since rank of parameter decoder/layers_5/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.347809 140172769531904 adafactor.py:358] Since rank of parameter decoder/layers_19/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:14.325534 139812357228544 adafactor.py:358] Since rank of parameter decoder/layers_18/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.345561 140188559153152 adafactor.py:358] Since rank of parameter decoder/layers_9/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.343116 140316961150976 adafactor.py:358] Since rank of parameter decoder/layers_16/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:14.346377 139997397456896 adafactor.py:358] Since rank of parameter decoder/layers_23/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.325942 139676058949632 adafactor.py:358] Since rank of parameter decoder/layers_5/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.348790 140172769531904 adafactor.py:358] Since rank of parameter decoder/layers_19/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:14.348851 140392160929792 adafactor.py:358] Since rank of parameter decoder/layers_17/extra_mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.326635 139812357228544 adafactor.py:358] Since rank of parameter decoder/layers_18/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:14.344011 140316961150976 adafactor.py:358] Since rank of parameter decoder/layers_16/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:14.346663 140188559153152 adafactor.py:358] Since rank of parameter decoder/layers_9/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:14.347469 139997397456896 adafactor.py:358] Since rank of parameter decoder/layers_23/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.327025 139676058949632 adafactor.py:358] Since rank of parameter decoder/layers_6/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.349750 140172769531904 adafactor.py:358] Since rank of parameter decoder/layers_19/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.349992 140392160929792 adafactor.py:358] Since rank of parameter decoder/layers_17/extra_mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.344939 140316961150976 adafactor.py:358] Since rank of parameter decoder/layers_16/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.327780 139812357228544 adafactor.py:358] Since rank of parameter decoder/layers_18/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:14.347749 140188559153152 adafactor.py:358] Since rank of parameter decoder/layers_9/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.348576 139997397456896 adafactor.py:358] Since rank of parameter decoder/layers_3/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.328115 139676058949632 adafactor.py:358] Since rank of parameter decoder/layers_6/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.350976 140172769531904 adafactor.py:358] Since rank of parameter decoder/layers_19/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:14.351121 140392160929792 adafactor.py:358] Since rank of parameter decoder/layers_17/extra_mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:14.328728 139812357228544 adafactor.py:358] Since rank of parameter decoder/layers_18/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:14.346075 140316961150976 adafactor.py:358] Since rank of parameter decoder/layers_16/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:14.329628 139812357228544 adafactor.py:358] Since rank of parameter decoder/layers_18/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.349754 139997397456896 adafactor.py:358] Since rank of parameter decoder/layers_3/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.329249 139676058949632 adafactor.py:358] Since rank of parameter decoder/layers_6/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:14.352129 140172769531904 adafactor.py:358] Since rank of parameter decoder/layers_19/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.349393 140188559153152 adafactor.py:358] Since rank of parameter decoder/layers_9/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.347185 140316961150976 adafactor.py:358] Since rank of parameter decoder/layers_16/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.330721 139812357228544 adafactor.py:358] Since rank of parameter decoder/layers_18/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:14.330370 139676058949632 adafactor.py:358] Since rank of parameter decoder/layers_6/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:14.350853 139997397456896 adafactor.py:358] Since rank of parameter decoder/layers_3/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:14.350517 140188559153152 adafactor.py:358] Since rank of parameter decoder/logits_dense/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.353317 140172769531904 adafactor.py:358] Since rank of parameter decoder/layers_19/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.348364 140316961150976 adafactor.py:358] Since rank of parameter decoder/layers_16/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.331264 139676058949632 adafactor.py:358] Since rank of parameter decoder/layers_6/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:14.331861 139812357228544 adafactor.py:358] Since rank of parameter decoder/layers_18/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.351591 140188559153152 adafactor.py:358] Since rank of parameter token_embedder/embedding 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:14.351960 139997397456896 adafactor.py:358] Since rank of parameter decoder/layers_3/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:14.354489 140172769531904 adafactor.py:358] Since rank of parameter decoder/layers_2/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.349520 140316961150976 adafactor.py:358] Since rank of parameter decoder/layers_17/extra_mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.332142 139676058949632 adafactor.py:358] Since rank of parameter decoder/layers_6/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.352855 139997397456896 adafactor.py:358] Since rank of parameter decoder/layers_3/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:14.355335 140392160929792 adafactor.py:358] Since rank of parameter decoder/layers_17/mlp/router/router_weights/w/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>, <FactorDim.NONE: None>) is ignored.
W0512 01:45:14.333027 139812357228544 adafactor.py:358] Since rank of parameter decoder/layers_18/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.355685 140172769531904 adafactor.py:358] Since rank of parameter decoder/layers_2/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.353826 139997397456896 adafactor.py:358] Since rank of parameter decoder/layers_3/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.350651 140316961150976 adafactor.py:358] Since rank of parameter decoder/layers_17/extra_mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.333275 139676058949632 adafactor.py:358] Since rank of parameter decoder/layers_6/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:14.356261 140392160929792 adafactor.py:358] Since rank of parameter decoder/layers_17/pre_extra_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:14.334158 139812357228544 adafactor.py:358] Since rank of parameter decoder/layers_19/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.354900 139997397456896 adafactor.py:358] Since rank of parameter decoder/layers_3/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:14.356892 140172769531904 adafactor.py:358] Since rank of parameter decoder/layers_2/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:14.334436 139676058949632 adafactor.py:358] Since rank of parameter decoder/layers_6/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.357227 140392160929792 adafactor.py:358] Since rank of parameter decoder/layers_17/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:14.351727 140316961150976 adafactor.py:358] Since rank of parameter decoder/layers_17/extra_mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:14.335266 139812357228544 adafactor.py:358] Since rank of parameter decoder/layers_19/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.355991 139997397456896 adafactor.py:358] Since rank of parameter decoder/layers_3/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.358158 140392160929792 adafactor.py:358] Since rank of parameter decoder/layers_17/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:14.358047 140172769531904 adafactor.py:358] Since rank of parameter decoder/layers_2/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:14.335544 139676058949632 adafactor.py:358] Since rank of parameter decoder/layers_6/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.336499 139812357228544 adafactor.py:358] Since rank of parameter decoder/layers_19/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:14.359070 140392160929792 adafactor.py:358] Since rank of parameter decoder/layers_17/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.357107 139997397456896 adafactor.py:358] Since rank of parameter decoder/layers_3/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.359045 140172769531904 adafactor.py:358] Since rank of parameter decoder/layers_2/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:14.336624 139676058949632 adafactor.py:358] Since rank of parameter decoder/layers_7/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.337602 139812357228544 adafactor.py:358] Since rank of parameter decoder/layers_19/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:14.360186 140392160929792 adafactor.py:358] Since rank of parameter decoder/layers_17/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:14.360042 140172769531904 adafactor.py:358] Since rank of parameter decoder/layers_2/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.358242 139997397456896 adafactor.py:358] Since rank of parameter decoder/layers_4/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.337758 139676058949632 adafactor.py:358] Since rank of parameter decoder/layers_7/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.338553 139812357228544 adafactor.py:358] Since rank of parameter decoder/layers_19/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:14.355853 140316961150976 adafactor.py:358] Since rank of parameter decoder/layers_17/mlp/router/router_weights/w/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>, <FactorDim.NONE: None>) is ignored.
W0512 01:45:14.361298 140392160929792 adafactor.py:358] Since rank of parameter decoder/layers_17/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.361269 140172769531904 adafactor.py:358] Since rank of parameter decoder/layers_2/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:14.359310 139997397456896 adafactor.py:358] Since rank of parameter decoder/layers_4/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.338872 139676058949632 adafactor.py:358] Since rank of parameter decoder/layers_7/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:14.339463 139812357228544 adafactor.py:358] Since rank of parameter decoder/layers_19/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.356819 140316961150976 adafactor.py:358] Since rank of parameter decoder/layers_17/pre_extra_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:14.362483 140392160929792 adafactor.py:358] Since rank of parameter decoder/layers_17/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.360367 139997397456896 adafactor.py:358] Since rank of parameter decoder/layers_4/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:14.362372 140172769531904 adafactor.py:358] Since rank of parameter decoder/layers_2/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.339953 139676058949632 adafactor.py:358] Since rank of parameter decoder/layers_7/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:14.340694 139812357228544 adafactor.py:358] Since rank of parameter decoder/layers_19/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:14.357827 140316961150976 adafactor.py:358] Since rank of parameter decoder/layers_17/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:14.363594 140392160929792 adafactor.py:358] Since rank of parameter decoder/layers_18/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.361480 139997397456896 adafactor.py:358] Since rank of parameter decoder/layers_4/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:14.340844 139676058949632 adafactor.py:358] Since rank of parameter decoder/layers_7/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:14.363451 140172769531904 adafactor.py:358] Since rank of parameter decoder/layers_2/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.358727 140316961150976 adafactor.py:358] Since rank of parameter decoder/layers_17/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:14.341788 139812357228544 adafactor.py:358] Since rank of parameter decoder/layers_19/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.362409 139997397456896 adafactor.py:358] Since rank of parameter decoder/layers_4/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:14.341812 139676058949632 adafactor.py:358] Since rank of parameter decoder/layers_7/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.364725 140392160929792 adafactor.py:358] Since rank of parameter decoder/layers_18/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.364643 140172769531904 adafactor.py:358] Since rank of parameter decoder/layers_20/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.359632 140316961150976 adafactor.py:358] Since rank of parameter decoder/layers_17/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.363300 139997397456896 adafactor.py:358] Since rank of parameter decoder/layers_4/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.343065 139812357228544 adafactor.py:358] Since rank of parameter decoder/layers_19/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.342889 139676058949632 adafactor.py:358] Since rank of parameter decoder/layers_7/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:14.365823 140392160929792 adafactor.py:358] Since rank of parameter decoder/layers_18/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:14.360746 140316961150976 adafactor.py:358] Since rank of parameter decoder/layers_17/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:14.365741 140172769531904 adafactor.py:358] Since rank of parameter decoder/layers_20/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.364407 139997397456896 adafactor.py:358] Since rank of parameter decoder/layers_4/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:14.344274 139812357228544 adafactor.py:358] Since rank of parameter decoder/layers_2/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.343988 139676058949632 adafactor.py:358] Since rank of parameter decoder/layers_7/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.367020 140392160929792 adafactor.py:358] Since rank of parameter decoder/layers_18/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:14.361899 140316961150976 adafactor.py:358] Since rank of parameter decoder/layers_17/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.366826 140172769531904 adafactor.py:358] Since rank of parameter decoder/layers_20/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:14.365528 139997397456896 adafactor.py:358] Since rank of parameter decoder/layers_4/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.367931 140392160929792 adafactor.py:358] Since rank of parameter decoder/layers_18/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:14.345082 139676058949632 adafactor.py:358] Since rank of parameter decoder/layers_7/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.345618 139812357228544 adafactor.py:358] Since rank of parameter decoder/layers_2/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.363049 140316961150976 adafactor.py:358] Since rank of parameter decoder/layers_17/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.367894 140172769531904 adafactor.py:358] Since rank of parameter decoder/layers_20/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:14.368836 140392160929792 adafactor.py:358] Since rank of parameter decoder/layers_18/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.368882 140172769531904 adafactor.py:358] Since rank of parameter decoder/layers_20/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:14.346247 139676058949632 adafactor.py:358] Since rank of parameter decoder/layers_8/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.346819 139812357228544 adafactor.py:358] Since rank of parameter decoder/layers_2/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:14.367056 139997397456896 adafactor.py:358] Since rank of parameter decoder/layers_4/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.364185 140316961150976 adafactor.py:358] Since rank of parameter decoder/layers_18/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.369960 140392160929792 adafactor.py:358] Since rank of parameter decoder/layers_18/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:14.369823 140172769531904 adafactor.py:358] Since rank of parameter decoder/layers_20/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.347320 139676058949632 adafactor.py:358] Since rank of parameter decoder/layers_8/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.368131 139997397456896 adafactor.py:358] Since rank of parameter decoder/layers_5/extra_mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.347920 139812357228544 adafactor.py:358] Since rank of parameter decoder/layers_2/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:14.365302 140316961150976 adafactor.py:358] Since rank of parameter decoder/layers_18/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.371165 140392160929792 adafactor.py:358] Since rank of parameter decoder/layers_18/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.370953 140172769531904 adafactor.py:358] Since rank of parameter decoder/layers_20/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:14.348438 139676058949632 adafactor.py:358] Since rank of parameter decoder/layers_8/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:14.349043 139812357228544 adafactor.py:358] Since rank of parameter decoder/layers_2/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:14.369251 139997397456896 adafactor.py:358] Since rank of parameter decoder/layers_5/extra_mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.366447 140316961150976 adafactor.py:358] Since rank of parameter decoder/layers_18/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:14.372022 140172769531904 adafactor.py:358] Since rank of parameter decoder/layers_20/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.372277 140392160929792 adafactor.py:358] Since rank of parameter decoder/layers_18/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.350003 139812357228544 adafactor.py:358] Since rank of parameter decoder/layers_2/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.349534 139676058949632 adafactor.py:358] Since rank of parameter decoder/layers_8/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:14.370331 139997397456896 adafactor.py:358] Since rank of parameter decoder/layers_5/extra_mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:14.367604 140316961150976 adafactor.py:358] Since rank of parameter decoder/layers_18/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:14.350502 139676058949632 adafactor.py:358] Since rank of parameter decoder/layers_8/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:14.373373 140392160929792 adafactor.py:358] Since rank of parameter decoder/layers_19/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.373286 140172769531904 adafactor.py:358] Since rank of parameter decoder/layers_20/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.351105 139812357228544 adafactor.py:358] Since rank of parameter decoder/layers_2/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:14.368518 140316961150976 adafactor.py:358] Since rank of parameter decoder/layers_18/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:14.351372 139676058949632 adafactor.py:358] Since rank of parameter decoder/layers_8/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.374520 140392160929792 adafactor.py:358] Since rank of parameter decoder/layers_19/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.352260 139812357228544 adafactor.py:358] Since rank of parameter decoder/layers_2/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.369406 140316961150976 adafactor.py:358] Since rank of parameter decoder/layers_18/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.352486 139676058949632 adafactor.py:358] Since rank of parameter decoder/layers_8/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:14.375375 140172769531904 adafactor.py:358] Since rank of parameter decoder/layers_21/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.375682 140392160929792 adafactor.py:358] Since rank of parameter decoder/layers_19/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:14.353396 139812357228544 adafactor.py:358] Since rank of parameter decoder/layers_2/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.370554 140316961150976 adafactor.py:358] Since rank of parameter decoder/layers_18/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:14.353628 139676058949632 adafactor.py:358] Since rank of parameter decoder/layers_8/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.374641 139997397456896 adafactor.py:358] Since rank of parameter decoder/layers_5/mlp/router/router_weights/w/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>, <FactorDim.NONE: None>) is ignored.
W0512 01:45:14.376500 140172769531904 adafactor.py:358] Since rank of parameter decoder/layers_21/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.376795 140392160929792 adafactor.py:358] Since rank of parameter decoder/layers_19/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:14.354542 139812357228544 adafactor.py:358] Since rank of parameter decoder/layers_20/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.371725 140316961150976 adafactor.py:358] Since rank of parameter decoder/layers_18/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.354833 139676058949632 adafactor.py:358] Since rank of parameter decoder/layers_8/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.375590 139997397456896 adafactor.py:358] Since rank of parameter decoder/layers_5/pre_extra_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:14.377689 140392160929792 adafactor.py:358] Since rank of parameter decoder/layers_19/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:14.377631 140172769531904 adafactor.py:358] Since rank of parameter decoder/layers_21/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:14.355662 139812357228544 adafactor.py:358] Since rank of parameter decoder/layers_20/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.372814 140316961150976 adafactor.py:358] Since rank of parameter decoder/layers_18/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.376462 139997397456896 adafactor.py:358] Since rank of parameter decoder/layers_5/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:14.378632 140392160929792 adafactor.py:358] Since rank of parameter decoder/layers_19/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.355920 139676058949632 adafactor.py:358] Since rank of parameter decoder/layers_9/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.378762 140172769531904 adafactor.py:358] Since rank of parameter decoder/layers_21/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:14.356935 139812357228544 adafactor.py:358] Since rank of parameter decoder/layers_20/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:14.377366 139997397456896 adafactor.py:358] Since rank of parameter decoder/layers_5/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:14.373950 140316961150976 adafactor.py:358] Since rank of parameter decoder/layers_19/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.357005 139676058949632 adafactor.py:358] Since rank of parameter decoder/layers_9/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.379816 140392160929792 adafactor.py:358] Since rank of parameter decoder/layers_19/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:14.379802 140172769531904 adafactor.py:358] Since rank of parameter decoder/layers_21/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:14.378245 139997397456896 adafactor.py:358] Since rank of parameter decoder/layers_5/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.358067 139812357228544 adafactor.py:358] Since rank of parameter decoder/layers_20/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:14.375034 140316961150976 adafactor.py:358] Since rank of parameter decoder/layers_19/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.380937 140392160929792 adafactor.py:358] Since rank of parameter decoder/layers_19/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.380736 140172769531904 adafactor.py:358] Since rank of parameter decoder/layers_21/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.358117 139676058949632 adafactor.py:358] Since rank of parameter decoder/layers_9/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:14.358969 139812357228544 adafactor.py:358] Since rank of parameter decoder/layers_20/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:14.379377 139997397456896 adafactor.py:358] Since rank of parameter decoder/layers_5/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:14.376246 140316961150976 adafactor.py:358] Since rank of parameter decoder/layers_19/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:14.359245 139676058949632 adafactor.py:358] Since rank of parameter decoder/layers_9/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:14.382043 140392160929792 adafactor.py:358] Since rank of parameter decoder/layers_19/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.381856 140172769531904 adafactor.py:358] Since rank of parameter decoder/layers_21/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:14.359920 139812357228544 adafactor.py:358] Since rank of parameter decoder/layers_20/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.380522 139997397456896 adafactor.py:358] Since rank of parameter decoder/layers_5/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.360147 139676058949632 adafactor.py:358] Since rank of parameter decoder/layers_9/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:14.377310 140316961150976 adafactor.py:358] Since rank of parameter decoder/layers_19/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:14.382948 140172769531904 adafactor.py:358] Since rank of parameter decoder/layers_21/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.383197 140392160929792 adafactor.py:358] Since rank of parameter decoder/layers_2/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.378232 140316961150976 adafactor.py:358] Since rank of parameter decoder/layers_19/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:14.361132 139812357228544 adafactor.py:358] Since rank of parameter decoder/layers_20/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:14.361046 139676058949632 adafactor.py:358] Since rank of parameter decoder/layers_9/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.381771 139997397456896 adafactor.py:358] Since rank of parameter decoder/layers_5/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.384143 140172769531904 adafactor.py:358] Since rank of parameter decoder/layers_21/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.384407 140392160929792 adafactor.py:358] Since rank of parameter decoder/layers_2/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.379354 140316961150976 adafactor.py:358] Since rank of parameter decoder/layers_19/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.362238 139812357228544 adafactor.py:358] Since rank of parameter decoder/layers_20/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.362167 139676058949632 adafactor.py:358] Since rank of parameter decoder/layers_9/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:14.382962 139997397456896 adafactor.py:358] Since rank of parameter decoder/layers_6/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.385514 140392160929792 adafactor.py:358] Since rank of parameter decoder/layers_2/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:14.385292 140172769531904 adafactor.py:358] Since rank of parameter decoder/layers_22/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.383845 140577131780096 adafactor.py:358] Since rank of parameter decoder/decoder_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:14.363384 139812357228544 adafactor.py:358] Since rank of parameter decoder/layers_20/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.380517 140316961150976 adafactor.py:358] Since rank of parameter decoder/layers_19/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:14.363247 139676058949632 adafactor.py:358] Since rank of parameter decoder/layers_9/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.384150 139997397456896 adafactor.py:358] Since rank of parameter decoder/layers_6/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.386422 140172769531904 adafactor.py:358] Since rank of parameter decoder/layers_22/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.386672 140392160929792 adafactor.py:358] Since rank of parameter decoder/layers_2/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:14.384911 140577131780096 adafactor.py:358] Since rank of parameter decoder/layers_0/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.381712 140316961150976 adafactor.py:358] Since rank of parameter decoder/layers_19/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.364831 139676058949632 adafactor.py:358] Since rank of parameter decoder/layers_9/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.387578 140392160929792 adafactor.py:358] Since rank of parameter decoder/layers_2/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:14.385400 139997397456896 adafactor.py:358] Since rank of parameter decoder/layers_6/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:14.365699 139812357228544 adafactor.py:358] Since rank of parameter decoder/layers_21/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.387803 140172769531904 adafactor.py:358] Since rank of parameter decoder/layers_22/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:14.386178 140577131780096 adafactor.py:358] Since rank of parameter decoder/layers_0/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.382826 140316961150976 adafactor.py:358] Since rank of parameter decoder/layers_19/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.386538 139997397456896 adafactor.py:358] Since rank of parameter decoder/layers_6/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:14.388528 140392160929792 adafactor.py:358] Since rank of parameter decoder/layers_2/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.365982 139676058949632 adafactor.py:358] Since rank of parameter decoder/logits_dense/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.383939 140316961150976 adafactor.py:358] Since rank of parameter decoder/layers_2/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.366839 139812357228544 adafactor.py:358] Since rank of parameter decoder/layers_21/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.387399 140577131780096 adafactor.py:358] Since rank of parameter decoder/layers_0/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:14.389183 140172769531904 adafactor.py:358] Since rank of parameter decoder/layers_22/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:14.387493 139997397456896 adafactor.py:358] Since rank of parameter decoder/layers_6/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:14.389637 140392160929792 adafactor.py:358] Since rank of parameter decoder/layers_2/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:14.367144 139676058949632 adafactor.py:358] Since rank of parameter token_embedder/embedding 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:14.367947 139812357228544 adafactor.py:358] Since rank of parameter decoder/layers_21/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:14.390285 140172769531904 adafactor.py:358] Since rank of parameter decoder/layers_22/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:14.388406 139997397456896 adafactor.py:358] Since rank of parameter decoder/layers_6/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.385250 140316961150976 adafactor.py:358] Since rank of parameter decoder/layers_2/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.388695 140577131780096 adafactor.py:358] Since rank of parameter decoder/layers_0/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:14.390840 140392160929792 adafactor.py:358] Since rank of parameter decoder/layers_2/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.391225 140172769531904 adafactor.py:358] Since rank of parameter decoder/layers_22/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.369106 139812357228544 adafactor.py:358] Since rank of parameter decoder/layers_21/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:14.389796 140577131780096 adafactor.py:358] Since rank of parameter decoder/layers_0/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:14.389660 139997397456896 adafactor.py:358] Since rank of parameter decoder/layers_6/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:14.386360 140316961150976 adafactor.py:358] Since rank of parameter decoder/layers_2/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:14.391964 140392160929792 adafactor.py:358] Since rank of parameter decoder/layers_2/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.370078 139812357228544 adafactor.py:358] Since rank of parameter decoder/layers_21/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:14.392358 140172769531904 adafactor.py:358] Since rank of parameter decoder/layers_22/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:14.390760 140577131780096 adafactor.py:358] Since rank of parameter decoder/layers_0/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.390809 139997397456896 adafactor.py:358] Since rank of parameter decoder/layers_6/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.393121 140392160929792 adafactor.py:358] Since rank of parameter decoder/layers_20/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.387605 140316961150976 adafactor.py:358] Since rank of parameter decoder/layers_2/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:14.370973 139812357228544 adafactor.py:358] Since rank of parameter decoder/layers_21/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.388492 140316961150976 adafactor.py:358] Since rank of parameter decoder/layers_2/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:14.391961 140577131780096 adafactor.py:358] Since rank of parameter decoder/layers_0/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:14.391974 139997397456896 adafactor.py:358] Since rank of parameter decoder/layers_6/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.393986 140172769531904 adafactor.py:358] Since rank of parameter decoder/layers_22/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.394287 140392160929792 adafactor.py:358] Since rank of parameter decoder/layers_20/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.372082 139812357228544 adafactor.py:358] Since rank of parameter decoder/layers_21/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:14.393101 140577131780096 adafactor.py:358] Since rank of parameter decoder/layers_0/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.389975 140316961150976 adafactor.py:358] Since rank of parameter decoder/layers_2/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.393136 139997397456896 adafactor.py:358] Since rank of parameter decoder/layers_7/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.395088 140172769531904 adafactor.py:358] Since rank of parameter decoder/layers_22/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.395395 140392160929792 adafactor.py:358] Since rank of parameter decoder/layers_20/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:14.373239 139812357228544 adafactor.py:358] Since rank of parameter decoder/layers_21/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.394316 140577131780096 adafactor.py:358] Since rank of parameter decoder/layers_0/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.394346 139997397456896 adafactor.py:358] Since rank of parameter decoder/layers_7/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.396325 140172769531904 adafactor.py:358] Since rank of parameter decoder/layers_23/extra_mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.396512 140392160929792 adafactor.py:358] Since rank of parameter decoder/layers_20/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:14.391096 140316961150976 adafactor.py:358] Since rank of parameter decoder/layers_2/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:14.374429 139812357228544 adafactor.py:358] Since rank of parameter decoder/layers_21/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.397409 140392160929792 adafactor.py:358] Since rank of parameter decoder/layers_20/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:14.397466 140172769531904 adafactor.py:358] Since rank of parameter decoder/layers_23/extra_mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.395483 140577131780096 adafactor.py:358] Since rank of parameter decoder/layers_1/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.392223 140316961150976 adafactor.py:358] Since rank of parameter decoder/layers_2/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.395524 139997397456896 adafactor.py:358] Since rank of parameter decoder/layers_7/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:14.375569 139812357228544 adafactor.py:358] Since rank of parameter decoder/layers_22/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.396605 140577131780096 adafactor.py:358] Since rank of parameter decoder/layers_1/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.398587 140392160929792 adafactor.py:358] Since rank of parameter decoder/layers_20/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.396711 139997397456896 adafactor.py:358] Since rank of parameter decoder/layers_7/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:14.393353 140316961150976 adafactor.py:358] Since rank of parameter decoder/layers_2/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.398653 140172769531904 adafactor.py:358] Since rank of parameter decoder/layers_23/extra_mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:14.376758 139812357228544 adafactor.py:358] Since rank of parameter decoder/layers_22/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.397790 140577131780096 adafactor.py:358] Since rank of parameter decoder/layers_1/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:14.394545 140316961150976 adafactor.py:358] Since rank of parameter decoder/layers_20/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.399959 140392160929792 adafactor.py:358] Since rank of parameter decoder/layers_20/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:14.397787 139997397456896 adafactor.py:358] Since rank of parameter decoder/layers_7/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:14.377900 139812357228544 adafactor.py:358] Since rank of parameter decoder/layers_22/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:14.398808 139997397456896 adafactor.py:358] Since rank of parameter decoder/layers_7/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.395661 140316961150976 adafactor.py:358] Since rank of parameter decoder/layers_20/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.399054 140577131780096 adafactor.py:358] Since rank of parameter decoder/layers_1/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:14.401290 140392160929792 adafactor.py:358] Since rank of parameter decoder/layers_20/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.379078 139812357228544 adafactor.py:358] Since rank of parameter decoder/layers_22/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:14.400046 140577131780096 adafactor.py:358] Since rank of parameter decoder/layers_1/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:14.399977 139997397456896 adafactor.py:358] Since rank of parameter decoder/layers_7/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:14.396820 140316961150976 adafactor.py:358] Since rank of parameter decoder/layers_20/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:14.379979 139812357228544 adafactor.py:358] Since rank of parameter decoder/layers_22/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:14.402687 140392160929792 adafactor.py:358] Since rank of parameter decoder/layers_20/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.400997 140577131780096 adafactor.py:358] Since rank of parameter decoder/layers_1/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.402785 140172769531904 adafactor.py:358] Since rank of parameter decoder/layers_23/mlp/router/router_weights/w/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>, <FactorDim.NONE: None>) is ignored.
W0512 01:45:14.401154 139997397456896 adafactor.py:358] Since rank of parameter decoder/layers_7/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.397933 140316961150976 adafactor.py:358] Since rank of parameter decoder/layers_20/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:14.380950 139812357228544 adafactor.py:358] Since rank of parameter decoder/layers_22/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.403687 140172769531904 adafactor.py:358] Since rank of parameter decoder/layers_23/pre_extra_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:14.398804 140316961150976 adafactor.py:358] Since rank of parameter decoder/layers_20/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:14.402210 140577131780096 adafactor.py:358] Since rank of parameter decoder/layers_1/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:14.402303 139997397456896 adafactor.py:358] Since rank of parameter decoder/layers_7/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.382223 139812357228544 adafactor.py:358] Since rank of parameter decoder/layers_22/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:14.399734 140316961150976 adafactor.py:358] Since rank of parameter decoder/layers_20/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.404602 140172769531904 adafactor.py:358] Since rank of parameter decoder/layers_23/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:14.403489 140577131780096 adafactor.py:358] Since rank of parameter decoder/layers_1/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.405322 140392160929792 adafactor.py:358] Since rank of parameter decoder/layers_21/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.405475 140172769531904 adafactor.py:358] Since rank of parameter decoder/layers_23/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:14.403506 139997397456896 adafactor.py:358] Since rank of parameter decoder/layers_8/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.383428 139812357228544 adafactor.py:358] Since rank of parameter decoder/layers_22/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.400810 140316961150976 adafactor.py:358] Since rank of parameter decoder/layers_20/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:14.406354 140172769531904 adafactor.py:358] Since rank of parameter decoder/layers_23/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.404615 140577131780096 adafactor.py:358] Since rank of parameter decoder/layers_1/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.406712 140392160929792 adafactor.py:358] Since rank of parameter decoder/layers_21/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.404689 139997397456896 adafactor.py:358] Since rank of parameter decoder/layers_8/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.384636 139812357228544 adafactor.py:358] Since rank of parameter decoder/layers_22/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.401932 140316961150976 adafactor.py:358] Since rank of parameter decoder/layers_20/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.407468 140172769531904 adafactor.py:358] Since rank of parameter decoder/layers_23/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:14.405731 140577131780096 adafactor.py:358] Since rank of parameter decoder/layers_10/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.405884 139997397456896 adafactor.py:358] Since rank of parameter decoder/layers_8/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:14.408041 140392160929792 adafactor.py:358] Since rank of parameter decoder/layers_21/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:14.385763 139812357228544 adafactor.py:358] Since rank of parameter decoder/layers_23/extra_mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.403013 140316961150976 adafactor.py:358] Since rank of parameter decoder/layers_20/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.406863 140577131780096 adafactor.py:358] Since rank of parameter decoder/layers_10/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.408563 140172769531904 adafactor.py:358] Since rank of parameter decoder/layers_23/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.407038 139997397456896 adafactor.py:358] Since rank of parameter decoder/layers_8/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:14.409284 140392160929792 adafactor.py:358] Since rank of parameter decoder/layers_21/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:14.386909 139812357228544 adafactor.py:358] Since rank of parameter decoder/layers_23/extra_mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.404551 140316961150976 adafactor.py:358] Since rank of parameter decoder/layers_21/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.409628 140172769531904 adafactor.py:358] Since rank of parameter decoder/layers_23/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.408121 140577131780096 adafactor.py:358] Since rank of parameter decoder/layers_10/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:14.408040 139997397456896 adafactor.py:358] Since rank of parameter decoder/layers_8/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:14.410436 140392160929792 adafactor.py:358] Since rank of parameter decoder/layers_21/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:14.388079 139812357228544 adafactor.py:358] Since rank of parameter decoder/layers_23/extra_mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:14.410695 140172769531904 adafactor.py:358] Since rank of parameter decoder/layers_3/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.405687 140316961150976 adafactor.py:358] Since rank of parameter decoder/layers_21/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.409024 139997397456896 adafactor.py:358] Since rank of parameter decoder/layers_8/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.409246 140577131780096 adafactor.py:358] Since rank of parameter decoder/layers_10/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:14.411478 140392160929792 adafactor.py:358] Since rank of parameter decoder/layers_21/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.410197 140577131780096 adafactor.py:358] Since rank of parameter decoder/layers_10/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:14.410181 139997397456896 adafactor.py:358] Since rank of parameter decoder/layers_8/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:14.406785 140316961150976 adafactor.py:358] Since rank of parameter decoder/layers_21/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:14.411798 140172769531904 adafactor.py:358] Since rank of parameter decoder/layers_3/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.412585 140392160929792 adafactor.py:358] Since rank of parameter decoder/layers_21/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:14.411164 140577131780096 adafactor.py:358] Since rank of parameter decoder/layers_10/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.407902 140316961150976 adafactor.py:358] Since rank of parameter decoder/layers_21/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:14.411343 139997397456896 adafactor.py:358] Since rank of parameter decoder/layers_8/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.413698 140392160929792 adafactor.py:358] Since rank of parameter decoder/layers_21/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.412890 140172769531904 adafactor.py:358] Since rank of parameter decoder/layers_3/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:14.408848 140316961150976 adafactor.py:358] Since rank of parameter decoder/layers_21/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:14.412424 140577131780096 adafactor.py:358] Since rank of parameter decoder/layers_10/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:14.412570 139997397456896 adafactor.py:358] Since rank of parameter decoder/layers_8/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.413940 140172769531904 adafactor.py:358] Since rank of parameter decoder/layers_3/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:14.414887 140392160929792 adafactor.py:358] Since rank of parameter decoder/layers_21/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.414808 140172769531904 adafactor.py:358] Since rank of parameter decoder/layers_3/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:14.392615 139812357228544 adafactor.py:358] Since rank of parameter decoder/layers_23/mlp/router/router_weights/w/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>, <FactorDim.NONE: None>) is ignored.
W0512 01:45:14.409770 140316961150976 adafactor.py:358] Since rank of parameter decoder/layers_21/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.413593 140577131780096 adafactor.py:358] Since rank of parameter decoder/layers_10/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.415743 140172769531904 adafactor.py:358] Since rank of parameter decoder/layers_3/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.413766 139997397456896 adafactor.py:358] Since rank of parameter decoder/layers_9/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.393568 139812357228544 adafactor.py:358] Since rank of parameter decoder/layers_23/pre_extra_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:14.416016 140392160929792 adafactor.py:358] Since rank of parameter decoder/layers_22/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.410873 140316961150976 adafactor.py:358] Since rank of parameter decoder/layers_21/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:14.414743 140577131780096 adafactor.py:358] Since rank of parameter decoder/layers_10/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.394475 139812357228544 adafactor.py:358] Since rank of parameter decoder/layers_23/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:14.414973 139997397456896 adafactor.py:358] Since rank of parameter decoder/layers_9/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.417068 140172769531904 adafactor.py:358] Since rank of parameter decoder/layers_3/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:14.417337 140392160929792 adafactor.py:358] Since rank of parameter decoder/layers_22/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.411981 140316961150976 adafactor.py:358] Since rank of parameter decoder/layers_21/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.395397 139812357228544 adafactor.py:358] Since rank of parameter decoder/layers_23/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:14.415941 140577131780096 adafactor.py:358] Since rank of parameter decoder/layers_11/extra_mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.416120 139997397456896 adafactor.py:358] Since rank of parameter decoder/layers_9/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:14.418150 140172769531904 adafactor.py:358] Since rank of parameter decoder/layers_3/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.396325 139812357228544 adafactor.py:358] Since rank of parameter decoder/layers_23/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.413113 140316961150976 adafactor.py:358] Since rank of parameter decoder/layers_21/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.417180 140577131780096 adafactor.py:358] Since rank of parameter decoder/layers_11/extra_mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.417377 139997397456896 adafactor.py:358] Since rank of parameter decoder/layers_9/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:14.418717 140392160929792 adafactor.py:358] Since rank of parameter decoder/layers_22/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:14.419253 140172769531904 adafactor.py:358] Since rank of parameter decoder/layers_3/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.414247 140316961150976 adafactor.py:358] Since rank of parameter decoder/layers_22/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.397493 139812357228544 adafactor.py:358] Since rank of parameter decoder/layers_23/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:14.418332 140577131780096 adafactor.py:358] Since rank of parameter decoder/layers_11/extra_mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:14.418316 139997397456896 adafactor.py:358] Since rank of parameter decoder/layers_9/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:14.420371 140172769531904 adafactor.py:358] Since rank of parameter decoder/layers_4/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.415340 140316961150976 adafactor.py:358] Since rank of parameter decoder/layers_22/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.398571 139812357228544 adafactor.py:358] Since rank of parameter decoder/layers_23/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.419253 139997397456896 adafactor.py:358] Since rank of parameter decoder/layers_9/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.419973 140392160929792 adafactor.py:358] Since rank of parameter decoder/layers_22/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:14.416506 140316961150976 adafactor.py:358] Since rank of parameter decoder/layers_22/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:14.399683 139812357228544 adafactor.py:358] Since rank of parameter decoder/layers_23/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.421467 140172769531904 adafactor.py:358] Since rank of parameter decoder/layers_4/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.420415 139997397456896 adafactor.py:358] Since rank of parameter decoder/layers_9/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:14.420900 140392160929792 adafactor.py:358] Since rank of parameter decoder/layers_22/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:14.421820 140392160929792 adafactor.py:358] Since rank of parameter decoder/layers_22/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.400803 139812357228544 adafactor.py:358] Since rank of parameter decoder/layers_3/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.417699 140316961150976 adafactor.py:358] Since rank of parameter decoder/layers_22/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:14.422533 140172769531904 adafactor.py:358] Since rank of parameter decoder/layers_4/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:14.423109 140392160929792 adafactor.py:358] Since rank of parameter decoder/layers_22/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:14.421591 139997397456896 adafactor.py:358] Since rank of parameter decoder/layers_9/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.418623 140316961150976 adafactor.py:358] Since rank of parameter decoder/layers_22/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:14.401951 139812357228544 adafactor.py:358] Since rank of parameter decoder/layers_3/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.422827 140577131780096 adafactor.py:358] Since rank of parameter decoder/layers_11/mlp/router/router_weights/w/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>, <FactorDim.NONE: None>) is ignored.
W0512 01:45:14.419536 140316961150976 adafactor.py:358] Since rank of parameter decoder/layers_22/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.423615 140172769531904 adafactor.py:358] Since rank of parameter decoder/layers_4/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:14.424596 140172769531904 adafactor.py:358] Since rank of parameter decoder/layers_4/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:14.423171 139997397456896 adafactor.py:358] Since rank of parameter decoder/layers_9/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.403033 139812357228544 adafactor.py:358] Since rank of parameter decoder/layers_3/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:14.424394 140392160929792 adafactor.py:358] Since rank of parameter decoder/layers_22/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.423843 140577131780096 adafactor.py:358] Since rank of parameter decoder/layers_11/pre_extra_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:14.425471 140172769531904 adafactor.py:358] Since rank of parameter decoder/layers_4/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.420615 140316961150976 adafactor.py:358] Since rank of parameter decoder/layers_22/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:14.404199 139812357228544 adafactor.py:358] Since rank of parameter decoder/layers_3/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:14.424426 139997397456896 adafactor.py:358] Since rank of parameter decoder/logits_dense/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.425611 140392160929792 adafactor.py:358] Since rank of parameter decoder/layers_22/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.424786 140577131780096 adafactor.py:358] Since rank of parameter decoder/layers_11/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:14.426533 140172769531904 adafactor.py:358] Since rank of parameter decoder/layers_4/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:14.421776 140316961150976 adafactor.py:358] Since rank of parameter decoder/layers_22/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.405111 139812357228544 adafactor.py:358] Since rank of parameter decoder/layers_3/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:14.425729 140577131780096 adafactor.py:358] Since rank of parameter decoder/layers_11/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:14.425655 139997397456896 adafactor.py:358] Since rank of parameter token_embedder/embedding 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:14.427593 140172769531904 adafactor.py:358] Since rank of parameter decoder/layers_4/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.406047 139812357228544 adafactor.py:358] Since rank of parameter decoder/layers_3/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.426850 140392160929792 adafactor.py:358] Since rank of parameter decoder/layers_23/extra_mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.422866 140316961150976 adafactor.py:358] Since rank of parameter decoder/layers_22/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.429135 140172769531904 adafactor.py:358] Since rank of parameter decoder/layers_4/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.427523 140577131780096 adafactor.py:358] Since rank of parameter decoder/layers_11/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.407131 139812357228544 adafactor.py:358] Since rank of parameter decoder/layers_3/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:14.428031 140392160929792 adafactor.py:358] Since rank of parameter decoder/layers_23/extra_mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.423938 140316961150976 adafactor.py:358] Since rank of parameter decoder/layers_23/extra_mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.425015 140316961150976 adafactor.py:358] Since rank of parameter decoder/layers_23/extra_mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.430215 140172769531904 adafactor.py:358] Since rank of parameter decoder/layers_5/extra_mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.408267 139812357228544 adafactor.py:358] Since rank of parameter decoder/layers_3/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.428689 140577131780096 adafactor.py:358] Since rank of parameter decoder/layers_11/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:14.426173 140316961150976 adafactor.py:358] Since rank of parameter decoder/layers_23/extra_mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:14.431280 140172769531904 adafactor.py:358] Since rank of parameter decoder/layers_5/extra_mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.429318 140392160929792 adafactor.py:358] Since rank of parameter decoder/layers_23/extra_mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:14.409388 139812357228544 adafactor.py:358] Since rank of parameter decoder/layers_3/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.429835 140577131780096 adafactor.py:358] Since rank of parameter decoder/layers_11/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.432351 140172769531904 adafactor.py:358] Since rank of parameter decoder/layers_5/extra_mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:14.410529 139812357228544 adafactor.py:358] Since rank of parameter decoder/layers_4/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.430986 140577131780096 adafactor.py:358] Since rank of parameter decoder/layers_11/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.411647 139812357228544 adafactor.py:358] Since rank of parameter decoder/layers_4/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.432332 140577131780096 adafactor.py:358] Since rank of parameter decoder/layers_12/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.433950 140392160929792 adafactor.py:358] Since rank of parameter decoder/layers_23/mlp/router/router_weights/w/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>, <FactorDim.NONE: None>) is ignored.
W0512 01:45:14.412781 139812357228544 adafactor.py:358] Since rank of parameter decoder/layers_4/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:14.433485 140577131780096 adafactor.py:358] Since rank of parameter decoder/layers_12/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.430402 140316961150976 adafactor.py:358] Since rank of parameter decoder/layers_23/mlp/router/router_weights/w/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>, <FactorDim.NONE: None>) is ignored.
W0512 01:45:14.413885 139812357228544 adafactor.py:358] Since rank of parameter decoder/layers_4/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:14.436428 140172769531904 adafactor.py:358] Since rank of parameter decoder/layers_5/mlp/router/router_weights/w/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>, <FactorDim.NONE: None>) is ignored.
W0512 01:45:14.435046 140392160929792 adafactor.py:358] Since rank of parameter decoder/layers_23/pre_extra_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:14.434617 140577131780096 adafactor.py:358] Since rank of parameter decoder/layers_12/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:14.431333 140316961150976 adafactor.py:358] Since rank of parameter decoder/layers_23/pre_extra_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:14.414828 139812357228544 adafactor.py:358] Since rank of parameter decoder/layers_4/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:14.437384 140172769531904 adafactor.py:358] Since rank of parameter decoder/layers_5/pre_extra_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:14.432255 140316961150976 adafactor.py:358] Since rank of parameter decoder/layers_23/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:14.436011 140392160929792 adafactor.py:358] Since rank of parameter decoder/layers_23/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:14.437026 140392160929792 adafactor.py:358] Since rank of parameter decoder/layers_23/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:14.435988 140577131780096 adafactor.py:358] Since rank of parameter decoder/layers_12/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:14.415734 139812357228544 adafactor.py:358] Since rank of parameter decoder/layers_4/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.433177 140316961150976 adafactor.py:358] Since rank of parameter decoder/layers_23/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:14.438252 140172769531904 adafactor.py:358] Since rank of parameter decoder/layers_5/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:14.436955 140577131780096 adafactor.py:358] Since rank of parameter decoder/layers_12/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:14.438025 140392160929792 adafactor.py:358] Since rank of parameter decoder/layers_23/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.416898 139812357228544 adafactor.py:358] Since rank of parameter decoder/layers_4/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:14.439170 140172769531904 adafactor.py:358] Since rank of parameter decoder/layers_5/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:14.439274 140392160929792 adafactor.py:358] Since rank of parameter decoder/layers_23/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:14.434122 140316961150976 adafactor.py:358] Since rank of parameter decoder/layers_23/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.437852 140577131780096 adafactor.py:358] Since rank of parameter decoder/layers_12/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.440037 140172769531904 adafactor.py:358] Since rank of parameter decoder/layers_5/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.418000 139812357228544 adafactor.py:358] Since rank of parameter decoder/layers_4/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.440493 140392160929792 adafactor.py:358] Since rank of parameter decoder/layers_23/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.435299 140316961150976 adafactor.py:358] Since rank of parameter decoder/layers_23/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:14.438965 140577131780096 adafactor.py:358] Since rank of parameter decoder/layers_12/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:14.441180 140172769531904 adafactor.py:358] Since rank of parameter decoder/layers_5/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:14.441725 140392160929792 adafactor.py:358] Since rank of parameter decoder/layers_23/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.436446 140316961150976 adafactor.py:358] Since rank of parameter decoder/layers_23/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.440107 140577131780096 adafactor.py:358] Since rank of parameter decoder/layers_12/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.419803 139812357228544 adafactor.py:358] Since rank of parameter decoder/layers_4/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.442322 140172769531904 adafactor.py:358] Since rank of parameter decoder/layers_5/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.437585 140316961150976 adafactor.py:358] Since rank of parameter decoder/layers_23/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.442936 140392160929792 adafactor.py:358] Since rank of parameter decoder/layers_3/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.441256 140577131780096 adafactor.py:358] Since rank of parameter decoder/layers_12/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.420937 139812357228544 adafactor.py:358] Since rank of parameter decoder/layers_5/extra_mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.443403 140172769531904 adafactor.py:358] Since rank of parameter decoder/layers_5/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.438708 140316961150976 adafactor.py:358] Since rank of parameter decoder/layers_3/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.422036 139812357228544 adafactor.py:358] Since rank of parameter decoder/layers_5/extra_mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.442483 140577131780096 adafactor.py:358] Since rank of parameter decoder/layers_13/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.444261 140392160929792 adafactor.py:358] Since rank of parameter decoder/layers_3/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.444519 140172769531904 adafactor.py:358] Since rank of parameter decoder/layers_6/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.439872 140316961150976 adafactor.py:358] Since rank of parameter decoder/layers_3/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.423121 139812357228544 adafactor.py:358] Since rank of parameter decoder/layers_5/extra_mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:14.443621 140577131780096 adafactor.py:358] Since rank of parameter decoder/layers_13/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.445512 140392160929792 adafactor.py:358] Since rank of parameter decoder/layers_3/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:14.445580 140172769531904 adafactor.py:358] Since rank of parameter decoder/layers_6/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.441008 140316961150976 adafactor.py:358] Since rank of parameter decoder/layers_3/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:14.444721 140577131780096 adafactor.py:358] Since rank of parameter decoder/layers_13/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:14.446686 140172769531904 adafactor.py:358] Since rank of parameter decoder/layers_6/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:14.446794 140392160929792 adafactor.py:358] Since rank of parameter decoder/layers_3/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:14.442154 140316961150976 adafactor.py:358] Since rank of parameter decoder/layers_3/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:14.445863 140577131780096 adafactor.py:358] Since rank of parameter decoder/layers_13/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:14.447816 140392160929792 adafactor.py:358] Since rank of parameter decoder/layers_3/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:14.447746 140172769531904 adafactor.py:358] Since rank of parameter decoder/layers_6/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:14.443071 140316961150976 adafactor.py:358] Since rank of parameter decoder/layers_3/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:14.446761 140577131780096 adafactor.py:358] Since rank of parameter decoder/layers_13/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:14.448889 140392160929792 adafactor.py:358] Since rank of parameter decoder/layers_3/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.447686 140577131780096 adafactor.py:358] Since rank of parameter decoder/layers_13/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.427559 139812357228544 adafactor.py:358] Since rank of parameter decoder/layers_5/mlp/router/router_weights/w/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>, <FactorDim.NONE: None>) is ignored.
W0512 01:45:14.444390 140316961150976 adafactor.py:358] Since rank of parameter decoder/layers_3/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.450191 140392160929792 adafactor.py:358] Since rank of parameter decoder/layers_3/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:14.448685 140172769531904 adafactor.py:358] Since rank of parameter decoder/layers_6/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:14.448786 140577131780096 adafactor.py:358] Since rank of parameter decoder/layers_13/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:14.445621 140316961150976 adafactor.py:358] Since rank of parameter decoder/layers_3/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:14.428585 139812357228544 adafactor.py:358] Since rank of parameter decoder/layers_5/pre_extra_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:14.449564 140172769531904 adafactor.py:358] Since rank of parameter decoder/layers_6/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.450698 140172769531904 adafactor.py:358] Since rank of parameter decoder/layers_6/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:14.449928 140577131780096 adafactor.py:358] Since rank of parameter decoder/layers_13/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.429540 139812357228544 adafactor.py:358] Since rank of parameter decoder/layers_5/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:14.451785 140172769531904 adafactor.py:358] Since rank of parameter decoder/layers_6/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.446747 140316961150976 adafactor.py:358] Since rank of parameter decoder/layers_3/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.430492 139812357228544 adafactor.py:358] Since rank of parameter decoder/layers_5/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:14.451049 140577131780096 adafactor.py:358] Since rank of parameter decoder/layers_13/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.452884 140172769531904 adafactor.py:358] Since rank of parameter decoder/layers_6/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.447877 140316961150976 adafactor.py:358] Since rank of parameter decoder/layers_3/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.431355 139812357228544 adafactor.py:358] Since rank of parameter decoder/layers_5/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.453959 140172769531904 adafactor.py:358] Since rank of parameter decoder/layers_7/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.452284 140577131780096 adafactor.py:358] Since rank of parameter decoder/layers_14/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.449061 140316961150976 adafactor.py:358] Since rank of parameter decoder/layers_4/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.451459 140392160929792 adafactor.py:358] Since rank of parameter decoder/layers_3/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.452694 140392160929792 adafactor.py:358] Since rank of parameter decoder/layers_3/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.453868 140392160929792 adafactor.py:358] Since rank of parameter decoder/layers_4/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.453457 140577131780096 adafactor.py:358] Since rank of parameter decoder/layers_14/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.450206 140316961150976 adafactor.py:358] Since rank of parameter decoder/layers_4/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.455026 140392160929792 adafactor.py:358] Since rank of parameter decoder/layers_4/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.433893 139812357228544 adafactor.py:358] Since rank of parameter decoder/layers_5/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:14.456263 140392160929792 adafactor.py:358] Since rank of parameter decoder/layers_4/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:14.455073 140172769531904 adafactor.py:358] Since rank of parameter decoder/layers_7/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.451322 140316961150976 adafactor.py:358] Since rank of parameter decoder/layers_4/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:14.454761 140577131780096 adafactor.py:358] Since rank of parameter decoder/layers_14/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:14.456144 140172769531904 adafactor.py:358] Since rank of parameter decoder/layers_7/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:14.435067 139812357228544 adafactor.py:358] Since rank of parameter decoder/layers_5/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.457483 140392160929792 adafactor.py:358] Since rank of parameter decoder/layers_4/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:14.452439 140316961150976 adafactor.py:358] Since rank of parameter decoder/layers_4/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:14.456029 140577131780096 adafactor.py:358] Since rank of parameter decoder/layers_14/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:14.436202 139812357228544 adafactor.py:358] Since rank of parameter decoder/layers_5/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.458591 140392160929792 adafactor.py:358] Since rank of parameter decoder/layers_4/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:14.453388 140316961150976 adafactor.py:358] Since rank of parameter decoder/layers_4/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:14.456959 140577131780096 adafactor.py:358] Since rank of parameter decoder/layers_14/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:14.457282 140172769531904 adafactor.py:358] Since rank of parameter decoder/layers_7/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:14.458174 140172769531904 adafactor.py:358] Since rank of parameter decoder/layers_7/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:14.454346 140316961150976 adafactor.py:358] Since rank of parameter decoder/layers_4/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.437358 139812357228544 adafactor.py:358] Since rank of parameter decoder/layers_6/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.457886 140577131780096 adafactor.py:358] Since rank of parameter decoder/layers_14/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.459585 140392160929792 adafactor.py:358] Since rank of parameter decoder/layers_4/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.459144 140172769531904 adafactor.py:358] Since rank of parameter decoder/layers_7/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.460222 140172769531904 adafactor.py:358] Since rank of parameter decoder/layers_7/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:14.438477 139812357228544 adafactor.py:358] Since rank of parameter decoder/layers_6/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.460840 140392160929792 adafactor.py:358] Since rank of parameter decoder/layers_4/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:14.459050 140577131780096 adafactor.py:358] Since rank of parameter decoder/layers_14/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:14.455799 140316961150976 adafactor.py:358] Since rank of parameter decoder/layers_4/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:14.461357 140172769531904 adafactor.py:358] Since rank of parameter decoder/layers_7/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.462141 140392160929792 adafactor.py:358] Since rank of parameter decoder/layers_4/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.460269 140577131780096 adafactor.py:358] Since rank of parameter decoder/layers_14/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.456962 140316961150976 adafactor.py:358] Since rank of parameter decoder/layers_4/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.440024 139812357228544 adafactor.py:358] Since rank of parameter decoder/layers_6/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:14.461381 140577131780096 adafactor.py:358] Since rank of parameter decoder/layers_14/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.441243 139812357228544 adafactor.py:358] Since rank of parameter decoder/layers_6/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:14.462443 140172769531904 adafactor.py:358] Since rank of parameter decoder/layers_7/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.458157 140316961150976 adafactor.py:358] Since rank of parameter decoder/layers_4/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.464346 140392160929792 adafactor.py:358] Since rank of parameter decoder/layers_4/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.462517 140577131780096 adafactor.py:358] Since rank of parameter decoder/layers_15/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.442155 139812357228544 adafactor.py:358] Since rank of parameter decoder/layers_6/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:14.463569 140172769531904 adafactor.py:358] Since rank of parameter decoder/layers_8/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.459292 140316961150976 adafactor.py:358] Since rank of parameter decoder/layers_5/extra_mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.443048 139812357228544 adafactor.py:358] Since rank of parameter decoder/layers_6/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.463641 140577131780096 adafactor.py:358] Since rank of parameter decoder/layers_15/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.465674 140392160929792 adafactor.py:358] Since rank of parameter decoder/layers_5/extra_mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.460408 140316961150976 adafactor.py:358] Since rank of parameter decoder/layers_5/extra_mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.464677 140172769531904 adafactor.py:358] Since rank of parameter decoder/layers_8/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.444312 139812357228544 adafactor.py:358] Since rank of parameter decoder/layers_6/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:14.464797 140577131780096 adafactor.py:358] Since rank of parameter decoder/layers_15/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:14.461519 140316961150976 adafactor.py:358] Since rank of parameter decoder/layers_5/extra_mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:14.467399 140392160929792 adafactor.py:358] Since rank of parameter decoder/layers_5/extra_mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.445506 139812357228544 adafactor.py:358] Since rank of parameter decoder/layers_6/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.465881 140577131780096 adafactor.py:358] Since rank of parameter decoder/layers_15/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:14.465735 140172769531904 adafactor.py:358] Since rank of parameter decoder/layers_8/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:14.466806 140172769531904 adafactor.py:358] Since rank of parameter decoder/layers_8/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:14.467746 140172769531904 adafactor.py:358] Since rank of parameter decoder/layers_8/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:14.466821 140577131780096 adafactor.py:358] Since rank of parameter decoder/layers_15/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:14.468673 140392160929792 adafactor.py:358] Since rank of parameter decoder/layers_5/extra_mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:14.446621 139812357228544 adafactor.py:358] Since rank of parameter decoder/layers_6/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.467772 140577131780096 adafactor.py:358] Since rank of parameter decoder/layers_15/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.447736 139812357228544 adafactor.py:358] Since rank of parameter decoder/layers_7/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.468668 140172769531904 adafactor.py:358] Since rank of parameter decoder/layers_8/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.468919 140577131780096 adafactor.py:358] Since rank of parameter decoder/layers_15/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:14.469772 140172769531904 adafactor.py:358] Since rank of parameter decoder/layers_8/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:14.465778 140316961150976 adafactor.py:358] Since rank of parameter decoder/layers_5/mlp/router/router_weights/w/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>, <FactorDim.NONE: None>) is ignored.
W0512 01:45:14.470864 140172769531904 adafactor.py:358] Since rank of parameter decoder/layers_8/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.448929 139812357228544 adafactor.py:358] Since rank of parameter decoder/layers_7/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.470023 140577131780096 adafactor.py:358] Since rank of parameter decoder/layers_15/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.466748 140316961150976 adafactor.py:358] Since rank of parameter decoder/layers_5/pre_extra_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:14.450065 139812357228544 adafactor.py:358] Since rank of parameter decoder/layers_7/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:14.472015 140172769531904 adafactor.py:358] Since rank of parameter decoder/layers_8/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.467672 140316961150976 adafactor.py:358] Since rank of parameter decoder/layers_5/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:14.471169 140577131780096 adafactor.py:358] Since rank of parameter decoder/layers_15/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.451164 139812357228544 adafactor.py:358] Since rank of parameter decoder/layers_7/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:14.473752 140392160929792 adafactor.py:358] Since rank of parameter decoder/layers_5/mlp/router/router_weights/w/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>, <FactorDim.NONE: None>) is ignored.
W0512 01:45:14.468581 140316961150976 adafactor.py:358] Since rank of parameter decoder/layers_5/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:14.472341 140577131780096 adafactor.py:358] Since rank of parameter decoder/layers_16/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.452051 139812357228544 adafactor.py:358] Since rank of parameter decoder/layers_7/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:14.469494 140316961150976 adafactor.py:358] Since rank of parameter decoder/layers_5/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.473152 140172769531904 adafactor.py:358] Since rank of parameter decoder/layers_9/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.474882 140392160929792 adafactor.py:358] Since rank of parameter decoder/layers_5/pre_extra_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:14.474246 140172769531904 adafactor.py:358] Since rank of parameter decoder/layers_9/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.453022 139812357228544 adafactor.py:358] Since rank of parameter decoder/layers_7/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.473519 140577131780096 adafactor.py:358] Since rank of parameter decoder/layers_16/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.475923 140392160929792 adafactor.py:358] Since rank of parameter decoder/layers_5/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:14.470679 140316961150976 adafactor.py:358] Since rank of parameter decoder/layers_5/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:14.475344 140172769531904 adafactor.py:358] Since rank of parameter decoder/layers_9/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:14.454109 139812357228544 adafactor.py:358] Since rank of parameter decoder/layers_7/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:14.476512 140172769531904 adafactor.py:358] Since rank of parameter decoder/layers_9/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:14.476911 140392160929792 adafactor.py:358] Since rank of parameter decoder/layers_5/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:14.474622 140577131780096 adafactor.py:358] Since rank of parameter decoder/layers_16/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:14.455174 139812357228544 adafactor.py:358] Since rank of parameter decoder/layers_7/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.477433 140172769531904 adafactor.py:358] Since rank of parameter decoder/layers_9/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:14.471848 140316961150976 adafactor.py:358] Since rank of parameter decoder/layers_5/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.475766 140577131780096 adafactor.py:358] Since rank of parameter decoder/layers_16/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:14.477922 140392160929792 adafactor.py:358] Since rank of parameter decoder/layers_5/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.478352 140172769531904 adafactor.py:358] Since rank of parameter decoder/layers_9/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.476692 140577131780096 adafactor.py:358] Since rank of parameter decoder/layers_16/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:14.472973 140316961150976 adafactor.py:358] Since rank of parameter decoder/layers_5/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.456290 139812357228544 adafactor.py:358] Since rank of parameter decoder/layers_7/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.479293 140392160929792 adafactor.py:358] Since rank of parameter decoder/layers_5/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:14.474131 140316961150976 adafactor.py:358] Since rank of parameter decoder/layers_6/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.477668 140577131780096 adafactor.py:358] Since rank of parameter decoder/layers_16/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.479513 140172769531904 adafactor.py:358] Since rank of parameter decoder/layers_9/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:14.457435 139812357228544 adafactor.py:358] Since rank of parameter decoder/layers_8/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.475417 140316961150976 adafactor.py:358] Since rank of parameter decoder/layers_6/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.480653 140172769531904 adafactor.py:358] Since rank of parameter decoder/layers_9/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.480767 140392160929792 adafactor.py:358] Since rank of parameter decoder/layers_5/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.458524 139812357228544 adafactor.py:358] Since rank of parameter decoder/layers_8/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.478785 140577131780096 adafactor.py:358] Since rank of parameter decoder/layers_16/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:14.476614 140316961150976 adafactor.py:358] Since rank of parameter decoder/layers_6/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:14.459614 139812357228544 adafactor.py:358] Since rank of parameter decoder/layers_8/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:14.482123 140392160929792 adafactor.py:358] Since rank of parameter decoder/layers_5/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.482164 140172769531904 adafactor.py:358] Since rank of parameter decoder/layers_9/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.477774 140316961150976 adafactor.py:358] Since rank of parameter decoder/layers_6/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:14.460727 139812357228544 adafactor.py:358] Since rank of parameter decoder/layers_8/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:14.479996 140577131780096 adafactor.py:358] Since rank of parameter decoder/layers_16/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.481213 140577131780096 adafactor.py:358] Since rank of parameter decoder/layers_16/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.483289 140172769531904 adafactor.py:358] Since rank of parameter decoder/logits_dense/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.483434 140392160929792 adafactor.py:358] Since rank of parameter decoder/layers_6/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.461669 139812357228544 adafactor.py:358] Since rank of parameter decoder/layers_8/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:14.478681 140316961150976 adafactor.py:358] Since rank of parameter decoder/layers_6/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:14.484438 140172769531904 adafactor.py:358] Since rank of parameter token_embedder/embedding 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:14.484746 140392160929792 adafactor.py:358] Since rank of parameter decoder/layers_6/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.482903 140577131780096 adafactor.py:358] Since rank of parameter decoder/layers_17/extra_mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.462559 139812357228544 adafactor.py:358] Since rank of parameter decoder/layers_8/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.479552 140316961150976 adafactor.py:358] Since rank of parameter decoder/layers_6/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.480700 140316961150976 adafactor.py:358] Since rank of parameter decoder/layers_6/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:14.463686 139812357228544 adafactor.py:358] Since rank of parameter decoder/layers_8/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:14.484265 140577131780096 adafactor.py:358] Since rank of parameter decoder/layers_17/extra_mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.486033 140392160929792 adafactor.py:358] Since rank of parameter decoder/layers_6/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:14.464830 139812357228544 adafactor.py:358] Since rank of parameter decoder/layers_8/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.481862 140316961150976 adafactor.py:358] Since rank of parameter decoder/layers_6/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.487269 140392160929792 adafactor.py:358] Since rank of parameter decoder/layers_6/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:14.485379 140577131780096 adafactor.py:358] Since rank of parameter decoder/layers_17/extra_mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:14.488246 140392160929792 adafactor.py:358] Since rank of parameter decoder/layers_6/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:14.465981 139812357228544 adafactor.py:358] Since rank of parameter decoder/layers_8/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.482978 140316961150976 adafactor.py:358] Since rank of parameter decoder/layers_6/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.489247 140392160929792 adafactor.py:358] Since rank of parameter decoder/layers_6/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.467056 139812357228544 adafactor.py:358] Since rank of parameter decoder/layers_9/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.484036 140316961150976 adafactor.py:358] Since rank of parameter decoder/layers_7/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.468142 139812357228544 adafactor.py:358] Since rank of parameter decoder/layers_9/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.485183 140316961150976 adafactor.py:358] Since rank of parameter decoder/layers_7/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.490578 140392160929792 adafactor.py:358] Since rank of parameter decoder/layers_6/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:14.489559 140577131780096 adafactor.py:358] Since rank of parameter decoder/layers_17/mlp/router/router_weights/w/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>, <FactorDim.NONE: None>) is ignored.
W0512 01:45:14.469261 139812357228544 adafactor.py:358] Since rank of parameter decoder/layers_9/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:14.486356 140316961150976 adafactor.py:358] Since rank of parameter decoder/layers_7/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:14.491762 140392160929792 adafactor.py:358] Since rank of parameter decoder/layers_6/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.490478 140577131780096 adafactor.py:358] Since rank of parameter decoder/layers_17/pre_extra_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:14.470383 139812357228544 adafactor.py:358] Since rank of parameter decoder/layers_9/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:14.487424 140316961150976 adafactor.py:358] Since rank of parameter decoder/layers_7/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:14.493001 140392160929792 adafactor.py:358] Since rank of parameter decoder/layers_6/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.491457 140577131780096 adafactor.py:358] Since rank of parameter decoder/layers_17/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:14.488301 140316961150976 adafactor.py:358] Since rank of parameter decoder/layers_7/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:14.471272 139812357228544 adafactor.py:358] Since rank of parameter decoder/layers_9/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:14.494316 140392160929792 adafactor.py:358] Since rank of parameter decoder/layers_7/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.492358 140577131780096 adafactor.py:358] Since rank of parameter decoder/layers_17/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:14.489216 140316961150976 adafactor.py:358] Since rank of parameter decoder/layers_7/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.472213 139812357228544 adafactor.py:358] Since rank of parameter decoder/layers_9/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.493263 140577131780096 adafactor.py:358] Since rank of parameter decoder/layers_17/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.473306 139812357228544 adafactor.py:358] Since rank of parameter decoder/layers_9/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:14.490317 140316961150976 adafactor.py:358] Since rank of parameter decoder/layers_7/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:14.495652 140392160929792 adafactor.py:358] Since rank of parameter decoder/layers_7/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.494372 140577131780096 adafactor.py:358] Since rank of parameter decoder/layers_17/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:14.491379 140316961150976 adafactor.py:358] Since rank of parameter decoder/layers_7/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.474382 139812357228544 adafactor.py:358] Since rank of parameter decoder/layers_9/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.496903 140392160929792 adafactor.py:358] Since rank of parameter decoder/layers_7/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:14.495521 140577131780096 adafactor.py:358] Since rank of parameter decoder/layers_17/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.492434 140316961150976 adafactor.py:358] Since rank of parameter decoder/layers_7/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.498229 140392160929792 adafactor.py:358] Since rank of parameter decoder/layers_7/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:14.475986 139812357228544 adafactor.py:358] Since rank of parameter decoder/layers_9/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.496669 140577131780096 adafactor.py:358] Since rank of parameter decoder/layers_17/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.493584 140316961150976 adafactor.py:358] Since rank of parameter decoder/layers_8/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.499261 140392160929792 adafactor.py:358] Since rank of parameter decoder/layers_7/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:14.477103 139812357228544 adafactor.py:358] Since rank of parameter decoder/logits_dense/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.497766 140577131780096 adafactor.py:358] Since rank of parameter decoder/layers_18/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.494688 140316961150976 adafactor.py:358] Since rank of parameter decoder/layers_8/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.500365 140392160929792 adafactor.py:358] Since rank of parameter decoder/layers_7/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.478184 139812357228544 adafactor.py:358] Since rank of parameter token_embedder/embedding 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:14.498869 140577131780096 adafactor.py:358] Since rank of parameter decoder/layers_18/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.495802 140316961150976 adafactor.py:358] Since rank of parameter decoder/layers_8/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:14.501663 140392160929792 adafactor.py:358] Since rank of parameter decoder/layers_7/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:14.500017 140577131780096 adafactor.py:358] Since rank of parameter decoder/layers_18/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:14.496949 140316961150976 adafactor.py:358] Since rank of parameter decoder/layers_8/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:14.501170 140577131780096 adafactor.py:358] Since rank of parameter decoder/layers_18/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:14.503002 140392160929792 adafactor.py:358] Since rank of parameter decoder/layers_7/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.498329 140316961150976 adafactor.py:358] Since rank of parameter decoder/layers_8/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:14.502073 140577131780096 adafactor.py:358] Since rank of parameter decoder/layers_18/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:14.504305 140392160929792 adafactor.py:358] Since rank of parameter decoder/layers_7/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.499240 140316961150976 adafactor.py:358] Since rank of parameter decoder/layers_8/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.503015 140577131780096 adafactor.py:358] Since rank of parameter decoder/layers_18/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.505662 140392160929792 adafactor.py:358] Since rank of parameter decoder/layers_8/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.500349 140316961150976 adafactor.py:358] Since rank of parameter decoder/layers_8/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:14.504144 140577131780096 adafactor.py:358] Since rank of parameter decoder/layers_18/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:14.501478 140316961150976 adafactor.py:358] Since rank of parameter decoder/layers_8/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.507132 140392160929792 adafactor.py:358] Since rank of parameter decoder/layers_8/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.505291 140577131780096 adafactor.py:358] Since rank of parameter decoder/layers_18/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.502627 140316961150976 adafactor.py:358] Since rank of parameter decoder/layers_8/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.506376 140577131780096 adafactor.py:358] Since rank of parameter decoder/layers_18/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.508358 140392160929792 adafactor.py:358] Since rank of parameter decoder/layers_8/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:14.503688 140316961150976 adafactor.py:358] Since rank of parameter decoder/layers_9/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.507518 140577131780096 adafactor.py:358] Since rank of parameter decoder/layers_19/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.509646 140392160929792 adafactor.py:358] Since rank of parameter decoder/layers_8/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:14.504794 140316961150976 adafactor.py:358] Since rank of parameter decoder/layers_9/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.508591 140577131780096 adafactor.py:358] Since rank of parameter decoder/layers_19/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.510795 140392160929792 adafactor.py:358] Since rank of parameter decoder/layers_8/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:14.505949 140316961150976 adafactor.py:358] Since rank of parameter decoder/layers_9/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:14.509742 140577131780096 adafactor.py:358] Since rank of parameter decoder/layers_19/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:14.511854 140392160929792 adafactor.py:358] Since rank of parameter decoder/layers_8/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.507072 140316961150976 adafactor.py:358] Since rank of parameter decoder/layers_9/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:14.510840 140577131780096 adafactor.py:358] Since rank of parameter decoder/layers_19/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:14.513183 140392160929792 adafactor.py:358] Since rank of parameter decoder/layers_8/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:14.507939 140316961150976 adafactor.py:358] Since rank of parameter decoder/layers_9/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:14.511776 140577131780096 adafactor.py:358] Since rank of parameter decoder/layers_19/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:14.508893 140316961150976 adafactor.py:358] Since rank of parameter decoder/layers_9/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.514497 140392160929792 adafactor.py:358] Since rank of parameter decoder/layers_8/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.512680 140577131780096 adafactor.py:358] Since rank of parameter decoder/layers_19/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.510044 140316961150976 adafactor.py:358] Since rank of parameter decoder/layers_9/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:14.513844 140577131780096 adafactor.py:358] Since rank of parameter decoder/layers_19/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:14.515861 140392160929792 adafactor.py:358] Since rank of parameter decoder/layers_8/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.511158 140316961150976 adafactor.py:358] Since rank of parameter decoder/layers_9/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.514958 140577131780096 adafactor.py:358] Since rank of parameter decoder/layers_19/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.517097 140392160929792 adafactor.py:358] Since rank of parameter decoder/layers_9/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.512315 140316961150976 adafactor.py:358] Since rank of parameter decoder/layers_9/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.516099 140577131780096 adafactor.py:358] Since rank of parameter decoder/layers_19/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.513424 140316961150976 adafactor.py:358] Since rank of parameter decoder/logits_dense/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.517216 140577131780096 adafactor.py:358] Since rank of parameter decoder/layers_2/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.518435 140392160929792 adafactor.py:358] Since rank of parameter decoder/layers_9/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.519542 140392160929792 adafactor.py:358] Since rank of parameter decoder/layers_9/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:14.514534 140316961150976 adafactor.py:358] Since rank of parameter token_embedder/embedding 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:14.518344 140577131780096 adafactor.py:358] Since rank of parameter decoder/layers_2/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.520778 140392160929792 adafactor.py:358] Since rank of parameter decoder/layers_9/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:14.519477 140577131780096 adafactor.py:358] Since rank of parameter decoder/layers_2/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:14.521852 140392160929792 adafactor.py:358] Since rank of parameter decoder/layers_9/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:14.520564 140577131780096 adafactor.py:358] Since rank of parameter decoder/layers_2/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:14.522930 140392160929792 adafactor.py:358] Since rank of parameter decoder/layers_9/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.521459 140577131780096 adafactor.py:358] Since rank of parameter decoder/layers_2/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:14.524245 140392160929792 adafactor.py:358] Since rank of parameter decoder/layers_9/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:14.522483 140577131780096 adafactor.py:358] Since rank of parameter decoder/layers_2/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.523621 140577131780096 adafactor.py:358] Since rank of parameter decoder/layers_2/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:14.525555 140392160929792 adafactor.py:358] Since rank of parameter decoder/layers_9/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.524722 140577131780096 adafactor.py:358] Since rank of parameter decoder/layers_2/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.525832 140577131780096 adafactor.py:358] Since rank of parameter decoder/layers_2/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.527716 140392160929792 adafactor.py:358] Since rank of parameter decoder/layers_9/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.526987 140577131780096 adafactor.py:358] Since rank of parameter decoder/layers_20/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.528928 140392160929792 adafactor.py:358] Since rank of parameter decoder/logits_dense/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.528132 140577131780096 adafactor.py:358] Since rank of parameter decoder/layers_20/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.530302 140392160929792 adafactor.py:358] Since rank of parameter token_embedder/embedding 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:14.529234 140577131780096 adafactor.py:358] Since rank of parameter decoder/layers_20/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:14.530352 140577131780096 adafactor.py:358] Since rank of parameter decoder/layers_20/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:14.531331 140577131780096 adafactor.py:358] Since rank of parameter decoder/layers_20/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:14.532300 140577131780096 adafactor.py:358] Since rank of parameter decoder/layers_20/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.533406 140577131780096 adafactor.py:358] Since rank of parameter decoder/layers_20/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:14.534524 140577131780096 adafactor.py:358] Since rank of parameter decoder/layers_20/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.535659 140577131780096 adafactor.py:358] Since rank of parameter decoder/layers_20/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.537629 140577131780096 adafactor.py:358] Since rank of parameter decoder/layers_21/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.538773 140577131780096 adafactor.py:358] Since rank of parameter decoder/layers_21/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.539963 140577131780096 adafactor.py:358] Since rank of parameter decoder/layers_21/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:14.541111 140577131780096 adafactor.py:358] Since rank of parameter decoder/layers_21/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:14.542054 140577131780096 adafactor.py:358] Since rank of parameter decoder/layers_21/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:14.542970 140577131780096 adafactor.py:358] Since rank of parameter decoder/layers_21/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.544082 140577131780096 adafactor.py:358] Since rank of parameter decoder/layers_21/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:14.545168 140577131780096 adafactor.py:358] Since rank of parameter decoder/layers_21/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.546280 140577131780096 adafactor.py:358] Since rank of parameter decoder/layers_21/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.547401 140577131780096 adafactor.py:358] Since rank of parameter decoder/layers_22/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.548470 140577131780096 adafactor.py:358] Since rank of parameter decoder/layers_22/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.549548 140577131780096 adafactor.py:358] Since rank of parameter decoder/layers_22/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:14.550686 140577131780096 adafactor.py:358] Since rank of parameter decoder/layers_22/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:14.551695 140577131780096 adafactor.py:358] Since rank of parameter decoder/layers_22/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:14.555794 140577131780096 adafactor.py:358] Since rank of parameter decoder/layers_22/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.556895 140577131780096 adafactor.py:358] Since rank of parameter decoder/layers_22/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:14.558078 140577131780096 adafactor.py:358] Since rank of parameter decoder/layers_22/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.559182 140577131780096 adafactor.py:358] Since rank of parameter decoder/layers_22/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.560320 140577131780096 adafactor.py:358] Since rank of parameter decoder/layers_23/extra_mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.561431 140577131780096 adafactor.py:358] Since rank of parameter decoder/layers_23/extra_mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.562642 140577131780096 adafactor.py:358] Since rank of parameter decoder/layers_23/extra_mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:14.566832 140577131780096 adafactor.py:358] Since rank of parameter decoder/layers_23/mlp/router/router_weights/w/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>, <FactorDim.NONE: None>) is ignored.
W0512 01:45:14.567809 140577131780096 adafactor.py:358] Since rank of parameter decoder/layers_23/pre_extra_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:14.568712 140577131780096 adafactor.py:358] Since rank of parameter decoder/layers_23/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:14.569608 140577131780096 adafactor.py:358] Since rank of parameter decoder/layers_23/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:14.570517 140577131780096 adafactor.py:358] Since rank of parameter decoder/layers_23/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.571735 140577131780096 adafactor.py:358] Since rank of parameter decoder/layers_23/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:14.572836 140577131780096 adafactor.py:358] Since rank of parameter decoder/layers_23/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.573925 140577131780096 adafactor.py:358] Since rank of parameter decoder/layers_23/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.575031 140577131780096 adafactor.py:358] Since rank of parameter decoder/layers_3/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.576201 140577131780096 adafactor.py:358] Since rank of parameter decoder/layers_3/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.577303 140577131780096 adafactor.py:358] Since rank of parameter decoder/layers_3/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:14.578385 140577131780096 adafactor.py:358] Since rank of parameter decoder/layers_3/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:14.579281 140577131780096 adafactor.py:358] Since rank of parameter decoder/layers_3/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:14.580276 140577131780096 adafactor.py:358] Since rank of parameter decoder/layers_3/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.581385 140577131780096 adafactor.py:358] Since rank of parameter decoder/layers_3/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:14.582514 140577131780096 adafactor.py:358] Since rank of parameter decoder/layers_3/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.583664 140577131780096 adafactor.py:358] Since rank of parameter decoder/layers_3/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.584826 140577131780096 adafactor.py:358] Since rank of parameter decoder/layers_4/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.585909 140577131780096 adafactor.py:358] Since rank of parameter decoder/layers_4/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.587002 140577131780096 adafactor.py:358] Since rank of parameter decoder/layers_4/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:14.588129 140577131780096 adafactor.py:358] Since rank of parameter decoder/layers_4/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:14.589095 140577131780096 adafactor.py:358] Since rank of parameter decoder/layers_4/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:14.589984 140577131780096 adafactor.py:358] Since rank of parameter decoder/layers_4/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.591075 140577131780096 adafactor.py:358] Since rank of parameter decoder/layers_4/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:14.592202 140577131780096 adafactor.py:358] Since rank of parameter decoder/layers_4/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.593750 140577131780096 adafactor.py:358] Since rank of parameter decoder/layers_4/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.594841 140577131780096 adafactor.py:358] Since rank of parameter decoder/layers_5/extra_mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.596128 140577131780096 adafactor.py:358] Since rank of parameter decoder/layers_5/extra_mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.597223 140577131780096 adafactor.py:358] Since rank of parameter decoder/layers_5/extra_mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:14.601384 140577131780096 adafactor.py:358] Since rank of parameter decoder/layers_5/mlp/router/router_weights/w/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>, <FactorDim.NONE: None>) is ignored.
W0512 01:45:14.602395 140577131780096 adafactor.py:358] Since rank of parameter decoder/layers_5/pre_extra_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:14.603319 140577131780096 adafactor.py:358] Since rank of parameter decoder/layers_5/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:14.604229 140577131780096 adafactor.py:358] Since rank of parameter decoder/layers_5/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:14.605123 140577131780096 adafactor.py:358] Since rank of parameter decoder/layers_5/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.606323 140577131780096 adafactor.py:358] Since rank of parameter decoder/layers_5/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:14.607478 140577131780096 adafactor.py:358] Since rank of parameter decoder/layers_5/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.608549 140577131780096 adafactor.py:358] Since rank of parameter decoder/layers_5/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.609623 140577131780096 adafactor.py:358] Since rank of parameter decoder/layers_6/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.610698 140577131780096 adafactor.py:358] Since rank of parameter decoder/layers_6/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.611897 140577131780096 adafactor.py:358] Since rank of parameter decoder/layers_6/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:14.612958 140577131780096 adafactor.py:358] Since rank of parameter decoder/layers_6/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:14.613816 140577131780096 adafactor.py:358] Since rank of parameter decoder/layers_6/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:14.614709 140577131780096 adafactor.py:358] Since rank of parameter decoder/layers_6/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.615890 140577131780096 adafactor.py:358] Since rank of parameter decoder/layers_6/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:14.616968 140577131780096 adafactor.py:358] Since rank of parameter decoder/layers_6/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.618090 140577131780096 adafactor.py:358] Since rank of parameter decoder/layers_6/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.619220 140577131780096 adafactor.py:358] Since rank of parameter decoder/layers_7/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.620403 140577131780096 adafactor.py:358] Since rank of parameter decoder/layers_7/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.621533 140577131780096 adafactor.py:358] Since rank of parameter decoder/layers_7/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:14.622700 140577131780096 adafactor.py:358] Since rank of parameter decoder/layers_7/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:14.623657 140577131780096 adafactor.py:358] Since rank of parameter decoder/layers_7/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:14.624635 140577131780096 adafactor.py:358] Since rank of parameter decoder/layers_7/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.625757 140577131780096 adafactor.py:358] Since rank of parameter decoder/layers_7/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:14.626884 140577131780096 adafactor.py:358] Since rank of parameter decoder/layers_7/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.628014 140577131780096 adafactor.py:358] Since rank of parameter decoder/layers_7/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.629143 140577131780096 adafactor.py:358] Since rank of parameter decoder/layers_8/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.630288 140577131780096 adafactor.py:358] Since rank of parameter decoder/layers_8/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.631460 140577131780096 adafactor.py:358] Since rank of parameter decoder/layers_8/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:14.632558 140577131780096 adafactor.py:358] Since rank of parameter decoder/layers_8/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:14.633502 140577131780096 adafactor.py:358] Since rank of parameter decoder/layers_8/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:14.634397 140577131780096 adafactor.py:358] Since rank of parameter decoder/layers_8/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.635553 140577131780096 adafactor.py:358] Since rank of parameter decoder/layers_8/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:14.636640 140577131780096 adafactor.py:358] Since rank of parameter decoder/layers_8/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.637779 140577131780096 adafactor.py:358] Since rank of parameter decoder/layers_8/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.639068 140577131780096 adafactor.py:358] Since rank of parameter decoder/layers_9/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.640208 140577131780096 adafactor.py:358] Since rank of parameter decoder/layers_9/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.641325 140577131780096 adafactor.py:358] Since rank of parameter decoder/layers_9/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:14.642527 140577131780096 adafactor.py:358] Since rank of parameter decoder/layers_9/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:14.643482 140577131780096 adafactor.py:358] Since rank of parameter decoder/layers_9/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:14.644417 140577131780096 adafactor.py:358] Since rank of parameter decoder/layers_9/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.645509 140577131780096 adafactor.py:358] Since rank of parameter decoder/layers_9/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:14.646621 140577131780096 adafactor.py:358] Since rank of parameter decoder/layers_9/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.648252 140577131780096 adafactor.py:358] Since rank of parameter decoder/layers_9/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.649374 140577131780096 adafactor.py:358] Since rank of parameter decoder/logits_dense/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:14.650485 140577131780096 adafactor.py:358] Since rank of parameter token_embedder/embedding 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
I0512 01:51:47.308800 140316961150976 train.py:421] Initialize/restore complete (397.86 seconds).
I0512 01:51:47.498167 140316961150976 utils.py:1372] Variable decoder/decoder_norm/scale                                                       size 2048         shape (embed=2048)                             partition spec ('model',)
I0512 01:51:47.498803 140316961150976 utils.py:1372] Variable decoder/layers_0/mlp/wi_0/kernel                                                 size 33554432     shape (mlp_embed=2048, mlp=16384)              partition spec (None, 'model')
I0512 01:51:47.498875 140316961150976 utils.py:1372] Variable decoder/layers_0/mlp/wi_1/kernel                                                 size 16777216     shape (mlp_embed=2048, mlp=8192)               partition spec (None, 'model')
I0512 01:51:47.498930 140316961150976 utils.py:1372] Variable decoder/layers_0/mlp/wo/kernel                                                   size 16777216     shape (mlp=8192, mlp_embed=2048)               partition spec ('model', None)
I0512 01:51:47.498980 140316961150976 utils.py:1372] Variable decoder/layers_0/pre_mlp_layer_norm/scale                                        size 2048         shape (embed=2048)                             partition spec ('model',)
I0512 01:51:47.499039 140316961150976 utils.py:1372] Variable decoder/layers_0/pre_self_attention_layer_norm/scale                             size 2048         shape (embed=2048)                             partition spec ('model',)
I0512 01:51:47.499089 140316961150976 utils.py:1372] Variable decoder/layers_0/self_attention/key/kernel                                       size 6291456      shape (embed=2048, joined_kv=3072)             partition spec (None, 'model')
I0512 01:51:47.499138 140316961150976 utils.py:1372] Variable decoder/layers_0/self_attention/out/kernel                                       size 6291456      shape (joined_kv=3072, embed=2048)             partition spec ('model', None)
I0512 01:51:47.499186 140316961150976 utils.py:1372] Variable decoder/layers_0/self_attention/query/kernel                                     size 6291456      shape (embed=2048, joined_kv=3072)             partition spec (None, 'model')
I0512 01:51:47.499234 140316961150976 utils.py:1372] Variable decoder/layers_0/self_attention/value/kernel                                     size 6291456      shape (embed=2048, joined_kv=3072)             partition spec (None, 'model')
I0512 01:51:47.499282 140316961150976 utils.py:1372] Variable decoder/layers_1/mlp/wi_0/kernel                                                 size 33554432     shape (mlp_embed=2048, mlp=16384)              partition spec (None, 'model')
I0512 01:51:47.499331 140316961150976 utils.py:1372] Variable decoder/layers_1/mlp/wi_1/kernel                                                 size 16777216     shape (mlp_embed=2048, mlp=8192)               partition spec (None, 'model')
I0512 01:51:47.499379 140316961150976 utils.py:1372] Variable decoder/layers_1/mlp/wo/kernel                                                   size 16777216     shape (mlp=8192, mlp_embed=2048)               partition spec ('model', None)
I0512 01:51:47.499427 140316961150976 utils.py:1372] Variable decoder/layers_1/pre_mlp_layer_norm/scale                                        size 2048         shape (embed=2048)                             partition spec ('model',)
I0512 01:51:47.499474 140316961150976 utils.py:1372] Variable decoder/layers_1/pre_self_attention_layer_norm/scale                             size 2048         shape (embed=2048)                             partition spec ('model',)
I0512 01:51:47.499520 140316961150976 utils.py:1372] Variable decoder/layers_1/self_attention/key/kernel                                       size 6291456      shape (embed=2048, joined_kv=3072)             partition spec (None, 'model')
I0512 01:51:47.499566 140316961150976 utils.py:1372] Variable decoder/layers_1/self_attention/out/kernel                                       size 6291456      shape (joined_kv=3072, embed=2048)             partition spec ('model', None)
I0512 01:51:47.499611 140316961150976 utils.py:1372] Variable decoder/layers_1/self_attention/query/kernel                                     size 6291456      shape (embed=2048, joined_kv=3072)             partition spec (None, 'model')
I0512 01:51:47.499655 140316961150976 utils.py:1372] Variable decoder/layers_1/self_attention/value/kernel                                     size 6291456      shape (embed=2048, joined_kv=3072)             partition spec (None, 'model')
I0512 01:51:47.499701 140316961150976 utils.py:1372] Variable decoder/layers_10/mlp/wi_0/kernel                                                size 33554432     shape (mlp_embed=2048, mlp=16384)              partition spec (None, 'model')
I0512 01:51:47.499747 140316961150976 utils.py:1372] Variable decoder/layers_10/mlp/wi_1/kernel                                                size 16777216     shape (mlp_embed=2048, mlp=8192)               partition spec (None, 'model')
I0512 01:51:47.499796 140316961150976 utils.py:1372] Variable decoder/layers_10/mlp/wo/kernel                                                  size 16777216     shape (mlp=8192, mlp_embed=2048)               partition spec ('model', None)
I0512 01:51:47.499842 140316961150976 utils.py:1372] Variable decoder/layers_10/pre_mlp_layer_norm/scale                                       size 2048         shape (embed=2048)                             partition spec ('model',)
I0512 01:51:47.499888 140316961150976 utils.py:1372] Variable decoder/layers_10/pre_self_attention_layer_norm/scale                            size 2048         shape (embed=2048)                             partition spec ('model',)
I0512 01:51:47.499933 140316961150976 utils.py:1372] Variable decoder/layers_10/self_attention/key/kernel                                      size 6291456      shape (embed=2048, joined_kv=3072)             partition spec (None, 'model')
I0512 01:51:47.499979 140316961150976 utils.py:1372] Variable decoder/layers_10/self_attention/out/kernel                                      size 6291456      shape (joined_kv=3072, embed=2048)             partition spec ('model', None)
I0512 01:51:47.500032 140316961150976 utils.py:1372] Variable decoder/layers_10/self_attention/query/kernel                                    size 6291456      shape (embed=2048, joined_kv=3072)             partition spec (None, 'model')
I0512 01:51:47.500079 140316961150976 utils.py:1372] Variable decoder/layers_10/self_attention/value/kernel                                    size 6291456      shape (embed=2048, joined_kv=3072)             partition spec (None, 'model')
I0512 01:51:47.500125 140316961150976 utils.py:1372] Variable decoder/layers_11/extra_mlp/wi_0/kernel                                          size 33554432     shape (mlp_embed=2048, mlp=16384)              partition spec (None, 'model')
I0512 01:51:47.500170 140316961150976 utils.py:1372] Variable decoder/layers_11/extra_mlp/wi_1/kernel                                          size 16777216     shape (mlp_embed=2048, mlp=8192)               partition spec (None, 'model')
I0512 01:51:47.500216 140316961150976 utils.py:1372] Variable decoder/layers_11/extra_mlp/wo/kernel                                            size 16777216     shape (mlp=8192, mlp_embed=2048)               partition spec ('model', None)
I0512 01:51:47.500262 140316961150976 utils.py:1372] Variable decoder/layers_11/mlp/expert/wi_0/kernel                                         size 268435456    shape (expert=8, mlp_embed=2048, expert_mlp=16384) partition spec ('expert', None, 'model')
I0512 01:51:47.500309 140316961150976 utils.py:1372] Variable decoder/layers_11/mlp/expert/wi_1/kernel                                         size 134217728    shape (expert=8, mlp_embed=2048, expert_mlp=8192) partition spec ('expert', None, 'model')
I0512 01:51:47.500356 140316961150976 utils.py:1372] Variable decoder/layers_11/mlp/expert/wo/kernel                                           size 134217728    shape (expert=8, expert_mlp=8192, mlp_embed=2048) partition spec ('expert', 'model', None)
I0512 01:51:47.500401 140316961150976 utils.py:1372] Variable decoder/layers_11/mlp/router/router_weights/w/kernel                             size 16384        shape (embed=2048, unmodeled=8)                partition spec ('model', None)
I0512 01:51:47.500447 140316961150976 utils.py:1372] Variable decoder/layers_11/pre_extra_mlp_layer_norm/scale                                 size 2048         shape (embed=2048)                             partition spec ('model',)
I0512 01:51:47.500492 140316961150976 utils.py:1372] Variable decoder/layers_11/pre_mlp_layer_norm/scale                                       size 2048         shape (embed=2048)                             partition spec ('model',)
I0512 01:51:47.500537 140316961150976 utils.py:1372] Variable decoder/layers_11/pre_self_attention_layer_norm/scale                            size 2048         shape (embed=2048)                             partition spec ('model',)
I0512 01:51:47.500582 140316961150976 utils.py:1372] Variable decoder/layers_11/self_attention/key/kernel                                      size 6291456      shape (embed=2048, joined_kv=3072)             partition spec (None, 'model')
I0512 01:51:47.500627 140316961150976 utils.py:1372] Variable decoder/layers_11/self_attention/out/kernel                                      size 6291456      shape (joined_kv=3072, embed=2048)             partition spec ('model', None)
I0512 01:51:47.500673 140316961150976 utils.py:1372] Variable decoder/layers_11/self_attention/query/kernel                                    size 6291456      shape (embed=2048, joined_kv=3072)             partition spec (None, 'model')
I0512 01:51:47.500718 140316961150976 utils.py:1372] Variable decoder/layers_11/self_attention/value/kernel                                    size 6291456      shape (embed=2048, joined_kv=3072)             partition spec (None, 'model')
I0512 01:51:47.500764 140316961150976 utils.py:1372] Variable decoder/layers_12/mlp/wi_0/kernel                                                size 33554432     shape (mlp_embed=2048, mlp=16384)              partition spec (None, 'model')
I0512 01:51:47.500813 140316961150976 utils.py:1372] Variable decoder/layers_12/mlp/wi_1/kernel                                                size 16777216     shape (mlp_embed=2048, mlp=8192)               partition spec (None, 'model')
I0512 01:51:47.500859 140316961150976 utils.py:1372] Variable decoder/layers_12/mlp/wo/kernel                                                  size 16777216     shape (mlp=8192, mlp_embed=2048)               partition spec ('model', None)
I0512 01:51:47.500955 140316961150976 utils.py:1372] Variable decoder/layers_12/pre_mlp_layer_norm/scale                                       size 2048         shape (embed=2048)                             partition spec ('model',)
I0512 01:51:47.501013 140316961150976 utils.py:1372] Variable decoder/layers_12/pre_self_attention_layer_norm/scale                            size 2048         shape (embed=2048)                             partition spec ('model',)
I0512 01:51:47.501062 140316961150976 utils.py:1372] Variable decoder/layers_12/self_attention/key/kernel                                      size 6291456      shape (embed=2048, joined_kv=3072)             partition spec (None, 'model')
I0512 01:51:47.501109 140316961150976 utils.py:1372] Variable decoder/layers_12/self_attention/out/kernel                                      size 6291456      shape (joined_kv=3072, embed=2048)             partition spec ('model', None)
I0512 01:51:47.501154 140316961150976 utils.py:1372] Variable decoder/layers_12/self_attention/query/kernel                                    size 6291456      shape (embed=2048, joined_kv=3072)             partition spec (None, 'model')
I0512 01:51:47.501199 140316961150976 utils.py:1372] Variable decoder/layers_12/self_attention/value/kernel                                    size 6291456      shape (embed=2048, joined_kv=3072)             partition spec (None, 'model')
I0512 01:51:47.501245 140316961150976 utils.py:1372] Variable decoder/layers_13/mlp/wi_0/kernel                                                size 33554432     shape (mlp_embed=2048, mlp=16384)              partition spec (None, 'model')
I0512 01:51:47.501291 140316961150976 utils.py:1372] Variable decoder/layers_13/mlp/wi_1/kernel                                                size 16777216     shape (mlp_embed=2048, mlp=8192)               partition spec (None, 'model')
I0512 01:51:47.501338 140316961150976 utils.py:1372] Variable decoder/layers_13/mlp/wo/kernel                                                  size 16777216     shape (mlp=8192, mlp_embed=2048)               partition spec ('model', None)
I0512 01:51:47.501384 140316961150976 utils.py:1372] Variable decoder/layers_13/pre_mlp_layer_norm/scale                                       size 2048         shape (embed=2048)                             partition spec ('model',)
I0512 01:51:47.501430 140316961150976 utils.py:1372] Variable decoder/layers_13/pre_self_attention_layer_norm/scale                            size 2048         shape (embed=2048)                             partition spec ('model',)
I0512 01:51:47.501476 140316961150976 utils.py:1372] Variable decoder/layers_13/self_attention/key/kernel                                      size 6291456      shape (embed=2048, joined_kv=3072)             partition spec (None, 'model')
I0512 01:51:47.501523 140316961150976 utils.py:1372] Variable decoder/layers_13/self_attention/out/kernel                                      size 6291456      shape (joined_kv=3072, embed=2048)             partition spec ('model', None)
I0512 01:51:47.501603 140316961150976 utils.py:1372] Variable decoder/layers_13/self_attention/query/kernel                                    size 6291456      shape (embed=2048, joined_kv=3072)             partition spec (None, 'model')
I0512 01:51:47.501653 140316961150976 utils.py:1372] Variable decoder/layers_13/self_attention/value/kernel                                    size 6291456      shape (embed=2048, joined_kv=3072)             partition spec (None, 'model')
I0512 01:51:47.501699 140316961150976 utils.py:1372] Variable decoder/layers_14/mlp/wi_0/kernel                                                size 33554432     shape (mlp_embed=2048, mlp=16384)              partition spec (None, 'model')
I0512 01:51:47.501745 140316961150976 utils.py:1372] Variable decoder/layers_14/mlp/wi_1/kernel                                                size 16777216     shape (mlp_embed=2048, mlp=8192)               partition spec (None, 'model')
I0512 01:51:47.501794 140316961150976 utils.py:1372] Variable decoder/layers_14/mlp/wo/kernel                                                  size 16777216     shape (mlp=8192, mlp_embed=2048)               partition spec ('model', None)
I0512 01:51:47.501840 140316961150976 utils.py:1372] Variable decoder/layers_14/pre_mlp_layer_norm/scale                                       size 2048         shape (embed=2048)                             partition spec ('model',)
I0512 01:51:47.501886 140316961150976 utils.py:1372] Variable decoder/layers_14/pre_self_attention_layer_norm/scale                            size 2048         shape (embed=2048)                             partition spec ('model',)
I0512 01:51:47.501932 140316961150976 utils.py:1372] Variable decoder/layers_14/self_attention/key/kernel                                      size 6291456      shape (embed=2048, joined_kv=3072)             partition spec (None, 'model')
I0512 01:51:47.501978 140316961150976 utils.py:1372] Variable decoder/layers_14/self_attention/out/kernel                                      size 6291456      shape (joined_kv=3072, embed=2048)             partition spec ('model', None)
I0512 01:51:47.502030 140316961150976 utils.py:1372] Variable decoder/layers_14/self_attention/query/kernel                                    size 6291456      shape (embed=2048, joined_kv=3072)             partition spec (None, 'model')
I0512 01:51:47.502077 140316961150976 utils.py:1372] Variable decoder/layers_14/self_attention/value/kernel                                    size 6291456      shape (embed=2048, joined_kv=3072)             partition spec (None, 'model')
I0512 01:51:47.502122 140316961150976 utils.py:1372] Variable decoder/layers_15/mlp/wi_0/kernel                                                size 33554432     shape (mlp_embed=2048, mlp=16384)              partition spec (None, 'model')
I0512 01:51:47.502169 140316961150976 utils.py:1372] Variable decoder/layers_15/mlp/wi_1/kernel                                                size 16777216     shape (mlp_embed=2048, mlp=8192)               partition spec (None, 'model')
I0512 01:51:47.502214 140316961150976 utils.py:1372] Variable decoder/layers_15/mlp/wo/kernel                                                  size 16777216     shape (mlp=8192, mlp_embed=2048)               partition spec ('model', None)
I0512 01:51:47.502260 140316961150976 utils.py:1372] Variable decoder/layers_15/pre_mlp_layer_norm/scale                                       size 2048         shape (embed=2048)                             partition spec ('model',)
I0512 01:51:47.502305 140316961150976 utils.py:1372] Variable decoder/layers_15/pre_self_attention_layer_norm/scale                            size 2048         shape (embed=2048)                             partition spec ('model',)
I0512 01:51:47.502350 140316961150976 utils.py:1372] Variable decoder/layers_15/self_attention/key/kernel                                      size 6291456      shape (embed=2048, joined_kv=3072)             partition spec (None, 'model')
I0512 01:51:47.502396 140316961150976 utils.py:1372] Variable decoder/layers_15/self_attention/out/kernel                                      size 6291456      shape (joined_kv=3072, embed=2048)             partition spec ('model', None)
I0512 01:51:47.502441 140316961150976 utils.py:1372] Variable decoder/layers_15/self_attention/query/kernel                                    size 6291456      shape (embed=2048, joined_kv=3072)             partition spec (None, 'model')
I0512 01:51:47.502487 140316961150976 utils.py:1372] Variable decoder/layers_15/self_attention/value/kernel                                    size 6291456      shape (embed=2048, joined_kv=3072)             partition spec (None, 'model')
I0512 01:51:47.502532 140316961150976 utils.py:1372] Variable decoder/layers_16/mlp/wi_0/kernel                                                size 33554432     shape (mlp_embed=2048, mlp=16384)              partition spec (None, 'model')
I0512 01:51:47.502578 140316961150976 utils.py:1372] Variable decoder/layers_16/mlp/wi_1/kernel                                                size 16777216     shape (mlp_embed=2048, mlp=8192)               partition spec (None, 'model')
I0512 01:51:47.502623 140316961150976 utils.py:1372] Variable decoder/layers_16/mlp/wo/kernel                                                  size 16777216     shape (mlp=8192, mlp_embed=2048)               partition spec ('model', None)
I0512 01:51:47.502667 140316961150976 utils.py:1372] Variable decoder/layers_16/pre_mlp_layer_norm/scale                                       size 2048         shape (embed=2048)                             partition spec ('model',)
I0512 01:51:47.502717 140316961150976 utils.py:1372] Variable decoder/layers_16/pre_self_attention_layer_norm/scale                            size 2048         shape (embed=2048)                             partition spec ('model',)
I0512 01:51:47.502770 140316961150976 utils.py:1372] Variable decoder/layers_16/self_attention/key/kernel                                      size 6291456      shape (embed=2048, joined_kv=3072)             partition spec (None, 'model')
I0512 01:51:47.502819 140316961150976 utils.py:1372] Variable decoder/layers_16/self_attention/out/kernel                                      size 6291456      shape (joined_kv=3072, embed=2048)             partition spec ('model', None)
I0512 01:51:47.502866 140316961150976 utils.py:1372] Variable decoder/layers_16/self_attention/query/kernel                                    size 6291456      shape (embed=2048, joined_kv=3072)             partition spec (None, 'model')
I0512 01:51:47.502921 140316961150976 utils.py:1372] Variable decoder/layers_16/self_attention/value/kernel                                    size 6291456      shape (embed=2048, joined_kv=3072)             partition spec (None, 'model')
I0512 01:51:47.502968 140316961150976 utils.py:1372] Variable decoder/layers_17/extra_mlp/wi_0/kernel                                          size 33554432     shape (mlp_embed=2048, mlp=16384)              partition spec (None, 'model')
I0512 01:51:47.503021 140316961150976 utils.py:1372] Variable decoder/layers_17/extra_mlp/wi_1/kernel                                          size 16777216     shape (mlp_embed=2048, mlp=8192)               partition spec (None, 'model')
I0512 01:51:47.503068 140316961150976 utils.py:1372] Variable decoder/layers_17/extra_mlp/wo/kernel                                            size 16777216     shape (mlp=8192, mlp_embed=2048)               partition spec ('model', None)
I0512 01:51:47.503136 140316961150976 utils.py:1372] Variable decoder/layers_17/mlp/expert/wi_0/kernel                                         size 268435456    shape (expert=8, mlp_embed=2048, expert_mlp=16384) partition spec ('expert', None, 'model')
I0512 01:51:47.503187 140316961150976 utils.py:1372] Variable decoder/layers_17/mlp/expert/wi_1/kernel                                         size 134217728    shape (expert=8, mlp_embed=2048, expert_mlp=8192) partition spec ('expert', None, 'model')
I0512 01:51:47.503233 140316961150976 utils.py:1372] Variable decoder/layers_17/mlp/expert/wo/kernel                                           size 134217728    shape (expert=8, expert_mlp=8192, mlp_embed=2048) partition spec ('expert', 'model', None)
I0512 01:51:47.503278 140316961150976 utils.py:1372] Variable decoder/layers_17/mlp/router/router_weights/w/kernel                             size 16384        shape (embed=2048, unmodeled=8)                partition spec ('model', None)
I0512 01:51:47.503324 140316961150976 utils.py:1372] Variable decoder/layers_17/pre_extra_mlp_layer_norm/scale                                 size 2048         shape (embed=2048)                             partition spec ('model',)
I0512 01:51:47.503370 140316961150976 utils.py:1372] Variable decoder/layers_17/pre_mlp_layer_norm/scale                                       size 2048         shape (embed=2048)                             partition spec ('model',)
I0512 01:51:47.503414 140316961150976 utils.py:1372] Variable decoder/layers_17/pre_self_attention_layer_norm/scale                            size 2048         shape (embed=2048)                             partition spec ('model',)
I0512 01:51:47.503458 140316961150976 utils.py:1372] Variable decoder/layers_17/self_attention/key/kernel                                      size 6291456      shape (embed=2048, joined_kv=3072)             partition spec (None, 'model')
I0512 01:51:47.503504 140316961150976 utils.py:1372] Variable decoder/layers_17/self_attention/out/kernel                                      size 6291456      shape (joined_kv=3072, embed=2048)             partition spec ('model', None)
I0512 01:51:47.503549 140316961150976 utils.py:1372] Variable decoder/layers_17/self_attention/query/kernel                                    size 6291456      shape (embed=2048, joined_kv=3072)             partition spec (None, 'model')
I0512 01:51:47.503595 140316961150976 utils.py:1372] Variable decoder/layers_17/self_attention/value/kernel                                    size 6291456      shape (embed=2048, joined_kv=3072)             partition spec (None, 'model')
I0512 01:51:47.503641 140316961150976 utils.py:1372] Variable decoder/layers_18/mlp/wi_0/kernel                                                size 33554432     shape (mlp_embed=2048, mlp=16384)              partition spec (None, 'model')
I0512 01:51:47.503687 140316961150976 utils.py:1372] Variable decoder/layers_18/mlp/wi_1/kernel                                                size 16777216     shape (mlp_embed=2048, mlp=8192)               partition spec (None, 'model')
I0512 01:51:47.503732 140316961150976 utils.py:1372] Variable decoder/layers_18/mlp/wo/kernel                                                  size 16777216     shape (mlp=8192, mlp_embed=2048)               partition spec ('model', None)
I0512 01:51:47.503778 140316961150976 utils.py:1372] Variable decoder/layers_18/pre_mlp_layer_norm/scale                                       size 2048         shape (embed=2048)                             partition spec ('model',)
I0512 01:51:47.503824 140316961150976 utils.py:1372] Variable decoder/layers_18/pre_self_attention_layer_norm/scale                            size 2048         shape (embed=2048)                             partition spec ('model',)
I0512 01:51:47.503868 140316961150976 utils.py:1372] Variable decoder/layers_18/self_attention/key/kernel                                      size 6291456      shape (embed=2048, joined_kv=3072)             partition spec (None, 'model')
I0512 01:51:47.503912 140316961150976 utils.py:1372] Variable decoder/layers_18/self_attention/out/kernel                                      size 6291456      shape (joined_kv=3072, embed=2048)             partition spec ('model', None)
I0512 01:51:47.503956 140316961150976 utils.py:1372] Variable decoder/layers_18/self_attention/query/kernel                                    size 6291456      shape (embed=2048, joined_kv=3072)             partition spec (None, 'model')
I0512 01:51:47.504000 140316961150976 utils.py:1372] Variable decoder/layers_18/self_attention/value/kernel                                    size 6291456      shape (embed=2048, joined_kv=3072)             partition spec (None, 'model')
I0512 01:51:47.504054 140316961150976 utils.py:1372] Variable decoder/layers_19/mlp/wi_0/kernel                                                size 33554432     shape (mlp_embed=2048, mlp=16384)              partition spec (None, 'model')
I0512 01:51:47.504099 140316961150976 utils.py:1372] Variable decoder/layers_19/mlp/wi_1/kernel                                                size 16777216     shape (mlp_embed=2048, mlp=8192)               partition spec (None, 'model')
I0512 01:51:47.504143 140316961150976 utils.py:1372] Variable decoder/layers_19/mlp/wo/kernel                                                  size 16777216     shape (mlp=8192, mlp_embed=2048)               partition spec ('model', None)
I0512 01:51:47.504187 140316961150976 utils.py:1372] Variable decoder/layers_19/pre_mlp_layer_norm/scale                                       size 2048         shape (embed=2048)                             partition spec ('model',)
I0512 01:51:47.504232 140316961150976 utils.py:1372] Variable decoder/layers_19/pre_self_attention_layer_norm/scale                            size 2048         shape (embed=2048)                             partition spec ('model',)
I0512 01:51:47.504276 140316961150976 utils.py:1372] Variable decoder/layers_19/self_attention/key/kernel                                      size 6291456      shape (embed=2048, joined_kv=3072)             partition spec (None, 'model')
I0512 01:51:47.504320 140316961150976 utils.py:1372] Variable decoder/layers_19/self_attention/out/kernel                                      size 6291456      shape (joined_kv=3072, embed=2048)             partition spec ('model', None)
I0512 01:51:47.504363 140316961150976 utils.py:1372] Variable decoder/layers_19/self_attention/query/kernel                                    size 6291456      shape (embed=2048, joined_kv=3072)             partition spec (None, 'model')
I0512 01:51:47.504407 140316961150976 utils.py:1372] Variable decoder/layers_19/self_attention/value/kernel                                    size 6291456      shape (embed=2048, joined_kv=3072)             partition spec (None, 'model')
I0512 01:51:47.504452 140316961150976 utils.py:1372] Variable decoder/layers_2/mlp/wi_0/kernel                                                 size 33554432     shape (mlp_embed=2048, mlp=16384)              partition spec (None, 'model')
I0512 01:51:47.504497 140316961150976 utils.py:1372] Variable decoder/layers_2/mlp/wi_1/kernel                                                 size 16777216     shape (mlp_embed=2048, mlp=8192)               partition spec (None, 'model')
I0512 01:51:47.504541 140316961150976 utils.py:1372] Variable decoder/layers_2/mlp/wo/kernel                                                   size 16777216     shape (mlp=8192, mlp_embed=2048)               partition spec ('model', None)
I0512 01:51:47.504585 140316961150976 utils.py:1372] Variable decoder/layers_2/pre_mlp_layer_norm/scale                                        size 2048         shape (embed=2048)                             partition spec ('model',)
I0512 01:51:47.504629 140316961150976 utils.py:1372] Variable decoder/layers_2/pre_self_attention_layer_norm/scale                             size 2048         shape (embed=2048)                             partition spec ('model',)
I0512 01:51:47.504673 140316961150976 utils.py:1372] Variable decoder/layers_2/self_attention/key/kernel                                       size 6291456      shape (embed=2048, joined_kv=3072)             partition spec (None, 'model')
I0512 01:51:47.504717 140316961150976 utils.py:1372] Variable decoder/layers_2/self_attention/out/kernel                                       size 6291456      shape (joined_kv=3072, embed=2048)             partition spec ('model', None)
I0512 01:51:47.504762 140316961150976 utils.py:1372] Variable decoder/layers_2/self_attention/query/kernel                                     size 6291456      shape (embed=2048, joined_kv=3072)             partition spec (None, 'model')
I0512 01:51:47.504808 140316961150976 utils.py:1372] Variable decoder/layers_2/self_attention/value/kernel                                     size 6291456      shape (embed=2048, joined_kv=3072)             partition spec (None, 'model')
I0512 01:51:47.504853 140316961150976 utils.py:1372] Variable decoder/layers_20/mlp/wi_0/kernel                                                size 33554432     shape (mlp_embed=2048, mlp=16384)              partition spec (None, 'model')
I0512 01:51:47.504898 140316961150976 utils.py:1372] Variable decoder/layers_20/mlp/wi_1/kernel                                                size 16777216     shape (mlp_embed=2048, mlp=8192)               partition spec (None, 'model')
I0512 01:51:47.504942 140316961150976 utils.py:1372] Variable decoder/layers_20/mlp/wo/kernel                                                  size 16777216     shape (mlp=8192, mlp_embed=2048)               partition spec ('model', None)
I0512 01:51:47.504986 140316961150976 utils.py:1372] Variable decoder/layers_20/pre_mlp_layer_norm/scale                                       size 2048         shape (embed=2048)                             partition spec ('model',)
I0512 01:51:47.505038 140316961150976 utils.py:1372] Variable decoder/layers_20/pre_self_attention_layer_norm/scale                            size 2048         shape (embed=2048)                             partition spec ('model',)
I0512 01:51:47.505083 140316961150976 utils.py:1372] Variable decoder/layers_20/self_attention/key/kernel                                      size 6291456      shape (embed=2048, joined_kv=3072)             partition spec (None, 'model')
I0512 01:51:47.505127 140316961150976 utils.py:1372] Variable decoder/layers_20/self_attention/out/kernel                                      size 6291456      shape (joined_kv=3072, embed=2048)             partition spec ('model', None)
I0512 01:51:47.505187 140316961150976 utils.py:1372] Variable decoder/layers_20/self_attention/query/kernel                                    size 6291456      shape (embed=2048, joined_kv=3072)             partition spec (None, 'model')
I0512 01:51:47.505234 140316961150976 utils.py:1372] Variable decoder/layers_20/self_attention/value/kernel                                    size 6291456      shape (embed=2048, joined_kv=3072)             partition spec (None, 'model')
I0512 01:51:47.505279 140316961150976 utils.py:1372] Variable decoder/layers_21/mlp/wi_0/kernel                                                size 33554432     shape (mlp_embed=2048, mlp=16384)              partition spec (None, 'model')
I0512 01:51:47.505324 140316961150976 utils.py:1372] Variable decoder/layers_21/mlp/wi_1/kernel                                                size 16777216     shape (mlp_embed=2048, mlp=8192)               partition spec (None, 'model')
I0512 01:51:47.505368 140316961150976 utils.py:1372] Variable decoder/layers_21/mlp/wo/kernel                                                  size 16777216     shape (mlp=8192, mlp_embed=2048)               partition spec ('model', None)
I0512 01:51:47.505412 140316961150976 utils.py:1372] Variable decoder/layers_21/pre_mlp_layer_norm/scale                                       size 2048         shape (embed=2048)                             partition spec ('model',)
I0512 01:51:47.505457 140316961150976 utils.py:1372] Variable decoder/layers_21/pre_self_attention_layer_norm/scale                            size 2048         shape (embed=2048)                             partition spec ('model',)
I0512 01:51:47.505501 140316961150976 utils.py:1372] Variable decoder/layers_21/self_attention/key/kernel                                      size 6291456      shape (embed=2048, joined_kv=3072)             partition spec (None, 'model')
I0512 01:51:47.505567 140316961150976 utils.py:1372] Variable decoder/layers_21/self_attention/out/kernel                                      size 6291456      shape (joined_kv=3072, embed=2048)             partition spec ('model', None)
I0512 01:51:47.505621 140316961150976 utils.py:1372] Variable decoder/layers_21/self_attention/query/kernel                                    size 6291456      shape (embed=2048, joined_kv=3072)             partition spec (None, 'model')
I0512 01:51:47.505668 140316961150976 utils.py:1372] Variable decoder/layers_21/self_attention/value/kernel                                    size 6291456      shape (embed=2048, joined_kv=3072)             partition spec (None, 'model')
I0512 01:51:47.505712 140316961150976 utils.py:1372] Variable decoder/layers_22/mlp/wi_0/kernel                                                size 33554432     shape (mlp_embed=2048, mlp=16384)              partition spec (None, 'model')
I0512 01:51:47.505757 140316961150976 utils.py:1372] Variable decoder/layers_22/mlp/wi_1/kernel                                                size 16777216     shape (mlp_embed=2048, mlp=8192)               partition spec (None, 'model')
I0512 01:51:47.505804 140316961150976 utils.py:1372] Variable decoder/layers_22/mlp/wo/kernel                                                  size 16777216     shape (mlp=8192, mlp_embed=2048)               partition spec ('model', None)
I0512 01:51:47.505848 140316961150976 utils.py:1372] Variable decoder/layers_22/pre_mlp_layer_norm/scale                                       size 2048         shape (embed=2048)                             partition spec ('model',)
I0512 01:51:47.505892 140316961150976 utils.py:1372] Variable decoder/layers_22/pre_self_attention_layer_norm/scale                            size 2048         shape (embed=2048)                             partition spec ('model',)
I0512 01:51:47.505935 140316961150976 utils.py:1372] Variable decoder/layers_22/self_attention/key/kernel                                      size 6291456      shape (embed=2048, joined_kv=3072)             partition spec (None, 'model')
I0512 01:51:47.505979 140316961150976 utils.py:1372] Variable decoder/layers_22/self_attention/out/kernel                                      size 6291456      shape (joined_kv=3072, embed=2048)             partition spec ('model', None)
I0512 01:51:47.506030 140316961150976 utils.py:1372] Variable decoder/layers_22/self_attention/query/kernel                                    size 6291456      shape (embed=2048, joined_kv=3072)             partition spec (None, 'model')
I0512 01:51:47.506075 140316961150976 utils.py:1372] Variable decoder/layers_22/self_attention/value/kernel                                    size 6291456      shape (embed=2048, joined_kv=3072)             partition spec (None, 'model')
I0512 01:51:47.506119 140316961150976 utils.py:1372] Variable decoder/layers_23/extra_mlp/wi_0/kernel                                          size 33554432     shape (mlp_embed=2048, mlp=16384)              partition spec (None, 'model')
I0512 01:51:47.506163 140316961150976 utils.py:1372] Variable decoder/layers_23/extra_mlp/wi_1/kernel                                          size 16777216     shape (mlp_embed=2048, mlp=8192)               partition spec (None, 'model')
I0512 01:51:47.506206 140316961150976 utils.py:1372] Variable decoder/layers_23/extra_mlp/wo/kernel                                            size 16777216     shape (mlp=8192, mlp_embed=2048)               partition spec ('model', None)
I0512 01:51:47.506251 140316961150976 utils.py:1372] Variable decoder/layers_23/mlp/expert/wi_0/kernel                                         size 268435456    shape (expert=8, mlp_embed=2048, expert_mlp=16384) partition spec ('expert', None, 'model')
I0512 01:51:47.506296 140316961150976 utils.py:1372] Variable decoder/layers_23/mlp/expert/wi_1/kernel                                         size 134217728    shape (expert=8, mlp_embed=2048, expert_mlp=8192) partition spec ('expert', None, 'model')
I0512 01:51:47.506340 140316961150976 utils.py:1372] Variable decoder/layers_23/mlp/expert/wo/kernel                                           size 134217728    shape (expert=8, expert_mlp=8192, mlp_embed=2048) partition spec ('expert', 'model', None)
I0512 01:51:47.506385 140316961150976 utils.py:1372] Variable decoder/layers_23/mlp/router/router_weights/w/kernel                             size 16384        shape (embed=2048, unmodeled=8)                partition spec ('model', None)
I0512 01:51:47.506429 140316961150976 utils.py:1372] Variable decoder/layers_23/pre_extra_mlp_layer_norm/scale                                 size 2048         shape (embed=2048)                             partition spec ('model',)
I0512 01:51:47.506473 140316961150976 utils.py:1372] Variable decoder/layers_23/pre_mlp_layer_norm/scale                                       size 2048         shape (embed=2048)                             partition spec ('model',)
I0512 01:51:47.506516 140316961150976 utils.py:1372] Variable decoder/layers_23/pre_self_attention_layer_norm/scale                            size 2048         shape (embed=2048)                             partition spec ('model',)
I0512 01:51:47.506560 140316961150976 utils.py:1372] Variable decoder/layers_23/self_attention/key/kernel                                      size 6291456      shape (embed=2048, joined_kv=3072)             partition spec (None, 'model')
I0512 01:51:47.506604 140316961150976 utils.py:1372] Variable decoder/layers_23/self_attention/out/kernel                                      size 6291456      shape (joined_kv=3072, embed=2048)             partition spec ('model', None)
I0512 01:51:47.506647 140316961150976 utils.py:1372] Variable decoder/layers_23/self_attention/query/kernel                                    size 6291456      shape (embed=2048, joined_kv=3072)             partition spec (None, 'model')
I0512 01:51:47.506691 140316961150976 utils.py:1372] Variable decoder/layers_23/self_attention/value/kernel                                    size 6291456      shape (embed=2048, joined_kv=3072)             partition spec (None, 'model')
I0512 01:51:47.506734 140316961150976 utils.py:1372] Variable decoder/layers_3/mlp/wi_0/kernel                                                 size 33554432     shape (mlp_embed=2048, mlp=16384)              partition spec (None, 'model')
I0512 01:51:47.506782 140316961150976 utils.py:1372] Variable decoder/layers_3/mlp/wi_1/kernel                                                 size 16777216     shape (mlp_embed=2048, mlp=8192)               partition spec (None, 'model')
I0512 01:51:47.506827 140316961150976 utils.py:1372] Variable decoder/layers_3/mlp/wo/kernel                                                   size 16777216     shape (mlp=8192, mlp_embed=2048)               partition spec ('model', None)
I0512 01:51:47.506870 140316961150976 utils.py:1372] Variable decoder/layers_3/pre_mlp_layer_norm/scale                                        size 2048         shape (embed=2048)                             partition spec ('model',)
I0512 01:51:47.506914 140316961150976 utils.py:1372] Variable decoder/layers_3/pre_self_attention_layer_norm/scale                             size 2048         shape (embed=2048)                             partition spec ('model',)
I0512 01:51:47.506957 140316961150976 utils.py:1372] Variable decoder/layers_3/self_attention/key/kernel                                       size 6291456      shape (embed=2048, joined_kv=3072)             partition spec (None, 'model')
I0512 01:51:47.507001 140316961150976 utils.py:1372] Variable decoder/layers_3/self_attention/out/kernel                                       size 6291456      shape (joined_kv=3072, embed=2048)             partition spec ('model', None)
I0512 01:51:47.507053 140316961150976 utils.py:1372] Variable decoder/layers_3/self_attention/query/kernel                                     size 6291456      shape (embed=2048, joined_kv=3072)             partition spec (None, 'model')
I0512 01:51:47.507097 140316961150976 utils.py:1372] Variable decoder/layers_3/self_attention/value/kernel                                     size 6291456      shape (embed=2048, joined_kv=3072)             partition spec (None, 'model')
I0512 01:51:47.507141 140316961150976 utils.py:1372] Variable decoder/layers_4/mlp/wi_0/kernel                                                 size 33554432     shape (mlp_embed=2048, mlp=16384)              partition spec (None, 'model')
I0512 01:51:47.507184 140316961150976 utils.py:1372] Variable decoder/layers_4/mlp/wi_1/kernel                                                 size 16777216     shape (mlp_embed=2048, mlp=8192)               partition spec (None, 'model')
I0512 01:51:47.507247 140316961150976 utils.py:1372] Variable decoder/layers_4/mlp/wo/kernel                                                   size 16777216     shape (mlp=8192, mlp_embed=2048)               partition spec ('model', None)
I0512 01:51:47.507294 140316961150976 utils.py:1372] Variable decoder/layers_4/pre_mlp_layer_norm/scale                                        size 2048         shape (embed=2048)                             partition spec ('model',)
I0512 01:51:47.507338 140316961150976 utils.py:1372] Variable decoder/layers_4/pre_self_attention_layer_norm/scale                             size 2048         shape (embed=2048)                             partition spec ('model',)
I0512 01:51:47.507381 140316961150976 utils.py:1372] Variable decoder/layers_4/self_attention/key/kernel                                       size 6291456      shape (embed=2048, joined_kv=3072)             partition spec (None, 'model')
I0512 01:51:47.507425 140316961150976 utils.py:1372] Variable decoder/layers_4/self_attention/out/kernel                                       size 6291456      shape (joined_kv=3072, embed=2048)             partition spec ('model', None)
I0512 01:51:47.507468 140316961150976 utils.py:1372] Variable decoder/layers_4/self_attention/query/kernel                                     size 6291456      shape (embed=2048, joined_kv=3072)             partition spec (None, 'model')
I0512 01:51:47.507512 140316961150976 utils.py:1372] Variable decoder/layers_4/self_attention/value/kernel                                     size 6291456      shape (embed=2048, joined_kv=3072)             partition spec (None, 'model')
I0512 01:51:47.507555 140316961150976 utils.py:1372] Variable decoder/layers_5/extra_mlp/wi_0/kernel                                           size 33554432     shape (mlp_embed=2048, mlp=16384)              partition spec (None, 'model')
I0512 01:51:47.507599 140316961150976 utils.py:1372] Variable decoder/layers_5/extra_mlp/wi_1/kernel                                           size 16777216     shape (mlp_embed=2048, mlp=8192)               partition spec (None, 'model')
I0512 01:51:47.507643 140316961150976 utils.py:1372] Variable decoder/layers_5/extra_mlp/wo/kernel                                             size 16777216     shape (mlp=8192, mlp_embed=2048)               partition spec ('model', None)
I0512 01:51:47.507688 140316961150976 utils.py:1372] Variable decoder/layers_5/mlp/expert/wi_0/kernel                                          size 268435456    shape (expert=8, mlp_embed=2048, expert_mlp=16384) partition spec ('expert', None, 'model')
I0512 01:51:47.507732 140316961150976 utils.py:1372] Variable decoder/layers_5/mlp/expert/wi_1/kernel                                          size 134217728    shape (expert=8, mlp_embed=2048, expert_mlp=8192) partition spec ('expert', None, 'model')
I0512 01:51:47.507779 140316961150976 utils.py:1372] Variable decoder/layers_5/mlp/expert/wo/kernel                                            size 134217728    shape (expert=8, expert_mlp=8192, mlp_embed=2048) partition spec ('expert', 'model', None)
I0512 01:51:47.507825 140316961150976 utils.py:1372] Variable decoder/layers_5/mlp/router/router_weights/w/kernel                              size 16384        shape (embed=2048, unmodeled=8)                partition spec ('model', None)
I0512 01:51:47.507869 140316961150976 utils.py:1372] Variable decoder/layers_5/pre_extra_mlp_layer_norm/scale                                  size 2048         shape (embed=2048)                             partition spec ('model',)
I0512 01:51:47.507912 140316961150976 utils.py:1372] Variable decoder/layers_5/pre_mlp_layer_norm/scale                                        size 2048         shape (embed=2048)                             partition spec ('model',)
I0512 01:51:47.507960 140316961150976 utils.py:1372] Variable decoder/layers_5/pre_self_attention_layer_norm/scale                             size 2048         shape (embed=2048)                             partition spec ('model',)
I0512 01:51:47.508009 140316961150976 utils.py:1372] Variable decoder/layers_5/self_attention/key/kernel                                       size 6291456      shape (embed=2048, joined_kv=3072)             partition spec (None, 'model')
I0512 01:51:47.508056 140316961150976 utils.py:1372] Variable decoder/layers_5/self_attention/out/kernel                                       size 6291456      shape (joined_kv=3072, embed=2048)             partition spec ('model', None)
I0512 01:51:47.508100 140316961150976 utils.py:1372] Variable decoder/layers_5/self_attention/query/kernel                                     size 6291456      shape (embed=2048, joined_kv=3072)             partition spec (None, 'model')
I0512 01:51:47.508143 140316961150976 utils.py:1372] Variable decoder/layers_5/self_attention/value/kernel                                     size 6291456      shape (embed=2048, joined_kv=3072)             partition spec (None, 'model')
I0512 01:51:47.508186 140316961150976 utils.py:1372] Variable decoder/layers_6/mlp/wi_0/kernel                                                 size 33554432     shape (mlp_embed=2048, mlp=16384)              partition spec (None, 'model')
I0512 01:51:47.508232 140316961150976 utils.py:1372] Variable decoder/layers_6/mlp/wi_1/kernel                                                 size 16777216     shape (mlp_embed=2048, mlp=8192)               partition spec (None, 'model')
I0512 01:51:47.508275 140316961150976 utils.py:1372] Variable decoder/layers_6/mlp/wo/kernel                                                   size 16777216     shape (mlp=8192, mlp_embed=2048)               partition spec ('model', None)
I0512 01:51:47.508319 140316961150976 utils.py:1372] Variable decoder/layers_6/pre_mlp_layer_norm/scale                                        size 2048         shape (embed=2048)                             partition spec ('model',)
I0512 01:51:47.508362 140316961150976 utils.py:1372] Variable decoder/layers_6/pre_self_attention_layer_norm/scale                             size 2048         shape (embed=2048)                             partition spec ('model',)
I0512 01:51:47.508405 140316961150976 utils.py:1372] Variable decoder/layers_6/self_attention/key/kernel                                       size 6291456      shape (embed=2048, joined_kv=3072)             partition spec (None, 'model')
I0512 01:51:47.508449 140316961150976 utils.py:1372] Variable decoder/layers_6/self_attention/out/kernel                                       size 6291456      shape (joined_kv=3072, embed=2048)             partition spec ('model', None)
I0512 01:51:47.508492 140316961150976 utils.py:1372] Variable decoder/layers_6/self_attention/query/kernel                                     size 6291456      shape (embed=2048, joined_kv=3072)             partition spec (None, 'model')
I0512 01:51:47.508536 140316961150976 utils.py:1372] Variable decoder/layers_6/self_attention/value/kernel                                     size 6291456      shape (embed=2048, joined_kv=3072)             partition spec (None, 'model')
I0512 01:51:47.508579 140316961150976 utils.py:1372] Variable decoder/layers_7/mlp/wi_0/kernel                                                 size 33554432     shape (mlp_embed=2048, mlp=16384)              partition spec (None, 'model')
I0512 01:51:47.508623 140316961150976 utils.py:1372] Variable decoder/layers_7/mlp/wi_1/kernel                                                 size 16777216     shape (mlp_embed=2048, mlp=8192)               partition spec (None, 'model')
I0512 01:51:47.508667 140316961150976 utils.py:1372] Variable decoder/layers_7/mlp/wo/kernel                                                   size 16777216     shape (mlp=8192, mlp_embed=2048)               partition spec ('model', None)
I0512 01:51:47.508711 140316961150976 utils.py:1372] Variable decoder/layers_7/pre_mlp_layer_norm/scale                                        size 2048         shape (embed=2048)                             partition spec ('model',)
I0512 01:51:47.508755 140316961150976 utils.py:1372] Variable decoder/layers_7/pre_self_attention_layer_norm/scale                             size 2048         shape (embed=2048)                             partition spec ('model',)
I0512 01:51:47.508802 140316961150976 utils.py:1372] Variable decoder/layers_7/self_attention/key/kernel                                       size 6291456      shape (embed=2048, joined_kv=3072)             partition spec (None, 'model')
I0512 01:51:47.508846 140316961150976 utils.py:1372] Variable decoder/layers_7/self_attention/out/kernel                                       size 6291456      shape (joined_kv=3072, embed=2048)             partition spec ('model', None)
I0512 01:51:47.508890 140316961150976 utils.py:1372] Variable decoder/layers_7/self_attention/query/kernel                                     size 6291456      shape (embed=2048, joined_kv=3072)             partition spec (None, 'model')
I0512 01:51:47.508933 140316961150976 utils.py:1372] Variable decoder/layers_7/self_attention/value/kernel                                     size 6291456      shape (embed=2048, joined_kv=3072)             partition spec (None, 'model')
I0512 01:51:47.508976 140316961150976 utils.py:1372] Variable decoder/layers_8/mlp/wi_0/kernel                                                 size 33554432     shape (mlp_embed=2048, mlp=16384)              partition spec (None, 'model')
I0512 01:51:47.509027 140316961150976 utils.py:1372] Variable decoder/layers_8/mlp/wi_1/kernel                                                 size 16777216     shape (mlp_embed=2048, mlp=8192)               partition spec (None, 'model')
I0512 01:51:47.509073 140316961150976 utils.py:1372] Variable decoder/layers_8/mlp/wo/kernel                                                   size 16777216     shape (mlp=8192, mlp_embed=2048)               partition spec ('model', None)
I0512 01:51:47.509116 140316961150976 utils.py:1372] Variable decoder/layers_8/pre_mlp_layer_norm/scale                                        size 2048         shape (embed=2048)                             partition spec ('model',)
I0512 01:51:47.509160 140316961150976 utils.py:1372] Variable decoder/layers_8/pre_self_attention_layer_norm/scale                             size 2048         shape (embed=2048)                             partition spec ('model',)
I0512 01:51:47.509203 140316961150976 utils.py:1372] Variable decoder/layers_8/self_attention/key/kernel                                       size 6291456      shape (embed=2048, joined_kv=3072)             partition spec (None, 'model')
I0512 01:51:47.509262 140316961150976 utils.py:1372] Variable decoder/layers_8/self_attention/out/kernel                                       size 6291456      shape (joined_kv=3072, embed=2048)             partition spec ('model', None)
I0512 01:51:47.509309 140316961150976 utils.py:1372] Variable decoder/layers_8/self_attention/query/kernel                                     size 6291456      shape (embed=2048, joined_kv=3072)             partition spec (None, 'model')
I0512 01:51:47.509353 140316961150976 utils.py:1372] Variable decoder/layers_8/self_attention/value/kernel                                     size 6291456      shape (embed=2048, joined_kv=3072)             partition spec (None, 'model')
I0512 01:51:47.509397 140316961150976 utils.py:1372] Variable decoder/layers_9/mlp/wi_0/kernel                                                 size 33554432     shape (mlp_embed=2048, mlp=16384)              partition spec (None, 'model')
I0512 01:51:47.509441 140316961150976 utils.py:1372] Variable decoder/layers_9/mlp/wi_1/kernel                                                 size 16777216     shape (mlp_embed=2048, mlp=8192)               partition spec (None, 'model')
I0512 01:51:47.509484 140316961150976 utils.py:1372] Variable decoder/layers_9/mlp/wo/kernel                                                   size 16777216     shape (mlp=8192, mlp_embed=2048)               partition spec ('model', None)
I0512 01:51:47.509527 140316961150976 utils.py:1372] Variable decoder/layers_9/pre_mlp_layer_norm/scale                                        size 2048         shape (embed=2048)                             partition spec ('model',)
I0512 01:51:47.509604 140316961150976 utils.py:1372] Variable decoder/layers_9/pre_self_attention_layer_norm/scale                             size 2048         shape (embed=2048)                             partition spec ('model',)
I0512 01:51:47.509652 140316961150976 utils.py:1372] Variable decoder/layers_9/self_attention/key/kernel                                       size 6291456      shape (embed=2048, joined_kv=3072)             partition spec (None, 'model')
I0512 01:51:47.509696 140316961150976 utils.py:1372] Variable decoder/layers_9/self_attention/out/kernel                                       size 6291456      shape (joined_kv=3072, embed=2048)             partition spec ('model', None)
I0512 01:51:47.509740 140316961150976 utils.py:1372] Variable decoder/layers_9/self_attention/query/kernel                                     size 6291456      shape (embed=2048, joined_kv=3072)             partition spec (None, 'model')
I0512 01:51:47.509786 140316961150976 utils.py:1372] Variable decoder/layers_9/self_attention/value/kernel                                     size 6291456      shape (embed=2048, joined_kv=3072)             partition spec (None, 'model')
I0512 01:51:47.509831 140316961150976 utils.py:1372] Variable decoder/logits_dense/kernel                                                      size 525074432    shape (embed=2048, vocab=256384)               partition spec (None, 'model')
I0512 01:51:47.509876 140316961150976 utils.py:1372] Variable token_embedder/embedding                                                         size 525074432    shape (vocab=256384, embed=2048)               partition spec ('model', None)
I0512 01:51:47.509986 140316961150976 utils.py:1372] Total number of parameters: 5412399104
I0512 01:51:47.510101 140316961150976 utils.py:1372] 
I0512 01:51:47.514605 140316961150976 utils.py:1372] Variable param_states/decoder/decoder_norm/scale/m                                        size 1            shape (1,)                                     partition spec None
I0512 01:51:47.514740 140316961150976 utils.py:1372] Variable param_states/decoder/decoder_norm/scale/v                                        size 2048         shape (embed=2048)                             partition spec ('model',)
I0512 01:51:47.514792 140316961150976 utils.py:1372] Variable param_states/decoder/decoder_norm/scale/v_col                                    size 1            shape (1,)                                     partition spec None
I0512 01:51:47.514836 140316961150976 utils.py:1372] Variable param_states/decoder/decoder_norm/scale/v_row                                    size 1            shape (1,)                                     partition spec None
I0512 01:51:47.514876 140316961150976 utils.py:1372] Variable param_states/decoder/layers_0/mlp/wi_0/kernel/m                                  size 1            shape (1,)                                     partition spec None
I0512 01:51:47.514918 140316961150976 utils.py:1372] Variable param_states/decoder/layers_0/mlp/wi_0/kernel/v                                  size 1            shape (1,)                                     partition spec None
I0512 01:51:47.514968 140316961150976 utils.py:1372] Variable param_states/decoder/layers_0/mlp/wi_0/kernel/v_col                              size 16384        shape (16384,)                                 partition spec None
I0512 01:51:47.515018 140316961150976 utils.py:1372] Variable param_states/decoder/layers_0/mlp/wi_0/kernel/v_row                              size 2048         shape (2048,)                                  partition spec None
I0512 01:51:47.515060 140316961150976 utils.py:1372] Variable param_states/decoder/layers_0/mlp/wi_1/kernel/m                                  size 1            shape (1,)                                     partition spec None
I0512 01:51:47.515101 140316961150976 utils.py:1372] Variable param_states/decoder/layers_0/mlp/wi_1/kernel/v                                  size 1            shape (1,)                                     partition spec None
I0512 01:51:47.515141 140316961150976 utils.py:1372] Variable param_states/decoder/layers_0/mlp/wi_1/kernel/v_col                              size 8192         shape (8192,)                                  partition spec None
I0512 01:51:47.515181 140316961150976 utils.py:1372] Variable param_states/decoder/layers_0/mlp/wi_1/kernel/v_row                              size 2048         shape (2048,)                                  partition spec None
I0512 01:51:47.515220 140316961150976 utils.py:1372] Variable param_states/decoder/layers_0/mlp/wo/kernel/m                                    size 1            shape (1,)                                     partition spec None
I0512 01:51:47.515260 140316961150976 utils.py:1372] Variable param_states/decoder/layers_0/mlp/wo/kernel/v                                    size 1            shape (1,)                                     partition spec None
I0512 01:51:47.515309 140316961150976 utils.py:1372] Variable param_states/decoder/layers_0/mlp/wo/kernel/v_col                                size 8192         shape (8192,)                                  partition spec None
I0512 01:51:47.515356 140316961150976 utils.py:1372] Variable param_states/decoder/layers_0/mlp/wo/kernel/v_row                                size 2048         shape (2048,)                                  partition spec None
I0512 01:51:47.515397 140316961150976 utils.py:1372] Variable param_states/decoder/layers_0/pre_mlp_layer_norm/scale/m                         size 1            shape (1,)                                     partition spec None
I0512 01:51:47.515440 140316961150976 utils.py:1372] Variable param_states/decoder/layers_0/pre_mlp_layer_norm/scale/v                         size 2048         shape (embed=2048)                             partition spec ('model',)
I0512 01:51:47.515481 140316961150976 utils.py:1372] Variable param_states/decoder/layers_0/pre_mlp_layer_norm/scale/v_col                     size 1            shape (1,)                                     partition spec None
I0512 01:51:47.515521 140316961150976 utils.py:1372] Variable param_states/decoder/layers_0/pre_mlp_layer_norm/scale/v_row                     size 1            shape (1,)                                     partition spec None
I0512 01:51:47.515560 140316961150976 utils.py:1372] Variable param_states/decoder/layers_0/pre_self_attention_layer_norm/scale/m              size 1            shape (1,)                                     partition spec None
I0512 01:51:47.515602 140316961150976 utils.py:1372] Variable param_states/decoder/layers_0/pre_self_attention_layer_norm/scale/v              size 2048         shape (embed=2048)                             partition spec ('model',)
I0512 01:51:47.515643 140316961150976 utils.py:1372] Variable param_states/decoder/layers_0/pre_self_attention_layer_norm/scale/v_col          size 1            shape (1,)                                     partition spec None
I0512 01:51:47.515682 140316961150976 utils.py:1372] Variable param_states/decoder/layers_0/pre_self_attention_layer_norm/scale/v_row          size 1            shape (1,)                                     partition spec None
I0512 01:51:47.515721 140316961150976 utils.py:1372] Variable param_states/decoder/layers_0/self_attention/key/kernel/m                        size 1            shape (1,)                                     partition spec None
I0512 01:51:47.515761 140316961150976 utils.py:1372] Variable param_states/decoder/layers_0/self_attention/key/kernel/v                        size 1            shape (1,)                                     partition spec None
I0512 01:51:47.515811 140316961150976 utils.py:1372] Variable param_states/decoder/layers_0/self_attention/key/kernel/v_col                    size 3072         shape (3072,)                                  partition spec None
I0512 01:51:47.515860 140316961150976 utils.py:1372] Variable param_states/decoder/layers_0/self_attention/key/kernel/v_row                    size 2048         shape (2048,)                                  partition spec None
I0512 01:51:47.515902 140316961150976 utils.py:1372] Variable param_states/decoder/layers_0/self_attention/out/kernel/m                        size 1            shape (1,)                                     partition spec None
I0512 01:51:47.515941 140316961150976 utils.py:1372] Variable param_states/decoder/layers_0/self_attention/out/kernel/v                        size 1            shape (1,)                                     partition spec None
I0512 01:51:47.516028 140316961150976 utils.py:1372] Variable param_states/decoder/layers_0/self_attention/out/kernel/v_col                    size 3072         shape (3072,)                                  partition spec None
I0512 01:51:47.516080 140316961150976 utils.py:1372] Variable param_states/decoder/layers_0/self_attention/out/kernel/v_row                    size 2048         shape (2048,)                                  partition spec None
I0512 01:51:47.516159 140316961150976 utils.py:1372] Variable param_states/decoder/layers_0/self_attention/query/kernel/m                      size 1            shape (1,)                                     partition spec None
I0512 01:51:47.516204 140316961150976 utils.py:1372] Variable param_states/decoder/layers_0/self_attention/query/kernel/v                      size 1            shape (1,)                                     partition spec None
I0512 01:51:47.516245 140316961150976 utils.py:1372] Variable param_states/decoder/layers_0/self_attention/query/kernel/v_col                  size 3072         shape (3072,)                                  partition spec None
I0512 01:51:47.516284 140316961150976 utils.py:1372] Variable param_states/decoder/layers_0/self_attention/query/kernel/v_row                  size 2048         shape (2048,)                                  partition spec None
I0512 01:51:47.516324 140316961150976 utils.py:1372] Variable param_states/decoder/layers_0/self_attention/value/kernel/m                      size 1            shape (1,)                                     partition spec None
I0512 01:51:47.516363 140316961150976 utils.py:1372] Variable param_states/decoder/layers_0/self_attention/value/kernel/v                      size 1            shape (1,)                                     partition spec None
I0512 01:51:47.516411 140316961150976 utils.py:1372] Variable param_states/decoder/layers_0/self_attention/value/kernel/v_col                  size 3072         shape (3072,)                                  partition spec None
I0512 01:51:47.516452 140316961150976 utils.py:1372] Variable param_states/decoder/layers_0/self_attention/value/kernel/v_row                  size 2048         shape (2048,)                                  partition spec None
I0512 01:51:47.516492 140316961150976 utils.py:1372] Variable param_states/decoder/layers_1/mlp/wi_0/kernel/m                                  size 1            shape (1,)                                     partition spec None
I0512 01:51:47.516532 140316961150976 utils.py:1372] Variable param_states/decoder/layers_1/mlp/wi_0/kernel/v                                  size 1            shape (1,)                                     partition spec None
I0512 01:51:47.516579 140316961150976 utils.py:1372] Variable param_states/decoder/layers_1/mlp/wi_0/kernel/v_col                              size 16384        shape (16384,)                                 partition spec None
I0512 01:51:47.516621 140316961150976 utils.py:1372] Variable param_states/decoder/layers_1/mlp/wi_0/kernel/v_row                              size 2048         shape (2048,)                                  partition spec None
I0512 01:51:47.516660 140316961150976 utils.py:1372] Variable param_states/decoder/layers_1/mlp/wi_1/kernel/m                                  size 1            shape (1,)                                     partition spec None
I0512 01:51:47.516699 140316961150976 utils.py:1372] Variable param_states/decoder/layers_1/mlp/wi_1/kernel/v                                  size 1            shape (1,)                                     partition spec None
I0512 01:51:47.516739 140316961150976 utils.py:1372] Variable param_states/decoder/layers_1/mlp/wi_1/kernel/v_col                              size 8192         shape (8192,)                                  partition spec None
I0512 01:51:47.516781 140316961150976 utils.py:1372] Variable param_states/decoder/layers_1/mlp/wi_1/kernel/v_row                              size 2048         shape (2048,)                                  partition spec None
I0512 01:51:47.516821 140316961150976 utils.py:1372] Variable param_states/decoder/layers_1/mlp/wo/kernel/m                                    size 1            shape (1,)                                     partition spec None
I0512 01:51:47.516860 140316961150976 utils.py:1372] Variable param_states/decoder/layers_1/mlp/wo/kernel/v                                    size 1            shape (1,)                                     partition spec None
I0512 01:51:47.516900 140316961150976 utils.py:1372] Variable param_states/decoder/layers_1/mlp/wo/kernel/v_col                                size 8192         shape (8192,)                                  partition spec None
I0512 01:51:47.516941 140316961150976 utils.py:1372] Variable param_states/decoder/layers_1/mlp/wo/kernel/v_row                                size 2048         shape (2048,)                                  partition spec None
I0512 01:51:47.516990 140316961150976 utils.py:1372] Variable param_states/decoder/layers_1/pre_mlp_layer_norm/scale/m                         size 1            shape (1,)                                     partition spec None
I0512 01:51:47.517042 140316961150976 utils.py:1372] Variable param_states/decoder/layers_1/pre_mlp_layer_norm/scale/v                         size 2048         shape (embed=2048)                             partition spec ('model',)
I0512 01:51:47.517085 140316961150976 utils.py:1372] Variable param_states/decoder/layers_1/pre_mlp_layer_norm/scale/v_col                     size 1            shape (1,)                                     partition spec None
I0512 01:51:47.517125 140316961150976 utils.py:1372] Variable param_states/decoder/layers_1/pre_mlp_layer_norm/scale/v_row                     size 1            shape (1,)                                     partition spec None
I0512 01:51:47.517164 140316961150976 utils.py:1372] Variable param_states/decoder/layers_1/pre_self_attention_layer_norm/scale/m              size 1            shape (1,)                                     partition spec None
I0512 01:51:47.517205 140316961150976 utils.py:1372] Variable param_states/decoder/layers_1/pre_self_attention_layer_norm/scale/v              size 2048         shape (embed=2048)                             partition spec ('model',)
I0512 01:51:47.517246 140316961150976 utils.py:1372] Variable param_states/decoder/layers_1/pre_self_attention_layer_norm/scale/v_col          size 1            shape (1,)                                     partition spec None
I0512 01:51:47.517285 140316961150976 utils.py:1372] Variable param_states/decoder/layers_1/pre_self_attention_layer_norm/scale/v_row          size 1            shape (1,)                                     partition spec None
I0512 01:51:47.517324 140316961150976 utils.py:1372] Variable param_states/decoder/layers_1/self_attention/key/kernel/m                        size 1            shape (1,)                                     partition spec None
I0512 01:51:47.517363 140316961150976 utils.py:1372] Variable param_states/decoder/layers_1/self_attention/key/kernel/v                        size 1            shape (1,)                                     partition spec None
I0512 01:51:47.517415 140316961150976 utils.py:1372] Variable param_states/decoder/layers_1/self_attention/key/kernel/v_col                    size 3072         shape (3072,)                                  partition spec None
I0512 01:51:47.517457 140316961150976 utils.py:1372] Variable param_states/decoder/layers_1/self_attention/key/kernel/v_row                    size 2048         shape (2048,)                                  partition spec None
I0512 01:51:47.517496 140316961150976 utils.py:1372] Variable param_states/decoder/layers_1/self_attention/out/kernel/m                        size 1            shape (1,)                                     partition spec None
I0512 01:51:47.517535 140316961150976 utils.py:1372] Variable param_states/decoder/layers_1/self_attention/out/kernel/v                        size 1            shape (1,)                                     partition spec None
I0512 01:51:47.517616 140316961150976 utils.py:1372] Variable param_states/decoder/layers_1/self_attention/out/kernel/v_col                    size 3072         shape (3072,)                                  partition spec None
I0512 01:51:47.517659 140316961150976 utils.py:1372] Variable param_states/decoder/layers_1/self_attention/out/kernel/v_row                    size 2048         shape (2048,)                                  partition spec None
I0512 01:51:47.517699 140316961150976 utils.py:1372] Variable param_states/decoder/layers_1/self_attention/query/kernel/m                      size 1            shape (1,)                                     partition spec None
I0512 01:51:47.517738 140316961150976 utils.py:1372] Variable param_states/decoder/layers_1/self_attention/query/kernel/v                      size 1            shape (1,)                                     partition spec None
I0512 01:51:47.517780 140316961150976 utils.py:1372] Variable param_states/decoder/layers_1/self_attention/query/kernel/v_col                  size 3072         shape (3072,)                                  partition spec None
I0512 01:51:47.517820 140316961150976 utils.py:1372] Variable param_states/decoder/layers_1/self_attention/query/kernel/v_row                  size 2048         shape (2048,)                                  partition spec None
I0512 01:51:47.517858 140316961150976 utils.py:1372] Variable param_states/decoder/layers_1/self_attention/value/kernel/m                      size 1            shape (1,)                                     partition spec None
I0512 01:51:47.517898 140316961150976 utils.py:1372] Variable param_states/decoder/layers_1/self_attention/value/kernel/v                      size 1            shape (1,)                                     partition spec None
I0512 01:51:47.517937 140316961150976 utils.py:1372] Variable param_states/decoder/layers_1/self_attention/value/kernel/v_col                  size 3072         shape (3072,)                                  partition spec None
I0512 01:51:47.517976 140316961150976 utils.py:1372] Variable param_states/decoder/layers_1/self_attention/value/kernel/v_row                  size 2048         shape (2048,)                                  partition spec None
I0512 01:51:47.518021 140316961150976 utils.py:1372] Variable param_states/decoder/layers_10/mlp/wi_0/kernel/m                                 size 1            shape (1,)                                     partition spec None
I0512 01:51:47.518063 140316961150976 utils.py:1372] Variable param_states/decoder/layers_10/mlp/wi_0/kernel/v                                 size 1            shape (1,)                                     partition spec None
I0512 01:51:47.518111 140316961150976 utils.py:1372] Variable param_states/decoder/layers_10/mlp/wi_0/kernel/v_col                             size 16384        shape (16384,)                                 partition spec None
I0512 01:51:47.518170 140316961150976 utils.py:1372] Variable param_states/decoder/layers_10/mlp/wi_0/kernel/v_row                             size 2048         shape (2048,)                                  partition spec None
I0512 01:51:47.518213 140316961150976 utils.py:1372] Variable param_states/decoder/layers_10/mlp/wi_1/kernel/m                                 size 1            shape (1,)                                     partition spec None
I0512 01:51:47.518254 140316961150976 utils.py:1372] Variable param_states/decoder/layers_10/mlp/wi_1/kernel/v                                 size 1            shape (1,)                                     partition spec None
I0512 01:51:47.518293 140316961150976 utils.py:1372] Variable param_states/decoder/layers_10/mlp/wi_1/kernel/v_col                             size 8192         shape (8192,)                                  partition spec None
I0512 01:51:47.518332 140316961150976 utils.py:1372] Variable param_states/decoder/layers_10/mlp/wi_1/kernel/v_row                             size 2048         shape (2048,)                                  partition spec None
I0512 01:51:47.518371 140316961150976 utils.py:1372] Variable param_states/decoder/layers_10/mlp/wo/kernel/m                                   size 1            shape (1,)                                     partition spec None
I0512 01:51:47.518409 140316961150976 utils.py:1372] Variable param_states/decoder/layers_10/mlp/wo/kernel/v                                   size 1            shape (1,)                                     partition spec None
I0512 01:51:47.518458 140316961150976 utils.py:1372] Variable param_states/decoder/layers_10/mlp/wo/kernel/v_col                               size 8192         shape (8192,)                                  partition spec None
I0512 01:51:47.518507 140316961150976 utils.py:1372] Variable param_states/decoder/layers_10/mlp/wo/kernel/v_row                               size 2048         shape (2048,)                                  partition spec None
I0512 01:51:47.518548 140316961150976 utils.py:1372] Variable param_states/decoder/layers_10/pre_mlp_layer_norm/scale/m                        size 1            shape (1,)                                     partition spec None
I0512 01:51:47.518591 140316961150976 utils.py:1372] Variable param_states/decoder/layers_10/pre_mlp_layer_norm/scale/v                        size 2048         shape (embed=2048)                             partition spec ('model',)
I0512 01:51:47.518632 140316961150976 utils.py:1372] Variable param_states/decoder/layers_10/pre_mlp_layer_norm/scale/v_col                    size 1            shape (1,)                                     partition spec None
I0512 01:51:47.518671 140316961150976 utils.py:1372] Variable param_states/decoder/layers_10/pre_mlp_layer_norm/scale/v_row                    size 1            shape (1,)                                     partition spec None
I0512 01:51:47.518710 140316961150976 utils.py:1372] Variable param_states/decoder/layers_10/pre_self_attention_layer_norm/scale/m             size 1            shape (1,)                                     partition spec None
I0512 01:51:47.518752 140316961150976 utils.py:1372] Variable param_states/decoder/layers_10/pre_self_attention_layer_norm/scale/v             size 2048         shape (embed=2048)                             partition spec ('model',)
I0512 01:51:47.518796 140316961150976 utils.py:1372] Variable param_states/decoder/layers_10/pre_self_attention_layer_norm/scale/v_col         size 1            shape (1,)                                     partition spec None
I0512 01:51:47.518836 140316961150976 utils.py:1372] Variable param_states/decoder/layers_10/pre_self_attention_layer_norm/scale/v_row         size 1            shape (1,)                                     partition spec None
I0512 01:51:47.518875 140316961150976 utils.py:1372] Variable param_states/decoder/layers_10/self_attention/key/kernel/m                       size 1            shape (1,)                                     partition spec None
I0512 01:51:47.518914 140316961150976 utils.py:1372] Variable param_states/decoder/layers_10/self_attention/key/kernel/v                       size 1            shape (1,)                                     partition spec None
I0512 01:51:47.518962 140316961150976 utils.py:1372] Variable param_states/decoder/layers_10/self_attention/key/kernel/v_col                   size 3072         shape (3072,)                                  partition spec None
I0512 01:51:47.519015 140316961150976 utils.py:1372] Variable param_states/decoder/layers_10/self_attention/key/kernel/v_row                   size 2048         shape (2048,)                                  partition spec None
I0512 01:51:47.519058 140316961150976 utils.py:1372] Variable param_states/decoder/layers_10/self_attention/out/kernel/m                       size 1            shape (1,)                                     partition spec None
I0512 01:51:47.519098 140316961150976 utils.py:1372] Variable param_states/decoder/layers_10/self_attention/out/kernel/v                       size 1            shape (1,)                                     partition spec None
I0512 01:51:47.519145 140316961150976 utils.py:1372] Variable param_states/decoder/layers_10/self_attention/out/kernel/v_col                   size 3072         shape (3072,)                                  partition spec None
I0512 01:51:47.519186 140316961150976 utils.py:1372] Variable param_states/decoder/layers_10/self_attention/out/kernel/v_row                   size 2048         shape (2048,)                                  partition spec None
I0512 01:51:47.519225 140316961150976 utils.py:1372] Variable param_states/decoder/layers_10/self_attention/query/kernel/m                     size 1            shape (1,)                                     partition spec None
I0512 01:51:47.519264 140316961150976 utils.py:1372] Variable param_states/decoder/layers_10/self_attention/query/kernel/v                     size 1            shape (1,)                                     partition spec None
I0512 01:51:47.519304 140316961150976 utils.py:1372] Variable param_states/decoder/layers_10/self_attention/query/kernel/v_col                 size 3072         shape (3072,)                                  partition spec None
I0512 01:51:47.519343 140316961150976 utils.py:1372] Variable param_states/decoder/layers_10/self_attention/query/kernel/v_row                 size 2048         shape (2048,)                                  partition spec None
I0512 01:51:47.519381 140316961150976 utils.py:1372] Variable param_states/decoder/layers_10/self_attention/value/kernel/m                     size 1            shape (1,)                                     partition spec None
I0512 01:51:47.519421 140316961150976 utils.py:1372] Variable param_states/decoder/layers_10/self_attention/value/kernel/v                     size 1            shape (1,)                                     partition spec None
I0512 01:51:47.519460 140316961150976 utils.py:1372] Variable param_states/decoder/layers_10/self_attention/value/kernel/v_col                 size 3072         shape (3072,)                                  partition spec None
I0512 01:51:47.519499 140316961150976 utils.py:1372] Variable param_states/decoder/layers_10/self_attention/value/kernel/v_row                 size 2048         shape (2048,)                                  partition spec None
I0512 01:51:47.519537 140316961150976 utils.py:1372] Variable param_states/decoder/layers_11/extra_mlp/wi_0/kernel/m                           size 1            shape (1,)                                     partition spec None
I0512 01:51:47.519577 140316961150976 utils.py:1372] Variable param_states/decoder/layers_11/extra_mlp/wi_0/kernel/v                           size 1            shape (1,)                                     partition spec None
I0512 01:51:47.519617 140316961150976 utils.py:1372] Variable param_states/decoder/layers_11/extra_mlp/wi_0/kernel/v_col                       size 16384        shape (16384,)                                 partition spec None
I0512 01:51:47.519656 140316961150976 utils.py:1372] Variable param_states/decoder/layers_11/extra_mlp/wi_0/kernel/v_row                       size 2048         shape (2048,)                                  partition spec None
I0512 01:51:47.519695 140316961150976 utils.py:1372] Variable param_states/decoder/layers_11/extra_mlp/wi_1/kernel/m                           size 1            shape (1,)                                     partition spec None
I0512 01:51:47.519735 140316961150976 utils.py:1372] Variable param_states/decoder/layers_11/extra_mlp/wi_1/kernel/v                           size 1            shape (1,)                                     partition spec None
I0512 01:51:47.519778 140316961150976 utils.py:1372] Variable param_states/decoder/layers_11/extra_mlp/wi_1/kernel/v_col                       size 8192         shape (8192,)                                  partition spec None
I0512 01:51:47.519819 140316961150976 utils.py:1372] Variable param_states/decoder/layers_11/extra_mlp/wi_1/kernel/v_row                       size 2048         shape (2048,)                                  partition spec None
I0512 01:51:47.519859 140316961150976 utils.py:1372] Variable param_states/decoder/layers_11/extra_mlp/wo/kernel/m                             size 1            shape (1,)                                     partition spec None
I0512 01:51:47.519899 140316961150976 utils.py:1372] Variable param_states/decoder/layers_11/extra_mlp/wo/kernel/v                             size 1            shape (1,)                                     partition spec None
I0512 01:51:47.519941 140316961150976 utils.py:1372] Variable param_states/decoder/layers_11/extra_mlp/wo/kernel/v_col                         size 8192         shape (8192,)                                  partition spec None
I0512 01:51:47.519983 140316961150976 utils.py:1372] Variable param_states/decoder/layers_11/extra_mlp/wo/kernel/v_row                         size 2048         shape (2048,)                                  partition spec None
I0512 01:51:47.520029 140316961150976 utils.py:1372] Variable param_states/decoder/layers_11/mlp/expert/wi_0/kernel/m                          size 1            shape (1,)                                     partition spec None
I0512 01:51:47.520071 140316961150976 utils.py:1372] Variable param_states/decoder/layers_11/mlp/expert/wi_0/kernel/v                          size 1            shape (1,)                                     partition spec None
I0512 01:51:47.520136 140316961150976 utils.py:1372] Variable param_states/decoder/layers_11/mlp/expert/wi_0/kernel/v_col                      size 131072       shape (8, 16384)                               partition spec ('expert',)
I0512 01:51:47.520183 140316961150976 utils.py:1372] Variable param_states/decoder/layers_11/mlp/expert/wi_0/kernel/v_row                      size 16384        shape (8, 2048)                                partition spec ('expert',)
I0512 01:51:47.520224 140316961150976 utils.py:1372] Variable param_states/decoder/layers_11/mlp/expert/wi_1/kernel/m                          size 1            shape (1,)                                     partition spec None
I0512 01:51:47.520264 140316961150976 utils.py:1372] Variable param_states/decoder/layers_11/mlp/expert/wi_1/kernel/v                          size 1            shape (1,)                                     partition spec None
I0512 01:51:47.520304 140316961150976 utils.py:1372] Variable param_states/decoder/layers_11/mlp/expert/wi_1/kernel/v_col                      size 65536        shape (8, 8192)                                partition spec ('expert',)
I0512 01:51:47.520345 140316961150976 utils.py:1372] Variable param_states/decoder/layers_11/mlp/expert/wi_1/kernel/v_row                      size 16384        shape (8, 2048)                                partition spec ('expert',)
I0512 01:51:47.520385 140316961150976 utils.py:1372] Variable param_states/decoder/layers_11/mlp/expert/wo/kernel/m                            size 1            shape (1,)                                     partition spec None
I0512 01:51:47.520425 140316961150976 utils.py:1372] Variable param_states/decoder/layers_11/mlp/expert/wo/kernel/v                            size 1            shape (1,)                                     partition spec None
I0512 01:51:47.520464 140316961150976 utils.py:1372] Variable param_states/decoder/layers_11/mlp/expert/wo/kernel/v_col                        size 65536        shape (8, 8192)                                partition spec ('expert',)
I0512 01:51:47.520505 140316961150976 utils.py:1372] Variable param_states/decoder/layers_11/mlp/expert/wo/kernel/v_row                        size 16384        shape (8, 2048)                                partition spec ('expert',)
I0512 01:51:47.520545 140316961150976 utils.py:1372] Variable param_states/decoder/layers_11/mlp/router/router_weights/w/kernel/m              size 1            shape (1,)                                     partition spec None
I0512 01:51:47.520588 140316961150976 utils.py:1372] Variable param_states/decoder/layers_11/mlp/router/router_weights/w/kernel/v              size 16384        shape (embed=2048, unmodeled=8)                partition spec ('model', None)
I0512 01:51:47.520630 140316961150976 utils.py:1372] Variable param_states/decoder/layers_11/mlp/router/router_weights/w/kernel/v_col          size 1            shape (1,)                                     partition spec None
I0512 01:51:47.520669 140316961150976 utils.py:1372] Variable param_states/decoder/layers_11/mlp/router/router_weights/w/kernel/v_row          size 1            shape (1,)                                     partition spec None
I0512 01:51:47.520709 140316961150976 utils.py:1372] Variable param_states/decoder/layers_11/pre_extra_mlp_layer_norm/scale/m                  size 1            shape (1,)                                     partition spec None
I0512 01:51:47.520750 140316961150976 utils.py:1372] Variable param_states/decoder/layers_11/pre_extra_mlp_layer_norm/scale/v                  size 2048         shape (embed=2048)                             partition spec ('model',)
I0512 01:51:47.520795 140316961150976 utils.py:1372] Variable param_states/decoder/layers_11/pre_extra_mlp_layer_norm/scale/v_col              size 1            shape (1,)                                     partition spec None
I0512 01:51:47.520835 140316961150976 utils.py:1372] Variable param_states/decoder/layers_11/pre_extra_mlp_layer_norm/scale/v_row              size 1            shape (1,)                                     partition spec None
I0512 01:51:47.520874 140316961150976 utils.py:1372] Variable param_states/decoder/layers_11/pre_mlp_layer_norm/scale/m                        size 1            shape (1,)                                     partition spec None
I0512 01:51:47.520915 140316961150976 utils.py:1372] Variable param_states/decoder/layers_11/pre_mlp_layer_norm/scale/v                        size 2048         shape (embed=2048)                             partition spec ('model',)
I0512 01:51:47.520956 140316961150976 utils.py:1372] Variable param_states/decoder/layers_11/pre_mlp_layer_norm/scale/v_col                    size 1            shape (1,)                                     partition spec None
I0512 01:51:47.520995 140316961150976 utils.py:1372] Variable param_states/decoder/layers_11/pre_mlp_layer_norm/scale/v_row                    size 1            shape (1,)                                     partition spec None
I0512 01:51:47.521042 140316961150976 utils.py:1372] Variable param_states/decoder/layers_11/pre_self_attention_layer_norm/scale/m             size 1            shape (1,)                                     partition spec None
I0512 01:51:47.521084 140316961150976 utils.py:1372] Variable param_states/decoder/layers_11/pre_self_attention_layer_norm/scale/v             size 2048         shape (embed=2048)                             partition spec ('model',)
I0512 01:51:47.521125 140316961150976 utils.py:1372] Variable param_states/decoder/layers_11/pre_self_attention_layer_norm/scale/v_col         size 1            shape (1,)                                     partition spec None
I0512 01:51:47.521164 140316961150976 utils.py:1372] Variable param_states/decoder/layers_11/pre_self_attention_layer_norm/scale/v_row         size 1            shape (1,)                                     partition spec None
I0512 01:51:47.521203 140316961150976 utils.py:1372] Variable param_states/decoder/layers_11/self_attention/key/kernel/m                       size 1            shape (1,)                                     partition spec None
I0512 01:51:47.521242 140316961150976 utils.py:1372] Variable param_states/decoder/layers_11/self_attention/key/kernel/v                       size 1            shape (1,)                                     partition spec None
I0512 01:51:47.521296 140316961150976 utils.py:1372] Variable param_states/decoder/layers_11/self_attention/key/kernel/v_col                   size 3072         shape (3072,)                                  partition spec None
I0512 01:51:47.521338 140316961150976 utils.py:1372] Variable param_states/decoder/layers_11/self_attention/key/kernel/v_row                   size 2048         shape (2048,)                                  partition spec None
I0512 01:51:47.521378 140316961150976 utils.py:1372] Variable param_states/decoder/layers_11/self_attention/out/kernel/m                       size 1            shape (1,)                                     partition spec None
I0512 01:51:47.521417 140316961150976 utils.py:1372] Variable param_states/decoder/layers_11/self_attention/out/kernel/v                       size 1            shape (1,)                                     partition spec None
I0512 01:51:47.521457 140316961150976 utils.py:1372] Variable param_states/decoder/layers_11/self_attention/out/kernel/v_col                   size 3072         shape (3072,)                                  partition spec None
I0512 01:51:47.521496 140316961150976 utils.py:1372] Variable param_states/decoder/layers_11/self_attention/out/kernel/v_row                   size 2048         shape (2048,)                                  partition spec None
I0512 01:51:47.521535 140316961150976 utils.py:1372] Variable param_states/decoder/layers_11/self_attention/query/kernel/m                     size 1            shape (1,)                                     partition spec None
I0512 01:51:47.521606 140316961150976 utils.py:1372] Variable param_states/decoder/layers_11/self_attention/query/kernel/v                     size 1            shape (1,)                                     partition spec None
I0512 01:51:47.521649 140316961150976 utils.py:1372] Variable param_states/decoder/layers_11/self_attention/query/kernel/v_col                 size 3072         shape (3072,)                                  partition spec None
I0512 01:51:47.521688 140316961150976 utils.py:1372] Variable param_states/decoder/layers_11/self_attention/query/kernel/v_row                 size 2048         shape (2048,)                                  partition spec None
I0512 01:51:47.521728 140316961150976 utils.py:1372] Variable param_states/decoder/layers_11/self_attention/value/kernel/m                     size 1            shape (1,)                                     partition spec None
I0512 01:51:47.521769 140316961150976 utils.py:1372] Variable param_states/decoder/layers_11/self_attention/value/kernel/v                     size 1            shape (1,)                                     partition spec None
I0512 01:51:47.521810 140316961150976 utils.py:1372] Variable param_states/decoder/layers_11/self_attention/value/kernel/v_col                 size 3072         shape (3072,)                                  partition spec None
I0512 01:51:47.521849 140316961150976 utils.py:1372] Variable param_states/decoder/layers_11/self_attention/value/kernel/v_row                 size 2048         shape (2048,)                                  partition spec None
I0512 01:51:47.521888 140316961150976 utils.py:1372] Variable param_states/decoder/layers_12/mlp/wi_0/kernel/m                                 size 1            shape (1,)                                     partition spec None
I0512 01:51:47.521929 140316961150976 utils.py:1372] Variable param_states/decoder/layers_12/mlp/wi_0/kernel/v                                 size 1            shape (1,)                                     partition spec None
I0512 01:51:47.521968 140316961150976 utils.py:1372] Variable param_states/decoder/layers_12/mlp/wi_0/kernel/v_col                             size 16384        shape (16384,)                                 partition spec None
I0512 01:51:47.522013 140316961150976 utils.py:1372] Variable param_states/decoder/layers_12/mlp/wi_0/kernel/v_row                             size 2048         shape (2048,)                                  partition spec None
I0512 01:51:47.522054 140316961150976 utils.py:1372] Variable param_states/decoder/layers_12/mlp/wi_1/kernel/m                                 size 1            shape (1,)                                     partition spec None
I0512 01:51:47.522110 140316961150976 utils.py:1372] Variable param_states/decoder/layers_12/mlp/wi_1/kernel/v                                 size 1            shape (1,)                                     partition spec None
I0512 01:51:47.522154 140316961150976 utils.py:1372] Variable param_states/decoder/layers_12/mlp/wi_1/kernel/v_col                             size 8192         shape (8192,)                                  partition spec None
I0512 01:51:47.522194 140316961150976 utils.py:1372] Variable param_states/decoder/layers_12/mlp/wi_1/kernel/v_row                             size 2048         shape (2048,)                                  partition spec None
I0512 01:51:47.522233 140316961150976 utils.py:1372] Variable param_states/decoder/layers_12/mlp/wo/kernel/m                                   size 1            shape (1,)                                     partition spec None
I0512 01:51:47.522272 140316961150976 utils.py:1372] Variable param_states/decoder/layers_12/mlp/wo/kernel/v                                   size 1            shape (1,)                                     partition spec None
I0512 01:51:47.522322 140316961150976 utils.py:1372] Variable param_states/decoder/layers_12/mlp/wo/kernel/v_col                               size 8192         shape (8192,)                                  partition spec None
I0512 01:51:47.522363 140316961150976 utils.py:1372] Variable param_states/decoder/layers_12/mlp/wo/kernel/v_row                               size 2048         shape (2048,)                                  partition spec None
I0512 01:51:47.522403 140316961150976 utils.py:1372] Variable param_states/decoder/layers_12/pre_mlp_layer_norm/scale/m                        size 1            shape (1,)                                     partition spec None
I0512 01:51:47.522445 140316961150976 utils.py:1372] Variable param_states/decoder/layers_12/pre_mlp_layer_norm/scale/v                        size 2048         shape (embed=2048)                             partition spec ('model',)
I0512 01:51:47.522486 140316961150976 utils.py:1372] Variable param_states/decoder/layers_12/pre_mlp_layer_norm/scale/v_col                    size 1            shape (1,)                                     partition spec None
I0512 01:51:47.522527 140316961150976 utils.py:1372] Variable param_states/decoder/layers_12/pre_mlp_layer_norm/scale/v_row                    size 1            shape (1,)                                     partition spec None
I0512 01:51:47.522566 140316961150976 utils.py:1372] Variable param_states/decoder/layers_12/pre_self_attention_layer_norm/scale/m             size 1            shape (1,)                                     partition spec None
I0512 01:51:47.522608 140316961150976 utils.py:1372] Variable param_states/decoder/layers_12/pre_self_attention_layer_norm/scale/v             size 2048         shape (embed=2048)                             partition spec ('model',)
I0512 01:51:47.522649 140316961150976 utils.py:1372] Variable param_states/decoder/layers_12/pre_self_attention_layer_norm/scale/v_col         size 1            shape (1,)                                     partition spec None
I0512 01:51:47.522689 140316961150976 utils.py:1372] Variable param_states/decoder/layers_12/pre_self_attention_layer_norm/scale/v_row         size 1            shape (1,)                                     partition spec None
I0512 01:51:47.522728 140316961150976 utils.py:1372] Variable param_states/decoder/layers_12/self_attention/key/kernel/m                       size 1            shape (1,)                                     partition spec None
I0512 01:51:47.522769 140316961150976 utils.py:1372] Variable param_states/decoder/layers_12/self_attention/key/kernel/v                       size 1            shape (1,)                                     partition spec None
I0512 01:51:47.522819 140316961150976 utils.py:1372] Variable param_states/decoder/layers_12/self_attention/key/kernel/v_col                   size 3072         shape (3072,)                                  partition spec None
I0512 01:51:47.522862 140316961150976 utils.py:1372] Variable param_states/decoder/layers_12/self_attention/key/kernel/v_row                   size 2048         shape (2048,)                                  partition spec None
I0512 01:51:47.522901 140316961150976 utils.py:1372] Variable param_states/decoder/layers_12/self_attention/out/kernel/m                       size 1            shape (1,)                                     partition spec None
I0512 01:51:47.522941 140316961150976 utils.py:1372] Variable param_states/decoder/layers_12/self_attention/out/kernel/v                       size 1            shape (1,)                                     partition spec None
I0512 01:51:47.522980 140316961150976 utils.py:1372] Variable param_states/decoder/layers_12/self_attention/out/kernel/v_col                   size 3072         shape (3072,)                                  partition spec None
I0512 01:51:47.523026 140316961150976 utils.py:1372] Variable param_states/decoder/layers_12/self_attention/out/kernel/v_row                   size 2048         shape (2048,)                                  partition spec None
I0512 01:51:47.523067 140316961150976 utils.py:1372] Variable param_states/decoder/layers_12/self_attention/query/kernel/m                     size 1            shape (1,)                                     partition spec None
I0512 01:51:47.523107 140316961150976 utils.py:1372] Variable param_states/decoder/layers_12/self_attention/query/kernel/v                     size 1            shape (1,)                                     partition spec None
I0512 01:51:47.523146 140316961150976 utils.py:1372] Variable param_states/decoder/layers_12/self_attention/query/kernel/v_col                 size 3072         shape (3072,)                                  partition spec None
I0512 01:51:47.523185 140316961150976 utils.py:1372] Variable param_states/decoder/layers_12/self_attention/query/kernel/v_row                 size 2048         shape (2048,)                                  partition spec None
I0512 01:51:47.523224 140316961150976 utils.py:1372] Variable param_states/decoder/layers_12/self_attention/value/kernel/m                     size 1            shape (1,)                                     partition spec None
I0512 01:51:47.523263 140316961150976 utils.py:1372] Variable param_states/decoder/layers_12/self_attention/value/kernel/v                     size 1            shape (1,)                                     partition spec None
I0512 01:51:47.523311 140316961150976 utils.py:1372] Variable param_states/decoder/layers_12/self_attention/value/kernel/v_col                 size 3072         shape (3072,)                                  partition spec None
I0512 01:51:47.523352 140316961150976 utils.py:1372] Variable param_states/decoder/layers_12/self_attention/value/kernel/v_row                 size 2048         shape (2048,)                                  partition spec None
I0512 01:51:47.523392 140316961150976 utils.py:1372] Variable param_states/decoder/layers_13/mlp/wi_0/kernel/m                                 size 1            shape (1,)                                     partition spec None
I0512 01:51:47.523432 140316961150976 utils.py:1372] Variable param_states/decoder/layers_13/mlp/wi_0/kernel/v                                 size 1            shape (1,)                                     partition spec None
I0512 01:51:47.523471 140316961150976 utils.py:1372] Variable param_states/decoder/layers_13/mlp/wi_0/kernel/v_col                             size 16384        shape (16384,)                                 partition spec None
I0512 01:51:47.523510 140316961150976 utils.py:1372] Variable param_states/decoder/layers_13/mlp/wi_0/kernel/v_row                             size 2048         shape (2048,)                                  partition spec None
I0512 01:51:47.523549 140316961150976 utils.py:1372] Variable param_states/decoder/layers_13/mlp/wi_1/kernel/m                                 size 1            shape (1,)                                     partition spec None
I0512 01:51:47.523588 140316961150976 utils.py:1372] Variable param_states/decoder/layers_13/mlp/wi_1/kernel/v                                 size 1            shape (1,)                                     partition spec None
I0512 01:51:47.523627 140316961150976 utils.py:1372] Variable param_states/decoder/layers_13/mlp/wi_1/kernel/v_col                             size 8192         shape (8192,)                                  partition spec None
I0512 01:51:47.523674 140316961150976 utils.py:1372] Variable param_states/decoder/layers_13/mlp/wi_1/kernel/v_row                             size 2048         shape (2048,)                                  partition spec None
I0512 01:51:47.523715 140316961150976 utils.py:1372] Variable param_states/decoder/layers_13/mlp/wo/kernel/m                                   size 1            shape (1,)                                     partition spec None
I0512 01:51:47.523755 140316961150976 utils.py:1372] Variable param_states/decoder/layers_13/mlp/wo/kernel/v                                   size 1            shape (1,)                                     partition spec None
I0512 01:51:47.523805 140316961150976 utils.py:1372] Variable param_states/decoder/layers_13/mlp/wo/kernel/v_col                               size 8192         shape (8192,)                                  partition spec None
I0512 01:51:47.523847 140316961150976 utils.py:1372] Variable param_states/decoder/layers_13/mlp/wo/kernel/v_row                               size 2048         shape (2048,)                                  partition spec None
I0512 01:51:47.523886 140316961150976 utils.py:1372] Variable param_states/decoder/layers_13/pre_mlp_layer_norm/scale/m                        size 1            shape (1,)                                     partition spec None
I0512 01:51:47.523928 140316961150976 utils.py:1372] Variable param_states/decoder/layers_13/pre_mlp_layer_norm/scale/v                        size 2048         shape (embed=2048)                             partition spec ('model',)
I0512 01:51:47.523970 140316961150976 utils.py:1372] Variable param_states/decoder/layers_13/pre_mlp_layer_norm/scale/v_col                    size 1            shape (1,)                                     partition spec None
I0512 01:51:47.524016 140316961150976 utils.py:1372] Variable param_states/decoder/layers_13/pre_mlp_layer_norm/scale/v_row                    size 1            shape (1,)                                     partition spec None
I0512 01:51:47.524072 140316961150976 utils.py:1372] Variable param_states/decoder/layers_13/pre_self_attention_layer_norm/scale/m             size 1            shape (1,)                                     partition spec None
I0512 01:51:47.524118 140316961150976 utils.py:1372] Variable param_states/decoder/layers_13/pre_self_attention_layer_norm/scale/v             size 2048         shape (embed=2048)                             partition spec ('model',)
I0512 01:51:47.524160 140316961150976 utils.py:1372] Variable param_states/decoder/layers_13/pre_self_attention_layer_norm/scale/v_col         size 1            shape (1,)                                     partition spec None
I0512 01:51:47.524200 140316961150976 utils.py:1372] Variable param_states/decoder/layers_13/pre_self_attention_layer_norm/scale/v_row         size 1            shape (1,)                                     partition spec None
I0512 01:51:47.524239 140316961150976 utils.py:1372] Variable param_states/decoder/layers_13/self_attention/key/kernel/m                       size 1            shape (1,)                                     partition spec None
I0512 01:51:47.524279 140316961150976 utils.py:1372] Variable param_states/decoder/layers_13/self_attention/key/kernel/v                       size 1            shape (1,)                                     partition spec None
I0512 01:51:47.524333 140316961150976 utils.py:1372] Variable param_states/decoder/layers_13/self_attention/key/kernel/v_col                   size 3072         shape (3072,)                                  partition spec None
I0512 01:51:47.524376 140316961150976 utils.py:1372] Variable param_states/decoder/layers_13/self_attention/key/kernel/v_row                   size 2048         shape (2048,)                                  partition spec None
I0512 01:51:47.524415 140316961150976 utils.py:1372] Variable param_states/decoder/layers_13/self_attention/out/kernel/m                       size 1            shape (1,)                                     partition spec None
I0512 01:51:47.524455 140316961150976 utils.py:1372] Variable param_states/decoder/layers_13/self_attention/out/kernel/v                       size 1            shape (1,)                                     partition spec None
I0512 01:51:47.524494 140316961150976 utils.py:1372] Variable param_states/decoder/layers_13/self_attention/out/kernel/v_col                   size 3072         shape (3072,)                                  partition spec None
I0512 01:51:47.524533 140316961150976 utils.py:1372] Variable param_states/decoder/layers_13/self_attention/out/kernel/v_row                   size 2048         shape (2048,)                                  partition spec None
I0512 01:51:47.524572 140316961150976 utils.py:1372] Variable param_states/decoder/layers_13/self_attention/query/kernel/m                     size 1            shape (1,)                                     partition spec None
I0512 01:51:47.524612 140316961150976 utils.py:1372] Variable param_states/decoder/layers_13/self_attention/query/kernel/v                     size 1            shape (1,)                                     partition spec None
I0512 01:51:47.524652 140316961150976 utils.py:1372] Variable param_states/decoder/layers_13/self_attention/query/kernel/v_col                 size 3072         shape (3072,)                                  partition spec None
I0512 01:51:47.524690 140316961150976 utils.py:1372] Variable param_states/decoder/layers_13/self_attention/query/kernel/v_row                 size 2048         shape (2048,)                                  partition spec None
I0512 01:51:47.524729 140316961150976 utils.py:1372] Variable param_states/decoder/layers_13/self_attention/value/kernel/m                     size 1            shape (1,)                                     partition spec None
I0512 01:51:47.524770 140316961150976 utils.py:1372] Variable param_states/decoder/layers_13/self_attention/value/kernel/v                     size 1            shape (1,)                                     partition spec None
I0512 01:51:47.524812 140316961150976 utils.py:1372] Variable param_states/decoder/layers_13/self_attention/value/kernel/v_col                 size 3072         shape (3072,)                                  partition spec None
I0512 01:51:47.524852 140316961150976 utils.py:1372] Variable param_states/decoder/layers_13/self_attention/value/kernel/v_row                 size 2048         shape (2048,)                                  partition spec None
I0512 01:51:47.524891 140316961150976 utils.py:1372] Variable param_states/decoder/layers_14/mlp/wi_0/kernel/m                                 size 1            shape (1,)                                     partition spec None
I0512 01:51:47.524931 140316961150976 utils.py:1372] Variable param_states/decoder/layers_14/mlp/wi_0/kernel/v                                 size 1            shape (1,)                                     partition spec None
I0512 01:51:47.524971 140316961150976 utils.py:1372] Variable param_states/decoder/layers_14/mlp/wi_0/kernel/v_col                             size 16384        shape (16384,)                                 partition spec None
I0512 01:51:47.525016 140316961150976 utils.py:1372] Variable param_states/decoder/layers_14/mlp/wi_0/kernel/v_row                             size 2048         shape (2048,)                                  partition spec None
I0512 01:51:47.525057 140316961150976 utils.py:1372] Variable param_states/decoder/layers_14/mlp/wi_1/kernel/m                                 size 1            shape (1,)                                     partition spec None
I0512 01:51:47.525097 140316961150976 utils.py:1372] Variable param_states/decoder/layers_14/mlp/wi_1/kernel/v                                 size 1            shape (1,)                                     partition spec None
I0512 01:51:47.525137 140316961150976 utils.py:1372] Variable param_states/decoder/layers_14/mlp/wi_1/kernel/v_col                             size 8192         shape (8192,)                                  partition spec None
I0512 01:51:47.525176 140316961150976 utils.py:1372] Variable param_states/decoder/layers_14/mlp/wi_1/kernel/v_row                             size 2048         shape (2048,)                                  partition spec None
I0512 01:51:47.525215 140316961150976 utils.py:1372] Variable param_states/decoder/layers_14/mlp/wo/kernel/m                                   size 1            shape (1,)                                     partition spec None
I0512 01:51:47.525254 140316961150976 utils.py:1372] Variable param_states/decoder/layers_14/mlp/wo/kernel/v                                   size 1            shape (1,)                                     partition spec None
I0512 01:51:47.525303 140316961150976 utils.py:1372] Variable param_states/decoder/layers_14/mlp/wo/kernel/v_col                               size 8192         shape (8192,)                                  partition spec None
I0512 01:51:47.525344 140316961150976 utils.py:1372] Variable param_states/decoder/layers_14/mlp/wo/kernel/v_row                               size 2048         shape (2048,)                                  partition spec None
I0512 01:51:47.525383 140316961150976 utils.py:1372] Variable param_states/decoder/layers_14/pre_mlp_layer_norm/scale/m                        size 1            shape (1,)                                     partition spec None
I0512 01:51:47.525425 140316961150976 utils.py:1372] Variable param_states/decoder/layers_14/pre_mlp_layer_norm/scale/v                        size 2048         shape (embed=2048)                             partition spec ('model',)
I0512 01:51:47.525466 140316961150976 utils.py:1372] Variable param_states/decoder/layers_14/pre_mlp_layer_norm/scale/v_col                    size 1            shape (1,)                                     partition spec None
I0512 01:51:47.525505 140316961150976 utils.py:1372] Variable param_states/decoder/layers_14/pre_mlp_layer_norm/scale/v_row                    size 1            shape (1,)                                     partition spec None
I0512 01:51:47.525567 140316961150976 utils.py:1372] Variable param_states/decoder/layers_14/pre_self_attention_layer_norm/scale/m             size 1            shape (1,)                                     partition spec None
I0512 01:51:47.525618 140316961150976 utils.py:1372] Variable param_states/decoder/layers_14/pre_self_attention_layer_norm/scale/v             size 2048         shape (embed=2048)                             partition spec ('model',)
I0512 01:51:47.525661 140316961150976 utils.py:1372] Variable param_states/decoder/layers_14/pre_self_attention_layer_norm/scale/v_col         size 1            shape (1,)                                     partition spec None
I0512 01:51:47.525701 140316961150976 utils.py:1372] Variable param_states/decoder/layers_14/pre_self_attention_layer_norm/scale/v_row         size 1            shape (1,)                                     partition spec None
I0512 01:51:47.525741 140316961150976 utils.py:1372] Variable param_states/decoder/layers_14/self_attention/key/kernel/m                       size 1            shape (1,)                                     partition spec None
I0512 01:51:47.525782 140316961150976 utils.py:1372] Variable param_states/decoder/layers_14/self_attention/key/kernel/v                       size 1            shape (1,)                                     partition spec None
I0512 01:51:47.525832 140316961150976 utils.py:1372] Variable param_states/decoder/layers_14/self_attention/key/kernel/v_col                   size 3072         shape (3072,)                                  partition spec None
I0512 01:51:47.525873 140316961150976 utils.py:1372] Variable param_states/decoder/layers_14/self_attention/key/kernel/v_row                   size 2048         shape (2048,)                                  partition spec None
I0512 01:51:47.525913 140316961150976 utils.py:1372] Variable param_states/decoder/layers_14/self_attention/out/kernel/m                       size 1            shape (1,)                                     partition spec None
I0512 01:51:47.525953 140316961150976 utils.py:1372] Variable param_states/decoder/layers_14/self_attention/out/kernel/v                       size 1            shape (1,)                                     partition spec None
I0512 01:51:47.525992 140316961150976 utils.py:1372] Variable param_states/decoder/layers_14/self_attention/out/kernel/v_col                   size 3072         shape (3072,)                                  partition spec None
I0512 01:51:47.526055 140316961150976 utils.py:1372] Variable param_states/decoder/layers_14/self_attention/out/kernel/v_row                   size 2048         shape (2048,)                                  partition spec None
I0512 01:51:47.526099 140316961150976 utils.py:1372] Variable param_states/decoder/layers_14/self_attention/query/kernel/m                     size 1            shape (1,)                                     partition spec None
I0512 01:51:47.526139 140316961150976 utils.py:1372] Variable param_states/decoder/layers_14/self_attention/query/kernel/v                     size 1            shape (1,)                                     partition spec None
I0512 01:51:47.526179 140316961150976 utils.py:1372] Variable param_states/decoder/layers_14/self_attention/query/kernel/v_col                 size 3072         shape (3072,)                                  partition spec None
I0512 01:51:47.526217 140316961150976 utils.py:1372] Variable param_states/decoder/layers_14/self_attention/query/kernel/v_row                 size 2048         shape (2048,)                                  partition spec None
I0512 01:51:47.526256 140316961150976 utils.py:1372] Variable param_states/decoder/layers_14/self_attention/value/kernel/m                     size 1            shape (1,)                                     partition spec None
I0512 01:51:47.526295 140316961150976 utils.py:1372] Variable param_states/decoder/layers_14/self_attention/value/kernel/v                     size 1            shape (1,)                                     partition spec None
I0512 01:51:47.526343 140316961150976 utils.py:1372] Variable param_states/decoder/layers_14/self_attention/value/kernel/v_col                 size 3072         shape (3072,)                                  partition spec None
I0512 01:51:47.526385 140316961150976 utils.py:1372] Variable param_states/decoder/layers_14/self_attention/value/kernel/v_row                 size 2048         shape (2048,)                                  partition spec None
I0512 01:51:47.526426 140316961150976 utils.py:1372] Variable param_states/decoder/layers_15/mlp/wi_0/kernel/m                                 size 1            shape (1,)                                     partition spec None
I0512 01:51:47.526466 140316961150976 utils.py:1372] Variable param_states/decoder/layers_15/mlp/wi_0/kernel/v                                 size 1            shape (1,)                                     partition spec None
I0512 01:51:47.526505 140316961150976 utils.py:1372] Variable param_states/decoder/layers_15/mlp/wi_0/kernel/v_col                             size 16384        shape (16384,)                                 partition spec None
I0512 01:51:47.526545 140316961150976 utils.py:1372] Variable param_states/decoder/layers_15/mlp/wi_0/kernel/v_row                             size 2048         shape (2048,)                                  partition spec None
I0512 01:51:47.526584 140316961150976 utils.py:1372] Variable param_states/decoder/layers_15/mlp/wi_1/kernel/m                                 size 1            shape (1,)                                     partition spec None
I0512 01:51:47.526623 140316961150976 utils.py:1372] Variable param_states/decoder/layers_15/mlp/wi_1/kernel/v                                 size 1            shape (1,)                                     partition spec None
I0512 01:51:47.526662 140316961150976 utils.py:1372] Variable param_states/decoder/layers_15/mlp/wi_1/kernel/v_col                             size 8192         shape (8192,)                                  partition spec None
I0512 01:51:47.526701 140316961150976 utils.py:1372] Variable param_states/decoder/layers_15/mlp/wi_1/kernel/v_row                             size 2048         shape (2048,)                                  partition spec None
I0512 01:51:47.526740 140316961150976 utils.py:1372] Variable param_states/decoder/layers_15/mlp/wo/kernel/m                                   size 1            shape (1,)                                     partition spec None
I0512 01:51:47.526782 140316961150976 utils.py:1372] Variable param_states/decoder/layers_15/mlp/wo/kernel/v                                   size 1            shape (1,)                                     partition spec None
I0512 01:51:47.526830 140316961150976 utils.py:1372] Variable param_states/decoder/layers_15/mlp/wo/kernel/v_col                               size 8192         shape (8192,)                                  partition spec None
I0512 01:51:47.526872 140316961150976 utils.py:1372] Variable param_states/decoder/layers_15/mlp/wo/kernel/v_row                               size 2048         shape (2048,)                                  partition spec None
I0512 01:51:47.526911 140316961150976 utils.py:1372] Variable param_states/decoder/layers_15/pre_mlp_layer_norm/scale/m                        size 1            shape (1,)                                     partition spec None
I0512 01:51:47.526953 140316961150976 utils.py:1372] Variable param_states/decoder/layers_15/pre_mlp_layer_norm/scale/v                        size 2048         shape (embed=2048)                             partition spec ('model',)
I0512 01:51:47.526995 140316961150976 utils.py:1372] Variable param_states/decoder/layers_15/pre_mlp_layer_norm/scale/v_col                    size 1            shape (1,)                                     partition spec None
I0512 01:51:47.527046 140316961150976 utils.py:1372] Variable param_states/decoder/layers_15/pre_mlp_layer_norm/scale/v_row                    size 1            shape (1,)                                     partition spec None
I0512 01:51:47.527086 140316961150976 utils.py:1372] Variable param_states/decoder/layers_15/pre_self_attention_layer_norm/scale/m             size 1            shape (1,)                                     partition spec None
I0512 01:51:47.527128 140316961150976 utils.py:1372] Variable param_states/decoder/layers_15/pre_self_attention_layer_norm/scale/v             size 2048         shape (embed=2048)                             partition spec ('model',)
I0512 01:51:47.527169 140316961150976 utils.py:1372] Variable param_states/decoder/layers_15/pre_self_attention_layer_norm/scale/v_col         size 1            shape (1,)                                     partition spec None
I0512 01:51:47.527208 140316961150976 utils.py:1372] Variable param_states/decoder/layers_15/pre_self_attention_layer_norm/scale/v_row         size 1            shape (1,)                                     partition spec None
I0512 01:51:47.527247 140316961150976 utils.py:1372] Variable param_states/decoder/layers_15/self_attention/key/kernel/m                       size 1            shape (1,)                                     partition spec None
I0512 01:51:47.527287 140316961150976 utils.py:1372] Variable param_states/decoder/layers_15/self_attention/key/kernel/v                       size 1            shape (1,)                                     partition spec None
I0512 01:51:47.527335 140316961150976 utils.py:1372] Variable param_states/decoder/layers_15/self_attention/key/kernel/v_col                   size 3072         shape (3072,)                                  partition spec None
I0512 01:51:47.527376 140316961150976 utils.py:1372] Variable param_states/decoder/layers_15/self_attention/key/kernel/v_row                   size 2048         shape (2048,)                                  partition spec None
I0512 01:51:47.527416 140316961150976 utils.py:1372] Variable param_states/decoder/layers_15/self_attention/out/kernel/m                       size 1            shape (1,)                                     partition spec None
I0512 01:51:47.527456 140316961150976 utils.py:1372] Variable param_states/decoder/layers_15/self_attention/out/kernel/v                       size 1            shape (1,)                                     partition spec None
I0512 01:51:47.527494 140316961150976 utils.py:1372] Variable param_states/decoder/layers_15/self_attention/out/kernel/v_col                   size 3072         shape (3072,)                                  partition spec None
I0512 01:51:47.527534 140316961150976 utils.py:1372] Variable param_states/decoder/layers_15/self_attention/out/kernel/v_row                   size 2048         shape (2048,)                                  partition spec None
I0512 01:51:47.527572 140316961150976 utils.py:1372] Variable param_states/decoder/layers_15/self_attention/query/kernel/m                     size 1            shape (1,)                                     partition spec None
I0512 01:51:47.527611 140316961150976 utils.py:1372] Variable param_states/decoder/layers_15/self_attention/query/kernel/v                     size 1            shape (1,)                                     partition spec None
I0512 01:51:47.527650 140316961150976 utils.py:1372] Variable param_states/decoder/layers_15/self_attention/query/kernel/v_col                 size 3072         shape (3072,)                                  partition spec None
I0512 01:51:47.527689 140316961150976 utils.py:1372] Variable param_states/decoder/layers_15/self_attention/query/kernel/v_row                 size 2048         shape (2048,)                                  partition spec None
I0512 01:51:47.527728 140316961150976 utils.py:1372] Variable param_states/decoder/layers_15/self_attention/value/kernel/m                     size 1            shape (1,)                                     partition spec None
I0512 01:51:47.527769 140316961150976 utils.py:1372] Variable param_states/decoder/layers_15/self_attention/value/kernel/v                     size 1            shape (1,)                                     partition spec None
I0512 01:51:47.527819 140316961150976 utils.py:1372] Variable param_states/decoder/layers_15/self_attention/value/kernel/v_col                 size 3072         shape (3072,)                                  partition spec None
I0512 01:51:47.527861 140316961150976 utils.py:1372] Variable param_states/decoder/layers_15/self_attention/value/kernel/v_row                 size 2048         shape (2048,)                                  partition spec None
I0512 01:51:47.527901 140316961150976 utils.py:1372] Variable param_states/decoder/layers_16/mlp/wi_0/kernel/m                                 size 1            shape (1,)                                     partition spec None
I0512 01:51:47.527941 140316961150976 utils.py:1372] Variable param_states/decoder/layers_16/mlp/wi_0/kernel/v                                 size 1            shape (1,)                                     partition spec None
I0512 01:51:47.527995 140316961150976 utils.py:1372] Variable param_states/decoder/layers_16/mlp/wi_0/kernel/v_col                             size 16384        shape (16384,)                                 partition spec None
I0512 01:51:47.528045 140316961150976 utils.py:1372] Variable param_states/decoder/layers_16/mlp/wi_0/kernel/v_row                             size 2048         shape (2048,)                                  partition spec None
I0512 01:51:47.528086 140316961150976 utils.py:1372] Variable param_states/decoder/layers_16/mlp/wi_1/kernel/m                                 size 1            shape (1,)                                     partition spec None
I0512 01:51:47.528126 140316961150976 utils.py:1372] Variable param_states/decoder/layers_16/mlp/wi_1/kernel/v                                 size 1            shape (1,)                                     partition spec None
I0512 01:51:47.528165 140316961150976 utils.py:1372] Variable param_states/decoder/layers_16/mlp/wi_1/kernel/v_col                             size 8192         shape (8192,)                                  partition spec None
I0512 01:51:47.528204 140316961150976 utils.py:1372] Variable param_states/decoder/layers_16/mlp/wi_1/kernel/v_row                             size 2048         shape (2048,)                                  partition spec None
I0512 01:51:47.528242 140316961150976 utils.py:1372] Variable param_states/decoder/layers_16/mlp/wo/kernel/m                                   size 1            shape (1,)                                     partition spec None
I0512 01:51:47.528282 140316961150976 utils.py:1372] Variable param_states/decoder/layers_16/mlp/wo/kernel/v                                   size 1            shape (1,)                                     partition spec None
I0512 01:51:47.528330 140316961150976 utils.py:1372] Variable param_states/decoder/layers_16/mlp/wo/kernel/v_col                               size 8192         shape (8192,)                                  partition spec None
I0512 01:51:47.528371 140316961150976 utils.py:1372] Variable param_states/decoder/layers_16/mlp/wo/kernel/v_row                               size 2048         shape (2048,)                                  partition spec None
I0512 01:51:47.528410 140316961150976 utils.py:1372] Variable param_states/decoder/layers_16/pre_mlp_layer_norm/scale/m                        size 1            shape (1,)                                     partition spec None
I0512 01:51:47.528453 140316961150976 utils.py:1372] Variable param_states/decoder/layers_16/pre_mlp_layer_norm/scale/v                        size 2048         shape (embed=2048)                             partition spec ('model',)
I0512 01:51:47.528494 140316961150976 utils.py:1372] Variable param_states/decoder/layers_16/pre_mlp_layer_norm/scale/v_col                    size 1            shape (1,)                                     partition spec None
I0512 01:51:47.528534 140316961150976 utils.py:1372] Variable param_states/decoder/layers_16/pre_mlp_layer_norm/scale/v_row                    size 1            shape (1,)                                     partition spec None
I0512 01:51:47.528574 140316961150976 utils.py:1372] Variable param_states/decoder/layers_16/pre_self_attention_layer_norm/scale/m             size 1            shape (1,)                                     partition spec None
I0512 01:51:47.528615 140316961150976 utils.py:1372] Variable param_states/decoder/layers_16/pre_self_attention_layer_norm/scale/v             size 2048         shape (embed=2048)                             partition spec ('model',)
I0512 01:51:47.528656 140316961150976 utils.py:1372] Variable param_states/decoder/layers_16/pre_self_attention_layer_norm/scale/v_col         size 1            shape (1,)                                     partition spec None
I0512 01:51:47.528695 140316961150976 utils.py:1372] Variable param_states/decoder/layers_16/pre_self_attention_layer_norm/scale/v_row         size 1            shape (1,)                                     partition spec None
I0512 01:51:47.528734 140316961150976 utils.py:1372] Variable param_states/decoder/layers_16/self_attention/key/kernel/m                       size 1            shape (1,)                                     partition spec None
I0512 01:51:47.528776 140316961150976 utils.py:1372] Variable param_states/decoder/layers_16/self_attention/key/kernel/v                       size 1            shape (1,)                                     partition spec None
I0512 01:51:47.528826 140316961150976 utils.py:1372] Variable param_states/decoder/layers_16/self_attention/key/kernel/v_col                   size 3072         shape (3072,)                                  partition spec None
I0512 01:51:47.528868 140316961150976 utils.py:1372] Variable param_states/decoder/layers_16/self_attention/key/kernel/v_row                   size 2048         shape (2048,)                                  partition spec None
I0512 01:51:47.528907 140316961150976 utils.py:1372] Variable param_states/decoder/layers_16/self_attention/out/kernel/m                       size 1            shape (1,)                                     partition spec None
I0512 01:51:47.528946 140316961150976 utils.py:1372] Variable param_states/decoder/layers_16/self_attention/out/kernel/v                       size 1            shape (1,)                                     partition spec None
I0512 01:51:47.528986 140316961150976 utils.py:1372] Variable param_states/decoder/layers_16/self_attention/out/kernel/v_col                   size 3072         shape (3072,)                                  partition spec None
I0512 01:51:47.529032 140316961150976 utils.py:1372] Variable param_states/decoder/layers_16/self_attention/out/kernel/v_row                   size 2048         shape (2048,)                                  partition spec None
I0512 01:51:47.529072 140316961150976 utils.py:1372] Variable param_states/decoder/layers_16/self_attention/query/kernel/m                     size 1            shape (1,)                                     partition spec None
I0512 01:51:47.529111 140316961150976 utils.py:1372] Variable param_states/decoder/layers_16/self_attention/query/kernel/v                     size 1            shape (1,)                                     partition spec None
I0512 01:51:47.529150 140316961150976 utils.py:1372] Variable param_states/decoder/layers_16/self_attention/query/kernel/v_col                 size 3072         shape (3072,)                                  partition spec None
I0512 01:51:47.529189 140316961150976 utils.py:1372] Variable param_states/decoder/layers_16/self_attention/query/kernel/v_row                 size 2048         shape (2048,)                                  partition spec None
I0512 01:51:47.529228 140316961150976 utils.py:1372] Variable param_states/decoder/layers_16/self_attention/value/kernel/m                     size 1            shape (1,)                                     partition spec None
I0512 01:51:47.529268 140316961150976 utils.py:1372] Variable param_states/decoder/layers_16/self_attention/value/kernel/v                     size 1            shape (1,)                                     partition spec None
I0512 01:51:47.529307 140316961150976 utils.py:1372] Variable param_states/decoder/layers_16/self_attention/value/kernel/v_col                 size 3072         shape (3072,)                                  partition spec None
I0512 01:51:47.529346 140316961150976 utils.py:1372] Variable param_states/decoder/layers_16/self_attention/value/kernel/v_row                 size 2048         shape (2048,)                                  partition spec None
I0512 01:51:47.529385 140316961150976 utils.py:1372] Variable param_states/decoder/layers_17/extra_mlp/wi_0/kernel/m                           size 1            shape (1,)                                     partition spec None
I0512 01:51:47.529425 140316961150976 utils.py:1372] Variable param_states/decoder/layers_17/extra_mlp/wi_0/kernel/v                           size 1            shape (1,)                                     partition spec None
I0512 01:51:47.529465 140316961150976 utils.py:1372] Variable param_states/decoder/layers_17/extra_mlp/wi_0/kernel/v_col                       size 16384        shape (16384,)                                 partition spec None
I0512 01:51:47.529504 140316961150976 utils.py:1372] Variable param_states/decoder/layers_17/extra_mlp/wi_0/kernel/v_row                       size 2048         shape (2048,)                                  partition spec None
I0512 01:51:47.529563 140316961150976 utils.py:1372] Variable param_states/decoder/layers_17/extra_mlp/wi_1/kernel/m                           size 1            shape (1,)                                     partition spec None
I0512 01:51:47.529613 140316961150976 utils.py:1372] Variable param_states/decoder/layers_17/extra_mlp/wi_1/kernel/v                           size 1            shape (1,)                                     partition spec None
I0512 01:51:47.529655 140316961150976 utils.py:1372] Variable param_states/decoder/layers_17/extra_mlp/wi_1/kernel/v_col                       size 8192         shape (8192,)                                  partition spec None
I0512 01:51:47.529695 140316961150976 utils.py:1372] Variable param_states/decoder/layers_17/extra_mlp/wi_1/kernel/v_row                       size 2048         shape (2048,)                                  partition spec None
I0512 01:51:47.529734 140316961150976 utils.py:1372] Variable param_states/decoder/layers_17/extra_mlp/wo/kernel/m                             size 1            shape (1,)                                     partition spec None
I0512 01:51:47.529775 140316961150976 utils.py:1372] Variable param_states/decoder/layers_17/extra_mlp/wo/kernel/v                             size 1            shape (1,)                                     partition spec None
I0512 01:51:47.529831 140316961150976 utils.py:1372] Variable param_states/decoder/layers_17/extra_mlp/wo/kernel/v_col                         size 8192         shape (8192,)                                  partition spec None
I0512 01:51:47.529874 140316961150976 utils.py:1372] Variable param_states/decoder/layers_17/extra_mlp/wo/kernel/v_row                         size 2048         shape (2048,)                                  partition spec None
I0512 01:51:47.529914 140316961150976 utils.py:1372] Variable param_states/decoder/layers_17/mlp/expert/wi_0/kernel/m                          size 1            shape (1,)                                     partition spec None
I0512 01:51:47.529970 140316961150976 utils.py:1372] Variable param_states/decoder/layers_17/mlp/expert/wi_0/kernel/v                          size 1            shape (1,)                                     partition spec None
I0512 01:51:47.530028 140316961150976 utils.py:1372] Variable param_states/decoder/layers_17/mlp/expert/wi_0/kernel/v_col                      size 131072       shape (8, 16384)                               partition spec ('expert',)
I0512 01:51:47.530073 140316961150976 utils.py:1372] Variable param_states/decoder/layers_17/mlp/expert/wi_0/kernel/v_row                      size 16384        shape (8, 2048)                                partition spec ('expert',)
I0512 01:51:47.530114 140316961150976 utils.py:1372] Variable param_states/decoder/layers_17/mlp/expert/wi_1/kernel/m                          size 1            shape (1,)                                     partition spec None
I0512 01:51:47.530154 140316961150976 utils.py:1372] Variable param_states/decoder/layers_17/mlp/expert/wi_1/kernel/v                          size 1            shape (1,)                                     partition spec None
I0512 01:51:47.530201 140316961150976 utils.py:1372] Variable param_states/decoder/layers_17/mlp/expert/wi_1/kernel/v_col                      size 65536        shape (8, 8192)                                partition spec ('expert',)
I0512 01:51:47.530243 140316961150976 utils.py:1372] Variable param_states/decoder/layers_17/mlp/expert/wi_1/kernel/v_row                      size 16384        shape (8, 2048)                                partition spec ('expert',)
I0512 01:51:47.530283 140316961150976 utils.py:1372] Variable param_states/decoder/layers_17/mlp/expert/wo/kernel/m                            size 1            shape (1,)                                     partition spec None
I0512 01:51:47.530323 140316961150976 utils.py:1372] Variable param_states/decoder/layers_17/mlp/expert/wo/kernel/v                            size 1            shape (1,)                                     partition spec None
I0512 01:51:47.530374 140316961150976 utils.py:1372] Variable param_states/decoder/layers_17/mlp/expert/wo/kernel/v_col                        size 65536        shape (8, 8192)                                partition spec ('expert',)
I0512 01:51:47.530417 140316961150976 utils.py:1372] Variable param_states/decoder/layers_17/mlp/expert/wo/kernel/v_row                        size 16384        shape (8, 2048)                                partition spec ('expert',)
I0512 01:51:47.530457 140316961150976 utils.py:1372] Variable param_states/decoder/layers_17/mlp/router/router_weights/w/kernel/m              size 1            shape (1,)                                     partition spec None
I0512 01:51:47.530501 140316961150976 utils.py:1372] Variable param_states/decoder/layers_17/mlp/router/router_weights/w/kernel/v              size 16384        shape (embed=2048, unmodeled=8)                partition spec ('model', None)
I0512 01:51:47.530543 140316961150976 utils.py:1372] Variable param_states/decoder/layers_17/mlp/router/router_weights/w/kernel/v_col          size 1            shape (1,)                                     partition spec None
I0512 01:51:47.530583 140316961150976 utils.py:1372] Variable param_states/decoder/layers_17/mlp/router/router_weights/w/kernel/v_row          size 1            shape (1,)                                     partition spec None
I0512 01:51:47.530622 140316961150976 utils.py:1372] Variable param_states/decoder/layers_17/pre_extra_mlp_layer_norm/scale/m                  size 1            shape (1,)                                     partition spec None
I0512 01:51:47.530664 140316961150976 utils.py:1372] Variable param_states/decoder/layers_17/pre_extra_mlp_layer_norm/scale/v                  size 2048         shape (embed=2048)                             partition spec ('model',)
I0512 01:51:47.530706 140316961150976 utils.py:1372] Variable param_states/decoder/layers_17/pre_extra_mlp_layer_norm/scale/v_col              size 1            shape (1,)                                     partition spec None
I0512 01:51:47.530745 140316961150976 utils.py:1372] Variable param_states/decoder/layers_17/pre_extra_mlp_layer_norm/scale/v_row              size 1            shape (1,)                                     partition spec None
I0512 01:51:47.530788 140316961150976 utils.py:1372] Variable param_states/decoder/layers_17/pre_mlp_layer_norm/scale/m                        size 1            shape (1,)                                     partition spec None
I0512 01:51:47.530830 140316961150976 utils.py:1372] Variable param_states/decoder/layers_17/pre_mlp_layer_norm/scale/v                        size 2048         shape (embed=2048)                             partition spec ('model',)
I0512 01:51:47.530871 140316961150976 utils.py:1372] Variable param_states/decoder/layers_17/pre_mlp_layer_norm/scale/v_col                    size 1            shape (1,)                                     partition spec None
I0512 01:51:47.530910 140316961150976 utils.py:1372] Variable param_states/decoder/layers_17/pre_mlp_layer_norm/scale/v_row                    size 1            shape (1,)                                     partition spec None
I0512 01:51:47.530950 140316961150976 utils.py:1372] Variable param_states/decoder/layers_17/pre_self_attention_layer_norm/scale/m             size 1            shape (1,)                                     partition spec None
I0512 01:51:47.530990 140316961150976 utils.py:1372] Variable param_states/decoder/layers_17/pre_self_attention_layer_norm/scale/v             size 2048         shape (embed=2048)                             partition spec ('model',)
I0512 01:51:47.531039 140316961150976 utils.py:1372] Variable param_states/decoder/layers_17/pre_self_attention_layer_norm/scale/v_col         size 1            shape (1,)                                     partition spec None
I0512 01:51:47.531079 140316961150976 utils.py:1372] Variable param_states/decoder/layers_17/pre_self_attention_layer_norm/scale/v_row         size 1            shape (1,)                                     partition spec None
I0512 01:51:47.531118 140316961150976 utils.py:1372] Variable param_states/decoder/layers_17/self_attention/key/kernel/m                       size 1            shape (1,)                                     partition spec None
I0512 01:51:47.531157 140316961150976 utils.py:1372] Variable param_states/decoder/layers_17/self_attention/key/kernel/v                       size 1            shape (1,)                                     partition spec None
I0512 01:51:47.531206 140316961150976 utils.py:1372] Variable param_states/decoder/layers_17/self_attention/key/kernel/v_col                   size 3072         shape (3072,)                                  partition spec None
I0512 01:51:47.531248 140316961150976 utils.py:1372] Variable param_states/decoder/layers_17/self_attention/key/kernel/v_row                   size 2048         shape (2048,)                                  partition spec None
I0512 01:51:47.531288 140316961150976 utils.py:1372] Variable param_states/decoder/layers_17/self_attention/out/kernel/m                       size 1            shape (1,)                                     partition spec None
I0512 01:51:47.531327 140316961150976 utils.py:1372] Variable param_states/decoder/layers_17/self_attention/out/kernel/v                       size 1            shape (1,)                                     partition spec None
I0512 01:51:47.531367 140316961150976 utils.py:1372] Variable param_states/decoder/layers_17/self_attention/out/kernel/v_col                   size 3072         shape (3072,)                                  partition spec None
I0512 01:51:47.531406 140316961150976 utils.py:1372] Variable param_states/decoder/layers_17/self_attention/out/kernel/v_row                   size 2048         shape (2048,)                                  partition spec None
I0512 01:51:47.531445 140316961150976 utils.py:1372] Variable param_states/decoder/layers_17/self_attention/query/kernel/m                     size 1            shape (1,)                                     partition spec None
I0512 01:51:47.531484 140316961150976 utils.py:1372] Variable param_states/decoder/layers_17/self_attention/query/kernel/v                     size 1            shape (1,)                                     partition spec None
I0512 01:51:47.531533 140316961150976 utils.py:1372] Variable param_states/decoder/layers_17/self_attention/query/kernel/v_col                 size 3072         shape (3072,)                                  partition spec None
I0512 01:51:47.531575 140316961150976 utils.py:1372] Variable param_states/decoder/layers_17/self_attention/query/kernel/v_row                 size 2048         shape (2048,)                                  partition spec None
I0512 01:51:47.531615 140316961150976 utils.py:1372] Variable param_states/decoder/layers_17/self_attention/value/kernel/m                     size 1            shape (1,)                                     partition spec None
I0512 01:51:47.531654 140316961150976 utils.py:1372] Variable param_states/decoder/layers_17/self_attention/value/kernel/v                     size 1            shape (1,)                                     partition spec None
I0512 01:51:47.531694 140316961150976 utils.py:1372] Variable param_states/decoder/layers_17/self_attention/value/kernel/v_col                 size 3072         shape (3072,)                                  partition spec None
I0512 01:51:47.531735 140316961150976 utils.py:1372] Variable param_states/decoder/layers_17/self_attention/value/kernel/v_row                 size 2048         shape (2048,)                                  partition spec None
I0512 01:51:47.531776 140316961150976 utils.py:1372] Variable param_states/decoder/layers_18/mlp/wi_0/kernel/m                                 size 1            shape (1,)                                     partition spec None
I0512 01:51:47.531820 140316961150976 utils.py:1372] Variable param_states/decoder/layers_18/mlp/wi_0/kernel/v                                 size 1            shape (1,)                                     partition spec None
I0512 01:51:47.531861 140316961150976 utils.py:1372] Variable param_states/decoder/layers_18/mlp/wi_0/kernel/v_col                             size 16384        shape (16384,)                                 partition spec None
I0512 01:51:47.531900 140316961150976 utils.py:1372] Variable param_states/decoder/layers_18/mlp/wi_0/kernel/v_row                             size 2048         shape (2048,)                                  partition spec None
I0512 01:51:47.531960 140316961150976 utils.py:1372] Variable param_states/decoder/layers_18/mlp/wi_1/kernel/m                                 size 1            shape (1,)                                     partition spec None
I0512 01:51:47.532010 140316961150976 utils.py:1372] Variable param_states/decoder/layers_18/mlp/wi_1/kernel/v                                 size 1            shape (1,)                                     partition spec None
I0512 01:51:47.532062 140316961150976 utils.py:1372] Variable param_states/decoder/layers_18/mlp/wi_1/kernel/v_col                             size 8192         shape (8192,)                                  partition spec None
I0512 01:51:47.532104 140316961150976 utils.py:1372] Variable param_states/decoder/layers_18/mlp/wi_1/kernel/v_row                             size 2048         shape (2048,)                                  partition spec None
I0512 01:51:47.532144 140316961150976 utils.py:1372] Variable param_states/decoder/layers_18/mlp/wo/kernel/m                                   size 1            shape (1,)                                     partition spec None
I0512 01:51:47.532184 140316961150976 utils.py:1372] Variable param_states/decoder/layers_18/mlp/wo/kernel/v                                   size 1            shape (1,)                                     partition spec None
I0512 01:51:47.532225 140316961150976 utils.py:1372] Variable param_states/decoder/layers_18/mlp/wo/kernel/v_col                               size 8192         shape (8192,)                                  partition spec None
I0512 01:51:47.532264 140316961150976 utils.py:1372] Variable param_states/decoder/layers_18/mlp/wo/kernel/v_row                               size 2048         shape (2048,)                                  partition spec None
I0512 01:51:47.532304 140316961150976 utils.py:1372] Variable param_states/decoder/layers_18/pre_mlp_layer_norm/scale/m                        size 1            shape (1,)                                     partition spec None
I0512 01:51:47.532347 140316961150976 utils.py:1372] Variable param_states/decoder/layers_18/pre_mlp_layer_norm/scale/v                        size 2048         shape (embed=2048)                             partition spec ('model',)
I0512 01:51:47.532390 140316961150976 utils.py:1372] Variable param_states/decoder/layers_18/pre_mlp_layer_norm/scale/v_col                    size 1            shape (1,)                                     partition spec None
I0512 01:51:47.532430 140316961150976 utils.py:1372] Variable param_states/decoder/layers_18/pre_mlp_layer_norm/scale/v_row                    size 1            shape (1,)                                     partition spec None
I0512 01:51:47.532469 140316961150976 utils.py:1372] Variable param_states/decoder/layers_18/pre_self_attention_layer_norm/scale/m             size 1            shape (1,)                                     partition spec None
I0512 01:51:47.532511 140316961150976 utils.py:1372] Variable param_states/decoder/layers_18/pre_self_attention_layer_norm/scale/v             size 2048         shape (embed=2048)                             partition spec ('model',)
I0512 01:51:47.532553 140316961150976 utils.py:1372] Variable param_states/decoder/layers_18/pre_self_attention_layer_norm/scale/v_col         size 1            shape (1,)                                     partition spec None
I0512 01:51:47.532593 140316961150976 utils.py:1372] Variable param_states/decoder/layers_18/pre_self_attention_layer_norm/scale/v_row         size 1            shape (1,)                                     partition spec None
I0512 01:51:47.532632 140316961150976 utils.py:1372] Variable param_states/decoder/layers_18/self_attention/key/kernel/m                       size 1            shape (1,)                                     partition spec None
I0512 01:51:47.532672 140316961150976 utils.py:1372] Variable param_states/decoder/layers_18/self_attention/key/kernel/v                       size 1            shape (1,)                                     partition spec None
I0512 01:51:47.532712 140316961150976 utils.py:1372] Variable param_states/decoder/layers_18/self_attention/key/kernel/v_col                   size 3072         shape (3072,)                                  partition spec None
I0512 01:51:47.532751 140316961150976 utils.py:1372] Variable param_states/decoder/layers_18/self_attention/key/kernel/v_row                   size 2048         shape (2048,)                                  partition spec None
I0512 01:51:47.532793 140316961150976 utils.py:1372] Variable param_states/decoder/layers_18/self_attention/out/kernel/m                       size 1            shape (1,)                                     partition spec None
I0512 01:51:47.532834 140316961150976 utils.py:1372] Variable param_states/decoder/layers_18/self_attention/out/kernel/v                       size 1            shape (1,)                                     partition spec None
I0512 01:51:47.532873 140316961150976 utils.py:1372] Variable param_states/decoder/layers_18/self_attention/out/kernel/v_col                   size 3072         shape (3072,)                                  partition spec None
I0512 01:51:47.532912 140316961150976 utils.py:1372] Variable param_states/decoder/layers_18/self_attention/out/kernel/v_row                   size 2048         shape (2048,)                                  partition spec None
I0512 01:51:47.532952 140316961150976 utils.py:1372] Variable param_states/decoder/layers_18/self_attention/query/kernel/m                     size 1            shape (1,)                                     partition spec None
I0512 01:51:47.532992 140316961150976 utils.py:1372] Variable param_states/decoder/layers_18/self_attention/query/kernel/v                     size 1            shape (1,)                                     partition spec None
I0512 01:51:47.533049 140316961150976 utils.py:1372] Variable param_states/decoder/layers_18/self_attention/query/kernel/v_col                 size 3072         shape (3072,)                                  partition spec None
I0512 01:51:47.533093 140316961150976 utils.py:1372] Variable param_states/decoder/layers_18/self_attention/query/kernel/v_row                 size 2048         shape (2048,)                                  partition spec None
I0512 01:51:47.533133 140316961150976 utils.py:1372] Variable param_states/decoder/layers_18/self_attention/value/kernel/m                     size 1            shape (1,)                                     partition spec None
I0512 01:51:47.533173 140316961150976 utils.py:1372] Variable param_states/decoder/layers_18/self_attention/value/kernel/v                     size 1            shape (1,)                                     partition spec None
I0512 01:51:47.533212 140316961150976 utils.py:1372] Variable param_states/decoder/layers_18/self_attention/value/kernel/v_col                 size 3072         shape (3072,)                                  partition spec None
I0512 01:51:47.533252 140316961150976 utils.py:1372] Variable param_states/decoder/layers_18/self_attention/value/kernel/v_row                 size 2048         shape (2048,)                                  partition spec None
I0512 01:51:47.533291 140316961150976 utils.py:1372] Variable param_states/decoder/layers_19/mlp/wi_0/kernel/m                                 size 1            shape (1,)                                     partition spec None
I0512 01:51:47.533332 140316961150976 utils.py:1372] Variable param_states/decoder/layers_19/mlp/wi_0/kernel/v                                 size 1            shape (1,)                                     partition spec None
I0512 01:51:47.533372 140316961150976 utils.py:1372] Variable param_states/decoder/layers_19/mlp/wi_0/kernel/v_col                             size 16384        shape (16384,)                                 partition spec None
I0512 01:51:47.533411 140316961150976 utils.py:1372] Variable param_states/decoder/layers_19/mlp/wi_0/kernel/v_row                             size 2048         shape (2048,)                                  partition spec None
I0512 01:51:47.533450 140316961150976 utils.py:1372] Variable param_states/decoder/layers_19/mlp/wi_1/kernel/m                                 size 1            shape (1,)                                     partition spec None
I0512 01:51:47.533490 140316961150976 utils.py:1372] Variable param_states/decoder/layers_19/mlp/wi_1/kernel/v                                 size 1            shape (1,)                                     partition spec None
I0512 01:51:47.533538 140316961150976 utils.py:1372] Variable param_states/decoder/layers_19/mlp/wi_1/kernel/v_col                             size 8192         shape (8192,)                                  partition spec None
I0512 01:51:47.533608 140316961150976 utils.py:1372] Variable param_states/decoder/layers_19/mlp/wi_1/kernel/v_row                             size 2048         shape (2048,)                                  partition spec None
I0512 01:51:47.533650 140316961150976 utils.py:1372] Variable param_states/decoder/layers_19/mlp/wo/kernel/m                                   size 1            shape (1,)                                     partition spec None
I0512 01:51:47.533690 140316961150976 utils.py:1372] Variable param_states/decoder/layers_19/mlp/wo/kernel/v                                   size 1            shape (1,)                                     partition spec None
I0512 01:51:47.533730 140316961150976 utils.py:1372] Variable param_states/decoder/layers_19/mlp/wo/kernel/v_col                               size 8192         shape (8192,)                                  partition spec None
I0512 01:51:47.533770 140316961150976 utils.py:1372] Variable param_states/decoder/layers_19/mlp/wo/kernel/v_row                               size 2048         shape (2048,)                                  partition spec None
I0512 01:51:47.533811 140316961150976 utils.py:1372] Variable param_states/decoder/layers_19/pre_mlp_layer_norm/scale/m                        size 1            shape (1,)                                     partition spec None
I0512 01:51:47.533854 140316961150976 utils.py:1372] Variable param_states/decoder/layers_19/pre_mlp_layer_norm/scale/v                        size 2048         shape (embed=2048)                             partition spec ('model',)
I0512 01:51:47.533896 140316961150976 utils.py:1372] Variable param_states/decoder/layers_19/pre_mlp_layer_norm/scale/v_col                    size 1            shape (1,)                                     partition spec None
I0512 01:51:47.533953 140316961150976 utils.py:1372] Variable param_states/decoder/layers_19/pre_mlp_layer_norm/scale/v_row                    size 1            shape (1,)                                     partition spec None
I0512 01:51:47.533997 140316961150976 utils.py:1372] Variable param_states/decoder/layers_19/pre_self_attention_layer_norm/scale/m             size 1            shape (1,)                                     partition spec None
I0512 01:51:47.534049 140316961150976 utils.py:1372] Variable param_states/decoder/layers_19/pre_self_attention_layer_norm/scale/v             size 2048         shape (embed=2048)                             partition spec ('model',)
I0512 01:51:47.534092 140316961150976 utils.py:1372] Variable param_states/decoder/layers_19/pre_self_attention_layer_norm/scale/v_col         size 1            shape (1,)                                     partition spec None
I0512 01:51:47.534132 140316961150976 utils.py:1372] Variable param_states/decoder/layers_19/pre_self_attention_layer_norm/scale/v_row         size 1            shape (1,)                                     partition spec None
I0512 01:51:47.534172 140316961150976 utils.py:1372] Variable param_states/decoder/layers_19/self_attention/key/kernel/m                       size 1            shape (1,)                                     partition spec None
I0512 01:51:47.534212 140316961150976 utils.py:1372] Variable param_states/decoder/layers_19/self_attention/key/kernel/v                       size 1            shape (1,)                                     partition spec None
I0512 01:51:47.534253 140316961150976 utils.py:1372] Variable param_states/decoder/layers_19/self_attention/key/kernel/v_col                   size 3072         shape (3072,)                                  partition spec None
I0512 01:51:47.534293 140316961150976 utils.py:1372] Variable param_states/decoder/layers_19/self_attention/key/kernel/v_row                   size 2048         shape (2048,)                                  partition spec None
I0512 01:51:47.534333 140316961150976 utils.py:1372] Variable param_states/decoder/layers_19/self_attention/out/kernel/m                       size 1            shape (1,)                                     partition spec None
I0512 01:51:47.534373 140316961150976 utils.py:1372] Variable param_states/decoder/layers_19/self_attention/out/kernel/v                       size 1            shape (1,)                                     partition spec None
I0512 01:51:47.534414 140316961150976 utils.py:1372] Variable param_states/decoder/layers_19/self_attention/out/kernel/v_col                   size 3072         shape (3072,)                                  partition spec None
I0512 01:51:47.534454 140316961150976 utils.py:1372] Variable param_states/decoder/layers_19/self_attention/out/kernel/v_row                   size 2048         shape (2048,)                                  partition spec None
I0512 01:51:47.534493 140316961150976 utils.py:1372] Variable param_states/decoder/layers_19/self_attention/query/kernel/m                     size 1            shape (1,)                                     partition spec None
I0512 01:51:47.534534 140316961150976 utils.py:1372] Variable param_states/decoder/layers_19/self_attention/query/kernel/v                     size 1            shape (1,)                                     partition spec None
I0512 01:51:47.534574 140316961150976 utils.py:1372] Variable param_states/decoder/layers_19/self_attention/query/kernel/v_col                 size 3072         shape (3072,)                                  partition spec None
I0512 01:51:47.534615 140316961150976 utils.py:1372] Variable param_states/decoder/layers_19/self_attention/query/kernel/v_row                 size 2048         shape (2048,)                                  partition spec None
I0512 01:51:47.534655 140316961150976 utils.py:1372] Variable param_states/decoder/layers_19/self_attention/value/kernel/m                     size 1            shape (1,)                                     partition spec None
I0512 01:51:47.534695 140316961150976 utils.py:1372] Variable param_states/decoder/layers_19/self_attention/value/kernel/v                     size 1            shape (1,)                                     partition spec None
I0512 01:51:47.534736 140316961150976 utils.py:1372] Variable param_states/decoder/layers_19/self_attention/value/kernel/v_col                 size 3072         shape (3072,)                                  partition spec None
I0512 01:51:47.534778 140316961150976 utils.py:1372] Variable param_states/decoder/layers_19/self_attention/value/kernel/v_row                 size 2048         shape (2048,)                                  partition spec None
I0512 01:51:47.534819 140316961150976 utils.py:1372] Variable param_states/decoder/layers_2/mlp/wi_0/kernel/m                                  size 1            shape (1,)                                     partition spec None
I0512 01:51:47.534861 140316961150976 utils.py:1372] Variable param_states/decoder/layers_2/mlp/wi_0/kernel/v                                  size 1            shape (1,)                                     partition spec None
I0512 01:51:47.534901 140316961150976 utils.py:1372] Variable param_states/decoder/layers_2/mlp/wi_0/kernel/v_col                              size 16384        shape (16384,)                                 partition spec None
I0512 01:51:47.534942 140316961150976 utils.py:1372] Variable param_states/decoder/layers_2/mlp/wi_0/kernel/v_row                              size 2048         shape (2048,)                                  partition spec None
I0512 01:51:47.534981 140316961150976 utils.py:1372] Variable param_states/decoder/layers_2/mlp/wi_1/kernel/m                                  size 1            shape (1,)                                     partition spec None
I0512 01:51:47.535028 140316961150976 utils.py:1372] Variable param_states/decoder/layers_2/mlp/wi_1/kernel/v                                  size 1            shape (1,)                                     partition spec None
I0512 01:51:47.535079 140316961150976 utils.py:1372] Variable param_states/decoder/layers_2/mlp/wi_1/kernel/v_col                              size 8192         shape (8192,)                                  partition spec None
I0512 01:51:47.535122 140316961150976 utils.py:1372] Variable param_states/decoder/layers_2/mlp/wi_1/kernel/v_row                              size 2048         shape (2048,)                                  partition spec None
I0512 01:51:47.535162 140316961150976 utils.py:1372] Variable param_states/decoder/layers_2/mlp/wo/kernel/m                                    size 1            shape (1,)                                     partition spec None
I0512 01:51:47.535202 140316961150976 utils.py:1372] Variable param_states/decoder/layers_2/mlp/wo/kernel/v                                    size 1            shape (1,)                                     partition spec None
I0512 01:51:47.535242 140316961150976 utils.py:1372] Variable param_states/decoder/layers_2/mlp/wo/kernel/v_col                                size 8192         shape (8192,)                                  partition spec None
I0512 01:51:47.535282 140316961150976 utils.py:1372] Variable param_states/decoder/layers_2/mlp/wo/kernel/v_row                                size 2048         shape (2048,)                                  partition spec None
I0512 01:51:47.535321 140316961150976 utils.py:1372] Variable param_states/decoder/layers_2/pre_mlp_layer_norm/scale/m                         size 1            shape (1,)                                     partition spec None
I0512 01:51:47.535364 140316961150976 utils.py:1372] Variable param_states/decoder/layers_2/pre_mlp_layer_norm/scale/v                         size 2048         shape (embed=2048)                             partition spec ('model',)
I0512 01:51:47.535406 140316961150976 utils.py:1372] Variable param_states/decoder/layers_2/pre_mlp_layer_norm/scale/v_col                     size 1            shape (1,)                                     partition spec None
I0512 01:51:47.535446 140316961150976 utils.py:1372] Variable param_states/decoder/layers_2/pre_mlp_layer_norm/scale/v_row                     size 1            shape (1,)                                     partition spec None
I0512 01:51:47.535486 140316961150976 utils.py:1372] Variable param_states/decoder/layers_2/pre_self_attention_layer_norm/scale/m              size 1            shape (1,)                                     partition spec None
I0512 01:51:47.535528 140316961150976 utils.py:1372] Variable param_states/decoder/layers_2/pre_self_attention_layer_norm/scale/v              size 2048         shape (embed=2048)                             partition spec ('model',)
I0512 01:51:47.535569 140316961150976 utils.py:1372] Variable param_states/decoder/layers_2/pre_self_attention_layer_norm/scale/v_col          size 1            shape (1,)                                     partition spec None
I0512 01:51:47.535609 140316961150976 utils.py:1372] Variable param_states/decoder/layers_2/pre_self_attention_layer_norm/scale/v_row          size 1            shape (1,)                                     partition spec None
I0512 01:51:47.535649 140316961150976 utils.py:1372] Variable param_states/decoder/layers_2/self_attention/key/kernel/m                        size 1            shape (1,)                                     partition spec None
I0512 01:51:47.535689 140316961150976 utils.py:1372] Variable param_states/decoder/layers_2/self_attention/key/kernel/v                        size 1            shape (1,)                                     partition spec None
I0512 01:51:47.535737 140316961150976 utils.py:1372] Variable param_states/decoder/layers_2/self_attention/key/kernel/v_col                    size 3072         shape (3072,)                                  partition spec None
I0512 01:51:47.535781 140316961150976 utils.py:1372] Variable param_states/decoder/layers_2/self_attention/key/kernel/v_row                    size 2048         shape (2048,)                                  partition spec None
I0512 01:51:47.535823 140316961150976 utils.py:1372] Variable param_states/decoder/layers_2/self_attention/out/kernel/m                        size 1            shape (1,)                                     partition spec None
I0512 01:51:47.535864 140316961150976 utils.py:1372] Variable param_states/decoder/layers_2/self_attention/out/kernel/v                        size 1            shape (1,)                                     partition spec None
I0512 01:51:47.535939 140316961150976 utils.py:1372] Variable param_states/decoder/layers_2/self_attention/out/kernel/v_col                    size 3072         shape (3072,)                                  partition spec None
I0512 01:51:47.536023 140316961150976 utils.py:1372] Variable param_states/decoder/layers_2/self_attention/out/kernel/v_row                    size 2048         shape (2048,)                                  partition spec None
I0512 01:51:47.536068 140316961150976 utils.py:1372] Variable param_states/decoder/layers_2/self_attention/query/kernel/m                      size 1            shape (1,)                                     partition spec None
I0512 01:51:47.536109 140316961150976 utils.py:1372] Variable param_states/decoder/layers_2/self_attention/query/kernel/v                      size 1            shape (1,)                                     partition spec None
I0512 01:51:47.536159 140316961150976 utils.py:1372] Variable param_states/decoder/layers_2/self_attention/query/kernel/v_col                  size 3072         shape (3072,)                                  partition spec None
I0512 01:51:47.536201 140316961150976 utils.py:1372] Variable param_states/decoder/layers_2/self_attention/query/kernel/v_row                  size 2048         shape (2048,)                                  partition spec None
I0512 01:51:47.536241 140316961150976 utils.py:1372] Variable param_states/decoder/layers_2/self_attention/value/kernel/m                      size 1            shape (1,)                                     partition spec None
I0512 01:51:47.536281 140316961150976 utils.py:1372] Variable param_states/decoder/layers_2/self_attention/value/kernel/v                      size 1            shape (1,)                                     partition spec None
I0512 01:51:47.536320 140316961150976 utils.py:1372] Variable param_states/decoder/layers_2/self_attention/value/kernel/v_col                  size 3072         shape (3072,)                                  partition spec None
I0512 01:51:47.536361 140316961150976 utils.py:1372] Variable param_states/decoder/layers_2/self_attention/value/kernel/v_row                  size 2048         shape (2048,)                                  partition spec None
I0512 01:51:47.536400 140316961150976 utils.py:1372] Variable param_states/decoder/layers_20/mlp/wi_0/kernel/m                                 size 1            shape (1,)                                     partition spec None
I0512 01:51:47.536441 140316961150976 utils.py:1372] Variable param_states/decoder/layers_20/mlp/wi_0/kernel/v                                 size 1            shape (1,)                                     partition spec None
I0512 01:51:47.536482 140316961150976 utils.py:1372] Variable param_states/decoder/layers_20/mlp/wi_0/kernel/v_col                             size 16384        shape (16384,)                                 partition spec None
I0512 01:51:47.536522 140316961150976 utils.py:1372] Variable param_states/decoder/layers_20/mlp/wi_0/kernel/v_row                             size 2048         shape (2048,)                                  partition spec None
I0512 01:51:47.536562 140316961150976 utils.py:1372] Variable param_states/decoder/layers_20/mlp/wi_1/kernel/m                                 size 1            shape (1,)                                     partition spec None
I0512 01:51:47.536603 140316961150976 utils.py:1372] Variable param_states/decoder/layers_20/mlp/wi_1/kernel/v                                 size 1            shape (1,)                                     partition spec None
I0512 01:51:47.536652 140316961150976 utils.py:1372] Variable param_states/decoder/layers_20/mlp/wi_1/kernel/v_col                             size 8192         shape (8192,)                                  partition spec None
I0512 01:51:47.536694 140316961150976 utils.py:1372] Variable param_states/decoder/layers_20/mlp/wi_1/kernel/v_row                             size 2048         shape (2048,)                                  partition spec None
I0512 01:51:47.536734 140316961150976 utils.py:1372] Variable param_states/decoder/layers_20/mlp/wo/kernel/m                                   size 1            shape (1,)                                     partition spec None
I0512 01:51:47.536777 140316961150976 utils.py:1372] Variable param_states/decoder/layers_20/mlp/wo/kernel/v                                   size 1            shape (1,)                                     partition spec None
I0512 01:51:47.536818 140316961150976 utils.py:1372] Variable param_states/decoder/layers_20/mlp/wo/kernel/v_col                               size 8192         shape (8192,)                                  partition spec None
I0512 01:51:47.536865 140316961150976 utils.py:1372] Variable param_states/decoder/layers_20/mlp/wo/kernel/v_row                               size 2048         shape (2048,)                                  partition spec None
I0512 01:51:47.536907 140316961150976 utils.py:1372] Variable param_states/decoder/layers_20/pre_mlp_layer_norm/scale/m                        size 1            shape (1,)                                     partition spec None
I0512 01:51:47.536951 140316961150976 utils.py:1372] Variable param_states/decoder/layers_20/pre_mlp_layer_norm/scale/v                        size 2048         shape (embed=2048)                             partition spec ('model',)
I0512 01:51:47.536993 140316961150976 utils.py:1372] Variable param_states/decoder/layers_20/pre_mlp_layer_norm/scale/v_col                    size 1            shape (1,)                                     partition spec None
I0512 01:51:47.537042 140316961150976 utils.py:1372] Variable param_states/decoder/layers_20/pre_mlp_layer_norm/scale/v_row                    size 1            shape (1,)                                     partition spec None
I0512 01:51:47.537082 140316961150976 utils.py:1372] Variable param_states/decoder/layers_20/pre_self_attention_layer_norm/scale/m             size 1            shape (1,)                                     partition spec None
I0512 01:51:47.537124 140316961150976 utils.py:1372] Variable param_states/decoder/layers_20/pre_self_attention_layer_norm/scale/v             size 2048         shape (embed=2048)                             partition spec ('model',)
I0512 01:51:47.537166 140316961150976 utils.py:1372] Variable param_states/decoder/layers_20/pre_self_attention_layer_norm/scale/v_col         size 1            shape (1,)                                     partition spec None
I0512 01:51:47.537207 140316961150976 utils.py:1372] Variable param_states/decoder/layers_20/pre_self_attention_layer_norm/scale/v_row         size 1            shape (1,)                                     partition spec None
I0512 01:51:47.537247 140316961150976 utils.py:1372] Variable param_states/decoder/layers_20/self_attention/key/kernel/m                       size 1            shape (1,)                                     partition spec None
I0512 01:51:47.537287 140316961150976 utils.py:1372] Variable param_states/decoder/layers_20/self_attention/key/kernel/v                       size 1            shape (1,)                                     partition spec None
I0512 01:51:47.537335 140316961150976 utils.py:1372] Variable param_states/decoder/layers_20/self_attention/key/kernel/v_col                   size 3072         shape (3072,)                                  partition spec None
I0512 01:51:47.537378 140316961150976 utils.py:1372] Variable param_states/decoder/layers_20/self_attention/key/kernel/v_row                   size 2048         shape (2048,)                                  partition spec None
I0512 01:51:47.537418 140316961150976 utils.py:1372] Variable param_states/decoder/layers_20/self_attention/out/kernel/m                       size 1            shape (1,)                                     partition spec None
I0512 01:51:47.537457 140316961150976 utils.py:1372] Variable param_states/decoder/layers_20/self_attention/out/kernel/v                       size 1            shape (1,)                                     partition spec None
I0512 01:51:47.537498 140316961150976 utils.py:1372] Variable param_states/decoder/layers_20/self_attention/out/kernel/v_col                   size 3072         shape (3072,)                                  partition spec None
I0512 01:51:47.537538 140316961150976 utils.py:1372] Variable param_states/decoder/layers_20/self_attention/out/kernel/v_row                   size 2048         shape (2048,)                                  partition spec None
I0512 01:51:47.537606 140316961150976 utils.py:1372] Variable param_states/decoder/layers_20/self_attention/query/kernel/m                     size 1            shape (1,)                                     partition spec None
I0512 01:51:47.537648 140316961150976 utils.py:1372] Variable param_states/decoder/layers_20/self_attention/query/kernel/v                     size 1            shape (1,)                                     partition spec None
I0512 01:51:47.537688 140316961150976 utils.py:1372] Variable param_states/decoder/layers_20/self_attention/query/kernel/v_col                 size 3072         shape (3072,)                                  partition spec None
I0512 01:51:47.537728 140316961150976 utils.py:1372] Variable param_states/decoder/layers_20/self_attention/query/kernel/v_row                 size 2048         shape (2048,)                                  partition spec None
I0512 01:51:47.537769 140316961150976 utils.py:1372] Variable param_states/decoder/layers_20/self_attention/value/kernel/m                     size 1            shape (1,)                                     partition spec None
I0512 01:51:47.537811 140316961150976 utils.py:1372] Variable param_states/decoder/layers_20/self_attention/value/kernel/v                     size 1            shape (1,)                                     partition spec None
I0512 01:51:47.537852 140316961150976 utils.py:1372] Variable param_states/decoder/layers_20/self_attention/value/kernel/v_col                 size 3072         shape (3072,)                                  partition spec None
I0512 01:51:47.537893 140316961150976 utils.py:1372] Variable param_states/decoder/layers_20/self_attention/value/kernel/v_row                 size 2048         shape (2048,)                                  partition spec None
I0512 01:51:47.537933 140316961150976 utils.py:1372] Variable param_states/decoder/layers_21/mlp/wi_0/kernel/m                                 size 1            shape (1,)                                     partition spec None
I0512 01:51:47.537990 140316961150976 utils.py:1372] Variable param_states/decoder/layers_21/mlp/wi_0/kernel/v                                 size 1            shape (1,)                                     partition spec None
I0512 01:51:47.538042 140316961150976 utils.py:1372] Variable param_states/decoder/layers_21/mlp/wi_0/kernel/v_col                             size 16384        shape (16384,)                                 partition spec None
I0512 01:51:47.538084 140316961150976 utils.py:1372] Variable param_states/decoder/layers_21/mlp/wi_0/kernel/v_row                             size 2048         shape (2048,)                                  partition spec None
I0512 01:51:47.538124 140316961150976 utils.py:1372] Variable param_states/decoder/layers_21/mlp/wi_1/kernel/m                                 size 1            shape (1,)                                     partition spec None
I0512 01:51:47.538165 140316961150976 utils.py:1372] Variable param_states/decoder/layers_21/mlp/wi_1/kernel/v                                 size 1            shape (1,)                                     partition spec None
I0512 01:51:47.538214 140316961150976 utils.py:1372] Variable param_states/decoder/layers_21/mlp/wi_1/kernel/v_col                             size 8192         shape (8192,)                                  partition spec None
I0512 01:51:47.538256 140316961150976 utils.py:1372] Variable param_states/decoder/layers_21/mlp/wi_1/kernel/v_row                             size 2048         shape (2048,)                                  partition spec None
I0512 01:51:47.538296 140316961150976 utils.py:1372] Variable param_states/decoder/layers_21/mlp/wo/kernel/m                                   size 1            shape (1,)                                     partition spec None
I0512 01:51:47.538336 140316961150976 utils.py:1372] Variable param_states/decoder/layers_21/mlp/wo/kernel/v                                   size 1            shape (1,)                                     partition spec None
I0512 01:51:47.538377 140316961150976 utils.py:1372] Variable param_states/decoder/layers_21/mlp/wo/kernel/v_col                               size 8192         shape (8192,)                                  partition spec None
I0512 01:51:47.538416 140316961150976 utils.py:1372] Variable param_states/decoder/layers_21/mlp/wo/kernel/v_row                               size 2048         shape (2048,)                                  partition spec None
I0512 01:51:47.538455 140316961150976 utils.py:1372] Variable param_states/decoder/layers_21/pre_mlp_layer_norm/scale/m                        size 1            shape (1,)                                     partition spec None
I0512 01:51:47.538499 140316961150976 utils.py:1372] Variable param_states/decoder/layers_21/pre_mlp_layer_norm/scale/v                        size 2048         shape (embed=2048)                             partition spec ('model',)
I0512 01:51:47.538541 140316961150976 utils.py:1372] Variable param_states/decoder/layers_21/pre_mlp_layer_norm/scale/v_col                    size 1            shape (1,)                                     partition spec None
I0512 01:51:47.538581 140316961150976 utils.py:1372] Variable param_states/decoder/layers_21/pre_mlp_layer_norm/scale/v_row                    size 1            shape (1,)                                     partition spec None
I0512 01:51:47.538621 140316961150976 utils.py:1372] Variable param_states/decoder/layers_21/pre_self_attention_layer_norm/scale/m             size 1            shape (1,)                                     partition spec None
I0512 01:51:47.538663 140316961150976 utils.py:1372] Variable param_states/decoder/layers_21/pre_self_attention_layer_norm/scale/v             size 2048         shape (embed=2048)                             partition spec ('model',)
I0512 01:51:47.538705 140316961150976 utils.py:1372] Variable param_states/decoder/layers_21/pre_self_attention_layer_norm/scale/v_col         size 1            shape (1,)                                     partition spec None
I0512 01:51:47.538745 140316961150976 utils.py:1372] Variable param_states/decoder/layers_21/pre_self_attention_layer_norm/scale/v_row         size 1            shape (1,)                                     partition spec None
I0512 01:51:47.538788 140316961150976 utils.py:1372] Variable param_states/decoder/layers_21/self_attention/key/kernel/m                       size 1            shape (1,)                                     partition spec None
I0512 01:51:47.538829 140316961150976 utils.py:1372] Variable param_states/decoder/layers_21/self_attention/key/kernel/v                       size 1            shape (1,)                                     partition spec None
I0512 01:51:47.538877 140316961150976 utils.py:1372] Variable param_states/decoder/layers_21/self_attention/key/kernel/v_col                   size 3072         shape (3072,)                                  partition spec None
I0512 01:51:47.538920 140316961150976 utils.py:1372] Variable param_states/decoder/layers_21/self_attention/key/kernel/v_row                   size 2048         shape (2048,)                                  partition spec None
I0512 01:51:47.538960 140316961150976 utils.py:1372] Variable param_states/decoder/layers_21/self_attention/out/kernel/m                       size 1            shape (1,)                                     partition spec None
I0512 01:51:47.539000 140316961150976 utils.py:1372] Variable param_states/decoder/layers_21/self_attention/out/kernel/v                       size 1            shape (1,)                                     partition spec None
I0512 01:51:47.539050 140316961150976 utils.py:1372] Variable param_states/decoder/layers_21/self_attention/out/kernel/v_col                   size 3072         shape (3072,)                                  partition spec None
I0512 01:51:47.539090 140316961150976 utils.py:1372] Variable param_states/decoder/layers_21/self_attention/out/kernel/v_row                   size 2048         shape (2048,)                                  partition spec None
I0512 01:51:47.539130 140316961150976 utils.py:1372] Variable param_states/decoder/layers_21/self_attention/query/kernel/m                     size 1            shape (1,)                                     partition spec None
I0512 01:51:47.539170 140316961150976 utils.py:1372] Variable param_states/decoder/layers_21/self_attention/query/kernel/v                     size 1            shape (1,)                                     partition spec None
I0512 01:51:47.539218 140316961150976 utils.py:1372] Variable param_states/decoder/layers_21/self_attention/query/kernel/v_col                 size 3072         shape (3072,)                                  partition spec None
I0512 01:51:47.539260 140316961150976 utils.py:1372] Variable param_states/decoder/layers_21/self_attention/query/kernel/v_row                 size 2048         shape (2048,)                                  partition spec None
I0512 01:51:47.539299 140316961150976 utils.py:1372] Variable param_states/decoder/layers_21/self_attention/value/kernel/m                     size 1            shape (1,)                                     partition spec None
I0512 01:51:47.539340 140316961150976 utils.py:1372] Variable param_states/decoder/layers_21/self_attention/value/kernel/v                     size 1            shape (1,)                                     partition spec None
I0512 01:51:47.539380 140316961150976 utils.py:1372] Variable param_states/decoder/layers_21/self_attention/value/kernel/v_col                 size 3072         shape (3072,)                                  partition spec None
I0512 01:51:47.539420 140316961150976 utils.py:1372] Variable param_states/decoder/layers_21/self_attention/value/kernel/v_row                 size 2048         shape (2048,)                                  partition spec None
I0512 01:51:47.539460 140316961150976 utils.py:1372] Variable param_states/decoder/layers_22/mlp/wi_0/kernel/m                                 size 1            shape (1,)                                     partition spec None
I0512 01:51:47.539501 140316961150976 utils.py:1372] Variable param_states/decoder/layers_22/mlp/wi_0/kernel/v                                 size 1            shape (1,)                                     partition spec None
I0512 01:51:47.539541 140316961150976 utils.py:1372] Variable param_states/decoder/layers_22/mlp/wi_0/kernel/v_col                             size 16384        shape (16384,)                                 partition spec None
I0512 01:51:47.539581 140316961150976 utils.py:1372] Variable param_states/decoder/layers_22/mlp/wi_0/kernel/v_row                             size 2048         shape (2048,)                                  partition spec None
I0512 01:51:47.539621 140316961150976 utils.py:1372] Variable param_states/decoder/layers_22/mlp/wi_1/kernel/m                                 size 1            shape (1,)                                     partition spec None
I0512 01:51:47.539661 140316961150976 utils.py:1372] Variable param_states/decoder/layers_22/mlp/wi_1/kernel/v                                 size 1            shape (1,)                                     partition spec None
I0512 01:51:47.539707 140316961150976 utils.py:1372] Variable param_states/decoder/layers_22/mlp/wi_1/kernel/v_col                             size 8192         shape (8192,)                                  partition spec None
I0512 01:51:47.539749 140316961150976 utils.py:1372] Variable param_states/decoder/layers_22/mlp/wi_1/kernel/v_row                             size 2048         shape (2048,)                                  partition spec None
I0512 01:51:47.539792 140316961150976 utils.py:1372] Variable param_states/decoder/layers_22/mlp/wo/kernel/m                                   size 1            shape (1,)                                     partition spec None
I0512 01:51:47.539833 140316961150976 utils.py:1372] Variable param_states/decoder/layers_22/mlp/wo/kernel/v                                   size 1            shape (1,)                                     partition spec None
I0512 01:51:47.539874 140316961150976 utils.py:1372] Variable param_states/decoder/layers_22/mlp/wo/kernel/v_col                               size 8192         shape (8192,)                                  partition spec None
I0512 01:51:47.539913 140316961150976 utils.py:1372] Variable param_states/decoder/layers_22/mlp/wo/kernel/v_row                               size 2048         shape (2048,)                                  partition spec None
I0512 01:51:47.539969 140316961150976 utils.py:1372] Variable param_states/decoder/layers_22/pre_mlp_layer_norm/scale/m                        size 1            shape (1,)                                     partition spec None
I0512 01:51:47.540024 140316961150976 utils.py:1372] Variable param_states/decoder/layers_22/pre_mlp_layer_norm/scale/v                        size 2048         shape (embed=2048)                             partition spec ('model',)
I0512 01:51:47.540068 140316961150976 utils.py:1372] Variable param_states/decoder/layers_22/pre_mlp_layer_norm/scale/v_col                    size 1            shape (1,)                                     partition spec None
I0512 01:51:47.540108 140316961150976 utils.py:1372] Variable param_states/decoder/layers_22/pre_mlp_layer_norm/scale/v_row                    size 1            shape (1,)                                     partition spec None
I0512 01:51:47.540148 140316961150976 utils.py:1372] Variable param_states/decoder/layers_22/pre_self_attention_layer_norm/scale/m             size 1            shape (1,)                                     partition spec None
I0512 01:51:47.540191 140316961150976 utils.py:1372] Variable param_states/decoder/layers_22/pre_self_attention_layer_norm/scale/v             size 2048         shape (embed=2048)                             partition spec ('model',)
I0512 01:51:47.540233 140316961150976 utils.py:1372] Variable param_states/decoder/layers_22/pre_self_attention_layer_norm/scale/v_col         size 1            shape (1,)                                     partition spec None
I0512 01:51:47.540273 140316961150976 utils.py:1372] Variable param_states/decoder/layers_22/pre_self_attention_layer_norm/scale/v_row         size 1            shape (1,)                                     partition spec None
I0512 01:51:47.540314 140316961150976 utils.py:1372] Variable param_states/decoder/layers_22/self_attention/key/kernel/m                       size 1            shape (1,)                                     partition spec None
I0512 01:51:47.540354 140316961150976 utils.py:1372] Variable param_states/decoder/layers_22/self_attention/key/kernel/v                       size 1            shape (1,)                                     partition spec None
I0512 01:51:47.540402 140316961150976 utils.py:1372] Variable param_states/decoder/layers_22/self_attention/key/kernel/v_col                   size 3072         shape (3072,)                                  partition spec None
I0512 01:51:47.540444 140316961150976 utils.py:1372] Variable param_states/decoder/layers_22/self_attention/key/kernel/v_row                   size 2048         shape (2048,)                                  partition spec None
I0512 01:51:47.540485 140316961150976 utils.py:1372] Variable param_states/decoder/layers_22/self_attention/out/kernel/m                       size 1            shape (1,)                                     partition spec None
I0512 01:51:47.540525 140316961150976 utils.py:1372] Variable param_states/decoder/layers_22/self_attention/out/kernel/v                       size 1            shape (1,)                                     partition spec None
I0512 01:51:47.540565 140316961150976 utils.py:1372] Variable param_states/decoder/layers_22/self_attention/out/kernel/v_col                   size 3072         shape (3072,)                                  partition spec None
I0512 01:51:47.540605 140316961150976 utils.py:1372] Variable param_states/decoder/layers_22/self_attention/out/kernel/v_row                   size 2048         shape (2048,)                                  partition spec None
I0512 01:51:47.540645 140316961150976 utils.py:1372] Variable param_states/decoder/layers_22/self_attention/query/kernel/m                     size 1            shape (1,)                                     partition spec None
I0512 01:51:47.540685 140316961150976 utils.py:1372] Variable param_states/decoder/layers_22/self_attention/query/kernel/v                     size 1            shape (1,)                                     partition spec None
I0512 01:51:47.540732 140316961150976 utils.py:1372] Variable param_states/decoder/layers_22/self_attention/query/kernel/v_col                 size 3072         shape (3072,)                                  partition spec None
I0512 01:51:47.540775 140316961150976 utils.py:1372] Variable param_states/decoder/layers_22/self_attention/query/kernel/v_row                 size 2048         shape (2048,)                                  partition spec None
I0512 01:51:47.540816 140316961150976 utils.py:1372] Variable param_states/decoder/layers_22/self_attention/value/kernel/m                     size 1            shape (1,)                                     partition spec None
I0512 01:51:47.540857 140316961150976 utils.py:1372] Variable param_states/decoder/layers_22/self_attention/value/kernel/v                     size 1            shape (1,)                                     partition spec None
I0512 01:51:47.540897 140316961150976 utils.py:1372] Variable param_states/decoder/layers_22/self_attention/value/kernel/v_col                 size 3072         shape (3072,)                                  partition spec None
I0512 01:51:47.540937 140316961150976 utils.py:1372] Variable param_states/decoder/layers_22/self_attention/value/kernel/v_row                 size 2048         shape (2048,)                                  partition spec None
I0512 01:51:47.540977 140316961150976 utils.py:1372] Variable param_states/decoder/layers_23/extra_mlp/wi_0/kernel/m                           size 1            shape (1,)                                     partition spec None
I0512 01:51:47.541024 140316961150976 utils.py:1372] Variable param_states/decoder/layers_23/extra_mlp/wi_0/kernel/v                           size 1            shape (1,)                                     partition spec None
I0512 01:51:47.541067 140316961150976 utils.py:1372] Variable param_states/decoder/layers_23/extra_mlp/wi_0/kernel/v_col                       size 16384        shape (16384,)                                 partition spec None
I0512 01:51:47.541107 140316961150976 utils.py:1372] Variable param_states/decoder/layers_23/extra_mlp/wi_0/kernel/v_row                       size 2048         shape (2048,)                                  partition spec None
I0512 01:51:47.541147 140316961150976 utils.py:1372] Variable param_states/decoder/layers_23/extra_mlp/wi_1/kernel/m                           size 1            shape (1,)                                     partition spec None
I0512 01:51:47.541187 140316961150976 utils.py:1372] Variable param_states/decoder/layers_23/extra_mlp/wi_1/kernel/v                           size 1            shape (1,)                                     partition spec None
I0512 01:51:47.541235 140316961150976 utils.py:1372] Variable param_states/decoder/layers_23/extra_mlp/wi_1/kernel/v_col                       size 8192         shape (8192,)                                  partition spec None
I0512 01:51:47.541277 140316961150976 utils.py:1372] Variable param_states/decoder/layers_23/extra_mlp/wi_1/kernel/v_row                       size 2048         shape (2048,)                                  partition spec None
I0512 01:51:47.541317 140316961150976 utils.py:1372] Variable param_states/decoder/layers_23/extra_mlp/wo/kernel/m                             size 1            shape (1,)                                     partition spec None
I0512 01:51:47.541357 140316961150976 utils.py:1372] Variable param_states/decoder/layers_23/extra_mlp/wo/kernel/v                             size 1            shape (1,)                                     partition spec None
I0512 01:51:47.541397 140316961150976 utils.py:1372] Variable param_states/decoder/layers_23/extra_mlp/wo/kernel/v_col                         size 8192         shape (8192,)                                  partition spec None
I0512 01:51:47.541436 140316961150976 utils.py:1372] Variable param_states/decoder/layers_23/extra_mlp/wo/kernel/v_row                         size 2048         shape (2048,)                                  partition spec None
I0512 01:51:47.541477 140316961150976 utils.py:1372] Variable param_states/decoder/layers_23/mlp/expert/wi_0/kernel/m                          size 1            shape (1,)                                     partition spec None
I0512 01:51:47.541517 140316961150976 utils.py:1372] Variable param_states/decoder/layers_23/mlp/expert/wi_0/kernel/v                          size 1            shape (1,)                                     partition spec None
I0512 01:51:47.541590 140316961150976 utils.py:1372] Variable param_states/decoder/layers_23/mlp/expert/wi_0/kernel/v_col                      size 131072       shape (8, 16384)                               partition spec ('expert',)
I0512 01:51:47.541639 140316961150976 utils.py:1372] Variable param_states/decoder/layers_23/mlp/expert/wi_0/kernel/v_row                      size 16384        shape (8, 2048)                                partition spec ('expert',)
I0512 01:51:47.541681 140316961150976 utils.py:1372] Variable param_states/decoder/layers_23/mlp/expert/wi_1/kernel/m                          size 1            shape (1,)                                     partition spec None
I0512 01:51:47.541722 140316961150976 utils.py:1372] Variable param_states/decoder/layers_23/mlp/expert/wi_1/kernel/v                          size 1            shape (1,)                                     partition spec None
I0512 01:51:47.541771 140316961150976 utils.py:1372] Variable param_states/decoder/layers_23/mlp/expert/wi_1/kernel/v_col                      size 65536        shape (8, 8192)                                partition spec ('expert',)
I0512 01:51:47.541816 140316961150976 utils.py:1372] Variable param_states/decoder/layers_23/mlp/expert/wi_1/kernel/v_row                      size 16384        shape (8, 2048)                                partition spec ('expert',)
I0512 01:51:47.541857 140316961150976 utils.py:1372] Variable param_states/decoder/layers_23/mlp/expert/wo/kernel/m                            size 1            shape (1,)                                     partition spec None
I0512 01:51:47.541898 140316961150976 utils.py:1372] Variable param_states/decoder/layers_23/mlp/expert/wo/kernel/v                            size 1            shape (1,)                                     partition spec None
I0512 01:51:47.541939 140316961150976 utils.py:1372] Variable param_states/decoder/layers_23/mlp/expert/wo/kernel/v_col                        size 65536        shape (8, 8192)                                partition spec ('expert',)
I0512 01:51:47.541995 140316961150976 utils.py:1372] Variable param_states/decoder/layers_23/mlp/expert/wo/kernel/v_row                        size 16384        shape (8, 2048)                                partition spec ('expert',)
I0512 01:51:47.542046 140316961150976 utils.py:1372] Variable param_states/decoder/layers_23/mlp/router/router_weights/w/kernel/m              size 1            shape (1,)                                     partition spec None
I0512 01:51:47.542093 140316961150976 utils.py:1372] Variable param_states/decoder/layers_23/mlp/router/router_weights/w/kernel/v              size 16384        shape (embed=2048, unmodeled=8)                partition spec ('model', None)
I0512 01:51:47.542136 140316961150976 utils.py:1372] Variable param_states/decoder/layers_23/mlp/router/router_weights/w/kernel/v_col          size 1            shape (1,)                                     partition spec None
I0512 01:51:47.542177 140316961150976 utils.py:1372] Variable param_states/decoder/layers_23/mlp/router/router_weights/w/kernel/v_row          size 1            shape (1,)                                     partition spec None
I0512 01:51:47.542217 140316961150976 utils.py:1372] Variable param_states/decoder/layers_23/pre_extra_mlp_layer_norm/scale/m                  size 1            shape (1,)                                     partition spec None
I0512 01:51:47.542260 140316961150976 utils.py:1372] Variable param_states/decoder/layers_23/pre_extra_mlp_layer_norm/scale/v                  size 2048         shape (embed=2048)                             partition spec ('model',)
I0512 01:51:47.542302 140316961150976 utils.py:1372] Variable param_states/decoder/layers_23/pre_extra_mlp_layer_norm/scale/v_col              size 1            shape (1,)                                     partition spec None
I0512 01:51:47.542343 140316961150976 utils.py:1372] Variable param_states/decoder/layers_23/pre_extra_mlp_layer_norm/scale/v_row              size 1            shape (1,)                                     partition spec None
I0512 01:51:47.542382 140316961150976 utils.py:1372] Variable param_states/decoder/layers_23/pre_mlp_layer_norm/scale/m                        size 1            shape (1,)                                     partition spec None
I0512 01:51:47.542425 140316961150976 utils.py:1372] Variable param_states/decoder/layers_23/pre_mlp_layer_norm/scale/v                        size 2048         shape (embed=2048)                             partition spec ('model',)
I0512 01:51:47.542467 140316961150976 utils.py:1372] Variable param_states/decoder/layers_23/pre_mlp_layer_norm/scale/v_col                    size 1            shape (1,)                                     partition spec None
I0512 01:51:47.542508 140316961150976 utils.py:1372] Variable param_states/decoder/layers_23/pre_mlp_layer_norm/scale/v_row                    size 1            shape (1,)                                     partition spec None
I0512 01:51:47.542547 140316961150976 utils.py:1372] Variable param_states/decoder/layers_23/pre_self_attention_layer_norm/scale/m             size 1            shape (1,)                                     partition spec None
I0512 01:51:47.542589 140316961150976 utils.py:1372] Variable param_states/decoder/layers_23/pre_self_attention_layer_norm/scale/v             size 2048         shape (embed=2048)                             partition spec ('model',)
I0512 01:51:47.542632 140316961150976 utils.py:1372] Variable param_states/decoder/layers_23/pre_self_attention_layer_norm/scale/v_col         size 1            shape (1,)                                     partition spec None
I0512 01:51:47.542681 140316961150976 utils.py:1372] Variable param_states/decoder/layers_23/pre_self_attention_layer_norm/scale/v_row         size 1            shape (1,)                                     partition spec None
I0512 01:51:47.542724 140316961150976 utils.py:1372] Variable param_states/decoder/layers_23/self_attention/key/kernel/m                       size 1            shape (1,)                                     partition spec None
I0512 01:51:47.542766 140316961150976 utils.py:1372] Variable param_states/decoder/layers_23/self_attention/key/kernel/v                       size 1            shape (1,)                                     partition spec None
I0512 01:51:47.542809 140316961150976 utils.py:1372] Variable param_states/decoder/layers_23/self_attention/key/kernel/v_col                   size 3072         shape (3072,)                                  partition spec None
I0512 01:51:47.542850 140316961150976 utils.py:1372] Variable param_states/decoder/layers_23/self_attention/key/kernel/v_row                   size 2048         shape (2048,)                                  partition spec None
I0512 01:51:47.542891 140316961150976 utils.py:1372] Variable param_states/decoder/layers_23/self_attention/out/kernel/m                       size 1            shape (1,)                                     partition spec None
I0512 01:51:47.542932 140316961150976 utils.py:1372] Variable param_states/decoder/layers_23/self_attention/out/kernel/v                       size 1            shape (1,)                                     partition spec None
I0512 01:51:47.542972 140316961150976 utils.py:1372] Variable param_states/decoder/layers_23/self_attention/out/kernel/v_col                   size 3072         shape (3072,)                                  partition spec None
I0512 01:51:47.543021 140316961150976 utils.py:1372] Variable param_states/decoder/layers_23/self_attention/out/kernel/v_row                   size 2048         shape (2048,)                                  partition spec None
I0512 01:51:47.543063 140316961150976 utils.py:1372] Variable param_states/decoder/layers_23/self_attention/query/kernel/m                     size 1            shape (1,)                                     partition spec None
I0512 01:51:47.543103 140316961150976 utils.py:1372] Variable param_states/decoder/layers_23/self_attention/query/kernel/v                     size 1            shape (1,)                                     partition spec None
I0512 01:51:47.543144 140316961150976 utils.py:1372] Variable param_states/decoder/layers_23/self_attention/query/kernel/v_col                 size 3072         shape (3072,)                                  partition spec None
I0512 01:51:47.543184 140316961150976 utils.py:1372] Variable param_states/decoder/layers_23/self_attention/query/kernel/v_row                 size 2048         shape (2048,)                                  partition spec None
I0512 01:51:47.543223 140316961150976 utils.py:1372] Variable param_states/decoder/layers_23/self_attention/value/kernel/m                     size 1            shape (1,)                                     partition spec None
I0512 01:51:47.543263 140316961150976 utils.py:1372] Variable param_states/decoder/layers_23/self_attention/value/kernel/v                     size 1            shape (1,)                                     partition spec None
I0512 01:51:47.543303 140316961150976 utils.py:1372] Variable param_states/decoder/layers_23/self_attention/value/kernel/v_col                 size 3072         shape (3072,)                                  partition spec None
I0512 01:51:47.543344 140316961150976 utils.py:1372] Variable param_states/decoder/layers_23/self_attention/value/kernel/v_row                 size 2048         shape (2048,)                                  partition spec None
I0512 01:51:47.543383 140316961150976 utils.py:1372] Variable param_states/decoder/layers_3/mlp/wi_0/kernel/m                                  size 1            shape (1,)                                     partition spec None
I0512 01:51:47.543425 140316961150976 utils.py:1372] Variable param_states/decoder/layers_3/mlp/wi_0/kernel/v                                  size 1            shape (1,)                                     partition spec None
I0512 01:51:47.543465 140316961150976 utils.py:1372] Variable param_states/decoder/layers_3/mlp/wi_0/kernel/v_col                              size 16384        shape (16384,)                                 partition spec None
I0512 01:51:47.543505 140316961150976 utils.py:1372] Variable param_states/decoder/layers_3/mlp/wi_0/kernel/v_row                              size 2048         shape (2048,)                                  partition spec None
I0512 01:51:47.543546 140316961150976 utils.py:1372] Variable param_states/decoder/layers_3/mlp/wi_1/kernel/m                                  size 1            shape (1,)                                     partition spec None
I0512 01:51:47.543586 140316961150976 utils.py:1372] Variable param_states/decoder/layers_3/mlp/wi_1/kernel/v                                  size 1            shape (1,)                                     partition spec None
I0512 01:51:47.543626 140316961150976 utils.py:1372] Variable param_states/decoder/layers_3/mlp/wi_1/kernel/v_col                              size 8192         shape (8192,)                                  partition spec None
I0512 01:51:47.543666 140316961150976 utils.py:1372] Variable param_states/decoder/layers_3/mlp/wi_1/kernel/v_row                              size 2048         shape (2048,)                                  partition spec None
I0512 01:51:47.543706 140316961150976 utils.py:1372] Variable param_states/decoder/layers_3/mlp/wo/kernel/m                                    size 1            shape (1,)                                     partition spec None
I0512 01:51:47.543747 140316961150976 utils.py:1372] Variable param_states/decoder/layers_3/mlp/wo/kernel/v                                    size 1            shape (1,)                                     partition spec None
I0512 01:51:47.543790 140316961150976 utils.py:1372] Variable param_states/decoder/layers_3/mlp/wo/kernel/v_col                                size 8192         shape (8192,)                                  partition spec None
I0512 01:51:47.543834 140316961150976 utils.py:1372] Variable param_states/decoder/layers_3/mlp/wo/kernel/v_row                                size 2048         shape (2048,)                                  partition spec None
I0512 01:51:47.543888 140316961150976 utils.py:1372] Variable param_states/decoder/layers_3/pre_mlp_layer_norm/scale/m                         size 1            shape (1,)                                     partition spec None
I0512 01:51:47.543940 140316961150976 utils.py:1372] Variable param_states/decoder/layers_3/pre_mlp_layer_norm/scale/v                         size 2048         shape (embed=2048)                             partition spec ('model',)
I0512 01:51:47.544001 140316961150976 utils.py:1372] Variable param_states/decoder/layers_3/pre_mlp_layer_norm/scale/v_col                     size 1            shape (1,)                                     partition spec None
I0512 01:51:47.544054 140316961150976 utils.py:1372] Variable param_states/decoder/layers_3/pre_mlp_layer_norm/scale/v_row                     size 1            shape (1,)                                     partition spec None
I0512 01:51:47.544095 140316961150976 utils.py:1372] Variable param_states/decoder/layers_3/pre_self_attention_layer_norm/scale/m              size 1            shape (1,)                                     partition spec None
I0512 01:51:47.544139 140316961150976 utils.py:1372] Variable param_states/decoder/layers_3/pre_self_attention_layer_norm/scale/v              size 2048         shape (embed=2048)                             partition spec ('model',)
I0512 01:51:47.544181 140316961150976 utils.py:1372] Variable param_states/decoder/layers_3/pre_self_attention_layer_norm/scale/v_col          size 1            shape (1,)                                     partition spec None
I0512 01:51:47.544221 140316961150976 utils.py:1372] Variable param_states/decoder/layers_3/pre_self_attention_layer_norm/scale/v_row          size 1            shape (1,)                                     partition spec None
I0512 01:51:47.544261 140316961150976 utils.py:1372] Variable param_states/decoder/layers_3/self_attention/key/kernel/m                        size 1            shape (1,)                                     partition spec None
I0512 01:51:47.544301 140316961150976 utils.py:1372] Variable param_states/decoder/layers_3/self_attention/key/kernel/v                        size 1            shape (1,)                                     partition spec None
I0512 01:51:47.544341 140316961150976 utils.py:1372] Variable param_states/decoder/layers_3/self_attention/key/kernel/v_col                    size 3072         shape (3072,)                                  partition spec None
I0512 01:51:47.544382 140316961150976 utils.py:1372] Variable param_states/decoder/layers_3/self_attention/key/kernel/v_row                    size 2048         shape (2048,)                                  partition spec None
I0512 01:51:47.544422 140316961150976 utils.py:1372] Variable param_states/decoder/layers_3/self_attention/out/kernel/m                        size 1            shape (1,)                                     partition spec None
I0512 01:51:47.544462 140316961150976 utils.py:1372] Variable param_states/decoder/layers_3/self_attention/out/kernel/v                        size 1            shape (1,)                                     partition spec None
I0512 01:51:47.544512 140316961150976 utils.py:1372] Variable param_states/decoder/layers_3/self_attention/out/kernel/v_col                    size 3072         shape (3072,)                                  partition spec None
I0512 01:51:47.544559 140316961150976 utils.py:1372] Variable param_states/decoder/layers_3/self_attention/out/kernel/v_row                    size 2048         shape (2048,)                                  partition spec None
I0512 01:51:47.544599 140316961150976 utils.py:1372] Variable param_states/decoder/layers_3/self_attention/query/kernel/m                      size 1            shape (1,)                                     partition spec None
I0512 01:51:47.544640 140316961150976 utils.py:1372] Variable param_states/decoder/layers_3/self_attention/query/kernel/v                      size 1            shape (1,)                                     partition spec None
I0512 01:51:47.544680 140316961150976 utils.py:1372] Variable param_states/decoder/layers_3/self_attention/query/kernel/v_col                  size 3072         shape (3072,)                                  partition spec None
I0512 01:51:47.544720 140316961150976 utils.py:1372] Variable param_states/decoder/layers_3/self_attention/query/kernel/v_row                  size 2048         shape (2048,)                                  partition spec None
I0512 01:51:47.544761 140316961150976 utils.py:1372] Variable param_states/decoder/layers_3/self_attention/value/kernel/m                      size 1            shape (1,)                                     partition spec None
I0512 01:51:47.544805 140316961150976 utils.py:1372] Variable param_states/decoder/layers_3/self_attention/value/kernel/v                      size 1            shape (1,)                                     partition spec None
I0512 01:51:47.544846 140316961150976 utils.py:1372] Variable param_states/decoder/layers_3/self_attention/value/kernel/v_col                  size 3072         shape (3072,)                                  partition spec None
I0512 01:51:47.544886 140316961150976 utils.py:1372] Variable param_states/decoder/layers_3/self_attention/value/kernel/v_row                  size 2048         shape (2048,)                                  partition spec None
I0512 01:51:47.544926 140316961150976 utils.py:1372] Variable param_states/decoder/layers_4/mlp/wi_0/kernel/m                                  size 1            shape (1,)                                     partition spec None
I0512 01:51:47.544967 140316961150976 utils.py:1372] Variable param_states/decoder/layers_4/mlp/wi_0/kernel/v                                  size 1            shape (1,)                                     partition spec None
I0512 01:51:47.545024 140316961150976 utils.py:1372] Variable param_states/decoder/layers_4/mlp/wi_0/kernel/v_col                              size 16384        shape (16384,)                                 partition spec None
I0512 01:51:47.545071 140316961150976 utils.py:1372] Variable param_states/decoder/layers_4/mlp/wi_0/kernel/v_row                              size 2048         shape (2048,)                                  partition spec None
I0512 01:51:47.545122 140316961150976 utils.py:1372] Variable param_states/decoder/layers_4/mlp/wi_1/kernel/m                                  size 1            shape (1,)                                     partition spec None
I0512 01:51:47.545165 140316961150976 utils.py:1372] Variable param_states/decoder/layers_4/mlp/wi_1/kernel/v                                  size 1            shape (1,)                                     partition spec None
I0512 01:51:47.545206 140316961150976 utils.py:1372] Variable param_states/decoder/layers_4/mlp/wi_1/kernel/v_col                              size 8192         shape (8192,)                                  partition spec None
I0512 01:51:47.545247 140316961150976 utils.py:1372] Variable param_states/decoder/layers_4/mlp/wi_1/kernel/v_row                              size 2048         shape (2048,)                                  partition spec None
I0512 01:51:47.545287 140316961150976 utils.py:1372] Variable param_states/decoder/layers_4/mlp/wo/kernel/m                                    size 1            shape (1,)                                     partition spec None
I0512 01:51:47.545327 140316961150976 utils.py:1372] Variable param_states/decoder/layers_4/mlp/wo/kernel/v                                    size 1            shape (1,)                                     partition spec None
I0512 01:51:47.545367 140316961150976 utils.py:1372] Variable param_states/decoder/layers_4/mlp/wo/kernel/v_col                                size 8192         shape (8192,)                                  partition spec None
I0512 01:51:47.545408 140316961150976 utils.py:1372] Variable param_states/decoder/layers_4/mlp/wo/kernel/v_row                                size 2048         shape (2048,)                                  partition spec None
I0512 01:51:47.545448 140316961150976 utils.py:1372] Variable param_states/decoder/layers_4/pre_mlp_layer_norm/scale/m                         size 1            shape (1,)                                     partition spec None
I0512 01:51:47.545491 140316961150976 utils.py:1372] Variable param_states/decoder/layers_4/pre_mlp_layer_norm/scale/v                         size 2048         shape (embed=2048)                             partition spec ('model',)
I0512 01:51:47.545534 140316961150976 utils.py:1372] Variable param_states/decoder/layers_4/pre_mlp_layer_norm/scale/v_col                     size 1            shape (1,)                                     partition spec None
I0512 01:51:47.545607 140316961150976 utils.py:1372] Variable param_states/decoder/layers_4/pre_mlp_layer_norm/scale/v_row                     size 1            shape (1,)                                     partition spec None
I0512 01:51:47.545650 140316961150976 utils.py:1372] Variable param_states/decoder/layers_4/pre_self_attention_layer_norm/scale/m              size 1            shape (1,)                                     partition spec None
I0512 01:51:47.545694 140316961150976 utils.py:1372] Variable param_states/decoder/layers_4/pre_self_attention_layer_norm/scale/v              size 2048         shape (embed=2048)                             partition spec ('model',)
I0512 01:51:47.545737 140316961150976 utils.py:1372] Variable param_states/decoder/layers_4/pre_self_attention_layer_norm/scale/v_col          size 1            shape (1,)                                     partition spec None
I0512 01:51:47.545779 140316961150976 utils.py:1372] Variable param_states/decoder/layers_4/pre_self_attention_layer_norm/scale/v_row          size 1            shape (1,)                                     partition spec None
I0512 01:51:47.545821 140316961150976 utils.py:1372] Variable param_states/decoder/layers_4/self_attention/key/kernel/m                        size 1            shape (1,)                                     partition spec None
I0512 01:51:47.545863 140316961150976 utils.py:1372] Variable param_states/decoder/layers_4/self_attention/key/kernel/v                        size 1            shape (1,)                                     partition spec None
I0512 01:51:47.545904 140316961150976 utils.py:1372] Variable param_states/decoder/layers_4/self_attention/key/kernel/v_col                    size 3072         shape (3072,)                                  partition spec None
I0512 01:51:47.545947 140316961150976 utils.py:1372] Variable param_states/decoder/layers_4/self_attention/key/kernel/v_row                    size 2048         shape (2048,)                                  partition spec None
I0512 01:51:47.545989 140316961150976 utils.py:1372] Variable param_states/decoder/layers_4/self_attention/out/kernel/m                        size 1            shape (1,)                                     partition spec None
I0512 01:51:47.546058 140316961150976 utils.py:1372] Variable param_states/decoder/layers_4/self_attention/out/kernel/v                        size 1            shape (1,)                                     partition spec None
I0512 01:51:47.546112 140316961150976 utils.py:1372] Variable param_states/decoder/layers_4/self_attention/out/kernel/v_col                    size 3072         shape (3072,)                                  partition spec None
I0512 01:51:47.546155 140316961150976 utils.py:1372] Variable param_states/decoder/layers_4/self_attention/out/kernel/v_row                    size 2048         shape (2048,)                                  partition spec None
I0512 01:51:47.546196 140316961150976 utils.py:1372] Variable param_states/decoder/layers_4/self_attention/query/kernel/m                      size 1            shape (1,)                                     partition spec None
I0512 01:51:47.546237 140316961150976 utils.py:1372] Variable param_states/decoder/layers_4/self_attention/query/kernel/v                      size 1            shape (1,)                                     partition spec None
I0512 01:51:47.546278 140316961150976 utils.py:1372] Variable param_states/decoder/layers_4/self_attention/query/kernel/v_col                  size 3072         shape (3072,)                                  partition spec None
I0512 01:51:47.546318 140316961150976 utils.py:1372] Variable param_states/decoder/layers_4/self_attention/query/kernel/v_row                  size 2048         shape (2048,)                                  partition spec None
I0512 01:51:47.546370 140316961150976 utils.py:1372] Variable param_states/decoder/layers_4/self_attention/value/kernel/m                      size 1            shape (1,)                                     partition spec None
I0512 01:51:47.546433 140316961150976 utils.py:1372] Variable param_states/decoder/layers_4/self_attention/value/kernel/v                      size 1            shape (1,)                                     partition spec None
I0512 01:51:47.546492 140316961150976 utils.py:1372] Variable param_states/decoder/layers_4/self_attention/value/kernel/v_col                  size 3072         shape (3072,)                                  partition spec None
I0512 01:51:47.546544 140316961150976 utils.py:1372] Variable param_states/decoder/layers_4/self_attention/value/kernel/v_row                  size 2048         shape (2048,)                                  partition spec None
I0512 01:51:47.546607 140316961150976 utils.py:1372] Variable param_states/decoder/layers_5/extra_mlp/wi_0/kernel/m                            size 1            shape (1,)                                     partition spec None
I0512 01:51:47.546673 140316961150976 utils.py:1372] Variable param_states/decoder/layers_5/extra_mlp/wi_0/kernel/v                            size 1            shape (1,)                                     partition spec None
I0512 01:51:47.546745 140316961150976 utils.py:1372] Variable param_states/decoder/layers_5/extra_mlp/wi_0/kernel/v_col                        size 16384        shape (16384,)                                 partition spec None
I0512 01:51:47.546810 140316961150976 utils.py:1372] Variable param_states/decoder/layers_5/extra_mlp/wi_0/kernel/v_row                        size 2048         shape (2048,)                                  partition spec None
I0512 01:51:47.546870 140316961150976 utils.py:1372] Variable param_states/decoder/layers_5/extra_mlp/wi_1/kernel/m                            size 1            shape (1,)                                     partition spec None
I0512 01:51:47.546918 140316961150976 utils.py:1372] Variable param_states/decoder/layers_5/extra_mlp/wi_1/kernel/v                            size 1            shape (1,)                                     partition spec None
I0512 01:51:47.546961 140316961150976 utils.py:1372] Variable param_states/decoder/layers_5/extra_mlp/wi_1/kernel/v_col                        size 8192         shape (8192,)                                  partition spec None
I0512 01:51:47.547011 140316961150976 utils.py:1372] Variable param_states/decoder/layers_5/extra_mlp/wi_1/kernel/v_row                        size 2048         shape (2048,)                                  partition spec None
I0512 01:51:47.547061 140316961150976 utils.py:1372] Variable param_states/decoder/layers_5/extra_mlp/wo/kernel/m                              size 1            shape (1,)                                     partition spec None
I0512 01:51:47.547118 140316961150976 utils.py:1372] Variable param_states/decoder/layers_5/extra_mlp/wo/kernel/v                              size 1            shape (1,)                                     partition spec None
I0512 01:51:47.547169 140316961150976 utils.py:1372] Variable param_states/decoder/layers_5/extra_mlp/wo/kernel/v_col                          size 8192         shape (8192,)                                  partition spec None
I0512 01:51:47.547212 140316961150976 utils.py:1372] Variable param_states/decoder/layers_5/extra_mlp/wo/kernel/v_row                          size 2048         shape (2048,)                                  partition spec None
I0512 01:51:47.547253 140316961150976 utils.py:1372] Variable param_states/decoder/layers_5/mlp/expert/wi_0/kernel/m                           size 1            shape (1,)                                     partition spec None
I0512 01:51:47.547295 140316961150976 utils.py:1372] Variable param_states/decoder/layers_5/mlp/expert/wi_0/kernel/v                           size 1            shape (1,)                                     partition spec None
I0512 01:51:47.547338 140316961150976 utils.py:1372] Variable param_states/decoder/layers_5/mlp/expert/wi_0/kernel/v_col                       size 131072       shape (8, 16384)                               partition spec ('expert',)
I0512 01:51:47.547396 140316961150976 utils.py:1372] Variable param_states/decoder/layers_5/mlp/expert/wi_0/kernel/v_row                       size 16384        shape (8, 2048)                                partition spec ('expert',)
I0512 01:51:47.547454 140316961150976 utils.py:1372] Variable param_states/decoder/layers_5/mlp/expert/wi_1/kernel/m                           size 1            shape (1,)                                     partition spec None
I0512 01:51:47.547504 140316961150976 utils.py:1372] Variable param_states/decoder/layers_5/mlp/expert/wi_1/kernel/v                           size 1            shape (1,)                                     partition spec None
I0512 01:51:47.547561 140316961150976 utils.py:1372] Variable param_states/decoder/layers_5/mlp/expert/wi_1/kernel/v_col                       size 65536        shape (8, 8192)                                partition spec ('expert',)
I0512 01:51:47.547608 140316961150976 utils.py:1372] Variable param_states/decoder/layers_5/mlp/expert/wi_1/kernel/v_row                       size 16384        shape (8, 2048)                                partition spec ('expert',)
I0512 01:51:47.547651 140316961150976 utils.py:1372] Variable param_states/decoder/layers_5/mlp/expert/wo/kernel/m                             size 1            shape (1,)                                     partition spec None
I0512 01:51:47.547692 140316961150976 utils.py:1372] Variable param_states/decoder/layers_5/mlp/expert/wo/kernel/v                             size 1            shape (1,)                                     partition spec None
I0512 01:51:47.547744 140316961150976 utils.py:1372] Variable param_states/decoder/layers_5/mlp/expert/wo/kernel/v_col                         size 65536        shape (8, 8192)                                partition spec ('expert',)
I0512 01:51:47.547798 140316961150976 utils.py:1372] Variable param_states/decoder/layers_5/mlp/expert/wo/kernel/v_row                         size 16384        shape (8, 2048)                                partition spec ('expert',)
I0512 01:51:47.547842 140316961150976 utils.py:1372] Variable param_states/decoder/layers_5/mlp/router/router_weights/w/kernel/m               size 1            shape (1,)                                     partition spec None
I0512 01:51:47.547889 140316961150976 utils.py:1372] Variable param_states/decoder/layers_5/mlp/router/router_weights/w/kernel/v               size 16384        shape (embed=2048, unmodeled=8)                partition spec ('model', None)
I0512 01:51:47.547935 140316961150976 utils.py:1372] Variable param_states/decoder/layers_5/mlp/router/router_weights/w/kernel/v_col           size 1            shape (1,)                                     partition spec None
I0512 01:51:47.547979 140316961150976 utils.py:1372] Variable param_states/decoder/layers_5/mlp/router/router_weights/w/kernel/v_row           size 1            shape (1,)                                     partition spec None
I0512 01:51:47.548027 140316961150976 utils.py:1372] Variable param_states/decoder/layers_5/pre_extra_mlp_layer_norm/scale/m                   size 1            shape (1,)                                     partition spec None
I0512 01:51:47.548073 140316961150976 utils.py:1372] Variable param_states/decoder/layers_5/pre_extra_mlp_layer_norm/scale/v                   size 2048         shape (embed=2048)                             partition spec ('model',)
I0512 01:51:47.548116 140316961150976 utils.py:1372] Variable param_states/decoder/layers_5/pre_extra_mlp_layer_norm/scale/v_col               size 1            shape (1,)                                     partition spec None
I0512 01:51:47.548157 140316961150976 utils.py:1372] Variable param_states/decoder/layers_5/pre_extra_mlp_layer_norm/scale/v_row               size 1            shape (1,)                                     partition spec None
I0512 01:51:47.548198 140316961150976 utils.py:1372] Variable param_states/decoder/layers_5/pre_mlp_layer_norm/scale/m                         size 1            shape (1,)                                     partition spec None
I0512 01:51:47.548240 140316961150976 utils.py:1372] Variable param_states/decoder/layers_5/pre_mlp_layer_norm/scale/v                         size 2048         shape (embed=2048)                             partition spec ('model',)
I0512 01:51:47.548283 140316961150976 utils.py:1372] Variable param_states/decoder/layers_5/pre_mlp_layer_norm/scale/v_col                     size 1            shape (1,)                                     partition spec None
I0512 01:51:47.548325 140316961150976 utils.py:1372] Variable param_states/decoder/layers_5/pre_mlp_layer_norm/scale/v_row                     size 1            shape (1,)                                     partition spec None
I0512 01:51:47.548384 140316961150976 utils.py:1372] Variable param_states/decoder/layers_5/pre_self_attention_layer_norm/scale/m              size 1            shape (1,)                                     partition spec None
I0512 01:51:47.548433 140316961150976 utils.py:1372] Variable param_states/decoder/layers_5/pre_self_attention_layer_norm/scale/v              size 2048         shape (embed=2048)                             partition spec ('model',)
I0512 01:51:47.548476 140316961150976 utils.py:1372] Variable param_states/decoder/layers_5/pre_self_attention_layer_norm/scale/v_col          size 1            shape (1,)                                     partition spec None
I0512 01:51:47.548517 140316961150976 utils.py:1372] Variable param_states/decoder/layers_5/pre_self_attention_layer_norm/scale/v_row          size 1            shape (1,)                                     partition spec None
I0512 01:51:47.548557 140316961150976 utils.py:1372] Variable param_states/decoder/layers_5/self_attention/key/kernel/m                        size 1            shape (1,)                                     partition spec None
I0512 01:51:47.548597 140316961150976 utils.py:1372] Variable param_states/decoder/layers_5/self_attention/key/kernel/v                        size 1            shape (1,)                                     partition spec None
I0512 01:51:47.548647 140316961150976 utils.py:1372] Variable param_states/decoder/layers_5/self_attention/key/kernel/v_col                    size 3072         shape (3072,)                                  partition spec None
I0512 01:51:47.548690 140316961150976 utils.py:1372] Variable param_states/decoder/layers_5/self_attention/key/kernel/v_row                    size 2048         shape (2048,)                                  partition spec None
I0512 01:51:47.548729 140316961150976 utils.py:1372] Variable param_states/decoder/layers_5/self_attention/out/kernel/m                        size 1            shape (1,)                                     partition spec None
I0512 01:51:47.548771 140316961150976 utils.py:1372] Variable param_states/decoder/layers_5/self_attention/out/kernel/v                        size 1            shape (1,)                                     partition spec None
I0512 01:51:47.548813 140316961150976 utils.py:1372] Variable param_states/decoder/layers_5/self_attention/out/kernel/v_col                    size 3072         shape (3072,)                                  partition spec None
I0512 01:51:47.548852 140316961150976 utils.py:1372] Variable param_states/decoder/layers_5/self_attention/out/kernel/v_row                    size 2048         shape (2048,)                                  partition spec None
I0512 01:51:47.548892 140316961150976 utils.py:1372] Variable param_states/decoder/layers_5/self_attention/query/kernel/m                      size 1            shape (1,)                                     partition spec None
I0512 01:51:47.548932 140316961150976 utils.py:1372] Variable param_states/decoder/layers_5/self_attention/query/kernel/v                      size 1            shape (1,)                                     partition spec None
I0512 01:51:47.548980 140316961150976 utils.py:1372] Variable param_states/decoder/layers_5/self_attention/query/kernel/v_col                  size 3072         shape (3072,)                                  partition spec None
I0512 01:51:47.549029 140316961150976 utils.py:1372] Variable param_states/decoder/layers_5/self_attention/query/kernel/v_row                  size 2048         shape (2048,)                                  partition spec None
I0512 01:51:47.549070 140316961150976 utils.py:1372] Variable param_states/decoder/layers_5/self_attention/value/kernel/m                      size 1            shape (1,)                                     partition spec None
I0512 01:51:47.549109 140316961150976 utils.py:1372] Variable param_states/decoder/layers_5/self_attention/value/kernel/v                      size 1            shape (1,)                                     partition spec None
I0512 01:51:47.549149 140316961150976 utils.py:1372] Variable param_states/decoder/layers_5/self_attention/value/kernel/v_col                  size 3072         shape (3072,)                                  partition spec None
I0512 01:51:47.549188 140316961150976 utils.py:1372] Variable param_states/decoder/layers_5/self_attention/value/kernel/v_row                  size 2048         shape (2048,)                                  partition spec None
I0512 01:51:47.549226 140316961150976 utils.py:1372] Variable param_states/decoder/layers_6/mlp/wi_0/kernel/m                                  size 1            shape (1,)                                     partition spec None
I0512 01:51:47.549267 140316961150976 utils.py:1372] Variable param_states/decoder/layers_6/mlp/wi_0/kernel/v                                  size 1            shape (1,)                                     partition spec None
I0512 01:51:47.549307 140316961150976 utils.py:1372] Variable param_states/decoder/layers_6/mlp/wi_0/kernel/v_col                              size 16384        shape (16384,)                                 partition spec None
I0512 01:51:47.549346 140316961150976 utils.py:1372] Variable param_states/decoder/layers_6/mlp/wi_0/kernel/v_row                              size 2048         shape (2048,)                                  partition spec None
I0512 01:51:47.549385 140316961150976 utils.py:1372] Variable param_states/decoder/layers_6/mlp/wi_1/kernel/m                                  size 1            shape (1,)                                     partition spec None
I0512 01:51:47.549424 140316961150976 utils.py:1372] Variable param_states/decoder/layers_6/mlp/wi_1/kernel/v                                  size 1            shape (1,)                                     partition spec None
I0512 01:51:47.549463 140316961150976 utils.py:1372] Variable param_states/decoder/layers_6/mlp/wi_1/kernel/v_col                              size 8192         shape (8192,)                                  partition spec None
I0512 01:51:47.549502 140316961150976 utils.py:1372] Variable param_states/decoder/layers_6/mlp/wi_1/kernel/v_row                              size 2048         shape (2048,)                                  partition spec None
I0512 01:51:47.549572 140316961150976 utils.py:1372] Variable param_states/decoder/layers_6/mlp/wo/kernel/m                                    size 1            shape (1,)                                     partition spec None
I0512 01:51:47.549622 140316961150976 utils.py:1372] Variable param_states/decoder/layers_6/mlp/wo/kernel/v                                    size 1            shape (1,)                                     partition spec None
I0512 01:51:47.549663 140316961150976 utils.py:1372] Variable param_states/decoder/layers_6/mlp/wo/kernel/v_col                                size 8192         shape (8192,)                                  partition spec None
I0512 01:51:47.549702 140316961150976 utils.py:1372] Variable param_states/decoder/layers_6/mlp/wo/kernel/v_row                                size 2048         shape (2048,)                                  partition spec None
I0512 01:51:47.549741 140316961150976 utils.py:1372] Variable param_states/decoder/layers_6/pre_mlp_layer_norm/scale/m                         size 1            shape (1,)                                     partition spec None
I0512 01:51:47.549787 140316961150976 utils.py:1372] Variable param_states/decoder/layers_6/pre_mlp_layer_norm/scale/v                         size 2048         shape (embed=2048)                             partition spec ('model',)
I0512 01:51:47.549830 140316961150976 utils.py:1372] Variable param_states/decoder/layers_6/pre_mlp_layer_norm/scale/v_col                     size 1            shape (1,)                                     partition spec None
I0512 01:51:47.549870 140316961150976 utils.py:1372] Variable param_states/decoder/layers_6/pre_mlp_layer_norm/scale/v_row                     size 1            shape (1,)                                     partition spec None
I0512 01:51:47.549909 140316961150976 utils.py:1372] Variable param_states/decoder/layers_6/pre_self_attention_layer_norm/scale/m              size 1            shape (1,)                                     partition spec None
I0512 01:51:47.549951 140316961150976 utils.py:1372] Variable param_states/decoder/layers_6/pre_self_attention_layer_norm/scale/v              size 2048         shape (embed=2048)                             partition spec ('model',)
I0512 01:51:47.549993 140316961150976 utils.py:1372] Variable param_states/decoder/layers_6/pre_self_attention_layer_norm/scale/v_col          size 1            shape (1,)                                     partition spec None
I0512 01:51:47.550040 140316961150976 utils.py:1372] Variable param_states/decoder/layers_6/pre_self_attention_layer_norm/scale/v_row          size 1            shape (1,)                                     partition spec None
I0512 01:51:47.550080 140316961150976 utils.py:1372] Variable param_states/decoder/layers_6/self_attention/key/kernel/m                        size 1            shape (1,)                                     partition spec None
I0512 01:51:47.550120 140316961150976 utils.py:1372] Variable param_states/decoder/layers_6/self_attention/key/kernel/v                        size 1            shape (1,)                                     partition spec None
I0512 01:51:47.550170 140316961150976 utils.py:1372] Variable param_states/decoder/layers_6/self_attention/key/kernel/v_col                    size 3072         shape (3072,)                                  partition spec None
I0512 01:51:47.550212 140316961150976 utils.py:1372] Variable param_states/decoder/layers_6/self_attention/key/kernel/v_row                    size 2048         shape (2048,)                                  partition spec None
I0512 01:51:47.550251 140316961150976 utils.py:1372] Variable param_states/decoder/layers_6/self_attention/out/kernel/m                        size 1            shape (1,)                                     partition spec None
I0512 01:51:47.550290 140316961150976 utils.py:1372] Variable param_states/decoder/layers_6/self_attention/out/kernel/v                        size 1            shape (1,)                                     partition spec None
I0512 01:51:47.550329 140316961150976 utils.py:1372] Variable param_states/decoder/layers_6/self_attention/out/kernel/v_col                    size 3072         shape (3072,)                                  partition spec None
I0512 01:51:47.550386 140316961150976 utils.py:1372] Variable param_states/decoder/layers_6/self_attention/out/kernel/v_row                    size 2048         shape (2048,)                                  partition spec None
I0512 01:51:47.550428 140316961150976 utils.py:1372] Variable param_states/decoder/layers_6/self_attention/query/kernel/m                      size 1            shape (1,)                                     partition spec None
I0512 01:51:47.550468 140316961150976 utils.py:1372] Variable param_states/decoder/layers_6/self_attention/query/kernel/v                      size 1            shape (1,)                                     partition spec None
I0512 01:51:47.550517 140316961150976 utils.py:1372] Variable param_states/decoder/layers_6/self_attention/query/kernel/v_col                  size 3072         shape (3072,)                                  partition spec None
I0512 01:51:47.550559 140316961150976 utils.py:1372] Variable param_states/decoder/layers_6/self_attention/query/kernel/v_row                  size 2048         shape (2048,)                                  partition spec None
I0512 01:51:47.550597 140316961150976 utils.py:1372] Variable param_states/decoder/layers_6/self_attention/value/kernel/m                      size 1            shape (1,)                                     partition spec None
I0512 01:51:47.550637 140316961150976 utils.py:1372] Variable param_states/decoder/layers_6/self_attention/value/kernel/v                      size 1            shape (1,)                                     partition spec None
I0512 01:51:47.550675 140316961150976 utils.py:1372] Variable param_states/decoder/layers_6/self_attention/value/kernel/v_col                  size 3072         shape (3072,)                                  partition spec None
I0512 01:51:47.550714 140316961150976 utils.py:1372] Variable param_states/decoder/layers_6/self_attention/value/kernel/v_row                  size 2048         shape (2048,)                                  partition spec None
I0512 01:51:47.550752 140316961150976 utils.py:1372] Variable param_states/decoder/layers_7/mlp/wi_0/kernel/m                                  size 1            shape (1,)                                     partition spec None
I0512 01:51:47.550797 140316961150976 utils.py:1372] Variable param_states/decoder/layers_7/mlp/wi_0/kernel/v                                  size 1            shape (1,)                                     partition spec None
I0512 01:51:47.550838 140316961150976 utils.py:1372] Variable param_states/decoder/layers_7/mlp/wi_0/kernel/v_col                              size 16384        shape (16384,)                                 partition spec None
I0512 01:51:47.550878 140316961150976 utils.py:1372] Variable param_states/decoder/layers_7/mlp/wi_0/kernel/v_row                              size 2048         shape (2048,)                                  partition spec None
I0512 01:51:47.550916 140316961150976 utils.py:1372] Variable param_states/decoder/layers_7/mlp/wi_1/kernel/m                                  size 1            shape (1,)                                     partition spec None
I0512 01:51:47.550956 140316961150976 utils.py:1372] Variable param_states/decoder/layers_7/mlp/wi_1/kernel/v                                  size 1            shape (1,)                                     partition spec None
I0512 01:51:47.550995 140316961150976 utils.py:1372] Variable param_states/decoder/layers_7/mlp/wi_1/kernel/v_col                              size 8192         shape (8192,)                                  partition spec None
I0512 01:51:47.551043 140316961150976 utils.py:1372] Variable param_states/decoder/layers_7/mlp/wi_1/kernel/v_row                              size 2048         shape (2048,)                                  partition spec None
I0512 01:51:47.551082 140316961150976 utils.py:1372] Variable param_states/decoder/layers_7/mlp/wo/kernel/m                                    size 1            shape (1,)                                     partition spec None
I0512 01:51:47.551121 140316961150976 utils.py:1372] Variable param_states/decoder/layers_7/mlp/wo/kernel/v                                    size 1            shape (1,)                                     partition spec None
I0512 01:51:47.551161 140316961150976 utils.py:1372] Variable param_states/decoder/layers_7/mlp/wo/kernel/v_col                                size 8192         shape (8192,)                                  partition spec None
I0512 01:51:47.551199 140316961150976 utils.py:1372] Variable param_states/decoder/layers_7/mlp/wo/kernel/v_row                                size 2048         shape (2048,)                                  partition spec None
I0512 01:51:47.551238 140316961150976 utils.py:1372] Variable param_states/decoder/layers_7/pre_mlp_layer_norm/scale/m                         size 1            shape (1,)                                     partition spec None
I0512 01:51:47.551280 140316961150976 utils.py:1372] Variable param_states/decoder/layers_7/pre_mlp_layer_norm/scale/v                         size 2048         shape (embed=2048)                             partition spec ('model',)
I0512 01:51:47.551322 140316961150976 utils.py:1372] Variable param_states/decoder/layers_7/pre_mlp_layer_norm/scale/v_col                     size 1            shape (1,)                                     partition spec None
I0512 01:51:47.551361 140316961150976 utils.py:1372] Variable param_states/decoder/layers_7/pre_mlp_layer_norm/scale/v_row                     size 1            shape (1,)                                     partition spec None
I0512 01:51:47.551400 140316961150976 utils.py:1372] Variable param_states/decoder/layers_7/pre_self_attention_layer_norm/scale/m              size 1            shape (1,)                                     partition spec None
I0512 01:51:47.551441 140316961150976 utils.py:1372] Variable param_states/decoder/layers_7/pre_self_attention_layer_norm/scale/v              size 2048         shape (embed=2048)                             partition spec ('model',)
I0512 01:51:47.551482 140316961150976 utils.py:1372] Variable param_states/decoder/layers_7/pre_self_attention_layer_norm/scale/v_col          size 1            shape (1,)                                     partition spec None
I0512 01:51:47.551521 140316961150976 utils.py:1372] Variable param_states/decoder/layers_7/pre_self_attention_layer_norm/scale/v_row          size 1            shape (1,)                                     partition spec None
I0512 01:51:47.551560 140316961150976 utils.py:1372] Variable param_states/decoder/layers_7/self_attention/key/kernel/m                        size 1            shape (1,)                                     partition spec None
I0512 01:51:47.551599 140316961150976 utils.py:1372] Variable param_states/decoder/layers_7/self_attention/key/kernel/v                        size 1            shape (1,)                                     partition spec None
I0512 01:51:47.551648 140316961150976 utils.py:1372] Variable param_states/decoder/layers_7/self_attention/key/kernel/v_col                    size 3072         shape (3072,)                                  partition spec None
I0512 01:51:47.551699 140316961150976 utils.py:1372] Variable param_states/decoder/layers_7/self_attention/key/kernel/v_row                    size 2048         shape (2048,)                                  partition spec None
I0512 01:51:47.551750 140316961150976 utils.py:1372] Variable param_states/decoder/layers_7/self_attention/out/kernel/m                        size 1            shape (1,)                                     partition spec None
I0512 01:51:47.551798 140316961150976 utils.py:1372] Variable param_states/decoder/layers_7/self_attention/out/kernel/v                        size 1            shape (1,)                                     partition spec None
I0512 01:51:47.551842 140316961150976 utils.py:1372] Variable param_states/decoder/layers_7/self_attention/out/kernel/v_col                    size 3072         shape (3072,)                                  partition spec None
I0512 01:51:47.551886 140316961150976 utils.py:1372] Variable param_states/decoder/layers_7/self_attention/out/kernel/v_row                    size 2048         shape (2048,)                                  partition spec None
I0512 01:51:47.551929 140316961150976 utils.py:1372] Variable param_states/decoder/layers_7/self_attention/query/kernel/m                      size 1            shape (1,)                                     partition spec None
I0512 01:51:47.551973 140316961150976 utils.py:1372] Variable param_states/decoder/layers_7/self_attention/query/kernel/v                      size 1            shape (1,)                                     partition spec None
I0512 01:51:47.552026 140316961150976 utils.py:1372] Variable param_states/decoder/layers_7/self_attention/query/kernel/v_col                  size 3072         shape (3072,)                                  partition spec None
I0512 01:51:47.552072 140316961150976 utils.py:1372] Variable param_states/decoder/layers_7/self_attention/query/kernel/v_row                  size 2048         shape (2048,)                                  partition spec None
I0512 01:51:47.552115 140316961150976 utils.py:1372] Variable param_states/decoder/layers_7/self_attention/value/kernel/m                      size 1            shape (1,)                                     partition spec None
I0512 01:51:47.552159 140316961150976 utils.py:1372] Variable param_states/decoder/layers_7/self_attention/value/kernel/v                      size 1            shape (1,)                                     partition spec None
I0512 01:51:47.552201 140316961150976 utils.py:1372] Variable param_states/decoder/layers_7/self_attention/value/kernel/v_col                  size 3072         shape (3072,)                                  partition spec None
I0512 01:51:47.552245 140316961150976 utils.py:1372] Variable param_states/decoder/layers_7/self_attention/value/kernel/v_row                  size 2048         shape (2048,)                                  partition spec None
I0512 01:51:47.552288 140316961150976 utils.py:1372] Variable param_states/decoder/layers_8/mlp/wi_0/kernel/m                                  size 1            shape (1,)                                     partition spec None
I0512 01:51:47.552331 140316961150976 utils.py:1372] Variable param_states/decoder/layers_8/mlp/wi_0/kernel/v                                  size 1            shape (1,)                                     partition spec None
I0512 01:51:47.552391 140316961150976 utils.py:1372] Variable param_states/decoder/layers_8/mlp/wi_0/kernel/v_col                              size 16384        shape (16384,)                                 partition spec None
I0512 01:51:47.552438 140316961150976 utils.py:1372] Variable param_states/decoder/layers_8/mlp/wi_0/kernel/v_row                              size 2048         shape (2048,)                                  partition spec None
I0512 01:51:47.552482 140316961150976 utils.py:1372] Variable param_states/decoder/layers_8/mlp/wi_1/kernel/m                                  size 1            shape (1,)                                     partition spec None
I0512 01:51:47.552525 140316961150976 utils.py:1372] Variable param_states/decoder/layers_8/mlp/wi_1/kernel/v                                  size 1            shape (1,)                                     partition spec None
I0512 01:51:47.552579 140316961150976 utils.py:1372] Variable param_states/decoder/layers_8/mlp/wi_1/kernel/v_col                              size 8192         shape (8192,)                                  partition spec None
I0512 01:51:47.552623 140316961150976 utils.py:1372] Variable param_states/decoder/layers_8/mlp/wi_1/kernel/v_row                              size 2048         shape (2048,)                                  partition spec None
I0512 01:51:47.552666 140316961150976 utils.py:1372] Variable param_states/decoder/layers_8/mlp/wo/kernel/m                                    size 1            shape (1,)                                     partition spec None
I0512 01:51:47.552708 140316961150976 utils.py:1372] Variable param_states/decoder/layers_8/mlp/wo/kernel/v                                    size 1            shape (1,)                                     partition spec None
I0512 01:51:47.552751 140316961150976 utils.py:1372] Variable param_states/decoder/layers_8/mlp/wo/kernel/v_col                                size 8192         shape (8192,)                                  partition spec None
I0512 01:51:47.552796 140316961150976 utils.py:1372] Variable param_states/decoder/layers_8/mlp/wo/kernel/v_row                                size 2048         shape (2048,)                                  partition spec None
I0512 01:51:47.552839 140316961150976 utils.py:1372] Variable param_states/decoder/layers_8/pre_mlp_layer_norm/scale/m                         size 1            shape (1,)                                     partition spec None
I0512 01:51:47.552887 140316961150976 utils.py:1372] Variable param_states/decoder/layers_8/pre_mlp_layer_norm/scale/v                         size 2048         shape (embed=2048)                             partition spec ('model',)
I0512 01:51:47.552933 140316961150976 utils.py:1372] Variable param_states/decoder/layers_8/pre_mlp_layer_norm/scale/v_col                     size 1            shape (1,)                                     partition spec None
I0512 01:51:47.552977 140316961150976 utils.py:1372] Variable param_states/decoder/layers_8/pre_mlp_layer_norm/scale/v_row                     size 1            shape (1,)                                     partition spec None
I0512 01:51:47.553028 140316961150976 utils.py:1372] Variable param_states/decoder/layers_8/pre_self_attention_layer_norm/scale/m              size 1            shape (1,)                                     partition spec None
I0512 01:51:47.553076 140316961150976 utils.py:1372] Variable param_states/decoder/layers_8/pre_self_attention_layer_norm/scale/v              size 2048         shape (embed=2048)                             partition spec ('model',)
I0512 01:51:47.553123 140316961150976 utils.py:1372] Variable param_states/decoder/layers_8/pre_self_attention_layer_norm/scale/v_col          size 1            shape (1,)                                     partition spec None
I0512 01:51:47.553179 140316961150976 utils.py:1372] Variable param_states/decoder/layers_8/pre_self_attention_layer_norm/scale/v_row          size 1            shape (1,)                                     partition spec None
I0512 01:51:47.553235 140316961150976 utils.py:1372] Variable param_states/decoder/layers_8/self_attention/key/kernel/m                        size 1            shape (1,)                                     partition spec None
I0512 01:51:47.553291 140316961150976 utils.py:1372] Variable param_states/decoder/layers_8/self_attention/key/kernel/v                        size 1            shape (1,)                                     partition spec None
I0512 01:51:47.553359 140316961150976 utils.py:1372] Variable param_states/decoder/layers_8/self_attention/key/kernel/v_col                    size 3072         shape (3072,)                                  partition spec None
I0512 01:51:47.553416 140316961150976 utils.py:1372] Variable param_states/decoder/layers_8/self_attention/key/kernel/v_row                    size 2048         shape (2048,)                                  partition spec None
I0512 01:51:47.553473 140316961150976 utils.py:1372] Variable param_states/decoder/layers_8/self_attention/out/kernel/m                        size 1            shape (1,)                                     partition spec None
I0512 01:51:47.553529 140316961150976 utils.py:1372] Variable param_states/decoder/layers_8/self_attention/out/kernel/v                        size 1            shape (1,)                                     partition spec None
I0512 01:51:47.553624 140316961150976 utils.py:1372] Variable param_states/decoder/layers_8/self_attention/out/kernel/v_col                    size 3072         shape (3072,)                                  partition spec None
I0512 01:51:47.553685 140316961150976 utils.py:1372] Variable param_states/decoder/layers_8/self_attention/out/kernel/v_row                    size 2048         shape (2048,)                                  partition spec None
I0512 01:51:47.553747 140316961150976 utils.py:1372] Variable param_states/decoder/layers_8/self_attention/query/kernel/m                      size 1            shape (1,)                                     partition spec None
I0512 01:51:47.553809 140316961150976 utils.py:1372] Variable param_states/decoder/layers_8/self_attention/query/kernel/v                      size 1            shape (1,)                                     partition spec None
I0512 01:51:47.553882 140316961150976 utils.py:1372] Variable param_states/decoder/layers_8/self_attention/query/kernel/v_col                  size 3072         shape (3072,)                                  partition spec None
I0512 01:51:47.553945 140316961150976 utils.py:1372] Variable param_states/decoder/layers_8/self_attention/query/kernel/v_row                  size 2048         shape (2048,)                                  partition spec None
I0512 01:51:47.554012 140316961150976 utils.py:1372] Variable param_states/decoder/layers_8/self_attention/value/kernel/m                      size 1            shape (1,)                                     partition spec None
I0512 01:51:47.554076 140316961150976 utils.py:1372] Variable param_states/decoder/layers_8/self_attention/value/kernel/v                      size 1            shape (1,)                                     partition spec None
I0512 01:51:47.554137 140316961150976 utils.py:1372] Variable param_states/decoder/layers_8/self_attention/value/kernel/v_col                  size 3072         shape (3072,)                                  partition spec None
I0512 01:51:47.554196 140316961150976 utils.py:1372] Variable param_states/decoder/layers_8/self_attention/value/kernel/v_row                  size 2048         shape (2048,)                                  partition spec None
I0512 01:51:47.554257 140316961150976 utils.py:1372] Variable param_states/decoder/layers_9/mlp/wi_0/kernel/m                                  size 1            shape (1,)                                     partition spec None
I0512 01:51:47.554320 140316961150976 utils.py:1372] Variable param_states/decoder/layers_9/mlp/wi_0/kernel/v                                  size 1            shape (1,)                                     partition spec None
I0512 01:51:47.554378 140316961150976 utils.py:1372] Variable param_states/decoder/layers_9/mlp/wi_0/kernel/v_col                              size 16384        shape (16384,)                                 partition spec None
I0512 01:51:47.554437 140316961150976 utils.py:1372] Variable param_states/decoder/layers_9/mlp/wi_0/kernel/v_row                              size 2048         shape (2048,)                                  partition spec None
I0512 01:51:47.554493 140316961150976 utils.py:1372] Variable param_states/decoder/layers_9/mlp/wi_1/kernel/m                                  size 1            shape (1,)                                     partition spec None
I0512 01:51:47.554552 140316961150976 utils.py:1372] Variable param_states/decoder/layers_9/mlp/wi_1/kernel/v                                  size 1            shape (1,)                                     partition spec None
I0512 01:51:47.554621 140316961150976 utils.py:1372] Variable param_states/decoder/layers_9/mlp/wi_1/kernel/v_col                              size 8192         shape (8192,)                                  partition spec None
I0512 01:51:47.554682 140316961150976 utils.py:1372] Variable param_states/decoder/layers_9/mlp/wi_1/kernel/v_row                              size 2048         shape (2048,)                                  partition spec None
I0512 01:51:47.554740 140316961150976 utils.py:1372] Variable param_states/decoder/layers_9/mlp/wo/kernel/m                                    size 1            shape (1,)                                     partition spec None
I0512 01:51:47.554800 140316961150976 utils.py:1372] Variable param_states/decoder/layers_9/mlp/wo/kernel/v                                    size 1            shape (1,)                                     partition spec None
I0512 01:51:47.554860 140316961150976 utils.py:1372] Variable param_states/decoder/layers_9/mlp/wo/kernel/v_col                                size 8192         shape (8192,)                                  partition spec None
I0512 01:51:47.554919 140316961150976 utils.py:1372] Variable param_states/decoder/layers_9/mlp/wo/kernel/v_row                                size 2048         shape (2048,)                                  partition spec None
I0512 01:51:47.554977 140316961150976 utils.py:1372] Variable param_states/decoder/layers_9/pre_mlp_layer_norm/scale/m                         size 1            shape (1,)                                     partition spec None
I0512 01:51:47.555069 140316961150976 utils.py:1372] Variable param_states/decoder/layers_9/pre_mlp_layer_norm/scale/v                         size 2048         shape (embed=2048)                             partition spec ('model',)
I0512 01:51:47.555137 140316961150976 utils.py:1372] Variable param_states/decoder/layers_9/pre_mlp_layer_norm/scale/v_col                     size 1            shape (1,)                                     partition spec None
I0512 01:51:47.555198 140316961150976 utils.py:1372] Variable param_states/decoder/layers_9/pre_mlp_layer_norm/scale/v_row                     size 1            shape (1,)                                     partition spec None
I0512 01:51:47.555257 140316961150976 utils.py:1372] Variable param_states/decoder/layers_9/pre_self_attention_layer_norm/scale/m              size 1            shape (1,)                                     partition spec None
I0512 01:51:47.555320 140316961150976 utils.py:1372] Variable param_states/decoder/layers_9/pre_self_attention_layer_norm/scale/v              size 2048         shape (embed=2048)                             partition spec ('model',)
I0512 01:51:47.555382 140316961150976 utils.py:1372] Variable param_states/decoder/layers_9/pre_self_attention_layer_norm/scale/v_col          size 1            shape (1,)                                     partition spec None
I0512 01:51:47.555442 140316961150976 utils.py:1372] Variable param_states/decoder/layers_9/pre_self_attention_layer_norm/scale/v_row          size 1            shape (1,)                                     partition spec None
I0512 01:51:47.555501 140316961150976 utils.py:1372] Variable param_states/decoder/layers_9/self_attention/key/kernel/m                        size 1            shape (1,)                                     partition spec None
I0512 01:51:47.555560 140316961150976 utils.py:1372] Variable param_states/decoder/layers_9/self_attention/key/kernel/v                        size 1            shape (1,)                                     partition spec None
I0512 01:51:47.555620 140316961150976 utils.py:1372] Variable param_states/decoder/layers_9/self_attention/key/kernel/v_col                    size 3072         shape (3072,)                                  partition spec None
I0512 01:51:47.555680 140316961150976 utils.py:1372] Variable param_states/decoder/layers_9/self_attention/key/kernel/v_row                    size 2048         shape (2048,)                                  partition spec None
I0512 01:51:47.555738 140316961150976 utils.py:1372] Variable param_states/decoder/layers_9/self_attention/out/kernel/m                        size 1            shape (1,)                                     partition spec None
I0512 01:51:47.555798 140316961150976 utils.py:1372] Variable param_states/decoder/layers_9/self_attention/out/kernel/v                        size 1            shape (1,)                                     partition spec None
I0512 01:51:47.555857 140316961150976 utils.py:1372] Variable param_states/decoder/layers_9/self_attention/out/kernel/v_col                    size 3072         shape (3072,)                                  partition spec None
I0512 01:51:47.555915 140316961150976 utils.py:1372] Variable param_states/decoder/layers_9/self_attention/out/kernel/v_row                    size 2048         shape (2048,)                                  partition spec None
I0512 01:51:47.556011 140316961150976 utils.py:1372] Variable param_states/decoder/layers_9/self_attention/query/kernel/m                      size 1            shape (1,)                                     partition spec None
I0512 01:51:47.556065 140316961150976 utils.py:1372] Variable param_states/decoder/layers_9/self_attention/query/kernel/v                      size 1            shape (1,)                                     partition spec None
I0512 01:51:47.556111 140316961150976 utils.py:1372] Variable param_states/decoder/layers_9/self_attention/query/kernel/v_col                  size 3072         shape (3072,)                                  partition spec None
I0512 01:51:47.556152 140316961150976 utils.py:1372] Variable param_states/decoder/layers_9/self_attention/query/kernel/v_row                  size 2048         shape (2048,)                                  partition spec None
I0512 01:51:47.556195 140316961150976 utils.py:1372] Variable param_states/decoder/layers_9/self_attention/value/kernel/m                      size 1            shape (1,)                                     partition spec None
I0512 01:51:47.556236 140316961150976 utils.py:1372] Variable param_states/decoder/layers_9/self_attention/value/kernel/v                      size 1            shape (1,)                                     partition spec None
I0512 01:51:47.556275 140316961150976 utils.py:1372] Variable param_states/decoder/layers_9/self_attention/value/kernel/v_col                  size 3072         shape (3072,)                                  partition spec None
I0512 01:51:47.556314 140316961150976 utils.py:1372] Variable param_states/decoder/layers_9/self_attention/value/kernel/v_row                  size 2048         shape (2048,)                                  partition spec None
I0512 01:51:47.556353 140316961150976 utils.py:1372] Variable param_states/decoder/logits_dense/kernel/m                                       size 1            shape (1,)                                     partition spec None
I0512 01:51:47.556393 140316961150976 utils.py:1372] Variable param_states/decoder/logits_dense/kernel/v                                       size 1            shape (1,)                                     partition spec None
I0512 01:51:47.556435 140316961150976 utils.py:1372] Variable param_states/decoder/logits_dense/kernel/v_col                                   size 256384       shape (256384,)                                partition spec None
I0512 01:51:47.556477 140316961150976 utils.py:1372] Variable param_states/decoder/logits_dense/kernel/v_row                                   size 2048         shape (2048,)                                  partition spec None
I0512 01:51:47.556518 140316961150976 utils.py:1372] Variable param_states/token_embedder/embedding/m                                          size 1            shape (1,)                                     partition spec None
I0512 01:51:47.556564 140316961150976 utils.py:1372] Variable param_states/token_embedder/embedding/v                                          size 1            shape (1,)                                     partition spec None
I0512 01:51:47.556606 140316961150976 utils.py:1372] Variable param_states/token_embedder/embedding/v_col                                      size 256384       shape (256384,)                                partition spec None
I0512 01:51:47.556645 140316961150976 utils.py:1372] Variable param_states/token_embedder/embedding/v_row                                      size 2048         shape (2048,)                                  partition spec None
I0512 01:51:47.556684 140316961150976 utils.py:1372] Variable step                                                                             size 1            shape ()                                       partition spec None
I0512 01:51:54.671413 140172769531904 train.py:421] Initialize/restore complete (405.31 seconds).
I0512 01:51:58.574298 140392160929792 train.py:421] Initialize/restore complete (409.30 seconds).
I0512 01:52:04.776322 139676058949632 train.py:421] Initialize/restore complete (415.53 seconds).
I0512 01:52:23.906591 140188559153152 train.py:421] Initialize/restore complete (434.63 seconds).
I0512 01:52:34.606290 139812357228544 train.py:421] Initialize/restore complete (445.31 seconds).
I0512 01:52:38.869419 139997397456896 train.py:421] Initialize/restore complete (449.51 seconds).
I0512 01:52:41.770583 140577131780096 train.py:421] Initialize/restore complete (452.38 seconds).
I0512 01:52:44.487528 140172769531904 train.py:573] Saving checkpoint before the training loop starts.
I0512 01:52:44.489373 140392160929792 train.py:573] Saving checkpoint before the training loop starts.
I0512 01:52:44.487987 140577131780096 train.py:573] Saving checkpoint before the training loop starts.
I0512 01:52:44.487298 140188559153152 train.py:573] Saving checkpoint before the training loop starts.
I0512 01:52:44.486860 139812357228544 train.py:573] Saving checkpoint before the training loop starts.
I0512 01:52:44.489619 139997397456896 train.py:573] Saving checkpoint before the training loop starts.
I0512 01:52:44.488792 139676058949632 train.py:573] Saving checkpoint before the training loop starts.
I0512 01:52:45.471577 140224410564160 logging_writer.py:64] [0] collection=train Got texts: {'config': "    from __gin__ import dynamic_registration\n    import __main__ as train_script\n    from flax import linen\n    import flaxformer\n    from flaxformer.architectures.moe import moe_architecture\n    from flaxformer.architectures.moe import moe_enums\n    from flaxformer.architectures.moe import moe_layers\n    from flaxformer.architectures.moe import routing\n    from flaxformer.architectures.t5 import t5_architecture\n    from flaxformer.components.attention import dense_attention\n    from flaxformer.components.attention import memory_efficient_attention\n    from flaxformer.components import dense\n    from flaxformer.components import embedding\n    from flaxformer.components import layer_norm\n    from gin import config\n    import seqio\n    import t5.data.mixtures\n    from t5x import adafactor\n    from t5x.contrib.moe import adafactor_utils\n    from t5x.contrib.moe import models\n    from t5x.contrib.moe import partitioning as moe_partitioning\n    from t5x.contrib.moe import trainer as moe_trainer\n    from t5x import gin_utils\n    from t5x import partitioning\n    from t5x import trainer\n    from t5x import utils\n    \n#### Macros:\n\n    ACTIVATION_DTYPE = 'bfloat16'\n    ACTIVATION_PARTITIONING_DIMS = 1\n    ARCHITECTURE = @t5_architecture.DecoderOnly()\n    AUX_LOSS_FACTOR = 0.01\n    BATCH_SIZE = 384\n    BIAS_INIT = @bias_init/linen.initializers.normal()\n    DECODER_SPARSE_LAYOUT = %flaxformer.architectures.moe.moe_enums.LayerLayout.MIXED\n    DROPOUT_FACTORY = @dropout_factory/linen.Dropout\n    DROPOUT_RATE = 0.0\n    EMBED_DIM = 2048\n    EVAL_EXPERT_CAPACITY_FACTOR = 2.0\n    EXPERT_DROPOUT_RATE = %DROPOUT_RATE\n    EXPERT_MLP_DIM = %MLP_DIM\n    GROUP_SIZE = 4096\n    HEAD_DIM = 128\n    JITTER_NOISE = 0.0\n    LABEL_SMOOTHING = 0.0\n    LOSS_NORMALIZING_FACTOR = 'NUM_REAL_TARGET_TOKENS'\n    MIXTURE_OR_TASK_MODULE = None\n    MIXTURE_OR_TASK_NAME = 'mix_full_lm_test'\n    MLP_DIM = 8192\n    MODEL = @models.MoeDecoderOnlyModel()\n    MODEL_DIR = 'gs://rosinality-tpu-bucket/openmoe_8b/training'\n    MODEL_PARALLEL_SUBMESH = None\n    MOE_TRUNCATED_DTYPE = 'bfloat16'\n    NUM_DECODER_LAYERS = 24\n    NUM_DECODER_SPARSE_LAYERS = 4\n    NUM_EMBEDDINGS = 256384\n    NUM_EXPERT_PARTITIONS = 8\n    NUM_EXPERTS = 8\n    NUM_HEADS = 24\n    NUM_MODEL_PARTITIONS = 4\n    NUM_SELECTED_EXPERTS = 2\n    OPTIMIZER = @adafactor.Adafactor()\n    RANDOM_SEED = None\n    ROUTER_Z_LOSS_FACTOR = 0.0001\n    SCALE = 0.1\n    SHUFFLE_TRAIN_EXAMPLES = True\n    TASK_FEATURE_LENGTHS = {'targets': 2048}\n    TRAIN_EXPERT_CAPACITY_FACTOR = 1.25\n    TRAIN_STEPS = 500000\n    USE_CACHED_TASKS = False\n    USE_HARDWARE_RNG = False\n    VOCABULARY = @seqio.SentencePieceVocabulary()\n    Z_LOSS = 0.0001\n    \n#### Parameters for adafactor.Adafactor:\n\n    adafactor.Adafactor.decay_rate = 0.8\n    adafactor.Adafactor.logical_factor_rules = @adafactor_utils.logical_factor_rules()\n    adafactor.Adafactor.step_offset = 0\n    \n#### Parameters for utils.CheckpointConfig:\n\n    utils.CheckpointConfig.restore = @utils.RestoreCheckpointConfig()\n    utils.CheckpointConfig.save = @utils.SaveCheckpointConfig()\n    \n#### Parameters for moe_partitioning.compute_num_model_partitions:\n\n    moe_partitioning.compute_num_model_partitions.model_parallel_submesh = \\\n        %MODEL_PARALLEL_SUBMESH\n    moe_partitioning.compute_num_model_partitions.num_model_partitions = \\\n        %NUM_MODEL_PARTITIONS\n    \n#### Parameters for utils.create_learning_rate_scheduler:\n\n    utils.create_learning_rate_scheduler.base_learning_rate = 1.0\n    utils.create_learning_rate_scheduler.factors = 'constant * rsqrt_decay'\n    utils.create_learning_rate_scheduler.warmup_steps = 10000\n    \n#### Parameters for train/utils.DatasetConfig:\n\n    train/utils.DatasetConfig.batch_size = %BATCH_SIZE\n    train/utils.DatasetConfig.mixture_or_task_name = %MIXTURE_OR_TASK_NAME\n    train/utils.DatasetConfig.module = %MIXTURE_OR_TASK_MODULE\n    train/utils.DatasetConfig.pack = True\n    train/utils.DatasetConfig.seed = 42\n    train/utils.DatasetConfig.shuffle = %SHUFFLE_TRAIN_EXAMPLES\n    train/utils.DatasetConfig.split = 'train'\n    train/utils.DatasetConfig.task_feature_lengths = %TASK_FEATURE_LENGTHS\n    train/utils.DatasetConfig.use_cached = %USE_CACHED_TASKS\n    \n#### Parameters for train_eval/utils.DatasetConfig:\n\n    train_eval/utils.DatasetConfig.batch_size = 128\n    train_eval/utils.DatasetConfig.mixture_or_task_name = %MIXTURE_OR_TASK_NAME\n    train_eval/utils.DatasetConfig.module = %MIXTURE_OR_TASK_MODULE\n    train_eval/utils.DatasetConfig.pack = True\n    train_eval/utils.DatasetConfig.seed = 42\n    train_eval/utils.DatasetConfig.shuffle = False\n    train_eval/utils.DatasetConfig.split = 'validation'\n    train_eval/utils.DatasetConfig.task_feature_lengths = %TASK_FEATURE_LENGTHS\n    train_eval/utils.DatasetConfig.use_cached = %USE_CACHED_TASKS\n    \n#### Parameters for t5_architecture.DecoderLayer:\n\n    t5_architecture.DecoderLayer.activation_partitioning_dims = \\\n        %ACTIVATION_PARTITIONING_DIMS\n    t5_architecture.DecoderLayer.dropout_factory = %DROPOUT_FACTORY\n    t5_architecture.DecoderLayer.encoder_decoder_attention = None\n    t5_architecture.DecoderLayer.layer_norm_factory = @layer_norm.T5LayerNorm\n    t5_architecture.DecoderLayer.mlp = @dense.MlpBlock()\n    t5_architecture.DecoderLayer.scanned = False\n    t5_architecture.DecoderLayer.self_attention = \\\n        @dense_attention.MultiHeadDotProductAttention()\n    \n#### Parameters for t5_architecture.DecoderOnly:\n\n    t5_architecture.DecoderOnly.decoder_factory = @moe_architecture.SparseDecoder\n    t5_architecture.DecoderOnly.dtype = %ACTIVATION_DTYPE\n    t5_architecture.DecoderOnly.shared_token_embedder_factory = @embedding.Embed\n    \n#### Parameters for output_logits/dense.DenseGeneral:\n\n    output_logits/dense.DenseGeneral.bias_init = %BIAS_INIT\n    output_logits/dense.DenseGeneral.dtype = 'float32'\n    output_logits/dense.DenseGeneral.features = %NUM_EMBEDDINGS\n    output_logits/dense.DenseGeneral.kernel_axis_names = ['embed', 'vocab']\n    output_logits/dense.DenseGeneral.kernel_init = \\\n        @output_logits_kernel_init/linen.initializers.variance_scaling()\n    output_logits/dense.DenseGeneral.use_bias = False\n    \n#### Parameters for dropout_factory/linen.Dropout:\n\n    dropout_factory/linen.Dropout.broadcast_dims = (-2,)\n    dropout_factory/linen.Dropout.rate = %DROPOUT_RATE\n    \n#### Parameters for embedding.Embed:\n\n    embedding.Embed.attend_dtype = 'float32'\n    embedding.Embed.cast_input_dtype = 'int32'\n    embedding.Embed.dtype = %ACTIVATION_DTYPE\n    embedding.Embed.embedding_init = @token_embedder_init/linen.initializers.normal()\n    embedding.Embed.features = %EMBED_DIM\n    embedding.Embed.name = 'token_embedder'\n    embedding.Embed.num_embeddings = %NUM_EMBEDDINGS\n    embedding.Embed.one_hot = True\n    \n#### Parameters for dense.MlpBlock:\n\n    dense.MlpBlock.activations = ('swiglu', 'linear')\n    dense.MlpBlock.bias_init = %BIAS_INIT\n    dense.MlpBlock.dtype = %ACTIVATION_DTYPE\n    dense.MlpBlock.final_dropout_rate = 0\n    dense.MlpBlock.input_axis_name = 'mlp_embed'\n    dense.MlpBlock.intermediate_dim = %MLP_DIM\n    dense.MlpBlock.intermediate_dropout_rate = %DROPOUT_RATE\n    dense.MlpBlock.kernel_init = @mlp_kernel_init/linen.initializers.variance_scaling()\n    dense.MlpBlock.output_axis_name = 'mlp_embed'\n    dense.MlpBlock.use_bias = False\n    \n#### Parameters for expert/dense.MlpBlock:\n\n    expert/dense.MlpBlock.activation_partitioning_dims = 1\n    expert/dense.MlpBlock.activations = ('swiglu', 'linear')\n    expert/dense.MlpBlock.bias_init = %BIAS_INIT\n    expert/dense.MlpBlock.data_sharding_constraints = ('expert_replicas', 'mlp')\n    expert/dense.MlpBlock.dtype = %MOE_TRUNCATED_DTYPE\n    expert/dense.MlpBlock.final_dropout_rate = 0.0\n    expert/dense.MlpBlock.input_axis_name = 'mlp_embed'\n    expert/dense.MlpBlock.intermediate_axis_name = 'expert_mlp'\n    expert/dense.MlpBlock.intermediate_dim = %EXPERT_MLP_DIM\n    expert/dense.MlpBlock.intermediate_dropout_rate = %EXPERT_DROPOUT_RATE\n    expert/dense.MlpBlock.kernel_init = \\\n        @expert_kernel_init/linen.initializers.variance_scaling()\n    expert/dense.MlpBlock.output_axis_name = 'mlp_embed'\n    expert/dense.MlpBlock.use_bias = False\n    \n#### Parameters for models.MoeDecoderOnlyModel:\n\n    models.MoeDecoderOnlyModel.aux_loss_factor = %AUX_LOSS_FACTOR\n    models.MoeDecoderOnlyModel.label_smoothing = %LABEL_SMOOTHING\n    models.MoeDecoderOnlyModel.loss_normalizing_factor = %LOSS_NORMALIZING_FACTOR\n    models.MoeDecoderOnlyModel.module = %ARCHITECTURE\n    models.MoeDecoderOnlyModel.optimizer_def = %OPTIMIZER\n    models.MoeDecoderOnlyModel.router_z_loss_factor = %ROUTER_Z_LOSS_FACTOR\n    models.MoeDecoderOnlyModel.vocabulary = %VOCABULARY\n    models.MoeDecoderOnlyModel.z_loss = %Z_LOSS\n    \n#### Parameters for moe_layers.MoeLayer:\n\n    moe_layers.MoeLayer.dtype = %MOE_TRUNCATED_DTYPE\n    moe_layers.MoeLayer.eval_capacity_factor = %EVAL_EXPERT_CAPACITY_FACTOR\n    moe_layers.MoeLayer.expert = @expert/dense.MlpBlock()\n    moe_layers.MoeLayer.max_group_size = %GROUP_SIZE\n    moe_layers.MoeLayer.num_expert_partitions = %NUM_EXPERT_PARTITIONS\n    moe_layers.MoeLayer.num_experts = %NUM_EXPERTS\n    moe_layers.MoeLayer.num_model_partitions = \\\n        @moe_partitioning.compute_num_model_partitions()\n    moe_layers.MoeLayer.train_capacity_factor = %TRAIN_EXPERT_CAPACITY_FACTOR\n    \n#### Parameters for sparse_decoder/moe_layers.MoeLayer:\n\n    sparse_decoder/moe_layers.MoeLayer.router = \\\n        @sparse_decoder/routing.TokensChooseMaskedRouter()\n    \n#### Parameters for moe_partitioning.MoePjitPartitioner:\n\n    moe_partitioning.MoePjitPartitioner.model_parallel_submesh = \\\n        %MODEL_PARALLEL_SUBMESH\n    moe_partitioning.MoePjitPartitioner.num_expert_partitions = %NUM_EXPERT_PARTITIONS\n    moe_partitioning.MoePjitPartitioner.num_partitions = %NUM_MODEL_PARTITIONS\n    \n#### Parameters for moe_trainer.MoeTrainer:\n\n    moe_trainer.MoeTrainer.learning_rate_fn = @utils.create_learning_rate_scheduler()\n    moe_trainer.MoeTrainer.num_expert_partitions = %NUM_EXPERT_PARTITIONS\n    moe_trainer.MoeTrainer.num_microbatches = 8\n    \n#### Parameters for dense_attention.MultiHeadDotProductAttention:\n\n    dense_attention.MultiHeadDotProductAttention.bias_init = %BIAS_INIT\n    dense_attention.MultiHeadDotProductAttention.broadcast_dropout = True\n    dense_attention.MultiHeadDotProductAttention.dropout_rate = %DROPOUT_RATE\n    dense_attention.MultiHeadDotProductAttention.dtype = %ACTIVATION_DTYPE\n    dense_attention.MultiHeadDotProductAttention.head_dim = %HEAD_DIM\n    dense_attention.MultiHeadDotProductAttention.kernel_init = \\\n        @attention_kernel_init/linen.initializers.variance_scaling()\n    dense_attention.MultiHeadDotProductAttention.num_heads = %NUM_HEADS\n    dense_attention.MultiHeadDotProductAttention.use_bias = False\n    dense_attention.MultiHeadDotProductAttention.use_rotary_embedding = True\n    \n#### Parameters for bias_init/linen.initializers.normal:\n\n    bias_init/linen.initializers.normal.stddev = 1e-06\n    \n#### Parameters for router_init/linen.initializers.normal:\n\n    router_init/linen.initializers.normal.stddev = 0.02\n    \n#### Parameters for token_embedder_init/linen.initializers.normal:\n\n    token_embedder_init/linen.initializers.normal.stddev = 1.0\n    \n#### Parameters for partitioning.PjitPartitioner:\n\n    partitioning.PjitPartitioner.logical_axis_rules = \\\n        @partitioning.standard_logical_axis_rules()\n    partitioning.PjitPartitioner.model_parallel_submesh = None\n    partitioning.PjitPartitioner.num_partitions = 1\n    \n#### Parameters for utils.RestoreCheckpointConfig:\n\n    utils.RestoreCheckpointConfig.path = []\n    \n#### Parameters for routing.RouterWeights:\n\n    routing.RouterWeights.bias_init = %BIAS_INIT\n    routing.RouterWeights.dtype = 'float32'\n    routing.RouterWeights.kernel_init = @router_init/linen.initializers.normal()\n    routing.RouterWeights.use_bias = False\n    \n#### Parameters for utils.SaveCheckpointConfig:\n\n    utils.SaveCheckpointConfig.dtype = 'float32'\n    utils.SaveCheckpointConfig.keep = 20\n    utils.SaveCheckpointConfig.period = 2500\n    utils.SaveCheckpointConfig.save_dataset = True\n    \n#### Parameters for seqio.SentencePieceVocabulary:\n\n    seqio.SentencePieceVocabulary.extra_ids = 300\n    seqio.SentencePieceVocabulary.sentencepiece_model_file = \\\n        'gs://rosinality-tpu-bucket/sentencepiece.model'\n    \n#### Parameters for moe_architecture.SparseDecoder:\n\n    moe_architecture.SparseDecoder.dropout_factory = %DROPOUT_FACTORY\n    moe_architecture.SparseDecoder.dtype = %ACTIVATION_DTYPE\n    moe_architecture.SparseDecoder.layer_factory = @t5_architecture.DecoderLayer\n    moe_architecture.SparseDecoder.layer_norm_factory = @layer_norm.T5LayerNorm\n    moe_architecture.SparseDecoder.num_layers = %NUM_DECODER_LAYERS\n    moe_architecture.SparseDecoder.num_sparse_layers = %NUM_DECODER_SPARSE_LAYERS\n    moe_architecture.SparseDecoder.output_logits_factory = \\\n        @output_logits/dense.DenseGeneral\n    moe_architecture.SparseDecoder.sparse_layer_factory = \\\n        @moe_architecture.SparseDecoderLayer\n    moe_architecture.SparseDecoder.sparse_layout = %DECODER_SPARSE_LAYOUT\n    \n#### Parameters for moe_architecture.SparseDecoderLayer:\n\n    moe_architecture.SparseDecoderLayer.activation_partitioning_dims = \\\n        %ACTIVATION_PARTITIONING_DIMS\n    moe_architecture.SparseDecoderLayer.dropout_factory = %DROPOUT_FACTORY\n    moe_architecture.SparseDecoderLayer.encoder_decoder_attention = None\n    moe_architecture.SparseDecoderLayer.extra_mlp = @dense.MlpBlock()\n    moe_architecture.SparseDecoderLayer.layer_norm_factory = @layer_norm.T5LayerNorm\n    moe_architecture.SparseDecoderLayer.mlp = @sparse_decoder/moe_layers.MoeLayer()\n    moe_architecture.SparseDecoderLayer.scanned = False\n    moe_architecture.SparseDecoderLayer.self_attention = \\\n        @dense_attention.MultiHeadDotProductAttention()\n    \n#### Parameters for layer_norm.T5LayerNorm:\n\n    layer_norm.T5LayerNorm.dtype = %ACTIVATION_DTYPE\n    \n#### Parameters for routing.TokensChooseMaskedRouter:\n\n    routing.TokensChooseMaskedRouter.dtype = 'float32'\n    routing.TokensChooseMaskedRouter.ignore_padding_tokens = False\n    routing.TokensChooseMaskedRouter.jitter_noise = %JITTER_NOISE\n    routing.TokensChooseMaskedRouter.num_selected_experts = %NUM_SELECTED_EXPERTS\n    routing.TokensChooseMaskedRouter.router_weights = @routing.RouterWeights()\n    \n#### Parameters for sparse_decoder/routing.TokensChooseMaskedRouter:\n\n    sparse_decoder/routing.TokensChooseMaskedRouter.batch_prioritized_routing = False\n    \n#### Parameters for sparse_encoder/routing.TokensChooseMaskedRouter:\n\n    sparse_encoder/routing.TokensChooseMaskedRouter.batch_prioritized_routing = False\n    \n#### Parameters for train_script.train:\n\n    train_script.train.checkpoint_cfg = @utils.CheckpointConfig()\n    train_script.train.eval_period = 2500\n    train_script.train.eval_steps = 20\n    train_script.train.infer_eval_dataset_cfg = None\n    train_script.train.model = %MODEL\n    train_script.train.model_dir = %MODEL_DIR\n    train_script.train.partitioner = @moe_partitioning.MoePjitPartitioner()\n    train_script.train.random_seed = 42\n    train_script.train.stats_period = 10\n    train_script.train.summarize_config_fn = @gin_utils.summarize_gin_config\n    train_script.train.total_steps = %TRAIN_STEPS\n    train_script.train.train_dataset_cfg = @train/utils.DatasetConfig()\n    train_script.train.train_eval_dataset_cfg = None\n    train_script.train.trainer_cls = @moe_trainer.MoeTrainer\n    train_script.train.use_hardware_rng = %USE_HARDWARE_RNG\n    \n#### Parameters for trainer.Trainer:\n\n    trainer.Trainer.learning_rate_fn = @utils.create_learning_rate_scheduler()\n    trainer.Trainer.num_microbatches = None\n    \n#### Parameters for attention_kernel_init/linen.initializers.variance_scaling:\n\n    attention_kernel_init/linen.initializers.variance_scaling.distribution = 'normal'\n    attention_kernel_init/linen.initializers.variance_scaling.mode = 'fan_in'\n    attention_kernel_init/linen.initializers.variance_scaling.scale = %SCALE\n    \n#### Parameters for expert_kernel_init/linen.initializers.variance_scaling:\n\n    expert_kernel_init/linen.initializers.variance_scaling.distribution = 'normal'\n    expert_kernel_init/linen.initializers.variance_scaling.mode = 'fan_in'\n    expert_kernel_init/linen.initializers.variance_scaling.scale = %SCALE\n    \n#### Parameters for mlp_kernel_init/linen.initializers.variance_scaling:\n\n    mlp_kernel_init/linen.initializers.variance_scaling.distribution = \\\n        'truncated_normal'\n    mlp_kernel_init/linen.initializers.variance_scaling.mode = 'fan_in'\n    mlp_kernel_init/linen.initializers.variance_scaling.scale = %SCALE\n    \n#### Parameters for output_logits_kernel_init/linen.initializers.variance_scaling:\n\n    output_logits_kernel_init/linen.initializers.variance_scaling.distribution = \\\n        'truncated_normal'\n    output_logits_kernel_init/linen.initializers.variance_scaling.mode = 'fan_in'\n    output_logits_kernel_init/linen.initializers.variance_scaling.scale = %SCALE"}.
I0512 01:52:46.247160 140224410564160 logging_writer.py:48] [0] collection=train timing/init_or_restore_seconds=397.857
I0512 01:52:46.256353 140316961150976 train.py:573] Saving checkpoint before the training loop starts.
I0512 01:52:46.358519 139676058949632 checkpoints.py:786] Saving checkpoint for step 0 to gs://rosinality-tpu-bucket/openmoe_8b/training/checkpoint_0.tmp-1715478766
I0512 01:52:46.382196 140172769531904 checkpoints.py:786] Saving checkpoint for step 0 to gs://rosinality-tpu-bucket/openmoe_8b/training/checkpoint_0.tmp-1715478766
I0512 01:52:46.418363 140188559153152 checkpoints.py:786] Saving checkpoint for step 0 to gs://rosinality-tpu-bucket/openmoe_8b/training/checkpoint_0.tmp-1715478766
I0512 01:52:46.436454 139997397456896 checkpoints.py:786] Saving checkpoint for step 0 to gs://rosinality-tpu-bucket/openmoe_8b/training/checkpoint_0.tmp-1715478766
I0512 01:52:46.637658 140392160929792 checkpoints.py:786] Saving checkpoint for step 0 to gs://rosinality-tpu-bucket/openmoe_8b/training/checkpoint_0.tmp-1715478766
I0512 01:52:46.647327 140316961150976 checkpoints.py:786] Saving checkpoint for step 0 to gs://rosinality-tpu-bucket/openmoe_8b/training/checkpoint_0.tmp-1715478766
I0512 01:52:46.668026 140577131780096 checkpoints.py:786] Saving checkpoint for step 0 to gs://rosinality-tpu-bucket/openmoe_8b/training/checkpoint_0.tmp-1715478766
I0512 01:52:47.015845 139812357228544 checkpoints.py:786] Saving checkpoint for step 0 to gs://rosinality-tpu-bucket/openmoe_8b/training/checkpoint_0.tmp-1715478766
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
I0000 00:00:1715478767.273469  353284 gcs_resource.cc:109] Using default AdmissionQueue with limit 32
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
I0000 00:00:1715478767.263940  355987 gcs_resource.cc:109] Using default AdmissionQueue with limit 32
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
I0000 00:00:1715478767.269533  349683 gcs_resource.cc:109] Using default AdmissionQueue with limit 32
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
I0000 00:00:1715478767.270996  382697 gcs_resource.cc:109] Using default AdmissionQueue with limit 32
I0000 00:00:1715478767.276778  386551 google_auth_provider.cc:180] Running on GCE, using service account 1081612546678-compute@developer.gserviceaccount.com
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
I0000 00:00:1715478767.279978  384045 gcs_resource.cc:109] Using default AdmissionQueue with limit 32
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
I0000 00:00:1715478767.288379  352750 gcs_resource.cc:109] Using default AdmissionQueue with limit 32
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
I0000 00:00:1715478767.398307  373603 gcs_resource.cc:109] Using default AdmissionQueue with limit 32
I0000 00:00:1715478767.407222  353566 google_auth_provider.cc:180] Running on GCE, using service account 1081612546678-compute@developer.gserviceaccount.com
I0000 00:00:1715478767.433600  356990 google_auth_provider.cc:180] Running on GCE, using service account 1081612546678-compute@developer.gserviceaccount.com
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
I0000 00:00:1715478767.474655  351449 gcs_resource.cc:109] Using default AdmissionQueue with limit 32
I0000 00:00:1715478767.477637  356446 google_auth_provider.cc:180] Running on GCE, using service account 1081612546678-compute@developer.gserviceaccount.com
I0000 00:00:1715478767.768169  387858 google_auth_provider.cc:180] Running on GCE, using service account 1081612546678-compute@developer.gserviceaccount.com
I0000 00:00:1715478767.774965  359691 google_auth_provider.cc:180] Running on GCE, using service account 1081612546678-compute@developer.gserviceaccount.com
I0000 00:00:1715478767.804594  377367 google_auth_provider.cc:180] Running on GCE, using service account 1081612546678-compute@developer.gserviceaccount.com
I0000 00:00:1715478767.875430  355205 google_auth_provider.cc:180] Running on GCE, using service account 1081612546678-compute@developer.gserviceaccount.com
I0512 01:53:22.091852 140172769531904 checkpoints.py:797] Writing dataset iterator state to 'train_ds-007-of-008'.
I0512 01:53:22.091135 140188559153152 checkpoints.py:797] Writing dataset iterator state to 'train_ds-005-of-008'.
I0512 01:53:22.092120 139997397456896 checkpoints.py:797] Writing dataset iterator state to 'train_ds-004-of-008'.
I0512 01:53:22.093877 140577131780096 checkpoints.py:797] Writing dataset iterator state to 'train_ds-001-of-008'.
I0512 01:53:22.092171 139676058949632 checkpoints.py:797] Writing dataset iterator state to 'train_ds-002-of-008'.
I0512 01:53:22.091530 140316961150976 checkpoints.py:797] Writing dataset iterator state to 'train_ds-000-of-008'.
I0512 01:53:22.093139 139812357228544 checkpoints.py:797] Writing dataset iterator state to 'train_ds-006-of-008'.
I0512 01:53:22.121765 140392160929792 checkpoints.py:797] Writing dataset iterator state to 'train_ds-003-of-008'.
2024-05-12 01:58:36.807526: W tensorflow/core/framework/op_kernel.cc:1839] OP_REQUIRES failed at iterator_ops.cc:1056 : FAILED_PRECONDITION: SentencepieceOp is stateful.
2024-05-12 01:58:36.807589: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: FAILED_PRECONDITION: SentencepieceOp is stateful.
E0512 01:58:36.808165 140577131780096 checkpoints.py:803] Input pipeline must be stateless in order to checkpoint. Cache stateful steps offline or disable iterator checkpointing.
Traceback (most recent call last):
  File "/home/rosinality/./t5x/t5x/train.py", line 964, in <module>
    config_utils.run(main)
  File "/home/rosinality/t5x/t5x/config_utils.py", line 214, in run
    gin_utils.run(main)
  File "/home/rosinality/t5x/t5x/gin_utils.py", line 127, in run
    app.run(
  File "/home/rosinality/openmoe_venv/lib/python3.10/site-packages/absl/app.py", line 308, in run
    _run_main(main, args)
  File "/home/rosinality/openmoe_venv/lib/python3.10/site-packages/absl/app.py", line 254, in _run_main
    sys.exit(main(argv))
  File "/home/rosinality/./t5x/t5x/train.py", line 898, in main
    _main(argv)
  File "/home/rosinality/./t5x/t5x/train.py", line 959, in _main
    train_using_gin()
  File "/home/rosinality/openmoe_venv/lib/python3.10/site-packages/gin/config.py", line 1605, in gin_wrapper
    utils.augment_exception_message_and_reraise(e, err_str)
  File "/home/rosinality/openmoe_venv/lib/python3.10/site-packages/gin/utils.py", line 41, in augment_exception_message_and_reraise
    raise proxy.with_traceback(exception.__traceback__) from None
  File "/home/rosinality/openmoe_venv/lib/python3.10/site-packages/gin/config.py", line 1582, in gin_wrapper
    return fn(*new_args, **new_kwargs)
  File "/home/rosinality/./t5x/t5x/train.py", line 574, in train
    checkpoint_manager.save(
  File "/home/rosinality/t5x/t5x/utils.py", line 406, in save
    self._checkpointer.save(
  File "/home/rosinality/t5x/t5x/utils.py", line 281, in save
    self._save_checkpointer.save(
  File "/home/rosinality/t5x/t5x/checkpoints.py", line 806, in save
    raise e
  File "/home/rosinality/t5x/t5x/checkpoints.py", line 800, in save
    self._dataset_iterator.save(
  File "/home/rosinality/openmoe_venv/lib/python3.10/site-packages/clu/data/dataset_iterator.py", line 288, in save
    self._it.save(filename)
  File "/home/rosinality/t5x/t5x/utils.py", line 697, in save
    return self._iterator.save(filename)
  File "/home/rosinality/openmoe_venv/lib/python3.10/site-packages/clu/data/dataset_iterator.py", line 204, in save
    self._ckpt.write(os.fspath(filename))
  File "/home/rosinality/openmoe_venv/lib/python3.10/site-packages/tensorflow/python/checkpoint/checkpoint.py", line 2320, in write
    return self._write(file_prefix, options)
  File "/home/rosinality/openmoe_venv/lib/python3.10/site-packages/tensorflow/python/checkpoint/checkpoint.py", line 2369, in _write
    output = self._saver.save(file_prefix=file_prefix, options=options)
  File "/home/rosinality/openmoe_venv/lib/python3.10/site-packages/tensorflow/python/checkpoint/checkpoint.py", line 1356, in save
    save_path, new_feed_additions = self._save_cached_when_graph_building(
  File "/home/rosinality/openmoe_venv/lib/python3.10/site-packages/tensorflow/python/checkpoint/checkpoint.py", line 1284, in _save_cached_when_graph_building
    self._gather_serialized_tensors(object_graph_tensor))
  File "/home/rosinality/openmoe_venv/lib/python3.10/site-packages/tensorflow/python/checkpoint/checkpoint.py", line 1245, in _gather_serialized_tensors
    save_util.serialize_graph_view(self._graph_view,
  File "/home/rosinality/openmoe_venv/lib/python3.10/site-packages/tensorflow/python/checkpoint/save_util.py", line 309, in serialize_graph_view
    serialized_tensors = _get_and_write_tensors_to_serialize(
  File "/home/rosinality/openmoe_venv/lib/python3.10/site-packages/tensorflow/python/checkpoint/save_util.py", line 157, in _get_and_write_tensors_to_serialize
    trackable, tensor_dict = _get_tensors_from_legacy_saveable(
  File "/home/rosinality/openmoe_venv/lib/python3.10/site-packages/tensorflow/python/checkpoint/save_util.py", line 190, in _get_tensors_from_legacy_saveable
    save_util_v1.generate_saveable_objects(
  File "/home/rosinality/openmoe_venv/lib/python3.10/site-packages/tensorflow/python/checkpoint/save_util_v1.py", line 180, in generate_saveable_objects
    maybe_saveable = saveable_object_util.create_saveable_object(
  File "/home/rosinality/openmoe_venv/lib/python3.10/site-packages/tensorflow/python/training/saving/saveable_object_util.py", line 473, in create_saveable_object
    return factory(name=key)
  File "/home/rosinality/openmoe_venv/lib/python3.10/site-packages/tensorflow/python/training/saving/saveable_object_util.py", line 536, in create_saveable
    tensor_dict = save_fn()
  File "/home/rosinality/openmoe_venv/lib/python3.10/site-packages/tensorflow/python/data/ops/iterator_ops.py", line 883, in _serialize_to_tensors
    serialized_iterator = gen_dataset_ops.serialize_iterator(
  File "/home/rosinality/openmoe_venv/lib/python3.10/site-packages/tensorflow/python/ops/gen_dataset_ops.py", line 6523, in serialize_iterator
    _ops.raise_from_not_ok_status(e, name)
  File "/home/rosinality/openmoe_venv/lib/python3.10/site-packages/tensorflow/python/framework/ops.py", line 5983, in raise_from_not_ok_status
    raise core._status_to_exception(e) from None  # pylint: disable=protected-access
tensorflow.python.framework.errors_impl.FailedPreconditionError: {{function_node __wrapped__SerializeIterator_device_/job:localhost/replica:0/task:0/device:CPU:0}} SentencepieceOp is stateful. [Op:SerializeIterator] name: 
  In call to configurable 'train' (<function train at 0x7fd9103463b0>)
I0000 00:00:1715479117.563618  355987 tfrt_cpu_pjrt_client.cc:352] TfrtCpuClient destroyed.
2024-05-12 01:58:46.425088: W tensorflow/core/framework/op_kernel.cc:1839] OP_REQUIRES failed at iterator_ops.cc:1056 : FAILED_PRECONDITION: SentencepieceOp is stateful.
2024-05-12 01:58:46.425154: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: FAILED_PRECONDITION: SentencepieceOp is stateful.
E0512 01:58:46.426114 139812357228544 checkpoints.py:803] Input pipeline must be stateless in order to checkpoint. Cache stateful steps offline or disable iterator checkpointing.
Traceback (most recent call last):
  File "/home/rosinality/./t5x/t5x/train.py", line 964, in <module>
    config_utils.run(main)
  File "/home/rosinality/t5x/t5x/config_utils.py", line 214, in run
    gin_utils.run(main)
  File "/home/rosinality/t5x/t5x/gin_utils.py", line 127, in run
    app.run(
  File "/home/rosinality/openmoe_venv/lib/python3.10/site-packages/absl/app.py", line 308, in run
    _run_main(main, args)
  File "/home/rosinality/openmoe_venv/lib/python3.10/site-packages/absl/app.py", line 254, in _run_main
    sys.exit(main(argv))
  File "/home/rosinality/./t5x/t5x/train.py", line 898, in main
    _main(argv)
  File "/home/rosinality/./t5x/t5x/train.py", line 959, in _main
    train_using_gin()
  File "/home/rosinality/openmoe_venv/lib/python3.10/site-packages/gin/config.py", line 1605, in gin_wrapper
    utils.augment_exception_message_and_reraise(e, err_str)
  File "/home/rosinality/openmoe_venv/lib/python3.10/site-packages/gin/utils.py", line 41, in augment_exception_message_and_reraise
    raise proxy.with_traceback(exception.__traceback__) from None
  File "/home/rosinality/openmoe_venv/lib/python3.10/site-packages/gin/config.py", line 1582, in gin_wrapper
    return fn(*new_args, **new_kwargs)
  File "/home/rosinality/./t5x/t5x/train.py", line 574, in train
    checkpoint_manager.save(
  File "/home/rosinality/t5x/t5x/utils.py", line 406, in save
    self._checkpointer.save(
  File "/home/rosinality/t5x/t5x/utils.py", line 281, in save
    self._save_checkpointer.save(
  File "/home/rosinality/t5x/t5x/checkpoints.py", line 806, in save
    raise e
  File "/home/rosinality/t5x/t5x/checkpoints.py", line 800, in save
    self._dataset_iterator.save(
  File "/home/rosinality/openmoe_venv/lib/python3.10/site-packages/clu/data/dataset_iterator.py", line 288, in save
    self._it.save(filename)
  File "/home/rosinality/t5x/t5x/utils.py", line 697, in save
    return self._iterator.save(filename)
  File "/home/rosinality/openmoe_venv/lib/python3.10/site-packages/clu/data/dataset_iterator.py", line 204, in save
    self._ckpt.write(os.fspath(filename))
  File "/home/rosinality/openmoe_venv/lib/python3.10/site-packages/tensorflow/python/checkpoint/checkpoint.py", line 2320, in write
    return self._write(file_prefix, options)
  File "/home/rosinality/openmoe_venv/lib/python3.10/site-packages/tensorflow/python/checkpoint/checkpoint.py", line 2369, in _write
    output = self._saver.save(file_prefix=file_prefix, options=options)
  File "/home/rosinality/openmoe_venv/lib/python3.10/site-packages/tensorflow/python/checkpoint/checkpoint.py", line 1356, in save
    save_path, new_feed_additions = self._save_cached_when_graph_building(
  File "/home/rosinality/openmoe_venv/lib/python3.10/site-packages/tensorflow/python/checkpoint/checkpoint.py", line 1284, in _save_cached_when_graph_building
    self._gather_serialized_tensors(object_graph_tensor))
  File "/home/rosinality/openmoe_venv/lib/python3.10/site-packages/tensorflow/python/checkpoint/checkpoint.py", line 1245, in _gather_serialized_tensors
    save_util.serialize_graph_view(self._graph_view,
  File "/home/rosinality/openmoe_venv/lib/python3.10/site-packages/tensorflow/python/checkpoint/save_util.py", line 309, in serialize_graph_view
    serialized_tensors = _get_and_write_tensors_to_serialize(
  File "/home/rosinality/openmoe_venv/lib/python3.10/site-packages/tensorflow/python/checkpoint/save_util.py", line 157, in _get_and_write_tensors_to_serialize
    trackable, tensor_dict = _get_tensors_from_legacy_saveable(
  File "/home/rosinality/openmoe_venv/lib/python3.10/site-packages/tensorflow/python/checkpoint/save_util.py", line 190, in _get_tensors_from_legacy_saveable
    save_util_v1.generate_saveable_objects(
  File "/home/rosinality/openmoe_venv/lib/python3.10/site-packages/tensorflow/python/checkpoint/save_util_v1.py", line 180, in generate_saveable_objects
    maybe_saveable = saveable_object_util.create_saveable_object(
  File "/home/rosinality/openmoe_venv/lib/python3.10/site-packages/tensorflow/python/training/saving/saveable_object_util.py", line 473, in create_saveable_object
    return factory(name=key)
  File "/home/rosinality/openmoe_venv/lib/python3.10/site-packages/tensorflow/python/training/saving/saveable_object_util.py", line 536, in create_saveable
    tensor_dict = save_fn()
  File "/home/rosinality/openmoe_venv/lib/python3.10/site-packages/tensorflow/python/data/ops/iterator_ops.py", line 883, in _serialize_to_tensors
    serialized_iterator = gen_dataset_ops.serialize_iterator(
  File "/home/rosinality/openmoe_venv/lib/python3.10/site-packages/tensorflow/python/ops/gen_dataset_ops.py", line 6523, in serialize_iterator
    _ops.raise_from_not_ok_status(e, name)
  File "/home/rosinality/openmoe_venv/lib/python3.10/site-packages/tensorflow/python/framework/ops.py", line 5983, in raise_from_not_ok_status
    raise core._status_to_exception(e) from None  # pylint: disable=protected-access
tensorflow.python.framework.errors_impl.FailedPreconditionError: {{function_node __wrapped__SerializeIterator_device_/job:localhost/replica:0/task:0/device:CPU:0}} SentencepieceOp is stateful. [Op:SerializeIterator] name: 
  In call to configurable 'train' (<function train at 0x7f27002023b0>)
2024-05-12 01:58:46.586258: W tensorflow/core/framework/op_kernel.cc:1839] OP_REQUIRES failed at iterator_ops.cc:1056 : FAILED_PRECONDITION: SentencepieceOp is stateful.
2024-05-12 01:58:46.586339: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: FAILED_PRECONDITION: SentencepieceOp is stateful.
E0512 01:58:46.587170 139997397456896 checkpoints.py:803] Input pipeline must be stateless in order to checkpoint. Cache stateful steps offline or disable iterator checkpointing.
Traceback (most recent call last):
  File "/home/rosinality/./t5x/t5x/train.py", line 964, in <module>
    config_utils.run(main)
  File "/home/rosinality/t5x/t5x/config_utils.py", line 214, in run
    gin_utils.run(main)
  File "/home/rosinality/t5x/t5x/gin_utils.py", line 127, in run
    app.run(
  File "/home/rosinality/openmoe_venv/lib/python3.10/site-packages/absl/app.py", line 308, in run
    _run_main(main, args)
  File "/home/rosinality/openmoe_venv/lib/python3.10/site-packages/absl/app.py", line 254, in _run_main
    sys.exit(main(argv))
  File "/home/rosinality/./t5x/t5x/train.py", line 898, in main
    _main(argv)
  File "/home/rosinality/./t5x/t5x/train.py", line 959, in _main
    train_using_gin()
  File "/home/rosinality/openmoe_venv/lib/python3.10/site-packages/gin/config.py", line 1605, in gin_wrapper
    utils.augment_exception_message_and_reraise(e, err_str)
  File "/home/rosinality/openmoe_venv/lib/python3.10/site-packages/gin/utils.py", line 41, in augment_exception_message_and_reraise
    raise proxy.with_traceback(exception.__traceback__) from None
  File "/home/rosinality/openmoe_venv/lib/python3.10/site-packages/gin/config.py", line 1582, in gin_wrapper
    return fn(*new_args, **new_kwargs)
  File "/home/rosinality/./t5x/t5x/train.py", line 574, in train
    checkpoint_manager.save(
  File "/home/rosinality/t5x/t5x/utils.py", line 406, in save
    self._checkpointer.save(
  File "/home/rosinality/t5x/t5x/utils.py", line 281, in save
    self._save_checkpointer.save(
  File "/home/rosinality/t5x/t5x/checkpoints.py", line 806, in save
    raise e
  File "/home/rosinality/t5x/t5x/checkpoints.py", line 800, in save
    self._dataset_iterator.save(
  File "/home/rosinality/openmoe_venv/lib/python3.10/site-packages/clu/data/dataset_iterator.py", line 288, in save
    self._it.save(filename)
  File "/home/rosinality/t5x/t5x/utils.py", line 697, in save
    return self._iterator.save(filename)
  File "/home/rosinality/openmoe_venv/lib/python3.10/site-packages/clu/data/dataset_iterator.py", line 204, in save
    self._ckpt.write(os.fspath(filename))
  File "/home/rosinality/openmoe_venv/lib/python3.10/site-packages/tensorflow/python/checkpoint/checkpoint.py", line 2320, in write
    return self._write(file_prefix, options)
  File "/home/rosinality/openmoe_venv/lib/python3.10/site-packages/tensorflow/python/checkpoint/checkpoint.py", line 2369, in _write
    output = self._saver.save(file_prefix=file_prefix, options=options)
  File "/home/rosinality/openmoe_venv/lib/python3.10/site-packages/tensorflow/python/checkpoint/checkpoint.py", line 1356, in save
    save_path, new_feed_additions = self._save_cached_when_graph_building(
  File "/home/rosinality/openmoe_venv/lib/python3.10/site-packages/tensorflow/python/checkpoint/checkpoint.py", line 1284, in _save_cached_when_graph_building
    self._gather_serialized_tensors(object_graph_tensor))
  File "/home/rosinality/openmoe_venv/lib/python3.10/site-packages/tensorflow/python/checkpoint/checkpoint.py", line 1245, in _gather_serialized_tensors
    save_util.serialize_graph_view(self._graph_view,
  File "/home/rosinality/openmoe_venv/lib/python3.10/site-packages/tensorflow/python/checkpoint/save_util.py", line 309, in serialize_graph_view
    serialized_tensors = _get_and_write_tensors_to_serialize(
  File "/home/rosinality/openmoe_venv/lib/python3.10/site-packages/tensorflow/python/checkpoint/save_util.py", line 157, in _get_and_write_tensors_to_serialize
    trackable, tensor_dict = _get_tensors_from_legacy_saveable(
  File "/home/rosinality/openmoe_venv/lib/python3.10/site-packages/tensorflow/python/checkpoint/save_util.py", line 190, in _get_tensors_from_legacy_saveable
    save_util_v1.generate_saveable_objects(
  File "/home/rosinality/openmoe_venv/lib/python3.10/site-packages/tensorflow/python/checkpoint/save_util_v1.py", line 180, in generate_saveable_objects
    maybe_saveable = saveable_object_util.create_saveable_object(
  File "/home/rosinality/openmoe_venv/lib/python3.10/site-packages/tensorflow/python/training/saving/saveable_object_util.py", line 473, in create_saveable_object
    return factory(name=key)
  File "/home/rosinality/openmoe_venv/lib/python3.10/site-packages/tensorflow/python/training/saving/saveable_object_util.py", line 536, in create_saveable
    tensor_dict = save_fn()
  File "/home/rosinality/openmoe_venv/lib/python3.10/site-packages/tensorflow/python/data/ops/iterator_ops.py", line 883, in _serialize_to_tensors
    serialized_iterator = gen_dataset_ops.serialize_iterator(
  File "/home/rosinality/openmoe_venv/lib/python3.10/site-packages/tensorflow/python/ops/gen_dataset_ops.py", line 6523, in serialize_iterator
    _ops.raise_from_not_ok_status(e, name)
  File "/home/rosinality/openmoe_venv/lib/python3.10/site-packages/tensorflow/python/framework/ops.py", line 5983, in raise_from_not_ok_status
    raise core._status_to_exception(e) from None  # pylint: disable=protected-access
tensorflow.python.framework.errors_impl.FailedPreconditionError: {{function_node __wrapped__SerializeIterator_device_/job:localhost/replica:0/task:0/device:CPU:0}} SentencepieceOp is stateful. [Op:SerializeIterator] name: 
  In call to configurable 'train' (<function train at 0x7f52158063b0>)
##### Command execution on worker 3 failed with exit status 1. Continuing.
I0000 00:00:1715479127.279318  349683 tfrt_cpu_pjrt_client.cc:352] TfrtCpuClient destroyed.
I0000 00:00:1715479127.460580  352750 tfrt_cpu_pjrt_client.cc:352] TfrtCpuClient destroyed.
2024-05-12 01:58:47.614527: W tensorflow/core/framework/op_kernel.cc:1839] OP_REQUIRES failed at iterator_ops.cc:1056 : FAILED_PRECONDITION: SentencepieceOp is stateful.
2024-05-12 01:58:47.614596: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: FAILED_PRECONDITION: SentencepieceOp is stateful.
E0512 01:58:47.615585 139676058949632 checkpoints.py:803] Input pipeline must be stateless in order to checkpoint. Cache stateful steps offline or disable iterator checkpointing.
Traceback (most recent call last):
  File "/home/rosinality/./t5x/t5x/train.py", line 964, in <module>
    config_utils.run(main)
  File "/home/rosinality/t5x/t5x/config_utils.py", line 214, in run
    gin_utils.run(main)
  File "/home/rosinality/t5x/t5x/gin_utils.py", line 127, in run
    app.run(
  File "/home/rosinality/openmoe_venv/lib/python3.10/site-packages/absl/app.py", line 308, in run
    _run_main(main, args)
  File "/home/rosinality/openmoe_venv/lib/python3.10/site-packages/absl/app.py", line 254, in _run_main
    sys.exit(main(argv))
  File "/home/rosinality/./t5x/t5x/train.py", line 898, in main
    _main(argv)
  File "/home/rosinality/./t5x/t5x/train.py", line 959, in _main
    train_using_gin()
  File "/home/rosinality/openmoe_venv/lib/python3.10/site-packages/gin/config.py", line 1605, in gin_wrapper
    utils.augment_exception_message_and_reraise(e, err_str)
  File "/home/rosinality/openmoe_venv/lib/python3.10/site-packages/gin/utils.py", line 41, in augment_exception_message_and_reraise
    raise proxy.with_traceback(exception.__traceback__) from None
  File "/home/rosinality/openmoe_venv/lib/python3.10/site-packages/gin/config.py", line 1582, in gin_wrapper
    return fn(*new_args, **new_kwargs)
  File "/home/rosinality/./t5x/t5x/train.py", line 574, in train
    checkpoint_manager.save(
  File "/home/rosinality/t5x/t5x/utils.py", line 406, in save
    self._checkpointer.save(
  File "/home/rosinality/t5x/t5x/utils.py", line 281, in save
    self._save_checkpointer.save(
  File "/home/rosinality/t5x/t5x/checkpoints.py", line 806, in save
    raise e
  File "/home/rosinality/t5x/t5x/checkpoints.py", line 800, in save
    self._dataset_iterator.save(
  File "/home/rosinality/openmoe_venv/lib/python3.10/site-packages/clu/data/dataset_iterator.py", line 288, in save
    self._it.save(filename)
  File "/home/rosinality/t5x/t5x/utils.py", line 697, in save
    return self._iterator.save(filename)
  File "/home/rosinality/openmoe_venv/lib/python3.10/site-packages/clu/data/dataset_iterator.py", line 204, in save
    self._ckpt.write(os.fspath(filename))
  File "/home/rosinality/openmoe_venv/lib/python3.10/site-packages/tensorflow/python/checkpoint/checkpoint.py", line 2320, in write
    return self._write(file_prefix, options)
  File "/home/rosinality/openmoe_venv/lib/python3.10/site-packages/tensorflow/python/checkpoint/checkpoint.py", line 2369, in _write
    output = self._saver.save(file_prefix=file_prefix, options=options)
  File "/home/rosinality/openmoe_venv/lib/python3.10/site-packages/tensorflow/python/checkpoint/checkpoint.py", line 1356, in save
    save_path, new_feed_additions = self._save_cached_when_graph_building(
  File "/home/rosinality/openmoe_venv/lib/python3.10/site-packages/tensorflow/python/checkpoint/checkpoint.py", line 1284, in _save_cached_when_graph_building
    self._gather_serialized_tensors(object_graph_tensor))
  File "/home/rosinality/openmoe_venv/lib/python3.10/site-packages/tensorflow/python/checkpoint/checkpoint.py", line 1245, in _gather_serialized_tensors
    save_util.serialize_graph_view(self._graph_view,
  File "/home/rosinality/openmoe_venv/lib/python3.10/site-packages/tensorflow/python/checkpoint/save_util.py", line 309, in serialize_graph_view
    serialized_tensors = _get_and_write_tensors_to_serialize(
  File "/home/rosinality/openmoe_venv/lib/python3.10/site-packages/tensorflow/python/checkpoint/save_util.py", line 157, in _get_and_write_tensors_to_serialize
    trackable, tensor_dict = _get_tensors_from_legacy_saveable(
  File "/home/rosinality/openmoe_venv/lib/python3.10/site-packages/tensorflow/python/checkpoint/save_util.py", line 190, in _get_tensors_from_legacy_saveable
    save_util_v1.generate_saveable_objects(
  File "/home/rosinality/openmoe_venv/lib/python3.10/site-packages/tensorflow/python/checkpoint/save_util_v1.py", line 180, in generate_saveable_objects
    maybe_saveable = saveable_object_util.create_saveable_object(
  File "/home/rosinality/openmoe_venv/lib/python3.10/site-packages/tensorflow/python/training/saving/saveable_object_util.py", line 473, in create_saveable_object
    return factory(name=key)
  File "/home/rosinality/openmoe_venv/lib/python3.10/site-packages/tensorflow/python/training/saving/saveable_object_util.py", line 536, in create_saveable
    tensor_dict = save_fn()
  File "/home/rosinality/openmoe_venv/lib/python3.10/site-packages/tensorflow/python/data/ops/iterator_ops.py", line 883, in _serialize_to_tensors
    serialized_iterator = gen_dataset_ops.serialize_iterator(
  File "/home/rosinality/openmoe_venv/lib/python3.10/site-packages/tensorflow/python/ops/gen_dataset_ops.py", line 6523, in serialize_iterator
    _ops.raise_from_not_ok_status(e, name)
  File "/home/rosinality/openmoe_venv/lib/python3.10/site-packages/tensorflow/python/framework/ops.py", line 5983, in raise_from_not_ok_status
    raise core._status_to_exception(e) from None  # pylint: disable=protected-access
tensorflow.python.framework.errors_impl.FailedPreconditionError: {{function_node __wrapped__SerializeIterator_device_/job:localhost/replica:0/task:0/device:CPU:0}} SentencepieceOp is stateful. [Op:SerializeIterator] name: 
  In call to configurable 'train' (<function train at 0x7f07442c63b0>)
I0000 00:00:1715479128.464615  384045 tfrt_cpu_pjrt_client.cc:352] TfrtCpuClient destroyed.
2024-05-12 01:58:52.850324: W tensorflow/core/framework/op_kernel.cc:1839] OP_REQUIRES failed at iterator_ops.cc:1056 : FAILED_PRECONDITION: SentencepieceOp is stateful.
2024-05-12 01:58:52.850391: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: FAILED_PRECONDITION: SentencepieceOp is stateful.
E0512 01:58:52.850918 140392160929792 checkpoints.py:803] Input pipeline must be stateless in order to checkpoint. Cache stateful steps offline or disable iterator checkpointing.
Traceback (most recent call last):
  File "/home/rosinality/./t5x/t5x/train.py", line 964, in <module>
    config_utils.run(main)
  File "/home/rosinality/t5x/t5x/config_utils.py", line 214, in run
    gin_utils.run(main)
  File "/home/rosinality/t5x/t5x/gin_utils.py", line 127, in run
    app.run(
  File "/home/rosinality/openmoe_venv/lib/python3.10/site-packages/absl/app.py", line 308, in run
    _run_main(main, args)
  File "/home/rosinality/openmoe_venv/lib/python3.10/site-packages/absl/app.py", line 254, in _run_main
    sys.exit(main(argv))
  File "/home/rosinality/./t5x/t5x/train.py", line 898, in main
    _main(argv)
  File "/home/rosinality/./t5x/t5x/train.py", line 959, in _main
    train_using_gin()
  File "/home/rosinality/openmoe_venv/lib/python3.10/site-packages/gin/config.py", line 1605, in gin_wrapper
    utils.augment_exception_message_and_reraise(e, err_str)
  File "/home/rosinality/openmoe_venv/lib/python3.10/site-packages/gin/utils.py", line 41, in augment_exception_message_and_reraise
    raise proxy.with_traceback(exception.__traceback__) from None
  File "/home/rosinality/openmoe_venv/lib/python3.10/site-packages/gin/config.py", line 1582, in gin_wrapper
    return fn(*new_args, **new_kwargs)
  File "/home/rosinality/./t5x/t5x/train.py", line 574, in train
    checkpoint_manager.save(
  File "/home/rosinality/t5x/t5x/utils.py", line 406, in save
    self._checkpointer.save(
  File "/home/rosinality/t5x/t5x/utils.py", line 281, in save
    self._save_checkpointer.save(
  File "/home/rosinality/t5x/t5x/checkpoints.py", line 806, in save
    raise e
  File "/home/rosinality/t5x/t5x/checkpoints.py", line 800, in save
    self._dataset_iterator.save(
  File "/home/rosinality/openmoe_venv/lib/python3.10/site-packages/clu/data/dataset_iterator.py", line 288, in save
    self._it.save(filename)
  File "/home/rosinality/t5x/t5x/utils.py", line 697, in save
    return self._iterator.save(filename)
  File "/home/rosinality/openmoe_venv/lib/python3.10/site-packages/clu/data/dataset_iterator.py", line 204, in save
    self._ckpt.write(os.fspath(filename))
  File "/home/rosinality/openmoe_venv/lib/python3.10/site-packages/tensorflow/python/checkpoint/checkpoint.py", line 2320, in write
    return self._write(file_prefix, options)
  File "/home/rosinality/openmoe_venv/lib/python3.10/site-packages/tensorflow/python/checkpoint/checkpoint.py", line 2369, in _write
    output = self._saver.save(file_prefix=file_prefix, options=options)
  File "/home/rosinality/openmoe_venv/lib/python3.10/site-packages/tensorflow/python/checkpoint/checkpoint.py", line 1356, in save
    save_path, new_feed_additions = self._save_cached_when_graph_building(
  File "/home/rosinality/openmoe_venv/lib/python3.10/site-packages/tensorflow/python/checkpoint/checkpoint.py", line 1284, in _save_cached_when_graph_building
    self._gather_serialized_tensors(object_graph_tensor))
  File "/home/rosinality/openmoe_venv/lib/python3.10/site-packages/tensorflow/python/checkpoint/checkpoint.py", line 1245, in _gather_serialized_tensors
    save_util.serialize_graph_view(self._graph_view,
  File "/home/rosinality/openmoe_venv/lib/python3.10/site-packages/tensorflow/python/checkpoint/save_util.py", line 309, in serialize_graph_view
    serialized_tensors = _get_and_write_tensors_to_serialize(
  File "/home/rosinality/openmoe_venv/lib/python3.10/site-packages/tensorflow/python/checkpoint/save_util.py", line 157, in _get_and_write_tensors_to_serialize
    trackable, tensor_dict = _get_tensors_from_legacy_saveable(
  File "/home/rosinality/openmoe_venv/lib/python3.10/site-packages/tensorflow/python/checkpoint/save_util.py", line 190, in _get_tensors_from_legacy_saveable
    save_util_v1.generate_saveable_objects(
  File "/home/rosinality/openmoe_venv/lib/python3.10/site-packages/tensorflow/python/checkpoint/save_util_v1.py", line 180, in generate_saveable_objects
    maybe_saveable = saveable_object_util.create_saveable_object(
  File "/home/rosinality/openmoe_venv/lib/python3.10/site-packages/tensorflow/python/training/saving/saveable_object_util.py", line 473, in create_saveable_object
    return factory(name=key)
  File "/home/rosinality/openmoe_venv/lib/python3.10/site-packages/tensorflow/python/training/saving/saveable_object_util.py", line 536, in create_saveable
    tensor_dict = save_fn()
  File "/home/rosinality/openmoe_venv/lib/python3.10/site-packages/tensorflow/python/data/ops/iterator_ops.py", line 883, in _serialize_to_tensors
    serialized_iterator = gen_dataset_ops.serialize_iterator(
  File "/home/rosinality/openmoe_venv/lib/python3.10/site-packages/tensorflow/python/ops/gen_dataset_ops.py", line 6523, in serialize_iterator
    _ops.raise_from_not_ok_status(e, name)
  File "/home/rosinality/openmoe_venv/lib/python3.10/site-packages/tensorflow/python/framework/ops.py", line 5983, in raise_from_not_ok_status
    raise core._status_to_exception(e) from None  # pylint: disable=protected-access
tensorflow.python.framework.errors_impl.FailedPreconditionError: {{function_node __wrapped__SerializeIterator_device_/job:localhost/replica:0/task:0/device:CPU:0}} SentencepieceOp is stateful. [Op:SerializeIterator] name: 
  In call to configurable 'train' (<function train at 0x7fadff2923b0>)
I0000 00:00:1715479133.628776  353284 tfrt_cpu_pjrt_client.cc:352] TfrtCpuClient destroyed.
2024-05-12 01:58:55.870721: W tensorflow/core/framework/op_kernel.cc:1839] OP_REQUIRES failed at iterator_ops.cc:1056 : FAILED_PRECONDITION: SentencepieceOp is stateful.
2024-05-12 01:58:55.870798: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: FAILED_PRECONDITION: SentencepieceOp is stateful.
E0512 01:58:55.871852 140172769531904 checkpoints.py:803] Input pipeline must be stateless in order to checkpoint. Cache stateful steps offline or disable iterator checkpointing.
Traceback (most recent call last):
  File "/home/rosinality/./t5x/t5x/train.py", line 964, in <module>
    config_utils.run(main)
  File "/home/rosinality/t5x/t5x/config_utils.py", line 214, in run
    gin_utils.run(main)
  File "/home/rosinality/t5x/t5x/gin_utils.py", line 127, in run
    app.run(
  File "/home/rosinality/openmoe_venv/lib/python3.10/site-packages/absl/app.py", line 308, in run
    _run_main(main, args)
  File "/home/rosinality/openmoe_venv/lib/python3.10/site-packages/absl/app.py", line 254, in _run_main
    sys.exit(main(argv))
  File "/home/rosinality/./t5x/t5x/train.py", line 898, in main
    _main(argv)
  File "/home/rosinality/./t5x/t5x/train.py", line 959, in _main
    train_using_gin()
  File "/home/rosinality/openmoe_venv/lib/python3.10/site-packages/gin/config.py", line 1605, in gin_wrapper
    utils.augment_exception_message_and_reraise(e, err_str)
  File "/home/rosinality/openmoe_venv/lib/python3.10/site-packages/gin/utils.py", line 41, in augment_exception_message_and_reraise
    raise proxy.with_traceback(exception.__traceback__) from None
  File "/home/rosinality/openmoe_venv/lib/python3.10/site-packages/gin/config.py", line 1582, in gin_wrapper
    return fn(*new_args, **new_kwargs)
  File "/home/rosinality/./t5x/t5x/train.py", line 574, in train
    checkpoint_manager.save(
  File "/home/rosinality/t5x/t5x/utils.py", line 406, in save
    self._checkpointer.save(
  File "/home/rosinality/t5x/t5x/utils.py", line 281, in save
    self._save_checkpointer.save(
  File "/home/rosinality/t5x/t5x/checkpoints.py", line 806, in save
    raise e
  File "/home/rosinality/t5x/t5x/checkpoints.py", line 800, in save
    self._dataset_iterator.save(
  File "/home/rosinality/openmoe_venv/lib/python3.10/site-packages/clu/data/dataset_iterator.py", line 288, in save
    self._it.save(filename)
  File "/home/rosinality/t5x/t5x/utils.py", line 697, in save
    return self._iterator.save(filename)
  File "/home/rosinality/openmoe_venv/lib/python3.10/site-packages/clu/data/dataset_iterator.py", line 204, in save
    self._ckpt.write(os.fspath(filename))
  File "/home/rosinality/openmoe_venv/lib/python3.10/site-packages/tensorflow/python/checkpoint/checkpoint.py", line 2320, in write
    return self._write(file_prefix, options)
  File "/home/rosinality/openmoe_venv/lib/python3.10/site-packages/tensorflow/python/checkpoint/checkpoint.py", line 2369, in _write
    output = self._saver.save(file_prefix=file_prefix, options=options)
  File "/home/rosinality/openmoe_venv/lib/python3.10/site-packages/tensorflow/python/checkpoint/checkpoint.py", line 1356, in save
    save_path, new_feed_additions = self._save_cached_when_graph_building(
  File "/home/rosinality/openmoe_venv/lib/python3.10/site-packages/tensorflow/python/checkpoint/checkpoint.py", line 1284, in _save_cached_when_graph_building
    self._gather_serialized_tensors(object_graph_tensor))
  File "/home/rosinality/openmoe_venv/lib/python3.10/site-packages/tensorflow/python/checkpoint/checkpoint.py", line 1245, in _gather_serialized_tensors
    save_util.serialize_graph_view(self._graph_view,
  File "/home/rosinality/openmoe_venv/lib/python3.10/site-packages/tensorflow/python/checkpoint/save_util.py", line 309, in serialize_graph_view
    serialized_tensors = _get_and_write_tensors_to_serialize(
  File "/home/rosinality/openmoe_venv/lib/python3.10/site-packages/tensorflow/python/checkpoint/save_util.py", line 157, in _get_and_write_tensors_to_serialize
    trackable, tensor_dict = _get_tensors_from_legacy_saveable(
  File "/home/rosinality/openmoe_venv/lib/python3.10/site-packages/tensorflow/python/checkpoint/save_util.py", line 190, in _get_tensors_from_legacy_saveable
    save_util_v1.generate_saveable_objects(
  File "/home/rosinality/openmoe_venv/lib/python3.10/site-packages/tensorflow/python/checkpoint/save_util_v1.py", line 180, in generate_saveable_objects
    maybe_saveable = saveable_object_util.create_saveable_object(
  File "/home/rosinality/openmoe_venv/lib/python3.10/site-packages/tensorflow/python/training/saving/saveable_object_util.py", line 473, in create_saveable_object
    return factory(name=key)
  File "/home/rosinality/openmoe_venv/lib/python3.10/site-packages/tensorflow/python/training/saving/saveable_object_util.py", line 536, in create_saveable
    tensor_dict = save_fn()
  File "/home/rosinality/openmoe_venv/lib/python3.10/site-packages/tensorflow/python/data/ops/iterator_ops.py", line 883, in _serialize_to_tensors
    serialized_iterator = gen_dataset_ops.serialize_iterator(
  File "/home/rosinality/openmoe_venv/lib/python3.10/site-packages/tensorflow/python/ops/gen_dataset_ops.py", line 6523, in serialize_iterator
    _ops.raise_from_not_ok_status(e, name)
  File "/home/rosinality/openmoe_venv/lib/python3.10/site-packages/tensorflow/python/framework/ops.py", line 5983, in raise_from_not_ok_status
    raise core._status_to_exception(e) from None  # pylint: disable=protected-access
tensorflow.python.framework.errors_impl.FailedPreconditionError: {{function_node __wrapped__SerializeIterator_device_/job:localhost/replica:0/task:0/device:CPU:0}} SentencepieceOp is stateful. [Op:SerializeIterator] name: 
  In call to configurable 'train' (<function train at 0x7f7aea5e63b0>)
2024-05-12 01:58:56.176963: W tensorflow/core/framework/op_kernel.cc:1839] OP_REQUIRES failed at iterator_ops.cc:1056 : FAILED_PRECONDITION: SentencepieceOp is stateful.
2024-05-12 01:58:56.177044: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: FAILED_PRECONDITION: SentencepieceOp is stateful.
E0512 01:58:56.177594 140316961150976 checkpoints.py:803] Input pipeline must be stateless in order to checkpoint. Cache stateful steps offline or disable iterator checkpointing.
Traceback (most recent call last):
  File "/home/rosinality/./t5x/t5x/train.py", line 964, in <module>
    config_utils.run(main)
  File "/home/rosinality/t5x/t5x/config_utils.py", line 214, in run
    gin_utils.run(main)
  File "/home/rosinality/t5x/t5x/gin_utils.py", line 127, in run
    app.run(
  File "/home/rosinality/openmoe_venv/lib/python3.10/site-packages/absl/app.py", line 308, in run
    _run_main(main, args)
  File "/home/rosinality/openmoe_venv/lib/python3.10/site-packages/absl/app.py", line 254, in _run_main
    sys.exit(main(argv))
  File "/home/rosinality/./t5x/t5x/train.py", line 898, in main
    _main(argv)
  File "/home/rosinality/./t5x/t5x/train.py", line 959, in _main
    train_using_gin()
  File "/home/rosinality/openmoe_venv/lib/python3.10/site-packages/gin/config.py", line 1605, in gin_wrapper
    utils.augment_exception_message_and_reraise(e, err_str)
  File "/home/rosinality/openmoe_venv/lib/python3.10/site-packages/gin/utils.py", line 41, in augment_exception_message_and_reraise
    raise proxy.with_traceback(exception.__traceback__) from None
  File "/home/rosinality/openmoe_venv/lib/python3.10/site-packages/gin/config.py", line 1582, in gin_wrapper
    return fn(*new_args, **new_kwargs)
  File "/home/rosinality/./t5x/t5x/train.py", line 574, in train
    checkpoint_manager.save(
  File "/home/rosinality/t5x/t5x/utils.py", line 406, in save
    self._checkpointer.save(
  File "/home/rosinality/t5x/t5x/utils.py", line 281, in save
    self._save_checkpointer.save(
  File "/home/rosinality/t5x/t5x/checkpoints.py", line 806, in save
    raise e
  File "/home/rosinality/t5x/t5x/checkpoints.py", line 800, in save
    self._dataset_iterator.save(
  File "/home/rosinality/openmoe_venv/lib/python3.10/site-packages/clu/data/dataset_iterator.py", line 288, in save
    self._it.save(filename)
  File "/home/rosinality/t5x/t5x/utils.py", line 697, in save
    return self._iterator.save(filename)
  File "/home/rosinality/openmoe_venv/lib/python3.10/site-packages/clu/data/dataset_iterator.py", line 204, in save
    self._ckpt.write(os.fspath(filename))
  File "/home/rosinality/openmoe_venv/lib/python3.10/site-packages/tensorflow/python/checkpoint/checkpoint.py", line 2320, in write
    return self._write(file_prefix, options)
  File "/home/rosinality/openmoe_venv/lib/python3.10/site-packages/tensorflow/python/checkpoint/checkpoint.py", line 2369, in _write
    output = self._saver.save(file_prefix=file_prefix, options=options)
  File "/home/rosinality/openmoe_venv/lib/python3.10/site-packages/tensorflow/python/checkpoint/checkpoint.py", line 1356, in save
    save_path, new_feed_additions = self._save_cached_when_graph_building(
  File "/home/rosinality/openmoe_venv/lib/python3.10/site-packages/tensorflow/python/checkpoint/checkpoint.py", line 1284, in _save_cached_when_graph_building
    self._gather_serialized_tensors(object_graph_tensor))
  File "/home/rosinality/openmoe_venv/lib/python3.10/site-packages/tensorflow/python/checkpoint/checkpoint.py", line 1245, in _gather_serialized_tensors
    save_util.serialize_graph_view(self._graph_view,
  File "/home/rosinality/openmoe_venv/lib/python3.10/site-packages/tensorflow/python/checkpoint/save_util.py", line 309, in serialize_graph_view
    serialized_tensors = _get_and_write_tensors_to_serialize(
  File "/home/rosinality/openmoe_venv/lib/python3.10/site-packages/tensorflow/python/checkpoint/save_util.py", line 157, in _get_and_write_tensors_to_serialize
    trackable, tensor_dict = _get_tensors_from_legacy_saveable(
  File "/home/rosinality/openmoe_venv/lib/python3.10/site-packages/tensorflow/python/checkpoint/save_util.py", line 190, in _get_tensors_from_legacy_saveable
    save_util_v1.generate_saveable_objects(
  File "/home/rosinality/openmoe_venv/lib/python3.10/site-packages/tensorflow/python/checkpoint/save_util_v1.py", line 180, in generate_saveable_objects
    maybe_saveable = saveable_object_util.create_saveable_object(
  File "/home/rosinality/openmoe_venv/lib/python3.10/site-packages/tensorflow/python/training/saving/saveable_object_util.py", line 473, in create_saveable_object
    return factory(name=key)
  File "/home/rosinality/openmoe_venv/lib/python3.10/site-packages/tensorflow/python/training/saving/saveable_object_util.py", line 536, in create_saveable
    tensor_dict = save_fn()
  File "/home/rosinality/openmoe_venv/lib/python3.10/site-packages/tensorflow/python/data/ops/iterator_ops.py", line 883, in _serialize_to_tensors
    serialized_iterator = gen_dataset_ops.serialize_iterator(
  File "/home/rosinality/openmoe_venv/lib/python3.10/site-packages/tensorflow/python/ops/gen_dataset_ops.py", line 6523, in serialize_iterator
    _ops.raise_from_not_ok_status(e, name)
  File "/home/rosinality/openmoe_venv/lib/python3.10/site-packages/tensorflow/python/framework/ops.py", line 5983, in raise_from_not_ok_status
    raise core._status_to_exception(e) from None  # pylint: disable=protected-access
tensorflow.python.framework.errors_impl.FailedPreconditionError: {{function_node __wrapped__SerializeIterator_device_/job:localhost/replica:0/task:0/device:CPU:0}} SentencepieceOp is stateful. [Op:SerializeIterator] name: 
  In call to configurable 'train' (<function train at 0x7f9c7ce5e3b0>)
##### Command execution on worker 2 failed with exit status 1. Continuing.
##### Command execution on worker 4 failed with exit status 1. Continuing.
I0000 00:00:1715479136.672109  373603 tfrt_cpu_pjrt_client.cc:352] TfrtCpuClient destroyed.
I0000 00:00:1715479136.956552  382697 tfrt_cpu_pjrt_client.cc:352] TfrtCpuClient destroyed.
##### Command execution on worker 6 failed with exit status 1. Continuing.
##### Command execution on worker 1 failed with exit status 1. Continuing.
2024-05-12 01:59:04.818450: W tensorflow/core/framework/op_kernel.cc:1839] OP_REQUIRES failed at iterator_ops.cc:1056 : FAILED_PRECONDITION: SentencepieceOp is stateful.
2024-05-12 01:59:04.818528: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: FAILED_PRECONDITION: SentencepieceOp is stateful.
E0512 01:59:04.819578 140188559153152 checkpoints.py:803] Input pipeline must be stateless in order to checkpoint. Cache stateful steps offline or disable iterator checkpointing.
Traceback (most recent call last):
  File "/home/rosinality/./t5x/t5x/train.py", line 964, in <module>
    config_utils.run(main)
  File "/home/rosinality/t5x/t5x/config_utils.py", line 214, in run
    gin_utils.run(main)
  File "/home/rosinality/t5x/t5x/gin_utils.py", line 127, in run
    app.run(
  File "/home/rosinality/openmoe_venv/lib/python3.10/site-packages/absl/app.py", line 308, in run
    _run_main(main, args)
  File "/home/rosinality/openmoe_venv/lib/python3.10/site-packages/absl/app.py", line 254, in _run_main
    sys.exit(main(argv))
  File "/home/rosinality/./t5x/t5x/train.py", line 898, in main
    _main(argv)
  File "/home/rosinality/./t5x/t5x/train.py", line 959, in _main
    train_using_gin()
  File "/home/rosinality/openmoe_venv/lib/python3.10/site-packages/gin/config.py", line 1605, in gin_wrapper
    utils.augment_exception_message_and_reraise(e, err_str)
  File "/home/rosinality/openmoe_venv/lib/python3.10/site-packages/gin/utils.py", line 41, in augment_exception_message_and_reraise
    raise proxy.with_traceback(exception.__traceback__) from None
  File "/home/rosinality/openmoe_venv/lib/python3.10/site-packages/gin/config.py", line 1582, in gin_wrapper
    return fn(*new_args, **new_kwargs)
  File "/home/rosinality/./t5x/t5x/train.py", line 574, in train
    checkpoint_manager.save(
  File "/home/rosinality/t5x/t5x/utils.py", line 406, in save
    self._checkpointer.save(
  File "/home/rosinality/t5x/t5x/utils.py", line 281, in save
    self._save_checkpointer.save(
  File "/home/rosinality/t5x/t5x/checkpoints.py", line 806, in save
    raise e
  File "/home/rosinality/t5x/t5x/checkpoints.py", line 800, in save
    self._dataset_iterator.save(
  File "/home/rosinality/openmoe_venv/lib/python3.10/site-packages/clu/data/dataset_iterator.py", line 288, in save
    self._it.save(filename)
  File "/home/rosinality/t5x/t5x/utils.py", line 697, in save
    return self._iterator.save(filename)
  File "/home/rosinality/openmoe_venv/lib/python3.10/site-packages/clu/data/dataset_iterator.py", line 204, in save
    self._ckpt.write(os.fspath(filename))
  File "/home/rosinality/openmoe_venv/lib/python3.10/site-packages/tensorflow/python/checkpoint/checkpoint.py", line 2320, in write
    return self._write(file_prefix, options)
  File "/home/rosinality/openmoe_venv/lib/python3.10/site-packages/tensorflow/python/checkpoint/checkpoint.py", line 2369, in _write
    output = self._saver.save(file_prefix=file_prefix, options=options)
  File "/home/rosinality/openmoe_venv/lib/python3.10/site-packages/tensorflow/python/checkpoint/checkpoint.py", line 1356, in save
    save_path, new_feed_additions = self._save_cached_when_graph_building(
  File "/home/rosinality/openmoe_venv/lib/python3.10/site-packages/tensorflow/python/checkpoint/checkpoint.py", line 1284, in _save_cached_when_graph_building
    self._gather_serialized_tensors(object_graph_tensor))
  File "/home/rosinality/openmoe_venv/lib/python3.10/site-packages/tensorflow/python/checkpoint/checkpoint.py", line 1245, in _gather_serialized_tensors
    save_util.serialize_graph_view(self._graph_view,
  File "/home/rosinality/openmoe_venv/lib/python3.10/site-packages/tensorflow/python/checkpoint/save_util.py", line 309, in serialize_graph_view
    serialized_tensors = _get_and_write_tensors_to_serialize(
  File "/home/rosinality/openmoe_venv/lib/python3.10/site-packages/tensorflow/python/checkpoint/save_util.py", line 157, in _get_and_write_tensors_to_serialize
    trackable, tensor_dict = _get_tensors_from_legacy_saveable(
  File "/home/rosinality/openmoe_venv/lib/python3.10/site-packages/tensorflow/python/checkpoint/save_util.py", line 190, in _get_tensors_from_legacy_saveable
    save_util_v1.generate_saveable_objects(
  File "/home/rosinality/openmoe_venv/lib/python3.10/site-packages/tensorflow/python/checkpoint/save_util_v1.py", line 180, in generate_saveable_objects
    maybe_saveable = saveable_object_util.create_saveable_object(
  File "/home/rosinality/openmoe_venv/lib/python3.10/site-packages/tensorflow/python/training/saving/saveable_object_util.py", line 473, in create_saveable_object
    return factory(name=key)
  File "/home/rosinality/openmoe_venv/lib/python3.10/site-packages/tensorflow/python/training/saving/saveable_object_util.py", line 536, in create_saveable
    tensor_dict = save_fn()
  File "/home/rosinality/openmoe_venv/lib/python3.10/site-packages/tensorflow/python/data/ops/iterator_ops.py", line 883, in _serialize_to_tensors
    serialized_iterator = gen_dataset_ops.serialize_iterator(
  File "/home/rosinality/openmoe_venv/lib/python3.10/site-packages/tensorflow/python/ops/gen_dataset_ops.py", line 6523, in serialize_iterator
    _ops.raise_from_not_ok_status(e, name)
  File "/home/rosinality/openmoe_venv/lib/python3.10/site-packages/tensorflow/python/framework/ops.py", line 5983, in raise_from_not_ok_status
    raise core._status_to_exception(e) from None  # pylint: disable=protected-access
tensorflow.python.framework.errors_impl.FailedPreconditionError: {{function_node __wrapped__SerializeIterator_device_/job:localhost/replica:0/task:0/device:CPU:0}} SentencepieceOp is stateful. [Op:SerializeIterator] name: 
  In call to configurable 'train' (<function train at 0x7f7e97a0e3b0>)
I0000 00:00:1715479145.541502  351449 tfrt_cpu_pjrt_client.cc:352] TfrtCpuClient destroyed.
##### Command execution on worker 7 failed with exit status 1. Continuing.
##### Command execution on worker 0 failed with exit status 1. Continuing.
##### Command execution on worker 5 failed with exit status 1. Continuing.
